{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW11: Meta-Learning in Reinforcement Learning\n",
        "\n",
        "> - Full Name: **[Your Full Name]**\n",
        "> - Student ID: **[Your Student ID]**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepRLCourse/Homework-11-Questions/blob/main/HW11_Notebook.ipynb)\n",
        "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/DeepRLCourse/Homework-11-Questions/main/HW11_Notebook.ipynb)\n",
        "\n",
        "## Overview\n",
        "This assignment focuses on **Meta-Learning in Reinforcement Learning**, exploring algorithms that enable agents to quickly adapt to new tasks by leveraging experience from related tasks. We'll implement and experiment with:\n",
        "\n",
        "1. **MAML (Model-Agnostic Meta-Learning)** - Gradient-based meta-learning for RL\n",
        "2. **RLÂ² (Recurrent Meta-RL)** - Black-box meta-learning using recurrent networks\n",
        "3. **PEARL (Probabilistic Embeddings)** - Context-based meta-RL with task embeddings\n",
        "4. **Few-Shot Adaptation** - Rapid learning on new tasks with minimal samples\n",
        "5. **Task Distributions** - Learning across families of related RL tasks\n",
        "\n",
        "The goal is to understand how meta-learning enables \"learning to learn\" and achieves fast adaptation to new tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Imports and Setup\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import trange\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Meta-Learning Environment Setup\n",
        "\n",
        "First, let's create a task distribution for meta-learning. We'll use a parameterized environment where tasks differ in reward functions or dynamics, enabling us to test few-shot adaptation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ParameterizedCartPoleEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Parameterized CartPole environment for meta-learning.\n",
        "    Tasks differ in pole length, mass, or reward structure.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, task_params=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Default task parameters\n",
        "        self.default_params = {\n",
        "            'pole_length': 0.5,\n",
        "            'pole_mass': 0.1,\n",
        "            'cart_mass': 1.0,\n",
        "            'gravity': 9.8,\n",
        "            'reward_scale': 1.0,\n",
        "            'success_threshold': 195.0\n",
        "        }\n",
        "        \n",
        "        # Set task parameters\n",
        "        self.params = self.default_params.copy()\n",
        "        if task_params:\n",
        "            self.params.update(task_params)\n",
        "        \n",
        "        # State space: [cart_pos, cart_vel, pole_angle, pole_vel]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Action space: [push_left, push_right]\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        \n",
        "        # Environment state\n",
        "        self.state = None\n",
        "        self.steps = 0\n",
        "        self.max_steps = 200\n",
        "        \n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset environment with random initial state.\"\"\"\n",
        "        # Random initial state\n",
        "        self.state = np.array([\n",
        "            np.random.uniform(-0.1, 0.1),  # cart position\n",
        "            np.random.uniform(-0.1, 0.1),  # cart velocity\n",
        "            np.random.uniform(-0.1, 0.1),  # pole angle\n",
        "            np.random.uniform(-0.1, 0.1)   # pole velocity\n",
        "        ], dtype=np.float32)\n",
        "        \n",
        "        self.steps = 0\n",
        "        return self.state.copy(), {}\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action and return next state.\"\"\"\n",
        "        if self.state is None:\n",
        "            raise RuntimeError(\"Environment not reset\")\n",
        "        \n",
        "        # Convert action to force\n",
        "        force = 10.0 if action == 1 else -10.0\n",
        "        \n",
        "        # CartPole dynamics with task parameters\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        \n",
        "        # Physical constants\n",
        "        g = self.params['gravity']\n",
        "        m_cart = self.params['cart_mass']\n",
        "        m_pole = self.params['pole_mass']\n",
        "        l = self.params['pole_length']\n",
        "        \n",
        "        # Total mass\n",
        "        total_mass = m_cart + m_pole\n",
        "        \n",
        "        # Dynamics\n",
        "        temp = (force + m_pole * l * theta_dot**2 * np.sin(theta)) / total_mass\n",
        "        theta_acc = (g * np.sin(theta) - np.cos(theta) * temp) / \\\n",
        "                   (l * (4/3 - m_pole * np.cos(theta)**2 / total_mass))\n",
        "        x_acc = temp - m_pole * l * theta_acc * np.cos(theta) / total_mass\n",
        "        \n",
        "        # Update state\n",
        "        x = x + x_dot * 0.02\n",
        "        x_dot = x_dot + x_acc * 0.02\n",
        "        theta = theta + theta_dot * 0.02\n",
        "        theta_dot = theta_dot + theta_acc * 0.02\n",
        "        \n",
        "        self.state = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n",
        "        self.steps += 1\n",
        "        \n",
        "        # Compute reward\n",
        "        reward = self._compute_reward()\n",
        "        \n",
        "        # Check termination\n",
        "        terminated = self._is_terminated()\n",
        "        truncated = self.steps >= self.max_steps\n",
        "        \n",
        "        return self.state.copy(), reward, terminated, truncated, {}\n",
        "    \n",
        "    def _compute_reward(self):\n",
        "        \"\"\"Compute reward based on task parameters.\"\"\"\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        \n",
        "        # Base reward: stay upright and centered\n",
        "        reward = 1.0\n",
        "        \n",
        "        # Penalty for being far from center\n",
        "        reward -= abs(x) * 0.1\n",
        "        \n",
        "        # Penalty for large angle\n",
        "        reward -= abs(theta) * 0.1\n",
        "        \n",
        "        # Scale reward\n",
        "        reward *= self.params['reward_scale']\n",
        "        \n",
        "        return reward\n",
        "    \n",
        "    def _is_terminated(self):\n",
        "        \"\"\"Check if episode should terminate.\"\"\"\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        \n",
        "        # Terminate if cart goes too far\n",
        "        if abs(x) > 2.4:\n",
        "            return True\n",
        "        \n",
        "        # Terminate if pole falls too far\n",
        "        if abs(theta) > 0.2095:  # ~12 degrees\n",
        "            return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def set_task_params(self, task_params):\n",
        "        \"\"\"Set new task parameters.\"\"\"\n",
        "        self.params.update(task_params)\n",
        "\n",
        "\n",
        "class TaskDistribution:\n",
        "    \"\"\"\n",
        "    Distribution over parameterized tasks for meta-learning.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, task_type='pole_length'):\n",
        "        self.task_type = task_type\n",
        "        \n",
        "        if task_type == 'pole_length':\n",
        "            self.param_ranges = {\n",
        "                'pole_length': (0.3, 0.8),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "        elif task_type == 'mass':\n",
        "            self.param_ranges = {\n",
        "                'pole_mass': (0.05, 0.2),\n",
        "                'cart_mass': (0.8, 1.5),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "        elif task_type == 'gravity':\n",
        "            self.param_ranges = {\n",
        "                'gravity': (8.0, 12.0),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "        elif task_type == 'mixed':\n",
        "            self.param_ranges = {\n",
        "                'pole_length': (0.3, 0.8),\n",
        "                'pole_mass': (0.05, 0.2),\n",
        "                'gravity': (8.0, 12.0),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "    \n",
        "    def sample_task(self):\n",
        "        \"\"\"Sample a random task from the distribution.\"\"\"\n",
        "        task_params = {}\n",
        "        for param, (low, high) in self.param_ranges.items():\n",
        "            task_params[param] = np.random.uniform(low, high)\n",
        "        \n",
        "        return ParameterizedCartPoleEnv(task_params)\n",
        "    \n",
        "    def sample_tasks(self, n_tasks):\n",
        "        \"\"\"Sample multiple tasks.\"\"\"\n",
        "        return [self.sample_task() for _ in range(n_tasks)]\n",
        "\n",
        "\n",
        "# Test the environment\n",
        "print(\"Testing Parameterized CartPole Environment...\")\n",
        "\n",
        "# Test different task types\n",
        "task_types = ['pole_length', 'mass', 'gravity', 'mixed']\n",
        "for task_type in task_types:\n",
        "    print(f\"\\nTesting {task_type} tasks:\")\n",
        "    task_dist = TaskDistribution(task_type)\n",
        "    \n",
        "    # Sample a few tasks\n",
        "    for i in range(3):\n",
        "        task = task_dist.sample_task()\n",
        "        obs, _ = task.reset()\n",
        "        \n",
        "        # Run a few steps\n",
        "        total_reward = 0\n",
        "        for step in range(10):\n",
        "            action = task.action_space.sample()\n",
        "            obs, reward, terminated, truncated, _ = task.step(action)\n",
        "            total_reward += reward\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        \n",
        "        print(f\"  Task {i+1}: Reward = {total_reward:.2f}, Params = {task.params}\")\n",
        "\n",
        "print(\"\\nEnvironment test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MAML (Model-Agnostic Meta-Learning) Implementation\n",
        "\n",
        "MAML finds initialization parameters that are good for fine-tuning on new tasks. It uses a two-level optimization process:\n",
        "- **Inner Loop**: Adapt to specific task using gradient descent\n",
        "- **Outer Loop**: Optimize initialization for fast adaptation across tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Policy network for MAML.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Policy network\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "        \n",
        "        # Value network\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass through policy and value networks.\"\"\"\n",
        "        policy_logits = self.policy(state)\n",
        "        value = self.value(state)\n",
        "        return policy_logits, value\n",
        "    \n",
        "    def get_action(self, state, deterministic=False):\n",
        "        \"\"\"Get action from policy.\"\"\"\n",
        "        policy_logits, value = self.forward(state)\n",
        "        \n",
        "        if deterministic:\n",
        "            action = torch.argmax(policy_logits, dim=-1)\n",
        "        else:\n",
        "            action_probs = F.softmax(policy_logits, dim=-1)\n",
        "            action = torch.multinomial(action_probs, 1).squeeze(-1)\n",
        "        \n",
        "        return action, value\n",
        "    \n",
        "    def log_prob(self, state, action):\n",
        "        \"\"\"Get log probability of action.\"\"\"\n",
        "        policy_logits, _ = self.forward(state)\n",
        "        log_probs = F.log_softmax(policy_logits, dim=-1)\n",
        "        return log_probs.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "\n",
        "class MAMLAgent:\n",
        "    \"\"\"\n",
        "    MAML agent for meta-learning in RL.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, meta_lr=0.001, inner_lr=0.01, \n",
        "                 inner_steps=1, hidden_dim=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.meta_lr = meta_lr\n",
        "        self.inner_lr = inner_lr\n",
        "        self.inner_steps = inner_steps\n",
        "        \n",
        "        # Meta-network (initialization)\n",
        "        self.meta_network = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "        self.meta_optimizer = optim.Adam(self.meta_network.parameters(), lr=meta_lr)\n",
        "        \n",
        "        # Copy of network for inner loop updates\n",
        "        self.inner_network = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "        \n",
        "    def collect_trajectories(self, env, network, num_episodes=5, max_steps=200):\n",
        "        \"\"\"Collect trajectories using given network.\"\"\"\n",
        "        trajectories = []\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            episode_data = {\n",
        "                'states': [],\n",
        "                'actions': [],\n",
        "                'rewards': [],\n",
        "                'values': [],\n",
        "                'log_probs': []\n",
        "            }\n",
        "            \n",
        "            for step in range(max_steps):\n",
        "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "                \n",
        "                # Get action and value\n",
        "                action, value = network.get_action(state_tensor)\n",
        "                log_prob = network.log_prob(state_tensor, action)\n",
        "                \n",
        "                # Execute action\n",
        "                next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
        "                done = terminated or truncated\n",
        "                \n",
        "                # Store data\n",
        "                episode_data['states'].append(obs)\n",
        "                episode_data['actions'].append(action.item())\n",
        "                episode_data['rewards'].append(reward)\n",
        "                episode_data['values'].append(value.item())\n",
        "                episode_data['log_probs'].append(log_prob.item())\n",
        "                \n",
        "                obs = next_obs\n",
        "                \n",
        "                if done:\n",
        "                    break\n",
        "            \n",
        "            trajectories.append(episode_data)\n",
        "        \n",
        "        return trajectories\n",
        "    \n",
        "    def compute_returns(self, rewards, gamma=0.99):\n",
        "        \"\"\"Compute discounted returns.\"\"\"\n",
        "        returns = []\n",
        "        G = 0\n",
        "        \n",
        "        for reward in reversed(rewards):\n",
        "            G = reward + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        \n",
        "        return returns\n",
        "    \n",
        "    def compute_advantages(self, returns, values):\n",
        "        \"\"\"Compute advantages using returns and values.\"\"\"\n",
        "        advantages = []\n",
        "        for ret, val in zip(returns, values):\n",
        "            advantages.append(ret - val)\n",
        "        return advantages\n",
        "    \n",
        "    def compute_loss(self, trajectories, network):\n",
        "        \"\"\"Compute policy gradient loss.\"\"\"\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "        \n",
        "        for trajectory in trajectories:\n",
        "            states = torch.FloatTensor(trajectory['states'])\n",
        "            actions = torch.LongTensor(trajectory['actions'])\n",
        "            rewards = trajectory['rewards']\n",
        "            values = torch.FloatTensor(trajectory['values'])\n",
        "            \n",
        "            # Compute returns\n",
        "            returns = self.compute_returns(rewards)\n",
        "            returns = torch.FloatTensor(returns)\n",
        "            \n",
        "            # Compute advantages\n",
        "            advantages = self.compute_advantages(returns, values)\n",
        "            advantages = torch.FloatTensor(advantages)\n",
        "            \n",
        "            # Normalize advantages\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "            \n",
        "            # Policy loss\n",
        "            log_probs = network.log_prob(states, actions)\n",
        "            policy_loss = -(log_probs * advantages).mean()\n",
        "            \n",
        "            # Value loss\n",
        "            predicted_values = network.value(states).squeeze()\n",
        "            value_loss = F.mse_loss(predicted_values, returns)\n",
        "            \n",
        "            # Total loss\n",
        "            loss = policy_loss + 0.5 * value_loss\n",
        "            total_loss += loss * len(states)\n",
        "            total_samples += len(states)\n",
        "        \n",
        "        return total_loss / total_samples if total_samples > 0 else torch.tensor(0.0)\n",
        "    \n",
        "    def inner_loop_update(self, task, network):\n",
        "        \"\"\"Perform inner loop update on a specific task.\"\"\"\n",
        "        # Collect trajectories\n",
        "        trajectories = self.collect_trajectories(task, network, num_episodes=3)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = self.compute_loss(trajectories, network)\n",
        "        \n",
        "        # Compute gradients\n",
        "        gradients = torch.autograd.grad(loss, network.parameters(), create_graph=True)\n",
        "        \n",
        "        # Update parameters\n",
        "        updated_params = []\n",
        "        for param, grad in zip(network.parameters(), gradients):\n",
        "            updated_params.append(param - self.inner_lr * grad)\n",
        "        \n",
        "        return updated_params, loss\n",
        "    \n",
        "    def meta_update(self, tasks, num_tasks=4):\n",
        "        \"\"\"Perform meta-update across multiple tasks.\"\"\"\n",
        "        meta_loss = 0\n",
        "        \n",
        "        for task in tasks[:num_tasks]:\n",
        "            # Copy meta-network for inner loop\n",
        "            self.inner_network.load_state_dict(self.meta_network.state_dict())\n",
        "            \n",
        "            # Inner loop adaptation\n",
        "            updated_params, inner_loss = self.inner_loop_update(task, self.inner_network)\n",
        "            \n",
        "            # Collect test trajectories with adapted network\n",
        "            # Create temporary network with updated parameters\n",
        "            temp_network = PolicyNetwork(self.state_dim, self.action_dim)\n",
        "            temp_network.load_state_dict(self.meta_network.state_dict())\n",
        "            \n",
        "            # Manually set updated parameters\n",
        "            for param, updated_param in zip(temp_network.parameters(), updated_params):\n",
        "                param.data = updated_param\n",
        "            \n",
        "            # Test trajectories\n",
        "            test_trajectories = self.collect_trajectories(task, temp_network, num_episodes=2)\n",
        "            test_loss = self.compute_loss(test_trajectories, temp_network)\n",
        "            \n",
        "            meta_loss += test_loss\n",
        "        \n",
        "        # Meta-gradient update\n",
        "        meta_loss = meta_loss / num_tasks\n",
        "        self.meta_optimizer.zero_grad()\n",
        "        meta_loss.backward()\n",
        "        self.meta_optimizer.step()\n",
        "        \n",
        "        return meta_loss.item()\n",
        "    \n",
        "    def adapt_to_task(self, task, num_adaptation_steps=5):\n",
        "        \"\"\"Adapt to a new task using MAML.\"\"\"\n",
        "        # Copy meta-network\n",
        "        adapted_network = PolicyNetwork(self.state_dim, self.action_dim)\n",
        "        adapted_network.load_state_dict(self.meta_network.state_dict())\n",
        "        \n",
        "        # Inner loop adaptation\n",
        "        for step in range(num_adaptation_steps):\n",
        "            # Collect trajectories\n",
        "            trajectories = self.collect_trajectories(task, adapted_network, num_episodes=2)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = self.compute_loss(trajectories, adapted_network)\n",
        "            \n",
        "            # Update parameters\n",
        "            optimizer = optim.Adam(adapted_network.parameters(), lr=self.inner_lr)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        return adapted_network\n",
        "\n",
        "\n",
        "# Test MAML implementation\n",
        "print(\"Testing MAML Implementation...\")\n",
        "\n",
        "# Create task distribution\n",
        "task_dist = TaskDistribution('pole_length')\n",
        "state_dim = 4  # CartPole state dimension\n",
        "action_dim = 2  # CartPole action dimension\n",
        "\n",
        "# Create MAML agent\n",
        "maml_agent = MAMLAgent(state_dim, action_dim, meta_lr=0.001, inner_lr=0.01)\n",
        "\n",
        "# Test meta-update\n",
        "print(\"Testing meta-update...\")\n",
        "tasks = task_dist.sample_tasks(4)\n",
        "meta_loss = maml_agent.meta_update(tasks)\n",
        "print(f\"Meta-loss: {meta_loss:.4f}\")\n",
        "\n",
        "# Test adaptation\n",
        "print(\"Testing task adaptation...\")\n",
        "new_task = task_dist.sample_task()\n",
        "adapted_network = maml_agent.adapt_to_task(new_task, num_adaptation_steps=3)\n",
        "\n",
        "# Test adapted network\n",
        "obs, _ = new_task.reset()\n",
        "state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "action, value = adapted_network.get_action(state_tensor)\n",
        "print(f\"Adapted network action: {action.item()}, value: {value.item():.4f}\")\n",
        "\n",
        "print(\"MAML test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. RLÂ² (Recurrent Meta-RL) Implementation\n",
        "\n",
        "RLÂ² uses recurrent networks to encode task information implicitly. The LSTM hidden state learns to adapt to different tasks without explicit inner loop optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
