{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW11: Meta-Learning in Reinforcement Learning\n",
        "\n",
        "> - Full Name: **[Your Full Name]**\n",
        "> - Student ID: **[Your Student ID]**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepRLCourse/Homework-11-Questions/blob/main/HW11_Notebook.ipynb)\n",
        "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/DeepRLCourse/Homework-11-Questions/main/HW11_Notebook.ipynb)\n",
        "\n",
        "## Overview\n",
        "This assignment focuses on **Meta-Learning in Reinforcement Learning**, exploring algorithms that enable agents to quickly adapt to new tasks by leveraging experience from related tasks. We'll implement and experiment with:\n",
        "\n",
        "1. **MAML (Model-Agnostic Meta-Learning)** - Gradient-based meta-learning for RL\n",
        "2. **RLÂ² (Recurrent Meta-RL)** - Black-box meta-learning using recurrent networks\n",
        "3. **PEARL (Probabilistic Embeddings)** - Context-based meta-RL with task embeddings\n",
        "4. **Few-Shot Adaptation** - Rapid learning on new tasks with minimal samples\n",
        "5. **Task Distributions** - Learning across families of related RL tasks\n",
        "\n",
        "The goal is to understand how meta-learning enables \"learning to learn\" and achieves fast adaptation to new tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Imports and Setup\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import trange\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Meta-Learning Environment Setup\n",
        "\n",
        "First, let's create a task distribution for meta-learning. We'll use a parameterized environment where tasks differ in reward functions or dynamics, enabling us to test few-shot adaptation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
