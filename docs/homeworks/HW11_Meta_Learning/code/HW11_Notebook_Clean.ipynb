{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW11: Meta-Learning in Reinforcement Learning\n",
        "\n",
        "> - Full Name: **[Your Full Name]**\n",
        "> - Student ID: **[Your Student ID]**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepRLCourse/Homework-11-Questions/blob/main/HW11_Notebook.ipynb)\n",
        "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/DeepRLCourse/Homework-11-Questions/main/HW11_Notebook.ipynb)\n",
        "\n",
        "## Overview\n",
        "This assignment focuses on **Meta-Learning in Reinforcement Learning**, exploring algorithms that enable agents to quickly adapt to new tasks by leveraging experience from related tasks. We'll implement and experiment with:\n",
        "\n",
        "1. **MAML (Model-Agnostic Meta-Learning)** - Gradient-based meta-learning for RL\n",
        "2. **RLÂ² (Recurrent Meta-RL)** - Black-box meta-learning using recurrent networks\n",
        "3. **PEARL (Probabilistic Embeddings)** - Context-based meta-RL with task embeddings\n",
        "4. **Few-Shot Adaptation** - Rapid learning on new tasks with minimal samples\n",
        "5. **Task Distributions** - Learning across families of related RL tasks\n",
        "\n",
        "The goal is to understand how meta-learning enables \"learning to learn\" and achieves fast adaptation to new tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Imports and Setup\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import trange\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Meta-Learning Environment Setup\n",
        "\n",
        "First, let's create a task distribution for meta-learning. We'll use a parameterized environment where tasks differ in reward functions or dynamics, enabling us to test few-shot adaptation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ParameterizedCartPoleEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Parameterized CartPole environment for meta-learning.\n",
        "    Tasks differ in pole length, mass, or reward structure.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, task_params=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Default task parameters\n",
        "        self.default_params = {\n",
        "            'pole_length': 0.5,\n",
        "            'pole_mass': 0.1,\n",
        "            'cart_mass': 1.0,\n",
        "            'gravity': 9.8,\n",
        "            'reward_scale': 1.0,\n",
        "            'success_threshold': 195.0\n",
        "        }\n",
        "        \n",
        "        # Set task parameters\n",
        "        self.params = self.default_params.copy()\n",
        "        if task_params:\n",
        "            self.params.update(task_params)\n",
        "        \n",
        "        # State space: [cart_pos, cart_vel, pole_angle, pole_vel]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Action space: [push_left, push_right]\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        \n",
        "        # Environment state\n",
        "        self.state = None\n",
        "        self.steps = 0\n",
        "        self.max_steps = 200\n",
        "        \n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset environment with random initial state.\"\"\"\n",
        "        # Random initial state\n",
        "        self.state = np.array([\n",
        "            np.random.uniform(-0.1, 0.1),  # cart position\n",
        "            np.random.uniform(-0.1, 0.1),  # cart velocity\n",
        "            np.random.uniform(-0.1, 0.1),  # pole angle\n",
        "            np.random.uniform(-0.1, 0.1)   # pole velocity\n",
        "        ], dtype=np.float32)\n",
        "        \n",
        "        self.steps = 0\n",
        "        return self.state.copy(), {}\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action and return next state.\"\"\"\n",
        "        if self.state is None:\n",
        "            raise RuntimeError(\"Environment not reset\")\n",
        "        \n",
        "        # Convert action to force\n",
        "        force = 10.0 if action == 1 else -10.0\n",
        "        \n",
        "        # CartPole dynamics with task parameters\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        \n",
        "        # Physical constants\n",
        "        g = self.params['gravity']\n",
        "        m_cart = self.params['cart_mass']\n",
        "        m_pole = self.params['pole_mass']\n",
        "        l = self.params['pole_length']\n",
        "        \n",
        "        # Total mass\n",
        "        total_mass = m_cart + m_pole\n",
        "        \n",
        "        # Dynamics\n",
        "        temp = (force + m_pole * l * theta_dot**2 * np.sin(theta)) / total_mass\n",
        "        theta_acc = (g * np.sin(theta) - np.cos(theta) * temp) / \\\n",
        "                   (l * (4/3 - m_pole * np.cos(theta)**2 / total_mass))\n",
        "        x_acc = temp - m_pole * l * theta_acc * np.cos(theta) / total_mass\n",
        "        \n",
        "        # Update state\n",
        "        x = x + x_dot * 0.02\n",
        "        x_dot = x_dot + x_acc * 0.02\n",
        "        theta = theta + theta_dot * 0.02\n",
        "        theta_dot = theta_dot + theta_acc * 0.02\n",
        "        \n",
        "        self.state = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n",
        "        self.steps += 1\n",
        "        \n",
        "        # Compute reward\n",
        "        reward = self._compute_reward()\n",
        "        \n",
        "        # Check termination\n",
        "        terminated = self._is_terminated()\n",
        "        truncated = self.steps >= self.max_steps\n",
        "        \n",
        "        return self.state.copy(), reward, terminated, truncated, {}\n",
        "    \n",
        "    def _compute_reward(self):\n",
        "        \"\"\"Compute reward based on task parameters.\"\"\"\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        \n",
        "        # Base reward: stay upright and centered\n",
        "        reward = 1.0\n",
        "        \n",
        "        # Penalty for being far from center\n",
        "        reward -= abs(x) * 0.1\n",
        "        \n",
        "        # Penalty for large angle\n",
        "        reward -= abs(theta) * 0.1\n",
        "        \n",
        "        # Scale reward\n",
        "        reward *= self.params['reward_scale']\n",
        "        \n",
        "        return reward\n",
        "    \n",
        "    def _is_terminated(self):\n",
        "        \"\"\"Check if episode should terminate.\"\"\"\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        \n",
        "        # Terminate if cart goes too far\n",
        "        if abs(x) > 2.4:\n",
        "            return True\n",
        "        \n",
        "        # Terminate if pole falls too far\n",
        "        if abs(theta) > 0.2095:  # ~12 degrees\n",
        "            return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def set_task_params(self, task_params):\n",
        "        \"\"\"Set new task parameters.\"\"\"\n",
        "        self.params.update(task_params)\n",
        "\n",
        "\n",
        "class TaskDistribution:\n",
        "    \"\"\"\n",
        "    Distribution over parameterized tasks for meta-learning.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, task_type='pole_length'):\n",
        "        self.task_type = task_type\n",
        "        \n",
        "        if task_type == 'pole_length':\n",
        "            self.param_ranges = {\n",
        "                'pole_length': (0.3, 0.8),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "        elif task_type == 'mass':\n",
        "            self.param_ranges = {\n",
        "                'pole_mass': (0.05, 0.2),\n",
        "                'cart_mass': (0.8, 1.5),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "        elif task_type == 'gravity':\n",
        "            self.param_ranges = {\n",
        "                'gravity': (8.0, 12.0),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "        elif task_type == 'mixed':\n",
        "            self.param_ranges = {\n",
        "                'pole_length': (0.3, 0.8),\n",
        "                'pole_mass': (0.05, 0.2),\n",
        "                'gravity': (8.0, 12.0),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "    \n",
        "    def sample_task(self):\n",
        "        \"\"\"Sample a random task from the distribution.\"\"\"\n",
        "        task_params = {}\n",
        "        for param, (low, high) in self.param_ranges.items():\n",
        "            task_params[param] = np.random.uniform(low, high)\n",
        "        \n",
        "        return ParameterizedCartPoleEnv(task_params)\n",
        "    \n",
        "    def sample_tasks(self, n_tasks):\n",
        "        \"\"\"Sample multiple tasks.\"\"\"\n",
        "        return [self.sample_task() for _ in range(n_tasks)]\n",
        "\n",
        "\n",
        "# Test the environment\n",
        "print(\"Testing Parameterized CartPole Environment...\")\n",
        "\n",
        "# Test different task types\n",
        "task_types = ['pole_length', 'mass', 'gravity', 'mixed']\n",
        "for task_type in task_types:\n",
        "    print(f\"\\nTesting {task_type} tasks:\")\n",
        "    task_dist = TaskDistribution(task_type)\n",
        "    \n",
        "    # Sample a few tasks\n",
        "    for i in range(3):\n",
        "        task = task_dist.sample_task()\n",
        "        obs, _ = task.reset()\n",
        "        \n",
        "        # Run a few steps\n",
        "        total_reward = 0\n",
        "        for step in range(10):\n",
        "            action = task.action_space.sample()\n",
        "            obs, reward, terminated, truncated, _ = task.step(action)\n",
        "            total_reward += reward\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        \n",
        "        print(f\"  Task {i+1}: Reward = {total_reward:.2f}, Params = {task.params}\")\n",
        "\n",
        "print(\"\\nEnvironment test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MAML (Model-Agnostic Meta-Learning) Implementation\n",
        "\n",
        "MAML finds initialization parameters that are good for fine-tuning on new tasks. It uses a two-level optimization process:\n",
        "- **Inner Loop**: Adapt to specific task using gradient descent\n",
        "- **Outer Loop**: Optimize initialization for fast adaptation across tasks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
