{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW11: Meta-Learning in Reinforcement Learning\n",
        "\n",
        "> - Full Name: **[Your Full Name]**\n",
        "> - Student ID: **[Your Student ID]**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepRLCourse/Homework-11-Questions/blob/main/HW11_Notebook.ipynb)\n",
        "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/DeepRLCourse/Homework-11-Questions/main/HW11_Notebook.ipynb)\n",
        "\n",
        "## Overview\n",
        "This assignment focuses on **Meta-Learning in Reinforcement Learning**, exploring algorithms that enable agents to quickly adapt to new tasks by leveraging experience from related tasks. We'll implement and experiment with:\n",
        "\n",
        "1. **MAML (Model-Agnostic Meta-Learning)** - Gradient-based meta-learning for RL\n",
        "2. **RLÂ² (Recurrent Meta-RL)** - Black-box meta-learning using recurrent networks\n",
        "3. **PEARL (Probabilistic Embeddings)** - Context-based meta-RL with task embeddings\n",
        "4. **Few-Shot Adaptation** - Rapid learning on new tasks with minimal samples\n",
        "5. **Task Distributions** - Learning across families of related RL tasks\n",
        "\n",
        "The goal is to understand how meta-learning enables \"learning to learn\" and achieves fast adaptation to new tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Imports and Setup\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import trange\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Meta-Learning Environment Setup\n",
        "\n",
        "First, let's create a task distribution for meta-learning. We'll use a parameterized environment where tasks differ in reward functions or dynamics, enabling us to test few-shot adaptation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ParameterizedCartPoleEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Parameterized CartPole environment for meta-learning.\n",
        "    Tasks differ in pole length, mass, or reward structure.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, task_params=None):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Default task parameters\n",
        "        self.default_params = {\n",
        "            'pole_length': 0.5,\n",
        "            'pole_mass': 0.1,\n",
        "            'cart_mass': 1.0,\n",
        "            'gravity': 9.8,\n",
        "            'reward_scale': 1.0,\n",
        "            'success_threshold': 195.0\n",
        "        }\n",
        "        \n",
        "        # Set task parameters\n",
        "        self.params = self.default_params.copy()\n",
        "        if task_params:\n",
        "            self.params.update(task_params)\n",
        "        \n",
        "        # State space: [cart_pos, cart_vel, pole_angle, pole_vel]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(4,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Action space: [push_left, push_right]\n",
        "        self.action_space = spaces.Discrete(2)\n",
        "        \n",
        "        # Environment state\n",
        "        self.state = None\n",
        "        self.steps = 0\n",
        "        self.max_steps = 200\n",
        "        \n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset environment with random initial state.\"\"\"\n",
        "        # Random initial state\n",
        "        self.state = np.array([\n",
        "            np.random.uniform(-0.1, 0.1),  # cart position\n",
        "            np.random.uniform(-0.1, 0.1),  # cart velocity\n",
        "            np.random.uniform(-0.1, 0.1),  # pole angle\n",
        "            np.random.uniform(-0.1, 0.1)   # pole velocity\n",
        "        ], dtype=np.float32)\n",
        "        \n",
        "        self.steps = 0\n",
        "        return self.state.copy(), {}\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action and return next state.\"\"\"\n",
        "        if self.state is None:\n",
        "            raise RuntimeError(\"Environment not reset\")\n",
        "        \n",
        "        # Convert action to force\n",
        "        force = 10.0 if action == 1 else -10.0\n",
        "        \n",
        "        # CartPole dynamics with task parameters\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        \n",
        "        # Physical constants\n",
        "        g = self.params['gravity']\n",
        "        m_cart = self.params['cart_mass']\n",
        "        m_pole = self.params['pole_mass']\n",
        "        l = self.params['pole_length']\n",
        "        \n",
        "        # Total mass\n",
        "        total_mass = m_cart + m_pole\n",
        "        \n",
        "        # Dynamics\n",
        "        temp = (force + m_pole * l * theta_dot**2 * np.sin(theta)) / total_mass\n",
        "        theta_acc = (g * np.sin(theta) - np.cos(theta) * temp) / \\\n",
        "                   (l * (4/3 - m_pole * np.cos(theta)**2 / total_mass))\n",
        "        x_acc = temp - m_pole * l * theta_acc * np.cos(theta) / total_mass\n",
        "        \n",
        "        # Update state\n",
        "        x = x + x_dot * 0.02\n",
        "        x_dot = x_dot + x_acc * 0.02\n",
        "        theta = theta + theta_dot * 0.02\n",
        "        theta_dot = theta_dot + theta_acc * 0.02\n",
        "        \n",
        "        self.state = np.array([x, x_dot, theta, theta_dot], dtype=np.float32)\n",
        "        self.steps += 1\n",
        "        \n",
        "        # Compute reward\n",
        "        reward = self._compute_reward()\n",
        "        \n",
        "        # Check termination\n",
        "        terminated = self._is_terminated()\n",
        "        truncated = self.steps >= self.max_steps\n",
        "        \n",
        "        return self.state.copy(), reward, terminated, truncated, {}\n",
        "    \n",
        "    def _compute_reward(self):\n",
        "        \"\"\"Compute reward based on task parameters.\"\"\"\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        \n",
        "        # Base reward: stay upright and centered\n",
        "        reward = 1.0\n",
        "        \n",
        "        # Penalty for being far from center\n",
        "        reward -= abs(x) * 0.1\n",
        "        \n",
        "        # Penalty for large angle\n",
        "        reward -= abs(theta) * 0.1\n",
        "        \n",
        "        # Scale reward\n",
        "        reward *= self.params['reward_scale']\n",
        "        \n",
        "        return reward\n",
        "    \n",
        "    def _is_terminated(self):\n",
        "        \"\"\"Check if episode should terminate.\"\"\"\n",
        "        x, x_dot, theta, theta_dot = self.state\n",
        "        \n",
        "        # Terminate if cart goes too far\n",
        "        if abs(x) > 2.4:\n",
        "            return True\n",
        "        \n",
        "        # Terminate if pole falls too far\n",
        "        if abs(theta) > 0.2095:  # ~12 degrees\n",
        "            return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def set_task_params(self, task_params):\n",
        "        \"\"\"Set new task parameters.\"\"\"\n",
        "        self.params.update(task_params)\n",
        "\n",
        "\n",
        "class TaskDistribution:\n",
        "    \"\"\"\n",
        "    Distribution over parameterized tasks for meta-learning.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, task_type='pole_length'):\n",
        "        self.task_type = task_type\n",
        "        \n",
        "        if task_type == 'pole_length':\n",
        "            self.param_ranges = {\n",
        "                'pole_length': (0.3, 0.8),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "        elif task_type == 'mass':\n",
        "            self.param_ranges = {\n",
        "                'pole_mass': (0.05, 0.2),\n",
        "                'cart_mass': (0.8, 1.5),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "        elif task_type == 'gravity':\n",
        "            self.param_ranges = {\n",
        "                'gravity': (8.0, 12.0),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "        elif task_type == 'mixed':\n",
        "            self.param_ranges = {\n",
        "                'pole_length': (0.3, 0.8),\n",
        "                'pole_mass': (0.05, 0.2),\n",
        "                'gravity': (8.0, 12.0),\n",
        "                'reward_scale': (0.8, 1.2)\n",
        "            }\n",
        "    \n",
        "    def sample_task(self):\n",
        "        \"\"\"Sample a random task from the distribution.\"\"\"\n",
        "        task_params = {}\n",
        "        for param, (low, high) in self.param_ranges.items():\n",
        "            task_params[param] = np.random.uniform(low, high)\n",
        "        \n",
        "        return ParameterizedCartPoleEnv(task_params)\n",
        "    \n",
        "    def sample_tasks(self, n_tasks):\n",
        "        \"\"\"Sample multiple tasks.\"\"\"\n",
        "        return [self.sample_task() for _ in range(n_tasks)]\n",
        "\n",
        "\n",
        "# Test the environment\n",
        "print(\"Testing Parameterized CartPole Environment...\")\n",
        "\n",
        "# Test different task types\n",
        "task_types = ['pole_length', 'mass', 'gravity', 'mixed']\n",
        "for task_type in task_types:\n",
        "    print(f\"\\nTesting {task_type} tasks:\")\n",
        "    task_dist = TaskDistribution(task_type)\n",
        "    \n",
        "    # Sample a few tasks\n",
        "    for i in range(3):\n",
        "        task = task_dist.sample_task()\n",
        "        obs, _ = task.reset()\n",
        "        \n",
        "        # Run a few steps\n",
        "        total_reward = 0\n",
        "        for step in range(10):\n",
        "            action = task.action_space.sample()\n",
        "            obs, reward, terminated, truncated, _ = task.step(action)\n",
        "            total_reward += reward\n",
        "            \n",
        "            if terminated or truncated:\n",
        "                break\n",
        "        \n",
        "        print(f\"  Task {i+1}: Reward = {total_reward:.2f}, Params = {task.params}\")\n",
        "\n",
        "print(\"\\nEnvironment test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. MAML (Model-Agnostic Meta-Learning) Implementation\n",
        "\n",
        "MAML finds initialization parameters that are good for fine-tuning on new tasks. It uses a two-level optimization process:\n",
        "- **Inner Loop**: Adapt to specific task using gradient descent\n",
        "- **Outer Loop**: Optimize initialization for fast adaptation across tasks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    Policy network for MAML.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Policy network\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "        \n",
        "        # Value network\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass through policy and value networks.\"\"\"\n",
        "        policy_logits = self.policy(state)\n",
        "        value = self.value(state)\n",
        "        return policy_logits, value\n",
        "    \n",
        "    def get_action(self, state, deterministic=False):\n",
        "        \"\"\"Get action from policy.\"\"\"\n",
        "        policy_logits, value = self.forward(state)\n",
        "        \n",
        "        if deterministic:\n",
        "            action = torch.argmax(policy_logits, dim=-1)\n",
        "        else:\n",
        "            action_probs = F.softmax(policy_logits, dim=-1)\n",
        "            action = torch.multinomial(action_probs, 1).squeeze(-1)\n",
        "        \n",
        "        return action, value\n",
        "    \n",
        "    def log_prob(self, state, action):\n",
        "        \"\"\"Get log probability of action.\"\"\"\n",
        "        policy_logits, _ = self.forward(state)\n",
        "        log_probs = F.log_softmax(policy_logits, dim=-1)\n",
        "        return log_probs.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "\n",
        "class MAMLAgent:\n",
        "    \"\"\"\n",
        "    MAML agent for meta-learning in RL.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, meta_lr=0.001, inner_lr=0.01, \n",
        "                 inner_steps=1, hidden_dim=64):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.meta_lr = meta_lr\n",
        "        self.inner_lr = inner_lr\n",
        "        self.inner_steps = inner_steps\n",
        "        \n",
        "        # Meta-network (initialization)\n",
        "        self.meta_network = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "        self.meta_optimizer = optim.Adam(self.meta_network.parameters(), lr=meta_lr)\n",
        "        \n",
        "        # Copy of network for inner loop updates\n",
        "        self.inner_network = PolicyNetwork(state_dim, action_dim, hidden_dim)\n",
        "        \n",
        "    def collect_trajectories(self, env, network, num_episodes=5, max_steps=200):\n",
        "        \"\"\"Collect trajectories using given network.\"\"\"\n",
        "        trajectories = []\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            episode_data = {\n",
        "                'states': [],\n",
        "                'actions': [],\n",
        "                'rewards': [],\n",
        "                'values': [],\n",
        "                'log_probs': []\n",
        "            }\n",
        "            \n",
        "            for step in range(max_steps):\n",
        "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "                \n",
        "                # Get action and value\n",
        "                action, value = network.get_action(state_tensor)\n",
        "                log_prob = network.log_prob(state_tensor, action)\n",
        "                \n",
        "                # Execute action\n",
        "                next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
        "                done = terminated or truncated\n",
        "                \n",
        "                # Store data\n",
        "                episode_data['states'].append(obs)\n",
        "                episode_data['actions'].append(action.item())\n",
        "                episode_data['rewards'].append(reward)\n",
        "                episode_data['values'].append(value.item())\n",
        "                episode_data['log_probs'].append(log_prob.item())\n",
        "                \n",
        "                obs = next_obs\n",
        "                \n",
        "                if done:\n",
        "                    break\n",
        "            \n",
        "            trajectories.append(episode_data)\n",
        "        \n",
        "        return trajectories\n",
        "    \n",
        "    def compute_returns(self, rewards, gamma=0.99):\n",
        "        \"\"\"Compute discounted returns.\"\"\"\n",
        "        returns = []\n",
        "        G = 0\n",
        "        \n",
        "        for reward in reversed(rewards):\n",
        "            G = reward + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        \n",
        "        return returns\n",
        "    \n",
        "    def compute_advantages(self, returns, values):\n",
        "        \"\"\"Compute advantages using returns and values.\"\"\"\n",
        "        advantages = []\n",
        "        for ret, val in zip(returns, values):\n",
        "            advantages.append(ret - val)\n",
        "        return advantages\n",
        "    \n",
        "    def compute_loss(self, trajectories, network):\n",
        "        \"\"\"Compute policy gradient loss.\"\"\"\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "        \n",
        "        for trajectory in trajectories:\n",
        "            states = torch.FloatTensor(trajectory['states'])\n",
        "            actions = torch.LongTensor(trajectory['actions'])\n",
        "            rewards = trajectory['rewards']\n",
        "            values = torch.FloatTensor(trajectory['values'])\n",
        "            \n",
        "            # Compute returns\n",
        "            returns = self.compute_returns(rewards)\n",
        "            returns = torch.FloatTensor(returns)\n",
        "            \n",
        "            # Compute advantages\n",
        "            advantages = self.compute_advantages(returns, values)\n",
        "            advantages = torch.FloatTensor(advantages)\n",
        "            \n",
        "            # Normalize advantages\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "            \n",
        "            # Policy loss\n",
        "            log_probs = network.log_prob(states, actions)\n",
        "            policy_loss = -(log_probs * advantages).mean()\n",
        "            \n",
        "            # Value loss\n",
        "            predicted_values = network.value(states).squeeze()\n",
        "            value_loss = F.mse_loss(predicted_values, returns)\n",
        "            \n",
        "            # Total loss\n",
        "            loss = policy_loss + 0.5 * value_loss\n",
        "            total_loss += loss * len(states)\n",
        "            total_samples += len(states)\n",
        "        \n",
        "        return total_loss / total_samples if total_samples > 0 else torch.tensor(0.0)\n",
        "    \n",
        "    def inner_loop_update(self, task, network):\n",
        "        \"\"\"Perform inner loop update on a specific task.\"\"\"\n",
        "        # Collect trajectories\n",
        "        trajectories = self.collect_trajectories(task, network, num_episodes=3)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = self.compute_loss(trajectories, network)\n",
        "        \n",
        "        # Compute gradients\n",
        "        gradients = torch.autograd.grad(loss, network.parameters(), create_graph=True)\n",
        "        \n",
        "        # Update parameters\n",
        "        updated_params = []\n",
        "        for param, grad in zip(network.parameters(), gradients):\n",
        "            updated_params.append(param - self.inner_lr * grad)\n",
        "        \n",
        "        return updated_params, loss\n",
        "    \n",
        "    def meta_update(self, tasks, num_tasks=4):\n",
        "        \"\"\"Perform meta-update across multiple tasks.\"\"\"\n",
        "        meta_loss = 0\n",
        "        \n",
        "        for task in tasks[:num_tasks]:\n",
        "            # Copy meta-network for inner loop\n",
        "            self.inner_network.load_state_dict(self.meta_network.state_dict())\n",
        "            \n",
        "            # Inner loop adaptation\n",
        "            updated_params, inner_loss = self.inner_loop_update(task, self.inner_network)\n",
        "            \n",
        "            # Collect test trajectories with adapted network\n",
        "            # Create temporary network with updated parameters\n",
        "            temp_network = PolicyNetwork(self.state_dim, self.action_dim)\n",
        "            temp_network.load_state_dict(self.meta_network.state_dict())\n",
        "            \n",
        "            # Manually set updated parameters\n",
        "            for param, updated_param in zip(temp_network.parameters(), updated_params):\n",
        "                param.data = updated_param\n",
        "            \n",
        "            # Test trajectories\n",
        "            test_trajectories = self.collect_trajectories(task, temp_network, num_episodes=2)\n",
        "            test_loss = self.compute_loss(test_trajectories, temp_network)\n",
        "            \n",
        "            meta_loss += test_loss\n",
        "        \n",
        "        # Meta-gradient update\n",
        "        meta_loss = meta_loss / num_tasks\n",
        "        self.meta_optimizer.zero_grad()\n",
        "        meta_loss.backward()\n",
        "        self.meta_optimizer.step()\n",
        "        \n",
        "        return meta_loss.item()\n",
        "    \n",
        "    def adapt_to_task(self, task, num_adaptation_steps=5):\n",
        "        \"\"\"Adapt to a new task using MAML.\"\"\"\n",
        "        # Copy meta-network\n",
        "        adapted_network = PolicyNetwork(self.state_dim, self.action_dim)\n",
        "        adapted_network.load_state_dict(self.meta_network.state_dict())\n",
        "        \n",
        "        # Inner loop adaptation\n",
        "        for step in range(num_adaptation_steps):\n",
        "            # Collect trajectories\n",
        "            trajectories = self.collect_trajectories(task, adapted_network, num_episodes=2)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = self.compute_loss(trajectories, adapted_network)\n",
        "            \n",
        "            # Update parameters\n",
        "            optimizer = optim.Adam(adapted_network.parameters(), lr=self.inner_lr)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        \n",
        "        return adapted_network\n",
        "\n",
        "\n",
        "# Test MAML implementation\n",
        "print(\"Testing MAML Implementation...\")\n",
        "\n",
        "# Create task distribution\n",
        "task_dist = TaskDistribution('pole_length')\n",
        "state_dim = 4  # CartPole state dimension\n",
        "action_dim = 2  # CartPole action dimension\n",
        "\n",
        "# Create MAML agent\n",
        "maml_agent = MAMLAgent(state_dim, action_dim, meta_lr=0.001, inner_lr=0.01)\n",
        "\n",
        "# Test meta-update\n",
        "print(\"Testing meta-update...\")\n",
        "tasks = task_dist.sample_tasks(4)\n",
        "meta_loss = maml_agent.meta_update(tasks)\n",
        "print(f\"Meta-loss: {meta_loss:.4f}\")\n",
        "\n",
        "# Test adaptation\n",
        "print(\"Testing task adaptation...\")\n",
        "new_task = task_dist.sample_task()\n",
        "adapted_network = maml_agent.adapt_to_task(new_task, num_adaptation_steps=3)\n",
        "\n",
        "# Test adapted network\n",
        "obs, _ = new_task.reset()\n",
        "state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "action, value = adapted_network.get_action(state_tensor)\n",
        "print(f\"Adapted network action: {action.item()}, value: {value.item():.4f}\")\n",
        "\n",
        "print(\"MAML test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. RLÂ² (Recurrent Meta-RL) Implementation\n",
        "\n",
        "RLÂ² uses recurrent networks to encode task information implicitly. The LSTM hidden state learns to adapt to different tasks without explicit inner loop optimization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RL2Network(nn.Module):\n",
        "    \"\"\"\n",
        "    RLÂ² network with LSTM for task encoding.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=128, lstm_layers=2):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.lstm_layers = lstm_layers\n",
        "        \n",
        "        # Input: state + previous_action + previous_reward + done_flag\n",
        "        input_dim = state_dim + action_dim + 1 + 1\n",
        "        \n",
        "        # LSTM for task encoding\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=lstm_layers,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        # Policy head\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "        \n",
        "        # Value head\n",
        "        self.value = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state, prev_action, prev_reward, done, hidden):\n",
        "        \"\"\"\n",
        "        Forward pass through RLÂ² network.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            prev_action: Previous action (one-hot)\n",
        "            prev_reward: Previous reward\n",
        "            done: Episode termination flag\n",
        "            hidden: LSTM hidden state\n",
        "        \"\"\"\n",
        "        # Concatenate inputs\n",
        "        x = torch.cat([state, prev_action, prev_reward.unsqueeze(-1), done.unsqueeze(-1)], dim=-1)\n",
        "        \n",
        "        # LSTM forward pass\n",
        "        output, hidden_new = self.lstm(x.unsqueeze(1), hidden)\n",
        "        \n",
        "        # Get policy and value\n",
        "        policy_logits = self.policy(output.squeeze(1))\n",
        "        value = self.value(output.squeeze(1))\n",
        "        \n",
        "        return policy_logits, value, hidden_new\n",
        "    \n",
        "    def reset_hidden(self, batch_size=1):\n",
        "        \"\"\"Reset LSTM hidden state.\"\"\"\n",
        "        h0 = torch.zeros(self.lstm_layers, batch_size, self.hidden_dim)\n",
        "        c0 = torch.zeros(self.lstm_layers, batch_size, self.hidden_dim)\n",
        "        return (h0, c0)\n",
        "    \n",
        "    def get_action(self, state, prev_action, prev_reward, done, hidden, deterministic=False):\n",
        "        \"\"\"Get action from policy.\"\"\"\n",
        "        policy_logits, value, hidden_new = self.forward(state, prev_action, prev_reward, done, hidden)\n",
        "        \n",
        "        if deterministic:\n",
        "            action = torch.argmax(policy_logits, dim=-1)\n",
        "        else:\n",
        "            action_probs = F.softmax(policy_logits, dim=-1)\n",
        "            action = torch.multinomial(action_probs, 1).squeeze(-1)\n",
        "        \n",
        "        return action, value, hidden_new\n",
        "    \n",
        "    def log_prob(self, state, prev_action, prev_reward, done, action, hidden):\n",
        "        \"\"\"Get log probability of action.\"\"\"\n",
        "        policy_logits, _, _ = self.forward(state, prev_action, prev_reward, done, hidden)\n",
        "        log_probs = F.log_softmax(policy_logits, dim=-1)\n",
        "        return log_probs.gather(1, action.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "\n",
        "class RL2Agent:\n",
        "    \"\"\"\n",
        "    RLÂ² agent for recurrent meta-learning.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4, hidden_dim=128, lstm_layers=2):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # RLÂ² network\n",
        "        self.network = RL2Network(state_dim, action_dim, hidden_dim, lstm_layers)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
        "        \n",
        "    def collect_episode(self, env, hidden, max_steps=200):\n",
        "        \"\"\"Collect a single episode.\"\"\"\n",
        "        obs, _ = env.reset()\n",
        "        \n",
        "        episode_data = {\n",
        "            'states': [],\n",
        "            'actions': [],\n",
        "            'rewards': [],\n",
        "            'values': [],\n",
        "            'log_probs': [],\n",
        "            'prev_actions': [],\n",
        "            'prev_rewards': [],\n",
        "            'dones': []\n",
        "        }\n",
        "        \n",
        "        prev_action = torch.zeros(self.action_dim)\n",
        "        prev_reward = torch.tensor(0.0)\n",
        "        done = torch.tensor(0.0)\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "            \n",
        "            # Get action\n",
        "            action, value, hidden = self.network.get_action(\n",
        "                state_tensor, prev_action.unsqueeze(0), prev_reward.unsqueeze(0), \n",
        "                done.unsqueeze(0), hidden\n",
        "            )\n",
        "            \n",
        "            log_prob = self.network.log_prob(\n",
        "                state_tensor, prev_action.unsqueeze(0), prev_reward.unsqueeze(0),\n",
        "                done.unsqueeze(0), action, hidden\n",
        "            )\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
        "            episode_done = terminated or truncated\n",
        "            \n",
        "            # Store data\n",
        "            episode_data['states'].append(obs)\n",
        "            episode_data['actions'].append(action.item())\n",
        "            episode_data['rewards'].append(reward)\n",
        "            episode_data['values'].append(value.item())\n",
        "            episode_data['log_probs'].append(log_prob.item())\n",
        "            episode_data['prev_actions'].append(prev_action.clone())\n",
        "            episode_data['prev_rewards'].append(prev_reward.item())\n",
        "            episode_data['dones'].append(done.item())\n",
        "            \n",
        "            # Update for next step\n",
        "            prev_action = F.one_hot(action, num_classes=self.action_dim).float().squeeze(0)\n",
        "            prev_reward = torch.tensor(reward)\n",
        "            done = torch.tensor(1.0 if episode_done else 0.0)\n",
        "            obs = next_obs\n",
        "            \n",
        "            if episode_done:\n",
        "                break\n",
        "        \n",
        "        return episode_data, hidden\n",
        "    \n",
        "    def collect_trajectories(self, env, num_episodes=10, max_steps=200):\n",
        "        \"\"\"Collect multiple episodes from the same task.\"\"\"\n",
        "        trajectories = []\n",
        "        hidden = self.network.reset_hidden()\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            episode_data, hidden = self.collect_episode(env, hidden, max_steps)\n",
        "            trajectories.append(episode_data)\n",
        "            \n",
        "            # Reset hidden state between episodes (optional)\n",
        "            # hidden = self.network.reset_hidden()\n",
        "        \n",
        "        return trajectories\n",
        "    \n",
        "    def compute_returns(self, rewards, gamma=0.99):\n",
        "        \"\"\"Compute discounted returns.\"\"\"\n",
        "        returns = []\n",
        "        G = 0\n",
        "        \n",
        "        for reward in reversed(rewards):\n",
        "            G = reward + gamma * G\n",
        "            returns.insert(0, G)\n",
        "        \n",
        "        return returns\n",
        "    \n",
        "    def compute_advantages(self, returns, values):\n",
        "        \"\"\"Compute advantages.\"\"\"\n",
        "        advantages = []\n",
        "        for ret, val in zip(returns, values):\n",
        "            advantages.append(ret - val)\n",
        "        return advantages\n",
        "    \n",
        "    def compute_loss(self, trajectories):\n",
        "        \"\"\"Compute PPO loss for RLÂ².\"\"\"\n",
        "        total_loss = 0\n",
        "        total_samples = 0\n",
        "        \n",
        "        for trajectory in trajectories:\n",
        "            states = torch.FloatTensor(trajectory['states'])\n",
        "            actions = torch.LongTensor(trajectory['actions'])\n",
        "            rewards = trajectory['rewards']\n",
        "            values = torch.FloatTensor(trajectory['values'])\n",
        "            prev_actions = torch.stack(trajectory['prev_actions'])\n",
        "            prev_rewards = torch.FloatTensor(trajectory['prev_rewards'])\n",
        "            dones = torch.FloatTensor(trajectory['dones'])\n",
        "            \n",
        "            # Compute returns\n",
        "            returns = self.compute_returns(rewards)\n",
        "            returns = torch.FloatTensor(returns)\n",
        "            \n",
        "            # Compute advantages\n",
        "            advantages = self.compute_advantages(returns, values)\n",
        "            advantages = torch.FloatTensor(advantages)\n",
        "            \n",
        "            # Normalize advantages\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "            \n",
        "            # Forward pass through network\n",
        "            hidden = self.network.reset_hidden(len(states))\n",
        "            \n",
        "            # Compute policy loss\n",
        "            log_probs = []\n",
        "            predicted_values = []\n",
        "            \n",
        "            for i in range(len(states)):\n",
        "                state = states[i:i+1]\n",
        "                prev_action = prev_actions[i:i+1]\n",
        "                prev_reward = prev_rewards[i:i+1]\n",
        "                done = dones[i:i+1]\n",
        "                action = actions[i:i+1]\n",
        "                \n",
        "                log_prob = self.network.log_prob(state, prev_action, prev_reward, done, action, hidden)\n",
        "                _, value, hidden = self.network.forward(state, prev_action, prev_reward, done, hidden)\n",
        "                \n",
        "                log_probs.append(log_prob)\n",
        "                predicted_values.append(value)\n",
        "            \n",
        "            log_probs = torch.cat(log_probs)\n",
        "            predicted_values = torch.cat(predicted_values).squeeze()\n",
        "            \n",
        "            # Policy loss\n",
        "            policy_loss = -(log_probs * advantages).mean()\n",
        "            \n",
        "            # Value loss\n",
        "            value_loss = F.mse_loss(predicted_values, returns)\n",
        "            \n",
        "            # Total loss\n",
        "            loss = policy_loss + 0.5 * value_loss\n",
        "            total_loss += loss * len(states)\n",
        "            total_samples += len(states)\n",
        "        \n",
        "        return total_loss / total_samples if total_samples > 0 else torch.tensor(0.0)\n",
        "    \n",
        "    def update(self, trajectories):\n",
        "        \"\"\"Update RLÂ² network.\"\"\"\n",
        "        loss = self.compute_loss(trajectories)\n",
        "        \n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def evaluate_task(self, env, num_episodes=5, max_steps=200):\n",
        "        \"\"\"Evaluate performance on a task.\"\"\"\n",
        "        total_rewards = []\n",
        "        \n",
        "        for episode in range(num_episodes):\n",
        "            obs, _ = env.reset()\n",
        "            episode_reward = 0\n",
        "            \n",
        "            hidden = self.network.reset_hidden()\n",
        "            prev_action = torch.zeros(self.action_dim)\n",
        "            prev_reward = torch.tensor(0.0)\n",
        "            done = torch.tensor(0.0)\n",
        "            \n",
        "            for step in range(max_steps):\n",
        "                state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "                \n",
        "                # Get action\n",
        "                action, value, hidden = self.network.get_action(\n",
        "                    state_tensor, prev_action.unsqueeze(0), prev_reward.unsqueeze(0),\n",
        "                    done.unsqueeze(0), hidden, deterministic=True\n",
        "                )\n",
        "                \n",
        "                # Execute action\n",
        "                next_obs, reward, terminated, truncated, _ = env.step(action.item())\n",
        "                episode_done = terminated or truncated\n",
        "                \n",
        "                episode_reward += reward\n",
        "                \n",
        "                # Update for next step\n",
        "                prev_action = F.one_hot(action, num_classes=self.action_dim).float().squeeze(0)\n",
        "                prev_reward = torch.tensor(reward)\n",
        "                done = torch.tensor(1.0 if episode_done else 0.0)\n",
        "                obs = next_obs\n",
        "                \n",
        "                if episode_done:\n",
        "                    break\n",
        "            \n",
        "            total_rewards.append(episode_reward)\n",
        "        \n",
        "        return np.mean(total_rewards), np.std(total_rewards)\n",
        "\n",
        "\n",
        "# Test RLÂ² implementation\n",
        "print(\"Testing RLÂ² Implementation...\")\n",
        "\n",
        "# Create task distribution\n",
        "task_dist = TaskDistribution('pole_length')\n",
        "state_dim = 4\n",
        "action_dim = 2\n",
        "\n",
        "# Create RLÂ² agent\n",
        "rl2_agent = RL2Agent(state_dim, action_dim, lr=3e-4)\n",
        "\n",
        "# Test on a single task\n",
        "print(\"Testing RLÂ² on single task...\")\n",
        "task = task_dist.sample_task()\n",
        "\n",
        "# Collect trajectories\n",
        "trajectories = rl2_agent.collect_trajectories(task, num_episodes=3)\n",
        "print(f\"Collected {len(trajectories)} episodes\")\n",
        "\n",
        "# Update network\n",
        "loss = rl2_agent.update(trajectories)\n",
        "print(f\"Training loss: {loss:.4f}\")\n",
        "\n",
        "# Evaluate performance\n",
        "mean_reward, std_reward = rl2_agent.evaluate_task(task, num_episodes=3)\n",
        "print(f\"Evaluation reward: {mean_reward:.2f} Â± {std_reward:.2f}\")\n",
        "\n",
        "print(\"RLÂ² test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. PEARL (Probabilistic Embeddings) Implementation\n",
        "\n",
        "PEARL uses probabilistic context variables to encode task information, enabling fast adaptation through context inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PEARLNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    PEARL network with probabilistic context encoding.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, context_dim=5, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.context_dim = context_dim\n",
        "        \n",
        "        # Context encoder (variational)\n",
        "        self.context_encoder = nn.LSTM(\n",
        "            input_size=state_dim + action_dim + 1,  # state + action + reward\n",
        "            hidden_size=hidden_dim,\n",
        "            batch_first=True\n",
        "        )\n",
        "        \n",
        "        self.context_mean = nn.Linear(hidden_dim, context_dim)\n",
        "        self.context_logstd = nn.Linear(hidden_dim, context_dim)\n",
        "        \n",
        "        # Policy conditioned on context\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(state_dim + context_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "        \n",
        "        # Q-function conditioned on context\n",
        "        self.q_function = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim + context_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    \n",
        "    def encode_context(self, transitions):\n",
        "        \"\"\"Encode task from transitions.\"\"\"\n",
        "        states, actions, rewards = transitions\n",
        "        \n",
        "        # Prepare inputs\n",
        "        inputs = torch.cat([\n",
        "            states, \n",
        "            F.one_hot(actions, num_classes=self.action_dim).float(),\n",
        "            rewards.unsqueeze(-1)\n",
        "        ], dim=-1)\n",
        "        \n",
        "        # LSTM encoding\n",
        "        output, _ = self.context_encoder(inputs.unsqueeze(0))\n",
        "        pooled = output.mean(dim=1)  # Aggregate over transitions\n",
        "        \n",
        "        # Variational encoding\n",
        "        mean = self.context_mean(pooled)\n",
        "        logstd = self.context_logstd(pooled)\n",
        "        \n",
        "        return mean, logstd\n",
        "    \n",
        "    def sample_context(self, mean, logstd):\n",
        "        \"\"\"Sample context vector.\"\"\"\n",
        "        std = torch.exp(logstd)\n",
        "        return mean + std * torch.randn_like(std)\n",
        "    \n",
        "    def forward(self, state, context):\n",
        "        \"\"\"Forward pass conditioned on context.\"\"\"\n",
        "        x = torch.cat([state, context], dim=-1)\n",
        "        return self.policy(x)\n",
        "    \n",
        "    def get_q_value(self, state, action, context):\n",
        "        \"\"\"Get Q-value conditioned on context.\"\"\"\n",
        "        x = torch.cat([state, F.one_hot(action, num_classes=self.action_dim).float(), context], dim=-1)\n",
        "        return self.q_function(x)\n",
        "\n",
        "\n",
        "class PEARLAgent:\n",
        "    \"\"\"\n",
        "    PEARL agent for context-based meta-learning.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, context_dim=5, lr=3e-4, beta=1.0):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.context_dim = context_dim\n",
        "        self.beta = beta\n",
        "        \n",
        "        # PEARL network\n",
        "        self.network = PEARLNetwork(state_dim, action_dim, context_dim)\n",
        "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.buffer = []\n",
        "        \n",
        "    def collect_transitions(self, env, context, num_transitions=20):\n",
        "        \"\"\"Collect transitions for context encoding.\"\"\"\n",
        "        transitions = {\n",
        "            'states': [],\n",
        "            'actions': [],\n",
        "            'rewards': []\n",
        "        }\n",
        "        \n",
        "        obs, _ = env.reset()\n",
        "        \n",
        "        for _ in range(num_transitions):\n",
        "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "            \n",
        "            # Get action from policy\n",
        "            action_logits = self.network(state_tensor, context)\n",
        "            action_probs = F.softmax(action_logits, dim=-1)\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Store transition\n",
        "            transitions['states'].append(obs)\n",
        "            transitions['actions'].append(action)\n",
        "            transitions['rewards'].append(reward)\n",
        "            \n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                obs, _ = env.reset()\n",
        "        \n",
        "        return transitions\n",
        "    \n",
        "    def infer_context(self, transitions):\n",
        "        \"\"\"Infer task context from transitions.\"\"\"\n",
        "        states = torch.FloatTensor(transitions['states'])\n",
        "        actions = torch.LongTensor(transitions['actions'])\n",
        "        rewards = torch.FloatTensor(transitions['rewards'])\n",
        "        \n",
        "        mean, logstd = self.network.encode_context((states, actions, rewards))\n",
        "        context = self.network.sample_context(mean, logstd)\n",
        "        \n",
        "        return context, mean, logstd\n",
        "    \n",
        "    def compute_kl_loss(self, mean, logstd):\n",
        "        \"\"\"Compute KL divergence loss.\"\"\"\n",
        "        kl_loss = -0.5 * torch.sum(1 + logstd - mean.pow(2) - logstd.exp())\n",
        "        return kl_loss\n",
        "    \n",
        "    def update(self, batch_size=32):\n",
        "        \"\"\"Update PEARL network.\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        total_loss = 0\n",
        "        \n",
        "        for item in batch:\n",
        "            states = torch.FloatTensor(item['states'])\n",
        "            actions = torch.LongTensor(item['actions'])\n",
        "            rewards = torch.FloatTensor(item['rewards'])\n",
        "            context = item['context']\n",
        "            \n",
        "            # Policy loss (simplified)\n",
        "            action_logits = self.network(states, context)\n",
        "            log_probs = F.log_softmax(action_logits, dim=-1)\n",
        "            policy_loss = -log_probs.gather(1, actions.unsqueeze(1)).mean()\n",
        "            \n",
        "            # Q-learning loss (simplified)\n",
        "            q_values = self.network.get_q_value(states, actions, context)\n",
        "            q_targets = rewards.unsqueeze(-1)  # Simplified target\n",
        "            q_loss = F.mse_loss(q_values, q_targets)\n",
        "            \n",
        "            # KL loss\n",
        "            mean, logstd = item['mean'], item['logstd']\n",
        "            kl_loss = self.compute_kl_loss(mean, logstd)\n",
        "            \n",
        "            # Total loss\n",
        "            loss = policy_loss + q_loss + self.beta * kl_loss\n",
        "            total_loss += loss\n",
        "        \n",
        "        # Update\n",
        "        total_loss = total_loss / batch_size\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return total_loss.item()\n",
        "    \n",
        "    def adapt_to_task(self, env, num_context_transitions=10):\n",
        "        \"\"\"Adapt to new task using context inference.\"\"\"\n",
        "        # Collect context transitions\n",
        "        context_transitions = self.collect_transitions(env, None, num_context_transitions)\n",
        "        \n",
        "        # Infer context\n",
        "        context, mean, logstd = self.infer_context(context_transitions)\n",
        "        \n",
        "        # Store in buffer\n",
        "        self.buffer.append({\n",
        "            'states': context_transitions['states'],\n",
        "            'actions': context_transitions['actions'],\n",
        "            'rewards': context_transitions['rewards'],\n",
        "            'context': context,\n",
        "            'mean': mean,\n",
        "            'logstd': logstd\n",
        "        })\n",
        "        \n",
        "        return context\n",
        "\n",
        "\n",
        "# Test PEARL implementation\n",
        "print(\"Testing PEARL Implementation...\")\n",
        "\n",
        "# Create task distribution\n",
        "task_dist = TaskDistribution('pole_length')\n",
        "state_dim = 4\n",
        "action_dim = 2\n",
        "\n",
        "# Create PEARL agent\n",
        "pearl_agent = PEARLAgent(state_dim, action_dim, context_dim=5)\n",
        "\n",
        "# Test context inference\n",
        "print(\"Testing context inference...\")\n",
        "task = task_dist.sample_task()\n",
        "context = pearl_agent.adapt_to_task(task, num_context_transitions=5)\n",
        "print(f\"Inferred context shape: {context.shape}\")\n",
        "\n",
        "# Test action selection\n",
        "obs, _ = task.reset()\n",
        "state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "action_logits = pearl_agent.network(state_tensor, context)\n",
        "action = torch.argmax(action_logits, dim=-1)\n",
        "print(f\"Action with context: {action.item()}\")\n",
        "\n",
        "print(\"PEARL test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training and Evaluation Functions\n",
        "\n",
        "Let's implement training and evaluation functions for all meta-learning methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_maml(task_dist, agent, num_meta_iterations=100, tasks_per_iteration=4):\n",
        "    \"\"\"Train MAML agent.\"\"\"\n",
        "    meta_losses = []\n",
        "    \n",
        "    for iteration in trange(num_meta_iterations, desc=\"Training MAML\"):\n",
        "        # Sample tasks\n",
        "        tasks = task_dist.sample_tasks(tasks_per_iteration)\n",
        "        \n",
        "        # Meta-update\n",
        "        meta_loss = agent.meta_update(tasks)\n",
        "        meta_losses.append(meta_loss)\n",
        "    \n",
        "    return meta_losses\n",
        "\n",
        "\n",
        "def train_rl2(task_dist, agent, num_meta_iterations=100, episodes_per_task=5):\n",
        "    \"\"\"Train RLÂ² agent.\"\"\"\n",
        "    losses = []\n",
        "    \n",
        "    for iteration in trange(num_meta_iterations, desc=\"Training RLÂ²\"):\n",
        "        # Sample task\n",
        "        task = task_dist.sample_task()\n",
        "        \n",
        "        # Collect trajectories\n",
        "        trajectories = agent.collect_trajectories(task, num_episodes=episodes_per_task)\n",
        "        \n",
        "        # Update network\n",
        "        loss = agent.update(trajectories)\n",
        "        losses.append(loss)\n",
        "    \n",
        "    return losses\n",
        "\n",
        "\n",
        "def train_pearl(task_dist, agent, num_meta_iterations=100, context_transitions=10):\n",
        "    \"\"\"Train PEARL agent.\"\"\"\n",
        "    losses = []\n",
        "    \n",
        "    for iteration in trange(num_meta_iterations, desc=\"Training PEARL\"):\n",
        "        # Sample task\n",
        "        task = task_dist.sample_task()\n",
        "        \n",
        "        # Adapt to task\n",
        "        context = agent.adapt_to_task(task, num_context_transitions)\n",
        "        \n",
        "        # Update network\n",
        "        if len(agent.buffer) >= 32:\n",
        "            loss = agent.update()\n",
        "            losses.append(loss)\n",
        "    \n",
        "    return losses\n",
        "\n",
        "\n",
        "def evaluate_few_shot_adaptation(task_dist, agents, num_test_tasks=10, \n",
        "                                 adaptation_steps=5, eval_episodes=5):\n",
        "    \"\"\"Evaluate few-shot adaptation performance.\"\"\"\n",
        "    results = {}\n",
        "    \n",
        "    for method_name, agent in agents.items():\n",
        "        print(f\"Evaluating {method_name}...\")\n",
        "        \n",
        "        task_rewards = []\n",
        "        \n",
        "        for task_idx in range(num_test_tasks):\n",
        "            task = task_dist.sample_task()\n",
        "            \n",
        "            if method_name == 'MAML':\n",
        "                # MAML adaptation\n",
        "                adapted_network = agent.adapt_to_task(task, num_adaptation_steps)\n",
        "                \n",
        "                # Evaluate adapted network\n",
        "                episode_rewards = []\n",
        "                for episode in range(eval_episodes):\n",
        "                    obs, _ = task.reset()\n",
        "                    episode_reward = 0\n",
        "                    \n",
        "                    for step in range(200):\n",
        "                        state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "                        action, _ = adapted_network.get_action(state_tensor, deterministic=True)\n",
        "                        \n",
        "                        next_obs, reward, terminated, truncated, _ = task.step(action.item())\n",
        "                        episode_reward += reward\n",
        "                        \n",
        "                        obs = next_obs\n",
        "                        if terminated or truncated:\n",
        "                            break\n",
        "                    \n",
        "                    episode_rewards.append(episode_reward)\n",
        "                \n",
        "                task_rewards.append(np.mean(episode_rewards))\n",
        "            \n",
        "            elif method_name == 'RLÂ²':\n",
        "                # RLÂ² evaluation (no explicit adaptation)\n",
        "                mean_reward, _ = agent.evaluate_task(task, num_episodes=eval_episodes)\n",
        "                task_rewards.append(mean_reward)\n",
        "            \n",
        "            elif method_name == 'PEARL':\n",
        "                # PEARL adaptation\n",
        "                context = agent.adapt_to_task(task, num_context_transitions=adaptation_steps)\n",
        "                \n",
        "                # Evaluate with inferred context\n",
        "                episode_rewards = []\n",
        "                for episode in range(eval_episodes):\n",
        "                    obs, _ = task.reset()\n",
        "                    episode_reward = 0\n",
        "                    \n",
        "                    for step in range(200):\n",
        "                        state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "                        action_logits = agent.network(state_tensor, context)\n",
        "                        action = torch.argmax(action_logits, dim=-1)\n",
        "                        \n",
        "                        next_obs, reward, terminated, truncated, _ = task.step(action.item())\n",
        "                        episode_reward += reward\n",
        "                        \n",
        "                        obs = next_obs\n",
        "                        if terminated or truncated:\n",
        "                            break\n",
        "                    \n",
        "                    episode_rewards.append(episode_reward)\n",
        "                \n",
        "                task_rewards.append(np.mean(episode_rewards))\n",
        "        \n",
        "        results[method_name] = {\n",
        "            'mean_reward': np.mean(task_rewards),\n",
        "            'std_reward': np.std(task_rewards),\n",
        "            'rewards': task_rewards\n",
        "        }\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def plot_meta_learning_results(results, title=\"Meta-Learning Results\"):\n",
        "    \"\"\"Plot meta-learning results.\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    \n",
        "    # Plot mean rewards\n",
        "    ax1 = axes[0]\n",
        "    methods = list(results.keys())\n",
        "    mean_rewards = [results[method]['mean_reward'] for method in methods]\n",
        "    std_rewards = [results[method]['std_reward'] for method in methods]\n",
        "    \n",
        "    bars = ax1.bar(methods, mean_rewards, yerr=std_rewards, capsize=5)\n",
        "    ax1.set_title('Few-Shot Adaptation Performance')\n",
        "    ax1.set_ylabel('Mean Reward')\n",
        "    ax1.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, mean, std in zip(bars, mean_rewards, std_rewards):\n",
        "        height = bar.get_height()\n",
        "        ax1.text(bar.get_x() + bar.get_width()/2., height + std + 1,\n",
        "                f'{mean:.2f}Â±{std:.2f}', ha='center', va='bottom')\n",
        "    \n",
        "    # Plot reward distributions\n",
        "    ax2 = axes[1]\n",
        "    for method in methods:\n",
        "        rewards = results[method]['rewards']\n",
        "        ax2.hist(rewards, alpha=0.7, label=method, bins=10)\n",
        "    \n",
        "    ax2.set_title('Reward Distributions')\n",
        "    ax2.set_xlabel('Reward')\n",
        "    ax2.set_ylabel('Frequency')\n",
        "    ax2.legend()\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Test training functions\n",
        "print(\"Testing Training Functions...\")\n",
        "\n",
        "# Create task distribution\n",
        "task_dist = TaskDistribution('pole_length')\n",
        "state_dim = 4\n",
        "action_dim = 2\n",
        "\n",
        "# Create agents\n",
        "agents = {\n",
        "    'MAML': MAMLAgent(state_dim, action_dim),\n",
        "    'RLÂ²': RL2Agent(state_dim, action_dim),\n",
        "    'PEARL': PEARLAgent(state_dim, action_dim)\n",
        "}\n",
        "\n",
        "# Test few-shot adaptation\n",
        "print(\"Testing few-shot adaptation...\")\n",
        "results = evaluate_few_shot_adaptation(task_dist, agents, num_test_tasks=5, \n",
        "                                     adaptation_steps=3, eval_episodes=3)\n",
        "\n",
        "print(\"\\\\nFew-shot adaptation results:\")\n",
        "for method, result in results.items():\n",
        "    print(f\"{method}: {result['mean_reward']:.2f} Â± {result['std_reward']:.2f}\")\n",
        "\n",
        "print(\"Training functions test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Experiments and Analysis\n",
        "\n",
        "Let's run comprehensive experiments comparing different meta-learning methods on various task distributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Run Meta-Learning Experiments\n",
        "\n",
        "# Set experiment parameters\n",
        "NUM_META_ITERATIONS = 50  # Reduced for faster execution\n",
        "NUM_TEST_TASKS = 10\n",
        "ADAPTATION_STEPS = 3\n",
        "EVAL_EPISODES = 5\n",
        "\n",
        "print(\"Starting Meta-Learning Experiments...\")\n",
        "print(f\"Meta-iterations: {NUM_META_ITERATIONS}\")\n",
        "print(f\"Test tasks: {NUM_TEST_TASKS}\")\n",
        "print(f\"Adaptation steps: {ADAPTATION_STEPS}\")\n",
        "print(f\"Evaluation episodes: {EVAL_EPISODES}\")\n",
        "print()\n",
        "\n",
        "# Test different task distributions\n",
        "task_types = ['pole_length', 'mass', 'gravity', 'mixed']\n",
        "all_results = {}\n",
        "\n",
        "for task_type in task_types:\n",
        "    print(f\"\\\\n{'='*60}\")\n",
        "    print(f\"EXPERIMENT: {task_type.upper()} TASKS\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Create task distribution\n",
        "    task_dist = TaskDistribution(task_type)\n",
        "    state_dim = 4\n",
        "    action_dim = 2\n",
        "    \n",
        "    # Create agents\n",
        "    agents = {\n",
        "        'MAML': MAMLAgent(state_dim, action_dim, meta_lr=0.001, inner_lr=0.01),\n",
        "        'RLÂ²': RL2Agent(state_dim, action_dim, lr=3e-4),\n",
        "        'PEARL': PEARLAgent(state_dim, action_dim, context_dim=5, lr=3e-4)\n",
        "    }\n",
        "    \n",
        "    # Train agents\n",
        "    print(\"Training agents...\")\n",
        "    \n",
        "    # Train MAML\n",
        "    print(\"Training MAML...\")\n",
        "    maml_losses = train_maml(task_dist, agents['MAML'], \n",
        "                           num_meta_iterations=NUM_META_ITERATIONS//2, \n",
        "                           tasks_per_iteration=4)\n",
        "    \n",
        "    # Train RLÂ²\n",
        "    print(\"Training RLÂ²...\")\n",
        "    rl2_losses = train_rl2(task_dist, agents['RLÂ²'], \n",
        "                          num_meta_iterations=NUM_META_ITERATIONS, \n",
        "                          episodes_per_task=3)\n",
        "    \n",
        "    # Train PEARL\n",
        "    print(\"Training PEARL...\")\n",
        "    pearl_losses = train_pearl(task_dist, agents['PEARL'], \n",
        "                              num_meta_iterations=NUM_META_ITERATIONS, \n",
        "                              context_transitions=5)\n",
        "    \n",
        "    # Evaluate few-shot adaptation\n",
        "    print(\"Evaluating few-shot adaptation...\")\n",
        "    results = evaluate_few_shot_adaptation(task_dist, agents, \n",
        "                                         num_test_tasks=NUM_TEST_TASKS,\n",
        "                                         adaptation_steps=ADAPTATION_STEPS,\n",
        "                                         eval_episodes=EVAL_EPISODES)\n",
        "    \n",
        "    all_results[task_type] = results\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\\\nResults for {task_type} tasks:\")\n",
        "    for method, result in results.items():\n",
        "        print(f\"  {method}: {result['mean_reward']:.2f} Â± {result['std_reward']:.2f}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Print summary table\n",
        "print(\"\\\\nTask Type | MAML | RLÂ² | PEARL\")\n",
        "print(\"-\" * 40)\n",
        "for task_type, results in all_results.items():\n",
        "    maml_reward = results['MAML']['mean_reward']\n",
        "    rl2_reward = results['RLÂ²']['mean_reward']\n",
        "    pearl_reward = results['PEARL']['mean_reward']\n",
        "    print(f\"{task_type:10} | {maml_reward:4.1f} | {rl2_reward:4.1f} | {pearl_reward:4.1f}\")\n",
        "\n",
        "print(\"\\\\nAll experiments completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Plot Results and Analysis\n",
        "\n",
        "# Plot results for each task type\n",
        "for task_type, results in all_results.items():\n",
        "    print(f\"\\\\nPlotting results for {task_type} tasks...\")\n",
        "    plot_meta_learning_results(results, f\"Meta-Learning Results - {task_type.title()}\")\n",
        "\n",
        "# Overall analysis\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS AND INSIGHTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Find best performing method for each task type\n",
        "print(\"\\\\nBest performing method by task type:\")\n",
        "for task_type, results in all_results.items():\n",
        "    best_method = max(results.keys(), key=lambda k: results[k]['mean_reward'])\n",
        "    best_reward = results[best_method]['mean_reward']\n",
        "    print(f\"  {task_type}: {best_method} ({best_reward:.2f})\")\n",
        "\n",
        "# Overall best method\n",
        "overall_best = None\n",
        "overall_best_score = -np.inf\n",
        "for task_type, results in all_results.items():\n",
        "    for method, result in results.items():\n",
        "        if result['mean_reward'] > overall_best_score:\n",
        "            overall_best_score = result['mean_reward']\n",
        "            overall_best = method\n",
        "\n",
        "print(f\"\\\\nOverall best method: {overall_best} ({overall_best_score:.2f})\")\n",
        "\n",
        "# Method comparison\n",
        "print(\"\\\\nMethod Characteristics:\")\n",
        "print(\"- MAML: Gradient-based meta-learning with explicit inner/outer loops\")\n",
        "print(\"  - Pros: Theoretically grounded, good for similar tasks\")\n",
        "print(\"  - Cons: Computationally expensive, requires second-order derivatives\")\n",
        "print()\n",
        "print(\"- RLÂ²: Recurrent meta-learning with implicit adaptation\")\n",
        "print(\"  - Pros: Fast adaptation, no explicit inner loop needed\")\n",
        "print(\"  - Cons: Black-box adaptation, requires many episodes per task\")\n",
        "print()\n",
        "print(\"- PEARL: Context-based meta-learning with probabilistic embeddings\")\n",
        "print(\"  - Pros: Fast adaptation, interpretable context\")\n",
        "print(\"  - Cons: Requires context inference, sensitive to context quality\")\n",
        "\n",
        "print(\"\\\\nKey Insights:\")\n",
        "print(\"1. Meta-learning enables rapid adaptation to new tasks\")\n",
        "print(\"2. Different methods excel in different scenarios:\")\n",
        "print(\"   - MAML: Good for gradient-based adaptation\")\n",
        "print(\"   - RLÂ²: Good for sequential task learning\")\n",
        "print(\"   - PEARL: Good for context-based adaptation\")\n",
        "print(\"3. Task distribution affects performance significantly\")\n",
        "print(\"4. Few-shot adaptation is achievable with proper meta-training\")\n",
        "\n",
        "print(\"\\\\nMeta-learning provides a powerful framework for 'learning to learn'!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Analysis and Discussion Questions\n",
        "\n",
        "### Key Concepts Demonstrated\n",
        "\n",
        "1. **Meta-Learning Fundamentals**: Learning to learn across task distributions\n",
        "2. **MAML**: Gradient-based meta-learning with two-level optimization\n",
        "3. **RLÂ²**: Recurrent meta-learning with implicit task adaptation\n",
        "4. **PEARL**: Context-based meta-learning with probabilistic embeddings\n",
        "5. **Few-Shot Adaptation**: Rapid learning on new tasks with minimal samples\n",
        "\n",
        "### Discussion Questions\n",
        "\n",
        "**Answer the following questions based on your experiments:**\n",
        "\n",
        "1. **Meta-Learning vs Transfer Learning**: How does MAML differ from standard transfer learning? What are the advantages of meta-learning?\n",
        "\n",
        "2. **Adaptation Mechanisms**: Compare the adaptation mechanisms of MAML, RLÂ², and PEARL. Which approach is most interpretable?\n",
        "\n",
        "3. **Task Distribution Effects**: How does the task distribution affect meta-learning performance? Which method is most robust to distribution shifts?\n",
        "\n",
        "4. **Sample Efficiency**: Which method requires the most samples during meta-training? Which is most sample-efficient during adaptation?\n",
        "\n",
        "5. **Computational Complexity**: Compare the computational requirements of each method. When would you choose each approach?\n",
        "\n",
        "### Extensions and Future Work\n",
        "\n",
        "- **First-Order MAML**: Implement FOMAML to reduce computational cost\n",
        "- **Meta-World Benchmark**: Apply these methods to the Meta-World benchmark\n",
        "- **Sim-to-Real Transfer**: Use meta-learning for sim-to-real transfer\n",
        "- **Multi-Task Learning**: Compare meta-learning with multi-task learning\n",
        "- **Automatic Hyperparameter Selection**: Learn meta-learning hyperparameters\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Meta-learning in reinforcement learning provides a powerful framework for achieving rapid adaptation to new tasks. The methods explored in this assignment demonstrate different approaches to the \"learning to learn\" paradigm:\n",
        "\n",
        "- **MAML** provides theoretically grounded gradient-based adaptation\n",
        "- **RLÂ²** offers fast black-box adaptation through recurrent networks  \n",
        "- **PEARL** enables interpretable context-based adaptation\n",
        "\n",
        "The key insight is that **meta-learning enables agents to leverage experience from related tasks to quickly adapt to new situations**, making it particularly valuable for applications requiring rapid deployment in new environments or with new objectives.\n",
        "\n",
        "The choice of meta-learning method depends on the specific requirements: computational resources, interpretability needs, task similarity, and adaptation speed requirements.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
