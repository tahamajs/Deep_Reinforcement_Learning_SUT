# HW4: Actor-Critic Methods

[![Deep RL](https://img.shields.io/badge/Deep-RL-blue.svg)](https://en.wikipedia.org/wiki/Reinforcement_learning)
[![Actor-Critic](https://img.shields.io/badge/Methods-Actor--Critic-red.svg)](https://www.deepmind.com/)
[![Status](https://img.shields.io/badge/Status-Complete-success.svg)](.)

## üìã Overview

This assignment explores **Actor-Critic methods**, which combine the best of both policy-based and value-based approaches. These methods use two neural networks: an **actor** that learns the policy and a **critic** that learns the value function to guide the actor's learning. We'll implement state-of-the-art algorithms including PPO, SAC, and DDPG.

## üéØ Learning Objectives

By completing this assignment, you will:

1. **Understand Actor-Critic Architecture**: Learn how actor and critic networks collaborate
2. **Master PPO**: Implement Proximal Policy Optimization with clipped objectives
3. **Continuous Control with DDPG**: Learn deterministic policy gradients
4. **Soft Actor-Critic (SAC)**: Understand maximum entropy RL and automatic temperature tuning
5. **On-Policy vs Off-Policy**: Compare PPO (on-policy) with SAC/DDPG (off-policy)
6. **Advanced Techniques**: Trust regions, target networks, and entropy regularization

## üìÇ Directory Structure

```
HW4_Actor_Critic/
‚îú‚îÄ‚îÄ code/
‚îÇ   ‚îú‚îÄ‚îÄ HW4_P1_PPO_Continuous.ipynb           # Proximal Policy Optimization
‚îÇ   ‚îî‚îÄ‚îÄ HW4_P2_SAC_DDPG_Continuous.ipynb      # SAC and DDPG comparison
‚îú‚îÄ‚îÄ answers/
‚îÇ   ‚îú‚îÄ‚îÄ HW4_P1_PPO_Continuous_Solution.ipynb
‚îÇ   ‚îú‚îÄ‚îÄ HW4_P2_SAC_DDPG_Continuous_Solution.ipynb
‚îÇ   ‚îî‚îÄ‚îÄ HW4_Solution.pdf
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îî‚îÄ‚îÄ HW4_Questions.pdf
‚îî‚îÄ‚îÄ README.md
```

## üìö Theoretical Background

### 1. Actor-Critic Framework

**Core Idea:** Combine policy gradient (actor) with value function approximation (critic).

```
Actor:  œÄŒ∏(a|s) - Selects actions
Critic: VœÜ(s) or QœÜ(s,a) - Evaluates actions
```

**Advantages:**
- Lower variance than pure policy gradients (critic reduces variance)
- More sample efficient than REINFORCE
- Can be on-policy (A2C, PPO) or off-policy (DDPG, SAC)
- Natural baseline through value function

**Update Rules:**
```
# Critic update (TD learning)
Œ¥t = rt + Œ≥V(st+1) - V(st)  # TD error
œÜ ‚Üê œÜ + Œ±c Œ¥t ‚àáœÜ V(st)

# Actor update (policy gradient with advantage)
Œ∏ ‚Üê Œ∏ + Œ±a Œ¥t ‚àáŒ∏ log œÄ(at|st)
```

### 2. Proximal Policy Optimization (PPO)

**Motivation:** Policy gradients can take large, destructive updates. PPO constrains updates to stay close to the current policy.

**Two Variants:**

#### a) PPO-Penalty
```
L(Œ∏) = ùîº[L^CPI(Œ∏) - Œ≤ KL[œÄŒ∏_old, œÄŒ∏]]
```

#### b) PPO-Clip (Most Common)
```
L^CLIP(Œ∏) = ùîº[min(rt(Œ∏)√Çt, clip(rt(Œ∏), 1-Œµ, 1+Œµ)√Çt)]

where rt(Œ∏) = œÄŒ∏(at|st) / œÄŒ∏_old(at|st)  # Importance sampling ratio
```

**Key Features:**
- **Clipping**: Limits how much policy can change
- **Multiple Epochs**: Reuse same batch for multiple updates
- **Generalized Advantage Estimation (GAE)**: Balances bias-variance
  ```
  √Çt = ‚àëk=0^‚àû (Œ≥Œª)^k Œ¥t+k
  where Œ¥t = rt + Œ≥V(st+1) - V(st)
  ```

**Pseudocode:**
```python
for iteration in range(N):
    # Collect trajectories using œÄŒ∏_old
    trajectories = collect_trajectories(œÄŒ∏_old)
    
    # Compute advantages using GAE
    advantages = compute_gae(trajectories)
    
    # Multiple epochs on same data
    for epoch in range(K):
        for batch in minibatches(trajectories):
            # Compute ratio
            ratio = œÄ(a|s) / œÄ_old(a|s)
            
            # PPO-Clip objective
            L_clip = min(ratio * advantage, 
                        clip(ratio, 1-Œµ, 1+Œµ) * advantage)
            
            # Optimize actor and critic
            loss = -L_clip + c1*L_value - c2*entropy
            optimize(loss)
    
    # Update old policy
    Œ∏_old ‚Üê Œ∏
```

**Why PPO Works:**
- Prevents catastrophic policy updates
- Allows multiple gradient steps per data collection
- More sample efficient than vanilla policy gradient
- Simpler than TRPO, similar performance

### 3. Deep Deterministic Policy Gradient (DDPG)

**Key Idea:** Extend DQN to continuous action spaces using deterministic policies.

**Architecture:**
```
Actor:  ŒºŒ∏(s) ‚Üí a  (deterministic policy)
Critic: QœÜ(s,a) ‚Üí ‚Ñù (action-value function)
```

**Deterministic Policy Gradient Theorem:**
```
‚àáŒ∏J ‚âà ùîº[‚àáa Q(s,a)|a=ŒºŒ∏(s) ‚àáŒ∏ ŒºŒ∏(s)]
```

**Key Components:**

#### a) Experience Replay
Store transitions (s, a, r, s') and sample randomly for training.

#### b) Target Networks
```
Œ∏' ‚Üê œÑŒ∏ + (1-œÑ)Œ∏'  # Soft update
œÜ' ‚Üê œÑœÜ + (1-œÑ)œÜ'
```

#### c) Exploration Noise
```
a = ŒºŒ∏(s) + Œµ, where Œµ ~ N(0, œÉ)  # Gaussian noise
```

**DDPG Algorithm:**
```python
# Initialize networks
actor = ŒºŒ∏(s)
critic = QœÜ(s,a)
target_actor = ŒºŒ∏'(s)
target_critic = QœÜ'(s,a)

for episode in range(N):
    for t in range(T):
        # Select action with exploration
        a = ŒºŒ∏(s) + Œµ
        
        # Execute and store
        s', r = env.step(a)
        buffer.store(s, a, r, s')
        
        # Sample batch
        batch = buffer.sample()
        
        # Critic update
        y = r + Œ≥ QœÜ'(s', ŒºŒ∏'(s'))
        L_critic = MSE(QœÜ(s,a), y)
        
        # Actor update
        L_actor = -ùîº[QœÜ(s, ŒºŒ∏(s))]
        
        # Soft target update
        Œ∏' ‚Üê œÑŒ∏ + (1-œÑ)Œ∏'
        œÜ' ‚Üê œÑœÜ + (1-œÑ)œÜ'
```

**Challenges:**
- Sensitive to hyperparameters
- Can overestimate Q-values
- Exploration in continuous spaces difficult

### 4. Soft Actor-Critic (SAC)

**Motivation:** Maximum entropy RL - maximize expected return AND policy entropy.

**Objective:**
```
J(Œ∏) = ‚àët ùîº[rt + Œ± H(œÄŒ∏(¬∑|st))]

where H(œÄ) = -ùîº[log œÄ(a|s)]  # Entropy
      Œ± = temperature parameter
```

**Why Maximum Entropy?**
- Encourages exploration automatically
- More robust to reward scale
- Prevents premature convergence
- Learns multi-modal policies

**Key Features:**

#### a) Stochastic Policy
```python
class SAC_Actor(nn.Module):
    def forward(self, state):
        mean = self.mean_layer(x)
        log_std = self.log_std_layer(x)
        std = log_std.exp()
        
        # Reparameterization trick
        normal = Normal(mean, std)
        x_t = normal.rsample()  # Sample with gradient
        action = torch.tanh(x_t)  # Squash to [-1, 1]
        
        # Log probability with change of variables
        log_prob = normal.log_prob(x_t)
        log_prob -= torch.log(1 - action.pow(2) + 1e-6)
        
        return action, log_prob
```

#### b) Twin Q-Networks
Minimize overestimation using two Q-functions:
```
Q_target = r + Œ≥(min(Q1', Q2') - Œ± log œÄ)
```

#### c) Automatic Temperature Tuning
Learn Œ± automatically to match target entropy:
```
L_Œ± = -ùîº[Œ±(log œÄ(a|s) + H_target)]
```

**SAC Algorithm:**
```python
# Initialize
actor = œÄŒ∏(a|s)
Q1, Q2 = QœÜ1(s,a), QœÜ2(s,a)
Q1_target, Q2_target = QœÜ1'(s,a), QœÜ2'(s,a)
log_Œ± = learnable parameter

for step in range(N):
    # Sample action
    a, log_prob = actor.sample(s)
    
    # Execute
    s', r = env.step(a)
    buffer.store(s, a, r, s', log_prob)
    
    # Sample batch
    batch = buffer.sample()
    
    # Critic update
    with torch.no_grad():
        a_next, log_prob_next = actor.sample(s')
        Q_next = min(Q1_target(s', a_next), 
                     Q2_target(s', a_next))
        y = r + Œ≥(Q_next - Œ± * log_prob_next)
    
    L_Q = MSE(Q1(s,a), y) + MSE(Q2(s,a), y)
    
    # Actor update
    a_new, log_prob_new = actor.sample(s)
    Q_new = min(Q1(s, a_new), Q2(s, a_new))
    L_actor = ùîº[Œ± * log_prob_new - Q_new]
    
    # Temperature update
    L_Œ± = -ùîº[log_Œ± * (log_prob + H_target)]
    
    # Soft target update
    update_targets(œÑ)
```

**Advantages of SAC:**
- State-of-the-art continuous control
- Robust to hyperparameters
- Automatic exploration via entropy
- Sample efficient (off-policy)

### 5. Comparison Table

| Algorithm | Policy Type | On/Off-Policy | Key Feature | Best For |
|-----------|-------------|---------------|-------------|----------|
| **PPO** | Stochastic | On-Policy | Trust region | Robotics, General |
| **DDPG** | Deterministic | Off-Policy | DPG + DQN | Continuous control |
| **SAC** | Stochastic | Off-Policy | Max entropy | Continuous, Exploration |

**When to Use:**
- **PPO**: Default choice, stable, good performance
- **DDPG**: Need deterministic policy, simpler than SAC
- **SAC**: Need best sample efficiency, complex exploration

## üíª Implementation Details

### Part 1: PPO for Continuous Control

**Environment:** Continuous control tasks (Pendulum, HalfCheetah, Ant)

**Network Architecture:**
```python
class PPO_Actor(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.mean = nn.Linear(256, action_dim)
        self.log_std = nn.Parameter(torch.zeros(action_dim))
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        mean = self.mean(x)
        std = torch.exp(self.log_std)
        return Normal(mean, std)

class PPO_Critic(nn.Module):
    def __init__(self, state_dim):
        super().__init__()
        self.fc1 = nn.Linear(state_dim, 256)
        self.fc2 = nn.Linear(256, 256)
        self.value = nn.Linear(256, 1)
    
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = F.relu(self.fc2(x))
        return self.value(x)
```

**Hyperparameters:**
- Clip ratio Œµ: 0.2
- GAE Œª: 0.95
- Epochs per update: 10
- Mini-batch size: 64
- Learning rate: 3e-4
- Value coefficient: 0.5
- Entropy coefficient: 0.01

### Part 2: SAC and DDPG Comparison

**Tasks:**
1. Implement DDPG with Ornstein-Uhlenbeck noise
2. Implement SAC with automatic temperature
3. Compare sample efficiency and final performance
4. Analyze exploration strategies

**DDPG Exploration Noise:**
```python
class OUNoise:
    def __init__(self, size, mu=0, theta=0.15, sigma=0.2):
        self.mu = mu * np.ones(size)
        self.theta = theta
        self.sigma = sigma
        self.reset()
    
    def reset(self):
        self.state = copy.copy(self.mu)
    
    def sample(self):
        dx = self.theta * (self.mu - self.state) + \
             self.sigma * np.random.randn(len(self.state))
        self.state += dx
        return self.state
```

## üìä Evaluation Metrics

1. **Episode Return**: Average return over evaluation episodes
2. **Sample Efficiency**: Return vs environment steps
3. **Training Stability**: Variance across random seeds
4. **Exploration**: Policy entropy over time (for stochastic policies)
5. **Q-Value Accuracy**: Difference between estimated and actual returns
6. **Wall-Clock Time**: Real training time

## üîß Requirements

```python
# Core Libraries
numpy>=1.21.0
matplotlib>=3.4.0
seaborn>=0.11.0

# RL Environments
gymnasium>=0.28.0
pybullet>=3.2.0  # For robotics environments
mujoco>=2.3.0    # For MuJoCo environments

# Deep Learning
torch>=2.0.0

# Utilities
pandas>=1.3.0
tqdm>=4.62.0
tensorboard>=2.9.0
```

## üöÄ Getting Started

```bash
cd HW4_Actor_Critic
pip install -r requirements.txt

# Run PPO
jupyter notebook code/HW4_P1_PPO_Continuous.ipynb

# Run SAC/DDPG comparison
jupyter notebook code/HW4_P2_SAC_DDPG_Continuous.ipynb
```

## üìà Expected Results

### PPO on Pendulum
- Converges in ~100-200 episodes
- Final performance: -200 to -150 reward
- Stable training with low variance

### SAC vs DDPG on HalfCheetah
- **SAC**: ~1M steps to 5000+ reward
- **DDPG**: ~1.5M steps, less stable
- **SAC**: Better exploration, more robust

## üêõ Common Issues & Solutions

### Issue 1: PPO Not Learning
**Solutions:**
- Check advantage normalization
- Ensure proper GAE computation
- Verify clipping ratio (try 0.1-0.3)
- Increase epochs or batch size

### Issue 2: DDPG Unstable
**Solutions:**
- Reduce learning rate
- Increase target update œÑ (try 0.005)
- Add gradient clipping
- Tune exploration noise

### Issue 3: SAC Overexplores
**Solutions:**
- Check target entropy (default: -dim(action))
- Monitor Œ± value
- Ensure proper reward scaling

## üìñ References

### Key Papers

1. **Schulman, J., et al. (2017)** - "Proximal Policy Optimization Algorithms" [arXiv:1707.06347](https://arxiv.org/abs/1707.06347)

2. **Lillicrap, T. P., et al. (2015)** - "Continuous control with deep reinforcement learning" [arXiv:1509.02971](https://arxiv.org/abs/1509.02971)

3. **Haarnoja, T., et al. (2018)** - "Soft Actor-Critic: Off-Policy Maximum Entropy Deep RL" [arXiv:1801.01290](https://arxiv.org/abs/1801.01290)

4. **Schulman, J., et al. (2016)** - "High-Dimensional Continuous Control Using Generalized Advantage Estimation" [arXiv:1506.02438](https://arxiv.org/abs/1506.02438)

## üí° Discussion Questions

1. Why does PPO outperform vanilla policy gradients?
2. How does SAC's entropy term improve exploration?
3. Why use twin Q-networks in SAC?
4. When would deterministic policies (DDPG) be preferred over stochastic (SAC)?
5. What are the trade-offs between on-policy (PPO) and off-policy (SAC) methods?

## üéì Extensions

- Implement TD3 (Twin Delayed DDPG)
- Add hindsight experience replay
- Try multi-task learning
- Implement distributed PPO (DPPO/IMPALA)

---

**Course:** Deep Reinforcement Learning  
**Last Updated:** 2024

