{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9PAzBIOgimq"
      },
      "source": [
        "# Assignment: Implementing MADDPG with the TorchRL Toolkit\n",
        "In this task, you’ll implement the MADDPG algorithm — a method used to train multiple agents to learn and collaborate effectively.\n",
        "\n",
        "To make things smoother, we’ll be using TorchRL, a library that simplifies building and training RL agents.\n",
        "\n",
        "The assignment has two main goals:\n",
        "\n",
        "1. Help you understand the key ideas behind MADDPG, especially the idea of centralized training (agents learn together) and decentralized execution (they act independently).\n",
        "\n",
        "2. Introduce you to important TorchRL.\n",
        "\n",
        "We’ve already set up the basic structure for you. Your job is to complete the missing pieces marked as TODOs. Before each coding step, we’ll explain what the MADDPG concept is and how to apply it using the right TorchRL tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5sNEpyyr2qw"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip3 install torchrl\n",
        "!pip3 install vmas\n",
        "!pip3 install tqdm\n",
        "!apt-get update -y\n",
        "!apt-get install -y x11-utils xvfb python3-opengl libgl1-mesa-glx libglu1-mesa\n",
        "!pip install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7WYX8P0YGGy"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndJCvEKxX7IT"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import tempfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import pyvirtualdisplay\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from tensordict import TensorDictBase\n",
        "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
        "from torch import multiprocessing\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
        "from torchrl.envs import (\n",
        "    check_env_specs,\n",
        "    RewardSum,\n",
        "    TransformedEnv,\n",
        "    VmasEnv,\n",
        ")\n",
        "from torchrl.modules import (\n",
        "    AdditiveGaussianModule,\n",
        "    MLP,\n",
        "    ProbabilisticActor,\n",
        "    TanhDelta,\n",
        ")\n",
        "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Setup Virtual Display ---\n",
        "try:\n",
        "    display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "    display.start()\n",
        "    print(\"Virtual display started.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not start virtual display: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTDTHy_qYbZj"
      },
      "source": [
        "##Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLxkb6VXYKuu"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "seed = 0\n",
        "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
        "device = torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device(\"cpu\")\n",
        "torch.manual_seed(seed)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Vmas Environment\n",
        "scenario_name = \"navigation\"\n",
        "n_agents = 3\n",
        "max_steps = 100  # Episode steps before done\n",
        "\n",
        "# Sampling\n",
        "frames_per_batch = 2000\n",
        "n_iters = 1500\n",
        "total_frames = frames_per_batch * n_iters\n",
        "num_vmas_envs = frames_per_batch // max_steps\n",
        "\n",
        "# Replay Buffer\n",
        "memory_size = 2_000_000\n",
        "\n",
        "# Training\n",
        "n_optimiser_steps = 10\n",
        "train_batch_size = 512\n",
        "lr = 3e-4\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# DDPG Algorithm\n",
        "gamma = 0.99\n",
        "polyak_tau = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHe9QGlZYflY"
      },
      "source": [
        "##Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8yKRbusYkIP"
      },
      "outputs": [],
      "source": [
        "# Each agent is in its own group, so group_name == agent_name\n",
        "custom_group_map = {f\"agent_{i}\": [f\"agent_{i}\"] for i in range(n_agents)}\n",
        "\n",
        "# Create the vectorized Vmas environment\n",
        "env = VmasEnv(\n",
        "    scenario=scenario_name,\n",
        "    num_envs=num_vmas_envs,\n",
        "    continuous_actions=True,\n",
        "    max_steps=max_steps,\n",
        "    device=device,\n",
        "    n_agents=n_agents,\n",
        "    group_map=custom_group_map,\n",
        ")\n",
        "\n",
        "# Wrap the environment to sum rewards for each agent group\n",
        "env = TransformedEnv(\n",
        "    env,\n",
        "    RewardSum(\n",
        "        in_keys=env.reward_keys,\n",
        "        reset_keys=[\"_reset\"] * len(env.group_map.keys()),\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Print environment specs\n",
        "print(f\"group_map: {env.group_map}\")\n",
        "print(\"action_spec:\", env.full_action_spec)\n",
        "print(\"reward_spec:\", env.full_reward_spec)\n",
        "print(\"done_spec:\", env.full_done_spec)\n",
        "print(\"observation_spec:\", env.observation_spec)\n",
        "\n",
        "check_env_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7LQITA1g1cP"
      },
      "source": [
        "##Part 1 : Decentralized Actor\n",
        "\n",
        "**1a. MADDPG Concept: The Agent's Brain**\n",
        "\n",
        "In MADDPG, each agent has its own independent \"actor\" network. This network takes the agent's observation and decides which action to take. It's the \"decentralized execution\" part of the algorithm.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Implement the Actor Network using torchrl.modules.MLP. This will be a standard PyTorch nn.Module that serves as the brain for a single agent.\n",
        "\n",
        "**1b. TorchRL vs TensorDictModule**\n",
        "\n",
        "A standard nn.Module doesn't know how to interact with TorchRL's data structures. We need to wrap it with a TensorDictModule. This wrapper acts as an adapter, telling your MLP which data \"key\" to read its input from in the TensorDict and which \"key\" to write its output to.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Wrap your AgentMLP in a TensorDictModule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxXuM9FgYn8g"
      },
      "source": [
        "##Policy Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SkGNtr-YznQ"
      },
      "outputs": [],
      "source": [
        "# Part 1: Create the Actor Network using torchrl.modules.MLP\n",
        "policy_modules = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    agent_modules = {}\n",
        "    for agent in agents:\n",
        "        ### TODO: PART 1a ###\n",
        "        # Create an actor network using `torchrl.modules.MLP`.\n",
        "        # - `in_features`: The dimension of the agent's observation.\n",
        "        # - `out_features`: The dimension of the agent's action.\n",
        "        # - `num_cells`: A list defining the hidden layer sizes (e.g., [256, 256]).\n",
        "        # - `activation_class`: The activation function (e.g., nn.ReLU).\n",
        "        ### YOUR CODE HERE ###\n",
        "        obs_dim = env.observation_spec[agent, \"observation\"].shape[0]\n",
        "        action_dim = env.full_action_spec[agent, \"action\"].shape[0]\n",
        "        agent_modules[agent] = MLP(\n",
        "            in_features=obs_dim,\n",
        "            out_features=action_dim,\n",
        "            num_cells=[256, 256],\n",
        "            activation_class=nn.ReLU\n",
        "        )\n",
        "\n",
        "    ### TODO: PART 1b ###\n",
        "    # Wrap the MLP actor in a TensorDictModule to handle I/O.\n",
        "    # - The input should be the agent's observation: `(agent, \"observation\")`.\n",
        "    # - The output should be the action parameters: `(agent, \"param\")`.\n",
        "    ### YOUR CODE HERE ###\n",
        "    agent_policy_modules = {}\n",
        "    for agent in agents:\n",
        "        agent_policy_modules[agent] = TensorDictModule(\n",
        "            agent_modules[agent],\n",
        "            in_keys=[(agent, \"observation\")],\n",
        "            out_keys=[(agent, \"param\")]\n",
        "        )\n",
        "    policy_modules[group] = TensorDictSequential(*agent_policy_modules.values())\n",
        "\n",
        "\n",
        "# Create Probabilistic Policies\n",
        "policies = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    agent_policies = []\n",
        "    for agent in agents:\n",
        "        agent_policies.append(\n",
        "            ProbabilisticActor(\n",
        "                module=policy_modules[group],\n",
        "                spec=env.full_action_spec[agent, \"action\"],\n",
        "                in_keys=[(agent, \"param\")],\n",
        "                out_keys=[(agent, \"action\")],\n",
        "                distribution_class=TanhDelta,\n",
        "                distribution_kwargs={\n",
        "                    \"low\": env.full_action_spec_unbatched[agent, \"action\"].space.low,\n",
        "                    \"high\": env.full_action_spec_unbatched[agent, \"action\"].space.high,\n",
        "                },\n",
        "                return_log_prob=False,\n",
        "            )\n",
        "        )\n",
        "    policies[group] = TensorDictSequential(*agent_policies)\n",
        "\n",
        "# Create Target Policies for DDPG\n",
        "target_policies = copy.deepcopy(policies)\n",
        "\n",
        "# Create Exploration Policies: An AdditiveGaussianModule is appended to the policy to add noise for exploration\n",
        "exploration_policies = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    first_actor = None\n",
        "    for module in policies[group].modules():\n",
        "        if isinstance(module, ProbabilisticActor):\n",
        "            first_actor = module\n",
        "            break\n",
        "    if first_actor is None:\n",
        "        raise RuntimeError(\"No ProbabilisticActor found in policies[group]\")\n",
        "\n",
        "    exploration_policy = TensorDictSequential(\n",
        "        policies[group],\n",
        "        AdditiveGaussianModule(\n",
        "            spec=first_actor.spec,\n",
        "            annealing_num_steps=total_frames // 3,\n",
        "            action_key=(group, \"action\"),\n",
        "            sigma_init=0.5,\n",
        "            sigma_end=0.05,\n",
        "        ),\n",
        "    )\n",
        "    exploration_policies[group] = exploration_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI4b4gE2iXaV"
      },
      "source": [
        "##Part 2: The Centralized Critic\n",
        "\n",
        "**2a. MADDPG Concept: Using Global Information**\n",
        "\n",
        "One of the key strengths of the MADDPG algorithm lies in its use of a centralized critic during training. Unlike the decentralized actors, which only have access to their individual observations, the centralized critic has access to the observations and actions of all agents. This broader perspective enables it to more accurately evaluate the quality of joint actions taken by the agents, which in turn leads to more stable and cooperative learning dynamics.\n",
        "\n",
        "Your task:\n",
        "\n",
        "Implement the CentralizedCritic using torchrl.modules.MLP. This network takes observations and actions of all agents, and return a scalar value representing the estimated state-action value. It's the \"centralized training\" part of the algorithm.\n",
        "\n",
        "**2b. TorchRL Tool: Assembling Inputs with TensorDictModule**\n",
        "\n",
        "How do we collect data from many different keys and feed it as one tensor to our critic? TensorDictModule can do more than just wrap a network; it can also perform operations. We can give it a list of in_keys and a lambda function to tell it how to combine them.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Create a TensorDictModule that gathers all observations and actions and concatenates them into a single tensor for the critic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpcuMBejirJc"
      },
      "source": [
        "##Critic Network Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqaa0g_LlLmJ"
      },
      "outputs": [],
      "source": [
        "# Part 2: Create the Centralized Critic using torchrl.modules.MLP\n",
        "critics = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    ### TODO: PART 2a ###\n",
        "    # Create the centralized critic network using `torchrl.modules.MLP`.\n",
        "    # - First, calculate `in_features`.\n",
        "    # - `out_features` should be 1, as the critic outputs a single Q-value.\n",
        "    ### YOUR CODE HERE ###\n",
        "    agent_critic_modules = {}\n",
        "    for agent in agents:\n",
        "        # Calculate input features: sum of all agents' obs + actions\n",
        "        obs_dim = env.observation_spec[agent, \"observation\"].shape[0]\n",
        "        action_dim = env.full_action_spec[agent, \"action\"].shape[0]\n",
        "        critic_in_features = sum(\n",
        "            env.observation_spec[other_agent, \"observation\"].shape[0] + \n",
        "            env.full_action_spec[other_agent, \"action\"].shape[0]\n",
        "            for other_agent in agents\n",
        "        )\n",
        "        agent_critic_modules[agent] = MLP(\n",
        "            in_features=critic_in_features,\n",
        "            out_features=1,\n",
        "            num_cells=[256, 256],\n",
        "            activation_class=nn.ReLU\n",
        "        )\n",
        "\n",
        "    ### TODO: PART 2b ###\n",
        "    # Wire up the critic. This involves creating a `cat_module` that\n",
        "    # concatenates all agent observations and actions into a single tensor.\n",
        "    ### YOUR CODE HERE ###\n",
        "    agent_critic_tdmodules = {}\n",
        "    for agent in agents:\n",
        "        # 1. Define the `cat_inputs` list for concatenation.\n",
        "        cat_inputs = []\n",
        "        for other_agent in agents:\n",
        "            cat_inputs.append((other_agent, \"observation\"))\n",
        "            cat_inputs.append((other_agent, \"action\"))\n",
        "\n",
        "        # 2. Create the `cat_module` using TensorDictModule and a lambda function.\n",
        "        cat_module = TensorDictModule(\n",
        "            lambda *tensors: torch.cat(tensors, dim=-1),\n",
        "            in_keys=cat_inputs,\n",
        "            out_keys=[(agent, \"obs_actions\")]\n",
        "        )\n",
        "\n",
        "        critic_module = TensorDictModule(\n",
        "            agent_critic_modules[agent],\n",
        "            in_keys=[(agent, \"obs_actions\")], # Must match cat_module's out_key\n",
        "            out_keys=[(agent, \"state_action_value\")],\n",
        "        )\n",
        "        agent_critic_tdmodules[agent] = TensorDictSequential(cat_module, critic_module)\n",
        "    critics[group] = TensorDictSequential(*agent_critic_tdmodules.values())\n",
        "\n",
        "print(\"Model and policy structure ready for review.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voH8phhhllef"
      },
      "source": [
        "##Part 3: The Learning Algorithm\n",
        "\n",
        "**3a. MADDPG Concept: The Update Step**\n",
        "\n",
        "The critic learns by comparing its Q-value prediction to a \"target\" value calculated from the reward and the next state's value. The actor then learns by performing gradient ascent to find actions that the critic scores highly.\n",
        "\n",
        "**3b. TorchRL Tool: DDPGLoss**\n",
        "\n",
        "This high-level module encapsulates the entire loss calculation for both the actor and the critic. You provide your networks, and it computes the gradients. Your only job is to tell it which keys to use for its calculations. You must also supply the actions from the target policies, as these are used to calculate the target Q-value, which is a key part of the DDPG algorithm.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "1.   Configure the DDPGLoss module with the correct keys.\n",
        "2.   In the main training loop, provide the target actions needed for the loss calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Q2kJQmZF5V"
      },
      "source": [
        "##Replay Buffer, Losses, and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n5mZTqNY8Ww"
      },
      "outputs": [],
      "source": [
        "# Part 3\n",
        "# Shared Replay Buffer\n",
        "shared_replay_buffer = ReplayBuffer(\n",
        "    storage=LazyMemmapStorage(memory_size, scratch_dir=tempfile.TemporaryDirectory().name),\n",
        "    sampler=RandomSampler(),\n",
        "    batch_size=train_batch_size,\n",
        ")\n",
        "if device.type != \"cpu\":\n",
        "    shared_replay_buffer.append_transform(lambda x: x.to(device))\n",
        "\n",
        "# DDPG Losses\n",
        "losses = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    loss_module = DDPGLoss(\n",
        "        actor_network=policies[group],\n",
        "        value_network=critics[group],\n",
        "        delay_value=True,\n",
        "        delay_actor=True,\n",
        "        loss_function=\"l2\",\n",
        "    )\n",
        "    ### TODO: PART 3a ###\n",
        "    # Use `loss_module.set_keys(...)` to map the tensor names to what the\n",
        "    # loss function expects. You must map \"reward\", \"done\", \"terminated\",\n",
        "    # and the output of your critic: \"state_action_value\".\n",
        "    ### YOUR CODE HERE ###\n",
        "    loss_module.set_keys(\n",
        "        reward=(group, \"reward\"),\n",
        "        done=(group, \"done\"),\n",
        "        terminated=(group, \"terminated\"),\n",
        "        value=(group, \"state_action_value\")\n",
        "    )\n",
        "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
        "    losses[group] = loss_module\n",
        "\n",
        "# Target Network Updaters and Optimizers\n",
        "target_updaters = {group: SoftUpdate(loss, tau=polyak_tau) for group, loss in losses.items()}\n",
        "optimisers = {\n",
        "    group: {\n",
        "        \"loss_actor\": torch.optim.Adam(loss.actor_network_params.flatten_keys().values(), lr=1e-4),\n",
        "        \"loss_value\": torch.optim.Adam(loss.value_network_params.flatten_keys().values(), lr=3e-4),\n",
        "    }\n",
        "    for group, loss in losses.items()\n",
        "}\n",
        "print(\"Losses, optimizers, and replay buffer are ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvEy6IURZMd9"
      },
      "source": [
        "##Data Collection and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6m1yO0iZQQb"
      },
      "outputs": [],
      "source": [
        "# Data Collection and Training\n",
        "collector = SyncDataCollector(\n",
        "    env,\n",
        "    TensorDictSequential(*exploration_policies.values()),\n",
        "    device=device,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        ")\n",
        "\n",
        "\n",
        "def process_batch(batch: TensorDictBase) -> TensorDictBase:\n",
        "    for group in env.group_map.keys():\n",
        "        keys = list(batch.keys(True, True))\n",
        "        group_shape = batch.get_item_shape(group)\n",
        "        nested_done_key = (\"next\", group, \"done\")\n",
        "        nested_terminated_key = (\"next\", group, \"terminated\")\n",
        "        if nested_done_key not in keys:\n",
        "            batch.set(\n",
        "                nested_done_key,\n",
        "                batch.get((\"next\", \"done\")).unsqueeze(-1).expand((*group_shape, 1)),\n",
        "            )\n",
        "        if nested_terminated_key not in keys:\n",
        "            batch.set(\n",
        "                nested_terminated_key,\n",
        "                batch.get((\"next\", \"terminated\"))\n",
        "                .unsqueeze(-1)\n",
        "                .expand((*group_shape, 1)),\n",
        "            )\n",
        "    return batch\n",
        "\n",
        "# Training Loop\n",
        "episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
        "pbar = tqdm(total=n_iters, desc=\"Training Progress\")\n",
        "\n",
        "for iteration, batch in enumerate(collector):\n",
        "    current_frames = batch.numel()\n",
        "    batch = process_batch(batch)\n",
        "    shared_replay_buffer.extend(batch.reshape(-1))\n",
        "\n",
        "    for group in env.group_map.keys():\n",
        "        for _ in range(n_optimiser_steps):\n",
        "            subdata = shared_replay_buffer.sample()\n",
        "\n",
        "            # --- Part 3b: Compute Target Actions for the Critic's Loss ---\n",
        "            with torch.no_grad():\n",
        "                next_td = subdata.get(\"next\")\n",
        "                # The DDPG loss needs to know what the *target* policies would do in\n",
        "                # the next state. Loop through all agent groups, run their `target_policies`\n",
        "                # on `next_td`, and store the resulting action under the key `(\"next\", other_group, \"action\")`.\n",
        "                for other_group in env.group_map.keys():\n",
        "                    next_td = target_policies[other_group](next_td)\n",
        "\n",
        "            loss_vals = losses[group](subdata)\n",
        "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
        "                loss = loss_vals[loss_name]\n",
        "                optimiser = optimisers[group][loss_name]\n",
        "                loss.backward()\n",
        "                params = optimiser.param_groups[0][\"params\"]\n",
        "                torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
        "                optimiser.step()\n",
        "                optimiser.zero_grad()\n",
        "            target_updaters[group].step()\n",
        "        exploration_policies[group][-1].step(current_frames)\n",
        "\n",
        "    for group in env.group_map.keys():\n",
        "        episode_reward_mean = (\n",
        "            batch.get((\"next\", group, \"episode_reward\"))[\n",
        "                batch.get((\"next\", group, \"done\"))\n",
        "            ]\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        episode_reward_mean_map[group].append(episode_reward_mean)\n",
        "\n",
        "    reward_strings = [\n",
        "        f\"{group}: {episode_reward_mean_map[group][-1]:.2f}\"\n",
        "        for group in env.group_map.keys()\n",
        "    ]\n",
        "    description = (\n",
        "        f\"Iter [{iteration+1}/{n_iters}] | Rewards: \" + \" | \".join(reward_strings)\n",
        "    )\n",
        "    pbar.set_description(description)\n",
        "    pbar.refresh()\n",
        "\n",
        "pbar.close()\n",
        "collector.shutdown()\n",
        "print(\"\\nTraining finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_eoTvo8ZZlS"
      },
      "source": [
        "##Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOuRkqpMZU_A"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(n_agents, 1, figsize=(10, 8), sharex=True)\n",
        "if n_agents == 1:\n",
        "    axs = [axs]\n",
        "for i, group in enumerate(env.group_map.keys()):\n",
        "    axs[i].plot(episode_reward_mean_map[group], label=f\"Episode reward mean {group}\")\n",
        "    axs[i].set_ylabel(\"Reward\")\n",
        "    axs[i].legend()\n",
        "axs[-1].set_xlabel(\"Training iterations\")\n",
        "fig.suptitle(\"Training Rewards\")\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MW6GGlkZjsr"
      },
      "source": [
        "##Evaluation and Rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWsThylfZmQG"
      },
      "outputs": [],
      "source": [
        "print(\"Starting evaluation and rendering...\")\n",
        "\n",
        "# Create a single environment for rendering\n",
        "render_env = VmasEnv(\n",
        "    scenario=scenario_name,\n",
        "    num_envs=1,\n",
        "    continuous_actions=True,\n",
        "    max_steps=max_steps,\n",
        "    device=device,\n",
        "    n_agents=n_agents,\n",
        "    group_map=custom_group_map,\n",
        ")\n",
        "\n",
        "td = render_env.reset()\n",
        "frames = []\n",
        "\n",
        "# Rollout Loop\n",
        "with torch.no_grad():\n",
        "    for _ in range(max_steps):\n",
        "        # 1. Run policies to get actions\n",
        "        for group in render_env.group_map.keys():\n",
        "            td = policies[group](td)\n",
        "\n",
        "        # 2. Step the environment\n",
        "        td_next = render_env.step(td)\n",
        "\n",
        "        # 3. Use the new observation for the next policy call\n",
        "        td = td_next.get(\"next\").clone()\n",
        "\n",
        "        # 4. Reset if the episode terminated\n",
        "        if td_next.get(\"done\").item():\n",
        "            td = render_env.reset()\n",
        "\n",
        "        # 5. Render the frame and append to list\n",
        "        frame = render_env.render(mode=\"rgb_array\")\n",
        "        frames.append(Image.fromarray(frame))\n",
        "\n",
        "# Save the rollout as a GIF\n",
        "gif_path = f\"{scenario_name}_evaluation.gif\"\n",
        "frames[0].save(\n",
        "    gif_path,\n",
        "    save_all=True,\n",
        "    append_images=frames[1:],\n",
        "    duration=100,\n",
        "    loop=0,\n",
        ")\n",
        "print(f\"✅ Saved animation as {gif_path}\")\n",
        "\n",
        "# To display the GIF in a Jupyter notebook, you can use the following:\n",
        "# from IPython.display import Image as IPImage\n",
        "# IPImage(filename=gif_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Converting MADDPG to IDDPG\n",
        "\n",
        "**MADDPG vs IDDPG: Key Differences**\n",
        "\n",
        "The main difference between MADDPG (Multi-Agent Deep Deterministic Policy Gradient) and IDDPG (Independent Deep Deterministic Policy Gradient) lies in the critic network:\n",
        "\n",
        "- **MADDPG**: Uses a centralized critic that has access to observations and actions of ALL agents\n",
        "- **IDDPG**: Uses independent critics, where each agent's critic only has access to its own observations and actions\n",
        "\n",
        "**Changes Required for IDDPG:**\n",
        "\n",
        "1. **Independent Critics**: Each agent gets its own critic that only sees its own state and action\n",
        "2. **No Centralized Information**: Remove the concatenation of all agents' observations and actions\n",
        "3. **Independent Learning**: Each agent learns based only on its own experience\n",
        "\n",
        "Let's implement IDDPG by modifying the critic network structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDDPG Implementation\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPLEMENTING IDDPG (Independent Deep Deterministic Policy Gradient)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create IDDPG Critics - Each agent has its own independent critic\n",
        "iddpg_critics = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    agent_critic_modules = {}\n",
        "    agent_critic_tdmodules = {}\n",
        "    \n",
        "    for agent in agents:\n",
        "        # IDDPG: Each critic only sees its own observation and action\n",
        "        obs_dim = env.observation_spec[agent, \"observation\"].shape[0]\n",
        "        action_dim = env.full_action_spec[agent, \"action\"].shape[0]\n",
        "        \n",
        "        # Independent critic input: only this agent's obs + action\n",
        "        critic_in_features = obs_dim + action_dim\n",
        "        \n",
        "        agent_critic_modules[agent] = MLP(\n",
        "            in_features=critic_in_features,\n",
        "            out_features=1,\n",
        "            num_cells=[256, 256],\n",
        "            activation_class=nn.ReLU\n",
        "        )\n",
        "        \n",
        "        # IDDPG: No concatenation across agents - each critic is independent\n",
        "        critic_module = TensorDictModule(\n",
        "            agent_critic_modules[agent],\n",
        "            in_keys=[(agent, \"observation\"), (agent, \"action\")],\n",
        "            out_keys=[(agent, \"state_action_value\")],\n",
        "        )\n",
        "        agent_critic_tdmodules[agent] = critic_module\n",
        "    \n",
        "    iddpg_critics[group] = TensorDictSequential(*agent_critic_tdmodules.values())\n",
        "\n",
        "print(\"IDDPG Critics created successfully!\")\n",
        "print(\"Key difference: Each critic only sees its own agent's observation and action\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDDPG Training Setup\n",
        "print(\"Setting up IDDPG training components...\")\n",
        "\n",
        "# IDDPG Losses - Same structure as MADDPG but with independent critics\n",
        "iddpg_losses = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    loss_module = DDPGLoss(\n",
        "        actor_network=policies[group],  # Same actors as MADDPG\n",
        "        value_network=iddpg_critics[group],  # Independent critics\n",
        "        delay_value=True,\n",
        "        delay_actor=True,\n",
        "        loss_function=\"l2\",\n",
        "    )\n",
        "    loss_module.set_keys(\n",
        "        reward=(group, \"reward\"),\n",
        "        done=(group, \"done\"),\n",
        "        terminated=(group, \"terminated\"),\n",
        "        value=(group, \"state_action_value\")\n",
        "    )\n",
        "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
        "    iddpg_losses[group] = loss_module\n",
        "\n",
        "# IDDPG Target Network Updaters and Optimizers\n",
        "iddpg_target_updaters = {group: SoftUpdate(loss, tau=polyak_tau) for group, loss in iddpg_losses.items()}\n",
        "iddpg_optimisers = {\n",
        "    group: {\n",
        "        \"loss_actor\": torch.optim.Adam(loss.actor_network_params.flatten_keys().values(), lr=1e-4),\n",
        "        \"loss_value\": torch.optim.Adam(loss.value_network_params.flatten_keys().values(), lr=3e-4),\n",
        "    }\n",
        "    for group, loss in iddpg_losses.items()\n",
        "}\n",
        "\n",
        "print(\"IDDPG training components ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDDPG Training Loop\n",
        "print(\"Starting IDDPG training...\")\n",
        "\n",
        "# Reset replay buffer for fair comparison\n",
        "iddpg_replay_buffer = ReplayBuffer(\n",
        "    storage=LazyMemmapStorage(memory_size, scratch_dir=tempfile.TemporaryDirectory().name),\n",
        "    sampler=RandomSampler(),\n",
        "    batch_size=train_batch_size,\n",
        ")\n",
        "if device.type != \"cpu\":\n",
        "    iddpg_replay_buffer.append_transform(lambda x: x.to(device))\n",
        "\n",
        "# IDDPG Data Collection\n",
        "iddpg_collector = SyncDataCollector(\n",
        "    env,\n",
        "    TensorDictSequential(*exploration_policies.values()),\n",
        "    device=device,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        ")\n",
        "\n",
        "# IDDPG Training Loop\n",
        "iddpg_episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
        "iddpg_pbar = tqdm(total=n_iters, desc=\"IDDPG Training Progress\")\n",
        "\n",
        "for iteration, batch in enumerate(iddpg_collector):\n",
        "    current_frames = batch.numel()\n",
        "    batch = process_batch(batch)\n",
        "    iddpg_replay_buffer.extend(batch.reshape(-1))\n",
        "\n",
        "    for group in env.group_map.keys():\n",
        "        for _ in range(n_optimiser_steps):\n",
        "            subdata = iddpg_replay_buffer.sample()\n",
        "\n",
        "            # IDDPG: Compute target actions (same as MADDPG)\n",
        "            with torch.no_grad():\n",
        "                next_td = subdata.get(\"next\")\n",
        "                for other_group in env.group_map.keys():\n",
        "                    next_td = target_policies[other_group](next_td)\n",
        "\n",
        "            # IDDPG Loss computation\n",
        "            loss_vals = iddpg_losses[group](subdata)\n",
        "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
        "                loss = loss_vals[loss_name]\n",
        "                optimiser = iddpg_optimisers[group][loss_name]\n",
        "                loss.backward()\n",
        "                params = optimiser.param_groups[0][\"params\"]\n",
        "                torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
        "                optimiser.step()\n",
        "                optimiser.zero_grad()\n",
        "            iddpg_target_updaters[group].step()\n",
        "        exploration_policies[group][-1].step(current_frames)\n",
        "\n",
        "    # Track rewards\n",
        "    for group in env.group_map.keys():\n",
        "        episode_reward_mean = (\n",
        "            batch.get((\"next\", group, \"episode_reward\"))[\n",
        "                batch.get((\"next\", group, \"done\"))\n",
        "            ]\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        iddpg_episode_reward_mean_map[group].append(episode_reward_mean)\n",
        "\n",
        "    reward_strings = [\n",
        "        f\"{group}: {iddpg_episode_reward_mean_map[group][-1]:.2f}\"\n",
        "        for group in env.group_map.keys()\n",
        "    ]\n",
        "    description = (\n",
        "        f\"IDDPG Iter [{iteration+1}/{n_iters}] | Rewards: \" + \" | \".join(reward_strings)\n",
        "    )\n",
        "    iddpg_pbar.set_description(description)\n",
        "    iddpg_pbar.refresh()\n",
        "\n",
        "iddpg_pbar.close()\n",
        "iddpg_collector.shutdown()\n",
        "print(\"\\nIDDPG Training finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison Analysis: MADDPG vs IDDPG\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPARISON ANALYSIS: MADDPG vs IDDPG\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Plot comparison\n",
        "fig, axs = plt.subplots(n_agents, 1, figsize=(12, 10), sharex=True)\n",
        "if n_agents == 1:\n",
        "    axs = [axs]\n",
        "\n",
        "for i, group in enumerate(env.group_map.keys()):\n",
        "    # Plot MADDPG results\n",
        "    axs[i].plot(episode_reward_mean_map[group], \n",
        "                label=f\"MADDPG {group}\", \n",
        "                color='blue', \n",
        "                alpha=0.7,\n",
        "                linewidth=2)\n",
        "    \n",
        "    # Plot IDDPG results\n",
        "    axs[i].plot(iddpg_episode_reward_mean_map[group], \n",
        "                label=f\"IDDPG {group}\", \n",
        "                color='red', \n",
        "                alpha=0.7,\n",
        "                linewidth=2)\n",
        "    \n",
        "    axs[i].set_ylabel(\"Episode Reward\")\n",
        "    axs[i].legend()\n",
        "    axs[i].grid(True, alpha=0.3)\n",
        "    axs[i].set_title(f\"Agent {group} Performance Comparison\")\n",
        "\n",
        "axs[-1].set_xlabel(\"Training Iterations\")\n",
        "fig.suptitle(\"MADDPG vs IDDPG Performance Comparison\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Calculate final performance metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for group in env.group_map.keys():\n",
        "    maddpg_final_reward = episode_reward_mean_map[group][-10:]  # Last 10 episodes\n",
        "    iddpg_final_reward = iddpg_episode_reward_mean_map[group][-10:]  # Last 10 episodes\n",
        "    \n",
        "    maddpg_mean = np.mean(maddpg_final_reward)\n",
        "    iddpg_mean = np.mean(iddpg_final_reward)\n",
        "    \n",
        "    maddpg_std = np.std(maddpg_final_reward)\n",
        "    iddpg_std = np.std(iddpg_final_reward)\n",
        "    \n",
        "    print(f\"\\n{group}:\")\n",
        "    print(f\"  MADDPG: {maddpg_mean:.3f} ± {maddpg_std:.3f}\")\n",
        "    print(f\"  IDDPG:  {iddpg_mean:.3f} ± {iddpg_std:.3f}\")\n",
        "    print(f\"  Difference: {maddpg_mean - iddpg_mean:.3f} ({'MADDPG better' if maddpg_mean > iddpg_mean else 'IDDPG better'})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis and Discussion: MADDPG vs IDDPG\n",
        "\n",
        "### Key Differences Implemented\n",
        "\n",
        "**1. Critic Network Architecture:**\n",
        "- **MADDPG**: Centralized critic that concatenates observations and actions from ALL agents\n",
        "- **IDDPG**: Independent critics where each agent's critic only sees its own observation and action\n",
        "\n",
        "**2. Information Sharing:**\n",
        "- **MADDPG**: During training, critics have access to global information (all agents' states and actions)\n",
        "- **IDDPG**: Each agent learns independently without access to other agents' information\n",
        "\n",
        "**3. Training Complexity:**\n",
        "- **MADDPG**: More complex due to centralized training, but potentially more stable\n",
        "- **IDDPG**: Simpler training process, but may suffer from non-stationarity issues\n",
        "\n",
        "### Expected Performance Differences\n",
        "\n",
        "**MADDPG Advantages:**\n",
        "- **Better Coordination**: Centralized critics can better evaluate joint actions\n",
        "- **More Stable Learning**: Global information helps reduce non-stationarity\n",
        "- **Better Convergence**: Can learn more complex cooperative strategies\n",
        "\n",
        "**IDDPG Advantages:**\n",
        "- **Decentralized Execution**: No need for centralized information during execution\n",
        "- **Scalability**: Easier to scale to more agents\n",
        "- **Simplicity**: Simpler implementation and training process\n",
        "\n",
        "**IDDPG Disadvantages:**\n",
        "- **Non-stationarity**: Each agent's environment changes as other agents learn\n",
        "- **Coordination Issues**: Harder to learn cooperative strategies\n",
        "- **Slower Convergence**: May take longer to reach optimal policies\n",
        "\n",
        "### Theoretical Expectations\n",
        "\n",
        "In cooperative multi-agent environments like navigation tasks:\n",
        "- **MADDPG** should generally perform better due to its ability to coordinate\n",
        "- **IDDPG** may struggle with coordination but should still learn reasonable individual policies\n",
        "- The performance gap should be more pronounced in tasks requiring tight coordination\n",
        "\n",
        "### Implementation Notes\n",
        "\n",
        "The key changes made to convert MADDPG to IDDPG:\n",
        "1. **Critic Input**: Changed from concatenated global state to individual agent state\n",
        "2. **Network Architecture**: Each agent gets its own independent critic network\n",
        "3. **Training Process**: Same DDPG loss computation but with independent critics\n",
        "4. **Target Actions**: Still computed using target policies (this could be further simplified for pure IDDPG)\n",
        "\n",
        "This implementation provides a fair comparison between the two approaches while maintaining the same training infrastructure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAqmEiTZQE5n"
      },
      "source": [
        "##Part 4\n",
        "\n",
        "Based on your understanding of the differences between MADDPG and IDDPG, modify the necessary sections of your MADDPG implementation to convert it into its independent variant (IDDPG). Run the modified code, clearly explain the changes you made and the rationale behind each modification, and finally analyze and discuss the differences you observe between the performance of MADDPG and IDDPG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Analysis and Discussion\n",
        "print(\"=\" * 80)\n",
        "print(\"COMPREHENSIVE ANALYSIS: MADDPG vs IDDPG\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\"\"\n",
        "KEY DIFFERENCES IMPLEMENTED:\n",
        "\n",
        "1. CRITIC ARCHITECTURE:\n",
        "   • MADDPG: Centralized critic sees ALL agents' observations and actions\n",
        "   • IDDPG: Independent critics, each sees only its own agent's observation and action\n",
        "\n",
        "2. INFORMATION SHARING:\n",
        "   • MADDPG: Agents share information during training (centralized training)\n",
        "   • IDDPG: Agents train completely independently (decentralized training)\n",
        "\n",
        "3. COMPUTATIONAL COMPLEXITY:\n",
        "   • MADDPG: Higher computational cost due to larger critic input dimensions\n",
        "   • IDDPG: Lower computational cost, scales linearly with number of agents\n",
        "\n",
        "4. COOPERATION CAPABILITY:\n",
        "   • MADDPG: Better at learning cooperative strategies due to global information\n",
        "   • IDDPG: May struggle with coordination but more robust to non-stationarity\n",
        "\n",
        "EXPECTED PERFORMANCE CHARACTERISTICS:\n",
        "\n",
        "• MADDPG typically performs better in cooperative environments where agents need\n",
        "  to coordinate their actions and share information.\n",
        "\n",
        "• IDDPG may perform better in competitive environments or when agents need to\n",
        "  be more independent and robust to changes in other agents' policies.\n",
        "\n",
        "• MADDPG requires more communication bandwidth and computational resources.\n",
        "\n",
        "• IDDPG is more scalable and can handle environments with many agents more easily.\n",
        "\n",
        "TRAINING STABILITY:\n",
        "• MADDPG: More stable due to centralized critic providing better value estimates\n",
        "• IDDPG: May be less stable due to non-stationarity of other agents' policies\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"CONCLUSION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "This implementation demonstrates the key differences between MADDPG and IDDPG:\n",
        "\n",
        "1. MADDPG uses centralized critics that have access to all agents' information,\n",
        "   enabling better coordination and cooperation.\n",
        "\n",
        "2. IDDPG uses independent critics that only see each agent's own information,\n",
        "   making it more scalable but potentially less cooperative.\n",
        "\n",
        "3. The choice between MADDPG and IDDPG depends on the specific requirements:\n",
        "   - Use MADDPG when cooperation and coordination are important\n",
        "   - Use IDDPG when scalability and independence are prioritized\n",
        "\n",
        "4. Both algorithms use the same actor networks (decentralized execution),\n",
        "   but differ in their critic architectures (centralized vs independent training).\n",
        "\n",
        "The performance comparison shows how these architectural differences affect\n",
        "learning dynamics and final performance in multi-agent environments.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comparison Plotting: MADDPG vs IDDPG\n",
        "print(\"=\" * 60)\n",
        "print(\"PERFORMANCE COMPARISON: MADDPG vs IDDPG\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "fig, axs = plt.subplots(n_agents, 1, figsize=(12, 10), sharex=True)\n",
        "if n_agents == 1:\n",
        "    axs = [axs]\n",
        "\n",
        "for i, group in enumerate(env.group_map.keys()):\n",
        "    # Plot MADDPG results\n",
        "    axs[i].plot(episode_reward_mean_map[group], \n",
        "                label=f\"MADDPG - {group}\", \n",
        "                color='blue', \n",
        "                alpha=0.7,\n",
        "                linewidth=2)\n",
        "    \n",
        "    # Plot IDDPG results\n",
        "    axs[i].plot(iddpg_episode_reward_mean_map[group], \n",
        "                label=f\"IDDPG - {group}\", \n",
        "                color='red', \n",
        "                alpha=0.7,\n",
        "                linewidth=2)\n",
        "    \n",
        "    axs[i].set_ylabel(\"Episode Reward\")\n",
        "    axs[i].legend()\n",
        "    axs[i].grid(True, alpha=0.3)\n",
        "    axs[i].set_title(f\"Agent {group} Performance Comparison\")\n",
        "\n",
        "axs[-1].set_xlabel(\"Training Iterations\")\n",
        "fig.suptitle(\"MADDPG vs IDDPG Performance Comparison\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Calculate final performance metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL PERFORMANCE ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for group in env.group_map.keys():\n",
        "    maddpg_final_reward = episode_reward_mean_map[group][-100:]  # Last 100 iterations\n",
        "    iddpg_final_reward = iddpg_episode_reward_mean_map[group][-100:]  # Last 100 iterations\n",
        "    \n",
        "    maddpg_mean = np.mean(maddpg_final_reward)\n",
        "    iddpg_mean = np.mean(iddpg_final_reward)\n",
        "    \n",
        "    maddpg_std = np.std(maddpg_final_reward)\n",
        "    iddpg_std = np.std(iddpg_final_reward)\n",
        "    \n",
        "    print(f\"\\n{group}:\")\n",
        "    print(f\"  MADDPG Final Performance: {maddpg_mean:.2f} ± {maddpg_std:.2f}\")\n",
        "    print(f\"  IDDPG Final Performance:  {iddpg_mean:.2f} ± {iddpg_std:.2f}\")\n",
        "    print(f\"  Performance Difference:   {maddpg_mean - iddpg_mean:.2f}\")\n",
        "    \n",
        "    if maddpg_mean > iddpg_mean:\n",
        "        print(f\"  → MADDPG performs {((maddpg_mean - iddpg_mean) / iddpg_mean * 100):.1f}% better\")\n",
        "    else:\n",
        "        print(f\"  → IDDPG performs {((iddpg_mean - maddpg_mean) / maddpg_mean * 100):.1f}% better\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training Comparison: MADDPG vs IDDPG\n",
        "print(\"=\" * 60)\n",
        "print(\"TRAINING COMPARISON: MADDPG vs IDDPG\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Reset replay buffer for fair comparison\n",
        "shared_replay_buffer.clear()\n",
        "\n",
        "# Create new collector for IDDPG training\n",
        "iddpg_collector = SyncDataCollector(\n",
        "    env,\n",
        "    TensorDictSequential(*exploration_policies.values()),\n",
        "    device=device,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        ")\n",
        "\n",
        "# Training Loop for IDDPG\n",
        "iddpg_episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
        "iddpg_pbar = tqdm(total=n_iters, desc=\"IDDPG Training Progress\")\n",
        "\n",
        "print(\"Starting IDDPG training...\")\n",
        "for iteration, batch in enumerate(iddpg_collector):\n",
        "    current_frames = batch.numel()\n",
        "    batch = process_batch(batch)\n",
        "    shared_replay_buffer.extend(batch.reshape(-1))\n",
        "\n",
        "    for group in env.group_map.keys():\n",
        "        for _ in range(n_optimiser_steps):\n",
        "            subdata = shared_replay_buffer.sample()\n",
        "\n",
        "            # Compute Target Actions for IDDPG\n",
        "            with torch.no_grad():\n",
        "                next_td = subdata.get(\"next\")\n",
        "                for other_group in env.group_map.keys():\n",
        "                    next_td = target_policies[other_group](next_td)\n",
        "\n",
        "            # Use IDDPG losses instead of MADDPG losses\n",
        "            loss_vals = iddpg_losses[group](subdata)\n",
        "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
        "                loss = loss_vals[loss_name]\n",
        "                optimiser = iddpg_optimisers[group][loss_name]\n",
        "                loss.backward()\n",
        "                params = optimiser.param_groups[0][\"params\"]\n",
        "                torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
        "                optimiser.step()\n",
        "                optimiser.zero_grad()\n",
        "            iddpg_target_updaters[group].step()\n",
        "        exploration_policies[group][-1].step(current_frames)\n",
        "\n",
        "    for group in env.group_map.keys():\n",
        "        episode_reward_mean = (\n",
        "            batch.get((\"next\", group, \"episode_reward\"))[\n",
        "                batch.get((\"next\", group, \"done\"))\n",
        "            ]\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        iddpg_episode_reward_mean_map[group].append(episode_reward_mean)\n",
        "\n",
        "    reward_strings = [\n",
        "        f\"{group}: {iddpg_episode_reward_mean_map[group][-1]:.2f}\"\n",
        "        for group in env.group_map.keys()\n",
        "    ]\n",
        "    description = (\n",
        "        f\"IDDPG Iter [{iteration+1}/{n_iters}] | Rewards: \" + \" | \".join(reward_strings)\n",
        "    )\n",
        "    iddpg_pbar.set_description(description)\n",
        "    iddpg_pbar.refresh()\n",
        "\n",
        "iddpg_pbar.close()\n",
        "iddpg_collector.shutdown()\n",
        "print(\"IDDPG training finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create IDDPG Losses and Optimizers\n",
        "iddpg_losses = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    loss_module = DDPGLoss(\n",
        "        actor_network=policies[group],\n",
        "        value_network=iddpg_critics[group],  # Use IDDPG critics instead\n",
        "        delay_value=True,\n",
        "        delay_actor=True,\n",
        "        loss_function=\"l2\",\n",
        "    )\n",
        "    loss_module.set_keys(\n",
        "        reward=(group, \"reward\"),\n",
        "        done=(group, \"done\"),\n",
        "        terminated=(group, \"terminated\"),\n",
        "        value=(group, \"state_action_value\")\n",
        "    )\n",
        "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
        "    iddpg_losses[group] = loss_module\n",
        "\n",
        "# Target Network Updaters and Optimizers for IDDPG\n",
        "iddpg_target_updaters = {group: SoftUpdate(loss, tau=polyak_tau) for group, loss in iddpg_losses.items()}\n",
        "iddpg_optimisers = {\n",
        "    group: {\n",
        "        \"loss_actor\": torch.optim.Adam(loss.actor_network_params.flatten_keys().values(), lr=1e-4),\n",
        "        \"loss_value\": torch.optim.Adam(loss.value_network_params.flatten_keys().values(), lr=3e-4),\n",
        "    }\n",
        "    for group, loss in iddpg_losses.items()\n",
        "}\n",
        "\n",
        "print(\"IDDPG losses and optimizers created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Converting MADDPG to IDDPG\n",
        "\n",
        "**MADDPG vs IDDPG: Key Differences**\n",
        "\n",
        "The main difference between MADDPG (Multi-Agent Deep Deterministic Policy Gradient) and IDDPG (Independent Deep Deterministic Policy Gradient) lies in the critic network:\n",
        "\n",
        "- **MADDPG**: Uses a centralized critic that has access to observations and actions of ALL agents\n",
        "- **IDDPG**: Uses independent critics, where each agent's critic only sees its own observations and actions\n",
        "\n",
        "**Changes Required:**\n",
        "\n",
        "1. **Critic Input**: Instead of concatenating all agents' observations and actions, each critic should only see its own agent's observation and action\n",
        "2. **Critic Architecture**: Each agent gets its own independent critic network\n",
        "3. **Training**: Each agent trains independently without sharing information during critic updates\n",
        "\n",
        "Let's implement IDDPG by modifying the critic creation:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 4: IDDPG Implementation\n",
        "print(\"=\" * 60)\n",
        "print(\"IMPLEMENTING IDDPG (Independent DDPG)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Create IDDPG Critics - Each agent has its own independent critic\n",
        "iddpg_critics = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    agent_critic_modules = {}\n",
        "    for agent in agents:\n",
        "        # For IDDPG, each critic only sees its own observation and action\n",
        "        obs_dim = env.observation_spec[agent, \"observation\"].shape[0]\n",
        "        action_dim = env.full_action_spec[agent, \"action\"].shape[0]\n",
        "        critic_in_features = obs_dim + action_dim  # Only own obs + action\n",
        "        \n",
        "        agent_critic_modules[agent] = MLP(\n",
        "            in_features=critic_in_features,\n",
        "            out_features=1,\n",
        "            num_cells=[256, 256],\n",
        "            activation_class=nn.ReLU\n",
        "        )\n",
        "\n",
        "    # Create independent critics for each agent\n",
        "    agent_critic_tdmodules = {}\n",
        "    for agent in agents:\n",
        "        # Each critic only concatenates its own observation and action\n",
        "        cat_inputs = [(agent, \"observation\"), (agent, \"action\")]\n",
        "        \n",
        "        cat_module = TensorDictModule(\n",
        "            lambda *tensors: torch.cat(tensors, dim=-1),\n",
        "            in_keys=cat_inputs,\n",
        "            out_keys=[(agent, \"own_obs_action\")]\n",
        "        )\n",
        "\n",
        "        critic_module = TensorDictModule(\n",
        "            agent_critic_modules[agent],\n",
        "            in_keys=[(agent, \"own_obs_action\")],\n",
        "            out_keys=[(agent, \"state_action_value\")],\n",
        "        )\n",
        "        agent_critic_tdmodules[agent] = TensorDictSequential(cat_module, critic_module)\n",
        "    iddpg_critics[group] = TensorDictSequential(*agent_critic_tdmodules.values())\n",
        "\n",
        "print(\"IDDPG Critics created successfully!\")\n",
        "print(\"Key difference: Each critic only sees its own agent's observation and action\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analysis: MADDPG vs IDDPG\n",
        "\n",
        "**Key Differences and Trade-offs:**\n",
        "\n",
        "### 1. **Information Sharing**\n",
        "- **MADDPG**: Centralized training allows critics to see all agents' information, enabling better coordination\n",
        "- **IDDPG**: Independent critics only see local information, making coordination more challenging\n",
        "\n",
        "### 2. **Scalability**\n",
        "- **MADDPG**: Input size grows quadratically with number of agents (O(n²))\n",
        "- **IDDPG**: Input size grows linearly with number of agents (O(n))\n",
        "\n",
        "### 3. **Training Stability**\n",
        "- **MADDPG**: More stable due to centralized information, but requires all agents' actions during training\n",
        "- **IDDPG**: Less stable due to non-stationary environment, but simpler to implement\n",
        "\n",
        "### 4. **Coordination Ability**\n",
        "- **MADDPG**: Better at learning coordinated behaviors due to global information\n",
        "- **IDDPG**: May struggle with complex coordination tasks\n",
        "\n",
        "### 5. **Computational Requirements**\n",
        "- **MADDPG**: Higher computational cost due to larger critic networks\n",
        "- **IDDPG**: Lower computational cost with smaller, independent networks\n",
        "\n",
        "**When to Use Each:**\n",
        "- **Use MADDPG** when: Coordination is crucial, computational resources are available, agents need to work together\n",
        "- **Use IDDPG** when: Agents can work independently, computational resources are limited, simpler implementation is preferred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDDPG Training Setup\n",
        "print(\"Setting up IDDPG training...\")\n",
        "\n",
        "# Create IDDPG losses\n",
        "iddpg_losses = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    loss_module = DDPGLoss(\n",
        "        actor_network=policies[group],\n",
        "        value_network=iddpg_critics[group],\n",
        "        delay_value=True,\n",
        "        delay_actor=True,\n",
        "        loss_function=\"l2\",\n",
        "    )\n",
        "    loss_module.set_keys(\n",
        "        reward=(group, \"reward\"),\n",
        "        done=(group, \"done\"),\n",
        "        terminated=(group, \"terminated\"),\n",
        "        value=(group, \"state_action_value\")\n",
        "    )\n",
        "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
        "    iddpg_losses[group] = loss_module\n",
        "\n",
        "# Create IDDPG target networks\n",
        "iddpg_target_critics = copy.deepcopy(iddpg_critics)\n",
        "\n",
        "# IDDPG Target Network Updaters and Optimizers\n",
        "iddpg_target_updaters = {group: SoftUpdate(loss, tau=polyak_tau) for group, loss in iddpg_losses.items()}\n",
        "iddpg_optimisers = {\n",
        "    group: {\n",
        "        \"loss_actor\": torch.optim.Adam(loss.actor_network_params.flatten_keys().values(), lr=1e-4),\n",
        "        \"loss_value\": torch.optim.Adam(loss.value_network_params.flatten_keys().values(), lr=3e-4),\n",
        "    }\n",
        "    for group, loss in iddpg_losses.items()\n",
        "}\n",
        "\n",
        "print(\"IDDPG training setup complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDDPG Implementation: Independent Critics\n",
        "print(\"Creating IDDPG (Independent DDPG) implementation...\")\n",
        "\n",
        "# Create Independent Critics for IDDPG\n",
        "iddpg_critics = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    agent_critic_modules = {}\n",
        "    for agent in agents:\n",
        "        # For IDDPG, each critic only sees its own agent's observation and action\n",
        "        obs_dim = env.observation_spec[agent, \"observation\"].shape[0]\n",
        "        action_dim = env.full_action_spec[agent, \"action\"].shape[0]\n",
        "        critic_in_features = obs_dim + action_dim  # Only own obs + action\n",
        "        \n",
        "        agent_critic_modules[agent] = MLP(\n",
        "            in_features=critic_in_features,\n",
        "            out_features=1,\n",
        "            num_cells=[256, 256],\n",
        "            activation_class=nn.ReLU\n",
        "        )\n",
        "\n",
        "    # Create independent critic modules for each agent\n",
        "    agent_critic_tdmodules = {}\n",
        "    for agent in agents:\n",
        "        # Concatenate only the agent's own observation and action\n",
        "        cat_module = TensorDictModule(\n",
        "            lambda obs, action: torch.cat([obs, action], dim=-1),\n",
        "            in_keys=[(agent, \"observation\"), (agent, \"action\")],\n",
        "            out_keys=[(agent, \"own_obs_action\")]\n",
        "        )\n",
        "\n",
        "        critic_module = TensorDictModule(\n",
        "            agent_critic_modules[agent],\n",
        "            in_keys=[(agent, \"own_obs_action\")],\n",
        "            out_keys=[(agent, \"state_action_value\")],\n",
        "        )\n",
        "        agent_critic_tdmodules[agent] = TensorDictSequential(cat_module, critic_module)\n",
        "    iddpg_critics[group] = TensorDictSequential(*agent_critic_tdmodules.values())\n",
        "\n",
        "print(\"IDDPG critics created successfully!\")\n",
        "print(f\"MADDPG critic input size: {sum(env.observation_spec[agent, 'observation'].shape[0] + env.full_action_spec[agent, 'action'].shape[0] for agent in env.group_map['agent_0'])}\")\n",
        "print(f\"IDDPG critic input size: {env.observation_spec['agent_0', 'observation'].shape[0] + env.full_action_spec['agent_0', 'action'].shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
