{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18f69eb",
   "metadata": {},
   "source": [
    "# Meta-Reinforcement Learning with MAML\n",
    "\n",
    "In this assignment notebook, we will implement **Model-Agnostic Meta-Learning (MAML)** from scratch for a custom `HalfCheetahBackward` environment. The goal is to learn policy parameters that can quickly adapt to new tasks in this case, running the HalfCheetah agent **backward** with just a few gradient steps. Each section below provides context and key steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMA9RHEIqikL"
   },
   "outputs": [],
   "source": [
    "!pip -q install gymnasium[mujoco]\n",
    "!pip install imageio -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26141088",
   "metadata": {},
   "source": [
    "## Environment & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7gHgXr0xh6W",
    "outputId": "f0e29da5-6926-4c3c-e730-9eded019b690"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import imageio\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e4063",
   "metadata": {},
   "source": [
    "\n",
    "This is necessary for running the HalfCheetah env on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKMYUz6fxd4o",
    "outputId": "a6682905-e453-491e-ca65-368ca260305a"
   },
   "outputs": [],
   "source": [
    "%env MUJOCO_GL=egl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0871967",
   "metadata": {},
   "source": [
    "This code displays a saved mp4 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zh-QCtmGx6NP"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def show_video(path):\n",
    "    mp4 = open(path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(\"\"\"\n",
    "    <video width=400 controls>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d05f9",
   "metadata": {},
   "source": [
    "Run this code to get started with the HalfCheetah environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9AGb7Uur7Df"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v5\", render_mode=\"rgb_array\")\n",
    "env.reset()\n",
    "frames = []\n",
    "\n",
    "for _ in range(100):\n",
    "    frames.append(env.render())\n",
    "    action = # TODO\n",
    "    # TODO\n",
    "    \n",
    "env.close()\n",
    "imageio.mimsave('./HalfCheetah.mp4', frames, fps=20)\n",
    "show_video('./HalfCheetah.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48731171",
   "metadata": {},
   "source": [
    "We modify the HalfCheetah environment by creating a wrapper on top it to reward the model for moving backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRzn-YjCBamz"
   },
   "outputs": [],
   "source": [
    "class HalfCheetahBackward(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.env = gym.make(\"HalfCheetah-v5\", render_mode=\"rgb_array\")\n",
    "        self.forward_reward_weight = 1.0\n",
    "        self.ctrl_cost_weight = 0.05\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        return self.env.reset(seed=seed, options=options)[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, done, tr, info = self.env.step(action)\n",
    "        reward =  -1 * self.forward_reward_weight * info[\"reward_forward\"] + self.ctrl_cost_weight * info[\"reward_ctrl\"]\n",
    "        return obs, reward, done, tr, info\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca1173",
   "metadata": {},
   "source": [
    "## Gaussian Policy Network\n",
    "We parameterize our policy π_θ(a|s) as a multivariate Gaussian:\n",
    "\\[\n",
    "$μ_θ(s) = f_θ(s), \\quad Σ_θ = \\text{diag}(\\exp(2φ))$\n",
    "\\]\n",
    "Here, `mean_head` outputs μ_θ(s) and `log_std` (φ) is a learned vector of log-standard deviations. Sampling and computing log-probabilities from this distribution is essential for the policy gradient update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4eoMc7EERWx"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=(64,64)):\n",
    "        super().__init__()\n",
    "        layers = # TODO\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.mean_head = # TODO\n",
    "        self.log_std = nn.Parameter(torch.zeros(act_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        mean = self.mean_head(h)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        mean, std = self(obs)\n",
    "        dist = # TODO Use Normal Distribution\n",
    "        action = # TODO\n",
    "        return action, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf23f6d",
   "metadata": {},
   "source": [
    "## Trajectory Collection & Return Computation\n",
    "The `rollout` function runs the policy in the environment for up to `max_steps`, storing:\n",
    "- Observations **s_t**\n",
    "- Actions **a_t** sampled from π_θ\n",
    "- Log-probabilities **log π_θ(a_t|s_t)**\n",
    "- Rewards **r_t**\n",
    "After the episode, we compute discounted returns:\n",
    "\\[\n",
    "$G_t = \\sum_{k=0}^{T-t} γ^k r_{t+k}$\n",
    "\\]\n",
    "These returns serve as our baselines for policy gradient estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HMaGZXOTPrR"
   },
   "outputs": [],
   "source": [
    "def rollout(env, policy, max_steps=200, gamma=0.99):\n",
    "    obs = env.reset()\n",
    "    obs_buf, logp_buf, ret_buf = [], [], []\n",
    "    rewards = []\n",
    "    for _ in range(max_steps):\n",
    "        # TODO \n",
    "        # Gather obs_buf, logp_buf, rewards\n",
    "\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        # TODO Calculate discounted returns\n",
    "    ret_buf = torch.tensor(returns, dtype=torch.float32)\n",
    "    return obs_buf, logp_buf, ret_buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete this section to evaluate the model and save a sample trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, num_episodes=10, max_episode_len=200 ,path=\"./Final_Evaluation.mp4\", no_video=False):\n",
    "    frames = []\n",
    "    mean_rewards = 0\n",
    "    for episode in range(num_episodes):\n",
    "        # TODO \n",
    "        # Evaluate the model over `num_episodes`\n",
    "        # Record on trajectory to be shown if `no_video` is False\n",
    "    print(f\"Mean Reward: {mean_rewards}\")\n",
    "    if not no_video:\n",
    "        imageio.mimsave(path, frames, fps=20)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bff13",
   "metadata": {},
   "source": [
    "## Model-Agnostic Meta-Learning (MAML)\n",
    "MAML seeks initial parameters $\\theta$ that can adapt to a new task with only a few gradient steps. For each task Tᵢ, we perform an **inner loop** update:\n",
    "\n",
    "\\[\n",
    "$\\theta'_i = \\theta - α \\nabla_\\theta \\mathcal{L}_{T_i}(\\theta)$\n",
    "\\]\n",
    "\n",
    "Then, the **meta-objective** (outer loop) minimizes the post-adaptation loss (optimize for the initial parameter $\\theta$):\n",
    "\n",
    "\\[\n",
    "$\\min_\\theta \\sum_i \\mathcal{L}_{T_i}(\\theta'_i) = \\sum_i \\mathcal{L}_{T_i}\\bigl(\\theta - α \\nabla_\\theta \\mathcal{L}_{T_i}(\\theta)\\bigr)$\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ueGeJCWiChP"
   },
   "outputs": [],
   "source": [
    "class MAML:\n",
    "    def __init__(\n",
    "        self,\n",
    "        task_env_cls,\n",
    "        inner_lr,\n",
    "        outer_lr,\n",
    "        inner_steps,\n",
    "        meta_batch_size,\n",
    "        max_episode_len=200,\n",
    "        gamma=0.99\n",
    "    ):\n",
    "        self.task_env_cls = task_env_cls\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.meta_batch_size = meta_batch_size\n",
    "        self.max_episode_len = max_episode_len\n",
    "        self.gamma = gamma\n",
    "        self.loss_history   = []\n",
    "        self.reward_history = []\n",
    "\n",
    "        obs_dim = # TODO\n",
    "        act_dim = # TODO\n",
    "\n",
    "        self.meta_policy = # TODO\n",
    "        self.meta_opt = # TODO Use weight_decay\n",
    "\n",
    "    def inner_update(self, env):\n",
    "        obs_s, logp_s, ret_s = # TODO Call rollout to gather data\n",
    "        pg_loss = # TODO\n",
    "\n",
    "        l2_reg = # TODO Define L2 normalization coef\n",
    "        l2_loss = # TODO Calculate L2 Norm\n",
    "        loss_s = pg_loss + l2_reg * l2_loss\n",
    "\n",
    "        grads = torch.autograd.grad(loss_s, self.meta_policy.parameters(), create_graph=True)\n",
    "        return grads\n",
    "\n",
    "    def adapt_policy(self, grads):\n",
    "        adapted = # TODO create a copy of the model and update its weights\n",
    "        return adapted\n",
    "\n",
    "    def meta_step(self):\n",
    "        total_meta_loss = 0.0\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for _ in range(self.meta_batch_size):\n",
    "            env = self.task_env_cls()\n",
    "\n",
    "            grads = self.inner_update(env)\n",
    "\n",
    "            # TODO Get adapted policy (use grads to update weights)\n",
    "            adapted = self.adapt_policy(grads)\n",
    "\n",
    "            obs_q, logp_q, ret_q = # TODO Get another rollout using the adapted policy\n",
    "            loss_q = # TODO Calculate Meta Loss\n",
    "            total_meta_loss += loss_q\n",
    "\n",
    "            total_reward += ret_q.mean().item()\n",
    "\n",
    "            env.close()\n",
    "\n",
    "        meta_loss = total_meta_loss / self.meta_batch_size\n",
    "        self.meta_opt.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.meta_opt.step()\n",
    "\n",
    "        self.loss_history.append(meta_loss.item())\n",
    "        self.reward_history.append(total_reward / self.meta_batch_size)\n",
    "\n",
    "        return meta_loss.item()\n",
    "\n",
    "    def train(self, meta_iters=501):\n",
    "        for it in tqdm(range(1, meta_iters)):\n",
    "            loss = self.meta_step()\n",
    "            if it % 10 == 0:\n",
    "                print(f\"\\t[Iter {it}]\\tloss={loss:.3f},\\treward={self.reward_history[-1]:.3f}\")\n",
    "        self.plot_metrics()\n",
    "        return self.meta_policy\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        iters = range(1, len(self.loss_history) + 1)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.loss_history, label=\"Loss\")\n",
    "        plt.plot(iters, self.reward_history, label=\"Avg Query Reward\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training Progress\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9590b37",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "The `train()` function instantiates the `MAML` class with hyperparameters and executes the meta-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHFWgSSWEu9q"
   },
   "outputs": [],
   "source": [
    "def train(inner_lr, outer_lr, inner_steps, meta_batch_size, max_episode_len, gamma=0.99):\n",
    "    maml = MAML(\n",
    "        task_env_cls=HalfCheetahBackward,\n",
    "        inner_lr=inner_lr,\n",
    "        outer_lr=outer_lr,\n",
    "        inner_steps=inner_steps,\n",
    "        meta_batch_size=meta_batch_size,\n",
    "        max_episode_len=max_episode_len,\n",
    "        gamma=gamma\n",
    "    )\n",
    "    meta_policy = maml.train(meta_iters=500)\n",
    "    return meta_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ1Qzy6UE3q4",
    "outputId": "c1b024da-825c-4f93-b6f8-40ae1eb58439"
   },
   "outputs": [],
   "source": [
    "policy = train(inner_lr= # TODO\n",
    "               outer_lr= # TODO\n",
    "               meta_batch_size= # TODO\n",
    "               max_episode_len= # TODO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qV7KclmSU1mT"
   },
   "outputs": [],
   "source": [
    "env = HalfCheetahBackward()\n",
    "evaluate(policy, env, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e89M4g-uocwu"
   },
   "outputs": [],
   "source": [
    "show_video('./Final_Evaluation.mp4')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
