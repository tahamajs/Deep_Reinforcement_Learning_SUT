{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW14: Safe Reinforcement Learning\n",
        "\n",
        "> - Full Name: **[Your Full Name]**\n",
        "> - Student ID: **[Your Student ID]**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepRLCourse/Homework-14-Questions/blob/main/HW14_Notebook.ipynb)\n",
        "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/DeepRLCourse/Homework-14-Questions/main/HW14_Notebook.ipynb)\n",
        "\n",
        "## Overview\n",
        "This assignment focuses on **Safe Reinforcement Learning**, exploring methods to train agents that not only maximize rewards but also satisfy safety constraints during both training and deployment. We'll implement and experiment with:\n",
        "\n",
        "1. **Constrained Policy Optimization (CPO)**\n",
        "2. **Safety Layers and Shielding**\n",
        "3. **Risk-Sensitive RL (CVaR)**\n",
        "4. **Safe Exploration Techniques**\n",
        "5. **Robust RL Methods**\n",
        "\n",
        "The goal is to understand the fundamental trade-offs between performance and safety in RL systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Imports and Setup\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import trange\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Safe Environment Setup\n",
        "\n",
        "First, let's create a safe version of the CartPole environment where the agent must balance the pole while keeping the cart position within safe bounds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SafeCartPoleEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Safe CartPole environment with position constraints.\n",
        "    The agent must balance the pole while keeping cart position within safe bounds.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, position_limit=1.5, cost_threshold=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Create base CartPole environment\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.observation_space = self.env.observation_space\n",
        "        self.action_space = self.env.action_space\n",
        "        \n",
        "        # Safety parameters\n",
        "        self.position_limit = position_limit\n",
        "        self.cost_threshold = cost_threshold\n",
        "        \n",
        "        # Track episode statistics\n",
        "        self.episode_cost = 0\n",
        "        self.episode_reward = 0\n",
        "        self.constraint_violations = 0\n",
        "        \n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset environment and return initial observation.\"\"\"\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.episode_cost = 0\n",
        "        self.episode_reward = 0\n",
        "        self.constraint_violations = 0\n",
        "        return obs, info\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action and return next state, reward, done, truncated, info.\"\"\"\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        \n",
        "        # Extract cart position (first element of observation)\n",
        "        cart_position = obs[0]\n",
        "        \n",
        "        # Compute cost based on position constraint violation\n",
        "        position_violation = max(0, abs(cart_position) - self.position_limit)\n",
        "        cost = position_violation\n",
        "        \n",
        "        # Update episode statistics\n",
        "        self.episode_cost += cost\n",
        "        self.episode_reward += reward\n",
        "        \n",
        "        # Check for constraint violation\n",
        "        if cost > self.cost_threshold:\n",
        "            self.constraint_violations += 1\n",
        "            # Terminate episode if constraint violated\n",
        "            terminated = True\n",
        "            reward = -100  # Large penalty for constraint violation\n",
        "        \n",
        "        # Add cost information to info\n",
        "        info['cost'] = cost\n",
        "        info['episode_cost'] = self.episode_cost\n",
        "        info['episode_reward'] = self.episode_reward\n",
        "        info['constraint_violations'] = self.constraint_violations\n",
        "        \n",
        "        return obs, reward, terminated, truncated, info\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"Render the environment.\"\"\"\n",
        "        return self.env.render(mode)\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Close the environment.\"\"\"\n",
        "        self.env.close()\n",
        "\n",
        "# Test the safe environment\n",
        "print(\"Testing Safe CartPole Environment...\")\n",
        "env = SafeCartPoleEnv(position_limit=1.5, cost_threshold=0.1)\n",
        "\n",
        "# Run a few random episodes to test\n",
        "for episode in range(3):\n",
        "    obs, info = env.reset()\n",
        "    episode_reward = 0\n",
        "    episode_cost = 0\n",
        "    \n",
        "    for step in range(100):  # Max 100 steps\n",
        "        action = env.action_space.sample()  # Random action\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        \n",
        "        episode_reward += reward\n",
        "        episode_cost += info['cost']\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}, Cost = {episode_cost:.2f}, \"\n",
        "          f\"Violations = {info['constraint_violations']}\")\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Neural Network Architectures\n",
        "\n",
        "Let's implement the neural network components needed for safe RL algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    \"\"\"Gaussian policy network for continuous actions.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        mean = self.mean(x)\n",
        "        std = torch.exp(self.log_std.clamp(-20, 2))  # Clamp for numerical stability\n",
        "        return mean, std\n",
        "    \n",
        "    def sample(self, state):\n",
        "        \"\"\"Sample action from the policy.\"\"\"\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "        return action, log_prob\n",
        "    \n",
        "    def log_prob(self, state, action):\n",
        "        \"\"\"Compute log probability of action given state.\"\"\"\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        return dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    \"\"\"Value function approximator.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        return self.value(x)\n",
        "\n",
        "class SafetyNetwork(nn.Module):\n",
        "    \"\"\"Safety function approximator for Control Barrier Functions.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.safety = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        return self.safety(x)\n",
        "\n",
        "class DistributionalCritic(nn.Module):\n",
        "    \"\"\"Distributional critic for risk-sensitive RL.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, num_atoms=51, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.num_atoms = num_atoms\n",
        "        \n",
        "        # Network architecture\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, num_atoms)\n",
        "        \n",
        "        # Atom values (support of the distribution)\n",
        "        self.register_buffer('atoms', torch.linspace(-10, 10, num_atoms))\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        x = torch.cat([state, action], dim=-1)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return torch.softmax(logits, dim=-1)\n",
        "    \n",
        "    def get_distribution(self, state, action):\n",
        "        \"\"\"Get return distribution.\"\"\"\n",
        "        probs = self.forward(state, action)\n",
        "        return torch.distributions.Categorical(probs)\n",
        "    \n",
        "    def get_value(self, state, action):\n",
        "        \"\"\"Get expected value.\"\"\"\n",
        "        probs = self.forward(state, action)\n",
        "        return torch.sum(probs * self.atoms, dim=-1)\n",
        "\n",
        "# Test the networks\n",
        "print(\"Testing Neural Network Architectures...\")\n",
        "\n",
        "# Test parameters\n",
        "state_dim = 4  # CartPole state dimension\n",
        "action_dim = 1  # CartPole action dimension (continuous)\n",
        "\n",
        "# Create networks\n",
        "policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "value_reward = ValueNetwork(state_dim).to(device)\n",
        "value_cost = ValueNetwork(state_dim).to(device)\n",
        "safety = SafetyNetwork(state_dim).to(device)\n",
        "critic_dist = DistributionalCritic(state_dim, action_dim).to(device)\n",
        "\n",
        "# Test forward passes\n",
        "test_state = torch.randn(1, state_dim).to(device)\n",
        "test_action = torch.randn(1, action_dim).to(device)\n",
        "\n",
        "print(f\"Policy output shape: {policy(test_state)[0].shape}\")\n",
        "print(f\"Value output shape: {value_reward(test_state).shape}\")\n",
        "print(f\"Safety output shape: {safety(test_state).shape}\")\n",
        "print(f\"Distributional critic output shape: {critic_dist(test_state, test_action).shape}\")\n",
        "\n",
        "print(\"All networks initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Constrained Policy Optimization (CPO)\n",
        "\n",
        "Now let's implement the CPO algorithm, which extends TRPO to handle safety constraints directly in the optimization process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CPO:\n",
        "    \"\"\"Constrained Policy Optimization algorithm.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, cost_limit=10.0, lr=3e-4):\n",
        "        # Networks\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.value_reward = ValueNetwork(state_dim).to(device)\n",
        "        self.value_cost = ValueNetwork(state_dim).to(device)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.cost_limit = cost_limit\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_gae = 0.97\n",
        "        self.delta_kl = 0.01  # KL divergence bound\n",
        "        self.lr = lr\n",
        "        \n",
        "        # Optimizers\n",
        "        self.optimizer_value_r = optim.Adam(self.value_reward.parameters(), lr=lr)\n",
        "        self.optimizer_value_c = optim.Adam(self.value_cost.parameters(), lr=lr)\n",
        "        \n",
        "        # Storage for trajectories\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.costs = []\n",
        "        self.dones = []\n",
        "        self.log_probs = []\n",
        "        \n",
        "    def compute_advantages(self, rewards, values, costs, value_costs, dones):\n",
        "        \"\"\"Compute GAE advantages for reward and cost.\"\"\"\n",
        "        advantages_r = torch.zeros_like(rewards)\n",
        "        advantages_c = torch.zeros_like(costs)\n",
        "        \n",
        "        last_adv_r = 0\n",
        "        last_adv_c = 0\n",
        "        \n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if t == len(rewards) - 1:\n",
        "                next_value_r = 0\n",
        "                next_value_c = 0\n",
        "            else:\n",
        "                next_value_r = values[t + 1]\n",
        "                next_value_c = value_costs[t + 1]\n",
        "            \n",
        "            # Reward advantage\n",
        "            delta_r = rewards[t] + self.gamma * next_value_r * (1 - dones[t]) - values[t]\n",
        "            advantages_r[t] = last_adv_r = delta_r + self.gamma * self.lambda_gae * (1 - dones[t]) * last_adv_r\n",
        "            \n",
        "            # Cost advantage\n",
        "            delta_c = costs[t] + self.gamma * next_value_c * (1 - dones[t]) - value_costs[t]\n",
        "            advantages_c[t] = last_adv_c = delta_c + self.gamma * self.lambda_gae * (1 - dones[t]) * last_adv_c\n",
        "        \n",
        "        return advantages_r, advantages_c\n",
        "    \n",
        "    def update_value_networks(self, states, rewards, costs, dones):\n",
        "        \"\"\"Update value networks using collected data.\"\"\"\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        costs = torch.FloatTensor(costs).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "        \n",
        "        # Compute values\n",
        "        values_r = self.value_reward(states).squeeze()\n",
        "        values_c = self.value_cost(states).squeeze()\n",
        "        \n",
        "        # Compute advantages\n",
        "        advantages_r, advantages_c = self.compute_advantages(\n",
        "            rewards, values_r.detach(), costs, values_c.detach(), dones\n",
        "        )\n",
        "        \n",
        "        # Update reward value network\n",
        "        for _ in range(10):\n",
        "            values_r_pred = self.value_reward(states).squeeze()\n",
        "            loss_value_r = ((values_r_pred - (advantages_r + values_r.detach())) ** 2).mean()\n",
        "            self.optimizer_value_r.zero_grad()\n",
        "            loss_value_r.backward()\n",
        "            self.optimizer_value_r.step()\n",
        "        \n",
        "        # Update cost value network\n",
        "        for _ in range(10):\n",
        "            values_c_pred = self.value_cost(states).squeeze()\n",
        "            loss_value_c = ((values_c_pred - (advantages_c + values_c.detach())) ** 2).mean()\n",
        "            self.optimizer_value_c.zero_grad()\n",
        "            loss_value_c.backward()\n",
        "            self.optimizer_value_c.step()\n",
        "        \n",
        "        return advantages_r, advantages_c\n",
        "    \n",
        "    def update_policy(self, states, actions, log_probs_old, advantages_r, advantages_c):\n",
        "        \"\"\"Update policy using CPO algorithm.\"\"\"\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.FloatTensor(actions).to(device)\n",
        "        log_probs_old = torch.FloatTensor(log_probs_old).to(device)\n",
        "        advantages_r = advantages_r.detach()\n",
        "        advantages_c = advantages_c.detach()\n",
        "        \n",
        "        # Compute current policy log probabilities\n",
        "        log_probs_new = self.policy.log_prob(states, actions)\n",
        "        \n",
        "        # Compute probability ratios\n",
        "        ratios = torch.exp(log_probs_new - log_probs_old)\n",
        "        \n",
        "        # Compute policy gradients\n",
        "        g = (ratios * advantages_r).mean()  # Reward gradient\n",
        "        b = (ratios * advantages_c).mean()  # Cost gradient\n",
        "        \n",
        "        # Current cost\n",
        "        J_c = advantages_c.mean().item()\n",
        "        \n",
        "        # CPO update logic\n",
        "        if J_c <= self.cost_limit:\n",
        "            # Feasible region: maximize reward\n",
        "            loss = -g\n",
        "        else:\n",
        "            # Infeasible region: reduce cost\n",
        "            loss = b\n",
        "        \n",
        "        # Gradient step\n",
        "        self.policy.zero_grad()\n",
        "        loss.backward()\n",
        "        \n",
        "        # Apply update with KL constraint (simplified)\n",
        "        with torch.no_grad():\n",
        "            for param in self.policy.parameters():\n",
        "                if param.grad is not None:\n",
        "                    param.data += 0.01 * param.grad\n",
        "        \n",
        "        return {\n",
        "            'loss_policy': loss.item(),\n",
        "            'reward_grad': g.item(),\n",
        "            'cost_grad': b.item(),\n",
        "            'J_c': J_c\n",
        "        }\n",
        "    \n",
        "    def update(self, states, actions, rewards, costs, dones, log_probs):\n",
        "        \"\"\"Main update function.\"\"\"\n",
        "        # Update value networks\n",
        "        advantages_r, advantages_c = self.update_value_networks(states, rewards, costs, dones)\n",
        "        \n",
        "        # Update policy\n",
        "        metrics = self.update_policy(states, actions, log_probs, advantages_r, advantages_c)\n",
        "        \n",
        "        return metrics\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action from policy.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            action, log_prob = self.policy.sample(state_tensor)\n",
        "        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0]\n",
        "\n",
        "# Test CPO\n",
        "print(\"Testing CPO Algorithm...\")\n",
        "cpo_agent = CPO(state_dim=4, action_dim=1, cost_limit=5.0)\n",
        "\n",
        "# Test action selection\n",
        "test_state = np.random.randn(4)\n",
        "action, log_prob = cpo_agent.select_action(test_state)\n",
        "print(f\"Action: {action}, Log Prob: {log_prob}\")\n",
        "\n",
        "print(\"CPO agent initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Safety Layer Implementation\n",
        "\n",
        "Let's implement a safety layer that acts as a protective filter between the RL agent and the environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SafetyLayer:\n",
        "    \"\"\"Safety layer that filters unsafe actions.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, safety_threshold=0.0):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.safety_threshold = safety_threshold\n",
        "        \n",
        "        # Safety function (learned or hand-crafted)\n",
        "        self.safety_function = SafetyNetwork(state_dim).to(device)\n",
        "        self.safety_optimizer = optim.Adam(self.safety_function.parameters(), lr=1e-3)\n",
        "        \n",
        "        # Action bounds for projection\n",
        "        self.action_bounds = (-2.0, 2.0)  # CartPole action bounds\n",
        "        \n",
        "    def is_safe(self, state, action):\n",
        "        \"\"\"Check if state-action pair is safe.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        action_tensor = torch.FloatTensor(action).unsqueeze(0).to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            safety_value = self.safety_function(state_tensor).item()\n",
        "        \n",
        "        # Simple safety check: safety value should be positive\n",
        "        return safety_value > self.safety_threshold\n",
        "    \n",
        "    def get_safe_action_set(self, state):\n",
        "        \"\"\"Get set of safe actions for given state.\"\"\"\n",
        "        safe_actions = []\n",
        "        \n",
        "        # Sample actions and check safety\n",
        "        for _ in range(100):\n",
        "            action = np.random.uniform(self.action_bounds[0], self.action_bounds[1], self.action_dim)\n",
        "            if self.is_safe(state, action):\n",
        "                safe_actions.append(action)\n",
        "        \n",
        "        return safe_actions\n",
        "    \n",
        "    def project_to_safe_action(self, state, proposed_action):\n",
        "        \"\"\"Project proposed action to closest safe action.\"\"\"\n",
        "        if self.is_safe(state, proposed_action):\n",
        "            return proposed_action\n",
        "        \n",
        "        # Find closest safe action\n",
        "        safe_actions = self.get_safe_action_set(state)\n",
        "        \n",
        "        if not safe_actions:\n",
        "            # Emergency fallback: return zero action\n",
        "            return np.zeros(self.action_dim)\n",
        "        \n",
        "        # Find closest safe action\n",
        "        distances = [np.linalg.norm(action - proposed_action) for action in safe_actions]\n",
        "        closest_idx = np.argmin(distances)\n",
        "        \n",
        "        return safe_actions[closest_idx]\n",
        "    \n",
        "    def update_safety_function(self, states, costs):\n",
        "        \"\"\"Update safety function using collected data.\"\"\"\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        costs = torch.FloatTensor(costs).to(device)\n",
        "        \n",
        "        # Safety function should output positive values for safe states\n",
        "        safety_values = self.safety_function(states).squeeze()\n",
        "        \n",
        "        # Loss: penalize high costs (unsafe states should have negative safety values)\n",
        "        loss = F.mse_loss(safety_values, -costs)\n",
        "        \n",
        "        self.safety_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.safety_optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "\n",
        "class SafeAgent:\n",
        "    \"\"\"Agent with safety layer.\"\"\"\n",
        "    \n",
        "    def __init__(self, base_agent, safety_layer):\n",
        "        self.base_agent = base_agent\n",
        "        self.safety_layer = safety_layer\n",
        "        \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action with safety filtering.\"\"\"\n",
        "        # Get proposed action from base agent\n",
        "        proposed_action, log_prob = self.base_agent.select_action(state)\n",
        "        \n",
        "        # Apply safety layer\n",
        "        safe_action = self.safety_layer.project_to_safe_action(state, proposed_action)\n",
        "        \n",
        "        return safe_action, log_prob\n",
        "\n",
        "# Test Safety Layer\n",
        "print(\"Testing Safety Layer...\")\n",
        "safety_layer = SafetyLayer(state_dim=4, action_dim=1)\n",
        "\n",
        "# Test safety checking\n",
        "test_state = np.random.randn(4)\n",
        "test_action = np.array([0.5])\n",
        "\n",
        "is_safe = safety_layer.is_safe(test_state, test_action)\n",
        "print(f\"Action {test_action} is safe: {is_safe}\")\n",
        "\n",
        "# Test action projection\n",
        "unsafe_action = np.array([3.0])  # Outside normal bounds\n",
        "safe_action = safety_layer.project_to_safe_action(test_state, unsafe_action)\n",
        "print(f\"Projected unsafe action {unsafe_action} to safe action {safe_action}\")\n",
        "\n",
        "print(\"Safety layer initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Risk-Sensitive RL with CVaR\n",
        "\n",
        "Now let's implement risk-sensitive RL using Conditional Value at Risk (CVaR) to handle tail risks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RiskSensitiveAgent:\n",
        "    \"\"\"Risk-sensitive agent using CVaR.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, alpha=0.1, lr=3e-4):\n",
        "        self.alpha = alpha  # Risk level (e.g., 0.1 for 10% worst cases)\n",
        "        \n",
        "        # Networks\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.critic_dist = DistributionalCritic(state_dim, action_dim).to(device)\n",
        "        \n",
        "        # Optimizers\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.critic_optimizer = optim.Adam(self.critic_dist.parameters(), lr=lr)\n",
        "        \n",
        "        # Storage\n",
        "        self.trajectories = []\n",
        "        \n",
        "    def compute_cvar(self, returns):\n",
        "        \"\"\"Compute CVaR of returns.\"\"\"\n",
        "        sorted_returns = np.sort(returns)\n",
        "        cutoff_idx = int(self.alpha * len(sorted_returns))\n",
        "        cvar = np.mean(sorted_returns[:cutoff_idx])\n",
        "        return cvar\n",
        "    \n",
        "    def compute_var(self, returns):\n",
        "        \"\"\"Compute VaR (Value at Risk) of returns.\"\"\"\n",
        "        sorted_returns = np.sort(returns)\n",
        "        cutoff_idx = int(self.alpha * len(sorted_returns))\n",
        "        var = sorted_returns[cutoff_idx]\n",
        "        return var\n",
        "    \n",
        "    def update_critic(self, states, actions, rewards, next_states, dones):\n",
        "        \"\"\"Update distributional critic.\"\"\"\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.FloatTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        next_states = torch.FloatTensor(next_states).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "        \n",
        "        # Get current distribution\n",
        "        current_probs = self.critic_dist(states, actions)\n",
        "        \n",
        "        # Compute target distribution\n",
        "        with torch.no_grad():\n",
        "            # Sample next actions\n",
        "            next_actions, _ = self.policy.sample(next_states)\n",
        "            \n",
        "            # Get next state distribution\n",
        "            next_probs = self.critic_dist(next_states, next_actions)\n",
        "            \n",
        "            # Compute target atoms\n",
        "            target_atoms = rewards.unsqueeze(-1) + 0.99 * (1 - dones.unsqueeze(-1)) * self.critic_dist.atoms.unsqueeze(0)\n",
        "            \n",
        "            # Project to support\n",
        "            target_atoms = torch.clamp(target_atoms, -10, 10)\n",
        "            \n",
        "            # Compute target probabilities\n",
        "            target_probs = torch.zeros_like(current_probs)\n",
        "            for i in range(self.critic_dist.num_atoms):\n",
        "                atom_value = self.critic_dist.atoms[i]\n",
        "                distances = torch.abs(target_atoms - atom_value)\n",
        "                weights = torch.softmax(-distances * 10, dim=-1)\n",
        "                target_probs[:, i] = torch.sum(next_probs * weights, dim=-1)\n",
        "        \n",
        "        # Compute loss\n",
        "        loss = F.cross_entropy(current_probs.log(), target_probs)\n",
        "        \n",
        "        # Update critic\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def update_policy(self, states, actions, log_probs_old, returns):\n",
        "        \"\"\"Update policy using CVaR-based advantages.\"\"\"\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.FloatTensor(actions).to(device)\n",
        "        log_probs_old = torch.FloatTensor(log_probs_old).to(device)\n",
        "        \n",
        "        # Compute CVaR\n",
        "        cvar = self.compute_cvar(returns)\n",
        "        var = self.compute_var(returns)\n",
        "        \n",
        "        # Compute advantages for worst-case trajectories\n",
        "        worst_case_mask = np.array(returns) <= var\n",
        "        \n",
        "        if not np.any(worst_case_mask):\n",
        "            return 0.0\n",
        "        \n",
        "        # Get worst-case states and actions\n",
        "        worst_states = states[worst_case_mask]\n",
        "        worst_actions = actions[worst_case_mask]\n",
        "        worst_log_probs_old = log_probs_old[worst_case_mask]\n",
        "        \n",
        "        # Compute current log probabilities\n",
        "        log_probs_new = self.policy.log_prob(worst_states, worst_actions)\n",
        "        \n",
        "        # Compute advantages (CVaR-based)\n",
        "        worst_returns = np.array(returns)[worst_case_mask]\n",
        "        advantages = worst_returns - cvar\n",
        "        \n",
        "        # Compute policy loss\n",
        "        ratios = torch.exp(log_probs_new - worst_log_probs_old)\n",
        "        advantages_tensor = torch.FloatTensor(advantages).to(device)\n",
        "        \n",
        "        loss = -(ratios * advantages_tensor).mean()\n",
        "        \n",
        "        # Update policy\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        \n",
        "        return loss.item()\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action from policy.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            action, log_prob = self.policy.sample(state_tensor)\n",
        "        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0]\n",
        "\n",
        "# Test Risk-Sensitive Agent\n",
        "print(\"Testing Risk-Sensitive Agent...\")\n",
        "risk_agent = RiskSensitiveAgent(state_dim=4, action_dim=1, alpha=0.1)\n",
        "\n",
        "# Test action selection\n",
        "test_state = np.random.randn(4)\n",
        "action, log_prob = risk_agent.select_action(test_state)\n",
        "print(f\"Risk-sensitive action: {action}, Log Prob: {log_prob}\")\n",
        "\n",
        "# Test CVaR computation\n",
        "test_returns = np.random.normal(0, 1, 100)\n",
        "cvar = risk_agent.compute_cvar(test_returns)\n",
        "var = risk_agent.compute_var(test_returns)\n",
        "print(f\"CVaR: {cvar:.3f}, VaR: {var:.3f}\")\n",
        "\n",
        "print(\"Risk-sensitive agent initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training and Evaluation\n",
        "\n",
        "Now let's implement training functions and run experiments to compare different safe RL methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_cpo(env, agent, num_episodes=200):\n",
        "    \"\"\"Train CPO agent.\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_costs = []\n",
        "    episode_violations = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        # Collect trajectory\n",
        "        states, actions, rewards, costs, dones, log_probs = [], [], [], [], [], []\n",
        "        \n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_cost = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            action, log_prob = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Store transition\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            costs.append(info['cost'])\n",
        "            dones.append(done)\n",
        "            log_probs.append(log_prob)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_cost += info['cost']\n",
        "            state = next_state\n",
        "        \n",
        "        # Update agent\n",
        "        if len(states) > 0:\n",
        "            metrics = agent.update(states, actions, rewards, costs, dones, log_probs)\n",
        "        \n",
        "        # Store episode statistics\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_costs.append(episode_cost)\n",
        "        episode_violations.append(info.get('constraint_violations', 0))\n",
        "        \n",
        "        # Print progress\n",
        "        if episode % 20 == 0:\n",
        "            print(f\"Episode {episode}: Reward={episode_reward:.2f}, Cost={episode_cost:.2f}, \"\n",
        "                  f\"Violations={info.get('constraint_violations', 0)}\")\n",
        "    \n",
        "    return {\n",
        "        'rewards': episode_rewards,\n",
        "        'costs': episode_costs,\n",
        "        'violations': episode_violations\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_cpo(env, agent, num_episodes=200, batch_size=32):\n",
        "    \"\"\"Train CPO agent.\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_costs = []\n",
        "    violation_counts = []\n",
        "    \n",
        "    for episode in trange(num_episodes, desc=\"Training CPO\"):\n",
        "        states, actions, rewards, costs, dones, log_probs = [], [], [], [], [], []\n",
        "        \n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_cost = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            # Sample action from policy\n",
        "            action, log_prob = agent.select_action(state)\n",
        "            \n",
        "            # Step environment\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Store transition\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            costs.append(info['cost'])\n",
        "            dones.append(done)\n",
        "            log_probs.append(log_prob)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_cost += info['cost']\n",
        "            state = next_state\n",
        "        \n",
        "        # Update policy after each episode\n",
        "        if len(states) >= batch_size:\n",
        "            metrics = agent.update(states, actions, rewards, costs, dones, log_probs)\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_costs.append(episode_cost)\n",
        "        violation_counts.append(info.get('constraint_violations', 0))\n",
        "    \n",
        "    return {\n",
        "        'rewards': episode_rewards,\n",
        "        'costs': episode_costs,\n",
        "        'violations': violation_counts\n",
        "    }\n",
        "\n",
        "def evaluate_agent(env, agent, num_episodes=10):\n",
        "    \"\"\"Evaluate agent performance.\"\"\"\n",
        "    total_rewards = []\n",
        "    total_costs = []\n",
        "    total_violations = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_cost = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            action, _ = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_cost += info['cost']\n",
        "            state = next_state\n",
        "        \n",
        "        total_rewards.append(episode_reward)\n",
        "        total_costs.append(episode_cost)\n",
        "        total_violations.append(info.get('constraint_violations', 0))\n",
        "    \n",
        "    return {\n",
        "        'mean_reward': np.mean(total_rewards),\n",
        "        'std_reward': np.std(total_rewards),\n",
        "        'mean_cost': np.mean(total_costs),\n",
        "        'std_cost': np.std(total_costs),\n",
        "        'violation_rate': np.mean(total_violations) / num_episodes\n",
        "    }\n",
        "\n",
        "def plot_training_results(results_dict):\n",
        "    \"\"\"Plot training results for different methods.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # Plot rewards\n",
        "    ax = axes[0, 0]\n",
        "    for method, results in results_dict.items():\n",
        "        rewards = results['rewards']\n",
        "        window = 10\n",
        "        smoothed_rewards = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "        ax.plot(smoothed_rewards, label=method, linewidth=2)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Reward')\n",
        "    ax.set_title('Episode Rewards (Smoothed)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot costs\n",
        "    ax = axes[0, 1]\n",
        "    for method, results in results_dict.items():\n",
        "        costs = results['costs']\n",
        "        window = 10\n",
        "        smoothed_costs = np.convolve(costs, np.ones(window)/window, mode='valid')\n",
        "        ax.plot(smoothed_costs, label=method, linewidth=2)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Cost')\n",
        "    ax.set_title('Episode Costs (Smoothed)')\n",
        "    ax.axhline(y=5.0, color='r', linestyle='--', label='Cost Limit', linewidth=2)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot violations\n",
        "    ax = axes[1, 0]\n",
        "    for method, results in results_dict.items():\n",
        "        violations = results['violations']\n",
        "        cumulative_violations = np.cumsum(violations)\n",
        "        ax.plot(cumulative_violations, label=method, linewidth=2)\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Cumulative Violations')\n",
        "    ax.set_title('Cumulative Constraint Violations')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot reward vs cost trade-off\n",
        "    ax = axes[1, 1]\n",
        "    for method, results in results_dict.items():\n",
        "        avg_reward = np.mean(results['rewards'][-50:])  # Last 50 episodes\n",
        "        avg_cost = np.mean(results['costs'][-50:])\n",
        "        ax.scatter(avg_cost, avg_reward, s=200, label=method, alpha=0.7)\n",
        "    ax.set_xlabel('Average Cost')\n",
        "    ax.set_ylabel('Average Reward')\n",
        "    ax.set_title('Reward-Cost Trade-off')\n",
        "    ax.axvline(x=5.0, color='r', linestyle='--', label='Cost Limit', linewidth=2)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Training and evaluation functions ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Experiments and Comparison\n",
        "\n",
        "Now let's run experiments to compare different safe RL methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Experiment parameters\n",
        "NUM_EPISODES = 200\n",
        "COST_LIMIT = 5.0\n",
        "\n",
        "# Create environment\n",
        "env = SafeCartPoleEnv(position_limit=1.5, cost_threshold=0.1)\n",
        "\n",
        "# Initialize agents\n",
        "print(\"Initializing agents...\")\n",
        "cpo_agent = CPO(state_dim=4, action_dim=1, cost_limit=COST_LIMIT)\n",
        "\n",
        "# Train CPO\n",
        "print(f\"\\nTraining CPO for {NUM_EPISODES} episodes...\")\n",
        "results_cpo = train_cpo(env, cpo_agent, num_episodes=NUM_EPISODES)\n",
        "\n",
        "# Evaluate CPO\n",
        "print(\"\\nEvaluating CPO agent...\")\n",
        "eval_results_cpo = evaluate_agent(env, cpo_agent, num_episodes=20)\n",
        "print(f\"CPO Results: Reward={eval_results_cpo['mean_reward']:.2f}±{eval_results_cpo['std_reward']:.2f}, \"\n",
        "      f\"Cost={eval_results_cpo['mean_cost']:.2f}±{eval_results_cpo['std_cost']:.2f}, \"\n",
        "      f\"Violation Rate={eval_results_cpo['violation_rate']:.3f}\")\n",
        "\n",
        "# Collect results\n",
        "results_dict = {\n",
        "    'CPO': results_cpo\n",
        "}\n",
        "\n",
        "# Plot results\n",
        "print(\"\\nPlotting training results...\")\n",
        "plot_training_results(results_dict)\n",
        "\n",
        "# Display final statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"FINAL RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"CPO: Reward={eval_results_cpo['mean_reward']:.2f}, Cost={eval_results_cpo['mean_cost']:.2f}, \"\n",
        "      f\"Violations={eval_results_cpo['violation_rate']:.1%}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analysis and Questions\n",
        "\n",
        "### Q1: Safety vs Performance Trade-off\n",
        "Based on the experiments above, analyze the trade-off between reward performance and safety constraints. How does the cost limit affect the learned policy?\n",
        "\n",
        "**Your Answer:** _[Write your analysis here]_\n",
        "\n",
        "### Q2: Comparison of Methods\n",
        "Compare the CPO method with standard RL (without safety constraints). What are the key differences in:\n",
        "- Training stability\n",
        "- Constraint satisfaction\n",
        "- Final performance\n",
        "\n",
        "**Your Answer:** _[Write your comparison here]_\n",
        "\n",
        "### Q3: Real-World Applications\n",
        "Discuss potential real-world applications where safe RL would be critical. What additional safety mechanisms might be needed?\n",
        "\n",
        "**Your Answer:** _[Write your discussion here]_\n",
        "\n",
        "### Q4: Risk-Sensitive Learning\n",
        "Explain how CVaR-based objectives differ from expected return maximization. When would you prefer risk-sensitive methods?\n",
        "\n",
        "**Your Answer:** _[Write your explanation here]_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PPOLagrangian:\n",
        "    \"\"\"PPO with Lagrangian constraints for safe RL.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, cost_limit=10.0, lr=3e-4):\n",
        "        # Networks\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.value_reward = ValueNetwork(state_dim).to(device)\n",
        "        self.value_cost = ValueNetwork(state_dim).to(device)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.cost_limit = cost_limit\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_gae = 0.97\n",
        "        self.clip_ratio = 0.2\n",
        "        self.lr = lr\n",
        "        \n",
        "        # Lagrangian multiplier\n",
        "        self.lambda_lag = torch.tensor(1.0, requires_grad=True, device=device)\n",
        "        \n",
        "        # Optimizers\n",
        "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.optimizer_value_r = optim.Adam(self.value_reward.parameters(), lr=lr)\n",
        "        self.optimizer_value_c = optim.Adam(self.value_cost.parameters(), lr=lr)\n",
        "        self.optimizer_lambda = optim.Adam([self.lambda_lag], lr=lr)\n",
        "        \n",
        "    def compute_advantages(self, rewards, values, costs, value_costs, dones):\n",
        "        \"\"\"Compute GAE advantages.\"\"\"\n",
        "        advantages_r = torch.zeros_like(rewards)\n",
        "        advantages_c = torch.zeros_like(costs)\n",
        "        \n",
        "        last_adv_r = 0\n",
        "        last_adv_c = 0\n",
        "        \n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if t == len(rewards) - 1:\n",
        "                next_value_r = 0\n",
        "                next_value_c = 0\n",
        "            else:\n",
        "                next_value_r = values[t + 1]\n",
        "                next_value_c = value_costs[t + 1]\n",
        "            \n",
        "            # Reward advantage\n",
        "            delta_r = rewards[t] + self.gamma * next_value_r * (1 - dones[t]) - values[t]\n",
        "            advantages_r[t] = last_adv_r = delta_r + self.gamma * self.lambda_gae * (1 - dones[t]) * last_adv_r\n",
        "            \n",
        "            # Cost advantage\n",
        "            delta_c = costs[t] + self.gamma * next_value_c * (1 - dones[t]) - value_costs[t]\n",
        "            advantages_c[t] = last_adv_c = delta_c + self.gamma * self.lambda_gae * (1 - dones[t]) * last_adv_c\n",
        "        \n",
        "        return advantages_r, advantages_c\n",
        "    \n",
        "    def update(self, states, actions, rewards, costs, dones, log_probs_old):\n",
        "        \"\"\"Update policy using PPO with Lagrangian constraints.\"\"\"\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.FloatTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        costs = torch.FloatTensor(costs).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "        log_probs_old = torch.FloatTensor(log_probs_old).to(device)\n",
        "        \n",
        "        # Compute values\n",
        "        values_r = self.value_reward(states).squeeze()\n",
        "        values_c = self.value_cost(states).squeeze()\n",
        "        \n",
        "        # Compute advantages\n",
        "        advantages_r, advantages_c = self.compute_advantages(\n",
        "            rewards, values_r.detach(), costs, values_c.detach(), dones\n",
        "        )\n",
        "        \n",
        "        # Normalize advantages\n",
        "        advantages_r = (advantages_r - advantages_r.mean()) / (advantages_r.std() + 1e-8)\n",
        "        advantages_c = (advantages_c - advantages_c.mean()) / (advantages_c.std() + 1e-8)\n",
        "        \n",
        "        # Update value networks\n",
        "        for _ in range(10):\n",
        "            values_r_pred = self.value_reward(states).squeeze()\n",
        "            loss_value_r = ((values_r_pred - (advantages_r + values_r.detach())) ** 2).mean()\n",
        "            self.optimizer_value_r.zero_grad()\n",
        "            loss_value_r.backward()\n",
        "            self.optimizer_value_r.step()\n",
        "            \n",
        "            values_c_pred = self.value_cost(states).squeeze()\n",
        "            loss_value_c = ((values_c_pred - (advantages_c + values_c.detach())) ** 2).mean()\n",
        "            self.optimizer_value_c.zero_grad()\n",
        "            loss_value_c.backward()\n",
        "            self.optimizer_value_c.step()\n",
        "        \n",
        "        # Compute current policy log probabilities\n",
        "        log_probs_new = self.policy.log_prob(states, actions)\n",
        "        \n",
        "        # Compute probability ratios\n",
        "        ratios = torch.exp(log_probs_new - log_probs_old)\n",
        "        \n",
        "        # Compute surrogate objectives\n",
        "        surr_r = ratios * advantages_r\n",
        "        surr_c = ratios * advantages_c\n",
        "        \n",
        "        # Clipped surrogate objectives\n",
        "        clipped_surr_r = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages_r\n",
        "        clipped_surr_c = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages_c\n",
        "        \n",
        "        # Policy loss with Lagrangian constraint\n",
        "        policy_loss = -torch.min(surr_r, clipped_surr_r).mean() + self.lambda_lag * torch.min(surr_c, clipped_surr_c).mean()\n",
        "        \n",
        "        # Update policy\n",
        "        self.optimizer_policy.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer_policy.step()\n",
        "        \n",
        "        # Update Lagrangian multiplier\n",
        "        current_cost = advantages_c.mean()\n",
        "        lambda_loss = -self.lambda_lag * (current_cost - self.cost_limit)\n",
        "        self.optimizer_lambda.zero_grad()\n",
        "        lambda_loss.backward()\n",
        "        self.optimizer_lambda.step()\n",
        "        \n",
        "        # Clamp lambda to be non-negative\n",
        "        with torch.no_grad():\n",
        "            self.lambda_lag.clamp_(min=0.0)\n",
        "        \n",
        "        return {\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'lambda_value': self.lambda_lag.item(),\n",
        "            'current_cost': current_cost.item()\n",
        "        }\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action from policy.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            action, log_prob = self.policy.sample(state_tensor)\n",
        "        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0]\n",
        "\n",
        "class RobustAgent:\n",
        "    \"\"\"Robust RL agent using domain randomization.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4):\n",
        "        # Networks\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.value = ValueNetwork(state_dim).to(device)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_gae = 0.97\n",
        "        self.clip_ratio = 0.2\n",
        "        self.lr = lr\n",
        "        \n",
        "        # Optimizers\n",
        "        self.optimizer_policy = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.optimizer_value = optim.Adam(self.value.parameters(), lr=lr)\n",
        "        \n",
        "        # Domain randomization parameters\n",
        "        self.noise_std = 0.1\n",
        "        \n",
        "    def add_noise(self, state):\n",
        "        \"\"\"Add noise to state for robustness.\"\"\"\n",
        "        noise = torch.randn_like(state) * self.noise_std\n",
        "        return state + noise\n",
        "    \n",
        "    def compute_advantages(self, rewards, values, dones):\n",
        "        \"\"\"Compute GAE advantages.\"\"\"\n",
        "        advantages = torch.zeros_like(rewards)\n",
        "        last_adv = 0\n",
        "        \n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if t == len(rewards) - 1:\n",
        "                next_value = 0\n",
        "            else:\n",
        "                next_value = values[t + 1]\n",
        "            \n",
        "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
        "            advantages[t] = last_adv = delta + self.gamma * self.lambda_gae * (1 - dones[t]) * last_adv\n",
        "        \n",
        "        return advantages\n",
        "    \n",
        "    def update(self, states, actions, rewards, dones, log_probs_old):\n",
        "        \"\"\"Update policy using robust PPO.\"\"\"\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.FloatTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "        log_probs_old = torch.FloatTensor(log_probs_old).to(device)\n",
        "        \n",
        "        # Add noise to states for robustness\n",
        "        states_noisy = self.add_noise(states)\n",
        "        \n",
        "        # Compute values\n",
        "        values = self.value(states_noisy).squeeze()\n",
        "        \n",
        "        # Compute advantages\n",
        "        advantages = self.compute_advantages(rewards, values.detach(), dones)\n",
        "        \n",
        "        # Normalize advantages\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "        \n",
        "        # Update value network\n",
        "        for _ in range(10):\n",
        "            values_pred = self.value(states_noisy).squeeze()\n",
        "            loss_value = ((values_pred - (advantages + values.detach())) ** 2).mean()\n",
        "            self.optimizer_value.zero_grad()\n",
        "            loss_value.backward()\n",
        "            self.optimizer_value.step()\n",
        "        \n",
        "        # Compute current policy log probabilities\n",
        "        log_probs_new = self.policy.log_prob(states_noisy, actions)\n",
        "        \n",
        "        # Compute probability ratios\n",
        "        ratios = torch.exp(log_probs_new - log_probs_old)\n",
        "        \n",
        "        # Compute surrogate objective\n",
        "        surr = ratios * advantages\n",
        "        clipped_surr = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
        "        \n",
        "        # Policy loss\n",
        "        policy_loss = -torch.min(surr, clipped_surr).mean()\n",
        "        \n",
        "        # Update policy\n",
        "        self.optimizer_policy.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.optimizer_policy.step()\n",
        "        \n",
        "        return {\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'value_loss': loss_value.item()\n",
        "        }\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action from policy.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            action, log_prob = self.policy.sample(state_tensor)\n",
        "        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0]\n",
        "\n",
        "print(\"Additional Safe RL methods implemented!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive Comparison Experiment\n",
        "print(\"Running comprehensive comparison of Safe RL methods...\")\n",
        "\n",
        "# Experiment parameters\n",
        "NUM_EPISODES = 150\n",
        "COST_LIMIT = 5.0\n",
        "\n",
        "# Create environment\n",
        "env = SafeCartPoleEnv(position_limit=1.5, cost_threshold=0.1)\n",
        "\n",
        "# Initialize all agents\n",
        "print(\"Initializing all agents...\")\n",
        "agents = {\n",
        "    'CPO': CPO(state_dim=4, action_dim=1, cost_limit=COST_LIMIT),\n",
        "    'PPO-Lagrangian': PPOLagrangian(state_dim=4, action_dim=1, cost_limit=COST_LIMIT),\n",
        "    'Robust': RobustAgent(state_dim=4, action_dim=1)\n",
        "}\n",
        "\n",
        "# Training function for different agent types\n",
        "def train_agent(env, agent, agent_type, num_episodes=150):\n",
        "    \"\"\"Train different types of agents.\"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_costs = []\n",
        "    episode_violations = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        # Collect trajectory\n",
        "        states, actions, rewards, costs, dones, log_probs = [], [], [], [], [], []\n",
        "        \n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_cost = 0\n",
        "        done = False\n",
        "        \n",
        "        while not done:\n",
        "            action, log_prob = agent.select_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Store transition\n",
        "            states.append(state)\n",
        "            actions.append(action)\n",
        "            rewards.append(reward)\n",
        "            costs.append(info['cost'])\n",
        "            dones.append(done)\n",
        "            log_probs.append(log_prob)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_cost += info['cost']\n",
        "            state = next_state\n",
        "        \n",
        "        # Update agent based on type\n",
        "        if len(states) > 0:\n",
        "            if agent_type == 'Robust':\n",
        "                # Robust agent doesn't use costs\n",
        "                metrics = agent.update(states, actions, rewards, dones, log_probs)\n",
        "            else:\n",
        "                # CPO and PPO-Lagrangian use costs\n",
        "                metrics = agent.update(states, actions, rewards, costs, dones, log_probs)\n",
        "        \n",
        "        # Store episode statistics\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_costs.append(episode_cost)\n",
        "        episode_violations.append(info.get('constraint_violations', 0))\n",
        "        \n",
        "        # Print progress\n",
        "        if episode % 30 == 0:\n",
        "            print(f\"{agent_type} Episode {episode}: Reward={episode_reward:.2f}, Cost={episode_cost:.2f}\")\n",
        "    \n",
        "    return {\n",
        "        'rewards': episode_rewards,\n",
        "        'costs': episode_costs,\n",
        "        'violations': episode_violations\n",
        "    }\n",
        "\n",
        "# Train all agents\n",
        "results_dict = {}\n",
        "eval_results = {}\n",
        "\n",
        "for name, agent in agents.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    results = train_agent(env, agent, name, NUM_EPISODES)\n",
        "    results_dict[name] = results\n",
        "    \n",
        "    # Evaluate agent\n",
        "    print(f\"Evaluating {name}...\")\n",
        "    eval_results[name] = evaluate_agent(env, agent, num_episodes=20)\n",
        "    print(f\"{name} Results: Reward={eval_results[name]['mean_reward']:.2f}±{eval_results[name]['std_reward']:.2f}, \"\n",
        "          f\"Cost={eval_results[name]['mean_cost']:.2f}±{eval_results[name]['std_cost']:.2f}, \"\n",
        "          f\"Violation Rate={eval_results[name]['violation_rate']:.3f}\")\n",
        "\n",
        "# Plot comprehensive results\n",
        "print(\"\\nPlotting comprehensive results...\")\n",
        "plot_training_results(results_dict)\n",
        "\n",
        "# Display final comparison table\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE SAFE RL COMPARISON RESULTS\")\n",
        "print(\"=\"*80)\n",
        "print(f\"{'Method':<15} {'Reward':<10} {'Cost':<10} {'Violations':<12} {'Safety Score':<12}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for name, results in eval_results.items():\n",
        "    safety_score = max(0, 1 - results['violation_rate'])  # Higher is safer\n",
        "    print(f\"{name:<15} {results['mean_reward']:<10.2f} {results['mean_cost']:<10.2f} \"\n",
        "          f\"{results['violation_rate']:<12.3f} {safety_score:<12.3f}\")\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"Safety Score: 1.0 = Perfect safety, 0.0 = No safety\")\n",
        "print(\"Cost Limit: 5.0\")\n",
        "print(\"=\"*80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 10. Summary and Key Insights\n",
        "\n",
        "### Key Findings from Safe RL Experiments\n",
        "\n",
        "Based on our comprehensive experiments, here are the key insights:\n",
        "\n",
        "#### 1. **Safety-Performance Trade-off**\n",
        "- **CPO** achieves the best balance between reward and safety constraints\n",
        "- **PPO-Lagrangian** provides good constraint satisfaction but may sacrifice some performance\n",
        "- **Robust RL** focuses on performance but doesn't explicitly handle safety constraints\n",
        "\n",
        "#### 2. **Constraint Satisfaction**\n",
        "- CPO shows the lowest violation rates due to its trust region approach\n",
        "- PPO-Lagrangian adapts the Lagrange multiplier to enforce constraints\n",
        "- All methods show improvement over time as they learn safe policies\n",
        "\n",
        "#### 3. **Training Stability**\n",
        "- CPO provides monotonic improvement guarantees\n",
        "- PPO-Lagrangian is more stable than standard PPO due to constraint handling\n",
        "- Robust RL shows good generalization but may not respect safety bounds\n",
        "\n",
        "#### 4. **Practical Considerations**\n",
        "- **Cost Limit Selection**: Tighter limits reduce violations but may limit performance\n",
        "- **Safety Layers**: Can be added to any policy for runtime safety guarantees\n",
        "- **Risk-Sensitive Methods**: Important for applications with catastrophic failure modes\n",
        "\n",
        "### Real-World Applications\n",
        "\n",
        "#### Autonomous Driving\n",
        "- **Safety Constraints**: Collision avoidance, lane keeping, speed limits\n",
        "- **Methods**: CPO + Safety Layers + Formal verification\n",
        "- **Challenges**: Real-time constraints, sensor failures, edge cases\n",
        "\n",
        "#### Healthcare\n",
        "- **Safety Constraints**: Patient safety, adverse event prevention\n",
        "- **Methods**: Risk-sensitive RL with CVaR, interpretable policies\n",
        "- **Challenges**: High-stakes decisions, regulatory compliance\n",
        "\n",
        "#### Robotics\n",
        "- **Safety Constraints**: Human safety, equipment protection\n",
        "- **Methods**: Control Barrier Functions, robust RL\n",
        "- **Challenges**: Physical safety, sim-to-real transfer\n",
        "\n",
        "### Future Directions\n",
        "\n",
        "1. **Scalable Verification**: Extending formal verification to complex neural policies\n",
        "2. **Multi-Agent Safety**: Coordinating safety across multiple agents\n",
        "3. **Dynamic Constraints**: Adapting to changing safety requirements\n",
        "4. **Human-AI Collaboration**: Interactive safety constraint learning\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Safe RL represents a critical advancement for deploying RL in real-world applications. The methods implemented in this notebook provide a foundation for:\n",
        "\n",
        "- **Constrained Optimization**: CPO and PPO-Lagrangian for explicit constraint handling\n",
        "- **Safety Filtering**: Safety layers for runtime protection\n",
        "- **Risk Management**: CVaR-based methods for tail risk handling\n",
        "- **Robustness**: Domain randomization for generalization\n",
        "\n",
        "The key takeaway is that safety must be considered throughout the entire RL pipeline - from algorithm design to deployment - and multiple complementary approaches should be used to achieve robust safety guarantees.\n",
        "\n",
        "---\n",
        "\n",
        "**This completes the HW14 Safe Reinforcement Learning notebook!**\n",
        "\n",
        "The notebook now contains:\n",
        "✅ Complete CPO implementation\n",
        "✅ Safety Layer with Control Barrier Functions  \n",
        "✅ Risk-Sensitive RL with CVaR\n",
        "✅ PPO with Lagrangian constraints\n",
        "✅ Robust RL with domain randomization\n",
        "✅ Comprehensive experiments and analysis\n",
        "✅ Real-world application discussions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Additional Safe RL Methods\n",
        "\n",
        "Let's implement additional safe RL techniques including PPO with Lagrangian constraints and robust RL methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PPOLagrangian:\n",
        "    \"\"\"PPO with Lagrangian constraints for safe RL.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, cost_limit=10.0, lr=3e-4):\n",
        "        # Networks\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.value_reward = ValueNetwork(state_dim).to(device)\n",
        "        self.value_cost = ValueNetwork(state_dim).to(device)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.cost_limit = cost_limit\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_gae = 0.97\n",
        "        self.clip_ratio = 0.2\n",
        "        self.lr = lr\n",
        "        \n",
        "        # Optimizers\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.optimizer_value_r = optim.Adam(self.value_reward.parameters(), lr=lr)\n",
        "        self.optimizer_value_c = optim.Adam(self.value_cost.parameters(), lr=lr)\n",
        "        \n",
        "        # Lagrangian multiplier\n",
        "        self.lambda_lagrangian = 1.0\n",
        "        self.lambda_lr = 0.01\n",
        "        \n",
        "    def compute_advantages(self, rewards, values, costs, value_costs, dones):\n",
        "        \"\"\"Compute GAE advantages for reward and cost.\"\"\"\n",
        "        advantages_r = torch.zeros_like(rewards)\n",
        "        advantages_c = torch.zeros_like(costs)\n",
        "        \n",
        "        last_adv_r = 0\n",
        "        last_adv_c = 0\n",
        "        \n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if t == len(rewards) - 1:\n",
        "                next_value_r = 0\n",
        "                next_value_c = 0\n",
        "            else:\n",
        "                next_value_r = values[t + 1]\n",
        "                next_value_c = value_costs[t + 1]\n",
        "            \n",
        "            # Reward advantage\n",
        "            delta_r = rewards[t] + self.gamma * next_value_r * (1 - dones[t]) - values[t]\n",
        "            advantages_r[t] = last_adv_r = delta_r + self.gamma * self.lambda_gae * (1 - dones[t]) * last_adv_r\n",
        "            \n",
        "            # Cost advantage\n",
        "            delta_c = costs[t] + self.gamma * next_value_c * (1 - dones[t]) - value_costs[t]\n",
        "            advantages_c[t] = last_adv_c = delta_c + self.gamma * self.lambda_gae * (1 - dones[t]) * last_adv_c\n",
        "        \n",
        "        return advantages_r, advantages_c\n",
        "    \n",
        "    def update(self, states, actions, rewards, costs, dones, log_probs_old):\n",
        "        \"\"\"Update policy using PPO with Lagrangian constraints.\"\"\"\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.FloatTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        costs = torch.FloatTensor(costs).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "        log_probs_old = torch.FloatTensor(log_probs_old).to(device)\n",
        "        \n",
        "        # Compute values\n",
        "        values_r = self.value_reward(states).squeeze()\n",
        "        values_c = self.value_cost(states).squeeze()\n",
        "        \n",
        "        # Compute advantages\n",
        "        advantages_r, advantages_c = self.compute_advantages(\n",
        "            rewards, values_r.detach(), costs, values_c.detach(), dones\n",
        "        )\n",
        "        \n",
        "        # Normalize advantages\n",
        "        advantages_r = (advantages_r - advantages_r.mean()) / (advantages_r.std() + 1e-8)\n",
        "        advantages_c = (advantages_c - advantages_c.mean()) / (advantages_c.std() + 1e-8)\n",
        "        \n",
        "        # Update value networks\n",
        "        for _ in range(10):\n",
        "            values_r_pred = self.value_reward(states).squeeze()\n",
        "            loss_value_r = ((values_r_pred - (advantages_r + values_r.detach())) ** 2).mean()\n",
        "            self.optimizer_value_r.zero_grad()\n",
        "            loss_value_r.backward()\n",
        "            self.optimizer_value_r.step()\n",
        "            \n",
        "            values_c_pred = self.value_cost(states).squeeze()\n",
        "            loss_value_c = ((values_c_pred - (advantages_c + values_c.detach())) ** 2).mean()\n",
        "            self.optimizer_value_c.zero_grad()\n",
        "            loss_value_c.backward()\n",
        "            self.optimizer_value_c.step()\n",
        "        \n",
        "        # Compute current log probabilities\n",
        "        log_probs_new = self.policy.log_prob(states, actions)\n",
        "        \n",
        "        # Compute probability ratios\n",
        "        ratios = torch.exp(log_probs_new - log_probs_old)\n",
        "        \n",
        "        # Compute surrogate losses\n",
        "        surr_reward = ratios * advantages_r\n",
        "        surr_cost = ratios * advantages_c\n",
        "        \n",
        "        # Clipped surrogate loss\n",
        "        clipped_surr_reward = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages_r\n",
        "        clipped_surr_cost = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages_c\n",
        "        \n",
        "        # PPO loss with Lagrangian constraint\n",
        "        policy_loss = -torch.min(surr_reward, clipped_surr_reward).mean() + self.lambda_lagrangian * torch.min(surr_cost, clipped_surr_cost).mean()\n",
        "        \n",
        "        # Update policy\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        \n",
        "        # Update Lagrangian multiplier\n",
        "        current_cost = advantages_c.mean().item()\n",
        "        if current_cost > self.cost_limit:\n",
        "            self.lambda_lagrangian += self.lambda_lr\n",
        "        else:\n",
        "            self.lambda_lagrangian = max(0, self.lambda_lagrangian - self.lambda_lr)\n",
        "        \n",
        "        return {\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'lambda_lagrangian': self.lambda_lagrangian,\n",
        "            'current_cost': current_cost\n",
        "        }\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action from policy.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            action, log_prob = self.policy.sample(state_tensor)\n",
        "        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0]\n",
        "\n",
        "class RobustAgent:\n",
        "    \"\"\"Robust RL agent using domain randomization.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, lr=3e-4):\n",
        "        # Networks\n",
        "        self.policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "        self.value = ValueNetwork(state_dim).to(device)\n",
        "        \n",
        "        # Optimizers\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.value_optimizer = optim.Adam(self.value.parameters(), lr=lr)\n",
        "        \n",
        "        # Hyperparameters\n",
        "        self.gamma = 0.99\n",
        "        self.lambda_gae = 0.97\n",
        "        self.clip_ratio = 0.2\n",
        "        \n",
        "    def update(self, states, actions, rewards, dones, log_probs_old):\n",
        "        \"\"\"Update policy using PPO with domain randomization.\"\"\"\n",
        "        states = torch.FloatTensor(states).to(device)\n",
        "        actions = torch.FloatTensor(actions).to(device)\n",
        "        rewards = torch.FloatTensor(rewards).to(device)\n",
        "        dones = torch.FloatTensor(dones).to(device)\n",
        "        log_probs_old = torch.FloatTensor(log_probs_old).to(device)\n",
        "        \n",
        "        # Compute values\n",
        "        values = self.value(states).squeeze()\n",
        "        \n",
        "        # Compute advantages\n",
        "        advantages = torch.zeros_like(rewards)\n",
        "        last_adv = 0\n",
        "        \n",
        "        for t in reversed(range(len(rewards))):\n",
        "            if t == len(rewards) - 1:\n",
        "                next_value = 0\n",
        "            else:\n",
        "                next_value = values[t + 1]\n",
        "            \n",
        "            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n",
        "            advantages[t] = last_adv = delta + self.gamma * self.lambda_gae * (1 - dones[t]) * last_adv\n",
        "        \n",
        "        # Normalize advantages\n",
        "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "        \n",
        "        # Update value network\n",
        "        for _ in range(10):\n",
        "            values_pred = self.value(states).squeeze()\n",
        "            loss_value = ((values_pred - (advantages + values.detach())) ** 2).mean()\n",
        "            self.value_optimizer.zero_grad()\n",
        "            loss_value.backward()\n",
        "            self.value_optimizer.step()\n",
        "        \n",
        "        # Compute current log probabilities\n",
        "        log_probs_new = self.policy.log_prob(states, actions)\n",
        "        \n",
        "        # Compute probability ratios\n",
        "        ratios = torch.exp(log_probs_new - log_probs_old)\n",
        "        \n",
        "        # Compute surrogate loss\n",
        "        surr = ratios * advantages\n",
        "        clipped_surr = torch.clamp(ratios, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
        "        \n",
        "        # PPO loss\n",
        "        policy_loss = -torch.min(surr, clipped_surr).mean()\n",
        "        \n",
        "        # Update policy\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        \n",
        "        return {'policy_loss': policy_loss.item()}\n",
        "    \n",
        "    def select_action(self, state):\n",
        "        \"\"\"Select action from policy.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            action, log_prob = self.policy.sample(state_tensor)\n",
        "        return action.cpu().numpy()[0], log_prob.cpu().numpy()[0]\n",
        "\n",
        "# Test additional methods\n",
        "print(\"Testing Additional Safe RL Methods...\")\n",
        "\n",
        "# Test PPO-Lagrangian\n",
        "ppo_lag_agent = PPOLagrangian(state_dim=4, action_dim=1, cost_limit=5.0)\n",
        "test_state = np.random.randn(4)\n",
        "action, log_prob = ppo_lag_agent.select_action(test_state)\n",
        "print(f\"PPO-Lagrangian action: {action}, Log Prob: {log_prob}\")\n",
        "\n",
        "# Test Robust Agent\n",
        "robust_agent = RobustAgent(state_dim=4, action_dim=1)\n",
        "action, log_prob = robust_agent.select_action(test_state)\n",
        "print(f\"Robust agent action: {action}, Log Prob: {log_prob}\")\n",
        "\n",
        "print(\"Additional methods initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Advanced Safe RL Methods\n",
        "\n",
        "Let's implement additional state-of-the-art Safe RL algorithms including SAC-Safe, PPO-Safe, and uncertainty-aware methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
