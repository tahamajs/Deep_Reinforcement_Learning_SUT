{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW14: Safe Reinforcement Learning\n",
        "\n",
        "> - Full Name: **[Your Full Name]**\n",
        "> - Student ID: **[Your Student ID]**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepRLCourse/Homework-14-Questions/blob/main/HW14_Notebook.ipynb)\n",
        "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/DeepRLCourse/Homework-14-Questions/main/HW14_Notebook.ipynb)\n",
        "\n",
        "## Overview\n",
        "This assignment focuses on **Safe Reinforcement Learning**, exploring methods to train agents that not only maximize rewards but also satisfy safety constraints during both training and deployment. We'll implement and experiment with:\n",
        "\n",
        "1. **Constrained Policy Optimization (CPO)**\n",
        "2. **Safety Layers and Shielding**\n",
        "3. **Risk-Sensitive RL (CVaR)**\n",
        "4. **Safe Exploration Techniques**\n",
        "5. **Robust RL Methods**\n",
        "\n",
        "The goal is to understand the fundamental trade-offs between performance and safety in RL systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Imports and Setup\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import trange\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Safe Environment Setup\n",
        "\n",
        "First, let's create a safe version of the CartPole environment where the agent must balance the pole while keeping the cart position within safe bounds.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SafeCartPoleEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Safe CartPole environment with position constraints.\n",
        "    The agent must balance the pole while keeping cart position within safe bounds.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, position_limit=1.5, cost_threshold=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Create base CartPole environment\n",
        "        self.env = gym.make('CartPole-v1')\n",
        "        self.observation_space = self.env.observation_space\n",
        "        self.action_space = self.env.action_space\n",
        "        \n",
        "        # Safety parameters\n",
        "        self.position_limit = position_limit\n",
        "        self.cost_threshold = cost_threshold\n",
        "        \n",
        "        # Track episode statistics\n",
        "        self.episode_cost = 0\n",
        "        self.episode_reward = 0\n",
        "        self.constraint_violations = 0\n",
        "        \n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset environment and return initial observation.\"\"\"\n",
        "        obs, info = self.env.reset(**kwargs)\n",
        "        self.episode_cost = 0\n",
        "        self.episode_reward = 0\n",
        "        self.constraint_violations = 0\n",
        "        return obs, info\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action and return next state, reward, done, truncated, info.\"\"\"\n",
        "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
        "        \n",
        "        # Extract cart position (first element of observation)\n",
        "        cart_position = obs[0]\n",
        "        \n",
        "        # Compute cost based on position constraint violation\n",
        "        position_violation = max(0, abs(cart_position) - self.position_limit)\n",
        "        cost = position_violation\n",
        "        \n",
        "        # Update episode statistics\n",
        "        self.episode_cost += cost\n",
        "        self.episode_reward += reward\n",
        "        \n",
        "        # Check for constraint violation\n",
        "        if cost > self.cost_threshold:\n",
        "            self.constraint_violations += 1\n",
        "            # Terminate episode if constraint violated\n",
        "            terminated = True\n",
        "            reward = -100  # Large penalty for constraint violation\n",
        "        \n",
        "        # Add cost information to info\n",
        "        info['cost'] = cost\n",
        "        info['episode_cost'] = self.episode_cost\n",
        "        info['episode_reward'] = self.episode_reward\n",
        "        info['constraint_violations'] = self.constraint_violations\n",
        "        \n",
        "        return obs, reward, terminated, truncated, info\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"Render the environment.\"\"\"\n",
        "        return self.env.render(mode)\n",
        "    \n",
        "    def close(self):\n",
        "        \"\"\"Close the environment.\"\"\"\n",
        "        self.env.close()\n",
        "\n",
        "# Test the safe environment\n",
        "print(\"Testing Safe CartPole Environment...\")\n",
        "env = SafeCartPoleEnv(position_limit=1.5, cost_threshold=0.1)\n",
        "\n",
        "# Run a few random episodes to test\n",
        "for episode in range(3):\n",
        "    obs, info = env.reset()\n",
        "    episode_reward = 0\n",
        "    episode_cost = 0\n",
        "    \n",
        "    for step in range(100):  # Max 100 steps\n",
        "        action = env.action_space.sample()  # Random action\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        \n",
        "        episode_reward += reward\n",
        "        episode_cost += info['cost']\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}, Cost = {episode_cost:.2f}, \"\n",
        "          f\"Violations = {info['constraint_violations']}\")\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Neural Network Architectures\n",
        "\n",
        "Let's implement the neural network components needed for safe RL algorithms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    \"\"\"Gaussian policy network for continuous actions.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
        "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        mean = self.mean(x)\n",
        "        std = torch.exp(self.log_std.clamp(-20, 2))  # Clamp for numerical stability\n",
        "        return mean, std\n",
        "    \n",
        "    def sample(self, state):\n",
        "        \"\"\"Sample action from the policy.\"\"\"\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        action = dist.sample()\n",
        "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "        return action, log_prob\n",
        "    \n",
        "    def log_prob(self, state, action):\n",
        "        \"\"\"Compute log probability of action given state.\"\"\"\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        return dist.log_prob(action).sum(dim=-1)\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    \"\"\"Value function approximator.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        return self.value(x)\n",
        "\n",
        "class SafetyNetwork(nn.Module):\n",
        "    \"\"\"Safety function approximator for Control Barrier Functions.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.safety = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        x = torch.tanh(self.fc1(state))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        return self.safety(x)\n",
        "\n",
        "class DistributionalCritic(nn.Module):\n",
        "    \"\"\"Distributional critic for risk-sensitive RL.\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, num_atoms=51, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.num_atoms = num_atoms\n",
        "        \n",
        "        # Network architecture\n",
        "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, num_atoms)\n",
        "        \n",
        "        # Atom values (support of the distribution)\n",
        "        self.register_buffer('atoms', torch.linspace(-10, 10, num_atoms))\n",
        "        \n",
        "    def forward(self, state, action):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        x = torch.cat([state, action], dim=-1)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "        x = torch.tanh(self.fc2(x))\n",
        "        logits = self.fc3(x)\n",
        "        return torch.softmax(logits, dim=-1)\n",
        "    \n",
        "    def get_distribution(self, state, action):\n",
        "        \"\"\"Get return distribution.\"\"\"\n",
        "        probs = self.forward(state, action)\n",
        "        return torch.distributions.Categorical(probs)\n",
        "    \n",
        "    def get_value(self, state, action):\n",
        "        \"\"\"Get expected value.\"\"\"\n",
        "        probs = self.forward(state, action)\n",
        "        return torch.sum(probs * self.atoms, dim=-1)\n",
        "\n",
        "# Test the networks\n",
        "print(\"Testing Neural Network Architectures...\")\n",
        "\n",
        "# Test parameters\n",
        "state_dim = 4  # CartPole state dimension\n",
        "action_dim = 1  # CartPole action dimension (continuous)\n",
        "\n",
        "# Create networks\n",
        "policy = PolicyNetwork(state_dim, action_dim).to(device)\n",
        "value_reward = ValueNetwork(state_dim).to(device)\n",
        "value_cost = ValueNetwork(state_dim).to(device)\n",
        "safety = SafetyNetwork(state_dim).to(device)\n",
        "critic_dist = DistributionalCritic(state_dim, action_dim).to(device)\n",
        "\n",
        "# Test forward passes\n",
        "test_state = torch.randn(1, state_dim).to(device)\n",
        "test_action = torch.randn(1, action_dim).to(device)\n",
        "\n",
        "print(f\"Policy output shape: {policy(test_state)[0].shape}\")\n",
        "print(f\"Value output shape: {value_reward(test_state).shape}\")\n",
        "print(f\"Safety output shape: {safety(test_state).shape}\")\n",
        "print(f\"Distributional critic output shape: {critic_dist(test_state, test_action).shape}\")\n",
        "\n",
        "print(\"All networks initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
