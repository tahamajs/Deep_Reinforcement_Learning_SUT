{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30512,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Deep Q-Learning Gym's Lunar Lander Environment\n\nIn this notebook, we will explore the implementation of a Deep Q-Learning (DQN) agent to navigate Gym's Lunar Lander environment. \n\nIn the Lunar Lander environment, the agent's task is to learn how to land a lunar module safely on the moon's surface. This requires the agent to balance fuel efficiency and safety considerations. The agent needs to learn from its past experiences, developing a strategy to approach the landing pad while minimizing its speed and using as little fuel as possible.\n\nLet's initialize a LunarLander-v2 environmnet, make random actions in the environment, then view a recording of it.","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-06-21T19:20:26.339215Z","iopub.execute_input":"2023-06-21T19:20:26.339632Z","iopub.status.idle":"2023-06-21T19:20:26.345129Z","shell.execute_reply.started":"2023-06-21T19:20:26.3396Z","shell.execute_reply":"2023-06-21T19:20:26.343795Z"}}},{"cell_type":"code","source":"# To use in Kaggle we need to install these two packages\n!pip install swig \n!pip install gym[box2d]","metadata":{"_kg_hide-input":false,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2025-02-26T07:44:29.965630Z","iopub.execute_input":"2025-02-26T07:44:29.965953Z","iopub.status.idle":"2025-02-26T07:45:23.976913Z","shell.execute_reply.started":"2025-02-26T07:44:29.965927Z","shell.execute_reply":"2025-02-26T07:45:23.975803Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting swig\n  Downloading swig-4.3.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: swig\nSuccessfully installed swig-4.3.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: gym[box2d] in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (1.23.5)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (0.0.8)\nCollecting box2d-py==2.3.5 (from gym[box2d])\n  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting pygame==2.1.0 (from gym[box2d])\n  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: swig==4.* in /opt/conda/lib/python3.10/site-packages (from gym[box2d]) (4.3.0)\nBuilding wheels for collected packages: box2d-py\n  Building wheel for box2d-py (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=494688 sha256=36d9a6d907be49e314c7203f9cd99650b5472d1404bc9b1538258da58d4911aa\n  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\nSuccessfully built box2d-py\nInstalling collected packages: box2d-py, pygame\nSuccessfully installed box2d-py-2.3.5 pygame-2.1.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import gym\n\nenv = gym.make('LunarLander-v2')\nenv.reset(seed=42)\n\n# Play one complete episode with random actions\nwhile True:\n    action = env.action_space.sample() \n    _, _, terminated, truncated, _ = env.step(action)\n    if terminated or truncated:\n        break\n    \nenv.close()","metadata":{"_kg_hide-input":false,"execution":{"iopub.status.busy":"2025-02-26T07:45:23.979218Z","iopub.execute_input":"2025-02-26T07:45:23.979663Z","iopub.status.idle":"2025-02-26T07:45:24.823107Z","shell.execute_reply.started":"2025-02-26T07:45:23.979628Z","shell.execute_reply":"2025-02-26T07:45:24.821851Z"},"trusted":true},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"![](https://i.imgur.com/tQ3zeQA.gif)\n\n## General Information\nThis information is from the official Gym documentation.\n\nhttps://www.gymlibrary.dev/environments/box2d/lunar_lander/\n\n| Feature Category  | Details                                |\n|-------------------|----------------------------------------|\n| Action Space      | Discrete(4)                            |\n| Observation Shape | (8,)                                   |\n| Observation High  | [1.5 1.5 5. 5. 3.14 5. 1. 1. ]         |\n| Observation Low   | [-1.5 -1.5 -5. -5. -3.14 -5. -0. -0. ] |\n| Import            | `gym.make(\"LunarLander-v2\")`           |\n\n## Description of Environment\n\nThis environment is a classic rocket trajectory optimization problem. According to Pontryagin’s maximum principle, it is optimal to fire the engine at full throttle or turn it off. This is the reason why this environment has discrete actions: engine on or off.\n\nThere are two environment versions: discrete or continuous. The landing pad is always at coordinates `(0,0)`. The coordinates are the first two numbers in the state vector. Landing outside of the landing pad is possible. Fuel is infinite, so an agent could learn to fly and then land on its first attempt.\n\n## Action Space\nThere are four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n\n| Action  | Result                          |\n|---------|---------------------------------|\n| 0       | Do nothing                      |\n| 1       | Fire left orientation engine    |\n| 2       | Fire main engine                |\n| 3       | Fire right orientation engine   |\n\n## Observation Space\nThe state is an 8-dimensional vector: the coordinates of the lander in `x` & `y`, its linear velocities in `x` & `y`, its angle, its angular velocity, and two booleans that represent whether each leg is in contact with the ground or not.\n\n| Observation  | Value                                   |\n|--------------|-----------------------------------------|\n| 0            | `x` coordinate (float)                  |\n| 1            | `y` coordinate (float)                  |\n| 2            | `x` linear velocity (float)             |\n| 3            | `y` linear velocity (float)             |\n| 4            | Angle in radians from -π to +π (float)  |\n| 5            | Angular velocity (float)                |\n| 6            | Left leg contact (bool)                 |\n| 7            | Right leg contact (bool)                |\n\n## Rewards\nReward for moving from the top of the screen to the landing pad and coming to rest is about 100-140 points. If the lander moves away from the landing pad, it loses reward. If the lander crashes, it receives an additional -100 points. If it comes to rest, it receives an additional +100 points. Each leg with ground contact is +10 points. Firing the main engine is -0.3 points each frame. Firing the side engine is -0.03 points each frame. Solved is 200 points.\n\n## Starting State\nThe lander starts at the top center of the viewport with a random initial force applied to its center of mass.\n\n## Episode Termination\nThe episode finishes if:\n\n1. The lander crashes (the lander body gets in contact with the moon);\n\n2. The lander gets outside of the viewport (`x` coordinate is greater than 1);\n\n3. The lander is not awake. From the Box2D docs, a body which is not awake is a body which doesn’t move and doesn’t collide with any other body:\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"---\n\n# Deep Reinforcement Learning\nTo address this challenge, we'll use deep reinforcement learning techniques to train an agent to land the spacecraft.\n\nSimpler tabular methods are limited to discrete observation spaces, meaning there are a finite number of possible states. In `LunarLander-v2` however, we're dealing with a continuous range of states across 8 different parameters, meaning there are a near-infinite number of possible states. We could try to bin similar values into groups, but due to the sensitive controls of the game, even slight errors can lead to significant missteps.\n\nTo get around this, we'll use a `neural network Q-function approximator`. This lets us predict the best actions to take for a given state, even when dealing with a vast number of potential states. It's a much better match for our complex landing challenge.\n\n## The DQN Algorithm:\n\nThis breakthrough algorithm was used by Mihn et al in 2015 to achieve human-level performance on several Atari 2600 games. \n\nThe original paper published in Nature can be viewed here:\n\nhttps://www.deepmind.com/publications/human-level-control-through-deep-reinforcement-learning\n\nThe algorithm:\n\n1. **Initialization**: Begin by initializing the parameters for two neural networks, $Q(s,a)$ (referred to as the online network) and $\\hat{Q}(s,a)$ (known as the target network), with random weights. Both networks serve the function of mapping a state-action pair to a Q-value, which is an estimate of the expected return from that pair. Also, set the exploration probability $\\epsilon$ to 1.0, and create an empty replay buffer to store past transition experiences.\n2. **Action Selection**: Utilize an epsilon-greedy strategy for action selection. With a probability of $\\epsilon$, select a random action $a$, but in all other instances, choose the action $a$ that maximizes the Q-value, i.e., $a = argmax_aQ(s,a)$.\n3. **Experience Collection**: Execute the chosen action $a$ within the environment emulator and observe the resulting immediate reward $r$ and the next state $s'$.\n4. **Experience Storage**: Store the transition $(s,a,r,s')$ in the replay buffer for future reference.\n5. **Sampling**: Randomly sample a mini-batch of transitions from the replay buffer for training the online network.\n6. **Target Computation**: For every transition in the sampled mini-batch, compute the target value $y$. If the episode has ended at this step, $y$ is simply the reward $r$. Otherwise, $y$ is the sum of the reward and the discounted estimated optimal future Q-value, i.e.,  $y = r + \\gamma \\max_{a' \\in A} \\hat{Q}(s', a')$\n7. **Loss Calculation**: Compute the loss, which is the squared difference between the Q-value predicted by the online network and the computed target, i.e., $\\mathcal{L} = (Q(s,a) - y)^2$\n8. **Online Network Update**: Update the parameters of the online network $Q(s,a)$ using Stochastic Gradient Descent (SGD) to minimize the loss.\n9. **Target Network Update**: Every $N$ steps, update the target network by copying the weights from the online network to the target network $\\hat{Q}(s,a)$.\n10. **Iterate**: Repeat the process from step 2 until convergence.\n\n### Defining the Deep Q-Network\nOur network will be a simple feedforward neural network that takes the state as input and produces Q-values for each action as output. For `LunarLander-v2` the state is an 8-dimensional vector and there are 4 possible actions.\n","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass DQN(torch.nn.Module):\n    '''\n    This class defines a deep Q-network (DQN), a type of artificial neural network used in reinforcement learning.\n    The DQN is used to estimate the Q-values, which represent the expected return for each action in each state.\n    \n    Parameters\n    ----------\n    state_size: int, default=8\n        The size of the state space.\n    action_size: int, default=4\n        The size of the action space.\n    hidden_size: int, default=64\n        The size of the hidden layers in the network.\n    '''\n    def __init__(self, state_size=8, action_size=4, hidden_size=64):\n        '''\n        Initialize a network with the following architecture:\n            Input layer (state_size, hidden_size)\n            Hidden layer 1 (hidden_size, hidden_size)\n            Output layer (hidden_size, action_size)\n        '''\n        super(DQN, self).__init__()\n        self.layer1 = torch.nn.Linear(state_size, hidden_size)\n        self.layer2 = torch.nn.Linear(hidden_size, hidden_size)\n        self.layer3 = torch.nn.Linear(hidden_size, action_size)\n\n    def forward(self, state):\n        '''\n        Define the forward pass of the DQN. This function is called when the network is called to estimate Q-values.\n        \n        Parameters\n        ----------\n        state: torch.Tensor\n            The state for which to estimate the Q-values.\n\n        Returns\n        -------\n        torch.Tensor\n            The estimated Q-values for each action in the input state.\n        '''\n        x = torch.relu(self.layer1(state))\n        x = torch.relu(self.layer2(x))\n        return self.layer3(x)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-02-26T07:45:24.824266Z","iopub.execute_input":"2025-02-26T07:45:24.824532Z","iopub.status.idle":"2025-02-26T07:45:28.702527Z","shell.execute_reply.started":"2025-02-26T07:45:24.824510Z","shell.execute_reply":"2025-02-26T07:45:28.701271Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### Defining the Replay Buffer\nIn the context of RL, we employ a structure known as the replay buffer, which utilizes a deque. The replay buffer stores and samples experiences, which helps us overcome the problem of *step correlation*.\n\nA *deque* (double-ended queue) is a data structure that enables the addition or removal of elements from both its ends, hence the name. It is particularly useful when there is a need for fast append and pop operations from either end of the container, which it provides at O(1) time complexity. In contrast, a list offers these operations at O(n) time complexity, making the deque a preferred choice in cases that necessitate more efficient operations.\n\nMoreover, a deque allows setting a maximum size. Once this maximum size is exceeded during an insertion (push) operation at the front, the deque automatically ejects the item at the rear, thereby maintaining its maximum length.\n\nIn the replay buffer, the `push` method is utilized to add an experience. If adding this experience exceeds the maximum buffer size, the oldest (rear-most) experience is automatically removed. This approach ensures that the replay buffer always contains the most recent experiences up to its capacity.\n\nThe `sample` method, on the other hand, is used to retrieve a random batch of experiences from the replay buffer. This randomness is critical in breaking correlations within the sequence of experiences, which leads to more robust learning.\n\nThis combination of recency and randomness allows us to learn on new training data, without training samples being highly correlated.","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\nfrom collections import deque\n\nclass ReplayBuffer:\n    '''\n    This class represents a replay buffer, a type of data structure commonly used in reinforcement learning algorithms.\n    The buffer stores past experiences in the environment, allowing the agent to sample and learn from them at later times.\n    This helps to break the correlation of sequential observations and stabilize the learning process.\n    \n    Parameters\n    ----------\n    buffer_size: int, default=10000\n        The maximum number of experiences that can be stored in the buffer.\n    '''\n    def __init__(self, buffer_size=10000):\n        self.buffer = deque(maxlen=buffer_size)\n\n    def push(self, state, action, reward, next_state, done):\n        '''\n        Add a new experience to the buffer. Each experience is a tuple containing a state, action, reward,\n        the resulting next state, and a done flag indicating whether the episode has ended.\n\n        Parameters\n        ----------\n        state: array-like\n            The state of the environment before taking the action.\n        action: int\n            The action taken by the agent.\n        reward: float\n            The reward received after taking the action.\n        next_state: array-like\n            The state of the environment after taking the action.\n        done: bool\n            A flag indicating whether the episode has ended after taking the action.\n        '''\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        '''\n        Randomly sample a batch of experiences from the buffer. The batch size must be smaller or equal to the current number of experiences in the buffer.\n\n        Parameters\n        ----------\n        batch_size: int\n            The number of experiences to sample from the buffer.\n\n        Returns\n        -------\n        tuple of numpy.ndarray\n            A tuple containing arrays of states, actions, rewards, next states, and done flags.\n        '''\n        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n        return np.stack(states), actions, rewards, np.stack(next_states), dones\n\n    def __len__(self):\n        '''\n        Get the current number of experiences in the buffer.\n\n        Returns\n        -------\n        int\n            The number of experiences in the buffer.\n        '''\n        return len(self.buffer)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-02-26T07:45:28.704053Z","iopub.execute_input":"2025-02-26T07:45:28.704770Z","iopub.status.idle":"2025-02-26T07:45:28.712387Z","shell.execute_reply.started":"2025-02-26T07:45:28.704743Z","shell.execute_reply":"2025-02-26T07:45:28.711073Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"### Define the DQN Agent\nThe DQN agent handles the interaction with the environment, selecting actions, collecting experiences, storing them in the replay buffer, and using these experiences to train the network. Let's walk through each part of this process:\n\n#### Initialisation\nThe `__init__` function sets up the agent:\n\n- `self.device`: We start by checking whether a GPU is available, and, if so, we use it, otherwise, we fall back to CPU. \n- `self.gamma`: This is the discount factor for future rewards, used in the Q-value update equation.\n- `self.batch_size`: This is the number of experiences we'll sample from the memory when updating the model.\n- `self.q_network` and `self.target_network`: These are two instances of the Q-Network. The first is the network we're actively training, and the second is a copy that gets updated less frequently. This helps to stabilize learning.\n- `self.optimizer`: This is the optimization algorithm used to update the Q-Network's parameters.\n- `self.memory`: This is a replay buffer that stores experiences. It's an instance of the `ReplayBuffer` class.\n\n#### Step Function\nThe `step` function is called after each timestep in the environment:\n\n- The function starts by storing the new experience in the replay buffer.\n- If enough experiences have been stored, it calls `self.update_model()`, which triggers a learning update.\n\n#### Action Selection\nThe act function is how the agent selects an action:\n\n- If a randomly drawn number is greater than $\\epsilon$, it selects the action with the highest predicted Q-value. This is known as exploitation: the agent uses what it has learned to select the best action.\n- If the random number is less than $\\epsilon$, it selects an action randomly. This is known as exploration: the agent explores the environment to learn more about it.\n\n#### Model Update\nThe `update_model` function is where the learning happens:\n\n- It starts by sampling a batch of experiences from the replay buffer.\n- It then calculates the current Q-values for the sampled states and actions, and the expected - Q-values based on the rewards and next states.\n- It calculates the loss, which is the mean squared difference between the current and expected Q-values.\n- It then backpropagates this loss through the Q-Network and updates the weights using the optimizer.\n\n#### Target Network Update\nFinally, the `update_target_network` function copies the weights from the Q-Network to the Target Network. This is done periodically (not every step), to stabilize the learning process. Without this, the Q-Network would be trying to follow a moving target, since it's learning from estimates produced by itself.","metadata":{}},{"cell_type":"code","source":"class DQNAgent:\n    '''\n    This class represents a Deep Q-Learning agent that uses a Deep Q-Network (DQN) and a replay memory to interact \n    with its environment.\n\n    Parameters\n    ----------\n    state_size: int, default=8\n        The size of the state space.\n    action_size: int, default=4\n        The size of the action space.\n    hidden_size: int, default=64\n        The size of the hidden layers in the network.\n    learning_rate: float, default=1e-3\n        The learning rate for the optimizer.\n    gamma: float, default=0.99\n        The discount factor for future rewards.\n    buffer_size: int, default=10000\n        The maximum size of the replay memory.\n    batch_size: int, default=64\n        The batch size for learning from the replay memory.\n    '''\n    def __init__(self, state_size=8, action_size=4, hidden_size=64, \n                 learning_rate=1e-3, gamma=0.99, buffer_size=10000, batch_size=64):\n        # Select device to train on (if CUDA available, use it, otherwise use CPU)\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        \n        # Discount factor for future rewards\n        self.gamma = gamma\n\n        # Batch size for sampling from the replay memory\n        self.batch_size = batch_size\n\n        # Number of possible actions\n        self.action_size = action_size\n\n        # Initialize the Q-Network and Target Network with the given state size, action size and hidden layer size\n        # Move the networks to the selected device\n        self.q_network = DQN(state_size, action_size, hidden_size).to(self.device)\n        self.target_network = DQN(state_size, action_size, hidden_size).to(self.device)\n        \n        # Set weights of target network to be the same as those of the q network\n        self.target_network.load_state_dict(self.q_network.state_dict())\n        \n        # Set target network to evaluation mode\n        self.target_network.eval()\n\n        # Initialize the optimizer for updating the Q-Network's parameters\n        self.optimizer = torch.optim.Adam(self.q_network.parameters(), lr=learning_rate)\n        \n        # Initialize the replay memory\n        self.memory = ReplayBuffer(buffer_size)\n\n    def step(self, state, action, reward, next_state, done):\n        '''\n        Perform a step in the environment, store the experience in the replay memory and potentially update the Q-network.\n\n        Parameters\n        ----------\n        state: array-like\n            The current state of the environment.\n        action: int\n            The action taken by the agent.\n        reward: float\n            The reward received after taking the action.\n        next_state: array-like\n            The state of the environment after taking the action.\n        done: bool\n            A flag indicating whether the episode has ended after taking the action.\n        '''\n        # Store the experience in memory\n        self.memory.push(state, action, reward, next_state, done)\n        \n        # If there are enough experiences in memory, perform a learning step\n        if len(self.memory) > self.batch_size:\n            self.update_model()\n\n    def act(self, state, eps=0.):\n        '''\n        Choose an action based on the current state and the epsilon-greedy policy.\n\n        Parameters\n        ----------\n        state: array-like\n            The current state of the environment.\n        eps: float, default=0.\n            The epsilon for the epsilon-greedy policy. With probability eps, a random action is chosen.\n\n        Returns\n        -------\n        int\n            The chosen action.\n        '''\n        # If a randomly chosen value is greater than eps\n        if random.random() > eps:  \n            # Convert state to a PyTorch tensor and set network to evaluation mode\n            state = torch.from_numpy(state).float().unsqueeze(0).to(self.device)  \n            self.q_network.eval()  \n\n            # With no gradient updates, get the action values from the DQN\n            with torch.no_grad():\n                action_values = self.q_network(state)\n\n            # Revert to training mode and return action\n            self.q_network.train() \n            return np.argmax(action_values.cpu().data.numpy())\n        else:\n            # Return a random action for random value > eps\n            return random.choice(np.arange(self.action_size))  \n        \n    def update_model(self):\n        '''\n        Update the Q-network based on a batch of experiences from the replay memory.\n        '''\n        # Sample a batch of experiences from memory\n        states, actions, rewards, next_states, dones = self.memory.sample(self.batch_size)\n\n        # Convert numpy arrays to PyTorch tensors\n        states = torch.from_numpy(states).float().to(self.device)\n        actions = torch.from_numpy(np.array(actions)).long().to(self.device)\n        rewards = torch.from_numpy(np.array(rewards)).float().to(self.device)\n        next_states = torch.from_numpy(next_states).float().to(self.device)\n        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(self.device)\n\n        # Get Q-values for the actions that were actually taken\n        q_values = self.q_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n        \n        # Get maximum Q-value for the next states from target network\n        next_q_values = self.target_network(next_states).max(1)[0].detach()\n        \n        # Compute the expected Q-values\n        expected_q_values = rewards + self.gamma * next_q_values * (1 - dones)\n\n        # Compute the loss between the current and expected Q values\n        loss = torch.nn.MSELoss()(q_values, expected_q_values)\n        \n        # Zero all gradients\n        self.optimizer.zero_grad()\n        \n        # Backpropagate the loss\n        loss.backward()\n        \n        # Step the optimizer\n        self.optimizer.step()\n\n    def update_target_network(self):\n        '''\n        Update the weights of the target network to match those of the Q-network.\n        '''\n        self.target_network.load_state_dict(self.q_network.state_dict())","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-02-26T07:45:28.715410Z","iopub.execute_input":"2025-02-26T07:45:28.715701Z","iopub.status.idle":"2025-02-26T07:45:28.748336Z","shell.execute_reply.started":"2025-02-26T07:45:28.715666Z","shell.execute_reply":"2025-02-26T07:45:28.747402Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Training the Agent\n\nTraining the agent involves having the agent interact with the `LunarLander-v2` environment over a sequence of steps. Over each step, the agent receives a state from the environment, selects an action, receives a reward and the next state, and then updates its understanding of the environment (the Q-table in the case of Q-Learning).\n\nThe `train` function orchestrates this process over a defined number of episodes, using the methods defined in the DQNAgent class. Here's how it works:\n\n#### Initial Setup\n- `scores`: This list stores the total reward obtained in each episode.\n- `scores_window`: This is a double-ended queue with a maximum length of 100. It holds the scores of the most recent 100 episodes and is used to monitor the agent's performance.\n-`eps`: This is the epsilon for epsilon-greedy action selection. It starts from `eps_start` and decays after each episode until it reaches `eps_end`.\n\n#### Episode Loop\nThe training process runs over a fixed number of episodes. In each episode:\n\n- The environment is reset to its initial state.\n- he agent then interacts with the environment until the episode is done (when a terminal state is reached).\n\n#### Step Loop\nIn each step of an episode:\n\n- The agent selects an action using the current policy (the act method in `DQNAgent`).\nThe selected action is applied to the environment using the step method, which returns the next state, the reward, and a boolean indicating whether the episode is done.\n- The agent's step method is called to update the agent's knowledge. This involves adding the experience to the replay buffer and, if enough experiences have been collected, triggering a learning update.\n- The state is updated to the next state, and the reward is added to the score.\n\nAfter each episode:\n\n- The score for the episode is added to `scores` and `scores_window`.\n- Epsilon is decayed according to `eps_decay`.\n- If the episode is a multiple of `target_update`, the target network is updated with the latest weights from the Q-Network.\n- Finally, every 100 episodes, the average score over the last 100 episodes is printed.\n\nThe function returns the list of scores for all episodes.\n\nThis training process, which combines experiences from the replay buffer and separate target and Q networks, helps to stabilize the learning and leads to a more robust policy.","metadata":{}},{"cell_type":"code","source":"def train(agent, env, n_episodes=2000, eps_start=1.0, eps_end=0.01, eps_decay=0.995, target_update=10):\n    '''\n    Train a DQN agent.\n    \n    Parameters\n    ----------\n    agent: DQNAgent\n        The agent to be trained.\n    env: gym.Env\n        The environment in which the agent is trained.\n    n_episodes: int, default=2000\n        The number of episodes for which to train the agent.\n    eps_start: float, default=1.0\n        The starting epsilon for epsilon-greedy action selection.\n    eps_end: float, default=0.01\n        The minimum value that epsilon can reach.\n    eps_decay: float, default=0.995\n        The decay rate for epsilon after each episode.\n    target_update: int, default=10\n        The frequency (number of episodes) with which the target network should be updated.\n        \n    Returns\n    -------\n    list of float\n        The total reward obtained in each episode.\n    '''\n\n    # Initialize the scores list and scores window\n    scores = []\n    scores_window = deque(maxlen=100)\n    eps = eps_start\n\n    # Loop over episodes\n    for i_episode in range(1, n_episodes + 1):\n        \n        # Reset environment and score at the start of each episode\n        state, _ = env.reset()\n        score = 0 \n\n        # Loop over steps\n        while True:\n            \n            # Select an action using current agent policy then apply in environment\n            action = agent.act(state, eps)\n            next_state, reward, terminated, truncated, _ = env.step(action) \n            done = terminated or truncated\n            \n            # Update the agent, state and score\n            agent.step(state, action, reward, next_state, done)\n            state = next_state \n            score += reward\n\n            # End the episode if done\n            if done:\n                break \n        \n        # At the end of episode append and save scores\n        scores_window.append(score)\n        scores.append(score) \n\n        # Decrease epsilon\n        eps = max(eps_end, eps_decay * eps)\n\n        # Print some info\n        print(f\"\\rEpisode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\", end=\"\")\n\n        # Update target network every target_update episodes\n        if i_episode % target_update == 0:\n            agent.update_target_network()\n            \n        # Print average score every 100 episodes\n        if i_episode % 100 == 0:\n            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n        \n        # This environment is considered to be solved for a mean score of 200 or greater, so stop training.\n        if i_episode % 100 == 0 and np.mean(scores_window) >= 200:\n            break\n            \n\n    return scores\n\n\n# Make an environment\nenv = gym.make('LunarLander-v2')\nstate_size = env.observation_space.shape[0]\naction_size = env.action_space.n\n\n# Initilize a DQN agent\nagent = DQNAgent(state_size, action_size)\n\n# Train it\nscores = train(agent, env, 200)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2025-02-26T07:45:28.750092Z","iopub.execute_input":"2025-02-26T07:45:28.750370Z","iopub.status.idle":"2025-02-26T07:46:55.471185Z","shell.execute_reply.started":"2025-02-26T07:45:28.750351Z","shell.execute_reply":"2025-02-26T07:46:55.470446Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Episode 100\tAverage Score: -136.72\nEpisode 200\tAverage Score: -38.914\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"#### Observations:\n- Our DQN agent is able to solve the game typically after playing around 1200 episodes.\n- Let's watch a video of this agent's performance:","metadata":{}},{"cell_type":"code","source":"env = gym.make('LunarLander-v2')\n\ndef play_DQN_episode(env, \n                     agent):\n    score = 0\n    state, _ = env.reset(seed=42)\n    \n    while True:\n        # eps=0 for predictions\n        action = agent.act(state, 0)\n        state, reward, terminated, truncated, _ = env.step(action) \n        done = terminated or truncated\n\n        score += reward\n\n        # End the episode if done\n        if done:\n            break \n\n    return score\n \nscore = play_DQN_episode(env, agent)\nprint(\"Score obtained:\", score)","metadata":{"_kg_hide-input":true,"trusted":true,"execution":{"iopub.status.busy":"2025-02-26T07:46:55.472106Z","iopub.execute_input":"2025-02-26T07:46:55.472333Z","iopub.status.idle":"2025-02-26T07:46:56.416567Z","shell.execute_reply.started":"2025-02-26T07:46:55.472315Z","shell.execute_reply":"2025-02-26T07:46:56.415754Z"}},"outputs":[{"name":"stdout","text":"Score obtained: -49.19626422783109\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Source: https://www.kaggle.com/code/auxeno/dqn-on-lunar-lander-rl","metadata":{}}]}