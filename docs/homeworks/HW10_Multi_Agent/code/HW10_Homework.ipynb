{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d682ca85",
   "metadata": {},
   "source": [
    "<!-- Centered layout with a university logo -->\n",
    "<div align=\"center\">\n",
    "\n",
    "  <!-- University Logo -->\n",
    "  <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=\"180\" height=\"180\" style=\"margin-bottom: 10px;\">\n",
    "  \n",
    "  <!-- Assignment Title -->\n",
    "  <h1></h1>\n",
    "  <h1 style=\"color:#0F5298; font-size: 40px; font-weight: bold; margin-bottom: 5px;\">Deep Reinforcement Learning</h1>\n",
    "  <h2 style=\"color:#0F5298; font-size: 32px; font-weight: normal; margin-top: 0px;\">Assignment 10 - Multi-Agent Reinforcement Learning</h2>\n",
    "\n",
    "  <!-- Department and University -->\n",
    "  <h3 style=\"color:#696880; font-size: 24px; margin-top: 20px;\">Computer Engineering Department</h3>\n",
    "  <h3 style=\"color:#696880; font-size: 22px; margin-top: -5px;\">Sharif University of Technology</h3>\n",
    "\n",
    "  <!-- Semester -->\n",
    "  <h3 style=\"color:#696880; font-size: 22px; margin-top: 20px;\">Spring 2025</h3>\n",
    "\n",
    "  <!-- Authors -->\n",
    "  <h3 style=\"color:green; font-size: 22px; margin-top: 20px;\">Full name: [FULL_NAME]</h3>\n",
    "  <h3 style=\"color:green; font-size: 22px; margin-top: 20px;\">Student ID: [STUDENT_ID]</h3>\n",
    "\n",
    "  <!-- Horizontal Line for Separation -->\n",
    "  <hr style=\"border: 1px solid #0F5298; width: 80%; margin-top: 30px;\">\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d128e3",
   "metadata": {},
   "source": [
    "## Setup & Overview  \n",
    "In this notebook, we explore Multi-Agent Reinforcement Learning (MARL) through various algorithms and environments.  \n",
    "We implement and compare several approaches:\n",
    "- **Independent Q-Learning** (IQL) - Each agent learns independently\n",
    "- **QMIX** - Value decomposition for cooperative settings\n",
    "- **MADDPG** - Multi-Agent Actor-Critic for mixed environments\n",
    "- **Communication Protocols** - CommNet and TarMAC\n",
    "- **Self-Play** - Training against past versions\n",
    "\n",
    "We'll work with classic game theory environments like Prisoner's Dilemma and Coordination Games, then move to more complex multi-agent scenarios.\n",
    "\n",
    "Follow the instructions carefully and complete the sections marked with **TODO**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a9b10",
   "metadata": {},
   "source": [
    "## Setup and Environment\n",
    "\n",
    "In the upcoming cells, we import necessary libraries, set up utility functions for reproducibility and plotting, and define the basic components of our multi-agent experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque, defaultdict\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def plot_logs(df, x_key, y_key, legend_key, **kwargs):\n",
    "    \"\"\"Plot learning curves for multi-agent experiments\"\"\"\n",
    "    num = len(df[legend_key].unique())\n",
    "    pal = sns.color_palette(\"hls\", num)\n",
    "    if 'palette' not in kwargs:\n",
    "        kwargs['palette'] = pal\n",
    "    ax = sns.lineplot(x=x_key, y=y_key, data=df, hue=legend_key, **kwargs)\n",
    "    return ax\n",
    "\n",
    "def plot_game_matrix(matrix, title=\"Game Matrix\", figsize=(8, 6)):\n",
    "    \"\"\"Plot a 2x2 game matrix\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(matrix, cmap='RdYlBu', aspect='auto')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            text = ax.text(j, i, f'{matrix[i, j][0]:.1f}, {matrix[i, j][1]:.1f}',\n",
    "                         ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['Action 0', 'Action 1'])\n",
    "    ax.set_yticklabels(['Action 0', 'Action 1'])\n",
    "    ax.set_xlabel('Agent 2 Action')\n",
    "    ax.set_ylabel('Agent 1 Action')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label('Payoff Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def set_seed(s):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649c952",
   "metadata": {},
   "source": [
    "## Game Theory Foundations\n",
    "\n",
    "We start with classic game theory environments to understand multi-agent interactions:\n",
    "\n",
    "### Prisoner's Dilemma\n",
    "- **Cooperation (C)**: Both agents cooperate, get moderate reward\n",
    "- **Defection (D)**: One agent defects while other cooperates, defector gets high reward\n",
    "- **Mutual Defection**: Both defect, get low reward\n",
    "\n",
    "### Coordination Game  \n",
    "- **Pure Coordination**: Both agents must choose same action for high reward\n",
    "- **Battle of Sexes**: Different preferences but coordination still beneficial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Define game matrices\n",
    "PRISONERS_DILEMMA = np.array([\n",
    "    [[3, 3], [0, 5]],  # Agent 1: C, Agent 2: C,D\n",
    "    [[5, 0], [1, 1]]   # Agent 1: D, Agent 2: C,D\n",
    "])\n",
    "\n",
    "COORDINATION_GAME = np.array([\n",
    "    [[2, 2], [0, 0]],   # Agent 1: A, Agent 2: A,B\n",
    "    [[0, 0], [1, 1]]    # Agent 1: B, Agent 2: A,B\n",
    "])\n",
    "\n",
    "BATTLE_OF_SEXES = np.array([\n",
    "    [[2, 1], [0, 0]],   # Agent 1: A, Agent 2: A,B\n",
    "    [[0, 0], [1, 2]]    # Agent 1: B, Agent 2: A,B\n",
    "])\n",
    "\n",
    "print(\"Prisoner's Dilemma Matrix:\")\n",
    "print(\"Agent 1\\\\Agent 2 | Cooperate | Defect\")\n",
    "print(\"Cooperate        |   3, 3    |  0, 5\")\n",
    "print(\"Defect           |   5, 0    |  1, 1\")\n",
    "print(\"\\nNash Equilibrium: (Defect, Defect)\")\n",
    "print(\"Pareto Optimal: (Cooperate, Cooperate)\")\n",
    "\n",
    "plot_game_matrix(PRISONERS_DILEMMA, \"Prisoner's Dilemma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb0a77",
   "metadata": {},
   "source": [
    "**Q:** Why is the Nash Equilibrium (Defect, Defect) suboptimal in the Prisoner's Dilemma?\n",
    "\n",
    "**A:** The Nash Equilibrium (Defect, Defect) is suboptimal because it represents a situation where both agents choose their individually rational strategy, but this leads to a worse outcome for both compared to mutual cooperation. Each agent defects because they fear being exploited if they cooperate while the other defects. However, if both could commit to cooperation, they would both be better off (3,3 vs 1,1). This illustrates the fundamental tension between individual rationality and collective welfare in competitive environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "plot_game_matrix(COORDINATION_GAME, \"Coordination Game\")\n",
    "print(\"\\nCoordination Game:\")\n",
    "print(\"Agent 1\\\\Agent 2 | Action A | Action B\")\n",
    "print(\"Action A         |   2, 2   |  0, 0\")\n",
    "print(\"Action B         |   0, 0   |  1, 1\")\n",
    "print(\"\\nNash Equilibria: (A,A) and (B,B)\")\n",
    "print(\"Pareto Optimal: (A,A)\")\n",
    "\n",
    "plot_game_matrix(BATTLE_OF_SEXES, \"Battle of Sexes\")\n",
    "print(\"\\nBattle of Sexes:\")\n",
    "print(\"Agent 1\\\\Agent 2 | Action A | Action B\")\n",
    "print(\"Action A         |   2, 1   |  0, 0\")\n",
    "print(\"Action B         |   0, 0   |  1, 2\")\n",
    "print(\"\\nNash Equilibria: (A,A) and (B,B)\")\n",
    "print(\"Agent 1 prefers (A,A), Agent 2 prefers (B,B)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60cdea",
   "metadata": {},
   "source": [
    "## Multi-Agent Environment\n",
    "\n",
    "We'll create a flexible environment that can handle different game matrices and multiple agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790119cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "@dataclass\n",
    "class MultiAgentGame:\n",
    "    \"\"\"Multi-agent game environment\"\"\"\n",
    "    payoff_matrix: np.ndarray\n",
    "    num_agents: int = 2\n",
    "    num_actions: int = 2\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute actions and return rewards\"\"\"\n",
    "        if self.num_agents == 2:\n",
    "            return self.payoff_matrix[actions[0], actions[1]]\n",
    "        else:\n",
    "            # For more than 2 agents, we'll use a different structure\n",
    "            raise NotImplementedError(\"Only 2-agent games implemented\")\n",
    "    \n",
    "    def get_optimal_strategies(self):\n",
    "        \"\"\"Find Nash equilibria\"\"\"\n",
    "        equilibria = []\n",
    "        \n",
    "        # Check all pure strategy combinations\n",
    "        for a1 in range(self.num_actions):\n",
    "            for a2 in range(self.num_actions):\n",
    "                is_equilibrium = True\n",
    "                \n",
    "                # Check if agent 1 wants to deviate\n",
    "                for a1_dev in range(self.num_actions):\n",
    "                    if a1_dev != a1:\n",
    "                        if self.payoff_matrix[a1_dev, a2][0] > self.payoff_matrix[a1, a2][0]:\n",
    "                            is_equilibrium = False\n",
    "                            break\n",
    "                \n",
    "                # Check if agent 2 wants to deviate\n",
    "                if is_equilibrium:\n",
    "                    for a2_dev in range(self.num_actions):\n",
    "                        if a2_dev != a2:\n",
    "                            if self.payoff_matrix[a1, a2_dev][1] > self.payoff_matrix[a1, a2][1]:\n",
    "                                is_equilibrium = False\n",
    "                                break\n",
    "                \n",
    "                if is_equilibrium:\n",
    "                    equilibria.append((a1, a2))\n",
    "        \n",
    "        return equilibria\n",
    "\n",
    "# Test the environment\n",
    "pd_env = MultiAgentGame(PRISONERS_DILEMMA)\n",
    "print(\"Prisoner's Dilemma Nash Equilibria:\", pd_env.get_optimal_strategies())\n",
    "\n",
    "coord_env = MultiAgentGame(COORDINATION_GAME)\n",
    "print(\"Coordination Game Nash Equilibria:\", coord_env.get_optimal_strategies())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2db6b",
   "metadata": {},
   "source": [
    "## Independent Q-Learning (IQL)\n",
    "\n",
    "The simplest approach to multi-agent RL: each agent learns independently, treating other agents as part of the environment.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Each agent maintains its own Q-table\n",
    "- No communication between agents\n",
    "- Non-stationarity: environment changes as other agents learn\n",
    "- No convergence guarantees\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a6229",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
