{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d682ca85",
   "metadata": {},
   "source": [
    "<!-- Centered layout with a university logo -->\n",
    "<div align=\"center\">\n",
    "\n",
    "  <!-- University Logo -->\n",
    "  <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=\"180\" height=\"180\" style=\"margin-bottom: 10px;\">\n",
    "  \n",
    "  <!-- Assignment Title -->\n",
    "  <h1></h1>\n",
    "  <h1 style=\"color:#0F5298; font-size: 40px; font-weight: bold; margin-bottom: 5px;\">Deep Reinforcement Learning</h1>\n",
    "  <h2 style=\"color:#0F5298; font-size: 32px; font-weight: normal; margin-top: 0px;\">Assignment 10 - Multi-Agent Reinforcement Learning</h2>\n",
    "\n",
    "  <!-- Department and University -->\n",
    "  <h3 style=\"color:#696880; font-size: 24px; margin-top: 20px;\">Computer Engineering Department</h3>\n",
    "  <h3 style=\"color:#696880; font-size: 22px; margin-top: -5px;\">Sharif University of Technology</h3>\n",
    "\n",
    "  <!-- Semester -->\n",
    "  <h3 style=\"color:#696880; font-size: 22px; margin-top: 20px;\">Spring 2025</h3>\n",
    "\n",
    "  <!-- Authors -->\n",
    "  <h3 style=\"color:green; font-size: 22px; margin-top: 20px;\">Full name: [FULL_NAME]</h3>\n",
    "  <h3 style=\"color:green; font-size: 22px; margin-top: 20px;\">Student ID: [STUDENT_ID]</h3>\n",
    "\n",
    "  <!-- Horizontal Line for Separation -->\n",
    "  <hr style=\"border: 1px solid #0F5298; width: 80%; margin-top: 30px;\">\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d128e3",
   "metadata": {},
   "source": [
    "## Setup & Overview  \n",
    "In this notebook, we explore Multi-Agent Reinforcement Learning (MARL) through various algorithms and environments.  \n",
    "We implement and compare several approaches:\n",
    "- **Independent Q-Learning** (IQL) - Each agent learns independently\n",
    "- **QMIX** - Value decomposition for cooperative settings\n",
    "- **MADDPG** - Multi-Agent Actor-Critic for mixed environments\n",
    "- **Communication Protocols** - CommNet and TarMAC\n",
    "- **Self-Play** - Training against past versions\n",
    "\n",
    "We'll work with classic game theory environments like Prisoner's Dilemma and Coordination Games, then move to more complex multi-agent scenarios.\n",
    "\n",
    "Follow the instructions carefully and complete the sections marked with **TODO**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a9b10",
   "metadata": {},
   "source": [
    "## Setup and Environment\n",
    "\n",
    "In the upcoming cells, we import necessary libraries, set up utility functions for reproducibility and plotting, and define the basic components of our multi-agent experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque, defaultdict\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def plot_logs(df, x_key, y_key, legend_key, **kwargs):\n",
    "    \"\"\"Plot learning curves for multi-agent experiments\"\"\"\n",
    "    num = len(df[legend_key].unique())\n",
    "    pal = sns.color_palette(\"hls\", num)\n",
    "    if 'palette' not in kwargs:\n",
    "        kwargs['palette'] = pal\n",
    "    ax = sns.lineplot(x=x_key, y=y_key, data=df, hue=legend_key, **kwargs)\n",
    "    return ax\n",
    "\n",
    "def plot_game_matrix(matrix, title=\"Game Matrix\", figsize=(8, 6)):\n",
    "    \"\"\"Plot a 2x2 game matrix\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(matrix, cmap='RdYlBu', aspect='auto')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            text = ax.text(j, i, f'{matrix[i, j][0]:.1f}, {matrix[i, j][1]:.1f}',\n",
    "                         ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['Action 0', 'Action 1'])\n",
    "    ax.set_yticklabels(['Action 0', 'Action 1'])\n",
    "    ax.set_xlabel('Agent 2 Action')\n",
    "    ax.set_ylabel('Agent 1 Action')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label('Payoff Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def set_seed(s):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649c952",
   "metadata": {},
   "source": [
    "## Game Theory Foundations\n",
    "\n",
    "We start with classic game theory environments to understand multi-agent interactions:\n",
    "\n",
    "### Prisoner's Dilemma\n",
    "- **Cooperation (C)**: Both agents cooperate, get moderate reward\n",
    "- **Defection (D)**: One agent defects while other cooperates, defector gets high reward\n",
    "- **Mutual Defection**: Both defect, get low reward\n",
    "\n",
    "### Coordination Game  \n",
    "- **Pure Coordination**: Both agents must choose same action for high reward\n",
    "- **Battle of Sexes**: Different preferences but coordination still beneficial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Define game matrices\n",
    "PRISONERS_DILEMMA = np.array([\n",
    "    [[3, 3], [0, 5]],  # Agent 1: C, Agent 2: C,D\n",
    "    [[5, 0], [1, 1]]   # Agent 1: D, Agent 2: C,D\n",
    "])\n",
    "\n",
    "COORDINATION_GAME = np.array([\n",
    "    [[2, 2], [0, 0]],   # Agent 1: A, Agent 2: A,B\n",
    "    [[0, 0], [1, 1]]    # Agent 1: B, Agent 2: A,B\n",
    "])\n",
    "\n",
    "BATTLE_OF_SEXES = np.array([\n",
    "    [[2, 1], [0, 0]],   # Agent 1: A, Agent 2: A,B\n",
    "    [[0, 0], [1, 2]]    # Agent 1: B, Agent 2: A,B\n",
    "])\n",
    "\n",
    "print(\"Prisoner's Dilemma Matrix:\")\n",
    "print(\"Agent 1\\\\Agent 2 | Cooperate | Defect\")\n",
    "print(\"Cooperate        |   3, 3    |  0, 5\")\n",
    "print(\"Defect           |   5, 0    |  1, 1\")\n",
    "print(\"\\nNash Equilibrium: (Defect, Defect)\")\n",
    "print(\"Pareto Optimal: (Cooperate, Cooperate)\")\n",
    "\n",
    "plot_game_matrix(PRISONERS_DILEMMA, \"Prisoner's Dilemma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb0a77",
   "metadata": {},
   "source": [
    "**Q:** Why is the Nash Equilibrium (Defect, Defect) suboptimal in the Prisoner's Dilemma?\n",
    "\n",
    "**A:** The Nash Equilibrium (Defect, Defect) is suboptimal because it represents a situation where both agents choose their individually rational strategy, but this leads to a worse outcome for both compared to mutual cooperation. Each agent defects because they fear being exploited if they cooperate while the other defects. However, if both could commit to cooperation, they would both be better off (3,3 vs 1,1). This illustrates the fundamental tension between individual rationality and collective welfare in competitive environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "plot_game_matrix(COORDINATION_GAME, \"Coordination Game\")\n",
    "print(\"\\nCoordination Game:\")\n",
    "print(\"Agent 1\\\\Agent 2 | Action A | Action B\")\n",
    "print(\"Action A         |   2, 2   |  0, 0\")\n",
    "print(\"Action B         |   0, 0   |  1, 1\")\n",
    "print(\"\\nNash Equilibria: (A,A) and (B,B)\")\n",
    "print(\"Pareto Optimal: (A,A)\")\n",
    "\n",
    "plot_game_matrix(BATTLE_OF_SEXES, \"Battle of Sexes\")\n",
    "print(\"\\nBattle of Sexes:\")\n",
    "print(\"Agent 1\\\\Agent 2 | Action A | Action B\")\n",
    "print(\"Action A         |   2, 1   |  0, 0\")\n",
    "print(\"Action B         |   0, 0   |  1, 2\")\n",
    "print(\"\\nNash Equilibria: (A,A) and (B,B)\")\n",
    "print(\"Agent 1 prefers (A,A), Agent 2 prefers (B,B)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60cdea",
   "metadata": {},
   "source": [
    "## Multi-Agent Environment\n",
    "\n",
    "We'll create a flexible environment that can handle different game matrices and multiple agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790119cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "@dataclass\n",
    "class MultiAgentGame:\n",
    "    \"\"\"Multi-agent game environment\"\"\"\n",
    "    payoff_matrix: np.ndarray\n",
    "    num_agents: int = 2\n",
    "    num_actions: int = 2\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute actions and return rewards\"\"\"\n",
    "        if self.num_agents == 2:\n",
    "            return self.payoff_matrix[actions[0], actions[1]]\n",
    "        else:\n",
    "            # For more than 2 agents, we'll use a different structure\n",
    "            raise NotImplementedError(\"Only 2-agent games implemented\")\n",
    "    \n",
    "    def get_optimal_strategies(self):\n",
    "        \"\"\"Find Nash equilibria\"\"\"\n",
    "        equilibria = []\n",
    "        \n",
    "        # Check all pure strategy combinations\n",
    "        for a1 in range(self.num_actions):\n",
    "            for a2 in range(self.num_actions):\n",
    "                is_equilibrium = True\n",
    "                \n",
    "                # Check if agent 1 wants to deviate\n",
    "                for a1_dev in range(self.num_actions):\n",
    "                    if a1_dev != a1:\n",
    "                        if self.payoff_matrix[a1_dev, a2][0] > self.payoff_matrix[a1, a2][0]:\n",
    "                            is_equilibrium = False\n",
    "                            break\n",
    "                \n",
    "                # Check if agent 2 wants to deviate\n",
    "                if is_equilibrium:\n",
    "                    for a2_dev in range(self.num_actions):\n",
    "                        if a2_dev != a2:\n",
    "                            if self.payoff_matrix[a1, a2_dev][1] > self.payoff_matrix[a1, a2][1]:\n",
    "                                is_equilibrium = False\n",
    "                                break\n",
    "                \n",
    "                if is_equilibrium:\n",
    "                    equilibria.append((a1, a2))\n",
    "        \n",
    "        return equilibria\n",
    "\n",
    "# Test the environment\n",
    "pd_env = MultiAgentGame(PRISONERS_DILEMMA)\n",
    "print(\"Prisoner's Dilemma Nash Equilibria:\", pd_env.get_optimal_strategies())\n",
    "\n",
    "coord_env = MultiAgentGame(COORDINATION_GAME)\n",
    "print(\"Coordination Game Nash Equilibria:\", coord_env.get_optimal_strategies())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2db6b",
   "metadata": {},
   "source": [
    "## Independent Q-Learning (IQL)\n",
    "\n",
    "The simplest approach to multi-agent RL: each agent learns independently, treating other agents as part of the environment.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Each agent maintains its own Q-table\n",
    "- No communication between agents\n",
    "- Non-stationarity: environment changes as other agents learn\n",
    "- No convergence guarantees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "class IndependentQLearning:\n",
    "    \"\"\"Independent Q-Learning agent\"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions, learning_rate=0.1, epsilon=0.1, gamma=0.9):\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.Q = np.zeros(num_actions)\n",
    "        self.action_counts = np.zeros(num_actions)\n",
    "        \n",
    "    def get_action(self):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update Q-values\"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        self.Q[action] += self.lr * (reward - self.Q[action])\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent state\"\"\"\n",
    "        self.Q = np.zeros(self.num_actions)\n",
    "        self.action_counts = np.zeros(self.num_actions)\n",
    "\n",
    "class MultiAgentExperiment:\n",
    "    \"\"\"Run multi-agent experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agents, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.agents = agents\n",
    "        self.num_episodes = num_episodes\n",
    "        self.logs = []\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Run the experiment\"\"\"\n",
    "        for episode in tqdm(range(self.num_episodes), desc=\"Training\"):\n",
    "            # Get actions from all agents\n",
    "            actions = [agent.get_action() for agent in self.agents]\n",
    "            \n",
    "            # Execute actions and get rewards\n",
    "            rewards = self.env.step(actions)\n",
    "            \n",
    "            # Update all agents\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                agent.update(actions[i], rewards[i])\n",
    "            \n",
    "            # Log episode data\n",
    "            self.logs.append({\n",
    "                'episode': episode,\n",
    "                'actions': actions.copy(),\n",
    "                'rewards': rewards.copy(),\n",
    "                'agent1_action': actions[0],\n",
    "                'agent2_action': actions[1],\n",
    "                'agent1_reward': rewards[0],\n",
    "                'agent2_reward': rewards[1]\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(self.logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5380ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Test Independent Q-Learning on Prisoner's Dilemma\n",
    "print(\"Testing Independent Q-Learning on Prisoner's Dilemma...\")\n",
    "\n",
    "# Create two IQL agents\n",
    "agent1 = IndependentQLearning(num_actions=2, learning_rate=0.1, epsilon=0.1)\n",
    "agent2 = IndependentQLearning(num_actions=2, learning_rate=0.1, epsilon=0.1)\n",
    "\n",
    "# Run experiment\n",
    "experiment = MultiAgentExperiment(pd_env, [agent1, agent2], num_episodes=2000)\n",
    "logs = experiment.run()\n",
    "\n",
    "# Analyze results\n",
    "print(f\"\\nFinal Q-values:\")\n",
    "print(f\"Agent 1 Q-values: {agent1.Q}\")\n",
    "print(f\"Agent 2 Q-values: {agent2.Q}\")\n",
    "\n",
    "print(f\"\\nAction counts:\")\n",
    "print(f\"Agent 1 action counts: {agent1.action_counts}\")\n",
    "print(f\"Agent 2 action counts: {agent2.action_counts}\")\n",
    "\n",
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot rewards over time\n",
    "axes[0].plot(logs['episode'], logs['agent1_reward'], alpha=0.3, label='Agent 1', color='blue')\n",
    "axes[0].plot(logs['episode'], logs['agent2_reward'], alpha=0.3, label='Agent 2', color='red')\n",
    "\n",
    "# Plot moving averages\n",
    "window = 100\n",
    "logs['agent1_ma'] = logs['agent1_reward'].rolling(window=window).mean()\n",
    "logs['agent2_ma'] = logs['agent2_reward'].rolling(window=window).mean()\n",
    "\n",
    "axes[0].plot(logs['episode'], logs['agent1_ma'], label='Agent 1 (MA)', color='blue', linewidth=2)\n",
    "axes[0].plot(logs['episode'], logs['agent2_ma'], label='Agent 2 (MA)', color='red', linewidth=2)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('Reward Learning Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot action selection over time\n",
    "axes[1].plot(logs['episode'], logs['agent1_action'], alpha=0.3, label='Agent 1', color='blue')\n",
    "axes[1].plot(logs['episode'], logs['agent2_action'], alpha=0.3, label='Agent 2', color='red')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Action')\n",
    "axes[1].set_title('Action Selection Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3fd82a",
   "metadata": {},
   "source": [
    "**Q:** What do you observe about the convergence behavior of Independent Q-Learning in the Prisoner's Dilemma?\n",
    "\n",
    "**A:** Independent Q-Learning typically converges to the Nash Equilibrium (Defect, Defect) because each agent learns independently and discovers that defecting gives higher individual rewards regardless of the other agent's action. The agents don't coordinate or communicate, so they can't escape the individual rationality trap that leads to the suboptimal Nash equilibrium. The learning curves show high variance initially due to exploration, then stabilize around the equilibrium rewards (1,1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241254dd",
   "metadata": {},
   "source": [
    "## QMIX: Value Decomposition for Cooperative MARL\n",
    "\n",
    "QMIX addresses the credit assignment problem in cooperative multi-agent settings by decomposing the joint Q-function into individual Q-functions while ensuring monotonicity.\n",
    "\n",
    "**Key Properties:**\n",
    "- **Monotonicity**: ∂Q_tot/∂Q_i ≥ 0 for all agents i\n",
    "- **Decentralized Execution**: Each agent can act independently using its local Q-function\n",
    "- **Centralized Training**: Uses global state information during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f952be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "class QMIXAgent(nn.Module):\n",
    "    \"\"\"Individual Q-network for QMIX\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)\n",
    "\n",
    "class QMIXMixer(nn.Module):\n",
    "    \"\"\"QMIX mixing network ensuring monotonicity\"\"\"\n",
    "    \n",
    "    def __init__(self, num_agents, state_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        # Hypernetworks for mixing weights\n",
    "        self.hyper_w1 = nn.Linear(state_dim, num_agents * hidden_dim)\n",
    "        self.hyper_w2 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.hyper_b1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.hyper_b2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, q_vals, state):\n",
    "        \"\"\"\n",
    "        Mix individual Q-values into joint Q-value\n",
    "        Args:\n",
    "            q_vals: [batch_size, num_agents] individual Q-values\n",
    "            state: [batch_size, state_dim] global state\n",
    "        \"\"\"\n",
    "        batch_size = q_vals.size(0)\n",
    "        \n",
    "        # Generate mixing weights (ensure monotonicity with abs)\n",
    "        w1 = torch.abs(self.hyper_w1(state))\n",
    "        b1 = self.hyper_b1(state)\n",
    "        w2 = torch.abs(self.hyper_w2(state))\n",
    "        b2 = self.hyper_b2(state)\n",
    "        \n",
    "        # Reshape weights\n",
    "        w1 = w1.view(batch_size, self.num_agents, -1)\n",
    "        w2 = w2.view(batch_size, -1, 1)\n",
    "        \n",
    "        # Mixing computation\n",
    "        hidden = F.elu(torch.bmm(q_vals.unsqueeze(1), w1) + b1.unsqueeze(1))\n",
    "        q_tot = torch.bmm(hidden, w2) + b2.unsqueeze(1)\n",
    "        \n",
    "        return q_tot.squeeze(1)\n",
    "\n",
    "class QMIX:\n",
    "    \"\"\"QMIX algorithm implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_agents, obs_dim, action_dim, state_dim, lr=0.0005):\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = nn.ModuleList([\n",
    "            QMIXAgent(obs_dim, action_dim) for _ in range(num_agents)\n",
    "        ])\n",
    "        self.mixer = QMIXMixer(num_agents, state_dim)\n",
    "        \n",
    "        # Target networks\n",
    "        self.target_agents = nn.ModuleList([\n",
    "            QMIXAgent(obs_dim, action_dim) for _ in range(num_agents)\n",
    "        ])\n",
    "        self.target_mixer = QMIXMixer(num_agents, state_dim)\n",
    "        \n",
    "        # Copy parameters to target networks\n",
    "        self.update_target_networks()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.agents.parameters()) + list(self.mixer.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005  # Soft update parameter\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        \"\"\"Copy parameters to target networks\"\"\"\n",
    "        for i in range(self.num_agents):\n",
    "            self.target_agents[i].load_state_dict(self.agents[i].state_dict())\n",
    "        self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "    \n",
    "    def soft_update_target_networks(self):\n",
    "        \"\"\"Soft update target networks\"\"\"\n",
    "        for i in range(self.num_agents):\n",
    "            for param, target_param in zip(self.agents[i].parameters(), \n",
    "                                        self.target_agents[i].parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + \n",
    "                                      (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for param, target_param in zip(self.mixer.parameters(), \n",
    "                                     self.target_mixer.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + \n",
    "                                  (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def get_actions(self, observations, epsilon=0.0):\n",
    "        \"\"\"Get actions for all agents\"\"\"\n",
    "        actions = []\n",
    "        for i, obs in enumerate(observations):\n",
    "            if np.random.random() < epsilon:\n",
    "                actions.append(np.random.randint(self.agents[i].network[-1].out_features))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = self.agents[i](obs)\n",
    "                    actions.append(q_vals.argmax().item())\n",
    "        return actions\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update QMIX networks\"\"\"\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, state_batch, next_state_batch, done_batch = batch\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_vals = []\n",
    "        for i in range(self.num_agents):\n",
    "            q_vals = self.agents[i](obs_batch[i])\n",
    "            current_q_vals.append(q_vals.gather(1, action_batch[i].unsqueeze(1)).squeeze(1))\n",
    "        \n",
    "        current_q_vals = torch.stack(current_q_vals, dim=1)\n",
    "        current_q_tot = self.mixer(current_q_vals, state_batch)\n",
    "        \n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_vals = []\n",
    "            for i in range(self.num_agents):\n",
    "                next_q_vals.append(self.target_agents[i](next_obs_batch[i]).max(1)[0])\n",
    "            \n",
    "            next_q_vals = torch.stack(next_q_vals, dim=1)\n",
    "            next_q_tot = self.target_mixer(next_q_vals, next_state_batch)\n",
    "            target_q_tot = reward_batch + self.gamma * next_q_tot * (1 - done_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_tot, target_q_tot)\n",
    "        \n",
    "        # Update networks\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        self.soft_update_target_networks()\n",
    "        \n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26a57d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b12a6229",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
