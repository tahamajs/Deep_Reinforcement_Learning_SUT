{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d682ca85",
   "metadata": {},
   "source": [
    "<!-- Centered layout with a university logo -->\n",
    "<div align=\"center\">\n",
    "\n",
    "  <!-- University Logo -->\n",
    "  <img src=\"https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png\" width=\"180\" height=\"180\" style=\"margin-bottom: 10px;\">\n",
    "  \n",
    "  <!-- Assignment Title -->\n",
    "  <h1></h1>\n",
    "  <h1 style=\"color:#0F5298; font-size: 40px; font-weight: bold; margin-bottom: 5px;\">Deep Reinforcement Learning</h1>\n",
    "  <h2 style=\"color:#0F5298; font-size: 32px; font-weight: normal; margin-top: 0px;\">Assignment 10 - Multi-Agent Reinforcement Learning</h2>\n",
    "\n",
    "  <!-- Department and University -->\n",
    "  <h3 style=\"color:#696880; font-size: 24px; margin-top: 20px;\">Computer Engineering Department</h3>\n",
    "  <h3 style=\"color:#696880; font-size: 22px; margin-top: -5px;\">Sharif University of Technology</h3>\n",
    "\n",
    "  <!-- Semester -->\n",
    "  <h3 style=\"color:#696880; font-size: 22px; margin-top: 20px;\">Spring 2025</h3>\n",
    "\n",
    "  <!-- Authors -->\n",
    "  <h3 style=\"color:green; font-size: 22px; margin-top: 20px;\">Full name: [FULL_NAME]</h3>\n",
    "  <h3 style=\"color:green; font-size: 22px; margin-top: 20px;\">Student ID: [STUDENT_ID]</h3>\n",
    "\n",
    "  <!-- Horizontal Line for Separation -->\n",
    "  <hr style=\"border: 1px solid #0F5298; width: 80%; margin-top: 30px;\">\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d128e3",
   "metadata": {},
   "source": [
    "## Setup & Overview  \n",
    "In this notebook, we explore Multi-Agent Reinforcement Learning (MARL) through various algorithms and environments.  \n",
    "We implement and compare several approaches:\n",
    "- **Independent Q-Learning** (IQL) - Each agent learns independently\n",
    "- **QMIX** - Value decomposition for cooperative settings\n",
    "- **MADDPG** - Multi-Agent Actor-Critic for mixed environments\n",
    "- **Communication Protocols** - CommNet and TarMAC\n",
    "- **Self-Play** - Training against past versions\n",
    "\n",
    "We'll work with classic game theory environments like Prisoner's Dilemma and Coordination Games, then move to more complex multi-agent scenarios.\n",
    "\n",
    "Follow the instructions carefully and complete the sections marked with **TODO**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349a9b10",
   "metadata": {},
   "source": [
    "## Setup and Environment\n",
    "\n",
    "In the upcoming cells, we import necessary libraries, set up utility functions for reproducibility and plotting, and define the basic components of our multi-agent experiments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b921c616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque, defaultdict\n",
    "import itertools\n",
    "from dataclasses import dataclass\n",
    "from copy import deepcopy\n",
    "from tqdm.notebook import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9544b38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "def plot_logs(df, x_key, y_key, legend_key, **kwargs):\n",
    "    \"\"\"Plot learning curves for multi-agent experiments\"\"\"\n",
    "    num = len(df[legend_key].unique())\n",
    "    pal = sns.color_palette(\"hls\", num)\n",
    "    if 'palette' not in kwargs:\n",
    "        kwargs['palette'] = pal\n",
    "    ax = sns.lineplot(x=x_key, y=y_key, data=df, hue=legend_key, **kwargs)\n",
    "    return ax\n",
    "\n",
    "def plot_game_matrix(matrix, title=\"Game Matrix\", figsize=(8, 6)):\n",
    "    \"\"\"Plot a 2x2 game matrix\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Create heatmap\n",
    "    im = ax.imshow(matrix, cmap='RdYlBu', aspect='auto')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix.shape[1]):\n",
    "            text = ax.text(j, i, f'{matrix[i, j][0]:.1f}, {matrix[i, j][1]:.1f}',\n",
    "                         ha=\"center\", va=\"center\", color=\"black\", fontsize=12)\n",
    "    \n",
    "    # Set labels\n",
    "    ax.set_xticks([0, 1])\n",
    "    ax.set_yticks([0, 1])\n",
    "    ax.set_xticklabels(['Action 0', 'Action 1'])\n",
    "    ax.set_yticklabels(['Action 0', 'Action 1'])\n",
    "    ax.set_xlabel('Agent 2 Action')\n",
    "    ax.set_ylabel('Agent 1 Action')\n",
    "    ax.set_title(title)\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im)\n",
    "    cbar.set_label('Payoff Value')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def set_seed(s):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    np.random.seed(s)\n",
    "    random.seed(s)\n",
    "    torch.manual_seed(s)\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c649c952",
   "metadata": {},
   "source": [
    "## Game Theory Foundations\n",
    "\n",
    "We start with classic game theory environments to understand multi-agent interactions:\n",
    "\n",
    "### Prisoner's Dilemma\n",
    "- **Cooperation (C)**: Both agents cooperate, get moderate reward\n",
    "- **Defection (D)**: One agent defects while other cooperates, defector gets high reward\n",
    "- **Mutual Defection**: Both defect, get low reward\n",
    "\n",
    "### Coordination Game  \n",
    "- **Pure Coordination**: Both agents must choose same action for high reward\n",
    "- **Battle of Sexes**: Different preferences but coordination still beneficial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb5261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Define game matrices\n",
    "PRISONERS_DILEMMA = np.array([\n",
    "    [[3, 3], [0, 5]],  # Agent 1: C, Agent 2: C,D\n",
    "    [[5, 0], [1, 1]]   # Agent 1: D, Agent 2: C,D\n",
    "])\n",
    "\n",
    "COORDINATION_GAME = np.array([\n",
    "    [[2, 2], [0, 0]],   # Agent 1: A, Agent 2: A,B\n",
    "    [[0, 0], [1, 1]]    # Agent 1: B, Agent 2: A,B\n",
    "])\n",
    "\n",
    "BATTLE_OF_SEXES = np.array([\n",
    "    [[2, 1], [0, 0]],   # Agent 1: A, Agent 2: A,B\n",
    "    [[0, 0], [1, 2]]    # Agent 1: B, Agent 2: A,B\n",
    "])\n",
    "\n",
    "print(\"Prisoner's Dilemma Matrix:\")\n",
    "print(\"Agent 1\\\\Agent 2 | Cooperate | Defect\")\n",
    "print(\"Cooperate        |   3, 3    |  0, 5\")\n",
    "print(\"Defect           |   5, 0    |  1, 1\")\n",
    "print(\"\\nNash Equilibrium: (Defect, Defect)\")\n",
    "print(\"Pareto Optimal: (Cooperate, Cooperate)\")\n",
    "\n",
    "plot_game_matrix(PRISONERS_DILEMMA, \"Prisoner's Dilemma\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddb0a77",
   "metadata": {},
   "source": [
    "**Q:** Why is the Nash Equilibrium (Defect, Defect) suboptimal in the Prisoner's Dilemma?\n",
    "\n",
    "**A:** The Nash Equilibrium (Defect, Defect) is suboptimal because it represents a situation where both agents choose their individually rational strategy, but this leads to a worse outcome for both compared to mutual cooperation. Each agent defects because they fear being exploited if they cooperate while the other defects. However, if both could commit to cooperation, they would both be better off (3,3 vs 1,1). This illustrates the fundamental tension between individual rationality and collective welfare in competitive environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c32838a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "plot_game_matrix(COORDINATION_GAME, \"Coordination Game\")\n",
    "print(\"\\nCoordination Game:\")\n",
    "print(\"Agent 1\\\\Agent 2 | Action A | Action B\")\n",
    "print(\"Action A         |   2, 2   |  0, 0\")\n",
    "print(\"Action B         |   0, 0   |  1, 1\")\n",
    "print(\"\\nNash Equilibria: (A,A) and (B,B)\")\n",
    "print(\"Pareto Optimal: (A,A)\")\n",
    "\n",
    "plot_game_matrix(BATTLE_OF_SEXES, \"Battle of Sexes\")\n",
    "print(\"\\nBattle of Sexes:\")\n",
    "print(\"Agent 1\\\\Agent 2 | Action A | Action B\")\n",
    "print(\"Action A         |   2, 1   |  0, 0\")\n",
    "print(\"Action B         |   0, 0   |  1, 2\")\n",
    "print(\"\\nNash Equilibria: (A,A) and (B,B)\")\n",
    "print(\"Agent 1 prefers (A,A), Agent 2 prefers (B,B)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60cdea",
   "metadata": {},
   "source": [
    "## Multi-Agent Environment\n",
    "\n",
    "We'll create a flexible environment that can handle different game matrices and multiple agents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790119cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "@dataclass\n",
    "class MultiAgentGame:\n",
    "    \"\"\"Multi-agent game environment\"\"\"\n",
    "    payoff_matrix: np.ndarray\n",
    "    num_agents: int = 2\n",
    "    num_actions: int = 2\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute actions and return rewards\"\"\"\n",
    "        if self.num_agents == 2:\n",
    "            return self.payoff_matrix[actions[0], actions[1]]\n",
    "        else:\n",
    "            # For more than 2 agents, we'll use a different structure\n",
    "            raise NotImplementedError(\"Only 2-agent games implemented\")\n",
    "    \n",
    "    def get_optimal_strategies(self):\n",
    "        \"\"\"Find Nash equilibria\"\"\"\n",
    "        equilibria = []\n",
    "        \n",
    "        # Check all pure strategy combinations\n",
    "        for a1 in range(self.num_actions):\n",
    "            for a2 in range(self.num_actions):\n",
    "                is_equilibrium = True\n",
    "                \n",
    "                # Check if agent 1 wants to deviate\n",
    "                for a1_dev in range(self.num_actions):\n",
    "                    if a1_dev != a1:\n",
    "                        if self.payoff_matrix[a1_dev, a2][0] > self.payoff_matrix[a1, a2][0]:\n",
    "                            is_equilibrium = False\n",
    "                            break\n",
    "                \n",
    "                # Check if agent 2 wants to deviate\n",
    "                if is_equilibrium:\n",
    "                    for a2_dev in range(self.num_actions):\n",
    "                        if a2_dev != a2:\n",
    "                            if self.payoff_matrix[a1, a2_dev][1] > self.payoff_matrix[a1, a2][1]:\n",
    "                                is_equilibrium = False\n",
    "                                break\n",
    "                \n",
    "                if is_equilibrium:\n",
    "                    equilibria.append((a1, a2))\n",
    "        \n",
    "        return equilibria\n",
    "\n",
    "# Test the environment\n",
    "pd_env = MultiAgentGame(PRISONERS_DILEMMA)\n",
    "print(\"Prisoner's Dilemma Nash Equilibria:\", pd_env.get_optimal_strategies())\n",
    "\n",
    "coord_env = MultiAgentGame(COORDINATION_GAME)\n",
    "print(\"Coordination Game Nash Equilibria:\", coord_env.get_optimal_strategies())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c2db6b",
   "metadata": {},
   "source": [
    "## Independent Q-Learning (IQL)\n",
    "\n",
    "The simplest approach to multi-agent RL: each agent learns independently, treating other agents as part of the environment.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- Each agent maintains its own Q-table\n",
    "- No communication between agents\n",
    "- Non-stationarity: environment changes as other agents learn\n",
    "- No convergence guarantees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d2d11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "class IndependentQLearning:\n",
    "    \"\"\"Independent Q-Learning agent\"\"\"\n",
    "    \n",
    "    def __init__(self, num_actions, learning_rate=0.1, epsilon=0.1, gamma=0.9):\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.Q = np.zeros(num_actions)\n",
    "        self.action_counts = np.zeros(num_actions)\n",
    "        \n",
    "    def get_action(self):\n",
    "        \"\"\"Epsilon-greedy action selection\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        \"\"\"Update Q-values\"\"\"\n",
    "        self.action_counts[action] += 1\n",
    "        self.Q[action] += self.lr * (reward - self.Q[action])\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset agent state\"\"\"\n",
    "        self.Q = np.zeros(self.num_actions)\n",
    "        self.action_counts = np.zeros(self.num_actions)\n",
    "\n",
    "class MultiAgentExperiment:\n",
    "    \"\"\"Run multi-agent experiments\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agents, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.agents = agents\n",
    "        self.num_episodes = num_episodes\n",
    "        self.logs = []\n",
    "        \n",
    "    def run(self):\n",
    "        \"\"\"Run the experiment\"\"\"\n",
    "        for episode in tqdm(range(self.num_episodes), desc=\"Training\"):\n",
    "            # Get actions from all agents\n",
    "            actions = [agent.get_action() for agent in self.agents]\n",
    "            \n",
    "            # Execute actions and get rewards\n",
    "            rewards = self.env.step(actions)\n",
    "            \n",
    "            # Update all agents\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                agent.update(actions[i], rewards[i])\n",
    "            \n",
    "            # Log episode data\n",
    "            self.logs.append({\n",
    "                'episode': episode,\n",
    "                'actions': actions.copy(),\n",
    "                'rewards': rewards.copy(),\n",
    "                'agent1_action': actions[0],\n",
    "                'agent2_action': actions[1],\n",
    "                'agent1_reward': rewards[0],\n",
    "                'agent2_reward': rewards[1]\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(self.logs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5380ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "# Test Independent Q-Learning on Prisoner's Dilemma\n",
    "print(\"Testing Independent Q-Learning on Prisoner's Dilemma...\")\n",
    "\n",
    "# Create two IQL agents\n",
    "agent1 = IndependentQLearning(num_actions=2, learning_rate=0.1, epsilon=0.1)\n",
    "agent2 = IndependentQLearning(num_actions=2, learning_rate=0.1, epsilon=0.1)\n",
    "\n",
    "# Run experiment\n",
    "experiment = MultiAgentExperiment(pd_env, [agent1, agent2], num_episodes=2000)\n",
    "logs = experiment.run()\n",
    "\n",
    "# Analyze results\n",
    "print(f\"\\nFinal Q-values:\")\n",
    "print(f\"Agent 1 Q-values: {agent1.Q}\")\n",
    "print(f\"Agent 2 Q-values: {agent2.Q}\")\n",
    "\n",
    "print(f\"\\nAction counts:\")\n",
    "print(f\"Agent 1 action counts: {agent1.action_counts}\")\n",
    "print(f\"Agent 2 action counts: {agent2.action_counts}\")\n",
    "\n",
    "# Plot learning curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot rewards over time\n",
    "axes[0].plot(logs['episode'], logs['agent1_reward'], alpha=0.3, label='Agent 1', color='blue')\n",
    "axes[0].plot(logs['episode'], logs['agent2_reward'], alpha=0.3, label='Agent 2', color='red')\n",
    "\n",
    "# Plot moving averages\n",
    "window = 100\n",
    "logs['agent1_ma'] = logs['agent1_reward'].rolling(window=window).mean()\n",
    "logs['agent2_ma'] = logs['agent2_reward'].rolling(window=window).mean()\n",
    "\n",
    "axes[0].plot(logs['episode'], logs['agent1_ma'], label='Agent 1 (MA)', color='blue', linewidth=2)\n",
    "axes[0].plot(logs['episode'], logs['agent2_ma'], label='Agent 2 (MA)', color='red', linewidth=2)\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Reward')\n",
    "axes[0].set_title('Reward Learning Curves')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Plot action selection over time\n",
    "axes[1].plot(logs['episode'], logs['agent1_action'], alpha=0.3, label='Agent 1', color='blue')\n",
    "axes[1].plot(logs['episode'], logs['agent2_action'], alpha=0.3, label='Agent 2', color='red')\n",
    "axes[1].set_xlabel('Episode')\n",
    "axes[1].set_ylabel('Action')\n",
    "axes[1].set_title('Action Selection Over Time')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3fd82a",
   "metadata": {},
   "source": [
    "**Q:** What do you observe about the convergence behavior of Independent Q-Learning in the Prisoner's Dilemma?\n",
    "\n",
    "**A:** Independent Q-Learning typically converges to the Nash Equilibrium (Defect, Defect) because each agent learns independently and discovers that defecting gives higher individual rewards regardless of the other agent's action. The agents don't coordinate or communicate, so they can't escape the individual rationality trap that leads to the suboptimal Nash equilibrium. The learning curves show high variance initially due to exploration, then stabilize around the equilibrium rewards (1,1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241254dd",
   "metadata": {},
   "source": [
    "## QMIX: Value Decomposition for Cooperative MARL\n",
    "\n",
    "QMIX addresses the credit assignment problem in cooperative multi-agent settings by decomposing the joint Q-function into individual Q-functions while ensuring monotonicity.\n",
    "\n",
    "**Key Properties:**\n",
    "- **Monotonicity**: ∂Q_tot/∂Q_i ≥ 0 for all agents i\n",
    "- **Decentralized Execution**: Each agent can act independently using its local Q-function\n",
    "- **Centralized Training**: Uses global state information during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f952be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "class QMIXAgent(nn.Module):\n",
    "    \"\"\"Individual Q-network for QMIX\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)\n",
    "\n",
    "class QMIXMixer(nn.Module):\n",
    "    \"\"\"QMIX mixing network ensuring monotonicity\"\"\"\n",
    "    \n",
    "    def __init__(self, num_agents, state_dim, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        # Hypernetworks for mixing weights\n",
    "        self.hyper_w1 = nn.Linear(state_dim, num_agents * hidden_dim)\n",
    "        self.hyper_w2 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.hyper_b1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.hyper_b2 = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, q_vals, state):\n",
    "        \"\"\"\n",
    "        Mix individual Q-values into joint Q-value\n",
    "        Args:\n",
    "            q_vals: [batch_size, num_agents] individual Q-values\n",
    "            state: [batch_size, state_dim] global state\n",
    "        \"\"\"\n",
    "        batch_size = q_vals.size(0)\n",
    "        \n",
    "        # Generate mixing weights (ensure monotonicity with abs)\n",
    "        w1 = torch.abs(self.hyper_w1(state))\n",
    "        b1 = self.hyper_b1(state)\n",
    "        w2 = torch.abs(self.hyper_w2(state))\n",
    "        b2 = self.hyper_b2(state)\n",
    "        \n",
    "        # Reshape weights\n",
    "        w1 = w1.view(batch_size, self.num_agents, -1)\n",
    "        w2 = w2.view(batch_size, -1, 1)\n",
    "        \n",
    "        # Mixing computation\n",
    "        hidden = F.elu(torch.bmm(q_vals.unsqueeze(1), w1) + b1.unsqueeze(1))\n",
    "        q_tot = torch.bmm(hidden, w2) + b2.unsqueeze(1)\n",
    "        \n",
    "        return q_tot.squeeze(1)\n",
    "\n",
    "class QMIX:\n",
    "    \"\"\"QMIX algorithm implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, num_agents, obs_dim, action_dim, state_dim, lr=0.0005):\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = nn.ModuleList([\n",
    "            QMIXAgent(obs_dim, action_dim) for _ in range(num_agents)\n",
    "        ])\n",
    "        self.mixer = QMIXMixer(num_agents, state_dim)\n",
    "        \n",
    "        # Target networks\n",
    "        self.target_agents = nn.ModuleList([\n",
    "            QMIXAgent(obs_dim, action_dim) for _ in range(num_agents)\n",
    "        ])\n",
    "        self.target_mixer = QMIXMixer(num_agents, state_dim)\n",
    "        \n",
    "        # Copy parameters to target networks\n",
    "        self.update_target_networks()\n",
    "        \n",
    "        # Optimizers\n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.agents.parameters()) + list(self.mixer.parameters()),\n",
    "            lr=lr\n",
    "        )\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005  # Soft update parameter\n",
    "    \n",
    "    def update_target_networks(self):\n",
    "        \"\"\"Copy parameters to target networks\"\"\"\n",
    "        for i in range(self.num_agents):\n",
    "            self.target_agents[i].load_state_dict(self.agents[i].state_dict())\n",
    "        self.target_mixer.load_state_dict(self.mixer.state_dict())\n",
    "    \n",
    "    def soft_update_target_networks(self):\n",
    "        \"\"\"Soft update target networks\"\"\"\n",
    "        for i in range(self.num_agents):\n",
    "            for param, target_param in zip(self.agents[i].parameters(), \n",
    "                                        self.target_agents[i].parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + \n",
    "                                      (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for param, target_param in zip(self.mixer.parameters(), \n",
    "                                     self.target_mixer.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + \n",
    "                                  (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def get_actions(self, observations, epsilon=0.0):\n",
    "        \"\"\"Get actions for all agents\"\"\"\n",
    "        actions = []\n",
    "        for i, obs in enumerate(observations):\n",
    "            if np.random.random() < epsilon:\n",
    "                actions.append(np.random.randint(self.agents[i].network[-1].out_features))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_vals = self.agents[i](obs)\n",
    "                    actions.append(q_vals.argmax().item())\n",
    "        return actions\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update QMIX networks\"\"\"\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, state_batch, next_state_batch, done_batch = batch\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_vals = []\n",
    "        for i in range(self.num_agents):\n",
    "            q_vals = self.agents[i](obs_batch[i])\n",
    "            current_q_vals.append(q_vals.gather(1, action_batch[i].unsqueeze(1)).squeeze(1))\n",
    "        \n",
    "        current_q_vals = torch.stack(current_q_vals, dim=1)\n",
    "        current_q_tot = self.mixer(current_q_vals, state_batch)\n",
    "        \n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            next_q_vals = []\n",
    "            for i in range(self.num_agents):\n",
    "                next_q_vals.append(self.target_agents[i](next_obs_batch[i]).max(1)[0])\n",
    "            \n",
    "            next_q_vals = torch.stack(next_q_vals, dim=1)\n",
    "            next_q_tot = self.target_mixer(next_q_vals, next_state_batch)\n",
    "            target_q_tot = reward_batch + self.gamma * next_q_tot * (1 - done_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_tot, target_q_tot)\n",
    "        \n",
    "        # Update networks\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        self.soft_update_target_networks()\n",
    "        \n",
    "        return loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26a57d",
   "metadata": {},
   "source": [
    "## MADDPG: Multi-Agent Deep Deterministic Policy Gradient\n",
    "\n",
    "MADDPG extends DDPG to multi-agent settings using centralized training with decentralized execution (CTDE).\n",
    "\n",
    "**Key Features:**\n",
    "- **Centralized Critics**: Each agent's critic sees all observations and actions\n",
    "- **Decentralized Actors**: Each agent's actor only sees its own observations\n",
    "- **Handles Mixed Environments**: Works for both cooperative and competitive settings\n",
    "- **Addresses Non-stationarity**: Critics use global information during training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78561c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor network for MADDPG\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()  # Actions in [-1, 1]\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.network(obs)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Centralized critic network for MADDPG\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, num_agents, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.num_agents = num_agents\n",
    "        total_obs_dim = obs_dim * num_agents\n",
    "        total_action_dim = action_dim * num_agents\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(total_obs_dim + total_action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, actions):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs: [batch_size, num_agents * obs_dim] concatenated observations\n",
    "            actions: [batch_size, num_agents * action_dim] concatenated actions\n",
    "        \"\"\"\n",
    "        x = torch.cat([obs, actions], dim=-1)\n",
    "        return self.network(x)\n",
    "\n",
    "class MADDPGAgent:\n",
    "    \"\"\"Individual MADDPG agent\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id, obs_dim, action_dim, num_agents, lr=0.001, tau=0.005):\n",
    "        self.agent_id = agent_id\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = Actor(obs_dim, action_dim)\n",
    "        self.critic = Critic(obs_dim, action_dim, num_agents)\n",
    "        self.target_actor = Actor(obs_dim, action_dim)\n",
    "        self.target_critic = Critic(obs_dim, action_dim, num_agents)\n",
    "        \n",
    "        # Copy parameters to target networks\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        self.tau = tau\n",
    "        self.gamma = 0.99\n",
    "    \n",
    "    def act(self, obs, noise=0.0):\n",
    "        \"\"\"Get action with optional noise\"\"\"\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(obs)\n",
    "            if noise > 0:\n",
    "                action += torch.randn_like(action) * noise\n",
    "            return torch.clamp(action, -1, 1)\n",
    "    \n",
    "    def update(self, batch, other_agents):\n",
    "        \"\"\"Update actor and critic networks\"\"\"\n",
    "        obs_batch, action_batch, reward_batch, next_obs_batch, done_batch = batch\n",
    "        \n",
    "        # Prepare centralized inputs\n",
    "        all_obs = torch.cat([obs_batch[i] for i in range(self.num_agents)], dim=1)\n",
    "        all_actions = torch.cat([action_batch[i] for i in range(self.num_agents)], dim=1)\n",
    "        all_next_obs = torch.cat([next_obs_batch[i] for i in range(self.num_agents)], dim=1)\n",
    "        \n",
    "        # Current Q-value\n",
    "        current_q = self.critic(all_obs, all_actions)\n",
    "        \n",
    "        # Target Q-value\n",
    "        with torch.no_grad():\n",
    "            next_actions = []\n",
    "            for i in range(self.num_agents):\n",
    "                if i == self.agent_id:\n",
    "                    next_actions.append(self.target_actor(next_obs_batch[i]))\n",
    "                else:\n",
    "                    next_actions.append(other_agents[i].target_actor(next_obs_batch[i]))\n",
    "            \n",
    "            all_next_actions = torch.cat(next_actions, dim=1)\n",
    "            target_q = reward_batch[self.agent_id] + self.gamma * self.target_critic(all_next_obs, all_next_actions) * (1 - done_batch)\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        # Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor loss\n",
    "        current_actions = []\n",
    "        for i in range(self.num_agents):\n",
    "            if i == self.agent_id:\n",
    "                current_actions.append(self.actor(obs_batch[i]))\n",
    "            else:\n",
    "                current_actions.append(action_batch[i])\n",
    "        \n",
    "        all_current_actions = torch.cat(current_actions, dim=1)\n",
    "        actor_loss = -self.critic(all_obs, all_current_actions).mean()\n",
    "        \n",
    "        # Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        self.soft_update_target_networks()\n",
    "        \n",
    "        return critic_loss.item(), actor_loss.item()\n",
    "    \n",
    "    def soft_update_target_networks(self):\n",
    "        \"\"\"Soft update target networks\"\"\"\n",
    "        for param, target_param in zip(self.actor.parameters(), self.target_actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for param, target_param in zip(self.critic.parameters(), self.target_critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "class MADDPG:\n",
    "    \"\"\"MADDPG algorithm coordinator\"\"\"\n",
    "    \n",
    "    def __init__(self, num_agents, obs_dim, action_dim, lr=0.001, tau=0.005):\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [\n",
    "            MADDPGAgent(i, obs_dim, action_dim, num_agents, lr, tau)\n",
    "            for i in range(num_agents)\n",
    "        ]\n",
    "    \n",
    "    def act(self, observations, noise=0.0):\n",
    "        \"\"\"Get actions for all agents\"\"\"\n",
    "        actions = []\n",
    "        for i, obs in enumerate(observations):\n",
    "            action = self.agents[i].act(obs, noise)\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update all agents\"\"\"\n",
    "        losses = []\n",
    "        for i in range(self.num_agents):\n",
    "            other_agents = [self.agents[j] for j in range(self.num_agents) if j != i]\n",
    "            critic_loss, actor_loss = self.agents[i].update(batch, other_agents)\n",
    "            losses.append((critic_loss, actor_loss))\n",
    "        return losses\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb5d70d",
   "metadata": {},
   "source": [
    "## Communication Protocols\n",
    "\n",
    "Communication is crucial for coordination in multi-agent systems. We implement two key approaches:\n",
    "\n",
    "### CommNet: Emergent Communication\n",
    "- Agents learn to communicate through continuous messages\n",
    "- No predefined communication protocol\n",
    "- Communication emerges from the task requirements\n",
    "\n",
    "### TarMAC: Targeted Multi-Agent Communication\n",
    "- Attention-based communication mechanism\n",
    "- Agents can selectively attend to messages from other agents\n",
    "- More sophisticated than simple averaging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bca790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [code]\n",
    "class CommNetAgent(nn.Module):\n",
    "    \"\"\"CommNet agent with emergent communication\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, comm_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.comm_dim = comm_dim\n",
    "        \n",
    "        # Observation encoder\n",
    "        self.obs_encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, comm_dim)\n",
    "        )\n",
    "        \n",
    "        # Communication GRU\n",
    "        self.comm_gru = nn.GRU(comm_dim, comm_dim, batch_first=True)\n",
    "        \n",
    "        # Action decoder\n",
    "        self.action_decoder = nn.Sequential(\n",
    "            nn.Linear(comm_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, comm_hidden=None, num_comm_rounds=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs: [batch_size, obs_dim] agent observation\n",
    "            comm_hidden: [batch_size, comm_dim] communication hidden state\n",
    "            num_comm_rounds: number of communication rounds\n",
    "        \"\"\"\n",
    "        # Encode observation\n",
    "        encoded_obs = self.obs_encoder(obs)\n",
    "        \n",
    "        if comm_hidden is None:\n",
    "            comm_hidden = encoded_obs\n",
    "        \n",
    "        # Communication rounds\n",
    "        for _ in range(num_comm_rounds):\n",
    "            # Average communication (simplified CommNet)\n",
    "            comm_input = comm_hidden.mean(dim=0, keepdim=True).expand_as(comm_hidden)\n",
    "            comm_hidden, _ = self.comm_gru(comm_input.unsqueeze(1), comm_hidden.unsqueeze(0))\n",
    "            comm_hidden = comm_hidden.squeeze(0)\n",
    "        \n",
    "        # Decode to action\n",
    "        action_logits = self.action_decoder(comm_hidden)\n",
    "        return action_logits, comm_hidden\n",
    "\n",
    "class TarMACAgent(nn.Module):\n",
    "    \"\"\"TarMAC agent with attention-based communication\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, comm_dim=32, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.comm_dim = comm_dim\n",
    "        \n",
    "        # Observation encoder\n",
    "        self.obs_encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, comm_dim)\n",
    "        )\n",
    "        \n",
    "        # Attention components\n",
    "        self.query_net = nn.Linear(comm_dim, comm_dim)\n",
    "        self.key_net = nn.Linear(comm_dim, comm_dim)\n",
    "        self.value_net = nn.Linear(comm_dim, comm_dim)\n",
    "        \n",
    "        # Action decoder\n",
    "        self.action_decoder = nn.Sequential(\n",
    "            nn.Linear(comm_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, other_messages=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            obs: [batch_size, obs_dim] agent observation\n",
    "            other_messages: [batch_size, num_others, comm_dim] messages from other agents\n",
    "        \"\"\"\n",
    "        # Encode observation\n",
    "        encoded_obs = self.obs_encoder(obs)\n",
    "        \n",
    "        if other_messages is None:\n",
    "            # No communication, use only own observation\n",
    "            attended_message = encoded_obs\n",
    "        else:\n",
    "            # Compute attention\n",
    "            query = self.query_net(encoded_obs)  # [batch_size, comm_dim]\n",
    "            keys = self.key_net(other_messages)  # [batch_size, num_others, comm_dim]\n",
    "            values = self.value_net(other_messages)  # [batch_size, num_others, comm_dim]\n",
    "            \n",
    "            # Attention weights\n",
    "            scores = torch.bmm(query.unsqueeze(1), keys.transpose(1, 2))  # [batch_size, 1, num_others]\n",
    "            attention_weights = F.softmax(scores, dim=-1)  # [batch_size, 1, num_others]\n",
    "            \n",
    "            # Weighted sum of values\n",
    "            attended_message = torch.bmm(attention_weights, values).squeeze(1)  # [batch_size, comm_dim]\n",
    "            \n",
    "            # Combine with own observation\n",
    "            attended_message = attended_message + encoded_obs\n",
    "        \n",
    "        # Decode to action\n",
    "        action_logits = self.action_decoder(attended_message)\n",
    "        return action_logits\n",
    "\n",
    "class CommunicationExperiment:\n",
    "    \"\"\"Experiment framework for communication protocols\"\"\"\n",
    "    \n",
    "    def __init__(self, env, agents, num_episodes=1000):\n",
    "        self.env = env\n",
    "        self.agents = agents\n",
    "        self.num_episodes = num_episodes\n",
    "        self.logs = []\n",
    "    \n",
    "    def run_commnet_experiment(self):\n",
    "        \"\"\"Run CommNet experiment\"\"\"\n",
    "        print(\"Running CommNet experiment...\")\n",
    "        \n",
    "        for episode in tqdm(range(self.num_episodes), desc=\"CommNet Training\"):\n",
    "            # Initialize communication hidden states\n",
    "            comm_hidden = None\n",
    "            \n",
    "            # Get actions with communication\n",
    "            actions = []\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                obs = torch.randn(1, 2)  # Simplified observation\n",
    "                action_logits, comm_hidden = agent(obs, comm_hidden)\n",
    "                action = Categorical(logits=action_logits).sample().item()\n",
    "                actions.append(action)\n",
    "            \n",
    "            # Execute actions\n",
    "            rewards = self.env.step(actions)\n",
    "            \n",
    "            # Log episode\n",
    "            self.logs.append({\n",
    "                'episode': episode,\n",
    "                'actions': actions,\n",
    "                'rewards': rewards,\n",
    "                'method': 'CommNet'\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(self.logs)\n",
    "    \n",
    "    def run_tarmac_experiment(self):\n",
    "        \"\"\"Run TarMAC experiment\"\"\"\n",
    "        print(\"Running TarMAC experiment...\")\n",
    "        \n",
    "        for episode in tqdm(range(self.num_episodes), desc=\"TarMAC Training\"):\n",
    "            # Collect messages from all agents\n",
    "            messages = []\n",
    "            for agent in self.agents:\n",
    "                obs = torch.randn(1, 2)  # Simplified observation\n",
    "                encoded_obs = agent.obs_encoder(obs)\n",
    "                messages.append(encoded_obs)\n",
    "            \n",
    "            # Get actions with attention\n",
    "            actions = []\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                obs = torch.randn(1, 2)\n",
    "                other_messages = torch.stack([m for j, m in enumerate(messages) if j != i])\n",
    "                other_messages = other_messages.unsqueeze(0)  # Add batch dimension\n",
    "                \n",
    "                action_logits = agent(obs, other_messages)\n",
    "                action = Categorical(logits=action_logits).sample().item()\n",
    "                actions.append(action)\n",
    "            \n",
    "            # Execute actions\n",
    "            rewards = self.env.step(actions)\n",
    "            \n",
    "            # Log episode\n",
    "            self.logs.append({\n",
    "                'episode': episode,\n",
    "                'actions': actions,\n",
    "                'rewards': rewards,\n",
    "                'method': 'TarMAC'\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(self.logs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a6229",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
