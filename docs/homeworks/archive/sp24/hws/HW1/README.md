# HW1: Introduction to Reinforcement Learning

## ğŸ“‹ Overview

This introductory assignment covers fundamental RL concepts and tabular methods, establishing the foundation for deep reinforcement learning.

## ğŸ“‚ Contents

```
HW1/
â”œâ”€â”€ RL_HW1.pdf                  # Assignment questions
â”œâ”€â”€ RL_HW1_Solution.pdf         # Complete solutions
â”œâ”€â”€ SP24_RL_HW1.zip            # Assignment package
â””â”€â”€ SP24_RL_HW1_Solution.zip   # Solution package
```

## ğŸ¯ Learning Objectives

- âœ… Master MDP formulation and Bellman equations
- âœ… Implement dynamic programming algorithms
- âœ… Understand temporal difference learning
- âœ… Apply tabular RL methods to GridWorld
- âœ… Analyze convergence properties

## ğŸ“š Topics Covered

### Part 1: Theoretical Foundations

**Markov Decision Processes:**

- State space, action space, transitions
- Reward function and discount factor
- Value functions (V and Q)
- Bellman equations

**Policy Evaluation and Improvement:**

- Policy iteration algorithm
- Value iteration algorithm
- Convergence guarantees

### Part 2: Tabular Methods

**Monte Carlo Methods:**

- First-visit vs every-visit MC
- MC policy evaluation
- MC control with exploring starts

**Temporal Difference Learning:**

- TD(0) prediction
- SARSA (on-policy TD control)
- Q-Learning (off-policy TD control)
- n-step TD methods

### Part 3: Implementation

**GridWorld Environment:**

```python
class GridWorld:
    # States: Grid positions
    # Actions: {up, down, left, right}
    # Rewards: Goal (+1), obstacle (-1), step (0)
    # Dynamics: Deterministic or stochastic transitions
```

**Key Algorithms to Implement:**

1. Value Iteration
2. Policy Iteration
3. Q-Learning
4. SARSA

## ğŸ“ Assignment Structure

### Theoretical Questions (30%)

- MDP formulation problems
- Bellman equation derivations
- Policy and value function analysis
- Convergence proofs

### Implementation (50%)

- Dynamic programming algorithms
- TD learning methods
- Experiments on GridWorld
- Performance comparisons

### Analysis (20%)

- Learning curves and convergence
- Algorithm comparison
- Hyperparameter sensitivity
- Discussion of results

## ğŸ’¡ Key Concepts

**Value Iteration:**

```
Initialize V(s) = 0
Repeat:
    V(s) â† max_a âˆ‘_{s',r} p(s',r|s,a)[r + Î³V(s')]
Until converged
Extract policy: Ï€(s) = argmax_a Q(s,a)
```

**Q-Learning:**

```
Initialize Q(s,a) = 0
For each episode:
    For each step:
        a = Îµ-greedy(s, Q)
        Take action a, observe r, s'
        Q(s,a) â† Q(s,a) + Î±[r + Î³ max_a' Q(s',a') - Q(s,a)]
```

## ğŸ“– References

- **Sutton & Barto (2018)** - Chapters 3-6
- [Berkeley CS285 Lecture Notes](http://rail.eecs.berkeley.edu/deeprlcourse/)

## â±ï¸ Time Estimate

- Reading & Theory: 4-6 hours
- Implementation: 6-10 hours
- Experiments & Analysis: 3-5 hours
- **Total**: 13-21 hours

---

**Difficulty**: â­â­â˜†â˜†â˜† (Foundational)  
**Prerequisites**: Probability, linear algebra, Python  
**Key Skills**: MDP analysis, dynamic programming, TD learning

