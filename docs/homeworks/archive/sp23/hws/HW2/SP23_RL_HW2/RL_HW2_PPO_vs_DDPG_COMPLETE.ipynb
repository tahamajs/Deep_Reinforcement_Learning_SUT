{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eloi8xzEKxK"
      },
      "source": [
        "# Configuration & Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1wxOlYMmEKxQ"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !apt install python-opengl\n",
        "    !apt install ffmpeg\n",
        "    !apt install xvfb\n",
        "    !pip install pyvirtualdisplay\n",
        "    from pyvirtualdisplay import Display\n",
        "    \n",
        "    # Start virtual display\n",
        "    dis = Display(visible=0, size=(600, 400))\n",
        "    dis.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gkq77hRTNnOq"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZySGWaENnOw"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "from typing import Deque, Dict, List, Tuple\n",
        "\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0guayZuNnOx"
      },
      "source": [
        "## Set random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-jU9MarNnOx"
      },
      "outputs": [],
      "source": [
        "if torch.backends.cudnn.enabled:\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "seed = 777\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTevK5iRNnOx"
      },
      "source": [
        "## Replay buffer\n",
        "Typically, people implement replay buffers with one of the following three data structures:\n",
        "\n",
        "- collections.deque\n",
        "- list\n",
        "- numpy.ndarray\n",
        "\n",
        "**deque** is very easy to handle once you initialize its maximum length (e.g. deque(maxlen=buffer_size)). However, the indexing operation of deque gets terribly slow as it grows up because it is [internally doubly linked list](https://wiki.python.org/moin/TimeComplexity#collections.deque). On the other hands, **list** is an array, so it is relatively faster than deque when you sample batches at every step. Its amortized cost of Get item is [O(1)](https://wiki.python.org/moin/TimeComplexity#list).\n",
        "\n",
        "Last but not least, let's see **numpy.ndarray**. numpy.ndarray is even faster than list due to the fact that it is [a homogeneous array of fixed-size items](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html#numpy.ndarray), so you can get the benefits of [locality of reference](https://en.wikipedia.org/wiki/Locality_of_reference), . Whereas list is an array of pointers to objects, even when all of them are of the same type.\n",
        "\n",
        "Here, we are going to implement a replay buffer using numpy.ndarray.\n",
        "\n",
        "Reference: \n",
        "- [OpenAI spinning-up](https://github.com/openai/spinningup/blob/master/spinup/algos/sac/sac.py#L10)\n",
        "- [rainbow-is-all-you-need](https://render.githubusercontent.com/view/ipynb?commit=032d11277cf2436853478a69ca5a4aba03202598&enc_url=68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f437572742d5061726b2f7261696e626f772d69732d616c6c2d796f752d6e6565642f303332643131323737636632343336383533343738613639636135613461626130333230323539382f30312e64716e2e6970796e62&nwo=Curt-Park%2Frainbow-is-all-you-need&path=01.dqn.ipynb&repository_id=191133946&repository_type=Repository#Replay-buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTSyz91HNnOy"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
        "\n",
        "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
        "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.done_buf = np.zeros([size], dtype=np.float32)\n",
        "        self.max_size, self.batch_size = size, batch_size\n",
        "        self.ptr, self.size = 0, 0\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "    def store(self,\n",
        "        obs: np.ndarray,\n",
        "        act: np.ndarray, \n",
        "        rew: float, \n",
        "        next_obs: np.ndarray, \n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"Store the transition in buffer.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        self.obs_buf[self.ptr] = obs\n",
        "        self.next_obs_buf[self.ptr] = next_obs\n",
        "        self.acts_buf[self.ptr] = act\n",
        "        self.rews_buf[self.ptr] = rew\n",
        "        self.done_buf[self.ptr] = done\n",
        "        self.ptr = (self.ptr + 1) % self.max_size\n",
        "        self.size = min(self.size + 1, self.max_size)\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
        "        return dict(\n",
        "            obs=self.obs_buf[idxs],\n",
        "            next_obs=self.next_obs_buf[idxs],\n",
        "            acts=self.acts_buf[idxs],\n",
        "            rews=self.rews_buf[idxs],\n",
        "            done=self.done_buf[idxs]\n",
        "        )\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        return self.size\n",
        "        # ==================================== Your Code (End) ====================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation: Replay Buffer Implementation\n",
        "\n",
        "The **ReplayBuffer** is a critical component for off-policy algorithms like DDPG. It stores past experiences (state, action, reward, next_state, done) and allows the agent to learn from random mini-batches of experiences.\n",
        "\n",
        "**Key Features:**\n",
        "- Uses `numpy.ndarray` for efficient memory access (O(1) indexing)\n",
        "- Circular buffer implementation: when full, oldest experiences are overwritten\n",
        "- Random sampling breaks temporal correlations in sequential data\n",
        "- Stores transitions in separate arrays for each component\n",
        "\n",
        "**Why Replay Buffer?**\n",
        "- **Stability**: Random sampling decorrelates sequential experiences\n",
        "- **Efficiency**: Reuses past experiences multiple times\n",
        "- **Memory**: Fixed size prevents unbounded memory growth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ-eZmnsNnOy"
      },
      "source": [
        "## OU Noise\n",
        "**Ornstein-Uhlenbeck** process generates temporally correlated exploration, and it effectively copes with physical control problems of inertia.\n",
        "\n",
        "$$\n",
        "dx_t = \\theta(\\mu - x_t) dt + \\sigma dW_t\n",
        "$$\n",
        "\n",
        "Reference: \n",
        "- [Udacity github](https://github.com/udacity/deep-reinforcement-learning/blob/master/ddpg-pendulum/ddpg_agent.py)\n",
        "- [Wiki](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgdO5XcUNnOy"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "class OUNoise:\n",
        "    \"\"\"Ornstein-Uhlenbeck process.\n",
        "    Taken from Udacity deep-reinforcement-learning github repository:\n",
        "    https://github.com/udacity/deep-reinforcement-learning/blob/master/\n",
        "    ddpg-pendulum/ddpg_agent.py\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, \n",
        "        size: int, \n",
        "        mu: float = 0.0, \n",
        "        theta: float = 0.15, \n",
        "        sigma: float = 0.2,\n",
        "    ):\n",
        "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
        "        self.state = np.float64(0.0)\n",
        "        self.mu = mu * np.ones(size)\n",
        "        self.theta = theta\n",
        "        self.sigma = sigma\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
        "        self.state = copy.copy(self.mu)\n",
        "\n",
        "    def sample(self) -> np.ndarray:\n",
        "        \"\"\"Update internal state and return it as a noise sample.\"\"\"\n",
        "        x = self.state\n",
        "        dx = self.theta * (self.mu - x) + self.sigma * np.array(\n",
        "            [random.random() for _ in range(len(x))]\n",
        "        )\n",
        "        self.state = x + dx\n",
        "        return self.state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation: Ornstein-Uhlenbeck Noise\n",
        "\n",
        "The **OU Noise** generates temporally correlated exploration noise, which is more effective than uncorrelated Gaussian noise for physical control tasks with inertia.\n",
        "\n",
        "**Formula:** $dx_t = \\theta(\\mu - x_t) dt + \\sigma dW_t$\n",
        "\n",
        "**Parameters:**\n",
        "- $\\mu$: mean value (usually 0)\n",
        "- $\\theta$: rate of mean reversion (how quickly noise returns to mean)\n",
        "- $\\sigma$: volatility/scale of random fluctuations\n",
        "\n",
        "**Why OU Noise for DDPG?**\n",
        "- Creates smooth, correlated exploration\n",
        "- Better for continuous control with momentum/inertia\n",
        "- Prevents abrupt action changes that could destabilize physical systems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTZ15C_3EKxT"
      },
      "source": [
        "# PPO\n",
        "\n",
        "- PPO: [J. Schulman et al., \"Proximal Policy Optimization Algorithms.\" arXiv preprint arXiv:1707.06347, 2017.](https://arxiv.org/abs/1707.06347.pdf)\n",
        "- TRPO: [Schulman, John, et al. \"Trust region policy optimization.\" International conference on machine learning. 2015.](http://proceedings.mlr.press/v37/schulman15.pdf)\n",
        "\n",
        "There are two kinds of algorithms of PPO: PPO-Penalty and PPO-Clip. Here, we'll implement PPO-clip version.\n",
        "\n",
        "TRPO computes the gradients with a complex second-order method. On the other hand, PPO tries to solve the problem with a first-order methods that keep new policies close to old. To simplify the surrogate objective, let $r(\\theta)$ denote the probability ratio\n",
        "\n",
        "$$ L^{CPI}(\\theta) = \\hat {\\mathbb{E}}_t \\left [ {\\pi_\\theta(a_t|s_t) \\over \\pi_{\\theta_{old}}(a_t|s_t)} \\hat A_t\\right] = \\hat {\\mathbb{E}}_t \\left [ r_t(\\theta) \\hat A_t \\right ].$$\n",
        "\n",
        "The objective is penalized further away from $r_t(\\theta)$\n",
        "\n",
        "$$ L^{CLIP}(\\theta)=\\hat {\\mathbb{E}}_t \\left [ \\min(r_t(\\theta) \\hat A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat A_t) \\right ] $$\n",
        "\n",
        "If the advantage is positive, the objective will increase. As a result, the action becomes more likely. If advantage is negative, the objective will decrease. AS a result, the action becomes less likely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation: PPO Network Architecture\n",
        "\n",
        "**PPO Actor Network:**\n",
        "- Outputs a **stochastic policy**: mean ($\\mu$) and standard deviation ($\\sigma$) of a Gaussian distribution\n",
        "- Uses `tanh` on mean to bound actions to [-1, 1]\n",
        "- `log_std` is clamped to prevent numerical instability\n",
        "- Samples actions from $\\mathcal{N}(\\mu, \\sigma^2)$ for exploration\n",
        "\n",
        "**Critic Network:**\n",
        "- Estimates the state value function $V(s)$\n",
        "- Used to compute advantages: $A(s,a) = Q(s,a) - V(s)$\n",
        "- Simpler than Q-networks (doesn't need action as input)\n",
        "\n",
        "**Key Design Choices:**\n",
        "- Separate networks for actor and critic (more stable than shared)\n",
        "- Uniform initialization on final layers (prevents large initial gradients)\n",
        "- ReLU activations in hidden layers (standard for deep RL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKyhiD5yFSyO"
      },
      "source": [
        "# DDPG \n",
        "\n",
        "[T. P. Lillicrap et al., \"Continuous control with deep reinforcement learning.\" arXiv preprint arXiv:1509.02971, 2015.](https://arxiv.org/pdf/1509.02971.pdf)\n",
        "\n",
        "Deep Q Network(DQN)([Mnih et al., 2013;2015](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)) algorithm is combined advances in deep learning with reinforcement learning. However, while DQN solves problems with high-dimentional observation spaces, it can only handle discrete and low-dimentional action spaces because of using greedy policy. For learning in high-dimentional and continous action spaces, the authors combine the actor-critic approach with insights from the recent success of DQN. Deep DPG(DDPG) is based on the deterministic policy gradient(DPG) algorithm ([Silver et al., 2014](http://proceedings.mlr.press/v32/silver14.pdf)). \n",
        "\n",
        "### Deterministic policy gradient\n",
        "The DPG algorithm maintains a parameterized actor function $\\mu(s|\\theta^{\\mu})$ which specifies the current policy by deterministically mapping states to a specific action. The critic $Q(s, a)$ is learned using the Bellman equation as in Q-learning. The actor is updated by following the applying the chain rule to the expected return from the start distribution $J$ with respect to the actor parameters\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\nabla_{\\theta^{\\mu}}J &\\approx E_{s_t\\sim\\rho^\\beta} [\\nabla_{\\theta^{\\mu}} Q(s,a|\\theta^Q)|_{s=s_t, a=\\mu(s_t|\\theta^\\mu)}] \\\\\n",
        "&= E_{s_t\\sim\\rho^\\beta} [\\nabla_{a} Q(s,a|\\theta^Q)|_{s=s_t, a=\\mu(s_t)} \\nabla_{\\theta^{\\mu}} \\mu(s|\\theta^\\mu)|_{s=s_t}]\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "### Soft update target network\n",
        "Since the network $(Q(s,a|\\theta^Q)$ being updated is also used in calculating the target value, the Q update is prone to divergence. To avoid this, the authors use **the target network** like DQN, but modified for actor-critic and using **soft target updates**. Target netwokrs is created by copying the actor and critic networks, $Q'(s,a|\\theta^{Q'})$ and $\\mu'(s|\\theta^{\\mu`})$ respectively, that are used for calculating the target values. The weights of these target networks are then updated by having them slowly track the learned networks:\n",
        "\n",
        "$$\n",
        "\\theta' \\leftarrow \\tau \\theta + (1 - \\tau)\\theta' \\ \\ \\ {with} \\ \\tau \\ll 1.\n",
        "$$\n",
        "\n",
        "It greatly improves the stability of learning.\n",
        "\n",
        "### Exploration for continuous action space\n",
        "An advantage of offpolicies algorithms such as DDPG is that we can treat the problem of exploration independently from the learning algorithm. The authors construct an exploration policy $\\mu'$ by adding noise sampled from a noise process $\\mathcal{N}$ to the actor policy\n",
        "\n",
        "$$\n",
        "\\mu'(s_t) = \\mu(s_t|\\theta^{\\mu}_t) + \\mathcal{N}\n",
        "$$\n",
        "\n",
        "$\\mathcal{N}$ can be chosen to suit the environment. The authors used **Ornstein-Uhlenbeck process** to generate temporally correlated exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation: DDPG Network Architecture\n",
        "\n",
        "**DDPG Actor Network:**\n",
        "- Outputs a **deterministic policy**: $\\mu(s)$\n",
        "- Uses `tanh` activation to bound actions to [-1, 1]\n",
        "- No stochasticity in the policy (exploration via noise)\n",
        "\n",
        "**DDPG Critic Network:**\n",
        "- Estimates the Q-function: $Q(s, a)$\n",
        "- Takes **both state and action** as input (concatenated)\n",
        "- Used to guide actor training via policy gradient\n",
        "\n",
        "**DDPG vs PPO Networks:**\n",
        "| Aspect | PPO | DDPG |\n",
        "|--------|-----|------|\n",
        "| Actor Output | Stochastic (\u03bc, \u03c3) | Deterministic (\u03bc) |\n",
        "| Critic Input | State only | State + Action |\n",
        "| Critic Output | V(s) | Q(s,a) |\n",
        "| Exploration | Policy entropy | External noise |\n",
        "\n",
        "**Target Networks:**\n",
        "DDPG uses target networks (not shown in architecture, but used in training) to stabilize learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkVtrKceEKxW"
      },
      "source": [
        "# Networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation: PPO Mini-Batch Iterator\n",
        "\n",
        "The `ppo_iter` function yields mini-batches from collected rollout data for multiple epochs.\n",
        "\n",
        "**Why Multiple Epochs?**\n",
        "- PPO is **on-policy** but reuses data multiple times\n",
        "- Clipping prevents too-large policy updates\n",
        "- More sample efficient than vanilla policy gradients\n",
        "\n",
        "**Process:**\n",
        "1. Collect `rollout_len` steps of experience\n",
        "2. For each `epoch`:\n",
        "   - Split data into `mini_batch_size` chunks\n",
        "   - Update policy on each mini-batch\n",
        "3. Discard old data and collect new rollouts\n",
        "\n",
        "**Trade-off:**\n",
        "- More epochs = more updates per data\n",
        "- But too many epochs = policy drift (mitigated by clipping)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeuLWo64L4Vi"
      },
      "source": [
        "## PPO Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation: PPO Agent Implementation\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "**1. Generalized Advantage Estimation (GAE):**\n",
        "```python\n",
        "delta = reward + gamma * V(next_state) - V(state)\n",
        "gae = delta + gamma * tau * gae\n",
        "```\n",
        "- $\\tau$ controls bias-variance trade-off\n",
        "- Higher $\\tau$ = more variance, less bias\n",
        "- Returns = advantages + values\n",
        "\n",
        "**2. Clipped Surrogate Objective:**\n",
        "```python\n",
        "ratio = exp(new_log_prob - old_log_prob)\n",
        "L = min(ratio * advantage, clip(ratio, 1-\u03b5, 1+\u03b5) * advantage)\n",
        "```\n",
        "- Prevents too-large policy updates\n",
        "- $\\epsilon$ typically 0.1-0.3\n",
        "\n",
        "**3. Entropy Regularization:**\n",
        "- Encourages exploration\n",
        "- Loss: `actor_loss - entropy_weight * entropy`\n",
        "- Higher entropy = more random actions\n",
        "\n",
        "**Training Loop:**\n",
        "1. Collect `rollout_len` transitions\n",
        "2. Compute returns and advantages with GAE\n",
        "3. Normalize advantages (improves stability)\n",
        "4. Update policy for `epoch` iterations\n",
        "5. Clear memory and repeat\n",
        "\n",
        "**PPO Advantages:**\n",
        "- \u2705 Stable and robust\n",
        "- \u2705 Few hyperparameters\n",
        "- \u2705 Works well across many tasks\n",
        "- \u274c On-policy (less sample efficient than DDPG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j91bMF9TPjPo"
      },
      "source": [
        "We will use two separated networks for actor and critic respectively. The actor network consists of two fully connected hidden layer with ReLU branched out two fully connected output layers for mean and standard deviation of Gaussian distribution. Pendulum-v0 has only one action which has a range from -2 to 2. In order to fit the range, the actor outputs the mean value with tanh. The result will be scaled in ActionNormalizer class. On the one hand, the critic network has three fully connected layers as two hidden layers (ReLU) and an output layer. One thing to note is that we initialize the last layers' weights and biases as uniformly distributed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation: DDPG Agent Implementation\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "**1. Deterministic Policy Gradient:**\n",
        "```python\n",
        "actor_loss = -Q(s, actor(s)).mean()\n",
        "```\n",
        "- Maximizes Q-value by adjusting actor parameters\n",
        "- Chain rule: $\\nabla_\\theta J = E[\\nabla_a Q(s,a) \\nabla_\\theta \\mu(s)]$\n",
        "\n",
        "**2. Target Networks:**\n",
        "- Separate target networks for actor and critic\n",
        "- Soft updates: $\\theta' \\leftarrow \\tau \\theta + (1-\\tau) \\theta'$\n",
        "- Stabilizes training (like DQN)\n",
        "\n",
        "**3. Replay Buffer:**\n",
        "- Stores all past experiences\n",
        "- Samples random mini-batches\n",
        "- Breaks temporal correlations\n",
        "\n",
        "**4. Exploration:**\n",
        "- Initial random steps: `initial_random_steps`\n",
        "- OU noise added to actions during training\n",
        "- No noise during testing\n",
        "\n",
        "**Training Loop:**\n",
        "1. Select action: $a = \\mu(s) + \\mathcal{N}$ (with noise)\n",
        "2. Store transition in replay buffer\n",
        "3. Sample mini-batch from buffer\n",
        "4. Compute target: $y = r + \\gamma Q'(s', \\mu'(s'))$\n",
        "5. Update critic: minimize $(Q(s,a) - y)^2$\n",
        "6. Update actor: maximize $Q(s, \\mu(s))$\n",
        "7. Soft update target networks\n",
        "\n",
        "**DDPG Advantages:**\n",
        "- \u2705 Off-policy (sample efficient)\n",
        "- \u2705 Deterministic policy (faster convergence)\n",
        "- \u2705 Works well for continuous control\n",
        "- \u274c Sensitive to hyperparameters\n",
        "- \u274c Can be unstable (needs careful tuning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a17WionpEKxX"
      },
      "outputs": [],
      "source": [
        "def init_layer_uniform(layer: nn.Linear, init_w: float = 3e-3) -> nn.Linear:\n",
        "    \"\"\"Init uniform parameters on the single layer.\"\"\"\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "        low = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "        \n",
        "        action = low + (action + 1.0) * 0.5 * (high - low)\n",
        "        action = np.clip(action, low, high)\n",
        "        \n",
        "        return action\n",
        "    # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "class PPOActor(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_dim: int, \n",
        "        out_dim: int, \n",
        "        log_std_min: int = -20,\n",
        "        log_std_max: int = 0,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(PPOActor, self).__init__()\n",
        "\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        low = self.action_space.low\n",
        "        high = self.action_space.high\n",
        "        \n",
        "        action = 2 * (action - low) / (high - low) - 1\n",
        "        action = np.clip(action, -1.0, 1.0)\n",
        "        \n",
        "        return action\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        \n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, in_dim: int):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # ==================================== Your Code (End) ====================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Explanation: Action Normalizer\n",
        "\n",
        "The `ActionNormalizer` wrapper transforms actions between the network's output range [-1, 1] and the environment's actual action range [low, high].\n",
        "\n",
        "**Why Normalize Actions?**\n",
        "- Neural networks with `tanh` output naturally produce values in [-1, 1]\n",
        "- Environments have arbitrary action ranges (e.g., Pendulum: [-2, 2])\n",
        "- Normalization allows the network to work with a standard range\n",
        "\n",
        "**Forward Transformation** (network \u2192 environment):\n",
        "```python\n",
        "action = low + (action + 1) * 0.5 * (high - low)\n",
        "```\n",
        "Maps [-1, 1] \u2192 [low, high]\n",
        "\n",
        "**Reverse Transformation** (environment \u2192 network):\n",
        "```python\n",
        "action = 2 * (action - low) / (high - low) - 1\n",
        "```\n",
        "Maps [low, high] \u2192 [-1, 1]\n",
        "\n",
        "**Example:** Pendulum-v1 actions are in [-2, 2]\n",
        "- Network outputs 0.5\n",
        "- Normalized: -2 + (0.5 + 1) * 0.5 * 4 = 1.0\n",
        "- Environment receives action = 1.0 \u2705"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY8ylNydFSyU"
      },
      "source": [
        "## DDPG Networks\n",
        "We are going to use two separated networks for actor and critic. The actor network has three fully connected layers and three non-linearity functions, **ReLU** for hidden layers and **tanh** for the output layer. On the other hand, the critic network has three fully connected layers, but it used two activation functions for hidden layers **ReLU**. Plus, its input sizes of critic network are sum of state sizes and action sizes. One thing to note is that we initialize the final layer's weights and biases so that they are **uniformly distributed.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyIgq8P0FSyV"
      },
      "outputs": [],
      "source": [
        "class DDPGActor(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_dim: int, \n",
        "        out_dim: int,\n",
        "        init_w: float = 3e-3,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(DDPGActor, self).__init__()\n",
        "        \n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. set the hidden layers\n",
        "        # 2. init hidden layers uniformly       \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # use a tanh function as a ativation function for output layer \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    \n",
        "    \n",
        "class DDPGCritic(nn.Module):\n",
        "    def __init__(\n",
        "        self, \n",
        "        in_dim: int, \n",
        "        init_w: float = 3e-3,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(DDPGCritic, self).__init__()\n",
        "        \n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. set the hidden layers\n",
        "        # 2. init hidden layers uniformly       \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    def forward(\n",
        "        self, state: torch.Tensor, action: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # notice that this value function is Q(s, a)\n",
        "        # ==================================== Your Code (End) ====================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt25nk9AN9CT"
      },
      "source": [
        "# Agents "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KiHRoaq4EKxY"
      },
      "source": [
        "## PPO Agent\n",
        "Here is a summary of PPOAgent class.\n",
        "\n",
        "| Method           | Note                                                 |\n",
        "|---               |---                                                   |\n",
        "|select_action     | select an action from the input state.               |\n",
        "|step              | take an action and return the response of the env.   |\n",
        "|update_model      | update the model by gradient descent.                |\n",
        "|train             | train the agent during num_frames.                   |\n",
        "|test              | test the agent (1 episode).                          |\n",
        "|_plot             | plot the training progresses.                        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE8Is2T9EKxZ"
      },
      "source": [
        "PPO updates the model several times(`epoch`) using the stacked memory. By `ppo_iter` function, It yield the samples of stacked memory by interacting a environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAurbe2YEKxa"
      },
      "outputs": [],
      "source": [
        "def ppo_iter(\n",
        "    epoch: int,\n",
        "    mini_batch_size: int,\n",
        "    states: torch.Tensor,\n",
        "    actions: torch.Tensor,\n",
        "    values: torch.Tensor,\n",
        "    log_probs: torch.Tensor,\n",
        "    returns: torch.Tensor,\n",
        "    advantages: torch.Tensor,\n",
        "):\n",
        "    \"\"\"Yield mini-batches.\"\"\"\n",
        "    \n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "        \n",
        "    # ==================================== Your Code (End) ====================================\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFja11svEKxa"
      },
      "outputs": [],
      "source": [
        "class PPOAgent:\n",
        "    \"\"\"PPO Agent.\n",
        "    Attributes:\n",
        "        env (gym.Env): Gym env for training\n",
        "        gamma (float): discount factor\n",
        "        tau (float): lambda of generalized advantage estimation (GAE)\n",
        "        batch_size (int): batch size for sampling\n",
        "        epsilon (float): amount of clipping surrogate objective\n",
        "        epoch (int): the number of update\n",
        "        rollout_len (int): the number of rollout\n",
        "        entropy_weight (float): rate of weighting entropy into the loss function\n",
        "        actor (nn.Module): target actor model to select actions\n",
        "        critic (nn.Module): critic model to predict state values\n",
        "        transition (list): temporory storage for the recent transition\n",
        "        device (torch.device): cpu / gpu\n",
        "        total_step (int): total step numbers\n",
        "        is_test (bool): flag to show the current mode (train / test)        \n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        batch_size: int,\n",
        "        gamma: float,\n",
        "        tau: float,\n",
        "        epsilon: float,\n",
        "        epoch: int,\n",
        "        rollout_len: int,\n",
        "        entropy_weight: float,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. set hyperparameters\n",
        "        # 2. check device: cpu/GPU\n",
        "        # 3. init actor critic networks\n",
        "        # 4. set Optimizer for each network\n",
        "        # 5. consider memory for training\n",
        "        # 6. set total step counts equal to 1\n",
        "        # 7. define a mode for train/test\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    \n",
        "        \n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. select action for train or test mode\n",
        "        # 2. if you are in train mode take care of filing considered memory\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "    def update_model(\n",
        "        self, next_state: np.ndarray\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. set device\n",
        "        # 2. for each step:\n",
        "        # 3.    calculate ratios\n",
        "        # 4.    calculate actor_loss\n",
        "        # 5.    calculate entropy\n",
        "        # 6.    calculate critic_loss\n",
        "        # 7.    Train  critic\n",
        "        # 8.    Train actor\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    \n",
        "\n",
        "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. set the status of trainig\n",
        "        # 2. Reset environment\n",
        "        # 3. for number of frames:\n",
        "        # 4.    select an action\n",
        "        # 5.    step in environment\n",
        "        # 6.    update model\n",
        "        # 7. terminate environment after training is finished\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "        \n",
        "\n",
        "    def test(self):\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. set the status of trainig\n",
        "        # 2. Reset environment\n",
        "        # 3. roll out one episode living in the environment and save frames for getting render\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "\n",
        "    def _plot(\n",
        "        self,\n",
        "        frame_idx: int,\n",
        "        scores: List[float],\n",
        "        actor_losses: List[float],\n",
        "        critic_losses: List[float],\n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. define a function for sub plots\n",
        "        # 2. for each variable plot the specific subplot\n",
        "        # 3. plot the whole figure\n",
        "        # ==================================== Your Code (End) ====================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyrmZWSWFSyV"
      },
      "source": [
        "## DDPG Agent\n",
        "Here is a summary of DDPGAgent class.\n",
        "\n",
        "| Method           | Note                                                 |\n",
        "|---               |---                                                  |\n",
        "|select_action     | select an action from the input state.               |\n",
        "|step              | take an action and return the response of the env.   |\n",
        "|update_model      | update the model by gradient descent.                |\n",
        "|train             | train the agent during num_frames.                   |\n",
        "|test              | test the agent (1 episode).                          |\n",
        "|\\_target_soft_update| soft update from the local model to the target model.|\n",
        "|\\_plot              | plot the training progresses.                        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuPhByk5FSyW"
      },
      "outputs": [],
      "source": [
        "class DDPGAgent:\n",
        "    \"\"\"DDPGAgent interacting with environment.\n",
        "    \n",
        "    Attribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        actor (nn.Module): target actor model to select actions\n",
        "        actor_target (nn.Module): actor model to predict next actions\n",
        "        actor_optimizer (Optimizer): optimizer for training actor\n",
        "        critic (nn.Module): critic model to predict state values\n",
        "        critic_target (nn.Module): target critic model to predict state values\n",
        "        critic_optimizer (Optimizer): optimizer for training critic\n",
        "        memory (ReplayBuffer): replay memory to store transitions\n",
        "        batch_size (int): batch size for sampling\n",
        "        gamma (float): discount factor\n",
        "        tau (float): parameter for soft target update\n",
        "        initial_random_steps (int): initial random action steps\n",
        "        noise (OUNoise): noise generator for exploration\n",
        "        device (torch.device): cpu / gpu\n",
        "        transition (list): temporory storage for the recent transition\n",
        "        total_step (int): total step numbers\n",
        "        is_test (bool): flag to show the current mode (train / test)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: gym.Env,\n",
        "        memory_size: int,\n",
        "        batch_size: int,\n",
        "        ou_noise_theta: float,\n",
        "        ou_noise_sigma: float,\n",
        "        gamma: float = 0.99,\n",
        "        tau: float = 5e-3,\n",
        "        initial_random_steps: int = 1e4,\n",
        "    ):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. initialize hyper parameters, reply buffer and environment\n",
        "        # 2. set device\n",
        "        # 3. set target entropy, log alpha and alpha optimizer\n",
        "        # 4. init actor network\n",
        "        # 5. init value fuction (value critic)\n",
        "        # 6. init OUNoise \n",
        "        # 7. set Optimizers\n",
        "        # consider stroring transitions in memeory, counting steps and specify train/test mode\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    \n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        \n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. check if initial random action should be conducted\n",
        "        # 2. add noise for exploration during training\n",
        "        # 3. store transition\n",
        "        # return selected action\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    \n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # step in environment and save transition in memory if you are not in test mode\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    \n",
        "    def update_model(self) -> torch.Tensor:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. set device\n",
        "        # 2. get a batch from memory and calculate the return\n",
        "        # 3. calculate the loss for actor and critic networks\n",
        "        # 4. update target\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    \n",
        "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. set the status of trainig\n",
        "        # 2. Reset environment\n",
        "        # 3. for number of frames:\n",
        "        # 4.    select an action\n",
        "        # 5.    step in environment\n",
        "        # 6.    update model\n",
        "        # 7. plot the computed variables\n",
        "        # 8. terminate environment after training is finished\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        \n",
        "    def test(self):\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # 1. set the status of trainig\n",
        "        # 2. Reset environment\n",
        "        # 3. roll out one episode living in the environment and save frames for getting render\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    \n",
        "    def _target_soft_update(self):\n",
        "        \"\"\"Soft-update: target = tau*local + (1-tau)*target.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    \n",
        "    def _plot(\n",
        "        self, \n",
        "        frame_idx: int, \n",
        "        scores: List[float], \n",
        "        actor_losses: List[float], \n",
        "        critic_losses: List[float], \n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "         \n",
        "        # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncwo8ekkEKxd"
      },
      "source": [
        "# Environment\n",
        "*ActionNormalizer* is an action wrapper class to normalize the action values ranged in (-1. 1). Thanks to this class, we can make the agent simply select action values within the zero centered range (-1, 1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2omZW-6CEKxd"
      },
      "outputs": [],
      "source": [
        "class ActionNormalizer(gym.ActionWrapper):\n",
        "    \"\"\"Rescale and relocate the actions.\"\"\"\n",
        "\n",
        "    def action(self, action: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Change the range (-1, 1) to (low, high).\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    \n",
        "\n",
        "    def reverse_action(self, action: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Change the range (low, high) to (-1, 1).\"\"\"\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cA1LxC3VEKxe"
      },
      "source": [
        "You can see [the code](https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py) and [configurations](https://github.com/openai/gym/blob/cedecb35e3428985fd4efad738befeb75b9077f1/gym/envs/__init__.py#L81) of Pendulum-v1 from OpenAI's repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwRS69ZmEKxe"
      },
      "outputs": [],
      "source": [
        "# environment\n",
        "env_id = \"Pendulum-v1\"\n",
        "env = gym.make(env_id)\n",
        "env = ActionNormalizer(env)\n",
        "env.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PPO vs DDPG: Comprehensive Comparison\n",
        "\n",
        "### Algorithm Type\n",
        "| Algorithm | Type | Learning | Sample Efficiency |\n",
        "|-----------|------|----------|-------------------|\n",
        "| PPO | Policy Gradient | On-policy | Lower (needs fresh data) |\n",
        "| DDPG | Actor-Critic | Off-policy | Higher (reuses old data) |\n",
        "\n",
        "### Policy Type\n",
        "- **PPO**: Stochastic policy \u2192 samples from $\\mathcal{N}(\\mu(s), \\sigma(s))$\n",
        "- **DDPG**: Deterministic policy \u2192 outputs $\\mu(s)$ directly\n",
        "\n",
        "### Exploration Strategy\n",
        "- **PPO**: Intrinsic (policy entropy)\n",
        "- **DDPG**: Extrinsic (OU noise added to actions)\n",
        "\n",
        "### Stability\n",
        "- **PPO**: More stable due to clipping mechanism\n",
        "- **DDPG**: Can be unstable, needs careful tuning\n",
        "\n",
        "### Key Innovations\n",
        "**PPO:**\n",
        "- Clipped surrogate objective prevents destructive policy updates\n",
        "- Generalized Advantage Estimation (GAE) for variance reduction\n",
        "- Multiple epochs on collected data\n",
        "\n",
        "**DDPG:**\n",
        "- Target networks for stability\n",
        "- Replay buffer for sample efficiency\n",
        "- Deterministic policy gradient\n",
        "\n",
        "### Hyperparameters\n",
        "**PPO Key Params:**\n",
        "- `epsilon`: clip range (0.1-0.3)\n",
        "- `gamma`: discount factor (0.9-0.99)\n",
        "- `tau`: GAE parameter (0.8-0.95)\n",
        "- `entropy_weight`: exploration bonus\n",
        "\n",
        "**DDPG Key Params:**\n",
        "- `gamma`: discount factor (0.95-0.99)\n",
        "- `tau`: soft update rate (0.001-0.01)\n",
        "- `ou_noise_theta`, `ou_noise_sigma`: exploration noise\n",
        "- `initial_random_steps`: warm-up period\n",
        "\n",
        "### When to Use?\n",
        "**Use PPO when:**\n",
        "- Stability is critical\n",
        "- Sample efficiency is not a concern\n",
        "- You want robust performance across diverse tasks\n",
        "\n",
        "**Use DDPG when:**\n",
        "- Sample efficiency matters\n",
        "- Deterministic policies are preferred\n",
        "- You have time to tune hyperparameters\n",
        "\n",
        "### Expected Performance on Pendulum-v1\n",
        "- **PPO**: Should converge to ~-200 to -150 reward\n",
        "- **DDPG**: Should converge to ~-150 to -100 reward (often slightly better)\n",
        "\n",
        "Both algorithms should learn to swing the pendulum up and balance it!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YPRqbybOEL2"
      },
      "source": [
        "# Train & Test "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZpoVD0YEKxf"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sn6_VF1_EKxf"
      },
      "outputs": [],
      "source": [
        "# parameters\n",
        "num_frames = 50000\n",
        "memory_size = 20000\n",
        "batch_size = 128\n",
        "ou_noise_theta = 1.0\n",
        "ou_noise_sigma = 0.1\n",
        "initial_random_steps = 10000\n",
        "\n",
        "ppo_agent = PPOAgent(\n",
        "    env,\n",
        "    gamma = 0.9,\n",
        "    tau = 0.8,\n",
        "    batch_size = 64,\n",
        "    epsilon = 0.2,\n",
        "    epoch = 64,\n",
        "    rollout_len = 2048,\n",
        "    entropy_weight = 0.005\n",
        ")\n",
        "\n",
        "ddpg_agent = DDPGAgent(\n",
        "    env, \n",
        "    memory_size, \n",
        "    batch_size,\n",
        "    ou_noise_theta,\n",
        "    ou_noise_sigma,\n",
        "    initial_random_steps=initial_random_steps\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNvkDrZcEKxf"
      },
      "source": [
        "## Train PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "penPfG3tEKxg"
      },
      "outputs": [],
      "source": [
        "ppo_agent.train(num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivDH4ooWM7VO"
      },
      "source": [
        "## Train DDPG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CluntoxKM5kF"
      },
      "outputs": [],
      "source": [
        "ddpg_agent.train(num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjtQZlM2EKxg"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXMKIAX8EKxg"
      },
      "outputs": [],
      "source": [
        "# test\n",
        "if IN_COLAB:\n",
        "    ppo_agent.env = gym.wrappers.Monitor(ppo_agent.env, \"videos\", force=True)\n",
        "    ddpg_agent.env = gym.wrappers.Monitor(ddpg_agent.env, \"videos\", force=True)\n",
        "ppo_frames = ppo_agent.test()\n",
        "ddpg_frames = ddpg_agent.test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZ9cZ8BwEKxg"
      },
      "source": [
        "## Render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gVAiT74AEKxh"
      },
      "outputs": [],
      "source": [
        "if IN_COLAB:  # for colab\n",
        "    import base64\n",
        "    import glob\n",
        "    import io\n",
        "    import os\n",
        "\n",
        "    from IPython.display import HTML, display\n",
        "\n",
        "    def ipython_show_video(path: str) -> None:\n",
        "        \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
        "        if not os.path.isfile(path):\n",
        "            raise NameError(\"Cannot access: {}\".format(path))\n",
        "\n",
        "        video = io.open(path, \"r+b\").read()\n",
        "        encoded = base64.b64encode(video)\n",
        "\n",
        "        display(HTML(\n",
        "            data=\"\"\"\n",
        "            <video alt=\"test\" controls>\n",
        "            <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "            </video>\n",
        "            \"\"\".format(encoded.decode(\"ascii\"))\n",
        "        ))\n",
        "\n",
        "    list_of_files = glob.glob(\"videos/*.mp4\")\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(latest_file)\n",
        "    ipython_show_video(latest_file)\n",
        "\n",
        "else:  # for jupyter\n",
        "    from matplotlib import animation\n",
        "    from JSAnimation.IPython_display import display_animation\n",
        "    from IPython.display import display\n",
        "\n",
        "\n",
        "    def display_frames_as_gif(frames):\n",
        "        \"\"\"Displays a list of frames as a gif, with controls.\"\"\"\n",
        "        patch = plt.imshow(frames[0])\n",
        "        plt.axis('off')\n",
        "\n",
        "        def animate(i):\n",
        "            patch.set_data(frames[i])\n",
        "\n",
        "        anim = animation.FuncAnimation(\n",
        "            plt.gcf(), animate, frames = len(frames), interval=50\n",
        "        )\n",
        "        display(display_animation(anim, default_mode='loop'))\n",
        "\n",
        "\n",
        "    # display \n",
        "    display_frames_as_gif(ppo_frames)\n",
        "    display_frames_as_gif(ddpg_frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igSqoIbdEKxh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}