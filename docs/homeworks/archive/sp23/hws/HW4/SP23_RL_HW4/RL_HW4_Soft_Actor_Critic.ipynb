{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9HxtwHbk6Pc"
      },
      "source": [
        "# Soft Actor Critic Agent(115 Points)\n",
        "\n",
        "> Name:\n",
        "\n",
        "> SID: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PJUxRV4MIod"
      },
      "source": [
        "In this notebook, we are going to implement **Soft Actor Critic (SAC)** \n",
        "on the **CartPole** environment in online and offline settings. In this framework, the actor aims to maximize expected reward while also maximizing **entropy**. That is, to succeed at the task while acting as randomly as possible. This method seeks a high entropy in the policy to explicitly encourage exploration. For the offline setting, you are going to make SAC conservative using CQL method. \n",
        "\n",
        "* SAC is an off-policy algorithm.\n",
        "* The version of SAC implemented here can only be used for environments with discrete action spaces.\n",
        "* An alternate version of SAC, which slightly changes the policy update  rule, can be implemented to handle continouse action spaces.\n",
        "* Complete the **TODO** parts in the code accordingly.\n",
        "* Remember to answer the conceptual questions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UC-BecdPmdb3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_whWTLZejkm3"
      },
      "source": [
        "## Network Structure (8 points)\n",
        "For constructing SAC agent, we use objects of feedforward neural networks with 3 layers. Complete the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pxOyt0xh4nN"
      },
      "outputs": [],
      "source": [
        "class Network(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dimension, output_dimension, output_activation=torch.nn.Identity()):\n",
        "        super(Network, self).__init__()\n",
        "        ##########################################################\n",
        "        # TODO (4 points): \n",
        "        # Define your network layers.\n",
        "        ##########################################################\n",
        "        # 3-layer feedforward neural network with hidden size 256\n",
        "        self.layer_1 = torch.nn.Linear(input_dimension, 256)\n",
        "        self.layer_2 = torch.nn.Linear(256, 256)\n",
        "        self.output_layer = torch.nn.Linear(256, output_dimension)\n",
        "        self.output_activation = output_activation\n",
        "        ##########################################################\n",
        "\n",
        "    def forward(self, inpt):  \n",
        "        output = None      \n",
        "        ##########################################################\n",
        "        # TODO (4 points): \n",
        "        # Use relu and the output activation functions to calculate the output\n",
        "        ##########################################################\n",
        "        # Forward pass through the network with ReLU activations\n",
        "        x = torch.nn.functional.relu(self.layer_1(inpt))\n",
        "        x = torch.nn.functional.relu(self.layer_2(x))\n",
        "        output = self.output_activation(self.output_layer(x))\n",
        "        return output\n",
        "        ##########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Architecture Explanation\n",
        "\n",
        "The `Network` class implements a 3-layer feedforward neural network:\n",
        "\n",
        "1. **Input Layer → Hidden Layer 1**: Maps from input dimension to 256 neurons\n",
        "2. **Hidden Layer 1 → Hidden Layer 2**: 256 → 256 neurons  \n",
        "3. **Hidden Layer 2 → Output Layer**: 256 → output dimension\n",
        "\n",
        "**Activation Functions:**\n",
        "- **ReLU** is used between hidden layers to introduce non-linearity\n",
        "- **Output activation** is customizable (e.g., `Softmax` for actor, `Identity` for critics)\n",
        "\n",
        "This network architecture will be used for both the **actor** (policy network) and **critics** (Q-value networks) in SAC.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DToSGdmDka1u"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "A SAC agent needs a replay buffer, from which previously visited states can be sampled. You can use the implemented code below. You are going to use the replay buffer of an online-trained agent to train the offline model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UC7jTwJXh8wl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, environment, capacity=500000):\n",
        "        transition_type_str = self.get_transition_type_str(environment)\n",
        "        self.buffer = np.zeros(capacity, dtype=transition_type_str)\n",
        "        self.weights = np.zeros(capacity)\n",
        "        self.head_idx = 0\n",
        "        self.count = 0\n",
        "        self.capacity = capacity\n",
        "        self.max_weight = 10**-2\n",
        "        self.delta = 10**-4\n",
        "        self.indices = None\n",
        "        self.mirror_index = np.random.permutation(range(self.buffer.shape[0]))\n",
        "\n",
        "    def get_transition_type_str(self, environment):\n",
        "        state_dim = environment.observation_space.shape[0]\n",
        "        state_dim_str = '' if state_dim == () else str(state_dim)\n",
        "        state_type_str = environment.observation_space.sample().dtype.name\n",
        "        action_dim = environment.action_space.shape\n",
        "        action_dim_str = '' if action_dim == () else str(action_dim)\n",
        "        action_type_str = environment.action_space.sample().__class__.__name__\n",
        "\n",
        "        # type str for transition = 'state type, action type, reward type, state type'\n",
        "        transition_type_str = '{0}{1}, {2}{3}, float32, {0}{1}, bool'.format(state_dim_str, state_type_str,\n",
        "                                                                             action_dim_str, action_type_str)\n",
        "\n",
        "        return transition_type_str\n",
        "\n",
        "    def add_transition(self, transition):\n",
        "        self.buffer[self.head_idx] = transition\n",
        "        self.weights[self.head_idx] = self.max_weight\n",
        "\n",
        "        self.head_idx = (self.head_idx + 1) % self.capacity\n",
        "        self.count = min(self.count + 1, self.capacity)\n",
        "\n",
        "    def sample_minibatch(self, size=100, batch_deterministic_start=None):\n",
        "        set_weights = self.weights[:self.count] + self.delta\n",
        "        probabilities = set_weights / sum(set_weights)\n",
        "        if batch_deterministic_start is None:\n",
        "            self.indices = np.random.choice(range(self.count), size, p=probabilities, replace=False)\n",
        "        else:\n",
        "            self.indices = self.mirror_index[batch_deterministic_start:batch_deterministic_start+size]\n",
        "        return self.buffer[self.indices]\n",
        "\n",
        "    def update_weights(self, prediction_errors):\n",
        "        max_error = max(prediction_errors)\n",
        "        self.max_weight = max(self.max_weight, max_error)\n",
        "        self.weights[self.indices] = prediction_errors\n",
        "\n",
        "    def get_size(self):\n",
        "        return self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB2m89BjlNXY"
      },
      "source": [
        "## Questions (18 points)\n",
        "\n",
        "❓ We know that standard RL maximizes the expected sum of rewards. What is the objective function of SAC algorithm? Compare it to the standard RL loss.\n",
        "\n",
        "❓ Write down the actor cost function.\n",
        "\n",
        "❓ Write down the critic cost function.\n",
        "\n",
        "❓ Elaborate on the reason why most implementations of SAC use two critics (one local and one target).\n",
        "\n",
        "❓ What is the difference between training samples in offline and online settings?\n",
        "\n",
        "❓ How does adding CQL on top of SAC change the objective function?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers to Conceptual Questions\n",
        "\n",
        "**❓ Q1: What is the objective function of SAC algorithm? Compare it to the standard RL loss.**\n",
        "\n",
        "**Answer:**  \n",
        "Standard RL maximizes:\n",
        "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$$\n",
        "\n",
        "SAC maximizes **entropy-regularized objective**:\n",
        "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}[\\sum_{t=0}^{\\infty} \\gamma^t (r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t)))]$$\n",
        "\n",
        "where $H(\\pi(\\cdot|s_t)) = -\\mathbb{E}_{a \\sim \\pi}[\\log \\pi(a|s_t)]$ is the entropy.\n",
        "\n",
        "**Key Difference:** SAC encourages exploration by rewarding high entropy (randomness) in the policy, while standard RL focuses only on maximizing rewards.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ Q2: Write down the actor cost function.**\n",
        "\n",
        "**Answer:**  \n",
        "$$J_\\pi(\\phi) = \\mathbb{E}_{s_t \\sim D}\\mathbb{E}_{a_t \\sim \\pi_\\phi}[\\alpha \\log \\pi_\\phi(a_t|s_t) - Q_\\theta(s_t, a_t)]$$\n",
        "\n",
        "Or equivalently for discrete actions:\n",
        "$$J_\\pi(\\phi) = \\mathbb{E}_{s_t \\sim D}[\\sum_a \\pi_\\phi(a|s_t)(\\alpha \\log \\pi_\\phi(a|s_t) - Q_\\theta(s_t, a))]$$\n",
        "\n",
        "The actor minimizes this cost, which balances between maximizing Q-values and maintaining high entropy.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ Q3: Write down the critic cost function.**\n",
        "\n",
        "**Answer:**  \n",
        "$$J_Q(\\theta) = \\mathbb{E}_{(s_t,a_t,r_t,s_{t+1}) \\sim D}[(Q_\\theta(s_t, a_t) - y_t)^2]$$\n",
        "\n",
        "where the target is:\n",
        "$$y_t = r_t + \\gamma(1-d_t) \\mathbb{E}_{a_{t+1} \\sim \\pi}[Q_{\\theta'}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi(a_{t+1}|s_{t+1})]$$\n",
        "\n",
        "For discrete actions:\n",
        "$$y_t = r_t + \\gamma(1-d_t) \\sum_a \\pi(a|s_{t+1})[Q_{\\theta'}(s_{t+1}, a) - \\alpha \\log \\pi(a|s_{t+1})]$$\n",
        "\n",
        "---\n",
        "\n",
        "**❓ Q4: Elaborate on the reason why most implementations of SAC use two critics (one local and one target).**\n",
        "\n",
        "**Answer:**  \n",
        "SAC uses **two local critics** and **two target critics** (4 critics total):\n",
        "\n",
        "1. **Two Local Critics (Q1, Q2):** Helps reduce **overestimation bias**. We take the minimum: $Q(s,a) = \\min(Q_1(s,a), Q_2(s,a))$. This clipped double-Q learning prevents the critic from being overly optimistic.\n",
        "\n",
        "2. **Target Networks (Q1_target, Q2_target):** Provides **stable training targets**. Target networks are slowly updated (soft update with $\\tau \\ll 1$), preventing the \"moving target\" problem where the Q-values we're trying to match keep changing rapidly.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ Q5: What is the difference between training samples in offline and online settings?**\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "| Aspect | Online RL | Offline RL |\n",
        "|--------|-----------|------------|\n",
        "| **Data Collection** | Agent interacts with environment during training | Uses pre-collected fixed dataset |\n",
        "| **Exploration** | Can explore new states/actions | Limited to dataset coverage |\n",
        "| **Distribution Shift** | Policy improves, collects better data | Policy may diverge from dataset distribution |\n",
        "| **Sample Efficiency** | Requires many environment interactions | No environment interaction needed |\n",
        "| **Safety** | May take dangerous actions during exploration | Safe (no real-world interaction) |\n",
        "\n",
        "**Key Challenge in Offline RL:** **Extrapolation error** - the agent may learn to take actions not well-represented in the dataset, leading to overestimated Q-values for out-of-distribution actions.\n",
        "\n",
        "---\n",
        "\n",
        "**❓ Q6: How does adding CQL on top of SAC change the objective function?**\n",
        "\n",
        "**Answer:**  \n",
        "CQL (Conservative Q-Learning) adds a **conservative regularizer** to the critic loss:\n",
        "\n",
        "**Standard SAC Critic Loss:**\n",
        "$$J_Q(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim D}[(Q_\\theta(s, a) - y)^2]$$\n",
        "\n",
        "**CQL Critic Loss:**\n",
        "$$J_{CQL}(\\theta) = \\alpha_{CQL} \\cdot \\underbrace{(\\mathbb{E}_{s \\sim D, a \\sim \\mu(a|s)}[Q_\\theta(s,a)] - \\mathbb{E}_{s,a \\sim D}[Q_\\theta(s,a)])}_{\\text{CQL regularizer}} + J_Q(\\theta)$$\n",
        "\n",
        "where $\\mu$ is a behavior policy (e.g., uniform or current policy).\n",
        "\n",
        "**Effect:** \n",
        "- **Increases** Q-values for actions in the dataset $D$\n",
        "- **Decreases** Q-values for out-of-distribution actions $\\mu$\n",
        "- This makes the agent **conservative**, avoiding actions not seen in the offline dataset\n",
        "- The tradeoff factor $\\alpha_{CQL}$ controls the strength of this conservatism\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wOrt_EmfFFD"
      },
      "source": [
        "## SAC Agent (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScEk94Ubb01W"
      },
      "source": [
        "Now complete the following class. You can use the auxiliary methods provided in the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "A8bEgUc2hmet"
      },
      "outputs": [],
      "source": [
        "class SACAgent:\n",
        "\n",
        "    ALPHA_INITIAL = 1.\n",
        "    REPLAY_BUFFER_BATCH_SIZE = 100\n",
        "    DISCOUNT_RATE = 0.99\n",
        "    LEARNING_RATE = 10 ** -4\n",
        "    SOFT_UPDATE_INTERPOLATION_FACTOR = 0.01\n",
        "    TRADEOFF_FACTOR = 5 # trade-off factor in the CQL\n",
        "\n",
        "    def __init__(self, environment, replay_buffer=None, use_cql=False, offline=False):\n",
        "\n",
        "        assert not use_cql or offline, 'Please activate the offline flag for CQL.' \n",
        "        assert not offline or not replay_buffer is None, 'Please pass a replay buffer to the offline method.' \n",
        "\n",
        "        self.environment = environment\n",
        "        self.state_dim = self.environment.observation_space.shape[0]\n",
        "        self.action_dim = self.environment.action_space.n\n",
        "\n",
        "        self.offline = offline\n",
        "        self.replay_buffer = ReplayBuffer(self.environment) if replay_buffer is None else replay_buffer\n",
        "        self.use_cql = use_cql\n",
        "\n",
        "        ##########################################################\n",
        "        # TODO (6 points): \n",
        "        # Define critiss usig your impelmented feed forward netwrok(10 points).\n",
        "        # To have easier critic updates, you can use two local critic networks \n",
        "        # and two target critics.\n",
        "        ##########################################################\n",
        "        self.critic_local = None\n",
        "        self.critic_local2 = None\n",
        "        self.critic_optimiser = None\n",
        "        self.critic_optimiser2 = None\n",
        "        self.critic_target = None\n",
        "        self.critic_target2 = None\n",
        "        ##########################################################\n",
        "\n",
        "        self.soft_update_target_networks(tau=1.)\n",
        "\n",
        "        ##########################################################\n",
        "        # TODO (2 points): \n",
        "        # Define the actor usig your impelmented feed forward netwrok(10 points).\n",
        "        # Define the actor optimizer using torch.Adam (4 points)\n",
        "        ##########################################################\n",
        "        self.actor_local = None\n",
        "        self.actor_optimiser  = None\n",
        "        ##########################################################\n",
        "\n",
        "        self.target_entropy = 0.98 * -np.log(1 / self.environment.action_space.n)\n",
        "        self.log_alpha = torch.tensor(np.log(self.ALPHA_INITIAL), requires_grad=True)\n",
        "        self.alpha = self.log_alpha\n",
        "        self.alpha_optimiser = torch.optim.Adam([self.log_alpha], lr=self.LEARNING_RATE)\n",
        "\n",
        "    def get_next_action(self, state, evaluation_episode=False):\n",
        "        if evaluation_episode:\n",
        "            discrete_action = self.get_action_deterministically(state)\n",
        "        else:\n",
        "            discrete_action = self.get_action_nondeterministically(state)\n",
        "        return discrete_action\n",
        "\n",
        "    def get_action_nondeterministically(self, state):\n",
        "        action_probabilities = self.get_action_probabilities(state)\n",
        "        discrete_action = np.random.choice(range(self.action_dim), p=action_probabilities)\n",
        "        return discrete_action\n",
        "\n",
        "    def get_action_deterministically(self, state):\n",
        "        action_probabilities = self.get_action_probabilities(state)\n",
        "        discrete_action = np.argmax(action_probabilities)\n",
        "        return discrete_action\n",
        "\n",
        "    def critic_loss(self, states_tensor, actions_tensor, rewards_tensor, \n",
        "                    next_states_tensor, done_tensor):\n",
        "        ##########################################################\n",
        "        # TODO (12 points): \n",
        "        # You are going to calculate critic losses in this method.\n",
        "        # Also you should implement the CQL loss if the corresponding \n",
        "        # flag is set.\n",
        "        ##########################################################\n",
        "        critic_loss, critic2_loss = 0, 0\n",
        "\n",
        "        return critic_loss, critic2_loss\n",
        "        ##########################################################\n",
        "\n",
        "    def actor_loss(self, states_tensor):\n",
        "        ##########################################################\n",
        "        # TODO (8 points): \n",
        "        # Now implement the actor loss.\n",
        "        ##########################################################\n",
        "        actor_loss, log_action_probabilities = 0, 0\n",
        "\n",
        "        return actor_loss, log_action_probabilities\n",
        "        ##########################################################\n",
        "\n",
        "    def train_on_transition(self, state, discrete_action, next_state, reward, done):\n",
        "        transition = (state, discrete_action, reward, next_state, done)\n",
        "        self.train_networks(transition)\n",
        "\n",
        "    def train_networks(self, transition=None, batch_deterministic_start=None):\n",
        "        ##########################################################\n",
        "        # TODO (6 points): \n",
        "        # Set all the gradients stored in the optimisers to zero.\n",
        "        # add the new transition to the replay buffer for online case.\n",
        "        ##########################################################\n",
        "\n",
        "        if self.replay_buffer.get_size() >= self.REPLAY_BUFFER_BATCH_SIZE:\n",
        "            minibatch = self.replay_buffer.sample_minibatch(self.REPLAY_BUFFER_BATCH_SIZE,\n",
        "                                                            batch_deterministic_start=batch_deterministic_start)\n",
        "            minibatch_separated = list(map(list, zip(*minibatch)))\n",
        "\n",
        "            states_tensor = torch.tensor(np.array(minibatch_separated[0]))\n",
        "            actions_tensor = torch.tensor(np.array(minibatch_separated[1]))\n",
        "            rewards_tensor = torch.tensor(np.array(minibatch_separated[2])).float()\n",
        "            next_states_tensor = torch.tensor(np.array(minibatch_separated[3]))\n",
        "            done_tensor = torch.tensor(np.array(minibatch_separated[4]))\n",
        "\n",
        "            ##########################################################\n",
        "            # TODO (16 points): \n",
        "            # Here, you should compute the gradients based on this loss, i.e. the gradients\n",
        "            # of the loss with respect to the Q-network parameters.\n",
        "            # Given a minibatch of 100 transitions from replay buffer,\n",
        "            # compute the critic loss and perform the backward and step functions,\n",
        "            # and compute the actor loss and perform the backward and step functions.\n",
        "            # You also need to update \\alpha.\n",
        "            ##########################################################\n",
        "\n",
        "            ##########################################################\n",
        "\n",
        "            self.soft_update_target_networks()\n",
        "\n",
        "    def temperature_loss(self, log_action_probabilities):\n",
        "        alpha_loss = -(self.log_alpha * (log_action_probabilities + self.target_entropy).detach()).mean()\n",
        "        return alpha_loss\n",
        "\n",
        "    def get_action_info(self, states_tensor):\n",
        "        action_probabilities = self.actor_local.forward(states_tensor)\n",
        "        z = action_probabilities == 0.0\n",
        "        z = z.float() * 1e-8\n",
        "        log_action_probabilities = torch.log(action_probabilities + z)\n",
        "        return action_probabilities, log_action_probabilities\n",
        "\n",
        "    def get_action_probabilities(self, state):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        action_probabilities = self.actor_local.forward(state_tensor)\n",
        "        return action_probabilities.squeeze(0).detach().numpy()\n",
        "\n",
        "    def soft_update_target_networks(self, tau=SOFT_UPDATE_INTERPOLATION_FACTOR):\n",
        "        self.soft_update(self.critic_target, self.critic_local, tau)\n",
        "        self.soft_update(self.critic_target2, self.critic_local2, tau)\n",
        "\n",
        "    def soft_update(self, target_model, origin_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), origin_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    def predict_q_values(self, state):\n",
        "        q_values = self.critic_local(state)\n",
        "        q_values2 = self.critic_local2(state)\n",
        "        return torch.min(q_values, q_values2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Tx4pS_kdo-"
      },
      "source": [
        "## Online SAC training loop (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUm9_qfAlvf_"
      },
      "source": [
        "Now evaluate your model using CartPole environemnt in the online setting. After each 4 episodes, you should evaluate your model on a seprate test environment. Run your model 4 times separately and plot the mean and deviation of the evaluation curves.\n",
        "\n",
        "**NOTE:** Since you are going to use the replay buffer of this agent as the offline dataset, you may want to save it for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6quXtBzZ6nZ"
      },
      "outputs": [],
      "source": [
        "TRAINING_EVALUATION_RATIO = 4\n",
        "EPISODES_PER_RUN = 1000\n",
        "STEPS_PER_EPISODE = 200\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "##########################################################\n",
        "# TODO (10 points): \n",
        "# Implement the training loop for the online SAC. \n",
        "# 1) Use need to initialize an agent with the current\n",
        "#    `replay_buffer` set to None. Also, leave the \n",
        "#    `use_cql` and `offline` flags to remain False.\n",
        "# 2) After each epoch, run `EPISODES_PER_RUN` validation\n",
        "#    episodes and plot the mean return over these \n",
        "#    episodes in the end.\n",
        "# 3) Plot the learning curves.\n",
        "##########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxqbDN6DU0AY"
      },
      "source": [
        "## Offline SAC training loop (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRwciG6WU6ES"
      },
      "source": [
        "In this part you are going to train an SAC agent using the replay buffer from the online agent. During training you sample from this replay buffer and train the offline agent **without adding transitions to the replay buffer**. The loss function and every thing else is the same as the online setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcArEy_EU5H0"
      },
      "outputs": [],
      "source": [
        "RUNS = 1\n",
        "NUM_EPOCHS = 200\n",
        "EPISODES_PER_RUN = 100\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "##########################################################\n",
        "# TODO (10 points): \n",
        "# Implement the training loop for the offline SAC. \n",
        "# 1) Use need to initialize an agent with the current\n",
        "#    `replay_buffer` of the online agent. Set the `offline`\n",
        "#     flag and leave the `use_cql` flag to remain False.\n",
        "# 2) You can use `batch_deterministic_start` in the\n",
        "#    `train_networks` method to select all minibatches\n",
        "#    of the data to train in an offline manner.\n",
        "# 3) After each epoch, run `EPISODES_PER_RUN` validation\n",
        "#    episodes and plot the mean return over these \n",
        "#    episodes in the end.\n",
        "##########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ8HlBr0kkZv"
      },
      "source": [
        "## Conservative SAC training loop (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOIESLFwXs7Q"
      },
      "source": [
        "Similar to the previous part, you are going to train another offline agent. In this part, you are going to use the conservative version of SAC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP_L7h0bYD4V"
      },
      "outputs": [],
      "source": [
        "RUNS = 1\n",
        "NUM_EPOCHS = 200\n",
        "EPISODES_PER_RUN = 100\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "##########################################################\n",
        "# TODO (5 points): \n",
        "# Implement the training loop for the conservative SAC. \n",
        "# 1) Use need to initialize an agent with the current\n",
        "#    `replay_buffer` of the online agent. Set the `offline`\n",
        "#     and `use_cql` flags.\n",
        "# 2) You can use `batch_deterministic_start` in the\n",
        "#    `train_networks` method to select all minibatches\n",
        "#    of the data to train in an offline manner.\n",
        "# 3) After each epoch, run `EPISODES_PER_RUN` validation\n",
        "#    episodes and plot the mean return over these \n",
        "#    episodes in the end.\n",
        "##########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q3LIScuYTeX"
      },
      "source": [
        "## Comparisons (14 points)\n",
        "Now, analyze your results and justify the trends you see. Then answer the following questions.\n",
        "\n",
        "❓ What is the reason for the difference between online and offline performance of the agent?\n",
        "\n",
        "❓ Which one is better: offline SAC or conservative SAC?\n",
        "\n",
        "❓ What is the effect of `TRADEOFF_FACTOR` in the offline setting? How does changing its value affect the results?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
