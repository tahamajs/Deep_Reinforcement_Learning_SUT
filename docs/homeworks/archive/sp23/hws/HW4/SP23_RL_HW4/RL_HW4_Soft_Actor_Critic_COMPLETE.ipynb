{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Soft Actor-Critic (SAC) for Reinforcement Learning: Complete Implementation and Analysis\n",
        "\n",
        "**Course:** Deep Reinforcement Learning  \n",
        "**Assignment:** HW4 - Soft Actor-Critic Agent (115 Points)  \n",
        "**Total Points:** 115\n",
        "\n",
        "---\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This notebook presents a comprehensive implementation of the Soft Actor-Critic (SAC) algorithm [1], a state-of-the-art off-policy deep reinforcement learning method. SAC maximizes a trade-off between expected return and entropy, encouraging exploration while learning optimal policies. We implement three variants: (1) **Online SAC** with environment interaction, (2) **Offline SAC** trained on fixed datasets, and (3) **Conservative SAC** using Conservative Q-Learning (CQL) [2] for robust offline learning. Experimental validation on the CartPole-v1 environment demonstrates the effectiveness of entropy regularization and the importance of conservatism in offline settings.\n",
        "\n",
        "**Keywords:** Soft Actor-Critic, Maximum Entropy RL, Offline RL, Conservative Q-Learning, Deep RL\n",
        "\n",
        "---\n",
        "\n",
        "## I. INTRODUCTION\n",
        "\n",
        "### A. Background\n",
        "\n",
        "Reinforcement Learning (RL) aims to learn optimal policies by maximizing cumulative rewards through environment interaction. Traditional RL algorithms face challenges in exploration-exploitation trade-offs and sample efficiency. Actor-critic methods combine value-based and policy-based approaches, using a critic to estimate value functions and an actor to update policies.\n",
        "\n",
        "### B. Soft Actor-Critic Overview\n",
        "\n",
        "Soft Actor-Critic (SAC) [1] addresses these challenges through:\n",
        "1. **Entropy Maximization**: Augments the standard RL objective with an entropy term\n",
        "2. **Off-Policy Learning**: Improves sample efficiency through experience replay\n",
        "3. **Stochastic Policies**: Maintains exploration throughout training\n",
        "4. **Automatic Temperature Tuning**: Adaptively adjusts exploration-exploitation balance\n",
        "\n",
        "### C. Contributions\n",
        "\n",
        "This implementation provides:\n",
        "- Complete SAC agent with discrete action spaces\n",
        "- Comparative analysis of online vs offline training paradigms\n",
        "- Conservative Q-Learning integration for offline RL\n",
        "- Empirical evaluation on standard benchmarks\n",
        "\n",
        "---\n",
        "\n",
        "## II. THEORETICAL FOUNDATIONS\n",
        "\n",
        "### A. Maximum Entropy Reinforcement Learning\n",
        "\n",
        "Standard RL maximizes expected cumulative reward:\n",
        "\n",
        "$$J_{\\\\text{standard}}(\\\\pi) = \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi}\\\\left[\\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r(s_t, a_t)\\\\right]$$\n",
        "\n",
        "SAC extends this with entropy regularization:\n",
        "\n",
        "$$J_{\\\\text{SAC}}(\\\\pi) = \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi}\\\\left[\\\\sum_{t=0}^{\\\\infty} \\\\gamma^t \\\\left(r(s_t, a_t) + \\\\alpha \\\\mathcal{H}(\\\\pi(\\\\cdot|s_t))\\\\right)\\\\right]$$\n",
        "\n",
        "where $\\\\mathcal{H}(\\\\pi(\\\\cdot|s_t)) = -\\\\mathbb{E}_{a \\\\sim \\\\pi}[\\\\log \\\\pi(a|s_t)]$ is the policy entropy and $\\\\alpha > 0$ is the temperature parameter controlling exploration.\n",
        "\n",
        "### B. Soft Policy Iteration\n",
        "\n",
        "SAC alternates between:\n",
        "\n",
        "**1) Soft Policy Evaluation**: Compute soft Q-function satisfying the soft Bellman equation:\n",
        "\n",
        "$$Q^{\\\\pi}(s_t, a_t) = r(s_t, a_t) + \\\\gamma \\\\mathbb{E}_{s_{t+1} \\\\sim p}[V^{\\\\pi}(s_{t+1})]$$\n",
        "\n",
        "where the soft state-value function is:\n",
        "\n",
        "$$V^{\\\\pi}(s_t) = \\\\mathbb{E}_{a_t \\\\sim \\\\pi}[Q^{\\\\pi}(s_t, a_t) - \\\\alpha \\\\log \\\\pi(a_t|s_t)]$$\n",
        "\n",
        "**2) Soft Policy Improvement**: Update policy towards:\n",
        "\n",
        "$$\\\\pi_{\\\\text{new}} = \\\\arg\\\\min_{\\\\pi'} D_{\\\\text{KL}}\\\\left(\\\\pi'(\\\\cdot|s_t) \\\\| \\\\frac{\\\\exp(Q^{\\\\pi_{\\\\text{old}}}(s_t, \\\\cdot))}{Z(s_t)}\\\\right)$$\n",
        "\n",
        "---\n",
        "\n",
        "## III. METHODOLOGY\n",
        "\n",
        "### A. Network Architecture\n",
        "\n",
        "We employ feedforward neural networks with the following architecture:\n",
        "- **Input Layer**: State dimension $d_s$\n",
        "- **Hidden Layer 1**: 256 neurons with ReLU activation\n",
        "- **Hidden Layer 2**: 256 neurons with ReLU activation  \n",
        "- **Output Layer**: Action dimension $d_a$ with task-specific activation\n",
        "\n",
        "### B. SAC Components\n",
        "\n",
        "**1) Critic Networks**: Two Q-networks $Q_{\\\\theta_1}, Q_{\\\\theta_2}$ to reduce overestimation bias (clipped double-Q learning)\n",
        "\n",
        "**2) Target Networks**: Slowly-updated copies $Q_{\\\\theta'_1}, Q_{\\\\theta'_2}$ for stable training\n",
        "\n",
        "**3) Actor Network**: Policy $\\\\pi_\\\\phi$ with Softmax output for discrete actions\n",
        "\n",
        "**4) Temperature Parameter**: Learnable $\\\\alpha$ with automatic tuning\n",
        "\n",
        "### C. Loss Functions\n",
        "\n",
        "**Critic Loss** (Mean Squared Bellman Error):\n",
        "\n",
        "$$L_Q(\\\\theta_i) = \\\\mathbb{E}_{(s,a,r,s',d) \\\\sim \\\\mathcal{D}}\\\\left[\\\\left(Q_{\\\\theta_i}(s,a) - y\\\\right)^2\\\\right]$$\n",
        "\n",
        "where target:\n",
        "\n",
        "$$y = r + \\\\gamma(1-d)\\\\sum_{a'} \\\\pi_\\\\phi(a'|s')\\\\left[\\\\min_{j=1,2} Q_{\\\\theta'_j}(s',a') - \\\\alpha \\\\log \\\\pi_\\\\phi(a'|s')\\\\right]$$\n",
        "\n",
        "**Actor Loss**:\n",
        "\n",
        "$$L_\\\\pi(\\\\phi) = \\\\mathbb{E}_{s \\\\sim \\\\mathcal{D}}\\\\left[\\\\sum_a \\\\pi_\\\\phi(a|s)\\\\left(\\\\alpha \\\\log \\\\pi_\\\\phi(a|s) - \\\\min_{j=1,2} Q_{\\\\theta_j}(s,a)\\\\right)\\\\right]$$\n",
        "\n",
        "**Temperature Loss**:\n",
        "\n",
        "$$L_\\\\alpha = \\\\mathbb{E}_{s \\\\sim \\\\mathcal{D}, a \\\\sim \\\\pi_\\\\phi}\\\\left[-\\\\alpha(\\\\log \\\\pi_\\\\phi(a|s) + \\\\bar{\\\\mathcal{H}})\\\\right]$$\n",
        "\n",
        "where $\\\\bar{\\\\mathcal{H}}$ is target entropy.\n",
        "\n",
        "### D. Conservative Q-Learning\n",
        "\n",
        "For offline RL, CQL adds a regularization term:\n",
        "\n",
        "$$L_{\\\\text{CQL}}(\\\\theta) = \\\\alpha_{\\\\text{CQL}}\\\\left(\\\\mathbb{E}_{s \\\\sim \\\\mathcal{D}}\\\\left[\\\\log\\\\sum_a \\\\exp Q_\\\\theta(s,a)\\\\right] - \\\\mathbb{E}_{(s,a) \\\\sim \\\\mathcal{D}}[Q_\\\\theta(s,a)]\\\\right) + L_Q(\\\\theta)$$\n",
        "\n",
        "This pushes down Q-values for out-of-distribution actions while maintaining values for in-dataset actions.\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "[1] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,\" in *ICML*, 2018.\n",
        "\n",
        "[2] A. Kumar, A. Zhou, G. Tucker, and S. Levine, \"Conservative q-learning for offline reinforcement learning,\" in *NeurIPS*, 2020.\n",
        "ue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IV. IMPLEMENTATION\n",
        "\n",
        "### A. Environment Setup and Dependencies\n",
        "\n",
        "We begin by importing required libraries and setting random seeds for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dependencies and Random Seed Configuration\n",
        "============================================\n",
        "This cell imports all necessary libraries and configures random seeds\n",
        "for reproducible experiments across PyTorch, NumPy, and Python's random module.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")\n",
        "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
        "print(f\"✓ Random seed set to: {seed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B. Neural Network Architecture (8 Points)\n",
        "\n",
        "The `Network` class implements a 3-layer feedforward neural network that serves as the foundation for both actor and critic networks in our SAC implementation.\n",
        "\n",
        "**Architecture Details:**\n",
        "- **Layer 1**: Input $\\rightarrow$ 256 neurons (ReLU activation)\n",
        "- **Layer 2**: 256 $\\rightarrow$ 256 neurons (ReLU activation)\n",
        "- **Layer 3**: 256 $\\rightarrow$ Output (Configurable activation)\n",
        "\n",
        "**Design Rationale:**\n",
        "1. **Hidden Layer Size (256)**: Sufficient capacity for CartPole while avoiding overfitting\n",
        "2. **ReLU Activation**: Provides non-linearity and computational efficiency\n",
        "3. **Modular Output Activation**: Allows `Identity` for critics and `Softmax` for actor\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "$$h_1 = \\text{ReLU}(W_1 x + b_1)$$\n",
        "$$h_2 = \\text{ReLU}(W_2 h_1 + b_2)$$\n",
        "$$y = \\sigma(W_3 h_2 + b_3)$$\n",
        "\n",
        "where $\\sigma$ is the output activation function and $x \\in \\mathbb{R}^{d_s}$ is the input state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Network(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward Neural Network for SAC\n",
        "    ==================================\n",
        "    A 3-layer fully-connected neural network used for both actor and critic networks.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dimension : int\n",
        "        Dimension of input features (state dimension)\n",
        "    output_dimension : int\n",
        "        Dimension of output (action dimension for actor, or action dimension for Q-values)\n",
        "    output_activation : torch.nn.Module\n",
        "        Activation function for output layer (default: Identity for critics, Softmax for actor)\n",
        "        \n",
        "    Architecture\n",
        "    ------------\n",
        "    Input → FC(256) → ReLU → FC(256) → ReLU → FC(output_dim) → output_activation\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        Network output of shape (batch_size, output_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dimension: int, output_dimension: int, \n",
        "                 output_activation: torch.nn.Module = torch.nn.Identity()):\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "        # SOLUTION: Define network layers (4 points)\n",
        "        # Layer 1: Input → 256 neurons\n",
        "        self.layer_1 = torch.nn.Linear(input_dimension, 256)\n",
        "        \n",
        "        # Layer 2: 256 → 256 neurons\n",
        "        self.layer_2 = torch.nn.Linear(256, 256)\n",
        "        \n",
        "        # Output layer: 256 → output_dimension\n",
        "        self.output_layer = torch.nn.Linear(256, output_dimension)\n",
        "        \n",
        "        # Store output activation function\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "    def forward(self, inpt: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        inpt : torch.Tensor\n",
        "            Input tensor of shape (batch_size, input_dimension)\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output tensor of shape (batch_size, output_dimension)\n",
        "        \"\"\"\n",
        "        \n",
        "        # SOLUTION: Implement forward pass (4 points)\n",
        "        # First hidden layer with ReLU activation\n",
        "        x = torch.nn.functional.relu(self.layer_1(inpt))\n",
        "        \n",
        "        # Second hidden layer with ReLU activation\n",
        "        x = torch.nn.functional.relu(self.layer_2(x))\n",
        "        \n",
        "        # Output layer with custom activation\n",
        "        output = self.output_activation(self.output_layer(x))\n",
        "        \n",
        "        return output\n",
        "\n",
        "# Test the network\n",
        "print(\"✓ Network class implemented successfully\")\n",
        "test_net = Network(4, 2)\n",
        "test_input = torch.randn(32, 4)\n",
        "test_output = test_net(test_input)\n",
        "print(f\"  Test: Input shape {test_input.shape} → Output shape {test_output.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C. Experience Replay Buffer\n",
        "\n",
        "The replay buffer stores transitions $(s, a, r, s', d)$ and enables off-policy learning by sampling mini-batches from past experiences. This breaks temporal correlations and improves sample efficiency.\n",
        "\n",
        "**Key Features:**\n",
        "1. **Prioritized Sampling**: Transitions are sampled with weights based on TD error\n",
        "2. **Circular Buffer**: Older experiences are overwritten when capacity is reached\n",
        "3. **Type Safety**: Uses NumPy structured arrays for efficient storage\n",
        "\n",
        "**Importance in SAC:**\n",
        "- Enables **off-policy** learning (can reuse old transitions)\n",
        "- **Decorrelates** samples (reduces variance in gradient estimates)\n",
        "- Allows **offline RL** by freezing the buffer and training without environment interaction\n",
        "\n",
        "**Buffer Operations:**\n",
        "- `add_transition()`: Store new $(s, a, r, s', d)$ tuple\n",
        "- `sample_minibatch()`: Randomly sample batch for training\n",
        "- `update_weights()`: Update priorities based on TD errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Experience Replay Buffer for Off-Policy RL\n",
        "    ==========================================\n",
        "    Stores and samples transitions for training SAC agent.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    environment : gym.Env\n",
        "        The environment to extract state/action space information\n",
        "    capacity : int, default=500000\n",
        "        Maximum number of transitions to store\n",
        "        \n",
        "    Attributes\n",
        "    ----------\n",
        "    buffer : np.ndarray\n",
        "        Circular buffer storing transitions\n",
        "    weights : np.ndarray\n",
        "        Priority weights for sampling\n",
        "    count : int\n",
        "        Current number of transitions stored\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, environment: gym.Env, capacity: int = 500000):\n",
        "        transition_type_str = self.get_transition_type_str(environment)\n",
        "        self.buffer = np.zeros(capacity, dtype=transition_type_str)\n",
        "        self.weights = np.zeros(capacity)\n",
        "        self.head_idx = 0\n",
        "        self.count = 0\n",
        "        self.capacity = capacity\n",
        "        self.max_weight = 10**-2\n",
        "        self.delta = 10**-4\n",
        "        self.indices = None\n",
        "        self.mirror_index = np.random.permutation(range(self.buffer.shape[0]))\n",
        "\n",
        "    def get_transition_type_str(self, environment: gym.Env) -> str:\n",
        "        \"\"\"Create NumPy dtype string for transition tuple.\"\"\"\n",
        "        state_dim = environment.observation_space.shape[0]\n",
        "        state_dim_str = '' if state_dim == () else str(state_dim)\n",
        "        state_type_str = environment.observation_space.sample().dtype.name\n",
        "        action_dim = environment.action_space.shape\n",
        "        action_dim_str = '' if action_dim == () else str(action_dim)\n",
        "        action_type_str = environment.action_space.sample().__class__.__name__\n",
        "\n",
        "        # Transition format: (state, action, reward, next_state, done)\n",
        "        transition_type_str = '{0}{1}, {2}{3}, float32, {0}{1}, bool'.format(\n",
        "            state_dim_str, state_type_str, action_dim_str, action_type_str)\n",
        "\n",
        "        return transition_type_str\n",
        "\n",
        "    def add_transition(self, transition: tuple):\n",
        "        \"\"\"\n",
        "        Add a new transition to the buffer.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        transition : tuple\n",
        "            (state, action, reward, next_state, done)\n",
        "        \"\"\"\n",
        "        self.buffer[self.head_idx] = transition\n",
        "        self.weights[self.head_idx] = self.max_weight\n",
        "\n",
        "        self.head_idx = (self.head_idx + 1) % self.capacity\n",
        "        self.count = min(self.count + 1, self.capacity)\n",
        "\n",
        "    def sample_minibatch(self, size: int = 100, \n",
        "                        batch_deterministic_start: Optional[int] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sample a minibatch of transitions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int\n",
        "            Number of transitions to sample\n",
        "        batch_deterministic_start : int, optional\n",
        "            If provided, sample deterministically starting from this index\n",
        "            (used for offline training to iterate through entire buffer)\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Array of sampled transitions\n",
        "        \"\"\"\n",
        "        set_weights = self.weights[:self.count] + self.delta\n",
        "        probabilities = set_weights / sum(set_weights)\n",
        "        \n",
        "        if batch_deterministic_start is None:\n",
        "            # Random sampling for online training\n",
        "            self.indices = np.random.choice(range(self.count), size, \n",
        "                                          p=probabilities, replace=False)\n",
        "        else:\n",
        "            # Deterministic sampling for offline training\n",
        "            self.indices = self.mirror_index[batch_deterministic_start:batch_deterministic_start+size]\n",
        "            \n",
        "        return self.buffer[self.indices]\n",
        "\n",
        "    def update_weights(self, prediction_errors: np.ndarray):\n",
        "        \"\"\"Update sampling weights based on TD errors.\"\"\"\n",
        "        max_error = max(prediction_errors)\n",
        "        self.max_weight = max(self.max_weight, max_error)\n",
        "        self.weights[self.indices] = prediction_errors\n",
        "\n",
        "    def get_size(self) -> int:\n",
        "        \"\"\"Return current buffer size.\"\"\"\n",
        "        return self.count\n",
        "\n",
        "print(\"✓ ReplayBuffer class implemented successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## V. CONCEPTUAL QUESTIONS (18 Points)\n",
        "\n",
        "This section addresses fundamental theoretical aspects of SAC, offline RL, and conservative Q-learning. Understanding these concepts is crucial for proper implementation and analysis.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
