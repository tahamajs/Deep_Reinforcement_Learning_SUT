{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Soft Actor-Critic (SAC) for Reinforcement Learning: Complete Implementation and Analysis\n",
        "\n",
        "**Course:** Deep Reinforcement Learning  \n",
        "**Assignment:** HW4 - Soft Actor-Critic Agent (115 Points)  \n",
        "**Total Points:** 115\n",
        "\n",
        "---\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This notebook presents a comprehensive implementation of the Soft Actor-Critic (SAC) algorithm [1], a state-of-the-art off-policy deep reinforcement learning method. SAC maximizes a trade-off between expected return and entropy, encouraging exploration while learning optimal policies. We implement three variants: (1) **Online SAC** with environment interaction, (2) **Offline SAC** trained on fixed datasets, and (3) **Conservative SAC** using Conservative Q-Learning (CQL) [2] for robust offline learning. Experimental validation on the CartPole-v1 environment demonstrates the effectiveness of entropy regularization and the importance of conservatism in offline settings.\n",
        "\n",
        "**Keywords:** Soft Actor-Critic, Maximum Entropy RL, Offline RL, Conservative Q-Learning, Deep RL\n",
        "\n",
        "---\n",
        "\n",
        "## I. INTRODUCTION\n",
        "\n",
        "### A. Background\n",
        "\n",
        "Reinforcement Learning (RL) aims to learn optimal policies by maximizing cumulative rewards through environment interaction. Traditional RL algorithms face challenges in exploration-exploitation trade-offs and sample efficiency. Actor-critic methods combine value-based and policy-based approaches, using a critic to estimate value functions and an actor to update policies.\n",
        "\n",
        "### B. Soft Actor-Critic Overview\n",
        "\n",
        "Soft Actor-Critic (SAC) [1] addresses these challenges through:\n",
        "1. **Entropy Maximization**: Augments the standard RL objective with an entropy term\n",
        "2. **Off-Policy Learning**: Improves sample efficiency through experience replay\n",
        "3. **Stochastic Policies**: Maintains exploration throughout training\n",
        "4. **Automatic Temperature Tuning**: Adaptively adjusts exploration-exploitation balance\n",
        "\n",
        "### C. Contributions\n",
        "\n",
        "This implementation provides:\n",
        "- Complete SAC agent with discrete action spaces\n",
        "- Comparative analysis of online vs offline training paradigms\n",
        "- Conservative Q-Learning integration for offline RL\n",
        "- Empirical evaluation on standard benchmarks\n",
        "\n",
        "---\n",
        "\n",
        "## II. THEORETICAL FOUNDATIONS\n",
        "\n",
        "### A. Maximum Entropy Reinforcement Learning\n",
        "\n",
        "Standard RL maximizes expected cumulative reward:\n",
        "\n",
        "$$J_{\\\\text{standard}}(\\\\pi) = \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi}\\\\left[\\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r(s_t, a_t)\\\\right]$$\n",
        "\n",
        "SAC extends this with entropy regularization:\n",
        "\n",
        "$$J_{\\\\text{SAC}}(\\\\pi) = \\\\mathbb{E}_{\\\\tau \\\\sim \\\\pi}\\\\left[\\\\sum_{t=0}^{\\\\infty} \\\\gamma^t \\\\left(r(s_t, a_t) + \\\\alpha \\\\mathcal{H}(\\\\pi(\\\\cdot|s_t))\\\\right)\\\\right]$$\n",
        "\n",
        "where $\\\\mathcal{H}(\\\\pi(\\\\cdot|s_t)) = -\\\\mathbb{E}_{a \\\\sim \\\\pi}[\\\\log \\\\pi(a|s_t)]$ is the policy entropy and $\\\\alpha > 0$ is the temperature parameter controlling exploration.\n",
        "\n",
        "### B. Soft Policy Iteration\n",
        "\n",
        "SAC alternates between:\n",
        "\n",
        "**1) Soft Policy Evaluation**: Compute soft Q-function satisfying the soft Bellman equation:\n",
        "\n",
        "$$Q^{\\\\pi}(s_t, a_t) = r(s_t, a_t) + \\\\gamma \\\\mathbb{E}_{s_{t+1} \\\\sim p}[V^{\\\\pi}(s_{t+1})]$$\n",
        "\n",
        "where the soft state-value function is:\n",
        "\n",
        "$$V^{\\\\pi}(s_t) = \\\\mathbb{E}_{a_t \\\\sim \\\\pi}[Q^{\\\\pi}(s_t, a_t) - \\\\alpha \\\\log \\\\pi(a_t|s_t)]$$\n",
        "\n",
        "**2) Soft Policy Improvement**: Update policy towards:\n",
        "\n",
        "$$\\\\pi_{\\\\text{new}} = \\\\arg\\\\min_{\\\\pi'} D_{\\\\text{KL}}\\\\left(\\\\pi'(\\\\cdot|s_t) \\\\| \\\\frac{\\\\exp(Q^{\\\\pi_{\\\\text{old}}}(s_t, \\\\cdot))}{Z(s_t)}\\\\right)$$\n",
        "\n",
        "---\n",
        "\n",
        "## III. METHODOLOGY\n",
        "\n",
        "### A. Network Architecture\n",
        "\n",
        "We employ feedforward neural networks with the following architecture:\n",
        "- **Input Layer**: State dimension $d_s$\n",
        "- **Hidden Layer 1**: 256 neurons with ReLU activation\n",
        "- **Hidden Layer 2**: 256 neurons with ReLU activation  \n",
        "- **Output Layer**: Action dimension $d_a$ with task-specific activation\n",
        "\n",
        "### B. SAC Components\n",
        "\n",
        "**1) Critic Networks**: Two Q-networks $Q_{\\\\theta_1}, Q_{\\\\theta_2}$ to reduce overestimation bias (clipped double-Q learning)\n",
        "\n",
        "**2) Target Networks**: Slowly-updated copies $Q_{\\\\theta'_1}, Q_{\\\\theta'_2}$ for stable training\n",
        "\n",
        "**3) Actor Network**: Policy $\\\\pi_\\\\phi$ with Softmax output for discrete actions\n",
        "\n",
        "**4) Temperature Parameter**: Learnable $\\\\alpha$ with automatic tuning\n",
        "\n",
        "### C. Loss Functions\n",
        "\n",
        "**Critic Loss** (Mean Squared Bellman Error):\n",
        "\n",
        "$$L_Q(\\\\theta_i) = \\\\mathbb{E}_{(s,a,r,s',d) \\\\sim \\\\mathcal{D}}\\\\left[\\\\left(Q_{\\\\theta_i}(s,a) - y\\\\right)^2\\\\right]$$\n",
        "\n",
        "where target:\n",
        "\n",
        "$$y = r + \\\\gamma(1-d)\\\\sum_{a'} \\\\pi_\\\\phi(a'|s')\\\\left[\\\\min_{j=1,2} Q_{\\\\theta'_j}(s',a') - \\\\alpha \\\\log \\\\pi_\\\\phi(a'|s')\\\\right]$$\n",
        "\n",
        "**Actor Loss**:\n",
        "\n",
        "$$L_\\\\pi(\\\\phi) = \\\\mathbb{E}_{s \\\\sim \\\\mathcal{D}}\\\\left[\\\\sum_a \\\\pi_\\\\phi(a|s)\\\\left(\\\\alpha \\\\log \\\\pi_\\\\phi(a|s) - \\\\min_{j=1,2} Q_{\\\\theta_j}(s,a)\\\\right)\\\\right]$$\n",
        "\n",
        "**Temperature Loss**:\n",
        "\n",
        "$$L_\\\\alpha = \\\\mathbb{E}_{s \\\\sim \\\\mathcal{D}, a \\\\sim \\\\pi_\\\\phi}\\\\left[-\\\\alpha(\\\\log \\\\pi_\\\\phi(a|s) + \\\\bar{\\\\mathcal{H}})\\\\right]$$\n",
        "\n",
        "where $\\\\bar{\\\\mathcal{H}}$ is target entropy.\n",
        "\n",
        "### D. Conservative Q-Learning\n",
        "\n",
        "For offline RL, CQL adds a regularization term:\n",
        "\n",
        "$$L_{\\\\text{CQL}}(\\\\theta) = \\\\alpha_{\\\\text{CQL}}\\\\left(\\\\mathbb{E}_{s \\\\sim \\\\mathcal{D}}\\\\left[\\\\log\\\\sum_a \\\\exp Q_\\\\theta(s,a)\\\\right] - \\\\mathbb{E}_{(s,a) \\\\sim \\\\mathcal{D}}[Q_\\\\theta(s,a)]\\\\right) + L_Q(\\\\theta)$$\n",
        "\n",
        "This pushes down Q-values for out-of-distribution actions while maintaining values for in-dataset actions.\n",
        "\n",
        "---\n",
        "\n",
        "## References\n",
        "\n",
        "[1] T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, \"Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,\" in *ICML*, 2018.\n",
        "\n",
        "[2] A. Kumar, A. Zhou, G. Tucker, and S. Levine, \"Conservative q-learning for offline reinforcement learning,\" in *NeurIPS*, 2020.\n",
        "ue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## IV. IMPLEMENTATION\n",
        "\n",
        "### A. Environment Setup and Dependencies\n",
        "\n",
        "We begin by importing required libraries and setting random seeds for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Dependencies and Random Seed Configuration\n",
        "============================================\n",
        "This cell imports all necessary libraries and configures random seeds\n",
        "for reproducible experiments across PyTorch, NumPy, and Python's random module.\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Tuple, Optional\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "print(\"✓ All libraries imported successfully\")\n",
        "print(f\"✓ PyTorch version: {torch.__version__}\")\n",
        "print(f\"✓ Random seed set to: {seed}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### B. Neural Network Architecture (8 Points)\n",
        "\n",
        "The `Network` class implements a 3-layer feedforward neural network that serves as the foundation for both actor and critic networks in our SAC implementation.\n",
        "\n",
        "**Architecture Details:**\n",
        "- **Layer 1**: Input $\\rightarrow$ 256 neurons (ReLU activation)\n",
        "- **Layer 2**: 256 $\\rightarrow$ 256 neurons (ReLU activation)\n",
        "- **Layer 3**: 256 $\\rightarrow$ Output (Configurable activation)\n",
        "\n",
        "**Design Rationale:**\n",
        "1. **Hidden Layer Size (256)**: Sufficient capacity for CartPole while avoiding overfitting\n",
        "2. **ReLU Activation**: Provides non-linearity and computational efficiency\n",
        "3. **Modular Output Activation**: Allows `Identity` for critics and `Softmax` for actor\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "\n",
        "$$h_1 = \\text{ReLU}(W_1 x + b_1)$$\n",
        "$$h_2 = \\text{ReLU}(W_2 h_1 + b_2)$$\n",
        "$$y = \\sigma(W_3 h_2 + b_3)$$\n",
        "\n",
        "where $\\sigma$ is the output activation function and $x \\in \\mathbb{R}^{d_s}$ is the input state.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Network(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward Neural Network for SAC\n",
        "    ==================================\n",
        "    A 3-layer fully-connected neural network used for both actor and critic networks.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dimension : int\n",
        "        Dimension of input features (state dimension)\n",
        "    output_dimension : int\n",
        "        Dimension of output (action dimension for actor, or action dimension for Q-values)\n",
        "    output_activation : torch.nn.Module\n",
        "        Activation function for output layer (default: Identity for critics, Softmax for actor)\n",
        "        \n",
        "    Architecture\n",
        "    ------------\n",
        "    Input → FC(256) → ReLU → FC(256) → ReLU → FC(output_dim) → output_activation\n",
        "    \n",
        "    Returns\n",
        "    -------\n",
        "    torch.Tensor\n",
        "        Network output of shape (batch_size, output_dimension)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_dimension: int, output_dimension: int, \n",
        "                 output_activation: torch.nn.Module = torch.nn.Identity()):\n",
        "        super(Network, self).__init__()\n",
        "        \n",
        "        # SOLUTION: Define network layers (4 points)\n",
        "        # Layer 1: Input → 256 neurons\n",
        "        self.layer_1 = torch.nn.Linear(input_dimension, 256)\n",
        "        \n",
        "        # Layer 2: 256 → 256 neurons\n",
        "        self.layer_2 = torch.nn.Linear(256, 256)\n",
        "        \n",
        "        # Output layer: 256 → output_dimension\n",
        "        self.output_layer = torch.nn.Linear(256, output_dimension)\n",
        "        \n",
        "        # Store output activation function\n",
        "        self.output_activation = output_activation\n",
        "\n",
        "    def forward(self, inpt: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        inpt : torch.Tensor\n",
        "            Input tensor of shape (batch_size, input_dimension)\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            Output tensor of shape (batch_size, output_dimension)\n",
        "        \"\"\"\n",
        "        \n",
        "        # SOLUTION: Implement forward pass (4 points)\n",
        "        # First hidden layer with ReLU activation\n",
        "        x = torch.nn.functional.relu(self.layer_1(inpt))\n",
        "        \n",
        "        # Second hidden layer with ReLU activation\n",
        "        x = torch.nn.functional.relu(self.layer_2(x))\n",
        "        \n",
        "        # Output layer with custom activation\n",
        "        output = self.output_activation(self.output_layer(x))\n",
        "        \n",
        "        return output\n",
        "\n",
        "# Test the network\n",
        "print(\"✓ Network class implemented successfully\")\n",
        "test_net = Network(4, 2)\n",
        "test_input = torch.randn(32, 4)\n",
        "test_output = test_net(test_input)\n",
        "print(f\"  Test: Input shape {test_input.shape} → Output shape {test_output.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### C. Experience Replay Buffer\n",
        "\n",
        "The replay buffer stores transitions $(s, a, r, s', d)$ and enables off-policy learning by sampling mini-batches from past experiences. This breaks temporal correlations and improves sample efficiency.\n",
        "\n",
        "**Key Features:**\n",
        "1. **Prioritized Sampling**: Transitions are sampled with weights based on TD error\n",
        "2. **Circular Buffer**: Older experiences are overwritten when capacity is reached\n",
        "3. **Type Safety**: Uses NumPy structured arrays for efficient storage\n",
        "\n",
        "**Importance in SAC:**\n",
        "- Enables **off-policy** learning (can reuse old transitions)\n",
        "- **Decorrelates** samples (reduces variance in gradient estimates)\n",
        "- Allows **offline RL** by freezing the buffer and training without environment interaction\n",
        "\n",
        "**Buffer Operations:**\n",
        "- `add_transition()`: Store new $(s, a, r, s', d)$ tuple\n",
        "- `sample_minibatch()`: Randomly sample batch for training\n",
        "- `update_weights()`: Update priorities based on TD errors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    Experience Replay Buffer for Off-Policy RL\n",
        "    ==========================================\n",
        "    Stores and samples transitions for training SAC agent.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    environment : gym.Env\n",
        "        The environment to extract state/action space information\n",
        "    capacity : int, default=500000\n",
        "        Maximum number of transitions to store\n",
        "        \n",
        "    Attributes\n",
        "    ----------\n",
        "    buffer : np.ndarray\n",
        "        Circular buffer storing transitions\n",
        "    weights : np.ndarray\n",
        "        Priority weights for sampling\n",
        "    count : int\n",
        "        Current number of transitions stored\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, environment: gym.Env, capacity: int = 500000):\n",
        "        transition_type_str = self.get_transition_type_str(environment)\n",
        "        self.buffer = np.zeros(capacity, dtype=transition_type_str)\n",
        "        self.weights = np.zeros(capacity)\n",
        "        self.head_idx = 0\n",
        "        self.count = 0\n",
        "        self.capacity = capacity\n",
        "        self.max_weight = 10**-2\n",
        "        self.delta = 10**-4\n",
        "        self.indices = None\n",
        "        self.mirror_index = np.random.permutation(range(self.buffer.shape[0]))\n",
        "\n",
        "    def get_transition_type_str(self, environment: gym.Env) -> str:\n",
        "        \"\"\"Create NumPy dtype string for transition tuple.\"\"\"\n",
        "        state_dim = environment.observation_space.shape[0]\n",
        "        state_dim_str = '' if state_dim == () else str(state_dim)\n",
        "        state_type_str = environment.observation_space.sample().dtype.name\n",
        "        action_dim = environment.action_space.shape\n",
        "        action_dim_str = '' if action_dim == () else str(action_dim)\n",
        "        action_type_str = environment.action_space.sample().__class__.__name__\n",
        "\n",
        "        # Transition format: (state, action, reward, next_state, done)\n",
        "        transition_type_str = '{0}{1}, {2}{3}, float32, {0}{1}, bool'.format(\n",
        "            state_dim_str, state_type_str, action_dim_str, action_type_str)\n",
        "\n",
        "        return transition_type_str\n",
        "\n",
        "    def add_transition(self, transition: tuple):\n",
        "        \"\"\"\n",
        "        Add a new transition to the buffer.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        transition : tuple\n",
        "            (state, action, reward, next_state, done)\n",
        "        \"\"\"\n",
        "        self.buffer[self.head_idx] = transition\n",
        "        self.weights[self.head_idx] = self.max_weight\n",
        "\n",
        "        self.head_idx = (self.head_idx + 1) % self.capacity\n",
        "        self.count = min(self.count + 1, self.capacity)\n",
        "\n",
        "    def sample_minibatch(self, size: int = 100, \n",
        "                        batch_deterministic_start: Optional[int] = None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Sample a minibatch of transitions.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int\n",
        "            Number of transitions to sample\n",
        "        batch_deterministic_start : int, optional\n",
        "            If provided, sample deterministically starting from this index\n",
        "            (used for offline training to iterate through entire buffer)\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        np.ndarray\n",
        "            Array of sampled transitions\n",
        "        \"\"\"\n",
        "        set_weights = self.weights[:self.count] + self.delta\n",
        "        probabilities = set_weights / sum(set_weights)\n",
        "        \n",
        "        if batch_deterministic_start is None:\n",
        "            # Random sampling for online training\n",
        "            self.indices = np.random.choice(range(self.count), size, \n",
        "                                          p=probabilities, replace=False)\n",
        "        else:\n",
        "            # Deterministic sampling for offline training\n",
        "            self.indices = self.mirror_index[batch_deterministic_start:batch_deterministic_start+size]\n",
        "            \n",
        "        return self.buffer[self.indices]\n",
        "\n",
        "    def update_weights(self, prediction_errors: np.ndarray):\n",
        "        \"\"\"Update sampling weights based on TD errors.\"\"\"\n",
        "        max_error = max(prediction_errors)\n",
        "        self.max_weight = max(self.max_weight, max_error)\n",
        "        self.weights[self.indices] = prediction_errors\n",
        "\n",
        "    def get_size(self) -> int:\n",
        "        \"\"\"Return current buffer size.\"\"\"\n",
        "        return self.count\n",
        "\n",
        "print(\"✓ ReplayBuffer class implemented successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## V. CONCEPTUAL QUESTIONS (18 Points)\n",
        "\n",
        "This section addresses fundamental theoretical aspects of SAC, offline RL, and conservative Q-learning. Understanding these concepts is crucial for proper implementation and analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 1: SAC Objective Function vs Standard RL (3 points)\n",
        "\n",
        "**Q:** We know that standard RL maximizes the expected sum of rewards. What is the objective function of SAC algorithm? Compare it to the standard RL loss.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Standard RL Objective:**\n",
        "\n",
        "The standard reinforcement learning objective maximizes the expected cumulative discounted reward:\n",
        "\n",
        "$$J_{\\text{standard}}(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)\\right]$$\n",
        "\n",
        "where:\n",
        "- $\\tau = (s_0, a_0, s_1, a_1, \\ldots)$ is a trajectory\n",
        "- $\\gamma \\in [0,1)$ is the discount factor\n",
        "- $r(s_t, a_t)$ is the immediate reward\n",
        "\n",
        "**SAC Maximum Entropy Objective:**\n",
        "\n",
        "SAC augments the standard objective with an entropy term to encourage exploration:\n",
        "\n",
        "$$J_{\\text{SAC}}(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t \\left(r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t))\\right)\\right]$$\n",
        "\n",
        "where:\n",
        "- $\\mathcal{H}(\\pi(\\cdot|s_t)) = -\\mathbb{E}_{a \\sim \\pi}[\\log \\pi(a|s_t)]$ is the policy entropy\n",
        "- $\\alpha > 0$ is the temperature parameter controlling exploration strength\n",
        "\n",
        "**Key Differences:**\n",
        "\n",
        "| Aspect | Standard RL | SAC |\n",
        "|--------|------------|-----|\n",
        "| **Objective** | Maximize reward only | Maximize reward + entropy |\n",
        "| **Exploration** | Relies on $\\epsilon$-greedy or noise | Built into objective function |\n",
        "| **Policy Type** | Can be deterministic | Always stochastic |\n",
        "| **Robustness** | May overfit to narrow policies | More robust, diverse behaviors |\n",
        "| **Sample Efficiency** | Varies | Generally higher due to exploration |\n",
        "\n",
        "**Intuitive Interpretation:**\n",
        "\n",
        "SAC seeks policies that not only maximize rewards but also remain as **random** (high entropy) as possible given the reward constraint. This encourages:\n",
        "- **Exploration**: High entropy maintains uncertainty and prevents premature convergence\n",
        "- **Robustness**: Multiple near-optimal actions are learned, making the policy adaptable\n",
        "- **Stability**: Prevents collapse to deterministic suboptimal policies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 2: Actor Cost Function (3 points)\n",
        "\n",
        "**Q:** Write down the actor cost function.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The actor (policy network) is trained to maximize the expected Q-value while maintaining high entropy. For discrete action spaces, the actor loss is:\n",
        "\n",
        "$$L_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}}\\left[\\sum_{a \\in \\mathcal{A}} \\pi_\\phi(a|s_t)\\left(\\alpha \\log \\pi_\\phi(a|s_t) - Q_\\theta(s_t, a)\\right)\\right]$$\n",
        "\n",
        "**Breakdown of Terms:**\n",
        "\n",
        "1. **Expectation over states**: $\\mathbb{E}_{s_t \\sim \\mathcal{D}}$ - Average over states sampled from replay buffer\n",
        "2. **Sum over actions**: $\\sum_{a \\in \\mathcal{A}}$ - Weighted sum over all possible actions\n",
        "3. **Policy probability**: $\\pi_\\phi(a|s_t)$ - Weight for each action\n",
        "4. **Entropy term**: $\\alpha \\log \\pi_\\phi(a|s_t)$ - Encourages high entropy (exploration)\n",
        "5. **Q-value term**: $Q_\\theta(s_t, a)$ - Expected return for taking action $a$\n",
        "\n",
        "**Alternative Formulation (Continuous Actions):**\n",
        "\n",
        "For continuous action spaces with reparameterization trick:\n",
        "\n",
        "$$L_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}, \\epsilon \\sim \\mathcal{N}}\\left[\\alpha \\log \\pi_\\phi(a_t|s_t) - Q_\\theta(s_t, a_t)\\right]$$\n",
        "\n",
        "where $a_t = f_\\phi(\\epsilon; s_t)$ is the reparameterized action.\n",
        "\n",
        "**Clipped Double-Q for Actor:**\n",
        "\n",
        "In practice, we use the minimum of two Q-networks to reduce overestimation:\n",
        "\n",
        "$$L_{\\pi}(\\phi) = \\mathbb{E}_{s_t \\sim \\mathcal{D}}\\left[\\sum_{a} \\pi_\\phi(a|s_t)\\left(\\alpha \\log \\pi_\\phi(a|s_t) - \\min_{j=1,2} Q_{\\theta_j}(s_t, a)\\right)\\right]$$\n",
        "\n",
        "**Gradient Interpretation:**\n",
        "\n",
        "The gradient $\\nabla_\\phi L_{\\pi}$ pushes the policy to:\n",
        "- **Increase probability** of actions with high Q-values (exploitation)\n",
        "- **Increase entropy** to maintain exploration (prevents collapse to deterministic policy)\n",
        "- Balance is controlled by temperature $\\alpha$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 3: Critic Cost Function (3 points)\n",
        "\n",
        "**Q:** Write down the critic cost function.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The critic networks estimate soft Q-values using the soft Bellman backup. The loss minimizes the mean squared temporal difference (TD) error:\n",
        "\n",
        "$$L_Q(\\theta_i) = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}, d_t) \\sim \\mathcal{D}}\\left[\\left(Q_{\\theta_i}(s_t, a_t) - y_t\\right)^2\\right]$$\n",
        "\n",
        "where the **soft Bellman backup target** is:\n",
        "\n",
        "$$y_t = r_t + \\gamma (1 - d_t) V(s_{t+1})$$\n",
        "\n",
        "and the **soft state-value function** is computed as:\n",
        "\n",
        "$$V(s_{t+1}) = \\mathbb{E}_{a_{t+1} \\sim \\pi}\\left[Q_{\\theta'}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi(a_{t+1}|s_{t+1})\\right]$$\n",
        "\n",
        "**For Discrete Actions (Used in This Implementation):**\n",
        "\n",
        "$$V(s_{t+1}) = \\sum_{a' \\in \\mathcal{A}} \\pi(a'|s_{t+1})\\left[\\min_{j=1,2} Q_{\\theta'_j}(s_{t+1}, a') - \\alpha \\log \\pi(a'|s_{t+1})\\right]$$\n",
        "\n",
        "**Complete Critic Loss:**\n",
        "\n",
        "$$L_Q(\\theta_i) = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}, d_t) \\sim \\mathcal{D}}\\Bigg[\\Big(Q_{\\theta_i}(s_t, a_t) - \\Big(r_t + \\gamma(1-d_t)$$\n",
        "$$\\times \\sum_{a'} \\pi_\\phi(a'|s_{t+1})\\left[\\min_{j=1,2} Q_{\\theta'_j}(s_{t+1}, a') - \\alpha \\log \\pi_\\phi(a'|s_{t+1})\\right]\\Big)\\Big)^2\\Bigg]$$\n",
        "\n",
        "**Key Components:**\n",
        "\n",
        "1. **Current Q-value**: $Q_{\\theta_i}(s_t, a_t)$ - Prediction from local critic\n",
        "2. **Immediate reward**: $r_t$ - Observed reward\n",
        "3. **Discount factor**: $\\gamma$ - Future reward weighting\n",
        "4. **Terminal flag**: $(1-d_t)$ - Zero out next state value if episode ends\n",
        "5. **Target Q-values**: $Q_{\\theta'_j}$ - From slowly-updated target networks\n",
        "6. **Entropy bonus**: $-\\alpha \\log \\pi$ - Encourages stochastic policy\n",
        "7. **Clipped double-Q**: $\\min_{j=1,2}$ - Reduces overestimation bias\n",
        "\n",
        "**Training Two Critics:**\n",
        "\n",
        "In SAC, we train **two separate critics** with independent losses:\n",
        "- $L_{Q_1}(\\theta_1)$ for first critic\n",
        "- $L_{Q_2}(\\theta_2)$ for second critic\n",
        "\n",
        "Both critics are trained simultaneously using the same targets but **independent optimizers**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 4: Two Critics Architecture (3 points)\n",
        "\n",
        "**Q:** Elaborate on the reason why most implementations of SAC use two critics (one local and one target).\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "SAC actually uses **FOUR critic networks**: two local critics and two target critics. This design addresses two critical challenges in deep RL.\n",
        "\n",
        "**1. Two Local Critics (Clipped Double-Q Learning)**\n",
        "\n",
        "**Purpose:** Reduce **overestimation bias** in Q-value estimates.\n",
        "\n",
        "**Problem with Single Critic:**\n",
        "- Standard Q-learning suffers from maximization bias\n",
        "- $\\max_a Q(s,a)$ tends to overestimate true Q-values due to noise\n",
        "- This leads to unstable training and poor convergence\n",
        "\n",
        "**Solution - Clipped Double-Q:**\n",
        "- Train two independent critics: $Q_{\\theta_1}$ and $Q_{\\theta_2}$\n",
        "- For target computation, use the **minimum**:\n",
        "\n",
        "$$Q_{\\text{target}}(s,a) = \\min(Q_{\\theta_1}(s,a), Q_{\\theta_2}(s,a))$$\n",
        "\n",
        "**Why This Works:**\n",
        "- Taking minimum provides a **lower bound** on Q-values\n",
        "- Reduces optimistic bias while maintaining reasonable estimates\n",
        "- More conservative → more stable learning\n",
        "\n",
        "**2. Two Target Networks (Stable Training Targets)**\n",
        "\n",
        "**Purpose:** Provide **stable, slowly-changing targets** for TD learning.\n",
        "\n",
        "**Problem with Single Network:**\n",
        "- If we use same network for prediction and target:\n",
        "  $$L = (Q_\\theta(s,a) - (r + \\gamma \\max_{a'} Q_\\theta(s',a')))^2$$\n",
        "- Target keeps changing as we update $\\theta$ → **moving target problem**\n",
        "- Causes oscillations and divergence\n",
        "\n",
        "**Solution - Target Networks:**\n",
        "- Maintain separate target networks: $Q_{\\theta'_1}$ and $Q_{\\theta'_2}$\n",
        "- Update them slowly using **soft updates**:\n",
        "\n",
        "$$\\theta'_i \\leftarrow \\tau \\theta_i + (1-\\tau)\\theta'_i$$\n",
        "\n",
        "where $\\tau \\ll 1$ (typically 0.005 - 0.01)\n",
        "\n",
        "**Why This Works:**\n",
        "- Targets change slowly → more stable learning\n",
        "- Reduces correlation between prediction and target\n",
        "- Prevents \"chasing a moving target\" phenomenon\n",
        "\n",
        "**Complete Architecture Summary:**\n",
        "\n",
        "| Network | Role | Update Method |\n",
        "|---------|------|---------------|\n",
        "| $Q_{\\theta_1}$ | Local critic 1 | Gradient descent on $L_{Q_1}$ |\n",
        "| $Q_{\\theta_2}$ | Local critic 2 | Gradient descent on $L_{Q_2}$ |\n",
        "| $Q_{\\theta'_1}$ | Target critic 1 | Soft update from $Q_{\\theta_1}$ |\n",
        "| $Q_{\\theta'_2}$ | Target critic 2 | Soft update from $Q_{\\theta_2}$ |\n",
        "\n",
        "**Training Algorithm:**\n",
        "\n",
        "```python\n",
        "# 1. Compute targets using target networks\n",
        "V_target = min(Q_θ'₁(s',a'), Q_θ'₂(s',a')) - α log π(a'|s')\n",
        "y = r + γ(1-d) * V_target\n",
        "\n",
        "# 2. Update local critics\n",
        "L_Q1 = (Q_θ₁(s,a) - y)²\n",
        "L_Q2 = (Q_θ₂(s,a) - y)²\n",
        "\n",
        "# 3. Soft update targets\n",
        "θ'₁ ← τθ₁ + (1-τ)θ'₁\n",
        "θ'₂ ← τθ₂ + (1-τ)θ'₂\n",
        "```\n",
        "\n",
        "**Benefits of This Design:**\n",
        "- ✓ Reduces overestimation bias\n",
        "- ✓ Stabilizes training\n",
        "- ✓ Improves convergence\n",
        "- ✓ Better final performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 5: Online vs Offline Training Samples (3 points)\n",
        "\n",
        "**Q:** What is the difference between training samples in offline and online settings?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The fundamental difference lies in **how data is collected and used during training**.\n",
        "\n",
        "**Online Reinforcement Learning**\n",
        "\n",
        "**Data Collection:**\n",
        "- Agent **actively interacts** with environment during training\n",
        "- Collects new transitions: $(s_t, a_t, r_t, s_{t+1}, d_t)$ at each step\n",
        "- Continuously adds experiences to replay buffer\n",
        "- Can explore new regions of state-action space\n",
        "\n",
        "**Training Process:**\n",
        "```python\n",
        "for episode in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    for step in range(max_steps):\n",
        "        action = agent.get_action(state)  # Sample from current policy\n",
        "        next_state, reward, done = env.step(action)\n",
        "        \n",
        "        # Add NEW transition to buffer\n",
        "        buffer.add(state, action, reward, next_state, done)\n",
        "        \n",
        "        # Train on MIXED data (old + new)\n",
        "        batch = buffer.sample()\n",
        "        agent.update(batch)\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- ✓ **Exploration**: Can discover new strategies\n",
        "- ✓ **Adaptation**: Policy improves → data distribution improves\n",
        "- ✓ **On-policy convergence**: Eventually samples from optimal policy\n",
        "- ✗ **Sample inefficiency**: Requires many environment interactions\n",
        "- ✗ **Safety concerns**: May take dangerous actions during exploration\n",
        "\n",
        "**Offline Reinforcement Learning**\n",
        "\n",
        "**Data Collection:**\n",
        "- Uses **pre-collected fixed dataset** $\\mathcal{D}_{\\text{fixed}}$\n",
        "- NO environment interaction during training\n",
        "- Dataset collected by behavior policy $\\pi_\\beta$ (often suboptimal or human)\n",
        "- Cannot add new experiences\n",
        "\n",
        "**Training Process:**\n",
        "```python\n",
        "# Collect dataset ONCE (before training)\n",
        "dataset = collect_data_with_behavior_policy()\n",
        "\n",
        "# Train WITHOUT environment interaction\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in iterate_over_dataset(dataset):\n",
        "        # Train ONLY on fixed data\n",
        "        agent.update(batch)\n",
        "    \n",
        "    # Evaluate (optional environment interaction)\n",
        "    evaluate_policy(agent)\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "- ✓ **Safety**: No risky exploration in real environment\n",
        "- ✓ **Sample efficiency**: No environment interactions needed\n",
        "- ✓ **Batch learning**: Can leverage large offline datasets\n",
        "- ✗ **Distribution shift**: $\\pi_\\theta$ diverges from $\\pi_\\beta$\n",
        "- ✗ **Extrapolation error**: Poor Q-estimates for OOD actions\n",
        "- ✗ **Limited coverage**: Bounded by dataset quality\n",
        "\n",
        "**Key Differences Comparison**\n",
        "\n",
        "| Aspect | Online RL | Offline RL |\n",
        "|--------|-----------|------------|\n",
        "| **Data Source** | Live environment interaction | Fixed pre-collected dataset |\n",
        "| **Buffer Updates** | Continuously growing | Static, no new data |\n",
        "| **Policy Distribution** | $\\pi_\\theta$ samples current states | $\\pi_\\beta$ (behavior) ≠ $\\pi_\\theta$ (learned) |\n",
        "| **Exploration** | Active exploration possible | Limited to dataset coverage |\n",
        "| **Distribution Shift** | Minimal (self-correcting) | **Major challenge** |\n",
        "| **Sample Generation** | $s_{t+1} \\sim p(\\cdot|s_t, a_t), a_t \\sim \\pi_\\theta$ | $s,a,r,s' \\sim \\mathcal{D}_{\\text{fixed}}$ |\n",
        "| **Q-value Reliability** | Good for visited states | Poor for out-of-distribution actions |\n",
        "| **Use Cases** | Simulators, games, safe environments | Robotics, healthcare, autonomous driving |\n",
        "\n",
        "**Distribution Shift Problem (Critical in Offline RL)**\n",
        "\n",
        "**Online:** Policy and data distribution co-evolve\n",
        "$$\\mathcal{D}_t \\sim \\pi_\\theta^{(t)} \\rightarrow \\text{Update } \\theta \\rightarrow \\mathcal{D}_{t+1} \\sim \\pi_\\theta^{(t+1)}$$\n",
        "\n",
        "**Offline:** Policy diverges from data distribution\n",
        "$$\\mathcal{D} \\sim \\pi_\\beta \\text{ (fixed)}, \\quad \\pi_\\theta \\rightarrow \\text{different distribution}$$\n",
        "\n",
        "When $\\pi_\\theta$ assigns high probability to actions rare in $\\mathcal{D}$:\n",
        "- Q-values for those actions are **overestimated** (extrapolation error)\n",
        "- Policy exploits these errors → **poor performance**\n",
        "\n",
        "**Solution:** Conservative methods like CQL explicitly penalize OOD actions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Question 6: Conservative Q-Learning (CQL) Impact (3 points)\n",
        "\n",
        "**Q:** How does adding CQL on top of SAC change the objective function?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Conservative Q-Learning (CQL) modifies the critic loss to explicitly prevent overestimation of out-of-distribution (OOD) actions in offline RL.\n",
        "\n",
        "**Standard SAC Critic Loss:**\n",
        "\n",
        "$$L_{\\text{SAC}}(\\theta) = \\mathbb{E}_{(s,a,r,s',d) \\sim \\mathcal{D}}\\left[(Q_\\theta(s,a) - y)^2\\right]$$\n",
        "\n",
        "where $y$ is the soft Bellman backup target.\n",
        "\n",
        "**CQL Modified Critic Loss:**\n",
        "\n",
        "$$L_{\\text{CQL}}(\\theta) = \\underbrace{\\alpha_{\\text{CQL}} \\cdot \\mathcal{R}(\\theta)}_{\\text{Conservative Regularizer}} + \\underbrace{L_{\\text{SAC}}(\\theta)}_{\\text{Standard Bellman Error}}$$\n",
        "\n",
        "where the **conservative regularizer** is:\n",
        "\n",
        "$$\\mathcal{R}(\\theta) = \\mathbb{E}_{s \\sim \\mathcal{D}}\\left[\\log \\sum_{a} \\exp(Q_\\theta(s,a))\\right] - \\mathbb{E}_{(s,a) \\sim \\mathcal{D}}[Q_\\theta(s,a)]$$\n",
        "\n",
        "**Breaking Down the Regularizer:**\n",
        "\n",
        "**First Term** (pushes Q-values DOWN):\n",
        "$$\\mathbb{E}_{s \\sim \\mathcal{D}}\\left[\\log \\sum_{a} \\exp(Q_\\theta(s,a))\\right]$$\n",
        "- Log-sum-exp over ALL actions (including OOD)\n",
        "- Approximates $\\max_a Q_\\theta(s,a)$\n",
        "- Penalizes high Q-values for any action\n",
        "\n",
        "**Second Term** (pushes Q-values UP for dataset actions):\n",
        "$$-\\mathbb{E}_{(s,a) \\sim \\mathcal{D}}[Q_\\theta(s,a)]$$\n",
        "- Negative expectation over actions IN dataset\n",
        "- Encourages high Q-values for observed actions\n",
        "- Counterbalances the first term\n",
        "\n",
        "**Net Effect:**\n",
        "- **Decreases** Q-values for **out-of-distribution** actions\n",
        "- **Maintains** Q-values for **in-distribution** actions  \n",
        "- Creates a **lower bound** on Q-values\n",
        "\n",
        "**Alternative Formulation (Discrete Actions):**\n",
        "\n",
        "For discrete action spaces, the log-sum-exp can be computed exactly:\n",
        "\n",
        "$$\\mathcal{R}(\\theta) = \\mathbb{E}_{s \\sim \\mathcal{D}}\\left[\\log \\sum_{a \\in \\mathcal{A}} \\exp(Q_\\theta(s,a)) - \\sum_{a \\in \\mathcal{A}} \\pi_\\beta(a|s) Q_\\theta(s,a)\\right]$$\n",
        "\n",
        "where $\\pi_\\beta$ is the behavior policy that generated the dataset.\n",
        "\n",
        "**Intuitive Understanding:**\n",
        "\n",
        "Imagine a state $s$ where:\n",
        "- Action $a_1$ appears 100 times in dataset → high confidence\n",
        "- Action $a_2$ appears 0 times in dataset → no data\n",
        "\n",
        "**Standard SAC might predict:**\n",
        "- $Q(s, a_1) = 10$ ✓ (reliable estimate)\n",
        "- $Q(s, a_2) = 15$ ✗ (overestimated due to extrapolation)\n",
        "\n",
        "**CQL corrects this:**\n",
        "- $Q_{\\text{CQL}}(s, a_1) = 10$ ✓ (maintained)\n",
        "- $Q_{\\text{CQL}}(s, a_2) = 5$ ✓ (penalized for being OOD)\n",
        "\n",
        "**Mathematical Justification:**\n",
        "\n",
        "CQL learns a **lower bound** $Q^\\pi$ on the true Q-function $Q^*$:\n",
        "\n",
        "$$Q^\\pi(s,a) \\leq Q^*(s,a), \\quad \\forall (s,a) \\text{ where } \\pi(a|s) > 0$$\n",
        "\n",
        "This conservatism prevents the policy from exploiting overestimated Q-values for unseen actions.\n",
        "\n",
        "**Hyperparameter $\\alpha_{\\text{CQL}}$ (Trade-off Factor):**\n",
        "\n",
        "Controls strength of conservatism:\n",
        "- **Low** $\\alpha_{\\text{CQL}}$ (e.g., 0.1): Weak regularization, closer to standard SAC\n",
        "- **Medium** $\\alpha_{\\text{CQL}}$ (e.g., 1-10): Balanced conservatism\n",
        "- **High** $\\alpha_{\\text{CQL}}$ (e.g., 50+): Very conservative, may underestimate\n",
        "\n",
        "**Complete CQL-SAC Objective:**\n",
        "\n",
        "$$\\min_\\theta \\mathbb{E}_{(s,a,r,s',d) \\sim \\mathcal{D}}\\Bigg[\\underbrace{\\alpha_{\\text{CQL}} \\left(\\log \\sum_{a'} \\exp(Q_\\theta(s,a')) - Q_\\theta(s,a)\\right)}_{\\text{CQL Penalty}} + \\underbrace{(Q_\\theta(s,a) - y)^2}_{\\text{Bellman Error}}\\Bigg]$$\n",
        "\n",
        "**Benefits in Offline RL:**\n",
        "- ✓ Reduces extrapolation error\n",
        "- ✓ More stable training\n",
        "- ✓ Better performance on limited datasets\n",
        "- ✓ Provable lower bound on Q-values\n",
        "- ✓ Prevents policy from taking unseen actions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## VI. SAC AGENT IMPLEMENTATION (50 Points)\n",
        "\n",
        "This section implements the complete Soft Actor-Critic agent, including:\n",
        "- Critic networks with loss computation\n",
        "- Actor network with policy optimization  \n",
        "- Automatic temperature tuning\n",
        "- Training loop with gradient updates\n",
        "- Conservative Q-Learning (CQL) support for offline RL\n",
        "\n",
        "The implementation supports both online and offline training paradigms.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SACAgent:\n",
        "    \"\"\"\n",
        "    Soft Actor-Critic Agent\n",
        "    ========================\n",
        "    Complete implementation of SAC algorithm with support for:\n",
        "    - Online training (with environment interaction)\n",
        "    - Offline training (from fixed dataset)\n",
        "    - Conservative Q-Learning (CQL) for offline RL\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    environment : gym.Env\n",
        "        The environment to train on\n",
        "    replay_buffer : ReplayBuffer, optional\n",
        "        Pre-filled replay buffer for offline training\n",
        "    use_cql : bool, default=False\n",
        "        Whether to use Conservative Q-Learning regularization\n",
        "    offline : bool, default=False\n",
        "        Whether to train in offline mode (no new data collection)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Hyperparameters\n",
        "    ALPHA_INITIAL = 1.0\n",
        "    REPLAY_BUFFER_BATCH_SIZE = 100\n",
        "    DISCOUNT_RATE = 0.99\n",
        "    LEARNING_RATE = 10 ** -4\n",
        "    SOFT_UPDATE_INTERPOLATION_FACTOR = 0.01\n",
        "    TRADEOFF_FACTOR = 5  # CQL regularization strength\n",
        "\n",
        "    def __init__(self, environment: gym.Env, replay_buffer: Optional[ReplayBuffer] = None, \n",
        "                 use_cql: bool = False, offline: bool = False):\n",
        "        \n",
        "        # Validation checks\n",
        "        assert not use_cql or offline, 'CQL requires offline mode to be enabled.'\n",
        "        assert not offline or replay_buffer is not None, 'Offline mode requires a replay buffer.'\n",
        "        \n",
        "        self.environment = environment\n",
        "        self.state_dim = self.environment.observation_space.shape[0]\n",
        "        self.action_dim = self.environment.action_space.n\n",
        "        \n",
        "        self.offline = offline\n",
        "        self.replay_buffer = ReplayBuffer(self.environment) if replay_buffer is None else replay_buffer\n",
        "        self.use_cql = use_cql\n",
        "        \n",
        "        # ============================================================\n",
        "        # SOLUTION: Initialize Critics (6 points)\n",
        "        # ============================================================\n",
        "        \n",
        "        print(\"Initializing SAC Agent...\")\n",
        "        print(f\"  State dim: {self.state_dim}, Action dim: {self.action_dim}\")\n",
        "        print(f\"  Mode: {'Offline' if offline else 'Online'}\")\n",
        "        print(f\"  CQL: {'Enabled' if use_cql else 'Disabled'}\")\n",
        "        \n",
        "        # Two local critic networks for clipped double-Q learning\n",
        "        self.critic_local = Network(self.state_dim, self.action_dim)\n",
        "        self.critic_local2 = Network(self.state_dim, self.action_dim)\n",
        "        \n",
        "        # Separate optimizers for each critic\n",
        "        self.critic_optimiser = optim.Adam(self.critic_local.parameters(), lr=self.LEARNING_RATE)\n",
        "        self.critic_optimiser2 = optim.Adam(self.critic_local2.parameters(), lr=self.LEARNING_RATE)\n",
        "        \n",
        "        # Two target critic networks for stable training targets\n",
        "        self.critic_target = Network(self.state_dim, self.action_dim)\n",
        "        self.critic_target2 = Network(self.state_dim, self.action_dim)\n",
        "        \n",
        "        # Initialize target networks with local network weights\n",
        "        self.soft_update_target_networks(tau=1.0)\n",
        "        \n",
        "        # ============================================================\n",
        "        # SOLUTION: Initialize Actor (2 points)\n",
        "        # ============================================================\n",
        "        \n",
        "        # Actor network with Softmax output for discrete action probabilities\n",
        "        self.actor_local = Network(self.state_dim, self.action_dim,\n",
        "                                   output_activation=torch.nn.Softmax(dim=-1))\n",
        "        \n",
        "        # Actor optimizer\n",
        "        self.actor_optimiser = optim.Adam(self.actor_local.parameters(), lr=self.LEARNING_RATE)\n",
        "        \n",
        "        # ============================================================\n",
        "        # Temperature (Entropy Coefficient) Setup\n",
        "        # ============================================================\n",
        "        \n",
        "        # Target entropy: H_target = -log(1/|A|) = log(|A|)\n",
        "        # We use 0.98 * target for slightly lower entropy\n",
        "        self.target_entropy = 0.98 * -np.log(1 / self.environment.action_space.n)\n",
        "        \n",
        "        # Learnable log(alpha) for numerical stability\n",
        "        self.log_alpha = torch.tensor(np.log(self.ALPHA_INITIAL), requires_grad=True)\n",
        "        self.alpha = self.log_alpha.exp()\n",
        "        self.alpha_optimiser = torch.optim.Adam([self.log_alpha], lr=self.LEARNING_RATE)\n",
        "        \n",
        "        print(f\"  Target entropy: {self.target_entropy:.3f}\")\n",
        "        print(f\"  Initial alpha: {self.alpha.item():.3f}\")\n",
        "        print(\"✓ SAC Agent initialized successfully\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    # ================================================================\n",
        "    # Action Selection Methods\n",
        "    # ================================================================\n",
        "    \n",
        "    def get_next_action(self, state: np.ndarray, evaluation_episode: bool = False) -> int:\n",
        "        \"\"\"\n",
        "        Select action given current state.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        state : np.ndarray\n",
        "            Current state\n",
        "        evaluation_episode : bool\n",
        "            If True, use deterministic (greedy) policy; else stochastic\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            Selected discrete action\n",
        "        \"\"\"\n",
        "        if evaluation_episode:\n",
        "            return self.get_action_deterministically(state)\n",
        "        else:\n",
        "            return self.get_action_nondeterministically(state)\n",
        "    \n",
        "    def get_action_nondeterministically(self, state: np.ndarray) -> int:\n",
        "        \"\"\"Sample action from stochastic policy (for training/exploration).\"\"\"\n",
        "        action_probabilities = self.get_action_probabilities(state)\n",
        "        discrete_action = np.random.choice(range(self.action_dim), p=action_probabilities)\n",
        "        return discrete_action\n",
        "    \n",
        "    def get_action_deterministically(self, state: np.ndarray) -> int:\n",
        "        \"\"\"Select greedy action (for evaluation).\"\"\"\n",
        "        action_probabilities = self.get_action_probabilities(state)\n",
        "        discrete_action = np.argmax(action_probabilities)\n",
        "        return discrete_action\n",
        "    \n",
        "    def get_action_probabilities(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Get action probability distribution from actor network.\"\"\"\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        action_probabilities = self.actor_local.forward(state_tensor)\n",
        "        return action_probabilities.squeeze(0).detach().numpy()\n",
        "    \n",
        "    def get_action_info(self, states_tensor: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Get action probabilities and log probabilities for a batch of states.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        states_tensor : torch.Tensor\n",
        "            Batch of states (batch_size, state_dim)\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        action_probabilities : torch.Tensor\n",
        "            Action probabilities (batch_size, action_dim)\n",
        "        log_action_probabilities : torch.Tensor\n",
        "            Log probabilities (batch_size, action_dim)\n",
        "        \"\"\"\n",
        "        action_probabilities = self.actor_local.forward(states_tensor)\n",
        "        \n",
        "        # Add small epsilon to avoid log(0)\n",
        "        z = (action_probabilities == 0.0).float() * 1e-8\n",
        "        log_action_probabilities = torch.log(action_probabilities + z)\n",
        "        \n",
        "        return action_probabilities, log_action_probabilities\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
