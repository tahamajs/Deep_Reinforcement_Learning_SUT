{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "96a1be8a2e9f4a1980fdbe0fa0e738ce",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Monte-carlo Tree Search\n",
    "\n",
    "\n",
    "In this notebook, we'll implement a MCTS planning and use it to solve a Gym env.\n",
    "\n",
    "![image.png](https://i.postimg.cc/6QmwnjPS/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "964899ece4104376be22f5b951e5f420",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "__How it works?__\n",
    "We just start with an empty tree and expand it. There are several common procedures.\n",
    "\n",
    "__1) Selection__\n",
    "Starting from the root, recursively select the node that corresponds to the tree policy.  \n",
    "\n",
    "There are several options for tree policies, which we saw earlier as exploration strategies: epsilon-greedy, Thomson sampling, UCB-1. It was shown that in MCTS, UCB-1 achieves a good result. Further, we will consider the one, but you can try to use others.\n",
    "\n",
    "Following the UCB-1 tree policy, we will choose an action that, on one hand, we expect to have the highest return, and on the other hand, we haven't explored much.\n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{a} = \\argmax_{a} \\dot{Q}(s, a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{Q}(s, a) = Q(s, a) + C_p \\sqrt{\\frac{2 \\log {N}}{n_a}}\n",
    "$$\n",
    "\n",
    "where: \n",
    "- $N$ - number of times we have visited state $s$,\n",
    "- $n_a$ - number of times we have taken action $a$,\n",
    "- $C_p$ - exploration balance parameter, which is performed between exploration and exploitation. \n",
    "\n",
    "Using Hoeffding inequality for rewards $R \\in [0,1]$ it can be shown that optimal $C_p = 1/\\sqrt{2}$. For rewards outside this range, the parameter should be tuned. We'll be using 10, but you can experiment with other values.\n",
    "\n",
    "__2) Expansion__\n",
    "After the selection procedure, we can achieve a leaf node or node in which we don't complete actions. In this case, we expand the tree by feasible actions and get new state nodes. \n",
    "\n",
    "__3) Simulation__\n",
    "How we can estimate node Q-values? The idea is to estimate action values for a given _rollout policy_ by averaging the return of many simulated trajectories from the current node. Simply, we can play with random or some special policy or use some model that can estimate it.\n",
    "\n",
    "__4) Backpropagation__\n",
    "The reward of the last simulation is backed up through the traversed nodes and propagates Q-value estimations, upwards to the root.\n",
    "\n",
    "$$\n",
    "Q({\\text{parent}}, a) = r + \\gamma \\cdot Q({\\text{child}}, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a21811d38df94892b5e8d81161fbfde6",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e8326f43f33a4416a654d417809f349d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We first need to make a wrapper for Gym environments to allow saving and loading game states to facilitate backtracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "77e5f887a3844683a8cd454f573bf8d8",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.core import Wrapper\n",
    "from pickle import dumps, loads\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "ActionResult = namedtuple(\n",
    "    \"action_result\", (\"snapshot\", \"observation\", \"reward\", \"is_done\", \"info\"))\n",
    "\n",
    "\n",
    "class WithSnapshots(Wrapper):\n",
    "    # Creates a wrapper that supports saving and loading environemnt states.\n",
    "    # Required for planning algorithms.\n",
    "\n",
    "\n",
    "    def get_snapshot(self, render=False):\n",
    "        # returns environment state that can be loaded with load_snapshot.\n",
    "        \n",
    "        \n",
    "        if render:\n",
    "            self.render()  \n",
    "            self.close()\n",
    "            \n",
    "        if self.unwrapped.viewer is not None:\n",
    "            self.unwrapped.viewer.close()\n",
    "            self.unwrapped.viewer = None\n",
    "        return dumps(self.env)\n",
    "    \n",
    "\n",
    "    def load_snapshot(self, snapshot, render=False):\n",
    "        # loads snapshot as current env state.\n",
    "        \n",
    "        assert not hasattr(self, \"_monitor\") or hasattr(\n",
    "            self.env, \"_monitor\"), \"can't backtrack while recording\"\n",
    "\n",
    "        if render:\n",
    "            self.render()  \n",
    "            self.close()\n",
    "        self.env = loads(snapshot)\n",
    "        \n",
    "\n",
    "    def get_result(self, snapshot, action):\n",
    "        # Breturns next snapshot and everything that env.step would have returned.\n",
    "        \n",
    "        self.load_snapshot(snapshot, render=False)\n",
    "        next_observation, reward, is_done, info = self.step(action)\n",
    "        next_snapshot = self.get_snapshot()\n",
    "        return ActionResult(next_snapshot, next_observation, reward, is_done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6e73f4a5cf6d47c4aa5f559f6a04bf8f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Try out snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f1319deede264069ac29749fc06d36ca",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# make env\n",
    "env = WithSnapshots(gym.make(\"CartPole-v0\"))\n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a92ac001af624e07978c6475e66e558f",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "print(\"initial_state:\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()\n",
    "\n",
    "# create first snapshot\n",
    "snap0 = env.get_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a45ce51b5a874edbb459aa447be8b732",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# play without making snapshots (faster)\n",
    "while True:\n",
    "    is_done = env.step(env.action_space.sample())[2]\n",
    "    if is_done:\n",
    "        print(\"Whoops! We died!\")\n",
    "        break\n",
    "\n",
    "print(\"final state:\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b0c2fd31817149809f3116643a768ece",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# reload initial state\n",
    "env.load_snapshot(snap0)\n",
    "\n",
    "print(\"\\n\\nAfter loading snapshot\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3f287e0d08084287b8f029d5c307fb7b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# get outcome (snapshot, observation, reward, is_done, info)\n",
    "res = env.get_result(snap0, env.action_space.sample())\n",
    "\n",
    "snap1, observation, reward = res[:3]\n",
    "\n",
    "# second step\n",
    "res2 = env.get_result(snap1, env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a2a09330874e49a6903eefb4598b2e34",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# MCTS: Monte-Carlo Tree Search\n",
    "\n",
    "\n",
    "Implementing the `Node` class - a simple class that acts like MCTS node and supports some of the MCTS algorithm steps.\n",
    "\n",
    "This MCTS implementation makes some assumptions about the environment, you can find those _in the notes section at the end of the notebook_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Overview\n",
    "\n",
    "In the following cell, we'll implement the `Node` class which represents each state in the MCTS tree. Each node stores:\n",
    "\n",
    "- **snapshot**: Environment state for backtracking\n",
    "- **observation**: The observation at this state  \n",
    "- **immediate_reward**: Reward received when reaching this node\n",
    "- **is_done**: Whether this is a terminal state\n",
    "- **qvalue_sum**: Sum of all Q-values from rollouts passing through this node\n",
    "- **times_visited**: Number of times this node has been visited\n",
    "- **children**: Set of child nodes (one per action)\n",
    "\n",
    "The key insight of MCTS is that we don't need to expand the entire game tree. Instead, we focus computational resources on the most promising paths using the UCB-1 exploration strategy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6c22f51b55214ac1b9250185285a5d6e",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "assert isinstance(env, WithSnapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "15fc696542104d99b0716137dc3ae654",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # a tree node for MCTS.\n",
    "    \n",
    "    parent = None  # parent Node\n",
    "    qvalue_sum = 0.  # sum of Q-values from all visits \n",
    "    times_visited = 0  # counter of visits \n",
    "\n",
    "    \n",
    "    def __init__(self, parent, action):\n",
    "        # Creates and empty node with no children.\n",
    "        \n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = set()  # set of child nodes\n",
    "\n",
    "        # get action outcome and save it\n",
    "        res = env.get_result(parent.snapshot, action)\n",
    "        self.snapshot, self.observation, self.immediate_reward, self.is_done, _ = res\n",
    "        \n",
    "\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    \n",
    "\n",
    "    def is_root(self):\n",
    "        return self.parent is None\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_qvalue_estimate(self):\n",
    "        return self.qvalue_sum / self.times_visited if self.times_visited != 0 else 0\n",
    "    \n",
    "\n",
    "    \n",
    "    def ucb_score(self, scale=10, max_value=1e100):\n",
    "        # param scale: Multiplies upper bound by that. From Hoeffding inequality, assumes reward range to be [0, scale].\n",
    "        # param max_value: a value that represents infinity (for unvisited nodes).\n",
    "\n",
    "        if self.times_visited == 0:\n",
    "            return max_value\n",
    "\n",
    "        # ==================================== Your Code (Begin) ==================================\n",
    "        \n",
    "        # Calculate UCB-1 score: Q(s,a) + C_p * sqrt(2 * log(N) / n_a)\n",
    "        # where N is parent's visit count and n_a is this node's visit count\n",
    "        \n",
    "        q_value = self.get_qvalue_estimate()\n",
    "        \n",
    "        # Calculate exploration bonus\n",
    "        exploration_bonus = scale * np.sqrt(2 * np.log(self.parent.times_visited) / self.times_visited)\n",
    "        \n",
    "        ucb_score = q_value + exploration_bonus\n",
    "        \n",
    "        return ucb_score\n",
    "        \n",
    "        # ==================================== Your Code (End) ====================================\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def select_best_leaf(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ==================================== Your Code (Begin) ==================================\n",
    "        \n",
    "        # Return the leaf with the highest priority to expand.\n",
    "        # Recursively pick nodes with the best UCB-1 score until it reaches a leaf.\n",
    "        \n",
    "        # If this is a leaf node, return itself\n",
    "        if self.is_leaf():\n",
    "            return self\n",
    "        \n",
    "        # Otherwise, select the child with the best UCB score and recurse\n",
    "        best_child = max(self.children, key=lambda child: child.ucb_score())\n",
    "        \n",
    "        return best_child.select_best_leaf()\n",
    "        \n",
    "        # ==================================== Your Code (End) ====================================\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def expand(self):\n",
    "        # expands the current node by creating all possible child nodes.\n",
    "        # returns one of those children.\n",
    "        \n",
    "        assert not self.is_done, \"can't expand from terminal state\"\n",
    "\n",
    "        for action in range(n_actions):\n",
    "            self.children.add(Node(self, action))\n",
    "\n",
    "        return self.select_best_leaf()\n",
    "    \n",
    "    \n",
    "\n",
    "    def rollout(self, t_max=10**4):\n",
    "        \n",
    "        # set env into the appropriate state\n",
    "        env.load_snapshot(self.snapshot)\n",
    "        obs = self.observation\n",
    "        is_done = self.is_done\n",
    "        \n",
    "        \n",
    "        # ==================================== Your Code (Begin) ==================================\n",
    "        \n",
    "        # If node is terminal, just return 0\n",
    "        if self.is_done:\n",
    "            return 0\n",
    "        \n",
    "        # Play the game from this state to the end (done) or for t_max steps.\n",
    "        # On each step, pick action at random.\n",
    "        \n",
    "        rollout_reward = 0\n",
    "        \n",
    "        for _ in range(t_max):\n",
    "            if is_done:\n",
    "                break\n",
    "            \n",
    "            # Take a random action\n",
    "            action = env.action_space.sample()\n",
    "            obs, reward, is_done, _ = env.step(action)\n",
    "            \n",
    "            # Accumulate the reward\n",
    "            rollout_reward += reward\n",
    "        \n",
    "        return rollout_reward  \n",
    "\n",
    "        \n",
    "        # ==================================== Your Code (End) ====================================\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def propagate(self, child_qvalue):\n",
    "        # Uses child Q-value (sum of rewards) to update parents recursively.\n",
    "        \n",
    "        # compute node Q-value\n",
    "        my_qvalue = self.immediate_reward + child_qvalue\n",
    "\n",
    "        # update qvalue_sum and times_visited\n",
    "        self.qvalue_sum += my_qvalue\n",
    "        self.times_visited += 1\n",
    "\n",
    "        # propagate upwards\n",
    "        if not self.is_root():\n",
    "            self.parent.propagate(my_qvalue)\n",
    "\n",
    "            \n",
    "            \n",
    "    def safe_delete(self):\n",
    "        # safe delete to prevent memory leak in some python versions \n",
    "        del self.parent\n",
    "        for child in self.children:\n",
    "            child.safe_delete()\n",
    "            del child"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Node Class Implementation\n",
    "\n",
    "The `Node` class implements the core MCTS data structure with four key methods:\n",
    "\n",
    "**1. `ucb_score()`**: Computes the Upper Confidence Bound (UCB-1) score to balance exploration and exploitation. The formula is:\n",
    "- Q-value (exploitation): Average reward from past visits\n",
    "- Exploration bonus: `C_p * sqrt(2 * log(N) / n_a)` encourages visiting less-explored nodes\n",
    "\n",
    "**2. `select_best_leaf()`**: Recursively traverses the tree by selecting children with the highest UCB scores until reaching a leaf node (no children yet).\n",
    "\n",
    "**3. `rollout()`**: Simulates a random playthrough from the current node to estimate its value. This uses a simple uniform random policy to play until the episode ends or t_max steps.\n",
    "\n",
    "**4. `propagate()`**: Backpropagates the rollout reward up the tree, updating Q-values and visit counts for all ancestor nodes.\n",
    "\n",
    "These four methods correspond to the four phases of MCTS: Selection, Expansion, Simulation, and Backpropagation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "dff86f659a0641e7b0256486de58734f",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class Root(Node):\n",
    "    def __init__(self, snapshot, observation):\n",
    "        # creates special node that acts like tree root\n",
    "        \n",
    "        self.parent = self.action = None\n",
    "        self.children = set()  # set of child nodes\n",
    "\n",
    "        # root: load snapshot and observation\n",
    "        self.snapshot = snapshot\n",
    "        self.observation = observation\n",
    "        self.immediate_reward = 0\n",
    "        self.is_done = False\n",
    "\n",
    "    @staticmethod\n",
    "    def from_node(node):\n",
    "        # initializes node as root\n",
    "        root = Root(node.snapshot, node.observation)\n",
    "        # copy data\n",
    "        copied_fields = [\"qvalue_sum\", \"times_visited\", \"children\", \"is_done\"]\n",
    "        for field in copied_fields:\n",
    "            setattr(root, field, getattr(node, field))\n",
    "        return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "27de95c593e84485a84fc4881c32b77d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Main MCTS Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7d9cc3774eb2440a8eef193a66540218",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def plan_mcts(root, n_iters=10):\n",
    "    \n",
    "    # build tree with monte-carlo tree search for n_iters iterations\n",
    "    \n",
    "    \n",
    "    # ==================================== Your Code (Begin) ==================================\n",
    "    \n",
    "    # For n_iters iterations, perform the MCTS algorithm:\n",
    "    # 1. Selection: Select best leaf using UCB-1\n",
    "    # 2. Expansion: Expand the leaf (if not done)\n",
    "    # 3. Simulation: Perform rollout from the new node\n",
    "    # 4. Backpropagation: Propagate the results upwards\n",
    "    \n",
    "    for _ in range(n_iters):\n",
    "        # 1. Selection: Select the best leaf node to expand\n",
    "        leaf = root.select_best_leaf()\n",
    "        \n",
    "        # 2. Expansion: If the leaf is not a terminal state, expand it\n",
    "        if not leaf.is_done:\n",
    "            leaf = leaf.expand()\n",
    "        \n",
    "        # 3. Simulation: Perform a rollout from the selected/expanded node\n",
    "        rollout_reward = leaf.rollout()\n",
    "        \n",
    "        # 4. Backpropagation: Propagate the rollout reward back up the tree\n",
    "        leaf.propagate(rollout_reward)\n",
    "    \n",
    "    # ==================================== Your Code (End) ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the MCTS Planning Loop\n",
    "\n",
    "The `plan_mcts()` function implements the main MCTS algorithm that builds a search tree over multiple iterations:\n",
    "\n",
    "**Each iteration performs 4 steps:**\n",
    "\n",
    "1. **Selection**: Start from root and use UCB-1 to select the most promising path down to a leaf node\n",
    "2. **Expansion**: If the leaf is not terminal, create child nodes for all possible actions\n",
    "3. **Simulation (Rollout)**: Play out the game randomly from the expanded node to estimate its value\n",
    "4. **Backpropagation**: Update Q-values and visit counts along the path from leaf to root\n",
    "\n",
    "**Why this works:**\n",
    "- UCB-1 ensures we explore promising but uncertain areas of the tree\n",
    "- More iterations = better estimates of action values\n",
    "- The tree grows asymmetrically toward high-reward regions\n",
    "- After planning, we can select actions by choosing the child with the highest average reward (pure exploitation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Implementation\n",
    "\n",
    "Now let's test our MCTS implementation on the CartPole environment. We'll:\n",
    "1. Initialize the environment and create a root node\n",
    "2. Run MCTS planning to build the search tree\n",
    "3. Execute actions by selecting children with the highest Q-values\n",
    "4. Continue planning at each step to refine our estimates\n",
    "\n",
    "The visualization will show how the agent balances the pole using MCTS planning. The total reward indicates how long the agent kept the pole balanced.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "efd67dee330149acbc9b6836fe62e120",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Plan and execute\n",
    "\n",
    "Here we use our MCTS implementation to find the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2e68ead807ee43bbbfa741e9ecfc32e7",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "env = WithSnapshots(gym.make(\"CartPole-v0\"))\n",
    "root_observation = env.reset()\n",
    "root_snapshot = env.get_snapshot()\n",
    "root = Root(root_snapshot, root_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2243965714ed4d7caeea7a9774be577f",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# plan from root:\n",
    "plan_mcts(root, n_iters=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execution Strategy: Re-planning at Each Step\n",
    "\n",
    "In this execution loop, we use MCTS in a **receding horizon** approach:\n",
    "\n",
    "1. **Plan**: Build a search tree from the current state using MCTS\n",
    "2. **Execute**: Select and execute the action with the highest average Q-value (exploitation only)\n",
    "3. **Observe**: Receive the next state and reward from the environment\n",
    "4. **Re-root**: Make the selected child the new root of the tree\n",
    "5. **Re-plan**: Build the tree further from the new root\n",
    "\n",
    "**Why re-plan at each step?**\n",
    "- The environment may be stochastic or partially observable\n",
    "- We get more accurate estimates as we get closer to a state\n",
    "- Allows the agent to adapt to new information\n",
    "- Pruning unrealized branches keeps memory manageable\n",
    "\n",
    "**Note:** In the code, we grow the tree with 100 additional iterations at each step to maintain good action estimates as we progress through the episode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0b68d67912554bc3ba9a1ef4ce3a9b01",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from itertools import count\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "total_reward = 0  # sum of rewards\n",
    "test_env = loads(root_snapshot)  # env used to show progress\n",
    "\n",
    "for i in count():\n",
    "\n",
    "    # ==================================== Your Code (Begin) ==================================\n",
    "    \n",
    "    # Select child with the highest mean Q-value (exploitation only, no exploration)\n",
    "    best_child = max(root.children, key=lambda child: child.get_qvalue_estimate())\n",
    "        \n",
    "    # ==================================== Your Code (End) ====================================\n",
    "\n",
    "    # take action\n",
    "    s, r, done, _ = test_env.step(best_child.action)\n",
    "\n",
    "    # show image\n",
    "    clear_output(True)\n",
    "    plt.title(\"step %i\" % i)\n",
    "    plt.imshow(test_env.render('rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        print(\"Finished with reward = \", total_reward)\n",
    "        break\n",
    "\n",
    "    # discard unrealized part of the tree (because not every child matters :()\n",
    "    for child in root.children:\n",
    "        if child != best_child:\n",
    "            child.safe_delete()\n",
    "\n",
    "    # declare best child a new root\n",
    "    root = Root.from_node(best_child)\n",
    "\n",
    "    assert not root.is_leaf(), \\\n",
    "        \"We ran out of tree! Need more planning! Try growing the tree right inside the loop.\"\n",
    "    plan_mcts(root,n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6842b82f0bfb4652a79207c63049ffab",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Notes\n",
    "\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "The full list of assumptions is:\n",
    "\n",
    "* __Finite number of actions__: we enumerate all actions in `expand`.\n",
    "* __Episodic (finite) MDP__: while technically it works for infinite MDPs, we perform a rollout for $10^4$ steps. If you are knowingly infinite, please adjust `t_max` to something more reasonable.\n",
    "* __Deterministic MDP__: `Node` represents the single outcome of taking `self.action` in `self.parent`, and does not support the situation where taking an action in a state may lead to different rewards and next states.\n",
    "* __No discounted rewards__: we assume $\\gamma=1$. If that isn't the case, you only need to change two lines in `rollout()` and use `my_qvalue = self.immediate_reward + gamma * child_qvalue` for `propagate()`.\n",
    "* __pickleable env__: won't work if e.g. your env is connected to a web-browser surfing the internet. For custom envs, you may need to modify get_snapshot/load_snapshot from `WithSnapshots`.\n",
    "\n",
    "#### On `get_best_leaf` and `expand` functions\n",
    "\n",
    "This MCTS implementation only selects leaf nodes for expansion.\n",
    "This doesn't break things down because `expand` adds all possible actions. Hence, all non-leaf nodes are by design fully expanded and shouldn't be selected.\n",
    "\n",
    "If you want to only add a few random action on each expand, you will also have to modify `get_best_leaf` to consider returning non-leafs.\n",
    "\n",
    "#### Rollout policy\n",
    "\n",
    "We use a simple uniform policy for rollouts. This introduces a negative bias to good situations that can be messed up completely with random bad action. As a simple example, if you tend to rollout with uniform policy, you better don't use sharp knives and walk near cliffs.\n",
    "\n",
    "You can improve that by integrating a reinforcement _learning_ algorithm with a computationally light agent. You can even train this agent on optimal policy found by the tree search."
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ccfd687329c94728b8a39ce470d0583d",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
