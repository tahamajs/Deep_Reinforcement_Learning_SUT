{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "96a1be8a2e9f4a1980fdbe0fa0e738ce",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Monte-carlo Tree Search\n",
    "\n",
    "\n",
    "In this notebook, we'll implement a MCTS planning and use it to solve a Gym env.\n",
    "\n",
    "![image.png](https://i.postimg.cc/6QmwnjPS/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "964899ece4104376be22f5b951e5f420",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "__How it works?__\n",
    "We just start with an empty tree and expand it. There are several common procedures.\n",
    "\n",
    "__1) Selection__\n",
    "Starting from the root, recursively select the node that corresponds to the tree policy.  \n",
    "\n",
    "There are several options for tree policies, which we saw earlier as exploration strategies: epsilon-greedy, Thomson sampling, UCB-1. It was shown that in MCTS, UCB-1 achieves a good result. Further, we will consider the one, but you can try to use others.\n",
    "\n",
    "Following the UCB-1 tree policy, we will choose an action that, on one hand, we expect to have the highest return, and on the other hand, we haven't explored much.\n",
    "\n",
    "$$\n",
    "\\DeclareMathOperator*{\\argmax}{arg\\,max}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{a} = \\argmax_{a} \\dot{Q}(s, a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\dot{Q}(s, a) = Q(s, a) + C_p \\sqrt{\\frac{2 \\log {N}}{n_a}}\n",
    "$$\n",
    "\n",
    "where: \n",
    "- $N$ - number of times we have visited state $s$,\n",
    "- $n_a$ - number of times we have taken action $a$,\n",
    "- $C_p$ - exploration balance parameter, which is performed between exploration and exploitation. \n",
    "\n",
    "Using Hoeffding inequality for rewards $R \\in [0,1]$ it can be shown that optimal $C_p = 1/\\sqrt{2}$. For rewards outside this range, the parameter should be tuned. We'll be using 10, but you can experiment with other values.\n",
    "\n",
    "__2) Expansion__\n",
    "After the selection procedure, we can achieve a leaf node or node in which we don't complete actions. In this case, we expand the tree by feasible actions and get new state nodes. \n",
    "\n",
    "__3) Simulation__\n",
    "How we can estimate node Q-values? The idea is to estimate action values for a given _rollout policy_ by averaging the return of many simulated trajectories from the current node. Simply, we can play with random or some special policy or use some model that can estimate it.\n",
    "\n",
    "__4) Backpropagation__\n",
    "The reward of the last simulation is backed up through the traversed nodes and propagates Q-value estimations, upwards to the root.\n",
    "\n",
    "$$\n",
    "Q({\\text{parent}}, a) = r + \\gamma \\cdot Q({\\text{child}}, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a21811d38df94892b5e8d81161fbfde6",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "e8326f43f33a4416a654d417809f349d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "We first need to make a wrapper for Gym environments to allow saving and loading game states to facilitate backtracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "77e5f887a3844683a8cd454f573bf8d8",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.core import Wrapper\n",
    "from pickle import dumps, loads\n",
    "from collections import namedtuple\n",
    "\n",
    "\n",
    "ActionResult = namedtuple(\n",
    "    \"action_result\", (\"snapshot\", \"observation\", \"reward\", \"is_done\", \"info\"))\n",
    "\n",
    "\n",
    "class WithSnapshots(Wrapper):\n",
    "    # Creates a wrapper that supports saving and loading environemnt states.\n",
    "    # Required for planning algorithms.\n",
    "\n",
    "\n",
    "    def get_snapshot(self, render=False):\n",
    "        # returns environment state that can be loaded with load_snapshot.\n",
    "        \n",
    "        \n",
    "        if render:\n",
    "            self.render()  \n",
    "            self.close()\n",
    "            \n",
    "        if self.unwrapped.viewer is not None:\n",
    "            self.unwrapped.viewer.close()\n",
    "            self.unwrapped.viewer = None\n",
    "        return dumps(self.env)\n",
    "    \n",
    "\n",
    "    def load_snapshot(self, snapshot, render=False):\n",
    "        # loads snapshot as current env state.\n",
    "        \n",
    "        assert not hasattr(self, \"_monitor\") or hasattr(\n",
    "            self.env, \"_monitor\"), \"can't backtrack while recording\"\n",
    "\n",
    "        if render:\n",
    "            self.render()  \n",
    "            self.close()\n",
    "        self.env = loads(snapshot)\n",
    "        \n",
    "\n",
    "    def get_result(self, snapshot, action):\n",
    "        # Breturns next snapshot and everything that env.step would have returned.\n",
    "        \n",
    "        self.load_snapshot(snapshot, render=False)\n",
    "        next_observation, reward, is_done, info = self.step(action)\n",
    "        next_snapshot = self.get_snapshot()\n",
    "        return ActionResult(next_snapshot, next_observation, reward, is_done, info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6e73f4a5cf6d47c4aa5f559f6a04bf8f",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "### Try out snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f1319deede264069ac29749fc06d36ca",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# make env\n",
    "env = WithSnapshots(gym.make(\"CartPole-v0\"))\n",
    "env.reset()\n",
    "\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a92ac001af624e07978c6475e66e558f",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "print(\"initial_state:\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()\n",
    "\n",
    "# create first snapshot\n",
    "snap0 = env.get_snapshot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "a45ce51b5a874edbb459aa447be8b732",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# play without making snapshots (faster)\n",
    "while True:\n",
    "    is_done = env.step(env.action_space.sample())[2]\n",
    "    if is_done:\n",
    "        print(\"Whoops! We died!\")\n",
    "        break\n",
    "\n",
    "print(\"final state:\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "b0c2fd31817149809f3116643a768ece",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# reload initial state\n",
    "env.load_snapshot(snap0)\n",
    "\n",
    "print(\"\\n\\nAfter loading snapshot\")\n",
    "plt.imshow(env.render('rgb_array'))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "3f287e0d08084287b8f029d5c307fb7b",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# get outcome (snapshot, observation, reward, is_done, info)\n",
    "res = env.get_result(snap0, env.action_space.sample())\n",
    "\n",
    "snap1, observation, reward = res[:3]\n",
    "\n",
    "# second step\n",
    "res2 = env.get_result(snap1, env.action_space.sample())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "a2a09330874e49a6903eefb4598b2e34",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# MCTS: Monte-Carlo Tree Search\n",
    "\n",
    "\n",
    "Implementing the `Node` class - a simple class that acts like MCTS node and supports some of the MCTS algorithm steps.\n",
    "\n",
    "This MCTS implementation makes some assumptions about the environment, you can find those _in the notes section at the end of the notebook_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6c22f51b55214ac1b9250185285a5d6e",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "assert isinstance(env, WithSnapshots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "15fc696542104d99b0716137dc3ae654",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # a tree node for MCTS.\n",
    "    \n",
    "    parent = None  # parent Node\n",
    "    qvalue_sum = 0.  # sum of Q-values from all visits \n",
    "    times_visited = 0  # counter of visits \n",
    "\n",
    "    \n",
    "    def __init__(self, parent, action):\n",
    "        # Creates and empty node with no children.\n",
    "        \n",
    "        self.parent = parent\n",
    "        self.action = action\n",
    "        self.children = set()  # set of child nodes\n",
    "\n",
    "        # get action outcome and save it\n",
    "        res = env.get_result(parent.snapshot, action)\n",
    "        self.snapshot, self.observation, self.immediate_reward, self.is_done, _ = res\n",
    "        \n",
    "\n",
    "        \n",
    "    def is_leaf(self):\n",
    "        return len(self.children) == 0\n",
    "    \n",
    "    \n",
    "\n",
    "    def is_root(self):\n",
    "        return self.parent is None\n",
    "    \n",
    "    \n",
    "\n",
    "    def get_qvalue_estimate(self):\n",
    "        return self.qvalue_sum / self.times_visited if self.times_visited != 0 else 0\n",
    "    \n",
    "\n",
    "    \n",
    "    def ucb_score(self, scale=10, max_value=1e100):\n",
    "        # param scale: Multiplies upper bound by that. From Hoeffding inequality, assumes reward range to be [0, scale].\n",
    "        # param max_value: a value that represents infinity (for unvisited nodes).\n",
    "\n",
    "        if self.times_visited == 0:\n",
    "            return max_value\n",
    "\n",
    "        # ==================================== Your Code (Begin) ==================================\n",
    "        \n",
    "        # calculate and return UCB-1 score\n",
    "        \n",
    "        # ==================================== Your Code (End) ====================================\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def select_best_leaf(self):\n",
    "        \n",
    "        \n",
    "        \n",
    "        # ==================================== Your Code (Begin) ==================================\n",
    "        \n",
    "        # return the leaf with the highest priority to expand.\n",
    "        # hint: recursively pick nodes with the best UCB-1 score until it reaches a leaf.\n",
    "        \n",
    "        # ==================================== Your Code (End) ====================================\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    def expand(self):\n",
    "        # expands the current node by creating all possible child nodes.\n",
    "        # returns one of those children.\n",
    "        \n",
    "        assert not self.is_done, \"can't expand from terminal state\"\n",
    "\n",
    "        for action in range(n_actions):\n",
    "            self.children.add(Node(self, action))\n",
    "\n",
    "        return self.select_best_leaf()\n",
    "    \n",
    "    \n",
    "\n",
    "    def rollout(self, t_max=10**4):\n",
    "        \n",
    "        # set env into the appropriate state\n",
    "        env.load_snapshot(self.snapshot)\n",
    "        obs = self.observation\n",
    "        is_done = self.is_done\n",
    "        \n",
    "        \n",
    "        # ==================================== Your Code (Begin) ==================================\n",
    "        \n",
    "        \n",
    "        # Play the game from this state to the end (done) or for t_max steps.\n",
    "        # On each step, pick action at random (hint: env.action_space.sample()).\n",
    "        \n",
    "        # Compute sum of rewards from current state till \n",
    "        # Note 1: use env.action_space.sample() for random action\n",
    "        # Note 2: if node is terminal (self.is_done is True), just return 0\n",
    "        \n",
    "        \n",
    "        # return rollout_reward  \n",
    "\n",
    "        \n",
    "        # ==================================== Your Code (End) ====================================\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def propagate(self, child_qvalue):\n",
    "        # Uses child Q-value (sum of rewards) to update parents recursively.\n",
    "        \n",
    "        # compute node Q-value\n",
    "        my_qvalue = self.immediate_reward + child_qvalue\n",
    "\n",
    "        # update qvalue_sum and times_visited\n",
    "        self.qvalue_sum += my_qvalue\n",
    "        self.times_visited += 1\n",
    "\n",
    "        # propagate upwards\n",
    "        if not self.is_root():\n",
    "            self.parent.propagate(my_qvalue)\n",
    "\n",
    "            \n",
    "            \n",
    "    def safe_delete(self):\n",
    "        # safe delete to prevent memory leak in some python versions \n",
    "        del self.parent\n",
    "        for child in self.children:\n",
    "            child.safe_delete()\n",
    "            del child"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "dff86f659a0641e7b0256486de58734f",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "class Root(Node):\n",
    "    def __init__(self, snapshot, observation):\n",
    "        # creates special node that acts like tree root\n",
    "        \n",
    "        self.parent = self.action = None\n",
    "        self.children = set()  # set of child nodes\n",
    "\n",
    "        # root: load snapshot and observation\n",
    "        self.snapshot = snapshot\n",
    "        self.observation = observation\n",
    "        self.immediate_reward = 0\n",
    "        self.is_done = False\n",
    "\n",
    "    @staticmethod\n",
    "    def from_node(node):\n",
    "        # initializes node as root\n",
    "        root = Root(node.snapshot, node.observation)\n",
    "        # copy data\n",
    "        copied_fields = [\"qvalue_sum\", \"times_visited\", \"children\", \"is_done\"]\n",
    "        for field in copied_fields:\n",
    "            setattr(root, field, getattr(node, field))\n",
    "        return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "27de95c593e84485a84fc4881c32b77d",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Main MCTS Loop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "7d9cc3774eb2440a8eef193a66540218",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "def plan_mcts(root, n_iters=10):\n",
    "    \n",
    "    # build tree with monte-carlo tree search for n_iters iterations\n",
    "    \n",
    "    \n",
    "    # ==================================== Your Code (Begin) ==================================\n",
    "    \n",
    "    # Hint: for n_iters: select best leaf, expand it, perform a rollout from it, propagate the results upwards.\n",
    "    # Note: don't forget to check the is_done condition in the loop\n",
    "    \n",
    "    # ==================================== Your Code (End) ===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "efd67dee330149acbc9b6836fe62e120",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Plan and execute\n",
    "\n",
    "Here we use our MCTS implementation to find the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2e68ead807ee43bbbfa741e9ecfc32e7",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "env = WithSnapshots(gym.make(\"CartPole-v0\"))\n",
    "root_observation = env.reset()\n",
    "root_snapshot = env.get_snapshot()\n",
    "root = Root(root_snapshot, root_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2243965714ed4d7caeea7a9774be577f",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# plan from root:\n",
    "plan_mcts(root, n_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "0b68d67912554bc3ba9a1ef4ce3a9b01",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from itertools import count\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "total_reward = 0  # sum of rewards\n",
    "test_env = loads(root_snapshot)  # env used to show progress\n",
    "\n",
    "for i in count():\n",
    "\n",
    "    # ==================================== Your Code (Begin) ==================================\n",
    "    \n",
    "    # best_child = <select child with the highest mean reward>\n",
    "        \n",
    "    # ==================================== Your Code (End) ====================================\n",
    "\n",
    "    # take action\n",
    "    s, r, done, _ = test_env.step(best_child.action)\n",
    "\n",
    "    # show image\n",
    "    clear_output(True)\n",
    "    plt.title(\"step %i\" % i)\n",
    "    plt.imshow(test_env.render('rgb_array'))\n",
    "    plt.show()\n",
    "\n",
    "    total_reward += r\n",
    "    if done:\n",
    "        print(\"Finished with reward = \", total_reward)\n",
    "        break\n",
    "\n",
    "    # discard unrealized part of the tree (because not every child matters :()\n",
    "    for child in root.children:\n",
    "        if child != best_child:\n",
    "            child.safe_delete()\n",
    "\n",
    "    # declare best child a new root\n",
    "    root = Root.from_node(best_child)\n",
    "\n",
    "    assert not root.is_leaf(), \\\n",
    "        \"We ran out of tree! Need more planning! Try growing the tree right inside the loop.\"\n",
    "    plan_mcts(root,n_iters=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "6842b82f0bfb4652a79207c63049ffab",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Notes\n",
    "\n",
    "\n",
    "#### Assumptions\n",
    "\n",
    "The full list of assumptions is:\n",
    "\n",
    "* __Finite number of actions__: we enumerate all actions in `expand`.\n",
    "* __Episodic (finite) MDP__: while technically it works for infinite MDPs, we perform a rollout for $10^4$ steps. If you are knowingly infinite, please adjust `t_max` to something more reasonable.\n",
    "* __Deterministic MDP__: `Node` represents the single outcome of taking `self.action` in `self.parent`, and does not support the situation where taking an action in a state may lead to different rewards and next states.\n",
    "* __No discounted rewards__: we assume $\\gamma=1$. If that isn't the case, you only need to change two lines in `rollout()` and use `my_qvalue = self.immediate_reward + gamma * child_qvalue` for `propagate()`.\n",
    "* __pickleable env__: won't work if e.g. your env is connected to a web-browser surfing the internet. For custom envs, you may need to modify get_snapshot/load_snapshot from `WithSnapshots`.\n",
    "\n",
    "#### On `get_best_leaf` and `expand` functions\n",
    "\n",
    "This MCTS implementation only selects leaf nodes for expansion.\n",
    "This doesn't break things down because `expand` adds all possible actions. Hence, all non-leaf nodes are by design fully expanded and shouldn't be selected.\n",
    "\n",
    "If you want to only add a few random action on each expand, you will also have to modify `get_best_leaf` to consider returning non-leafs.\n",
    "\n",
    "#### Rollout policy\n",
    "\n",
    "We use a simple uniform policy for rollouts. This introduces a negative bias to good situations that can be messed up completely with random bad action. As a simple example, if you tend to rollout with uniform policy, you better don't use sharp knives and walk near cliffs.\n",
    "\n",
    "You can improve that by integrating a reinforcement _learning_ algorithm with a computationally light agent. You can even train this agent on optimal policy found by the tree search."
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "ccfd687329c94728b8a39ce470d0583d",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
