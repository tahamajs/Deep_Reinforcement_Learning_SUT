{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Thompson Sampling for Multi-Armed Bandits: A Comprehensive Study\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This notebook presents a comprehensive implementation and analysis of Thompson Sampling algorithm for the Multi-Armed Bandit (MAB) problem. We compare Thompson Sampling with classical approaches including ε-Greedy and Upper Confidence Bound (UCB) algorithms. The study examines algorithm performance under various conditions including different variance levels and non-stationary environments. Experimental results demonstrate Thompson Sampling's superior performance in balancing exploration-exploitation trade-offs through Bayesian inference.\n",
        "\n",
        "**Keywords:** Thompson Sampling, Multi-Armed Bandit, Reinforcement Learning, Bayesian Inference, Exploration-Exploitation\n",
        "\n",
        "---\n",
        "\n",
        "## I. INTRODUCTION\n",
        "\n",
        "### A. Multi-Armed Bandit Problem\n",
        "\n",
        "The Multi-Armed Bandit (MAB) problem is a fundamental framework in reinforcement learning that addresses the exploration-exploitation dilemma. In this problem, an agent must choose between K arms (actions) at each time step, where each arm provides stochastic rewards drawn from an unknown distribution. The objective is to maximize cumulative reward over time T by identifying and exploiting the optimal arm while gathering information about all arms.\n",
        "\n",
        "### B. Motivation\n",
        "\n",
        "The MAB problem has numerous real-world applications:\n",
        "- **Clinical Trials:** Allocating patients to treatments\n",
        "- **Online Advertising:** Selecting ads to display\n",
        "- **Recommendation Systems:** Choosing content to recommend\n",
        "- **Resource Allocation:** Distributing computational resources\n",
        "\n",
        "### C. Thompson Sampling Overview\n",
        "\n",
        "Thompson Sampling, introduced by Thompson (1933), is a Bayesian approach that naturally balances exploration and exploitation by maintaining posterior distributions over arm parameters. Unlike deterministic algorithms like UCB or simple heuristics like ε-Greedy, Thompson Sampling uses probability matching: the probability of selecting an arm equals the probability that arm is optimal given current beliefs.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## II. METHODOLOGY\n",
        "\n",
        "### A. Experimental Setup and Dependencies\n",
        "\n",
        "We utilize the following Python libraries for our implementation:\n",
        "- **NumPy:** For numerical computations and random sampling\n",
        "- **SciPy:** For statistical distributions (Gaussian/Normal)\n",
        "- **Matplotlib:** For visualization of results\n",
        "\n",
        "The following cell imports the necessary dependencies:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ydGWGgzGRyCv"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "loycX-rxvWss"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### B. Visualization Utilities\n",
        "\n",
        "We define visualization functions to analyze algorithm performance:\n",
        "\n",
        "1. **plot_distribution():** Visualizes the probability density functions of arm reward distributions\n",
        "2. **plot_regret():** Plots cumulative regret over time  \n",
        "3. **plot():** Combined visualization showing both regret and arm distributions\n",
        "\n",
        "These visualizations help us understand:\n",
        "- How quickly algorithms identify the optimal arm\n",
        "- The relationship between arm distributions and algorithm performance\n",
        "- Convergence patterns under different conditions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xcyZl2WrfOdn"
      },
      "outputs": [],
      "source": [
        "def plot_distribution(ax, pdfs, title=''): \n",
        "    x = np.linspace(0., 10., 200)\n",
        "    ymax = 0    \n",
        "    for index, pdf in enumerate(pdfs):\n",
        "        y = norm.pdf(x, pdf.mean, np.sqrt(pdf.var))\n",
        "\n",
        "        p = ax.plot(x, y, lw = 2)\n",
        "        c = p[0].get_markeredgecolor()    \n",
        "        ax.fill_between(x, y, 0, color=c, alpha=0.2 )    \n",
        "        ax.autoscale(tight=True)\n",
        "        ax.vlines(pdf.mean, 0, y.max(), colors = c, linestyles = \"--\", lw = 2)    \n",
        "\n",
        "        ymax = max( ymax, y[1:].max()*1.05 )\n",
        "    ax.set_ylim([0,ymax])\n",
        "\n",
        "def plot_regret(ax, regret, title=''): \n",
        "    ax.plot(regret)\n",
        "\n",
        "def plot(regret, mab):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
        "    plot_regret(axs[0], regret)\n",
        "    plot_distribution(axs[1], mab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KRawbhcxR0A9"
      },
      "outputs": [],
      "source": [
        "class Arm:\n",
        "    def __init__(self, mean, var):\n",
        "        self.mean = mean\n",
        "        self.var = var\n",
        "\n",
        "    def sample(self):\n",
        "        return np.random.normal(self.mean, np.sqrt(self.var))\n",
        "\n",
        "    def name(self):\n",
        "        return 'N(' + str(self.mean) + ',' + str(self.var) + ')'"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### C. Arm Class Definition\n",
        "\n",
        "The `Arm` class represents a single bandit arm with Gaussian reward distribution:\n",
        "\n",
        "**Mathematical Formulation:**\n",
        "Each arm i generates rewards from: R_i ~ N(μ_i, σ_i²)\n",
        "\n",
        "Where:\n",
        "- μ_i: Mean reward (unknown to the agent)\n",
        "- σ_i²: Variance (known in our setting)\n",
        "\n",
        "**Class Methods:**\n",
        "- `sample()`: Generates a stochastic reward from the arm's distribution\n",
        "- `name()`: Returns string representation for identification\n",
        "\n",
        "This abstraction allows us to easily create multi-armed bandit environments with different reward distributions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04awn6EFPG7d"
      },
      "source": [
        "# Thompson Sampling\n",
        "\n",
        "In this exercise we will run thompson sampling for 2-armed bandit with gaussian distribution.\n",
        "For simplicity assume we know variance of distribution of arms and only mean is unknown\n",
        "for prior assume gaussian distribution."
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## III. THEORETICAL BACKGROUND\n",
        "\n",
        "### A. Bayesian Inference for Gaussian Bandits\n",
        "\n",
        "In our setting, we assume:\n",
        "- Each arm has Gaussian reward distribution: R ~ N(μ, σ²)\n",
        "- Variance σ² is **known**\n",
        "- Mean μ is **unknown** and must be learned\n",
        "\n",
        "**Prior Distribution:**\n",
        "We use a Gaussian prior over the unknown mean: μ ~ N(μ₀, σ₀²)\n",
        "\n",
        "**Likelihood:**\n",
        "Each observation r follows: r | μ ~ N(μ, σ²)\n",
        "\n",
        "**Posterior Update (Bayesian Rule):**\n",
        "After observing reward r, the posterior is also Gaussian: μ | r ~ N(μ₁, σ₁²)\n",
        "\n",
        "Where:\n",
        "- Posterior variance: σ₁² = 1 / (1/σ₀² + 1/σ²)\n",
        "- Posterior mean: μ₁ = σ₁² × (μ₀/σ₀² + r/σ²)\n",
        "\n",
        "This conjugate prior property makes Thompson Sampling computationally efficient for Gaussian bandits.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "If the variance is unknown, we would use a **Normal-Inverse-Gamma (NIG)** distribution as the conjugate prior for the Gaussian likelihood with unknown mean and variance.\n",
        "\n",
        "Alternatively, we could use:\n",
        "- **Inverse-Gamma** distribution for the variance alone\n",
        "- **Gamma** distribution for the precision (inverse of variance)\n",
        "- **Student's t-distribution** as a marginal distribution when integrating out the variance\n",
        "\n",
        "The Normal-Inverse-Gamma is the most common choice because it's the conjugate prior for Gaussian data with both unknown mean and variance, which makes Bayesian updates analytically tractable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETjtn1IibGT0"
      },
      "source": [
        "## 1.\n",
        "if variance was unknown what prior distribtion would be suitable?"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## V. EXPERIMENTAL QUESTIONS\n",
        "\n",
        "### Question 1: Prior Distribution for Unknown Variance\n",
        "\n",
        "**Problem Statement:**\n",
        "If the variance was unknown (in addition to the mean), what prior distribution would be suitable?\n",
        "\n",
        "**Theoretical Analysis:**\n",
        "\n",
        "When both mean μ and variance σ² are unknown in a Gaussian distribution, we need a joint prior distribution over (μ, σ²). The most appropriate choice is the **Normal-Inverse-Gamma (NIG)** distribution.\n",
        "\n",
        "**A. Normal-Inverse-Gamma Prior**\n",
        "\n",
        "The NIG distribution is the conjugate prior for Gaussian data with unknown mean and variance:\n",
        "\n",
        "- σ² ~ Inverse-Gamma(α, β)\n",
        "- μ | σ² ~ N(μ₀, σ²/κ)\n",
        "\n",
        "**Parameters:**\n",
        "- μ₀: prior mean location\n",
        "- κ: prior precision (confidence in μ₀)  \n",
        "- α: shape parameter for variance\n",
        "- β: scale parameter for variance\n",
        "\n",
        "**B. Alternative Approaches**\n",
        "\n",
        "1. **Normal-Inverse-Chi-Square:** Another parameterization of the same family\n",
        "2. **Normal-Gamma:** Uses precision τ = 1/σ² instead of variance\n",
        "3. **Student's t-distribution:** Marginal distribution when integrating out σ²\n",
        "\n",
        "**C. Bayesian Update**\n",
        "\n",
        "The conjugacy property ensures that after observing data, the posterior remains in the NIG family with updated parameters, making Thompson Sampling tractable even with unknown variance.\n",
        "\n",
        "**D. Practical Considerations**\n",
        "\n",
        "- **Known variance:** Simpler (Normal prior only) - used in this notebook\n",
        "- **Unknown variance:** More realistic but computationally complex (NIG prior)\n",
        "- **Trade-off:** Complexity vs. modeling accuracy\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99BrmMVIbNkF"
      },
      "source": [
        "## 2.\n",
        "Implement Thompson Sampling algorithm. For comparison also implement ϵ-Greedy and UCB algorithms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Implementation Overview:**\n",
        "\n",
        "We implement three classic bandit algorithms:\n",
        "\n",
        "1. **Thompson Sampling**: Bayesian approach that samples from posterior distributions\n",
        "   - Naturally balances exploration and exploitation\n",
        "   - Probabilistic action selection based on uncertainty\n",
        "   \n",
        "2. **Upper Confidence Bound (UCB)**: Deterministic approach using confidence intervals\n",
        "   - Selects arm with highest upper confidence bound\n",
        "   - Theoretical regret guarantees\n",
        "   \n",
        "3. **ε-Greedy**: Simple approach with random exploration\n",
        "   - Explores randomly with probability ε\n",
        "   - Exploits best known arm with probability (1-ε)\n",
        "\n",
        "All three maintain value estimates Q(a) and update them incrementally using: Q(a) ← Q(a) + α[R - Q(a)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6_MJQc-SJ4d"
      },
      "outputs": [],
      "source": [
        "class ThompsonSampling:\n",
        "    def __init__(self, var_list, **kwargs):\n",
        "        \"\"\"\n",
        "        variance of arms are known to policy\n",
        "        \"\"\"\n",
        "        self.var_list = var_list\n",
        "        self.n_arms = len(var_list)\n",
        "        \n",
        "        # Initialize prior beliefs: mean and variance for each arm\n",
        "        self.prior_means = np.zeros(self.n_arms)\n",
        "        self.prior_vars = np.ones(self.n_arms) * 100  # Large initial variance (high uncertainty)\n",
        "        \n",
        "        # Track number of pulls for each arm\n",
        "        self.n_pulls = np.zeros(self.n_arms)\n",
        "\n",
        "    def select_arm(self, *args):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Sample from the posterior distribution of each arm\n",
        "        samples = []\n",
        "        for i in range(self.n_arms):\n",
        "            # Sample from N(prior_mean, prior_var)\n",
        "            sample = np.random.normal(self.prior_means[i], np.sqrt(self.prior_vars[i]))\n",
        "            samples.append(sample)\n",
        "        \n",
        "        # Select arm with highest sample (optimistic estimate)\n",
        "        selected_arm = np.argmax(samples)\n",
        "        \n",
        "        # return index of selected arm\n",
        "        return selected_arm\n",
        "\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    def update(self, idx, reward):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Bayesian update for Gaussian with known variance\n",
        "        # Posterior is also Gaussian with updated mean and variance\n",
        "        \n",
        "        # Prior: N(μ₀, σ₀²)\n",
        "        # Likelihood: N(reward, σ²) where σ² is the known arm variance\n",
        "        # Posterior: N(μ₁, σ₁²)\n",
        "        \n",
        "        prior_mean = self.prior_means[idx]\n",
        "        prior_var = self.prior_vars[idx]\n",
        "        obs_var = self.var_list[idx]\n",
        "        \n",
        "        # Bayesian update formulas\n",
        "        posterior_var = 1.0 / (1.0/prior_var + 1.0/obs_var)\n",
        "        posterior_mean = posterior_var * (prior_mean/prior_var + reward/obs_var)\n",
        "        \n",
        "        # Update beliefs\n",
        "        self.prior_means[idx] = posterior_mean\n",
        "        self.prior_vars[idx] = posterior_var\n",
        "        self.n_pulls[idx] += 1\n",
        "        \n",
        "        # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRCYR3ooSbpq"
      },
      "outputs": [],
      "source": [
        "class UCB:    \n",
        "    def __init__(self, n_bandits, c_level):\n",
        "        \"\"\"\n",
        "        c_level: coefficient of uncertainty\n",
        "        \"\"\"\n",
        "        self.n_bandits = n_bandits\n",
        "        self.c_level = c_level\n",
        "        \n",
        "        # Track estimates and counts\n",
        "        self.Q = np.zeros(n_bandits)  # Estimated values\n",
        "        self.N = np.zeros(n_bandits)  # Number of times each arm was pulled\n",
        "    \n",
        "    def select_arm(self, t):\n",
        "        \"\"\"\n",
        "        t: step time\n",
        "        \"\"\"\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Initially pull each arm once to avoid division by zero\n",
        "        for i in range(self.n_bandits):\n",
        "            if self.N[i] == 0:\n",
        "                return i\n",
        "        \n",
        "        # Calculate UCB values for each arm\n",
        "        ucb_values = self.Q + self.c_level * np.sqrt(np.log(t + 1) / self.N)\n",
        "        \n",
        "        # return index of selected arm\n",
        "        return np.argmax(ucb_values)\n",
        "\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    def update(self, idx, reward):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Incremental update of mean estimate\n",
        "        self.N[idx] += 1\n",
        "        self.Q[idx] = self.Q[idx] + (reward - self.Q[idx]) / self.N[idx]\n",
        "        \n",
        "        # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku7iA1qoJyRj"
      },
      "outputs": [],
      "source": [
        "class eGreedy:    \n",
        "    def __init__(self, n_bandits, epsilon):\n",
        "        \"\"\"\n",
        "        epsilon must be given\n",
        "        \"\"\"\n",
        "        self.n_bandits = n_bandits\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "        # Track estimates and counts\n",
        "        self.Q = np.zeros(n_bandits)  # Estimated values\n",
        "        self.N = np.zeros(n_bandits)  # Number of times each arm was pulled\n",
        "    \n",
        "    def select_arm(self, *args):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Explore with probability epsilon\n",
        "        if np.random.random() < self.epsilon:\n",
        "            # Explore: select random arm\n",
        "            return np.random.randint(0, self.n_bandits)\n",
        "        else:\n",
        "            # Exploit: select best arm based on current estimates\n",
        "            return np.argmax(self.Q)\n",
        "\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    def update(self, idx, reward):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Incremental update of mean estimate\n",
        "        self.N[idx] += 1\n",
        "        self.Q[idx] = self.Q[idx] + (reward - self.Q[idx]) / self.N[idx]\n",
        "        \n",
        "        # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Question 2: Algorithm Implementation\n",
        "\n",
        "**Problem Statement:**\n",
        "Implement Thompson Sampling algorithm and compare with ε-Greedy and UCB algorithms.\n",
        "\n",
        "**A. Thompson Sampling Algorithm**\n",
        "\n",
        "**Pseudocode:**\n",
        "```\n",
        "Initialize: For each arm i, set prior N(μ₀ᵢ, σ₀ᵢ²)\n",
        "For t = 1, 2, ..., T:\n",
        "    For each arm i:\n",
        "        Sample θᵢ ~ N(μₜᵢ, σₜᵢ²)\n",
        "    Select arm aₜ = argmax_i θᵢ\n",
        "    Observe reward rₜ\n",
        "    Update posterior for arm aₜ using Bayesian rule\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "- Probability matching: P(select arm i) = P(arm i is optimal | data)\n",
        "- Natural exploration: High uncertainty → more exploration\n",
        "- Optimal asymptotic performance (Lai & Robbins lower bound)\n",
        "\n",
        "**B. ε-Greedy Algorithm**\n",
        "\n",
        "**Pseudocode:**\n",
        "```\n",
        "Initialize: Q(a) = 0 for all arms, N(a) = 0\n",
        "For t = 1, 2, ..., T:\n",
        "    With probability ε: select random arm\n",
        "    With probability 1-ε: select aₜ = argmax_a Q(a)\n",
        "    Observe reward rₜ\n",
        "    Update: Q(aₜ) ← Q(aₜ) + α[rₜ - Q(aₜ)]\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "- Simple and intuitive\n",
        "- Fixed exploration rate ε\n",
        "- Suboptimal: explores uniformly regardless of uncertainty\n",
        "\n",
        "**C. Upper Confidence Bound (UCB) Algorithm**\n",
        "\n",
        "**Pseudocode:**\n",
        "```\n",
        "Initialize: Q(a) = 0, N(a) = 0 for all arms\n",
        "For t = 1, 2, ..., T:\n",
        "    Select aₜ = argmax_a [Q(a) + c√(ln(t)/N(a))]\n",
        "    Observe reward rₜ\n",
        "    Update: Q(aₜ) ← Q(aₜ) + α[rₜ - Q(aₜ)]\n",
        "```\n",
        "\n",
        "**Key Features:**\n",
        "- Optimism in the face of uncertainty\n",
        "- Deterministic selection\n",
        "- Theoretical regret bound: O(√(KT ln T))\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkoURY09P1QG"
      },
      "source": [
        "## 3\n",
        "run simulation for arms described as cells below and describe the differences of regret with different variance in arms distributions\n",
        "\n",
        "rum_sim1 must return cumulitive regret formulated as \n",
        "$$\n",
        "R(T)=\\sum_{i=1}^2 N_i(T) \\Delta_i\n",
        "$$\n",
        "\n",
        "where $N_i(T)$ is number of times arm $i$ was selected until step $T$, $\\Delta_i=\\mu^*-\\mu_i$, $\\mu^*$ is largest mean in arms distribtions and $\\mu_i$ is mean of distribution of arm $i$\n",
        "\n",
        "to get average regret we rum simulation 50 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99fCVaPDdeid"
      },
      "outputs": [],
      "source": [
        "def run_sim1(policy, mab, step_num=100):\n",
        "    \"\"\"\n",
        "    run simulation of multi-armed bandit\n",
        "    mab: list of arms\n",
        "    \"\"\"\n",
        "    best_mean = np.max([b.mean for b in mab])\n",
        "    regret = []\n",
        "    cumulative_regret = 0\n",
        "    \n",
        "    for k in range(step_num):          \n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Select arm using policy\n",
        "        arm_idx = policy.select_arm(k)\n",
        "        \n",
        "        # Sample reward from selected arm\n",
        "        reward = mab[arm_idx].sample()\n",
        "        \n",
        "        # Update policy with observed reward\n",
        "        policy.update(arm_idx, reward)\n",
        "        \n",
        "        # Calculate instantaneous regret\n",
        "        instantaneous_regret = best_mean - mab[arm_idx].mean\n",
        "        cumulative_regret += instantaneous_regret\n",
        "        \n",
        "        # Store cumulative regret\n",
        "        regret.append(cumulative_regret)\n",
        " \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    \n",
        "    return regret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Understanding Regret:**\n",
        "\n",
        "Regret measures how much reward we lose compared to always selecting the optimal arm.\n",
        "\n",
        "**Cumulative Regret Formula:** R(T) = Σᵢ Nᵢ(T) × Δᵢ\n",
        "\n",
        "Where:\n",
        "- **Nᵢ(T)**: Number of times arm i was selected up to time T\n",
        "- **Δᵢ = μ* - μᵢ**: Gap between optimal mean μ* and arm i's mean\n",
        "- **μ***: Maximum mean among all arms\n",
        "\n",
        "**Interpretation:**\n",
        "- Perfect algorithm: Always picks optimal arm → R(T) = 0\n",
        "- Random selection: R(T) grows linearly with T\n",
        "- Good algorithms: R(T) grows sublinearly (e.g., logarithmic for UCB)\n",
        "\n",
        "**In our plots:**\n",
        "- Y-axis shows cumulative regret over time\n",
        "- Flatter curves = better performance (less regret)\n",
        "- Initial exploration causes regret growth, then should plateau as optimal arm is identified\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkTH2mchdkcA"
      },
      "source": [
        "### 3.1\n",
        "Assume Multi-Armed Bandit variables are as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZMkXIR5WdiWX"
      },
      "outputs": [],
      "source": [
        "mab = [Arm(6, 0.5), Arm(4, 0.5)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Thompson Sampling Explanation:**\n",
        "\n",
        "Thompson Sampling uses Bayesian inference to balance exploration and exploitation:\n",
        "1. We maintain a posterior distribution over each arm's mean reward\n",
        "2. At each step, we sample from each arm's posterior distribution\n",
        "3. We select the arm with the highest sample (optimistic action)\n",
        "4. After observing the reward, we update the posterior using Bayes' rule\n",
        "\n",
        "With low variance (0.5), the arms are more predictable. Thompson Sampling should converge quickly to the optimal arm, and the regret plot should show initial growth that plateaus.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5rmrS8paPLT"
      },
      "source": [
        "#### Thompson Sampling\n",
        "run and describe the result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Question 3: Performance Evaluation with Different Variance Levels\n",
        "\n",
        "**Problem Statement:**\n",
        "Run simulations for arms with different variance levels and analyze the impact on regret.\n",
        "\n",
        "**A. Regret Formulation**\n",
        "\n",
        "**Cumulative Regret:**\n",
        "The regret at time T measures the total expected reward loss compared to always selecting the optimal arm:\n",
        "\n",
        "R(T) = Σᵢ₌₁ᴷ Nᵢ(T) × Δᵢ\n",
        "\n",
        "Where:\n",
        "- **K**: Number of arms\n",
        "- **Nᵢ(T)**: Number of times arm i was selected up to time T\n",
        "- **Δᵢ = μ* - μᵢ**: Suboptimality gap of arm i\n",
        "- **μ***: Mean reward of the optimal arm (μ* = maxᵢ μᵢ)\n",
        "- **μᵢ**: Mean reward of arm i\n",
        "\n",
        "**Instantaneous Regret:**\n",
        "At each time step t: r(t) = μ* - μₐₜ\n",
        "\n",
        "**Cumulative Regret:**\n",
        "R(T) = Σₜ₌₁ᵀ r(t)\n",
        "\n",
        "**B. Performance Goals**\n",
        "\n",
        "**Ideal Performance:** R(T) = 0 (always select optimal arm)\n",
        "\n",
        "**Random Selection:** R(T) = O(T) (linear growth)\n",
        "\n",
        "**Good Algorithms:** R(T) = O(log T) (logarithmic growth)\n",
        "- UCB achieves O(√(KT ln T))\n",
        "- Thompson Sampling achieves O(√(KT)) empirically\n",
        "\n",
        "**C. Impact of Variance**\n",
        "\n",
        "**Low Variance (σ² = 0.5):**\n",
        "- Rewards are concentrated around the mean\n",
        "- Each sample is highly informative\n",
        "- Algorithms converge quickly\n",
        "- Lower cumulative regret\n",
        "\n",
        "**High Variance (σ² = 10):**\n",
        "- Rewards have high stochasticity\n",
        "- Each sample is less informative\n",
        "- Requires more samples to distinguish arms\n",
        "- Higher cumulative regret, slower convergence\n",
        "\n",
        "**Hypothesis:**\n",
        "Thompson Sampling should outperform ε-Greedy and UCB, especially in high-variance scenarios, due to its principled uncertainty quantification.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**ε-Greedy Explanation:**\n",
        "\n",
        "ε-Greedy is a simple exploration strategy:\n",
        "- With probability ε, explore by selecting a random arm\n",
        "- With probability (1-ε), exploit by selecting the arm with highest estimated value\n",
        "\n",
        "The trade-off:\n",
        "- **Small ε (e.g., 0.01)**: More exploitation, faster convergence but may get stuck in suboptimal arm\n",
        "- **Large ε (e.g., 0.2)**: More exploration, slower to converge but less likely to miss the optimal arm\n",
        "- Unlike Thompson Sampling, ε-Greedy explores uniformly without considering uncertainty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "A-ZFMhVtJvxU",
        "outputId": "e512e23b-4422-41b6-f858-9812ff801156"
      },
      "outputs": [],
      "source": [
        "regret = [run_sim1(ThompsonSampling([b.var for b in mab]), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nytkv2z0Z7Ol"
      },
      "source": [
        "#### ϵ-Greedy\n",
        "run for different values of ϵ and compare results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**UCB (Upper Confidence Bound) Explanation:**\n",
        "\n",
        "UCB uses the principle of \"optimism in the face of uncertainty\":\n",
        "- For each arm, compute: UCB(a) = Q(a) + c × √(ln(t) / N(a))\n",
        "  - Q(a): estimated mean reward\n",
        "  - N(a): number of times arm was pulled\n",
        "  - t: total time steps\n",
        "  - c: confidence level parameter\n",
        "\n",
        "The algorithm always selects the arm with highest UCB value. The confidence term decreases with more observations, reducing exploration over time.\n",
        "\n",
        "**Parameter c:**\n",
        "- **Small c (e.g., 0.5)**: Less exploration, may converge faster but risk missing optimal arm\n",
        "- **Large c (e.g., 3.0)**: More exploration, guaranteed to find optimal but slower convergence\n",
        "- **c=2**: Often works well in practice, providing theoretical regret bounds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "K6qvTojUN7w6",
        "outputId": "c836d4ec-a7a2-4512-cb0c-eca22f52e97f"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1  # Try different values: 0.01, 0.05, 0.1, 0.2\n",
        "regret = [run_sim1(eGreedy(2, epsilon=epsilon), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPPmqGSKaC6K"
      },
      "source": [
        "#### UCB\n",
        "run for different values of confidence level and compare results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "nYjcq48NU3_I",
        "outputId": "cc97d7d6-e656-419d-86cc-265872fc3466"
      },
      "outputs": [],
      "source": [
        "c_level = 2.0  # Try different values: 0.5, 1.0, 2.0, 3.0\n",
        "regret = [run_sim1(UCB(2, c_level=c_level), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Impact of High Variance:**\n",
        "\n",
        "Now we test with variance = 10 (compared to 0.5 before). Key differences:\n",
        "- **More noise**: Each reward sample is less informative about the true mean\n",
        "- **Slower learning**: All algorithms need more samples to distinguish between arms\n",
        "- **Higher regret**: The uncertainty makes it harder to identify the optimal arm quickly\n",
        "- **Thompson Sampling advantage**: Should handle high variance better because it explicitly models uncertainty\n",
        "\n",
        "With high variance, we run more steps (500 vs 100) to see convergence patterns more clearly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_BkFPDxeGkJ"
      },
      "source": [
        "### 3.2\n",
        "Assume Multi-Armed Bandit variables are as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r0oA15uneIbq"
      },
      "outputs": [],
      "source": [
        "mab = [Arm(6, 10), Arm(4, 10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO2piyfaz3I"
      },
      "source": [
        "#### Thompson Sampling\n",
        "run and compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "SVCgTL5o0-lV",
        "outputId": "7f4c40a9-23b9-4e9a-d982-9b8452e30eb2"
      },
      "outputs": [],
      "source": [
        "regret = [run_sim1(ThompsonSampling([b.var for b in mab]), mab, step_num=500) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGkpS7ihal_v"
      },
      "source": [
        "#### ϵ-Greedy\n",
        "run for different values of ϵ and compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "aM3iSz2FQ5hG",
        "outputId": "bfbc4382-aa00-4443-e756-ef39d8779448"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.1  # Try different values: 0.01, 0.05, 0.1, 0.2\n",
        "regret = [run_sim1(eGreedy(2, epsilon=epsilon), mab, step_num=500) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRZM45nqatQG"
      },
      "source": [
        "#### UCB\n",
        "run for different values of confidence level and compare results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Non-Stationary Environment Results:**\n",
        "\n",
        "In this simulation, the first arm's mean changes from 6 to 2 at step 100:\n",
        "- **Before step 100**: Arm 1 is optimal (mean=6 vs 4)\n",
        "- **After step 100**: Arm 2 becomes optimal (mean=4 vs 2)\n",
        "\n",
        "**Expected behavior:**\n",
        "- Standard Thompson Sampling will learn to prefer Arm 1 initially\n",
        "- After the change at step 100, it should eventually detect the change and switch\n",
        "- However, the posterior has accumulated strong belief that Arm 1 is best, making adaptation slow\n",
        "- You should see regret accelerate after step 100 as the algorithm continues pulling the now-suboptimal Arm 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "ZQhIEdblWRW-",
        "outputId": "bf110f08-4943-4545-8a70-d43baf8db0a7"
      },
      "outputs": [],
      "source": [
        "c_level = 2.0  # Try different values: 0.5, 1.0, 2.0, 3.0\n",
        "regret = [run_sim1(UCB(2, c_level=c_level), mab, step_num=500) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT05wlFUQVa9"
      },
      "source": [
        "## 4\n",
        "simulation below assumes a non-stationary multi-armed bandit. specifically in this simulation mean value of distribution of first arm changes in step 100. describe the result of thompson sampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Improved Thompson Sampling for Non-Stationary Environments:**\n",
        "\n",
        "To handle non-stationary environments, we use a **sliding window approach**:\n",
        "- Keep only the most recent N observations (e.g., buffer_size=30)\n",
        "- Discard old observations that may reflect outdated arm statistics\n",
        "- Recompute posterior based only on recent data\n",
        "\n",
        "**Benefits:**\n",
        "- **Faster adaptation**: Old beliefs are \"forgotten\" as buffer fills with new data\n",
        "- **Maintains exploration**: By resetting the posterior periodically, we remain open to changes\n",
        "- **Trade-off**: Buffer size controls adaptation speed vs. statistical efficiency\n",
        "\n",
        "**Expected results:**\n",
        "- Should see faster recovery after the distribution change at step 100\n",
        "- Regret growth should slow down more quickly compared to standard Thompson Sampling\n",
        "- The algorithm effectively \"forgets\" that Arm 1 was previously optimal\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7JXPSSBu3bh"
      },
      "outputs": [],
      "source": [
        "def run_sim2(ts, mab, step_num=200, change_step=100):\n",
        "    init_mean = mab[0].mean\n",
        "    best_mean = np.max([b.mean for b in mab])\n",
        "    regret = []\n",
        "    cumulative_regret = 0\n",
        "    \n",
        "    for i in range(step_num):\n",
        "        if i == change_step:\n",
        "            mab[0].mean = 2\n",
        "            best_mean = np.max([b.mean for b in mab])\n",
        "            \n",
        "                  \n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Select arm using Thompson Sampling\n",
        "        arm_idx = ts.select_arm(i)\n",
        "        \n",
        "        # Sample reward from selected arm\n",
        "        reward = mab[arm_idx].sample()\n",
        "        \n",
        "        # Update policy with observed reward\n",
        "        ts.update(arm_idx, reward)\n",
        "        \n",
        "        # Calculate instantaneous regret\n",
        "        instantaneous_regret = best_mean - mab[arm_idx].mean\n",
        "        cumulative_regret += instantaneous_regret\n",
        "        \n",
        "        # Store cumulative regret\n",
        "        regret.append(cumulative_regret)\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    \n",
        "    mab[0].mean = init_mean\n",
        "    return regret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "25SvdnQoOw4g",
        "outputId": "386f527b-7730-46c8-b5f3-a77090bdb2a4"
      },
      "outputs": [],
      "source": [
        "mab = [Arm(6, 0.5), Arm(4, 0.5)]\n",
        "regret = [run_sim2(ThompsonSampling([b.var for b in mab]), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7VvJ3-yQpUn"
      },
      "source": [
        "### 4.1\n",
        "change thompson sampling algorithm to improve results in non-stationary MAB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb7az3O9ydYz"
      },
      "outputs": [],
      "source": [
        "class NewThompsonSampling:\n",
        "    def __init__(self, var_list, buffer_size=30, **kwargs):\n",
        "        \"\"\"\n",
        "        Thompson Sampling with sliding window for non-stationary environments\n",
        "        buffer_size: maximum number of recent observations to keep per arm\n",
        "        \"\"\"\n",
        "        self.var_list = var_list\n",
        "        self.n_arms = len(var_list)\n",
        "        self.buffer_size = buffer_size\n",
        "        \n",
        "        # Keep sliding window of recent rewards for each arm\n",
        "        self.reward_buffers = [[] for _ in range(self.n_arms)]\n",
        "        \n",
        "        # Initialize prior beliefs\n",
        "        self.prior_means = np.zeros(self.n_arms)\n",
        "        self.prior_vars = np.ones(self.n_arms) * 100\n",
        "\n",
        "    def select_arm(self, *args):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Sample from the posterior distribution of each arm\n",
        "        samples = []\n",
        "        for i in range(self.n_arms):\n",
        "            # Sample from N(prior_mean, prior_var)\n",
        "            sample = np.random.normal(self.prior_means[i], np.sqrt(self.prior_vars[i]))\n",
        "            samples.append(sample)\n",
        "        \n",
        "        # Select arm with highest sample\n",
        "        selected_arm = np.argmax(samples)\n",
        "\n",
        "        # return index of selected arm\n",
        "        return selected_arm\n",
        "\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    def update(self, idx, reward):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # Add reward to buffer\n",
        "        self.reward_buffers[idx].append(reward)\n",
        "        \n",
        "        # Keep only recent rewards (sliding window)\n",
        "        if len(self.reward_buffers[idx]) > self.buffer_size:\n",
        "            self.reward_buffers[idx].pop(0)\n",
        "        \n",
        "        # Update posterior based on recent observations only\n",
        "        if len(self.reward_buffers[idx]) > 0:\n",
        "            recent_rewards = self.reward_buffers[idx]\n",
        "            n_obs = len(recent_rewards)\n",
        "            obs_var = self.var_list[idx]\n",
        "            \n",
        "            # Use a fixed prior variance to allow adaptation\n",
        "            prior_var = 100.0\n",
        "            \n",
        "            # Bayesian update using only recent data\n",
        "            posterior_var = 1.0 / (1.0/prior_var + n_obs/obs_var)\n",
        "            posterior_mean = posterior_var * (0.0/prior_var + sum(recent_rewards)/obs_var)\n",
        "            \n",
        "            self.prior_means[idx] = posterior_mean\n",
        "            self.prior_vars[idx] = posterior_var\n",
        "        \n",
        "        # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "nWmZ2NEWHttZ",
        "outputId": "ab3a9af8-32d8-437f-a205-27cbca4287c8"
      },
      "outputs": [],
      "source": [
        "mab = [Arm(6, 0.5), Arm(4, 0.5)]\n",
        "regret = [run_sim2(NewThompsonSampling([b.var for b in mab]), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
