{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ydGWGgzGRyCv"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "loycX-rxvWss"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import norm\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xcyZl2WrfOdn"
      },
      "outputs": [],
      "source": [
        "def plot_distribution(ax, pdfs, title=''): \n",
        "    x = np.linspace(0., 10., 200)\n",
        "    ymax = 0    \n",
        "    for index, pdf in enumerate(pdfs):\n",
        "        y = norm.pdf(x, pdf.mean, np.sqrt(pdf.var))\n",
        "\n",
        "        p = ax.plot(x, y, lw = 2)\n",
        "        c = p[0].get_markeredgecolor()    \n",
        "        ax.fill_between(x, y, 0, color=c, alpha=0.2 )    \n",
        "        ax.autoscale(tight=True)\n",
        "        ax.vlines(pdf.mean, 0, y.max(), colors = c, linestyles = \"--\", lw = 2)    \n",
        "\n",
        "        ymax = max( ymax, y[1:].max()*1.05 )\n",
        "    ax.set_ylim([0,ymax])\n",
        "\n",
        "def plot_regret(ax, regret, title=''): \n",
        "    ax.plot(regret)\n",
        "\n",
        "def plot(regret, mab):\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(6, 3))\n",
        "    plot_regret(axs[0], regret)\n",
        "    plot_distribution(axs[1], mab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KRawbhcxR0A9"
      },
      "outputs": [],
      "source": [
        "class Arm:\n",
        "    def __init__(self, mean, var):\n",
        "        self.mean = mean\n",
        "        self.var = var\n",
        "\n",
        "    def sample(self):\n",
        "        return np.random.normal(self.mean, np.sqrt(self.var))\n",
        "\n",
        "    def name(self):\n",
        "        return 'N(' + str(self.mean) + ',' + str(self.var) + ')'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04awn6EFPG7d"
      },
      "source": [
        "# Thompson Sampling\n",
        "\n",
        "In this exercise we will run thompson sampling for 2-armed bandit with gaussian distribution.\n",
        "For simplicity assume we know variance of distribution of arms and only mean is unknown\n",
        "for prior assume gaussian distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETjtn1IibGT0"
      },
      "source": [
        "## 1.\n",
        "if variance was unknown what prior distribtion would be suitable?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99BrmMVIbNkF"
      },
      "source": [
        "## 2.\n",
        "Implement Thompson Sampling algorithm. For comparison also implement ϵ-Greedy and UCB algorithms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "a6_MJQc-SJ4d"
      },
      "outputs": [],
      "source": [
        "class ThompsonSampling:\n",
        "    def __init__(self, var_list, **kwargs):\n",
        "        \"\"\"\n",
        "        variance of arms are known to policy\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def select_arm(self, *args):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # select arm based on estimate of prior distribution\n",
        "        \n",
        "        # return index of selected arm\n",
        "\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        pass\n",
        "\n",
        "    def update(self, idx, reward):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # update prior based on reward\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gRCYR3ooSbpq"
      },
      "outputs": [],
      "source": [
        "class UCB:    \n",
        "    def __init__(self, n_bandits, c_level):\n",
        "        \"\"\"\n",
        "        c_level: coefficient of uncertainty\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def select_arm(self, t):\n",
        "        \"\"\"\n",
        "        t: step time\n",
        "        \"\"\"\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # select arm based on UCB\n",
        "        \n",
        "        # return index of selected arm\n",
        "\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        pass\n",
        "\n",
        "    def update(self, idx, reward):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # update based on reward\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ku7iA1qoJyRj"
      },
      "outputs": [],
      "source": [
        "class eGreedy:    \n",
        "    def __init__(self, n_bandits, epsilon):\n",
        "        \"\"\"\n",
        "        epsilon must be given\n",
        "        \"\"\"\n",
        "        pass\n",
        "    \n",
        "    def select_arm(self, *args):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # select arm based on e-Greedy\n",
        "        \n",
        "        # return index of selected arm\n",
        "\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        pass\n",
        "\n",
        "    def update(self, idx, reward):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # update based on reward\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkoURY09P1QG"
      },
      "source": [
        "## 3\n",
        "run simulation for arms described as cells below and describe the differences of regret with different variance in arms distributions\n",
        "\n",
        "rum_sim1 must return cumulitive regret formulated as \n",
        "$$\n",
        "R(T)=\\sum_{i=1}^2 N_i(T) \\Delta_i\n",
        "$$\n",
        "\n",
        "where $N_i(T)$ is number of times arm $i$ was selected until step $T$, $\\Delta_i=\\mu^*-\\mu_i$, $\\mu^*$ is largest mean in arms distribtions and $\\mu_i$ is mean of distribution of arm $i$\n",
        "\n",
        "to get average regret we rum simulation 50 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "99fCVaPDdeid"
      },
      "outputs": [],
      "source": [
        "def run_sim1(policy, mab, step_num=100):\n",
        "    \"\"\"\n",
        "    run simulation of multi-armed bandit\n",
        "    mab: list of arms\n",
        "    \"\"\"\n",
        "    best_mean = np.max([b.mean for b in mab])\n",
        "    regret = []\n",
        "    for k in range(step_num):          \n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # run policy algorithm and return cumulative regret\n",
        " \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        continue\n",
        "    return regret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkTH2mchdkcA"
      },
      "source": [
        "### 3.1\n",
        "Assume Multi-Armed Bandit variables are as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ZMkXIR5WdiWX"
      },
      "outputs": [],
      "source": [
        "mab = [Arm(6, 0.5), Arm(4, 0.5)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5rmrS8paPLT"
      },
      "source": [
        "#### Thompson Sampling\n",
        "run and describe the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "A-ZFMhVtJvxU",
        "outputId": "e512e23b-4422-41b6-f858-9812ff801156"
      },
      "outputs": [],
      "source": [
        "regret = [run_sim1(ThompsonSampling([b.var for b in mab]), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nytkv2z0Z7Ol"
      },
      "source": [
        "#### ϵ-Greedy\n",
        "run for different values of ϵ and compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "K6qvTojUN7w6",
        "outputId": "c836d4ec-a7a2-4512-cb0c-eca22f52e97f"
      },
      "outputs": [],
      "source": [
        "epsilon = None\n",
        "regret = [run_sim1(eGreedy(2, epsilon=epsilon), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPPmqGSKaC6K"
      },
      "source": [
        "#### UCB\n",
        "run for different values of confidence level and compare results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "nYjcq48NU3_I",
        "outputId": "cc97d7d6-e656-419d-86cc-265872fc3466"
      },
      "outputs": [],
      "source": [
        "c_level = None\n",
        "regret = [run_sim1(UCB(2, c_level=c_level), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_BkFPDxeGkJ"
      },
      "source": [
        "### 3.2\n",
        "Assume Multi-Armed Bandit variables are as follows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "r0oA15uneIbq"
      },
      "outputs": [],
      "source": [
        "mab = [Arm(6, 10), Arm(4, 10)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLO2piyfaz3I"
      },
      "source": [
        "#### Thompson Sampling\n",
        "run and compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "SVCgTL5o0-lV",
        "outputId": "7f4c40a9-23b9-4e9a-d982-9b8452e30eb2"
      },
      "outputs": [],
      "source": [
        "regret = [run_sim1(ThompsonSampling([b.var for b in mab]), mab, step_num=500) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGkpS7ihal_v"
      },
      "source": [
        "#### ϵ-Greedy\n",
        "run for different values of ϵ and compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "aM3iSz2FQ5hG",
        "outputId": "bfbc4382-aa00-4443-e756-ef39d8779448"
      },
      "outputs": [],
      "source": [
        "epsilon = None\n",
        "regret = [run_sim1(eGreedy(2, epsilon=epsilon), mab, step_num=500) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRZM45nqatQG"
      },
      "source": [
        "#### UCB\n",
        "run for different values of confidence level and compare results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "ZQhIEdblWRW-",
        "outputId": "bf110f08-4943-4545-8a70-d43baf8db0a7"
      },
      "outputs": [],
      "source": [
        "c_level = None\n",
        "regret = [run_sim1(UCB(2, c_level=c_level), mab, step_num=500) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT05wlFUQVa9"
      },
      "source": [
        "## 4\n",
        "simulation below assumes a non-stationary multi-armed bandit. specifically in this simulation mean value of distribution of first arm changes in step 100. describe the result of thompson sampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "B7JXPSSBu3bh"
      },
      "outputs": [],
      "source": [
        "def run_sim2(ts, mab, step_num=200, change_step=100):\n",
        "    init_mean = mab[0].mean\n",
        "    best_mean = np.max([b.mean for b in mab])\n",
        "    regret = []\n",
        "    for i in range(step_num):\n",
        "        if i == change_step:\n",
        "            mab[0].mean = 2\n",
        "            best_mean = np.max([b.mean for b in mab])\n",
        "            \n",
        "                  \n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # run tompson sampling algorithm and return cumulative regret\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "    mab[0].mean = init_mean\n",
        "    return regret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "25SvdnQoOw4g",
        "outputId": "386f527b-7730-46c8-b5f3-a77090bdb2a4"
      },
      "outputs": [],
      "source": [
        "mab = [Arm(6, 0.5), Arm(4, 0.5)]\n",
        "regret = [run_sim2(ThompsonSampling([b.var for b in mab]), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7VvJ3-yQpUn"
      },
      "source": [
        "### 4.1\n",
        "change thompson sampling algorithm to improve results in non-stationary MAB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "Gb7az3O9ydYz"
      },
      "outputs": [],
      "source": [
        "class NewThompsonSampling:\n",
        "    def __init__(self, var_list, buffer_size=30, **kwargs):\n",
        "        pass\n",
        "\n",
        "    def select_arm(self, *args):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # select arm based on estimate of prior distribution\n",
        "\n",
        "        # return index of selected arm\n",
        "\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        pass\n",
        "\n",
        "    def update(self, idx, reward):\n",
        "        # ==================================== Your Code (Begin) ==================================\n",
        "        \n",
        "        # update prior based on reward\n",
        "        \n",
        "        # ==================================== Your Code (End) ====================================\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "id": "nWmZ2NEWHttZ",
        "outputId": "ab3a9af8-32d8-437f-a205-27cbca4287c8"
      },
      "outputs": [],
      "source": [
        "mab = [Arm(6, 0.5), Arm(4, 0.5)]\n",
        "regret = [run_sim2(NewThompsonSampling([b.var for b in mab]), mab) for _ in range(50)]\n",
        "plot(np.mean(regret, axis=0), mab)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
