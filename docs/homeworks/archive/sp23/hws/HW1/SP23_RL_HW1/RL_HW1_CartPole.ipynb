{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ekcge5vHPpR"
      },
      "source": [
        "# Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ6K6gUyFP8v",
        "outputId": "b524a6f1-dcce-4602-ca63-69fcbadec9b3"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!pip install 'imageio==2.4.0'\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip3 install gymnasium[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lzUJnJrQEN1r"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import base64\n",
        "import random\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import gymnasium as gym\n",
        "from itertools import count\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csY30Op-HVgu"
      },
      "source": [
        "# Utility functions for rendering evironment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B_g_w4-VFqXz"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "  \n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "    \n",
        "    return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IH96rj7THfsS"
      },
      "outputs": [],
      "source": [
        "def create_policy_eval_video(env, policy, filename, num_episodes=1, fps=30):\n",
        "  \n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "        for _ in range(num_episodes):\n",
        "            state, info = env.reset()\n",
        "            video.append_data(env.render())\n",
        "            while True:\n",
        "                state = torch.from_numpy(state).unsqueeze(0).to(DEVICE)\n",
        "                action = policy(state)\n",
        "                state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "                video.append_data(env.render())\n",
        "                if terminated:\n",
        "                    break\n",
        "    return embed_mp4(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozFEZ_bqH71W"
      },
      "source": [
        "# Replay Memory and Q-Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "84EJUO1JH7h4"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WD_Av2C7IIZ4"
      },
      "outputs": [],
      "source": [
        "# Complete the Q-Network below. \n",
        "# The Q-Network takes a state as input and the output is a vector so that each element is the q-value for an action.\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ZW4F6SQeMZ"
      },
      "source": [
        "# Policies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6SfSblT_6f"
      },
      "source": [
        "Now we define 2 policies. We use greedy policy for evaluation and e-greedy during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fokLsyg5Qc41"
      },
      "outputs": [],
      "source": [
        "# This function takes in a state and returns the best action according to your q-network.\n",
        "# Don't forget \"torch.no_grad()\". We don't want gradient flowing through our network. \n",
        "\n",
        "# state shape: (1, state_size) -> output shape: (1, 1)  \n",
        "def greedy_policy(qnet, state):\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "    # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_iE-12xgRc2y"
      },
      "outputs": [],
      "source": [
        "# state shape: (1, state_size) -> output shape: (1, 1)\n",
        "# Don't forget \"torch.no_grad()\". We don't want gradient flowing through our network.\n",
        "\n",
        "def e_greedy_policy(qnet, state, current_timestep):\n",
        "    \n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * current_timestep / EPS_DECAY)\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "    # With probability \"eps_threshold\" choose a random action \n",
        "    # and with probability 1-\"eps_threshold\" choose the best action according to your Q-Network.\n",
        "    \n",
        "    # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PvG1MpOK9mX"
      },
      "source": [
        "# Initial setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "5Sc1a-6ZLAE1",
        "outputId": "023fab53-d9e4-45f4-8f04-e362b6b90ba9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "n_actions = env.action_space.n\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "q_network = DQN(n_observations, n_actions).to(device)\n",
        "target_network = DQN(n_observations, n_actions).to(device)\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "create_policy_eval_video(env, lambda s: greedy_policy(q_network, s), \"random_agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWq08ZENXx6h"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "USGbCrKbFusn",
        "outputId": "637da598-4626-4f28-a0ee-0d165a3129fa"
      },
      "outputs": [],
      "source": [
        "num_episodes = 200\n",
        "episode_returns = []\n",
        "episode_durations = []\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "    # 1. Start a new episode\n",
        "    # 2. Run the environment for 1 step using e-greedy policy\n",
        "    # 3. Add the (state, action, next_state, reward) to replay memory\n",
        "    # 4. Optimize your q_network for 1 iteration\n",
        "    #       4.1 Sample one batch from replay memory\n",
        "    #       4.2 Compute predicted state-action values using q_network\n",
        "    #       4.3 Compute expected state-action values using target_network (Don't forget \"no_grad\" because we don't want gradient through target_network)\n",
        "    #       4.4 Compute loss function and optimize q_network for 1 step\n",
        "    # 5. Soft update the weights of target_network\n",
        "    #       θ′ ← τ θ + (1 −τ )θ′\n",
        "    #       θ   is q_network weights\n",
        "    #       θ′  is target_network weights            \n",
        "    # 6. Keep track of the total reward for each episode to plot later\n",
        "\n",
        "    # ==================================== Your Code (End) ====================================  \n",
        "\n",
        "print('Complete')\n",
        "plt.plot(range(1, num_episodes+1), episode_durations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "1s50clnmF_az",
        "outputId": "9ba83866-f303-4b75-bbfb-88fdb2a0e934"
      },
      "outputs": [],
      "source": [
        "# Render trained model\n",
        "\n",
        "create_policy_eval_video(env, lambda s: greedy_policy(q_network, s), \"trained_agent\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM5dPBUyEHQe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
