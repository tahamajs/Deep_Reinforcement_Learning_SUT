{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ekcge5vHPpR"
      },
      "source": [
        "# Deep Q-Network (DQN) for CartPole-v1: A Comprehensive Implementation\n",
        "\n",
        "## Abstract\n",
        "This notebook presents a complete implementation of the Deep Q-Network (DQN) algorithm [1] for solving the CartPole-v1 control task from OpenAI Gymnasium. We provide detailed theoretical foundations, mathematical formulations, and practical implementation of key reinforcement learning concepts including experience replay, target networks, and ε-greedy exploration strategies. The implementation demonstrates how deep learning can be effectively combined with Q-learning to solve continuous state space problems.\n",
        "\n",
        "---\n",
        "\n",
        "## I. Introduction\n",
        "\n",
        "### A. Motivation\n",
        "Traditional Q-learning algorithms maintain a tabular representation of state-action values, which becomes intractable for problems with large or continuous state spaces. The Deep Q-Network (DQN) addresses this limitation by using a deep neural network as a function approximator, enabling the agent to generalize across similar states.\n",
        "\n",
        "### B. Problem Statement\n",
        "The CartPole-v1 environment presents a classic control problem where an agent must balance a pole on a moving cart. The state space is continuous (4-dimensional), and the action space is discrete (2 actions: move left or right). The goal is to learn a policy that maximizes the cumulative reward by keeping the pole balanced as long as possible.\n",
        "\n",
        "### C. Key Contributions of DQN [1]\n",
        "1. **Experience Replay**: Breaking temporal correlations in training data\n",
        "2. **Target Network**: Stabilizing the learning process\n",
        "3. **Deep Neural Network**: Function approximation for continuous state spaces\n",
        "4. **ε-greedy Exploration**: Balancing exploration and exploitation\n",
        "\n",
        "---\n",
        "\n",
        "## II. Theoretical Background\n",
        "\n",
        "### A. Markov Decision Process (MDP)\n",
        "The reinforcement learning problem is formalized as a Markov Decision Process defined by the tuple (S, A, P, R, γ):\n",
        "- **S**: State space\n",
        "- **A**: Action space  \n",
        "- **P**: Transition probability P(s'|s,a)\n",
        "- **R**: Reward function R(s,a,s')\n",
        "- **γ**: Discount factor ∈ [0,1]\n",
        "\n",
        "### B. Q-Learning Foundation\n",
        "The optimal action-value function Q*(s,a) represents the expected cumulative discounted reward starting from state s, taking action a, and following the optimal policy thereafter:\n",
        "\n",
        "**Q*(s,a) = E[R_t + γR_{t+1} + γ²R_{t+2} + ... | s_t=s, a_t=a]**\n",
        "\n",
        "The Bellman optimality equation states:\n",
        "\n",
        "**Q*(s,a) = E_{s'}[r + γ max_{a'} Q*(s',a') | s,a]**\n",
        "\n",
        "### C. Deep Q-Network Architecture\n",
        "DQN approximates Q*(s,a) using a neural network with parameters θ:\n",
        "\n",
        "**Q(s,a;θ) ≈ Q*(s,a)**\n",
        "\n",
        "The network takes a state s as input and outputs Q-values for all actions simultaneously.\n",
        "\n",
        "### D. Key DQN Innovations\n",
        "\n",
        "#### 1. Experience Replay\n",
        "Store transitions (s_t, a_t, r_t, s_{t+1}) in replay memory D. During training, sample random mini-batches to:\n",
        "- Break temporal correlations between consecutive samples\n",
        "- Improve data efficiency through reuse of experiences\n",
        "- Reduce variance in updates\n",
        "\n",
        "#### 2. Target Network\n",
        "Maintain two networks:\n",
        "- **Q-network** (parameters θ): Updated every step\n",
        "- **Target network** (parameters θ⁻): Updated slowly (soft updates)\n",
        "\n",
        "This separation stabilizes training by providing consistent targets during learning.\n",
        "\n",
        "#### 3. Loss Function\n",
        "The temporal difference (TD) error is minimized using:\n",
        "\n",
        "**L(θ) = E_{(s,a,r,s')~D}[(r + γ max_{a'} Q(s',a';θ⁻) - Q(s,a;θ))²]**\n",
        "\n",
        "Where:\n",
        "- r + γ max_{a'} Q(s',a';θ⁻) is the TD target (using target network)\n",
        "- Q(s,a;θ) is the current prediction (using Q-network)\n",
        "\n",
        "---\n",
        "\n",
        "## III. Algorithm Overview\n",
        "\n",
        "### DQN Algorithm Pseudocode\n",
        "\n",
        "```\n",
        "Initialize replay memory D with capacity N\n",
        "Initialize Q-network with random weights θ\n",
        "Initialize target network with weights θ⁻ = θ\n",
        "\n",
        "For episode = 1 to M:\n",
        "    Initialize state s_1\n",
        "    For t = 1 to T:\n",
        "        Select action a_t:\n",
        "            with probability ε: random action (exploration)\n",
        "            otherwise: a_t = argmax_a Q(s_t, a; θ) (exploitation)\n",
        "        \n",
        "        Execute action a_t, observe reward r_t and next state s_{t+1}\n",
        "        Store transition (s_t, a_t, r_t, s_{t+1}) in D\n",
        "        \n",
        "        Sample random mini-batch of transitions from D\n",
        "        For each transition (s, a, r, s'):\n",
        "            if s' is terminal:\n",
        "                y = r\n",
        "            else:\n",
        "                y = r + γ max_{a'} Q(s', a'; θ⁻)\n",
        "        \n",
        "        Perform gradient descent on (y - Q(s,a;θ))²\n",
        "        \n",
        "        Soft update target network: θ⁻ ← τθ + (1-τ)θ⁻\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## IV. Implementation Details\n",
        "\n",
        "### A. Environment Specifications\n",
        "- **State Space**: 4-dimensional continuous (cart position, cart velocity, pole angle, pole angular velocity)\n",
        "- **Action Space**: 2 discrete actions (push cart left or right)\n",
        "- **Reward**: +1 for every timestep the pole remains upright\n",
        "- **Episode Termination**: Pole angle > 12° or cart position > 2.4 units\n",
        "\n",
        "### B. Network Architecture\n",
        "- Input layer: 4 neurons (state dimensions)\n",
        "- Hidden layer 1: 128 neurons with ReLU activation\n",
        "- Hidden layer 2: 128 neurons with ReLU activation\n",
        "- Output layer: 2 neurons (Q-values for each action, no activation)\n",
        "\n",
        "### C. Hyperparameters\n",
        "The following hyperparameters are used (justified in Section V):\n",
        "- Batch size: 128\n",
        "- Discount factor γ: 0.99\n",
        "- Learning rate α: 1e-4\n",
        "- Replay memory capacity: 10,000\n",
        "- Target network update rate τ: 0.005\n",
        "- Initial exploration ε: 0.9\n",
        "- Final exploration ε: 0.05\n",
        "- Exploration decay: 1000 steps\n",
        "\n",
        "---\n",
        "\n",
        "## V. References\n",
        "[1] Mnih, V., et al. (2015). \"Human-level control through deep reinforcement learning.\" Nature, 518(7540), 529-533.\n",
        "\n",
        "[2] Sutton, R. S., & Barto, A. G. (2018). \"Reinforcement learning: An introduction.\" MIT press.\n",
        "\n",
        "[3] van Hasselt, H., Guez, A., & Silver, D. (2016). \"Deep reinforcement learning with double Q-learning.\" AAAI.\n",
        "\n",
        "---\n",
        "\n",
        "# VI. Installations and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJ6K6gUyFP8v",
        "outputId": "b524a6f1-dcce-4602-ca63-69fcbadec9b3"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get update\n",
        "!pip install 'imageio==2.4.0'\n",
        "!sudo apt-get install -y xvfb ffmpeg\n",
        "!pip3 install gymnasium[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "lzUJnJrQEN1r"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import base64\n",
        "import random\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import gymnasium as gym\n",
        "from itertools import count\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csY30Op-HVgu"
      },
      "source": [
        "# Utility Functions for Rendering Environment\n",
        "\n",
        "These helper functions allow us to:\n",
        "1. Record agent performance as video\n",
        "2. Embed videos in the notebook for visualization\n",
        "3. Evaluate trained policies visually"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "B_g_w4-VFqXz"
      },
      "outputs": [],
      "source": [
        "def embed_mp4(filename):\n",
        "    \"\"\"Convert MP4 video to base64 and embed in HTML for notebook display\"\"\"\n",
        "    video = open(filename,'rb').read()\n",
        "    b64 = base64.b64encode(video)\n",
        "    tag = '''\n",
        "    <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "    Your browser does not support the video tag.\n",
        "    </video>'''.format(b64.decode())\n",
        "    \n",
        "    return IPython.display.HTML(tag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IH96rj7THfsS"
      },
      "outputs": [],
      "source": [
        "def create_policy_eval_video(env, policy, filename, num_episodes=1, fps=30):\n",
        "    \"\"\"Create a video of the agent following a given policy\"\"\"\n",
        "    filename = filename + \".mp4\"\n",
        "    with imageio.get_writer(filename, fps=fps) as video:\n",
        "        for _ in range(num_episodes):\n",
        "            state, info = env.reset()\n",
        "            video.append_data(env.render())\n",
        "            while True:\n",
        "                state = torch.from_numpy(state).unsqueeze(0).to(DEVICE)\n",
        "                action = policy(state)\n",
        "                state, reward, terminated, truncated, _ = env.step(action.item())\n",
        "                video.append_data(env.render())\n",
        "                if terminated:\n",
        "                    break\n",
        "    return embed_mp4(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozFEZ_bqH71W"
      },
      "source": [
        "# VIII. Experience Replay Memory and Q-Network Architecture\n",
        "\n",
        "## A. Experience Replay: Theoretical Foundation\n",
        "\n",
        "### 1. Motivation for Experience Replay\n",
        "Standard online reinforcement learning suffers from:\n",
        "- **Temporal Correlation**: Consecutive samples are highly correlated, violating the i.i.d. assumption\n",
        "- **Sample Inefficiency**: Each experience is used once and discarded\n",
        "- **Catastrophic Forgetting**: Rapid policy changes can erase previous learning\n",
        "\n",
        "### 2. Experience Replay Mechanism\n",
        "Store transitions τ = (s_t, a_t, r_t, s_{t+1}) in replay buffer D with capacity N. During training:\n",
        "1. Store current transition: D ← D ∪ {τ_t}\n",
        "2. Sample mini-batch: B ~ Uniform(D) where |B| = batch_size\n",
        "3. Update network using samples from B\n",
        "\n",
        "### 3. Mathematical Benefits\n",
        "By sampling uniformly from D, we:\n",
        "- **Decorrelate samples**: P(τ_i, τ_j in B) = 1/N² for i≠j\n",
        "- **Stabilize gradients**: Reduce variance in gradient estimates\n",
        "- **Improve sample efficiency**: Each transition can be reused multiple times\n",
        "\n",
        "**Expected gradient with replay**:\n",
        "∇L(θ) = E_{(s,a,r,s')~Uniform(D)}[∇_θ(y - Q(s,a;θ))²]\n",
        "\n",
        "where y = r + γ max_{a'} Q(s',a';θ⁻)\n",
        "\n",
        "## B. Replay Memory Implementation Details\n",
        "\n",
        "### Data Structure\n",
        "- **Circular Buffer**: Uses collections.deque with maxlen for automatic FIFO replacement\n",
        "- **Capacity**: 10,000 transitions (tunable based on memory constraints)\n",
        "- **Sampling**: Random uniform sampling without replacement within each batch\n",
        "\n",
        "## C. Q-Network Architecture: Deep Neural Network Design\n",
        "\n",
        "### 1. Architecture Specifications\n",
        "The Q-network Q(s;θ) : R^n → R^m maps states to action values:\n",
        "\n",
        "**Input Layer**: n = 4 neurons (state dimensionality)\n",
        "↓\n",
        "**Hidden Layer 1**: 128 neurons + ReLU\n",
        "   Mathematically: h₁ = ReLU(W₁s + b₁)\n",
        "↓\n",
        "**Hidden Layer 2**: 128 neurons + ReLU\n",
        "   Mathematically: h₂ = ReLU(W₂h₁ + b₂)\n",
        "↓\n",
        "**Output Layer**: m = 2 neurons (action dimensionality)\n",
        "   Mathematically: Q(s) = W₃h₂ + b₃\n",
        "\n",
        "### 2. Activation Function Choice\n",
        "\n",
        "**ReLU (Rectified Linear Unit)**: f(x) = max(0, x)\n",
        "- **Advantages**:\n",
        "  * Mitigates vanishing gradient problem\n",
        "  * Computationally efficient\n",
        "  * Promotes sparse activation\n",
        "  * Empirically effective for deep networks\n",
        "\n",
        "**No activation on output**: Q-values can be any real number (unbounded)\n",
        "\n",
        "### 3. Network Capacity\n",
        "Total parameters:\n",
        "- Layer 1: 4 × 128 + 128 = 640 parameters\n",
        "- Layer 2: 128 × 128 + 128 = 16,512 parameters  \n",
        "- Layer 3: 128 × 2 + 2 = 258 parameters\n",
        "- **Total**: 17,410 trainable parameters\n",
        "\n",
        "This capacity is sufficient for CartPole while avoiding overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "84EJUO1JH7h4"
      },
      "outputs": [],
      "source": [
        "class ReplayMemory(object):\n",
        "    \"\"\"Experience Replay Memory with fixed capacity\"\"\"\n",
        "    def __init__(self, capacity):\n",
        "        self.memory = deque([], maxlen=capacity)\n",
        "\n",
        "    def push(self, transition):\n",
        "        \"\"\"Save a transition\"\"\"\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Randomly sample a batch of transitions\"\"\"\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "WD_Av2C7IIZ4"
      },
      "outputs": [],
      "source": [
        "# Complete the Q-Network below. \n",
        "# The Q-Network takes a state as input and the output is a vector so that each element is the q-value for an action.\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"Deep Q-Network with 3 fully connected layers\"\"\"\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # Define a simple feedforward neural network with 3 layers\n",
        "        # Architecture: Input -> 128 -> 128 -> Output\n",
        "        self.layer1 = nn.Linear(n_observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "        # ==================================== Your Code (End) ====================================\n",
        "\n",
        "    def forward(self, x):\n",
        "        # ==================================== Your Code (Begin) ====================================\n",
        "        # Forward pass through the network with ReLU activations\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)  # No activation on output layer (Q-values can be any real number)\n",
        "        # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ZW4F6SQeMZ"
      },
      "source": [
        "# IX. Action Selection Policies: Exploration-Exploitation Trade-off\n",
        "\n",
        "## A. The Exploration-Exploitation Dilemma\n",
        "\n",
        "One of the fundamental challenges in reinforcement learning is balancing:\n",
        "- **Exploitation**: Choosing actions known to yield high rewards\n",
        "- **Exploration**: Trying new actions to discover potentially better strategies\n",
        "\n",
        "This dilemma is formalized as the **multi-armed bandit problem** extended to sequential decision-making.\n",
        "\n",
        "## B. Greedy Policy (Pure Exploitation)\n",
        "\n",
        "### 1. Definition\n",
        "The greedy policy deterministically selects the action with maximum Q-value:\n",
        "\n",
        "**π_greedy(s) = argmax_{a∈A} Q(s,a;θ)**\n",
        "\n",
        "### 2. Implementation Details\n",
        "```python\n",
        "with torch.no_grad():  # Disable gradient computation for efficiency\n",
        "    action = argmax_a Q(s,a)  # Select best action\n",
        "```\n",
        "\n",
        "### 3. Use Cases\n",
        "- **Policy Evaluation**: Assessing learned policy performance\n",
        "- **Testing**: Final deployment after training\n",
        "- **Deterministic Behavior**: When exploration is undesirable\n",
        "\n",
        "### 4. Limitations\n",
        "- **No exploration**: Cannot discover better actions\n",
        "- **Suboptimal convergence**: May converge to local optima\n",
        "- **Sensitive to initialization**: Poor initial estimates persist\n",
        "\n",
        "## C. ε-Greedy Policy (Exploration-Exploitation Balance)\n",
        "\n",
        "### 1. Mathematical Formulation\n",
        "The ε-greedy policy is defined as:\n",
        "\n",
        "**π_ε(a|s) = {\n",
        "    1-ε + ε/|A|,  if a = argmax_{a'} Q(s,a';θ)  (best action)\n",
        "    ε/|A|,         otherwise                      (random actions)\n",
        "}**\n",
        "\n",
        "Simplified interpretation:\n",
        "- Probability ε: select random action (uniform over A)\n",
        "- Probability 1-ε: select best action\n",
        "\n",
        "### 2. Epsilon Decay Schedule\n",
        "To shift from exploration to exploitation over time, ε is decayed:\n",
        "\n",
        "**ε(t) = ε_end + (ε_start - ε_end) × exp(-t/τ)**\n",
        "\n",
        "Where:\n",
        "- **ε_start = 0.9**: Initial exploration (90% random actions)\n",
        "- **ε_end = 0.05**: Final exploration (5% random actions)\n",
        "- **τ = 1000**: Decay constant (time scale)\n",
        "- **t**: Current timestep\n",
        "\n",
        "### 3. Decay Behavior Analysis\n",
        "The exponential decay ensures:\n",
        "- **Rapid initial exploration**: High ε at start discovers state space\n",
        "- **Gradual transition**: Smooth shift from exploration to exploitation\n",
        "- **Minimal final exploration**: Small ε_end maintains robustness\n",
        "\n",
        "**Half-life**: t_{1/2} = τ ln(2) ≈ 693 steps (time for ε to decay by 50%)\n",
        "\n",
        "### 4. Theoretical Justification\n",
        "ε-greedy exploration provides:\n",
        "- **PAC (Probably Approximately Correct) guarantees**: Bounded suboptimality with high probability\n",
        "- **Convergence assurance**: Infinite exploration ensures visiting all state-action pairs\n",
        "- **Computational efficiency**: Simple to implement and compute\n",
        "\n",
        "### 5. Implementation Considerations\n",
        "```python\n",
        "sample = random()  # Uniform [0,1]\n",
        "if sample > ε(t):\n",
        "    action = argmax_a Q(s,a;θ)  # Exploitation\n",
        "else:\n",
        "    action = random(A)           # Exploration\n",
        "```\n",
        "\n",
        "## D. Alternative Exploration Strategies (Not Implemented Here)\n",
        "\n",
        "### 1. Boltzmann Exploration (Softmax)\n",
        "**π(a|s) = exp(Q(s,a)/τ) / Σ_{a'} exp(Q(s,a')/τ)**\n",
        "- Temperature τ controls randomness\n",
        "- Favors high-value actions probabilistically\n",
        "\n",
        "### 2. Upper Confidence Bound (UCB)\n",
        "Selects actions with highest upper confidence bound on value estimate\n",
        "\n",
        "### 3. Thompson Sampling\n",
        "Bayesian approach: sample from posterior distribution over Q-values\n",
        "\n",
        "### 4. Noisy Networks\n",
        "Add parametric noise to network weights for exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6SfSblT_6f"
      },
      "source": [
        "Now we define 2 policies. We use greedy policy for evaluation and e-greedy during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "fokLsyg5Qc41"
      },
      "outputs": [],
      "source": [
        "# This function takes in a state and returns the best action according to your q-network.\n",
        "# Don't forget \"torch.no_grad()\". We don't want gradient flowing through our network. \n",
        "\n",
        "# state shape: (1, state_size) -> output shape: (1, 1)  \n",
        "def greedy_policy(qnet, state):\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "    with torch.no_grad():\n",
        "        # Get Q-values for all actions and select the action with maximum Q-value\n",
        "        return qnet(state).max(1)[1].view(1, 1)\n",
        "    # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "_iE-12xgRc2y"
      },
      "outputs": [],
      "source": [
        "# state shape: (1, state_size) -> output shape: (1, 1)\n",
        "# Don't forget \"torch.no_grad()\". We don't want gradient flowing through our network.\n",
        "\n",
        "def e_greedy_policy(qnet, state, current_timestep):\n",
        "    \"\"\"Epsilon-greedy action selection with exponential decay\"\"\"\n",
        "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * current_timestep / EPS_DECAY)\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "    # With probability \"eps_threshold\" choose a random action \n",
        "    # and with probability 1-\"eps_threshold\" choose the best action according to your Q-Network.\n",
        "    \n",
        "    sample = random.random()\n",
        "    if sample > eps_threshold:\n",
        "        # Exploitation: choose best action\n",
        "        with torch.no_grad():\n",
        "            return qnet(state).max(1)[1].view(1, 1)\n",
        "    else:\n",
        "        # Exploration: choose random action\n",
        "        return torch.tensor([[random.randrange(n_actions)]], device=device, dtype=torch.long)\n",
        "    # ==================================== Your Code (End) ===================================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PvG1MpOK9mX"
      },
      "source": [
        "# X. Experimental Setup and Hyperparameter Configuration\n",
        "\n",
        "## A. Hyperparameter Selection and Justification\n",
        "\n",
        "### 1. BATCH_SIZE = 128\n",
        "**Purpose**: Number of transitions sampled from replay buffer per update\n",
        "\n",
        "**Trade-offs**:\n",
        "- **Larger batches**: \n",
        "  * More stable gradient estimates (lower variance)\n",
        "  * Better GPU utilization\n",
        "  * Slower learning (fewer updates per epoch)\n",
        "- **Smaller batches**:\n",
        "  * Higher gradient variance\n",
        "  * More frequent updates\n",
        "  * May escape local minima better\n",
        "\n",
        "**Justification**: 128 provides good balance for CartPole. Gradient variance is acceptable while maintaining computational efficiency.\n",
        "\n",
        "**Mathematical insight**: Variance of gradient estimate decreases as O(1/√batch_size)\n",
        "\n",
        "### 2. GAMMA (γ) = 0.99\n",
        "**Purpose**: Discount factor for future rewards\n",
        "\n",
        "**Interpretation**: Future reward T steps ahead is discounted by γ^T\n",
        "- γ = 0: Only immediate rewards matter (myopic)\n",
        "- γ → 1: All future rewards equally important (far-sighted)\n",
        "\n",
        "**Effective horizon**: H_eff = 1/(1-γ) = 100 steps\n",
        "\n",
        "**Justification**: CartPole episodes can last 500 steps. γ=0.99 ensures agent considers long-term consequences while maintaining numerical stability.\n",
        "\n",
        "**Effect on learning**: Higher γ increases variance in value estimates but enables long-term planning.\n",
        "\n",
        "### 3. Exploration Parameters\n",
        "**EPS_START = 0.9**: Initial exploration rate (90%)\n",
        "- **Justification**: High initial exploration ensures comprehensive state space coverage\n",
        "- **Critical for**: Discovering successful strategies early in training\n",
        "\n",
        "**EPS_END = 0.05**: Final exploration rate (5%)\n",
        "- **Justification**: Maintains minimal exploration to adapt to environment changes\n",
        "- **Prevents**: Complete elimination of exploration (robustness)\n",
        "\n",
        "**EPS_DECAY = 1000**: Decay time constant\n",
        "- **Justification**: Allows ~2000 steps (4-10 episodes) for exploration phase\n",
        "- **Balances**: Sufficient exploration vs. timely exploitation\n",
        "\n",
        "### 4. TAU (τ) = 0.005\n",
        "**Purpose**: Soft update rate for target network\n",
        "\n",
        "**Update rule**: θ⁻ ← τθ + (1-τ)θ⁻\n",
        "\n",
        "**Effective time constant**: T_eff = 1/τ = 200 updates\n",
        "\n",
        "**Trade-offs**:\n",
        "- **Small τ (slow updates)**: \n",
        "  * More stable learning\n",
        "  * Target network lags behind\n",
        "  * Prevents oscillations\n",
        "- **Large τ (fast updates)**:\n",
        "  * Rapid target adaptation  \n",
        "  * Less stability\n",
        "  * Approaches standard Q-learning (τ=1)\n",
        "\n",
        "**Justification**: τ=0.005 provides stability while preventing excessive lag. Target network tracks Q-network over ~200 updates.\n",
        "\n",
        "### 5. LR (Learning Rate) = 1e-4\n",
        "**Purpose**: Step size for gradient descent\n",
        "\n",
        "**Adam optimizer adaptive learning**: Combines momentum and RMSprop\n",
        "- Maintains per-parameter learning rates\n",
        "- Adapts to gradient history\n",
        "- More robust than SGD\n",
        "\n",
        "**Trade-offs**:\n",
        "- **High LR**: Faster initial learning, instability, overshooting\n",
        "- **Low LR**: Stable convergence, slow learning, may get stuck\n",
        "\n",
        "**Justification**: 1e-4 is conservative for DQN. Ensures stable learning in CartPole's relatively simple environment.\n",
        "\n",
        "**Typical range**: [1e-5, 1e-3] for deep RL\n",
        "\n",
        "### 6. Memory Capacity = 10,000\n",
        "**Purpose**: Maximum transitions stored in replay buffer\n",
        "\n",
        "**Memory requirement**: 10,000 × (4 + 1 + 1 + 4) × 4 bytes = 400 KB (negligible)\n",
        "\n",
        "**Trade-offs**:\n",
        "- **Larger capacity**:\n",
        "  * More diverse experiences\n",
        "  * Better decorrelation\n",
        "  * Slower adaptation to policy changes\n",
        "- **Smaller capacity**:\n",
        "  * Less memory usage\n",
        "  * Faster adaptation\n",
        "  * Risk of overfitting to recent experiences\n",
        "\n",
        "**Justification**: 10,000 transitions represents ~20-50 episodes. Provides sufficient diversity for CartPole while maintaining adaptation capability.\n",
        "\n",
        "## B. System Components\n",
        "\n",
        "### 1. Environment: CartPole-v1\n",
        "- **State**: [cart_position, cart_velocity, pole_angle, pole_angular_velocity]\n",
        "- **Actions**: {0: Push left, 1: Push right}\n",
        "- **Reward**: +1 per timestep\n",
        "- **Terminal**: |angle| > 12° or |position| > 2.4 or t > 500\n",
        "\n",
        "### 2. Q-Network (Policy Network)\n",
        "- **Role**: Current Q-value approximation\n",
        "- **Updates**: Every timestep (if batch available)\n",
        "- **Gradients**: Backpropagation from TD error\n",
        "\n",
        "### 3. Target Network\n",
        "- **Role**: Stable target for TD computation\n",
        "- **Updates**: Soft updates with rate τ\n",
        "- **Purpose**: Prevent moving target problem\n",
        "\n",
        "### 4. Optimizer: Adam\n",
        "- **Parameters**: β₁=0.9, β₂=0.999, ε=1e-8 (PyTorch defaults)\n",
        "- **Advantages**: Adaptive learning rates, momentum, bias correction\n",
        "- **Memory**: Maintains first and second moment estimates\n",
        "\n",
        "### 5. Loss Function: Smooth L1 (Huber Loss)\n",
        "**Definition**:\n",
        "L_δ(x) = {\n",
        "  0.5x²,        if |x| ≤ δ\n",
        "  δ(|x| - 0.5δ), if |x| > δ\n",
        "}\n",
        "\n",
        "**Advantages over MSE**:\n",
        "- Less sensitive to outliers\n",
        "- Quadratic for small errors (fast convergence)\n",
        "- Linear for large errors (robustness)\n",
        "\n",
        "## C. Computational Resources\n",
        "- **Device**: CUDA GPU if available, else CPU\n",
        "- **Training time**: ~2-5 minutes on CPU, ~30 seconds on GPU\n",
        "- **Memory usage**: < 1 GB RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "5Sc1a-6ZLAE1",
        "outputId": "023fab53-d9e4-45f4-8f04-e362b6b90ba9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 1000\n",
        "TAU = 0.005\n",
        "LR = 1e-4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = gym.make(\"CartPole-v1\", render_mode='rgb_array')\n",
        "n_actions = env.action_space.n\n",
        "state, info = env.reset()\n",
        "n_observations = len(state)\n",
        "q_network = DQN(n_observations, n_actions).to(device)\n",
        "target_network = DQN(n_observations, n_actions).to(device)\n",
        "target_network.load_state_dict(q_network.state_dict())  # Initialize target network with same weights\n",
        "optimizer = optim.Adam(q_network.parameters(), lr=LR)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"State space: {n_observations}\")\n",
        "print(f\"Action space: {n_actions}\")\n",
        "print(\"\\nRandom agent before training:\")\n",
        "create_policy_eval_video(env, lambda s: greedy_policy(q_network, s), \"random_agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWq08ZENXx6h"
      },
      "source": [
        "# Training Loop\n",
        "\n",
        "## DQN Algorithm Steps:\n",
        "\n",
        "### For each episode:\n",
        "1. **Reset environment** and get initial state\n",
        "2. **Interact with environment**:\n",
        "   - Select action using ε-greedy policy\n",
        "   - Execute action and observe reward and next state\n",
        "   - Store transition in replay memory\n",
        "   \n",
        "3. **Learn from experience**:\n",
        "   - Sample random batch from replay memory\n",
        "   - Compute predicted Q-values: Q(s,a) using q_network\n",
        "   - Compute target Q-values: r + γ × max_a' Q(s',a') using target_network\n",
        "   - Compute loss (e.g., Huber loss or MSE)\n",
        "   - Update q_network via gradient descent\n",
        "   \n",
        "4. **Soft update target network**:\n",
        "   - θ' ← τθ + (1-τ)θ'\n",
        "   - Slowly update target network to track q_network\n",
        "   \n",
        "5. **Track performance**:\n",
        "   - Record episode duration and total reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "USGbCrKbFusn",
        "outputId": "637da598-4626-4f28-a0ee-0d165a3129fa"
      },
      "outputs": [],
      "source": [
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "num_episodes = 200\n",
        "episode_returns = []\n",
        "episode_durations = []\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # ==================================== Your Code (Begin) ====================================\n",
        "    # 1. Start a new episode\n",
        "    state, info = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "    \n",
        "    total_reward = 0\n",
        "    t = 0\n",
        "    \n",
        "    for t in count():\n",
        "        # 2. Run the environment for 1 step using e-greedy policy\n",
        "        action = e_greedy_policy(q_network, state, i_episode * 500 + t)\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        total_reward += reward.item()\n",
        "        \n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        \n",
        "        # 3. Add the (state, action, next_state, reward) to replay memory\n",
        "        memory.push(Transition(state, action, next_state, reward))\n",
        "        \n",
        "        # Move to next state\n",
        "        state = next_state\n",
        "        \n",
        "        # 4. Optimize your q_network for 1 iteration\n",
        "        if len(memory) >= BATCH_SIZE:\n",
        "            # 4.1 Sample one batch from replay memory\n",
        "            transitions = memory.sample(BATCH_SIZE)\n",
        "            batch = Transition(*zip(*transitions))\n",
        "            \n",
        "            # Create masks for non-final states\n",
        "            non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), \n",
        "                                         device=device, dtype=torch.bool)\n",
        "            non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "            \n",
        "            state_batch = torch.cat(batch.state)\n",
        "            action_batch = torch.cat(batch.action)\n",
        "            reward_batch = torch.cat(batch.reward)\n",
        "            \n",
        "            # 4.2 Compute predicted state-action values using q_network\n",
        "            # Q(s_t, a) - the model computes Q(s_t), then we select the columns of actions taken\n",
        "            state_action_values = q_network(state_batch).gather(1, action_batch)\n",
        "            \n",
        "            # 4.3 Compute expected state-action values using target_network\n",
        "            next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
        "            with torch.no_grad():\n",
        "                next_state_values[non_final_mask] = target_network(non_final_next_states).max(1)[0]\n",
        "            # Compute the expected Q values: r + gamma * max_a' Q(s', a')\n",
        "            expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
        "            \n",
        "            # 4.4 Compute loss function and optimize q_network for 1 step\n",
        "            # Huber loss is less sensitive to outliers than MSE\n",
        "            criterion = nn.SmoothL1Loss()\n",
        "            loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "            \n",
        "            # Optimize the model\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_value_(q_network.parameters(), 100)\n",
        "            optimizer.step()\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            episode_durations.append(t + 1)\n",
        "            episode_returns.append(total_reward)\n",
        "            break\n",
        "    \n",
        "    # 5. Soft update the weights of target_network\n",
        "    # θ′ ← τ θ + (1 −τ )θ′\n",
        "    target_net_state_dict = target_network.state_dict()\n",
        "    policy_net_state_dict = q_network.state_dict()\n",
        "    for key in policy_net_state_dict:\n",
        "        target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
        "    target_network.load_state_dict(target_net_state_dict)\n",
        "    \n",
        "    # Print progress every 10 episodes\n",
        "    if (i_episode + 1) % 10 == 0:\n",
        "        avg_duration = sum(episode_durations[-10:]) / 10\n",
        "        avg_return = sum(episode_returns[-10:]) / 10\n",
        "        print(f'Episode {i_episode+1}/{num_episodes} | Avg Duration: {avg_duration:.1f} | Avg Return: {avg_return:.1f}')\n",
        "\n",
        "    # ==================================== Your Code (End) ====================================  \n",
        "\n",
        "print('\\nTraining Complete!')\n",
        "print(f'Final Average Duration (last 10 episodes): {sum(episode_durations[-10:]) / 10:.1f}')\n",
        "print(f'Final Average Return (last 10 episodes): {sum(episode_returns[-10:]) / 10:.1f}')\n",
        "\n",
        "# Plot results\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, num_episodes+1), episode_durations)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Duration')\n",
        "plt.title('Episode Durations Over Training')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, num_episodes+1), episode_returns)\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Total Return')\n",
        "plt.title('Episode Returns Over Training')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "\n",
        "Now let's visualize how well our trained agent performs!\n",
        "Compare this video to the random agent from before training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "1s50clnmF_az",
        "outputId": "9ba83866-f303-4b75-bbfb-88fdb2a0e934"
      },
      "outputs": [],
      "source": [
        "# Render trained model\n",
        "print(\"Trained agent performance:\")\n",
        "create_policy_eval_video(env, lambda s: greedy_policy(q_network, s), \"trained_agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Analysis and Discussion\n",
        "\n",
        "## What We Learned:\n",
        "1. **Experience Replay** helps break correlation between consecutive samples\n",
        "2. **Target Network** provides stable learning targets\n",
        "3. **ε-greedy exploration** balances exploration and exploitation\n",
        "4. **Soft updates** of target network prevent instability\n",
        "\n",
        "## Expected Results:\n",
        "- CartPole is considered solved when average reward ≥ 195 over 100 episodes\n",
        "- The agent should learn to balance the pole for longer durations\n",
        "- Training curves should show increasing episode durations\n",
        "\n",
        "## Further Improvements:\n",
        "- **Double DQN**: Reduce overestimation bias\n",
        "- **Dueling DQN**: Separate value and advantage streams\n",
        "- **Prioritized Experience Replay**: Sample important transitions more frequently\n",
        "- **Noisy Networks**: Add parametric noise for exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uM5dPBUyEHQe"
      },
      "outputs": [],
      "source": [
        "# Optional: Save the trained model\n",
        "torch.save({\n",
        "    'q_network_state_dict': q_network.state_dict(),\n",
        "    'target_network_state_dict': target_network.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'episode_durations': episode_durations,\n",
        "    'episode_returns': episode_returns,\n",
        "}, 'dqn_cartpole.pth')\n",
        "\n",
        "print(\"Model saved as 'dqn_cartpole.pth'\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
