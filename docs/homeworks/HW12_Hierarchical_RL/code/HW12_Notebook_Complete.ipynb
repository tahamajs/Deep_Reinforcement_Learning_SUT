{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW12: Hierarchical Reinforcement Learning\n",
        "\n",
        "**Course:** Deep Reinforcement Learning  \n",
        "**Assignment:** Homework 12 - Hierarchical RL  \n",
        "**Date:** 2024\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "Hierarchical Reinforcement Learning (HRL) structures policies across multiple levels of abstraction, enabling agents to solve complex, long-horizon tasks by decomposing them into simpler subtasks. This assignment explores temporal abstraction, options framework, feudal architectures, and goal-conditioned policies.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "1. **Temporal Abstraction**: Understand multi-scale decision making\n",
        "2. **Options Framework**: Master semi-Markov decision processes\n",
        "3. **Feudal Hierarchies**: Learn manager-worker architectures\n",
        "4. **Goal-Conditioned RL**: Train policies with diverse goals\n",
        "5. **Skill Discovery**: Learn reusable primitives automatically\n",
        "6. **Credit Assignment**: Address challenges across temporal scales\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction to Hierarchical RL](#introduction)\n",
        "2. [Options Framework](#options-framework)\n",
        "3. [Feudal Hierarchies](#feudal-hierarchies)\n",
        "4. [Goal-Conditioned RL](#goal-conditioned-rl)\n",
        "5. [Skill Discovery](#skill-discovery)\n",
        "6. [HAM Framework](#ham-framework)\n",
        "7. [Evaluation and Comparison](#evaluation)\n",
        "8. [Conclusion](#conclusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, deque\n",
        "import gym\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
