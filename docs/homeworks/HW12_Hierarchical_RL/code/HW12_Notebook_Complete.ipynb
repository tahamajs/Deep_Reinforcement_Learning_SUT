{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW12: Hierarchical Reinforcement Learning\n",
        "\n",
        "**Course:** Deep Reinforcement Learning  \n",
        "**Assignment:** Homework 12 - Hierarchical RL  \n",
        "**Date:** 2024\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "Hierarchical Reinforcement Learning (HRL) structures policies across multiple levels of abstraction, enabling agents to solve complex, long-horizon tasks by decomposing them into simpler subtasks. This assignment explores temporal abstraction, options framework, feudal architectures, and goal-conditioned policies.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "1. **Temporal Abstraction**: Understand multi-scale decision making\n",
        "2. **Options Framework**: Master semi-Markov decision processes\n",
        "3. **Feudal Hierarchies**: Learn manager-worker architectures\n",
        "4. **Goal-Conditioned RL**: Train policies with diverse goals\n",
        "5. **Skill Discovery**: Learn reusable primitives automatically\n",
        "6. **Credit Assignment**: Address challenges across temporal scales\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction to Hierarchical RL](#introduction)\n",
        "2. [Options Framework](#options-framework)\n",
        "3. [Feudal Hierarchies](#feudal-hierarchies)\n",
        "4. [Goal-Conditioned RL](#goal-conditioned-rl)\n",
        "5. [Skill Discovery](#skill-discovery)\n",
        "6. [HAM Framework](#ham-framework)\n",
        "7. [Evaluation and Comparison](#evaluation)\n",
        "8. [Conclusion](#conclusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, deque\n",
        "import gym\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Hierarchical RL\n",
        "\n",
        "### Motivation for Hierarchy\n",
        "\n",
        "**Challenges in Flat RL:**\n",
        "- **Long Horizons**: Credit assignment difficult over 1000+ steps\n",
        "- **Sparse Rewards**: Random exploration ineffective\n",
        "- **Complex Tasks**: Atomic actions insufficient\n",
        "- **Transfer**: Hard to reuse learned behaviors\n",
        "\n",
        "**Benefits of Hierarchy:**\n",
        "- Temporal abstraction (plan at multiple scales)\n",
        "- Reusable skills/subpolicies\n",
        "- Exploration structure\n",
        "- Transfer learning\n",
        "- Compositional generalization\n",
        "\n",
        "### Human Example:\n",
        "```\n",
        "Task: Make dinner\n",
        "├─ Shop for ingredients\n",
        "│  ├─ Drive to store\n",
        "│  ├─ Find items\n",
        "│  └─ Checkout\n",
        "├─ Prepare food\n",
        "│  ├─ Chop vegetables\n",
        "│  ├─ Cook proteins\n",
        "│  └─ Mix ingredients\n",
        "└─ Serve meal\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1.1: Create a Simple GridWorld Environment for HRL\n",
        "class GridWorld:\n",
        "    \"\"\"\n",
        "    A simple grid world environment for testing hierarchical RL algorithms.\n",
        "    The agent must navigate from start to goal, potentially using hierarchical actions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, width=10, height=10, start=(0, 0), goal=(9, 9)):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.state = start\n",
        "        \n",
        "        # Define atomic actions: up, down, left, right\n",
        "        self.atomic_actions = [(0, 1), (0, -1), (-1, 0), (1, 0)]\n",
        "        self.action_names = ['up', 'down', 'left', 'right']\n",
        "        \n",
        "        # Define hierarchical actions (options)\n",
        "        self.options = {}\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to initial state\"\"\"\n",
        "        self.state = self.start\n",
        "        return self.state\n",
        "        \n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        if isinstance(action, int):\n",
        "            # Atomic action\n",
        "            dx, dy = self.atomic_actions[action]\n",
        "            new_x = max(0, min(self.width-1, self.state[0] + dx))\n",
        "            new_y = max(0, min(self.height-1, self.state[1] + dy))\n",
        "            self.state = (new_x, new_y)\n",
        "        else:\n",
        "            # Hierarchical action (option)\n",
        "            self.state = action\n",
        "            \n",
        "        # Calculate reward\n",
        "        reward = 1.0 if self.state == self.goal else -0.01\n",
        "        \n",
        "        # Check if done\n",
        "        done = self.state == self.goal\n",
        "        \n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "    def get_distance_to_goal(self, state):\n",
        "        \"\"\"Calculate Manhattan distance to goal\"\"\"\n",
        "        return abs(state[0] - self.goal[0]) + abs(state[1] - self.goal[1])\n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Render the current state\"\"\"\n",
        "        grid = np.zeros((self.height, self.width))\n",
        "        grid[self.start[1], self.start[0]] = 1  # Start\n",
        "        grid[self.goal[1], self.goal[0]] = 2     # Goal\n",
        "        grid[self.state[1], self.state[0]] = 3   # Current position\n",
        "        \n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(grid, cmap='viridis')\n",
        "        plt.title(f\"GridWorld - Current: {self.state}, Goal: {self.goal}\")\n",
        "        plt.show()\n",
        "\n",
        "# Test the environment\n",
        "env = GridWorld()\n",
        "print(\"GridWorld Environment Created!\")\n",
        "print(f\"Start: {env.start}, Goal: {env.goal}\")\n",
        "print(f\"Atomic actions: {env.action_names}\")\n",
        "\n",
        "# Test a few steps\n",
        "state = env.reset()\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "for i in range(5):\n",
        "    action = np.random.randint(4)  # Random atomic action\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    print(f\"Step {i+1}: Action {env.action_names[action]} -> State {next_state}, Reward {reward:.2f}\")\n",
        "    if done:\n",
        "        print(\"Goal reached!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Options Framework\n",
        "\n",
        "### Formal Definition\n",
        "\n",
        "An **Option** is a temporally extended action defined as:\n",
        "```\n",
        "Option ω = (I_ω, π_ω, β_ω)\n",
        "\n",
        "where:\n",
        "- I_ω ⊆ S: Initiation set (where option can start)\n",
        "- π_ω: S × A → [0,1]: Option policy\n",
        "- β_ω: S → [0,1]: Termination function\n",
        "```\n",
        "\n",
        "### Semi-Markov Decision Process (SMDP)\n",
        "\n",
        "Instead of choosing action at each step, choose option, execute until termination.\n",
        "\n",
        "**Option-Value Functions:**\n",
        "- Q(s, ω) = Expected return from executing option ω in state s\n",
        "- Intra-option learning: Can update Q while executing option\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.1: Implement Handcrafted Options\n",
        "class Option:\n",
        "    \"\"\"\n",
        "    A handcrafted option for navigation tasks.\n",
        "    Each option represents a skill like \"move towards goal\" or \"explore area\".\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, name, initiation_set, policy_func, termination_func):\n",
        "        self.name = name\n",
        "        self.initiation_set = initiation_set\n",
        "        self.policy_func = policy_func\n",
        "        self.termination_func = termination_func\n",
        "        \n",
        "    def can_initiate(self, state):\n",
        "        \"\"\"Check if option can be initiated in given state\"\"\"\n",
        "        return state in self.initiation_set\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        \"\"\"Get action from option policy\"\"\"\n",
        "        return self.policy_func(state)\n",
        "    \n",
        "    def should_terminate(self, state):\n",
        "        \"\"\"Check if option should terminate\"\"\"\n",
        "        return self.termination_func(state)\n",
        "\n",
        "class NavigateToGoalOption(Option):\n",
        "    \"\"\"Option that navigates towards a specific goal location\"\"\"\n",
        "    \n",
        "    def __init__(self, goal_location, env):\n",
        "        self.goal = goal_location\n",
        "        self.env = env\n",
        "        \n",
        "        # Can initiate from any state\n",
        "        initiation_set = set()\n",
        "        for x in range(env.width):\n",
        "            for y in range(env.height):\n",
        "                initiation_set.add((x, y))\n",
        "        \n",
        "        super().__init__(\n",
        "            name=f\"NavigateTo{goal_location}\",\n",
        "            initiation_set=initiation_set,\n",
        "            policy_func=self._navigate_policy,\n",
        "            termination_func=self._termination_condition\n",
        "        )\n",
        "    \n",
        "    def _navigate_policy(self, state):\n",
        "        \"\"\"Policy: move towards goal using Manhattan distance\"\"\"\n",
        "        current_x, current_y = state\n",
        "        goal_x, goal_y = self.goal\n",
        "        \n",
        "        # Calculate direction to goal\n",
        "        dx = goal_x - current_x\n",
        "        dy = goal_y - current_y\n",
        "        \n",
        "        # Choose action that moves towards goal\n",
        "        if abs(dx) > abs(dy):\n",
        "            return 3 if dx > 0 else 2  # right or left\n",
        "        else:\n",
        "            return 0 if dy > 0 else 1  # up or down\n",
        "    \n",
        "    def _termination_condition(self, state):\n",
        "        \"\"\"Terminate when close to goal\"\"\"\n",
        "        distance = self.env.get_distance_to_goal(state)\n",
        "        return distance <= 1\n",
        "\n",
        "class ExploreOption(Option):\n",
        "    \"\"\"Option that explores the environment randomly\"\"\"\n",
        "    \n",
        "    def __init__(self, env, exploration_steps=5):\n",
        "        self.env = env\n",
        "        self.exploration_steps = exploration_steps\n",
        "        self.steps_taken = 0\n",
        "        \n",
        "        # Can initiate from any state\n",
        "        initiation_set = set()\n",
        "        for x in range(env.width):\n",
        "            for y in range(env.height):\n",
        "                initiation_set.add((x, y))\n",
        "        \n",
        "        super().__init__(\n",
        "            name=\"Explore\",\n",
        "            initiation_set=initiation_set,\n",
        "            policy_func=self._explore_policy,\n",
        "            termination_func=self._termination_condition\n",
        "        )\n",
        "    \n",
        "    def _explore_policy(self, state):\n",
        "        \"\"\"Policy: random exploration\"\"\"\n",
        "        self.steps_taken += 1\n",
        "        return np.random.randint(4)  # Random atomic action\n",
        "    \n",
        "    def _termination_condition(self, state):\n",
        "        \"\"\"Terminate after exploration_steps\"\"\"\n",
        "        if self.steps_taken >= self.exploration_steps:\n",
        "            self.steps_taken = 0  # Reset for next use\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Test handcrafted options\n",
        "env = GridWorld()\n",
        "goal_option = NavigateToGoalOption((9, 9), env)\n",
        "explore_option = ExploreOption(env, exploration_steps=3)\n",
        "\n",
        "print(\"Handcrafted Options Created:\")\n",
        "print(f\"1. {goal_option.name}\")\n",
        "print(f\"2. {explore_option.name}\")\n",
        "\n",
        "# Test goal navigation option\n",
        "state = env.reset()\n",
        "print(f\"\\nTesting {goal_option.name}:\")\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "for step in range(10):\n",
        "    if goal_option.can_initiate(state):\n",
        "        action = goal_option.get_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        print(f\"Step {step+1}: Action {env.action_names[action]} -> State {next_state}\")\n",
        "        \n",
        "        if goal_option.should_terminate(next_state):\n",
        "            print(\"Option terminated!\")\n",
        "            break\n",
        "        state = next_state\n",
        "    else:\n",
        "        print(\"Cannot initiate option from current state\")\n",
        "        break\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
