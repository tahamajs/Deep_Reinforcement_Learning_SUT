{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW12: Hierarchical Reinforcement Learning\n",
        "\n",
        "**Course:** Deep Reinforcement Learning  \n",
        "**Assignment:** Homework 12 - Hierarchical RL  \n",
        "**Date:** 2024\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "Hierarchical Reinforcement Learning (HRL) structures policies across multiple levels of abstraction, enabling agents to solve complex, long-horizon tasks by decomposing them into simpler subtasks. This assignment explores temporal abstraction, options framework, feudal architectures, and goal-conditioned policies.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "1. **Temporal Abstraction**: Understand multi-scale decision making\n",
        "2. **Options Framework**: Master semi-Markov decision processes\n",
        "3. **Feudal Hierarchies**: Learn manager-worker architectures\n",
        "4. **Goal-Conditioned RL**: Train policies with diverse goals\n",
        "5. **Skill Discovery**: Learn reusable primitives automatically\n",
        "6. **Credit Assignment**: Address challenges across temporal scales\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction to Hierarchical RL](#introduction)\n",
        "2. [Options Framework](#options-framework)\n",
        "3. [Feudal Hierarchies](#feudal-hierarchies)\n",
        "4. [Goal-Conditioned RL](#goal-conditioned-rl)\n",
        "5. [Skill Discovery](#skill-discovery)\n",
        "6. [HAM Framework](#ham-framework)\n",
        "7. [Evaluation and Comparison](#evaluation)\n",
        "8. [Conclusion](#conclusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, deque\n",
        "import gym\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Hierarchical RL\n",
        "\n",
        "### Motivation for Hierarchy\n",
        "\n",
        "**Challenges in Flat RL:**\n",
        "- **Long Horizons**: Credit assignment difficult over 1000+ steps\n",
        "- **Sparse Rewards**: Random exploration ineffective\n",
        "- **Complex Tasks**: Atomic actions insufficient\n",
        "- **Transfer**: Hard to reuse learned behaviors\n",
        "\n",
        "**Benefits of Hierarchy:**\n",
        "- Temporal abstraction (plan at multiple scales)\n",
        "- Reusable skills/subpolicies\n",
        "- Exploration structure\n",
        "- Transfer learning\n",
        "- Compositional generalization\n",
        "\n",
        "### Human Example:\n",
        "```\n",
        "Task: Make dinner\n",
        "├─ Shop for ingredients\n",
        "│  ├─ Drive to store\n",
        "│  ├─ Find items\n",
        "│  └─ Checkout\n",
        "├─ Prepare food\n",
        "│  ├─ Chop vegetables\n",
        "│  ├─ Cook proteins\n",
        "│  └─ Mix ingredients\n",
        "└─ Serve meal\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1.1: Create a Simple GridWorld Environment for HRL\n",
        "class GridWorld:\n",
        "    \"\"\"\n",
        "    A simple grid world environment for testing hierarchical RL algorithms.\n",
        "    The agent must navigate from start to goal, potentially using hierarchical actions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, width=10, height=10, start=(0, 0), goal=(9, 9)):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.state = start\n",
        "        \n",
        "        # Define atomic actions: up, down, left, right\n",
        "        self.atomic_actions = [(0, 1), (0, -1), (-1, 0), (1, 0)]\n",
        "        self.action_names = ['up', 'down', 'left', 'right']\n",
        "        \n",
        "        # Define hierarchical actions (options)\n",
        "        self.options = {}\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to initial state\"\"\"\n",
        "        self.state = self.start\n",
        "        return self.state\n",
        "        \n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        if isinstance(action, int):\n",
        "            # Atomic action\n",
        "            dx, dy = self.atomic_actions[action]\n",
        "            new_x = max(0, min(self.width-1, self.state[0] + dx))\n",
        "            new_y = max(0, min(self.height-1, self.state[1] + dy))\n",
        "            self.state = (new_x, new_y)\n",
        "        else:\n",
        "            # Hierarchical action (option)\n",
        "            self.state = action\n",
        "            \n",
        "        # Calculate reward\n",
        "        reward = 1.0 if self.state == self.goal else -0.01\n",
        "        \n",
        "        # Check if done\n",
        "        done = self.state == self.goal\n",
        "        \n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "    def get_distance_to_goal(self, state):\n",
        "        \"\"\"Calculate Manhattan distance to goal\"\"\"\n",
        "        return abs(state[0] - self.goal[0]) + abs(state[1] - self.goal[1])\n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Render the current state\"\"\"\n",
        "        grid = np.zeros((self.height, self.width))\n",
        "        grid[self.start[1], self.start[0]] = 1  # Start\n",
        "        grid[self.goal[1], self.goal[0]] = 2     # Goal\n",
        "        grid[self.state[1], self.state[0]] = 3   # Current position\n",
        "        \n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(grid, cmap='viridis')\n",
        "        plt.title(f\"GridWorld - Current: {self.state}, Goal: {self.goal}\")\n",
        "        plt.show()\n",
        "\n",
        "# Test the environment\n",
        "env = GridWorld()\n",
        "print(\"GridWorld Environment Created!\")\n",
        "print(f\"Start: {env.start}, Goal: {env.goal}\")\n",
        "print(f\"Atomic actions: {env.action_names}\")\n",
        "\n",
        "# Test a few steps\n",
        "state = env.reset()\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "for i in range(5):\n",
        "    action = np.random.randint(4)  # Random atomic action\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    print(f\"Step {i+1}: Action {env.action_names[action]} -> State {next_state}, Reward {reward:.2f}\")\n",
        "    if done:\n",
        "        print(\"Goal reached!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Options Framework\n",
        "\n",
        "### Formal Definition\n",
        "\n",
        "An **Option** is a temporally extended action defined as:\n",
        "```\n",
        "Option ω = (I_ω, π_ω, β_ω)\n",
        "\n",
        "where:\n",
        "- I_ω ⊆ S: Initiation set (where option can start)\n",
        "- π_ω: S × A → [0,1]: Option policy\n",
        "- β_ω: S → [0,1]: Termination function\n",
        "```\n",
        "\n",
        "### Semi-Markov Decision Process (SMDP)\n",
        "\n",
        "Instead of choosing action at each step, choose option, execute until termination.\n",
        "\n",
        "**Option-Value Functions:**\n",
        "- Q(s, ω) = Expected return from executing option ω in state s\n",
        "- Intra-option learning: Can update Q while executing option\n",
        "gi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.1: Implement Handcrafted Options\n",
        "class Option:\n",
        "    \"\"\"\n",
        "    A handcrafted option for navigation tasks.\n",
        "    Each option represents a skill like \"move towards goal\" or \"explore area\".\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, name, initiation_set, policy_func, termination_func):\n",
        "        self.name = name\n",
        "        self.initiation_set = initiation_set\n",
        "        self.policy_func = policy_func\n",
        "        self.termination_func = termination_func\n",
        "        \n",
        "    def can_initiate(self, state):\n",
        "        \"\"\"Check if option can be initiated in given state\"\"\"\n",
        "        return state in self.initiation_set\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        \"\"\"Get action from option policy\"\"\"\n",
        "        return self.policy_func(state)\n",
        "    \n",
        "    def should_terminate(self, state):\n",
        "        \"\"\"Check if option should terminate\"\"\"\n",
        "        return self.termination_func(state)\n",
        "\n",
        "class NavigateToGoalOption(Option):\n",
        "    \"\"\"Option that navigates towards a specific goal location\"\"\"\n",
        "    \n",
        "    def __init__(self, goal_location, env):\n",
        "        self.goal = goal_location\n",
        "        self.env = env\n",
        "        \n",
        "        # Can initiate from any state\n",
        "        initiation_set = set()\n",
        "        for x in range(env.width):\n",
        "            for y in range(env.height):\n",
        "                initiation_set.add((x, y))\n",
        "        \n",
        "        super().__init__(\n",
        "            name=f\"NavigateTo{goal_location}\",\n",
        "            initiation_set=initiation_set,\n",
        "            policy_func=self._navigate_policy,\n",
        "            termination_func=self._termination_condition\n",
        "        )\n",
        "    \n",
        "    def _navigate_policy(self, state):\n",
        "        \"\"\"Policy: move towards goal using Manhattan distance\"\"\"\n",
        "        current_x, current_y = state\n",
        "        goal_x, goal_y = self.goal\n",
        "        \n",
        "        # Calculate direction to goal\n",
        "        dx = goal_x - current_x\n",
        "        dy = goal_y - current_y\n",
        "        \n",
        "        # Choose action that moves towards goal\n",
        "        if abs(dx) > abs(dy):\n",
        "            return 3 if dx > 0 else 2  # right or left\n",
        "        else:\n",
        "            return 0 if dy > 0 else 1  # up or down\n",
        "    \n",
        "    def _termination_condition(self, state):\n",
        "        \"\"\"Terminate when close to goal\"\"\"\n",
        "        distance = self.env.get_distance_to_goal(state)\n",
        "        return distance <= 1\n",
        "\n",
        "class ExploreOption(Option):\n",
        "    \"\"\"Option that explores the environment randomly\"\"\"\n",
        "    \n",
        "    def __init__(self, env, exploration_steps=5):\n",
        "        self.env = env\n",
        "        self.exploration_steps = exploration_steps\n",
        "        self.steps_taken = 0\n",
        "        \n",
        "        # Can initiate from any state\n",
        "        initiation_set = set()\n",
        "        for x in range(env.width):\n",
        "            for y in range(env.height):\n",
        "                initiation_set.add((x, y))\n",
        "        \n",
        "        super().__init__(\n",
        "            name=\"Explore\",\n",
        "            initiation_set=initiation_set,\n",
        "            policy_func=self._explore_policy,\n",
        "            termination_func=self._termination_condition\n",
        "        )\n",
        "    \n",
        "    def _explore_policy(self, state):\n",
        "        \"\"\"Policy: random exploration\"\"\"\n",
        "        self.steps_taken += 1\n",
        "        return np.random.randint(4)  # Random atomic action\n",
        "    \n",
        "    def _termination_condition(self, state):\n",
        "        \"\"\"Terminate after exploration_steps\"\"\"\n",
        "        if self.steps_taken >= self.exploration_steps:\n",
        "            self.steps_taken = 0  # Reset for next use\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Test handcrafted options\n",
        "env = GridWorld()\n",
        "goal_option = NavigateToGoalOption((9, 9), env)\n",
        "explore_option = ExploreOption(env, exploration_steps=3)\n",
        "\n",
        "print(\"Handcrafted Options Created:\")\n",
        "print(f\"1. {goal_option.name}\")\n",
        "print(f\"2. {explore_option.name}\")\n",
        "\n",
        "# Test goal navigation option\n",
        "state = env.reset()\n",
        "print(f\"\\nTesting {goal_option.name}:\")\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "for step in range(10):\n",
        "    if goal_option.can_initiate(state):\n",
        "        action = goal_option.get_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        print(f\"Step {step+1}: Action {env.action_names[action]} -> State {next_state}\")\n",
        "        \n",
        "        if goal_option.should_terminate(next_state):\n",
        "            print(\"Option terminated!\")\n",
        "            break\n",
        "        state = next_state\n",
        "    else:\n",
        "        print(\"Cannot initiate option from current state\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.2: Implement Option-Critic Architecture\n",
        "class OptionCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    Option-Critic architecture for learning options automatically.\n",
        "    Learns option policies, termination functions, and option-value functions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, num_options, action_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.num_options = num_options\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Shared representation\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Option policies\n",
        "        self.option_policies = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, action_dim)\n",
        "            ) for _ in range(num_options)\n",
        "        ])\n",
        "        \n",
        "        # Termination functions\n",
        "        self.terminations = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_options),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        # Q-value over options\n",
        "        self.q_omega = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_options)\n",
        "        )\n",
        "        \n",
        "        # Intra-option Q-values\n",
        "        self.q_intra = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, action_dim)\n",
        "            ) for _ in range(num_options)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, state, current_option=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the network\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            current_option: Current active option (if any)\n",
        "        \n",
        "        Returns:\n",
        "            If current_option is provided:\n",
        "                - action_logits: Action probabilities for current option\n",
        "                - beta: Termination probability for current option\n",
        "            Else:\n",
        "                - q_omega: Q-values for all options\n",
        "        \"\"\"\n",
        "        features = self.encoder(state)\n",
        "        \n",
        "        if current_option is not None:\n",
        "            # Get action from current option\n",
        "            action_logits = self.option_policies[current_option](features)\n",
        "            \n",
        "            # Termination probability\n",
        "            beta = self.terminations(features)[:, current_option]\n",
        "            \n",
        "            return action_logits, beta\n",
        "        else:\n",
        "            # Select option\n",
        "            q_omega = self.q_omega(features)\n",
        "            return q_omega\n",
        "    \n",
        "    def get_intra_option_q(self, state, option):\n",
        "        \"\"\"Get intra-option Q-values\"\"\"\n",
        "        features = self.encoder(state)\n",
        "        return self.q_intra[option](features)\n",
        "    \n",
        "    def select_option(self, state, epsilon=0.1):\n",
        "        \"\"\"Select option using epsilon-greedy policy\"\"\"\n",
        "        with torch.no_grad():\n",
        "            q_values = self.forward(state)\n",
        "            if np.random.random() < epsilon:\n",
        "                return np.random.randint(self.num_options)\n",
        "            else:\n",
        "                return q_values.argmax().item()\n",
        "    \n",
        "    def get_action(self, state, option):\n",
        "        \"\"\"Get action from option policy\"\"\"\n",
        "        with torch.no_grad():\n",
        "            action_logits, _ = self.forward(state, option)\n",
        "            action_probs = F.softmax(action_logits, dim=-1)\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "            return action\n",
        "\n",
        "class OptionCriticAgent:\n",
        "    \"\"\"Agent that uses Option-Critic architecture\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, num_options, action_dim, lr=1e-3):\n",
        "        self.model = OptionCritic(state_dim, num_options, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        \n",
        "        self.num_options = num_options\n",
        "        self.current_option = None\n",
        "        self.option_steps = 0\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset agent state\"\"\"\n",
        "        self.current_option = None\n",
        "        self.option_steps = 0\n",
        "    \n",
        "    def act(self, state, epsilon=0.1):\n",
        "        \"\"\"Select action using current option or select new option\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        \n",
        "        # Check if we need to select a new option\n",
        "        if self.current_option is None:\n",
        "            self.current_option = self.model.select_option(state_tensor, epsilon)\n",
        "            self.option_steps = 0\n",
        "        \n",
        "        # Get action from current option\n",
        "        action = self.model.get_action(state_tensor, self.current_option)\n",
        "        \n",
        "        # Check termination\n",
        "        with torch.no_grad():\n",
        "            _, beta = self.model.forward(state_tensor, self.current_option)\n",
        "            if np.random.random() < beta.item() or self.option_steps > 20:\n",
        "                self.current_option = None  # Terminate option\n",
        "        \n",
        "        self.option_steps += 1\n",
        "        return action\n",
        "    \n",
        "    def update(self, batch):\n",
        "        \"\"\"Update the model using a batch of experiences\"\"\"\n",
        "        states, actions, rewards, next_states, options, dones = batch\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        options = torch.LongTensor(options)\n",
        "        dones = torch.BoolTensor(dones)\n",
        "        \n",
        "        # Compute losses\n",
        "        q_loss = self._compute_q_loss(states, actions, rewards, next_states, options, dones)\n",
        "        policy_loss = self._compute_policy_loss(states, actions, options)\n",
        "        termination_loss = self._compute_termination_loss(states, next_states, options)\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = q_loss + policy_loss + termination_loss\n",
        "        \n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return {\n",
        "            'q_loss': q_loss.item(),\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'termination_loss': termination_loss.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "    \n",
        "    def _compute_q_loss(self, states, actions, rewards, next_states, options, dones):\n",
        "        \"\"\"Compute Q-learning loss\"\"\"\n",
        "        # Intra-option Q-values\n",
        "        q_values = []\n",
        "        for i, option in enumerate(options):\n",
        "            q_val = self.model.get_intra_option_q(states[i:i+1], option.item())\n",
        "            q_values.append(q_val[0, actions[i]])\n",
        "        q_values = torch.stack(q_values)\n",
        "        \n",
        "        # Target Q-values\n",
        "        with torch.no_grad():\n",
        "            next_q_values = []\n",
        "            for i, option in enumerate(options):\n",
        "                if dones[i]:\n",
        "                    next_q_val = 0\n",
        "                else:\n",
        "                    next_q_val = self.model.get_intra_option_q(next_states[i:i+1], option.item()).max()\n",
        "                next_q_values.append(next_q_val)\n",
        "            next_q_values = torch.stack(next_q_values)\n",
        "            \n",
        "            targets = rewards + 0.99 * next_q_values\n",
        "        \n",
        "        return F.mse_loss(q_values, targets)\n",
        "    \n",
        "    def _compute_policy_loss(self, states, actions, options):\n",
        "        \"\"\"Compute policy gradient loss\"\"\"\n",
        "        policy_loss = 0\n",
        "        for i, option in enumerate(options):\n",
        "            action_logits, _ = self.model.forward(states[i:i+1], option.item())\n",
        "            log_probs = F.log_softmax(action_logits, dim=-1)\n",
        "            policy_loss -= log_probs[0, actions[i]]\n",
        "        \n",
        "        return policy_loss / len(options)\n",
        "    \n",
        "    def _compute_termination_loss(self, states, next_states, options):\n",
        "        \"\"\"Compute termination function loss\"\"\"\n",
        "        termination_loss = 0\n",
        "        for i, option in enumerate(options):\n",
        "            _, beta = self.model.forward(states[i:i+1], option.item())\n",
        "            # Encourage termination when option is no longer useful\n",
        "            termination_loss += beta[0]\n",
        "        \n",
        "        return termination_loss / len(options)\n",
        "\n",
        "# Test Option-Critic\n",
        "print(\"Option-Critic Architecture Created!\")\n",
        "print(\"Testing with GridWorld environment...\")\n",
        "\n",
        "# Create agent\n",
        "state_dim = 2  # (x, y) coordinates\n",
        "num_options = 3\n",
        "action_dim = 4  # up, down, left, right\n",
        "\n",
        "agent = OptionCriticAgent(state_dim, num_options, action_dim)\n",
        "env = GridWorld()\n",
        "\n",
        "# Test agent\n",
        "state = env.reset()\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "for step in range(10):\n",
        "    action = agent.act(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    print(f\"Step {step+1}: Action {env.action_names[action]} -> State {next_state}, Reward {reward:.2f}\")\n",
        "    \n",
        "    if done:\n",
        "        print(\"Goal reached!\")\n",
        "        break\n",
        "    state = next_state\n",
        "\n",
        "agent.reset()\n",
        "print(\"Agent reset for next episode.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
