{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW12: Hierarchical Reinforcement Learning\n",
        "\n",
        "**Course:** Deep Reinforcement Learning  \n",
        "**Assignment:** Homework 12 - Hierarchical RL  \n",
        "**Date:** 2024\n",
        "\n",
        "---\n",
        "\n",
        "## Overview\n",
        "\n",
        "Hierarchical Reinforcement Learning (HRL) structures policies across multiple levels of abstraction, enabling agents to solve complex, long-horizon tasks by decomposing them into simpler subtasks. This assignment explores temporal abstraction, options framework, feudal architectures, and goal-conditioned policies.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "1. **Temporal Abstraction**: Understand multi-scale decision making\n",
        "2. **Options Framework**: Master semi-Markov decision processes\n",
        "3. **Feudal Hierarchies**: Learn manager-worker architectures\n",
        "4. **Goal-Conditioned RL**: Train policies with diverse goals\n",
        "5. **Skill Discovery**: Learn reusable primitives automatically\n",
        "6. **Credit Assignment**: Address challenges across temporal scales\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction to Hierarchical RL](#introduction)\n",
        "2. [Options Framework](#options-framework)\n",
        "3. [Feudal Hierarchies](#feudal-hierarchies)\n",
        "4. [Goal-Conditioned RL](#goal-conditioned-rl)\n",
        "5. [Skill Discovery](#skill-discovery)\n",
        "6. [HAM Framework](#ham-framework)\n",
        "7. [Evaluation and Comparison](#evaluation)\n",
        "8. [Conclusion](#conclusion)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict, deque\n",
        "import gym\n",
        "import random\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction to Hierarchical RL\n",
        "\n",
        "### Motivation for Hierarchy\n",
        "\n",
        "**Challenges in Flat RL:**\n",
        "- **Long Horizons**: Credit assignment difficult over 1000+ steps\n",
        "- **Sparse Rewards**: Random exploration ineffective\n",
        "- **Complex Tasks**: Atomic actions insufficient\n",
        "- **Transfer**: Hard to reuse learned behaviors\n",
        "\n",
        "**Benefits of Hierarchy:**\n",
        "- Temporal abstraction (plan at multiple scales)\n",
        "- Reusable skills/subpolicies\n",
        "- Exploration structure\n",
        "- Transfer learning\n",
        "- Compositional generalization\n",
        "\n",
        "### Human Example:\n",
        "```\n",
        "Task: Make dinner\n",
        "├─ Shop for ingredients\n",
        "│  ├─ Drive to store\n",
        "│  ├─ Find items\n",
        "│  └─ Checkout\n",
        "├─ Prepare food\n",
        "│  ├─ Chop vegetables\n",
        "│  ├─ Cook proteins\n",
        "│  └─ Mix ingredients\n",
        "└─ Serve meal\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 1.1: Create a Simple GridWorld Environment for HRL\n",
        "class GridWorld:\n",
        "    \"\"\"\n",
        "    A simple grid world environment for testing hierarchical RL algorithms.\n",
        "    The agent must navigate from start to goal, potentially using hierarchical actions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, width=10, height=10, start=(0, 0), goal=(9, 9)):\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.start = start\n",
        "        self.goal = goal\n",
        "        self.state = start\n",
        "        \n",
        "        # Define atomic actions: up, down, left, right\n",
        "        self.atomic_actions = [(0, 1), (0, -1), (-1, 0), (1, 0)]\n",
        "        self.action_names = ['up', 'down', 'left', 'right']\n",
        "        \n",
        "        # Define hierarchical actions (options)\n",
        "        self.options = {}\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset environment to initial state\"\"\"\n",
        "        self.state = self.start\n",
        "        return self.state\n",
        "        \n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment\"\"\"\n",
        "        if isinstance(action, int):\n",
        "            # Atomic action\n",
        "            dx, dy = self.atomic_actions[action]\n",
        "            new_x = max(0, min(self.width-1, self.state[0] + dx))\n",
        "            new_y = max(0, min(self.height-1, self.state[1] + dy))\n",
        "            self.state = (new_x, new_y)\n",
        "        else:\n",
        "            # Hierarchical action (option)\n",
        "            self.state = action\n",
        "            \n",
        "        # Calculate reward\n",
        "        reward = 1.0 if self.state == self.goal else -0.01\n",
        "        \n",
        "        # Check if done\n",
        "        done = self.state == self.goal\n",
        "        \n",
        "        return self.state, reward, done, {}\n",
        "    \n",
        "    def get_distance_to_goal(self, state):\n",
        "        \"\"\"Calculate Manhattan distance to goal\"\"\"\n",
        "        return abs(state[0] - self.goal[0]) + abs(state[1] - self.goal[1])\n",
        "    \n",
        "    def render(self):\n",
        "        \"\"\"Render the current state\"\"\"\n",
        "        grid = np.zeros((self.height, self.width))\n",
        "        grid[self.start[1], self.start[0]] = 1  # Start\n",
        "        grid[self.goal[1], self.goal[0]] = 2     # Goal\n",
        "        grid[self.state[1], self.state[0]] = 3   # Current position\n",
        "        \n",
        "        plt.figure(figsize=(8, 8))\n",
        "        plt.imshow(grid, cmap='viridis')\n",
        "        plt.title(f\"GridWorld - Current: {self.state}, Goal: {self.goal}\")\n",
        "        plt.show()\n",
        "\n",
        "# Test the environment\n",
        "env = GridWorld()\n",
        "print(\"GridWorld Environment Created!\")\n",
        "print(f\"Start: {env.start}, Goal: {env.goal}\")\n",
        "print(f\"Atomic actions: {env.action_names}\")\n",
        "\n",
        "# Test a few steps\n",
        "state = env.reset()\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "for i in range(5):\n",
        "    action = np.random.randint(4)  # Random atomic action\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    print(f\"Step {i+1}: Action {env.action_names[action]} -> State {next_state}, Reward {reward:.2f}\")\n",
        "    if done:\n",
        "        print(\"Goal reached!\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Options Framework\n",
        "\n",
        "### Formal Definition\n",
        "\n",
        "An **Option** is a temporally extended action defined as:\n",
        "```\n",
        "Option ω = (I_ω, π_ω, β_ω)\n",
        "\n",
        "where:\n",
        "- I_ω ⊆ S: Initiation set (where option can start)\n",
        "- π_ω: S × A → [0,1]: Option policy\n",
        "- β_ω: S → [0,1]: Termination function\n",
        "```\n",
        "\n",
        "### Semi-Markov Decision Process (SMDP)\n",
        "\n",
        "Instead of choosing action at each step, choose option, execute until termination.\n",
        "\n",
        "**Option-Value Functions:**\n",
        "- Q(s, ω) = Expected return from executing option ω in state s\n",
        "- Intra-option learning: Can update Q while executing option\n",
        "gi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.1: Implement Handcrafted Options\n",
        "class Option:\n",
        "    \"\"\"\n",
        "    A handcrafted option for navigation tasks.\n",
        "    Each option represents a skill like \"move towards goal\" or \"explore area\".\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, name, initiation_set, policy_func, termination_func):\n",
        "        self.name = name\n",
        "        self.initiation_set = initiation_set\n",
        "        self.policy_func = policy_func\n",
        "        self.termination_func = termination_func\n",
        "        \n",
        "    def can_initiate(self, state):\n",
        "        \"\"\"Check if option can be initiated in given state\"\"\"\n",
        "        return state in self.initiation_set\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        \"\"\"Get action from option policy\"\"\"\n",
        "        return self.policy_func(state)\n",
        "    \n",
        "    def should_terminate(self, state):\n",
        "        \"\"\"Check if option should terminate\"\"\"\n",
        "        return self.termination_func(state)\n",
        "\n",
        "class NavigateToGoalOption(Option):\n",
        "    \"\"\"Option that navigates towards a specific goal location\"\"\"\n",
        "    \n",
        "    def __init__(self, goal_location, env):\n",
        "        self.goal = goal_location\n",
        "        self.env = env\n",
        "        \n",
        "        # Can initiate from any state\n",
        "        initiation_set = set()\n",
        "        for x in range(env.width):\n",
        "            for y in range(env.height):\n",
        "                initiation_set.add((x, y))\n",
        "        \n",
        "        super().__init__(\n",
        "            name=f\"NavigateTo{goal_location}\",\n",
        "            initiation_set=initiation_set,\n",
        "            policy_func=self._navigate_policy,\n",
        "            termination_func=self._termination_condition\n",
        "        )\n",
        "    \n",
        "    def _navigate_policy(self, state):\n",
        "        \"\"\"Policy: move towards goal using Manhattan distance\"\"\"\n",
        "        current_x, current_y = state\n",
        "        goal_x, goal_y = self.goal\n",
        "        \n",
        "        # Calculate direction to goal\n",
        "        dx = goal_x - current_x\n",
        "        dy = goal_y - current_y\n",
        "        \n",
        "        # Choose action that moves towards goal\n",
        "        if abs(dx) > abs(dy):\n",
        "            return 3 if dx > 0 else 2  # right or left\n",
        "        else:\n",
        "            return 0 if dy > 0 else 1  # up or down\n",
        "    \n",
        "    def _termination_condition(self, state):\n",
        "        \"\"\"Terminate when close to goal\"\"\"\n",
        "        distance = self.env.get_distance_to_goal(state)\n",
        "        return distance <= 1\n",
        "\n",
        "class ExploreOption(Option):\n",
        "    \"\"\"Option that explores the environment randomly\"\"\"\n",
        "    \n",
        "    def __init__(self, env, exploration_steps=5):\n",
        "        self.env = env\n",
        "        self.exploration_steps = exploration_steps\n",
        "        self.steps_taken = 0\n",
        "        \n",
        "        # Can initiate from any state\n",
        "        initiation_set = set()\n",
        "        for x in range(env.width):\n",
        "            for y in range(env.height):\n",
        "                initiation_set.add((x, y))\n",
        "        \n",
        "        super().__init__(\n",
        "            name=\"Explore\",\n",
        "            initiation_set=initiation_set,\n",
        "            policy_func=self._explore_policy,\n",
        "            termination_func=self._termination_condition\n",
        "        )\n",
        "    \n",
        "    def _explore_policy(self, state):\n",
        "        \"\"\"Policy: random exploration\"\"\"\n",
        "        self.steps_taken += 1\n",
        "        return np.random.randint(4)  # Random atomic action\n",
        "    \n",
        "    def _termination_condition(self, state):\n",
        "        \"\"\"Terminate after exploration_steps\"\"\"\n",
        "        if self.steps_taken >= self.exploration_steps:\n",
        "            self.steps_taken = 0  # Reset for next use\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "# Test handcrafted options\n",
        "env = GridWorld()\n",
        "goal_option = NavigateToGoalOption((9, 9), env)\n",
        "explore_option = ExploreOption(env, exploration_steps=3)\n",
        "\n",
        "print(\"Handcrafted Options Created:\")\n",
        "print(f\"1. {goal_option.name}\")\n",
        "print(f\"2. {explore_option.name}\")\n",
        "\n",
        "# Test goal navigation option\n",
        "state = env.reset()\n",
        "print(f\"\\nTesting {goal_option.name}:\")\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "for step in range(10):\n",
        "    if goal_option.can_initiate(state):\n",
        "        action = goal_option.get_action(state)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        print(f\"Step {step+1}: Action {env.action_names[action]} -> State {next_state}\")\n",
        "        \n",
        "        if goal_option.should_terminate(next_state):\n",
        "            print(\"Option terminated!\")\n",
        "            break\n",
        "        state = next_state\n",
        "    else:\n",
        "        print(\"Cannot initiate option from current state\")\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 2.2: Implement Option-Critic Architecture\n",
        "class OptionCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    Option-Critic architecture for learning options automatically.\n",
        "    Learns option policies, termination functions, and option-value functions.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, num_options, action_dim, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.num_options = num_options\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Shared representation\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Option policies\n",
        "        self.option_policies = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, action_dim)\n",
        "            ) for _ in range(num_options)\n",
        "        ])\n",
        "        \n",
        "        # Termination functions\n",
        "        self.terminations = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_options),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        # Q-value over options\n",
        "        self.q_omega = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_options)\n",
        "        )\n",
        "        \n",
        "        # Intra-option Q-values\n",
        "        self.q_intra = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, action_dim)\n",
        "            ) for _ in range(num_options)\n",
        "        ])\n",
        "    \n",
        "    def forward(self, state, current_option=None):\n",
        "        \"\"\"\n",
        "        Forward pass through the network\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            current_option: Current active option (if any)\n",
        "        \n",
        "        Returns:\n",
        "            If current_option is provided:\n",
        "                - action_logits: Action probabilities for current option\n",
        "                - beta: Termination probability for current option\n",
        "            Else:\n",
        "                - q_omega: Q-values for all options\n",
        "        \"\"\"\n",
        "        features = self.encoder(state)\n",
        "        \n",
        "        if current_option is not None:\n",
        "            # Get action from current option\n",
        "            action_logits = self.option_policies[current_option](features)\n",
        "            \n",
        "            # Termination probability\n",
        "            beta = self.terminations(features)[:, current_option]\n",
        "            \n",
        "            return action_logits, beta\n",
        "        else:\n",
        "            # Select option\n",
        "            q_omega = self.q_omega(features)\n",
        "            return q_omega\n",
        "    \n",
        "    def get_intra_option_q(self, state, option):\n",
        "        \"\"\"Get intra-option Q-values\"\"\"\n",
        "        features = self.encoder(state)\n",
        "        return self.q_intra[option](features)\n",
        "    \n",
        "    def select_option(self, state, epsilon=0.1):\n",
        "        \"\"\"Select option using epsilon-greedy policy\"\"\"\n",
        "        with torch.no_grad():\n",
        "            q_values = self.forward(state)\n",
        "            if np.random.random() < epsilon:\n",
        "                return np.random.randint(self.num_options)\n",
        "            else:\n",
        "                return q_values.argmax().item()\n",
        "    \n",
        "    def get_action(self, state, option):\n",
        "        \"\"\"Get action from option policy\"\"\"\n",
        "        with torch.no_grad():\n",
        "            action_logits, _ = self.forward(state, option)\n",
        "            action_probs = F.softmax(action_logits, dim=-1)\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "            return action\n",
        "\n",
        "class OptionCriticAgent:\n",
        "    \"\"\"Agent that uses Option-Critic architecture\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, num_options, action_dim, lr=1e-3):\n",
        "        self.model = OptionCritic(state_dim, num_options, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        \n",
        "        self.num_options = num_options\n",
        "        self.current_option = None\n",
        "        self.option_steps = 0\n",
        "        \n",
        "    def reset(self):\n",
        "        \"\"\"Reset agent state\"\"\"\n",
        "        self.current_option = None\n",
        "        self.option_steps = 0\n",
        "    \n",
        "    def act(self, state, epsilon=0.1):\n",
        "        \"\"\"Select action using current option or select new option\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        \n",
        "        # Check if we need to select a new option\n",
        "        if self.current_option is None:\n",
        "            self.current_option = self.model.select_option(state_tensor, epsilon)\n",
        "            self.option_steps = 0\n",
        "        \n",
        "        # Get action from current option\n",
        "        action = self.model.get_action(state_tensor, self.current_option)\n",
        "        \n",
        "        # Check termination\n",
        "        with torch.no_grad():\n",
        "            _, beta = self.model.forward(state_tensor, self.current_option)\n",
        "            if np.random.random() < beta.item() or self.option_steps > 20:\n",
        "                self.current_option = None  # Terminate option\n",
        "        \n",
        "        self.option_steps += 1\n",
        "        return action\n",
        "    \n",
        "    def update(self, batch):\n",
        "        \"\"\"Update the model using a batch of experiences\"\"\"\n",
        "        states, actions, rewards, next_states, options, dones = batch\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor(states)\n",
        "        actions = torch.LongTensor(actions)\n",
        "        rewards = torch.FloatTensor(rewards)\n",
        "        next_states = torch.FloatTensor(next_states)\n",
        "        options = torch.LongTensor(options)\n",
        "        dones = torch.BoolTensor(dones)\n",
        "        \n",
        "        # Compute losses\n",
        "        q_loss = self._compute_q_loss(states, actions, rewards, next_states, options, dones)\n",
        "        policy_loss = self._compute_policy_loss(states, actions, options)\n",
        "        termination_loss = self._compute_termination_loss(states, next_states, options)\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = q_loss + policy_loss + termination_loss\n",
        "        \n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return {\n",
        "            'q_loss': q_loss.item(),\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'termination_loss': termination_loss.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "    \n",
        "    def _compute_q_loss(self, states, actions, rewards, next_states, options, dones):\n",
        "        \"\"\"Compute Q-learning loss\"\"\"\n",
        "        # Intra-option Q-values\n",
        "        q_values = []\n",
        "        for i, option in enumerate(options):\n",
        "            q_val = self.model.get_intra_option_q(states[i:i+1], option.item())\n",
        "            q_values.append(q_val[0, actions[i]])\n",
        "        q_values = torch.stack(q_values)\n",
        "        \n",
        "        # Target Q-values\n",
        "        with torch.no_grad():\n",
        "            next_q_values = []\n",
        "            for i, option in enumerate(options):\n",
        "                if dones[i]:\n",
        "                    next_q_val = 0\n",
        "                else:\n",
        "                    next_q_val = self.model.get_intra_option_q(next_states[i:i+1], option.item()).max()\n",
        "                next_q_values.append(next_q_val)\n",
        "            next_q_values = torch.stack(next_q_values)\n",
        "            \n",
        "            targets = rewards + 0.99 * next_q_values\n",
        "        \n",
        "        return F.mse_loss(q_values, targets)\n",
        "    \n",
        "    def _compute_policy_loss(self, states, actions, options):\n",
        "        \"\"\"Compute policy gradient loss\"\"\"\n",
        "        policy_loss = 0\n",
        "        for i, option in enumerate(options):\n",
        "            action_logits, _ = self.model.forward(states[i:i+1], option.item())\n",
        "            log_probs = F.log_softmax(action_logits, dim=-1)\n",
        "            policy_loss -= log_probs[0, actions[i]]\n",
        "        \n",
        "        return policy_loss / len(options)\n",
        "    \n",
        "    def _compute_termination_loss(self, states, next_states, options):\n",
        "        \"\"\"Compute termination function loss\"\"\"\n",
        "        termination_loss = 0\n",
        "        for i, option in enumerate(options):\n",
        "            _, beta = self.model.forward(states[i:i+1], option.item())\n",
        "            # Encourage termination when option is no longer useful\n",
        "            termination_loss += beta[0]\n",
        "        \n",
        "        return termination_loss / len(options)\n",
        "\n",
        "# Test Option-Critic\n",
        "print(\"Option-Critic Architecture Created!\")\n",
        "print(\"Testing with GridWorld environment...\")\n",
        "\n",
        "# Create agent\n",
        "state_dim = 2  # (x, y) coordinates\n",
        "num_options = 3\n",
        "action_dim = 4  # up, down, left, right\n",
        "\n",
        "agent = OptionCriticAgent(state_dim, num_options, action_dim)\n",
        "env = GridWorld()\n",
        "\n",
        "# Test agent\n",
        "state = env.reset()\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "for step in range(10):\n",
        "    action = agent.act(state)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    print(f\"Step {step+1}: Action {env.action_names[action]} -> State {next_state}, Reward {reward:.2f}\")\n",
        "    \n",
        "    if done:\n",
        "        print(\"Goal reached!\")\n",
        "        break\n",
        "    state = next_state\n",
        "\n",
        "agent.reset()\n",
        "print(\"Agent reset for next episode.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feudal Hierarchies\n",
        "\n",
        "### Key Idea: Manager-Worker Architecture\n",
        "\n",
        "**FeudalNet (Feudal Networks):**\n",
        "- **Manager**: Sets goals at high level\n",
        "- **Worker**: Achieves goals at low level\n",
        "- **Communication**: Manager provides goals to worker\n",
        "\n",
        "### Architecture Components:\n",
        "\n",
        "1. **Perception Module**: Shared state representation\n",
        "2. **Manager**: LSTM that sets goals every c timesteps\n",
        "3. **Worker**: LSTM that receives goals and produces actions\n",
        "4. **Reward Structure**: \n",
        "   - Manager reward: Cosine similarity between goals and state transitions\n",
        "   - Worker reward: Intrinsic + extrinsic rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 3.1: Implement FeudalNet Architecture\n",
        "class FeudalNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Feudal Networks implementation with Manager-Worker hierarchy.\n",
        "    Manager sets goals, Worker achieves them.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, goal_dim=8, c=10, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.goal_dim = goal_dim\n",
        "        self.c = c  # Manager horizon\n",
        "        \n",
        "        # Perception module (shared)\n",
        "        self.perception = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Manager (sets goals)\n",
        "        self.manager = nn.LSTM(hidden_dim, goal_dim, batch_first=True)\n",
        "        \n",
        "        # Worker (achieves goals)\n",
        "        self.worker = nn.LSTM(hidden_dim + goal_dim, hidden_dim, batch_first=True)\n",
        "        self.worker_policy = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "        \n",
        "        # Initialize hidden states\n",
        "        self.manager_hidden = None\n",
        "        self.worker_hidden = None\n",
        "        \n",
        "    def forward(self, state, t):\n",
        "        \"\"\"\n",
        "        Forward pass through FeudalNet\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            t: Current timestep\n",
        "        \n",
        "        Returns:\n",
        "            action_logits: Action probabilities\n",
        "            goal: Current goal from manager\n",
        "            manager_hidden: Updated manager hidden state\n",
        "            worker_hidden: Updated worker hidden state\n",
        "        \"\"\"\n",
        "        batch_size = state.shape[0]\n",
        "        \n",
        "        # Shared perception\n",
        "        z = self.perception(state)\n",
        "        \n",
        "        # Manager operates every c timesteps\n",
        "        if t % self.c == 0:\n",
        "            # Manager sets goal\n",
        "            if self.manager_hidden is None:\n",
        "                self.manager_hidden = self._init_hidden(batch_size, self.manager)\n",
        "            \n",
        "            z_reshaped = z.unsqueeze(1)  # Add sequence dimension\n",
        "            goal_output, self.manager_hidden = self.manager(z_reshaped, self.manager_hidden)\n",
        "            goal = F.normalize(goal_output.squeeze(1), dim=-1)  # Normalize goal\n",
        "        else:\n",
        "            # Use previous goal\n",
        "            goal = getattr(self, 'current_goal', torch.zeros(batch_size, self.goal_dim))\n",
        "        \n",
        "        self.current_goal = goal\n",
        "        \n",
        "        # Worker receives goal and state\n",
        "        if self.worker_hidden is None:\n",
        "            self.worker_hidden = self._init_hidden(batch_size, self.worker)\n",
        "        \n",
        "        w_input = torch.cat([z, goal], dim=-1)\n",
        "        w_input_reshaped = w_input.unsqueeze(1)  # Add sequence dimension\n",
        "        w_output, self.worker_hidden = self.worker(w_input_reshaped, self.worker_hidden)\n",
        "        \n",
        "        # Worker action\n",
        "        action_logits = self.worker_policy(w_output.squeeze(1))\n",
        "        \n",
        "        return action_logits, goal, self.manager_hidden, self.worker_hidden\n",
        "    \n",
        "    def _init_hidden(self, batch_size, lstm):\n",
        "        \"\"\"Initialize LSTM hidden states\"\"\"\n",
        "        h0 = torch.zeros(1, batch_size, lstm.hidden_size)\n",
        "        c0 = torch.zeros(1, batch_size, lstm.hidden_size)\n",
        "        return (h0, c0)\n",
        "    \n",
        "    def reset_hidden(self):\n",
        "        \"\"\"Reset hidden states for new episode\"\"\"\n",
        "        self.manager_hidden = None\n",
        "        self.worker_hidden = None\n",
        "        self.current_goal = None\n",
        "\n",
        "class FeudalAgent:\n",
        "    \"\"\"Agent using FeudalNet architecture\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, goal_dim=8, c=10, lr=1e-3):\n",
        "        self.model = FeudalNet(state_dim, action_dim, goal_dim, c)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.goal_dim = goal_dim\n",
        "        self.c = c\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.buffer = []\n",
        "        self.max_buffer_size = 10000\n",
        "        \n",
        "    def act(self, state, t, epsilon=0.1):\n",
        "        \"\"\"Select action using FeudalNet\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            action_logits, goal, _, _ = self.model(state_tensor, t)\n",
        "            \n",
        "            if np.random.random() < epsilon:\n",
        "                action = np.random.randint(self.action_dim)\n",
        "            else:\n",
        "                action_probs = F.softmax(action_logits, dim=-1)\n",
        "                action = torch.multinomial(action_probs, 1).item()\n",
        "        \n",
        "        return action, goal.squeeze(0).numpy()\n",
        "    \n",
        "    def store_experience(self, state, action, reward, next_state, goal, t):\n",
        "        \"\"\"Store experience in buffer\"\"\"\n",
        "        experience = {\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state,\n",
        "            'goal': goal,\n",
        "            't': t\n",
        "        }\n",
        "        \n",
        "        self.buffer.append(experience)\n",
        "        if len(self.buffer) > self.max_buffer_size:\n",
        "            self.buffer.pop(0)\n",
        "    \n",
        "    def compute_feudal_rewards(self, trajectory):\n",
        "        \"\"\"Compute manager and worker rewards for feudal training\"\"\"\n",
        "        manager_rewards = []\n",
        "        worker_rewards = []\n",
        "        \n",
        "        for i in range(len(trajectory) - self.c):\n",
        "            # Manager reward: cosine similarity between goal and state transition\n",
        "            state_i = trajectory[i]['state']\n",
        "            state_i_plus_c = trajectory[i + self.c]['state']\n",
        "            \n",
        "            # Compute state transition vector\n",
        "            transition = np.array(state_i_plus_c) - np.array(state_i)\n",
        "            \n",
        "            # Manager reward (cosine similarity)\n",
        "            goal = trajectory[i]['goal']\n",
        "            if np.linalg.norm(transition) > 0 and np.linalg.norm(goal) > 0:\n",
        "                cosine_sim = np.dot(transition, goal) / (np.linalg.norm(transition) * np.linalg.norm(goal))\n",
        "                manager_reward = cosine_sim\n",
        "            else:\n",
        "                manager_reward = 0\n",
        "            \n",
        "            manager_rewards.append(manager_reward)\n",
        "            \n",
        "            # Worker reward: intrinsic + extrinsic\n",
        "            intrinsic_reward = manager_reward  # Progress toward goal\n",
        "            extrinsic_reward = trajectory[i]['reward']\n",
        "            worker_reward = extrinsic_reward + 0.1 * intrinsic_reward\n",
        "            \n",
        "            worker_rewards.append(worker_reward)\n",
        "        \n",
        "        return manager_rewards, worker_rewards\n",
        "    \n",
        "    def update(self, batch_size=32):\n",
        "        \"\"\"Update the model using feudal rewards\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return {}\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        # Compute feudal rewards\n",
        "        manager_rewards, worker_rewards = self.compute_feudal_rewards(batch)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor([exp['state'] for exp in batch])\n",
        "        actions = torch.LongTensor([exp['action'] for exp in batch])\n",
        "        rewards = torch.FloatTensor(worker_rewards[:len(batch)])\n",
        "        \n",
        "        # Forward pass\n",
        "        action_logits, goals, _, _ = self.model(states, 0)\n",
        "        \n",
        "        # Compute losses\n",
        "        action_probs = F.softmax(action_logits, dim=-1)\n",
        "        log_probs = F.log_softmax(action_logits, dim=-1)\n",
        "        \n",
        "        # Policy loss (worker)\n",
        "        policy_loss = -(log_probs.gather(1, actions.unsqueeze(1)) * rewards.unsqueeze(1)).mean()\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = policy_loss\n",
        "        \n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return {\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "    \n",
        "    def reset(self):\n",
        "        \"\"\"Reset agent for new episode\"\"\"\n",
        "        self.model.reset_hidden()\n",
        "\n",
        "# Test FeudalNet\n",
        "print(\"FeudalNet Architecture Created!\")\n",
        "print(\"Testing with GridWorld environment...\")\n",
        "\n",
        "# Create agent\n",
        "state_dim = 2  # (x, y) coordinates\n",
        "action_dim = 4  # up, down, left, right\n",
        "goal_dim = 8\n",
        "c = 5  # Manager horizon\n",
        "\n",
        "agent = FeudalAgent(state_dim, action_dim, goal_dim, c)\n",
        "env = GridWorld()\n",
        "\n",
        "# Test agent\n",
        "state = env.reset()\n",
        "print(f\"Initial state: {state}\")\n",
        "\n",
        "for step in range(15):\n",
        "    action, goal = agent.act(state, step)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    \n",
        "    print(f\"Step {step+1}: Action {env.action_names[action]} -> State {next_state}, Reward {reward:.2f}\")\n",
        "    print(f\"  Goal: {goal[:3]}...\")  # Show first 3 dimensions\n",
        "    \n",
        "    # Store experience\n",
        "    agent.store_experience(state, action, reward, next_state, goal, step)\n",
        "    \n",
        "    if done:\n",
        "        print(\"Goal reached!\")\n",
        "        break\n",
        "    state = next_state\n",
        "\n",
        "# Update model\n",
        "losses = agent.update()\n",
        "print(f\"\\nTraining losses: {losses}\")\n",
        "\n",
        "agent.reset()\n",
        "print(\"Agent reset for next episode.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Goal-Conditioned RL\n",
        "\n",
        "### Key Idea: Train policy to reach any goal state\n",
        "\n",
        "**Universal Value Function Approximators (UVFA):**\n",
        "- Policy conditioned on current state and goal: π(a|s,g)\n",
        "- Q-function conditioned on state, action, and goal: Q(s,a,g)\n",
        "\n",
        "**Hindsight Experience Replay (HER):**\n",
        "- Augment failed trajectories with alternative goals\n",
        "- \"What if the goal was different?\"\n",
        "- Dramatically improves sample efficiency in sparse reward settings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 4.1: Implement Goal-Conditioned Policy\n",
        "class GoalConditionedPolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    Goal-conditioned policy that learns to reach any goal state.\n",
        "    Uses Universal Value Function Approximators (UVFA).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, goal_dim, action_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.goal_dim = goal_dim\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Policy network: π(a|s,g)\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(state_dim + goal_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "        \n",
        "        # Q-function: Q(s,a,g)\n",
        "        self.q_function = nn.Sequential(\n",
        "            nn.Linear(state_dim + action_dim + goal_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        \n",
        "    def forward(self, state, goal):\n",
        "        \"\"\"\n",
        "        Forward pass through goal-conditioned policy\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            goal: Target goal\n",
        "        \n",
        "        Returns:\n",
        "            action_logits: Action probabilities\n",
        "        \"\"\"\n",
        "        x = torch.cat([state, goal], dim=-1)\n",
        "        return self.policy(x)\n",
        "    \n",
        "    def get_q_value(self, state, action, goal):\n",
        "        \"\"\"Get Q-value for state-action-goal\"\"\"\n",
        "        x = torch.cat([state, action, goal], dim=-1)\n",
        "        return self.q_function(x)\n",
        "    \n",
        "    def act(self, state, goal, epsilon=0.1):\n",
        "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
        "        with torch.no_grad():\n",
        "            action_logits = self.forward(state, goal)\n",
        "            \n",
        "            if np.random.random() < epsilon:\n",
        "                action = np.random.randint(self.action_dim)\n",
        "            else:\n",
        "                action_probs = F.softmax(action_logits, dim=-1)\n",
        "                action = torch.multinomial(action_probs, 1).item()\n",
        "        \n",
        "        return action\n",
        "\n",
        "class GoalConditionedAgent:\n",
        "    \"\"\"Agent using goal-conditioned policy with HER\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, goal_dim, action_dim, lr=1e-3):\n",
        "        self.model = GoalConditionedPolicy(state_dim, goal_dim, action_dim)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.goal_dim = goal_dim\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.buffer = []\n",
        "        self.max_buffer_size = 10000\n",
        "        \n",
        "    def act(self, state, goal, epsilon=0.1):\n",
        "        \"\"\"Select action\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        goal_tensor = torch.FloatTensor(goal).unsqueeze(0)\n",
        "        \n",
        "        return self.model.act(state_tensor, goal_tensor, epsilon)\n",
        "    \n",
        "    def store_experience(self, state, action, reward, next_state, goal, done):\n",
        "        \"\"\"Store experience in buffer\"\"\"\n",
        "        experience = {\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state,\n",
        "            'goal': goal,\n",
        "            'done': done\n",
        "        }\n",
        "        \n",
        "        self.buffer.append(experience)\n",
        "        if len(self.buffer) > self.max_buffer_size:\n",
        "            self.buffer.pop(0)\n",
        "    \n",
        "    def hindsight_experience_replay(self, trajectory, strategy='future'):\n",
        "        \"\"\"\n",
        "        Augment trajectory with hindsight goals\n",
        "        \n",
        "        Args:\n",
        "            trajectory: List of experiences\n",
        "            strategy: 'future', 'final', or 'random'\n",
        "        \"\"\"\n",
        "        augmented_experiences = []\n",
        "        \n",
        "        for i, exp in enumerate(trajectory):\n",
        "            # Original experience\n",
        "            augmented_experiences.append(exp)\n",
        "            \n",
        "            # Hindsight: \"what if goal was different?\"\n",
        "            if strategy == 'future':\n",
        "                # Sample achieved state as goal\n",
        "                if i < len(trajectory) - 1:\n",
        "                    future_idx = np.random.randint(i, len(trajectory))\n",
        "                    new_goal = trajectory[future_idx]['next_state']\n",
        "                else:\n",
        "                    new_goal = exp['next_state']\n",
        "            elif strategy == 'final':\n",
        "                new_goal = trajectory[-1]['next_state']\n",
        "            elif strategy == 'random':\n",
        "                new_goal = np.random.uniform(-1, 1, self.goal_dim)\n",
        "            \n",
        "            # Recompute reward with new goal\n",
        "            new_reward = self.compute_reward(exp['next_state'], new_goal)\n",
        "            \n",
        "            # Create modified experience\n",
        "            modified_exp = exp.copy()\n",
        "            modified_exp['goal'] = new_goal\n",
        "            modified_exp['reward'] = new_reward\n",
        "            \n",
        "            augmented_experiences.append(modified_exp)\n",
        "        \n",
        "        return augmented_experiences\n",
        "    \n",
        "    def compute_reward(self, state, goal):\n",
        "        \"\"\"Compute reward for reaching goal\"\"\"\n",
        "        # Simple reward: negative distance to goal\n",
        "        distance = np.linalg.norm(np.array(state) - np.array(goal))\n",
        "        return -distance\n",
        "    \n",
        "    def update(self, batch_size=32):\n",
        "        \"\"\"Update the model using goal-conditioned learning\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return {}\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor([exp['state'] for exp in batch])\n",
        "        actions = torch.LongTensor([exp['action'] for exp in batch])\n",
        "        rewards = torch.FloatTensor([exp['reward'] for exp in batch])\n",
        "        next_states = torch.FloatTensor([exp['next_state'] for exp in batch])\n",
        "        goals = torch.FloatTensor([exp['goal'] for exp in batch])\n",
        "        dones = torch.BoolTensor([exp['done'] for exp in batch])\n",
        "        \n",
        "        # Compute Q-values\n",
        "        q_values = self.model.get_q_value(states, F.one_hot(actions, self.action_dim).float(), goals)\n",
        "        \n",
        "        # Compute target Q-values\n",
        "        with torch.no_grad():\n",
        "            # Get next actions from policy\n",
        "            next_action_logits = self.model.forward(next_states, goals)\n",
        "            next_action_probs = F.softmax(next_action_logits, dim=-1)\n",
        "            next_actions = torch.multinomial(next_action_probs, 1).squeeze(1)\n",
        "            \n",
        "            # Compute next Q-values\n",
        "            next_q_values = self.model.get_q_value(\n",
        "                next_states, \n",
        "                F.one_hot(next_actions, self.action_dim).float(), \n",
        "                goals\n",
        "            )\n",
        "            \n",
        "            targets = rewards + 0.99 * next_q_values * (~dones).float()\n",
        "        \n",
        "        # Q-learning loss\n",
        "        q_loss = F.mse_loss(q_values.squeeze(), targets)\n",
        "        \n",
        "        # Policy loss (using Q-values as advantage)\n",
        "        action_logits = self.model.forward(states, goals)\n",
        "        log_probs = F.log_softmax(action_logits, dim=-1)\n",
        "        policy_loss = -(log_probs.gather(1, actions.unsqueeze(1)) * q_values).mean()\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = q_loss + policy_loss\n",
        "        \n",
        "        # Optimize\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return {\n",
        "            'q_loss': q_loss.item(),\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "# Test Goal-Conditioned RL\n",
        "print(\"Goal-Conditioned Policy Created!\")\n",
        "print(\"Testing with GridWorld environment...\")\n",
        "\n",
        "# Create agent\n",
        "state_dim = 2  # (x, y) coordinates\n",
        "goal_dim = 2   # (x, y) goal coordinates\n",
        "action_dim = 4  # up, down, left, right\n",
        "\n",
        "agent = GoalConditionedAgent(state_dim, goal_dim, action_dim)\n",
        "env = GridWorld()\n",
        "\n",
        "# Test agent with different goals\n",
        "goals = [(5, 5), (9, 9), (0, 9), (9, 0)]\n",
        "\n",
        "for goal in goals:\n",
        "    print(f\"\\nTesting with goal: {goal}\")\n",
        "    state = env.reset()\n",
        "    \n",
        "    for step in range(10):\n",
        "        action = agent.act(state, goal)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        # Compute goal-conditioned reward\n",
        "        goal_reward = agent.compute_reward(next_state, goal)\n",
        "        \n",
        "        print(f\"Step {step+1}: Action {env.action_names[action]} -> State {next_state}, Reward {goal_reward:.2f}\")\n",
        "        \n",
        "        # Store experience\n",
        "        agent.store_experience(state, action, goal_reward, next_state, goal, done)\n",
        "        \n",
        "        if done:\n",
        "            print(\"Goal reached!\")\n",
        "            break\n",
        "        state = next_state\n",
        "\n",
        "# Update model\n",
        "losses = agent.update()\n",
        "print(f\"\\nTraining losses: {losses}\")\n",
        "\n",
        "# Test HER\n",
        "print(\"\\nTesting Hindsight Experience Replay...\")\n",
        "trajectory = agent.buffer[-10:]  # Last 10 experiences\n",
        "augmented = agent.hindsight_experience_replay(trajectory, strategy='future')\n",
        "print(f\"Original experiences: {len(trajectory)}\")\n",
        "print(f\"Augmented experiences: {len(augmented)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Skill Discovery\n",
        "\n",
        "### Diversity is All You Need (DIAYN)\n",
        "\n",
        "**Objective:** Learn diverse skills without rewards\n",
        "\n",
        "**Key Components:**\n",
        "1. **Skill-conditioned policy**: π(a|s,z) where z is skill ID\n",
        "2. **Discriminator**: Predict skill from state transitions\n",
        "3. **Intrinsic reward**: Reward for making states predictive of skill\n",
        "\n",
        "**Training Process:**\n",
        "1. Sample random skill z\n",
        "2. Execute policy π(a|s,z) \n",
        "3. Compute intrinsic reward based on discriminator\n",
        "4. Update policy and discriminator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Exercise 5.1: Implement DIAYN (Diversity is All You Need)\n",
        "class SkillConditionedPolicy(nn.Module):\n",
        "    \"\"\"Policy conditioned on skill ID\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, num_skills, action_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.num_skills = num_skills\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Skill embedding\n",
        "        self.skill_embedding = nn.Embedding(num_skills, hidden_dim)\n",
        "        \n",
        "        # Policy network\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(state_dim + hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "        \n",
        "    def forward(self, state, skill):\n",
        "        \"\"\"Forward pass through skill-conditioned policy\"\"\"\n",
        "        skill_emb = self.skill_embedding(skill)\n",
        "        x = torch.cat([state, skill_emb], dim=-1)\n",
        "        return self.policy(x)\n",
        "    \n",
        "    def act(self, state, skill, epsilon=0.1):\n",
        "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
        "        with torch.no_grad():\n",
        "            action_logits = self.forward(state, skill)\n",
        "            \n",
        "            if np.random.random() < epsilon:\n",
        "                action = np.random.randint(self.action_dim)\n",
        "            else:\n",
        "                action_probs = F.softmax(action_logits, dim=-1)\n",
        "                action = torch.multinomial(action_probs, 1).item()\n",
        "        \n",
        "        return action\n",
        "\n",
        "class SkillDiscriminator(nn.Module):\n",
        "    \"\"\"Discriminator that predicts skill from state\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, num_skills, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.discriminator = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_skills)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"Predict skill from state\"\"\"\n",
        "        return self.discriminator(state)\n",
        "    \n",
        "    def get_skill_prob(self, state, skill):\n",
        "        \"\"\"Get probability of skill given state\"\"\"\n",
        "        logits = self.forward(state)\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        return probs[:, skill]\n",
        "\n",
        "class DIAYN:\n",
        "    \"\"\"DIAYN algorithm for unsupervised skill discovery\"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, num_skills, action_dim, lr=1e-3):\n",
        "        self.state_dim = state_dim\n",
        "        self.num_skills = num_skills\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Models\n",
        "        self.policy = SkillConditionedPolicy(state_dim, num_skills, action_dim)\n",
        "        self.discriminator = SkillDiscriminator(state_dim, num_skills)\n",
        "        \n",
        "        # Optimizers\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.discriminator_optimizer = optim.Adam(self.discriminator.parameters(), lr=lr)\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.buffer = []\n",
        "        self.max_buffer_size = 10000\n",
        "        \n",
        "    def act(self, state, skill, epsilon=0.1):\n",
        "        \"\"\"Select action using skill-conditioned policy\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        skill_tensor = torch.LongTensor([skill])\n",
        "        \n",
        "        return self.policy.act(state_tensor, skill_tensor, epsilon)\n",
        "    \n",
        "    def store_experience(self, state, action, next_state, skill):\n",
        "        \"\"\"Store experience in buffer\"\"\"\n",
        "        experience = {\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'next_state': next_state,\n",
        "            'skill': skill\n",
        "        }\n",
        "        \n",
        "        self.buffer.append(experience)\n",
        "        if len(self.buffer) > self.max_buffer_size:\n",
        "            self.buffer.pop(0)\n",
        "    \n",
        "    def compute_intrinsic_reward(self, state, skill):\n",
        "        \"\"\"Compute intrinsic reward for skill discovery\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        skill_tensor = torch.LongTensor([skill])\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Get skill probability from discriminator\n",
        "            skill_prob = self.discriminator.get_skill_prob(state_tensor, skill_tensor)\n",
        "            \n",
        "            # Intrinsic reward: log p(skill|state) - log(1/num_skills)\n",
        "            # This encourages states that are predictive of the skill\n",
        "            intrinsic_reward = torch.log(skill_prob + 1e-8) - np.log(1.0 / self.num_skills)\n",
        "            \n",
        "            return intrinsic_reward.item()\n",
        "    \n",
        "    def update_policy(self, batch_size=32):\n",
        "        \"\"\"Update skill-conditioned policy\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return {}\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor([exp['state'] for exp in batch])\n",
        "        actions = torch.LongTensor([exp['action'] for exp in batch])\n",
        "        skills = torch.LongTensor([exp['skill'] for exp in batch])\n",
        "        \n",
        "        # Compute intrinsic rewards\n",
        "        intrinsic_rewards = []\n",
        "        for i, exp in enumerate(batch):\n",
        "            reward = self.compute_intrinsic_reward(exp['next_state'], exp['skill'])\n",
        "            intrinsic_rewards.append(reward)\n",
        "        \n",
        "        intrinsic_rewards = torch.FloatTensor(intrinsic_rewards)\n",
        "        \n",
        "        # Policy loss\n",
        "        action_logits = self.policy.forward(states, skills)\n",
        "        log_probs = F.log_softmax(action_logits, dim=-1)\n",
        "        policy_loss = -(log_probs.gather(1, actions.unsqueeze(1)) * intrinsic_rewards.unsqueeze(1)).mean()\n",
        "        \n",
        "        # Optimize policy\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        \n",
        "        return {'policy_loss': policy_loss.item()}\n",
        "    \n",
        "    def update_discriminator(self, batch_size=32):\n",
        "        \"\"\"Update skill discriminator\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return {}\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        # Convert to tensors\n",
        "        states = torch.FloatTensor([exp['next_state'] for exp in batch])  # Use next_state\n",
        "        skills = torch.LongTensor([exp['skill'] for exp in batch])\n",
        "        \n",
        "        # Discriminator loss\n",
        "        logits = self.discriminator.forward(states)\n",
        "        discriminator_loss = F.cross_entropy(logits, skills)\n",
        "        \n",
        "        # Optimize discriminator\n",
        "        self.discriminator_optimizer.zero_grad()\n",
        "        discriminator_loss.backward()\n",
        "        self.discriminator_optimizer.step()\n",
        "        \n",
        "        return {'discriminator_loss': discriminator_loss.item()}\n",
        "    \n",
        "    def train_episode(self, env, max_steps=50):\n",
        "        \"\"\"Train for one episode\"\"\"\n",
        "        # Sample random skill\n",
        "        skill = np.random.randint(self.num_skills)\n",
        "        \n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action = self.act(state, skill)\n",
        "            \n",
        "            # Execute action\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            \n",
        "            # Store experience\n",
        "            self.store_experience(state, action, next_state, skill)\n",
        "            \n",
        "            # Compute intrinsic reward\n",
        "            intrinsic_reward = self.compute_intrinsic_reward(next_state, skill)\n",
        "            episode_reward += intrinsic_reward\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        return episode_reward, skill\n",
        "\n",
        "# Test DIAYN\n",
        "print(\"DIAYN Algorithm Created!\")\n",
        "print(\"Testing skill discovery...\")\n",
        "\n",
        "# Create DIAYN agent\n",
        "state_dim = 2  # (x, y) coordinates\n",
        "num_skills = 4  # Number of skills to discover\n",
        "action_dim = 4  # up, down, left, right\n",
        "\n",
        "diayn = DIAYN(state_dim, num_skills, action_dim)\n",
        "env = GridWorld()\n",
        "\n",
        "# Train for several episodes\n",
        "print(\"Training DIAYN for skill discovery...\")\n",
        "\n",
        "for episode in range(10):\n",
        "    episode_reward, skill = diayn.train_episode(env)\n",
        "    \n",
        "    # Update models\n",
        "    policy_losses = diayn.update_policy()\n",
        "    discriminator_losses = diayn.update_discriminator()\n",
        "    \n",
        "    print(f\"Episode {episode+1}: Skill {skill}, Reward {episode_reward:.2f}\")\n",
        "    print(f\"  Policy Loss: {policy_losses.get('policy_loss', 0):.4f}\")\n",
        "    print(f\"  Discriminator Loss: {discriminator_losses.get('discriminator_loss', 0):.4f}\")\n",
        "\n",
        "# Test discovered skills\n",
        "print(\"\\nTesting discovered skills...\")\n",
        "for skill in range(num_skills):\n",
        "    print(f\"\\nSkill {skill}:\")\n",
        "    state = env.reset()\n",
        "    \n",
        "    for step in range(10):\n",
        "        action = diayn.act(state, skill, epsilon=0.0)  # No exploration\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        \n",
        "        intrinsic_reward = diayn.compute_intrinsic_reward(next_state, skill)\n",
        "        print(f\"  Step {step+1}: Action {env.action_names[action]} -> State {next_state}, Intrinsic Reward {intrinsic_reward:.2f}\")\n",
        "        \n",
        "        if done:\n",
        "            print(\"  Goal reached!\")\n",
        "            break\n",
        "        state = next_state\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
