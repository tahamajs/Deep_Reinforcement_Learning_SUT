{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HW12: Hierarchical Reinforcement Learning\n",
        "\n",
        "> - Full Name: **[Your Full Name]**\n",
        "> - Student ID: **[Your Student ID]**\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DeepRLCourse/Homework-12-Questions/blob/main/HW12_Notebook.ipynb)\n",
        "[![Open In kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://raw.githubusercontent.com/DeepRLCourse/Homework-12-Questions/main/HW12_Notebook.ipynb)\n",
        "\n",
        "## Overview\n",
        "This assignment focuses on **Hierarchical Reinforcement Learning (HRL)**, exploring methods to structure policies across multiple levels of abstraction. We'll implement and experiment with:\n",
        "\n",
        "1. **Options Framework** - Semi-Markov decision processes\n",
        "2. **Feudal Hierarchies** - Manager-worker architectures\n",
        "3. **Goal-Conditioned RL** - Universal Value Function Approximators (UVFA)\n",
        "4. **Hindsight Experience Replay (HER)** - Learning from failed trajectories\n",
        "5. **Skill Discovery** - DIAYN for unsupervised skill learning\n",
        "\n",
        "The goal is to understand how temporal abstraction enables agents to solve complex, long-horizon tasks by decomposing them into simpler subtasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Imports and Setup\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import trange\n",
        "from collections import defaultdict, deque\n",
        "import random\n",
        "from typing import Tuple, List, Dict, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Hierarchical Environment Setup\n",
        "\n",
        "First, let's create a hierarchical environment that benefits from temporal abstraction. We'll use a multi-room navigation task where the agent must navigate through multiple rooms to reach a goal.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiRoomEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Multi-room navigation environment for hierarchical RL.\n",
        "    Agent must navigate through rooms to reach a goal.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, num_rooms=4, room_size=5):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.num_rooms = num_rooms\n",
        "        self.room_size = room_size\n",
        "        self.grid_size = num_rooms * room_size\n",
        "        \n",
        "        # State: (x, y, room_id, goal_x, goal_y, goal_room)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=self.grid_size, shape=(6,), dtype=np.float32\n",
        "        )\n",
        "        \n",
        "        # Actions: 0=up, 1=down, 2=left, 3=right\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "        \n",
        "        # Room layout\n",
        "        self.rooms = self._create_rooms()\n",
        "        self.doors = self._create_doors()\n",
        "        \n",
        "        # Goal and agent positions\n",
        "        self.goal_pos = None\n",
        "        self.goal_room = None\n",
        "        self.agent_pos = None\n",
        "        self.agent_room = None\n",
        "        \n",
        "    def _create_rooms(self):\n",
        "        \"\"\"Create room boundaries.\"\"\"\n",
        "        rooms = {}\n",
        "        for i in range(self.num_rooms):\n",
        "            x_start = i * self.room_size\n",
        "            x_end = (i + 1) * self.room_size\n",
        "            rooms[i] = (x_start, x_end, 0, self.room_size)\n",
        "        return rooms\n",
        "    \n",
        "    def _create_doors(self):\n",
        "        \"\"\"Create doors between rooms.\"\"\"\n",
        "        doors = []\n",
        "        for i in range(self.num_rooms - 1):\n",
        "            door_x = (i + 1) * self.room_size - 1\n",
        "            door_y = self.room_size // 2\n",
        "            doors.append((door_x, door_y))\n",
        "        return doors\n",
        "    \n",
        "    def reset(self, **kwargs):\n",
        "        \"\"\"Reset environment.\"\"\"\n",
        "        # Random goal in last room\n",
        "        self.goal_room = self.num_rooms - 1\n",
        "        goal_x = np.random.randint(\n",
        "            self.goal_room * self.room_size + 1,\n",
        "            (self.goal_room + 1) * self.room_size - 1\n",
        "        )\n",
        "        goal_y = np.random.randint(1, self.room_size - 1)\n",
        "        self.goal_pos = (goal_x, goal_y)\n",
        "        \n",
        "        # Random start in first room\n",
        "        self.agent_room = 0\n",
        "        start_x = np.random.randint(1, self.room_size - 1)\n",
        "        start_y = np.random.randint(1, self.room_size - 1)\n",
        "        self.agent_pos = (start_x, start_y)\n",
        "        \n",
        "        return self._get_observation(), {}\n",
        "    \n",
        "    def step(self, action):\n",
        "        \"\"\"Execute action.\"\"\"\n",
        "        x, y = self.agent_pos\n",
        "        \n",
        "        # Action effects\n",
        "        if action == 0:  # up\n",
        "            new_y = max(0, y - 1)\n",
        "            new_pos = (x, new_y)\n",
        "        elif action == 1:  # down\n",
        "            new_y = min(self.grid_size - 1, y + 1)\n",
        "            new_pos = (x, new_y)\n",
        "        elif action == 2:  # left\n",
        "            new_x = max(0, x - 1)\n",
        "            new_pos = (new_x, y)\n",
        "        elif action == 3:  # right\n",
        "            new_x = min(self.grid_size - 1, x + 1)\n",
        "            new_pos = (new_x, y)\n",
        "        \n",
        "        # Check if move is valid (not through walls, but can go through doors)\n",
        "        if self._is_valid_move(self.agent_pos, new_pos):\n",
        "            self.agent_pos = new_pos\n",
        "            self.agent_room = self._get_room_from_pos(new_pos)\n",
        "        \n",
        "        # Compute reward\n",
        "        reward = self._compute_reward()\n",
        "        \n",
        "        # Check if done\n",
        "        done = self._is_goal_reached()\n",
        "        \n",
        "        return self._get_observation(), reward, done, False, {}\n",
        "    \n",
        "    def _is_valid_move(self, old_pos, new_pos):\n",
        "        \"\"\"Check if move is valid.\"\"\"\n",
        "        old_x, old_y = old_pos\n",
        "        new_x, new_y = new_pos\n",
        "        \n",
        "        # Check room boundaries\n",
        "        old_room = self._get_room_from_pos(old_pos)\n",
        "        new_room = self._get_room_from_pos(new_pos)\n",
        "        \n",
        "        # Can move within room\n",
        "        if old_room == new_room:\n",
        "            return True\n",
        "        \n",
        "        # Can move through doors\n",
        "        if (old_x, old_y) in self.doors or (new_x, new_y) in self.doors:\n",
        "            return True\n",
        "        \n",
        "        return False\n",
        "    \n",
        "    def _get_room_from_pos(self, pos):\n",
        "        \"\"\"Get room ID from position.\"\"\"\n",
        "        x, y = pos\n",
        "        return min(x // self.room_size, self.num_rooms - 1)\n",
        "    \n",
        "    def _compute_reward(self):\n",
        "        \"\"\"Compute reward based on progress toward goal.\"\"\"\n",
        "        agent_x, agent_y = self.agent_pos\n",
        "        goal_x, goal_y = self.goal_pos\n",
        "        \n",
        "        # Distance to goal\n",
        "        distance = np.sqrt((agent_x - goal_x)**2 + (agent_y - goal_y)**2)\n",
        "        \n",
        "        # Sparse reward: only when reaching goal\n",
        "        if self._is_goal_reached():\n",
        "            return 100.0\n",
        "        \n",
        "        # Small negative reward for each step (encourage efficiency)\n",
        "        return -0.1\n",
        "    \n",
        "    def _is_goal_reached(self):\n",
        "        \"\"\"Check if agent reached goal.\"\"\"\n",
        "        return (self.agent_pos == self.goal_pos and \n",
        "                self.agent_room == self.goal_room)\n",
        "    \n",
        "    def _get_observation(self):\n",
        "        \"\"\"Get current observation.\"\"\"\n",
        "        agent_x, agent_y = self.agent_pos\n",
        "        goal_x, goal_y = self.goal_pos\n",
        "        \n",
        "        return np.array([\n",
        "            agent_x, agent_y, self.agent_room,\n",
        "            goal_x, goal_y, self.goal_room\n",
        "        ], dtype=np.float32)\n",
        "    \n",
        "    def render(self, mode='human'):\n",
        "        \"\"\"Render environment.\"\"\"\n",
        "        if mode == 'human':\n",
        "            grid = np.zeros((self.grid_size, self.grid_size))\n",
        "            \n",
        "            # Mark rooms\n",
        "            for room_id, (x_start, x_end, y_start, y_end) in self.rooms.items():\n",
        "                grid[y_start:y_end, x_start:x_end] = room_id + 1\n",
        "            \n",
        "            # Mark doors\n",
        "            for door_x, door_y in self.doors:\n",
        "                grid[door_y, door_x] = 0.5\n",
        "            \n",
        "            # Mark agent\n",
        "            agent_x, agent_y = self.agent_pos\n",
        "            grid[agent_y, agent_x] = -1\n",
        "            \n",
        "            # Mark goal\n",
        "            goal_x, goal_y = self.goal_pos\n",
        "            grid[goal_y, goal_x] = -2\n",
        "            \n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.imshow(grid, cmap='tab10')\n",
        "            plt.title('Multi-Room Environment')\n",
        "            plt.xlabel('X Position')\n",
        "            plt.ylabel('Y Position')\n",
        "            plt.colorbar()\n",
        "            plt.show()\n",
        "\n",
        "# Test the environment\n",
        "print(\"Testing Multi-Room Environment...\")\n",
        "env = MultiRoomEnv(num_rooms=3, room_size=4)\n",
        "\n",
        "# Run a few random episodes\n",
        "for episode in range(3):\n",
        "    obs, info = env.reset()\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for step in range(50):  # Max 50 steps\n",
        "        action = env.action_space.sample()\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "        \n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    \n",
        "    print(f\"Episode {episode + 1}: Reward = {episode_reward:.2f}, \"\n",
        "          f\"Final Room = {obs[2]:.0f}, Goal Room = {obs[5]:.0f}\")\n",
        "\n",
        "print(\"Environment test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Options Framework Implementation\n",
        "\n",
        "The Options Framework provides temporal abstraction by allowing agents to choose temporally extended actions. An option consists of:\n",
        "- **Initiation Set**: States where the option can be executed\n",
        "- **Policy**: How to behave while executing the option\n",
        "- **Termination Function**: When to stop executing the option\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Option:\n",
        "    \"\"\"\n",
        "    An option represents a temporally extended action.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, option_id, initiation_set, policy, termination_function):\n",
        "        self.option_id = option_id\n",
        "        self.initiation_set = initiation_set  # Function: state -> bool\n",
        "        self.policy = policy  # Function: state -> action\n",
        "        self.termination_function = termination_function  # Function: state -> termination_prob\n",
        "    \n",
        "    def can_initiate(self, state):\n",
        "        \"\"\"Check if option can be initiated in given state.\"\"\"\n",
        "        return self.initiation_set(state)\n",
        "    \n",
        "    def get_action(self, state):\n",
        "        \"\"\"Get action from option policy.\"\"\"\n",
        "        return self.policy(state)\n",
        "    \n",
        "    def should_terminate(self, state):\n",
        "        \"\"\"Check if option should terminate.\"\"\"\n",
        "        return self.termination_function(state)\n",
        "\n",
        "\n",
        "class OptionCritic(nn.Module):\n",
        "    \"\"\"\n",
        "    Option-Critic architecture for learning options end-to-end.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, num_options, action_dim, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.num_options = num_options\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Shared state encoder\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Option policies (one for each option)\n",
        "        self.option_policies = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, action_dim)\n",
        "            ) for _ in range(num_options)\n",
        "        ])\n",
        "        \n",
        "        # Termination functions\n",
        "        self.termination_functions = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_options),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "        \n",
        "        # Q-value over options\n",
        "        self.q_omega = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, num_options)\n",
        "        )\n",
        "        \n",
        "        # Value function for options\n",
        "        self.value_function = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state, current_option=None):\n",
        "        \"\"\"Forward pass through the network.\"\"\"\n",
        "        features = self.encoder(state)\n",
        "        \n",
        "        if current_option is not None:\n",
        "            # Get action from current option\n",
        "            action_logits = self.option_policies[current_option](features)\n",
        "            \n",
        "            # Get termination probability\n",
        "            termination_probs = self.termination_functions(features)\n",
        "            termination_prob = termination_probs[:, current_option]\n",
        "            \n",
        "            return action_logits, termination_prob\n",
        "        else:\n",
        "            # Select option\n",
        "            q_omega = self.q_omega(features)\n",
        "            return q_omega\n",
        "    \n",
        "    def get_value(self, state):\n",
        "        \"\"\"Get state value.\"\"\"\n",
        "        features = self.encoder(state)\n",
        "        return self.value_function(features)\n",
        "    \n",
        "    def select_option(self, state, epsilon=0.1):\n",
        "        \"\"\"Select option using epsilon-greedy policy.\"\"\"\n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.randint(self.num_options)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            q_omega = self.forward(state)\n",
        "            return q_omega.argmax(dim=-1).item()\n",
        "    \n",
        "    def get_action_and_termination(self, state, option):\n",
        "        \"\"\"Get action and termination probability for given option.\"\"\"\n",
        "        action_logits, termination_prob = self.forward(state, option)\n",
        "        \n",
        "        # Sample action\n",
        "        action_probs = F.softmax(action_logits, dim=-1)\n",
        "        action = torch.multinomial(action_probs, 1).item()\n",
        "        \n",
        "        # Sample termination\n",
        "        should_terminate = np.random.random() < termination_prob.item()\n",
        "        \n",
        "        return action, should_terminate\n",
        "\n",
        "\n",
        "class OptionLearningAgent:\n",
        "    \"\"\"\n",
        "    Agent that learns options using the Option-Critic algorithm.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, num_options, action_dim, lr=3e-4, gamma=0.99):\n",
        "        self.state_dim = state_dim\n",
        "        self.num_options = num_options\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Networks\n",
        "        self.option_critic = OptionCritic(state_dim, num_options, action_dim)\n",
        "        self.optimizer = optim.Adam(self.option_critic.parameters(), lr=lr)\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.buffer = []\n",
        "        \n",
        "    def select_action(self, state, current_option=None, epsilon=0.1):\n",
        "        \"\"\"Select action using current option or select new option.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        \n",
        "        if current_option is None:\n",
        "            # Select new option\n",
        "            option = self.option_critic.select_option(state_tensor, epsilon)\n",
        "            return option, None\n",
        "        else:\n",
        "            # Get action from current option\n",
        "            action, should_terminate = self.option_critic.get_action_and_termination(\n",
        "                state_tensor, current_option\n",
        "            )\n",
        "            return action, should_terminate\n",
        "    \n",
        "    def store_transition(self, state, option, action, reward, next_state, terminated, truncated):\n",
        "        \"\"\"Store transition in buffer.\"\"\"\n",
        "        self.buffer.append({\n",
        "            'state': state,\n",
        "            'option': option,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state,\n",
        "            'terminated': terminated,\n",
        "            'truncated': truncated\n",
        "        })\n",
        "    \n",
        "    def update(self, batch_size=32):\n",
        "        \"\"\"Update networks using stored experiences.\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        states = torch.FloatTensor([t['state'] for t in batch])\n",
        "        options = torch.LongTensor([t['option'] for t in batch])\n",
        "        actions = torch.LongTensor([t['action'] for t in batch])\n",
        "        rewards = torch.FloatTensor([t['reward'] for t in batch])\n",
        "        next_states = torch.FloatTensor([t['next_state'] for t in batch])\n",
        "        terminated = torch.BoolTensor([t['terminated'] for t in batch])\n",
        "        \n",
        "        # Compute Q-values\n",
        "        q_omega = self.option_critic.q_omega(self.option_critic.encoder(states))\n",
        "        q_values = q_omega.gather(1, options.unsqueeze(1)).squeeze(1)\n",
        "        \n",
        "        # Compute target Q-values\n",
        "        with torch.no_grad():\n",
        "            next_q_omega = self.option_critic.q_omega(self.option_critic.encoder(next_states))\n",
        "            next_values = self.option_critic.get_value(next_states).squeeze(1)\n",
        "            \n",
        "            # Target for Q-learning\n",
        "            targets = rewards + self.gamma * next_values * (~terminated)\n",
        "        \n",
        "        # Q-learning loss\n",
        "        q_loss = F.mse_loss(q_values, targets)\n",
        "        \n",
        "        # Policy gradient loss (simplified)\n",
        "        action_logits, termination_probs = self.option_critic(states, options)\n",
        "        action_probs = F.softmax(action_logits, dim=-1)\n",
        "        log_probs = F.log_softmax(action_logits, dim=-1)\n",
        "        \n",
        "        # Advantage estimation (simplified)\n",
        "        advantages = targets - q_values.detach()\n",
        "        \n",
        "        # Policy loss\n",
        "        policy_loss = -(log_probs.gather(1, actions.unsqueeze(1)).squeeze(1) * advantages).mean()\n",
        "        \n",
        "        # Termination loss (encourage appropriate termination)\n",
        "        termination_loss = -(termination_probs.gather(1, options.unsqueeze(1)).squeeze(1) * advantages).mean()\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = q_loss + policy_loss + termination_loss\n",
        "        \n",
        "        # Update\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return {\n",
        "            'q_loss': q_loss.item(),\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'termination_loss': termination_loss.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "\n",
        "# Test the Option-Critic implementation\n",
        "print(\"Testing Option-Critic Implementation...\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = MultiRoomEnv(num_rooms=3, room_size=4)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "num_options = 3  # Navigate to room 1, 2, and 3\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = OptionLearningAgent(state_dim, num_options, action_dim)\n",
        "\n",
        "# Test action selection\n",
        "obs, _ = env.reset()\n",
        "print(f\"Initial observation: {obs}\")\n",
        "\n",
        "# Select initial option\n",
        "option, _ = agent.select_action(obs)\n",
        "print(f\"Selected option: {option}\")\n",
        "\n",
        "# Get action from option\n",
        "action, should_terminate = agent.select_action(obs, option)\n",
        "print(f\"Action from option {option}: {action}, Should terminate: {should_terminate}\")\n",
        "\n",
        "print(\"Option-Critic test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Feudal Hierarchies\n",
        "\n",
        "Feudal RL implements a manager-worker hierarchy where:\n",
        "- **Manager**: Sets high-level goals/commands\n",
        "- **Worker**: Executes low-level actions to achieve goals\n",
        "- **Communication**: Manager communicates goals to worker through embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeudalNet(nn.Module):\n",
        "    \"\"\"\n",
        "    Feudal Networks implementation with Manager-Worker hierarchy.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, goal_dim=16, c=10, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.goal_dim = goal_dim\n",
        "        self.c = c  # Manager update frequency\n",
        "        \n",
        "        # Shared perception module\n",
        "        self.perception = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        \n",
        "        # Manager network (LSTM for temporal dependencies)\n",
        "        self.manager = nn.LSTM(hidden_dim, goal_dim, batch_first=True)\n",
        "        \n",
        "        # Worker network (receives state + goal)\n",
        "        self.worker = nn.LSTM(hidden_dim + goal_dim, hidden_dim, batch_first=True)\n",
        "        self.worker_policy = nn.Linear(hidden_dim, action_dim)\n",
        "        \n",
        "        # Value functions\n",
        "        self.manager_value = nn.Linear(goal_dim, 1)\n",
        "        self.worker_value = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "    def forward(self, state, manager_hidden=None, worker_hidden=None, t=0):\n",
        "        \"\"\"\n",
        "        Forward pass through feudal network.\n",
        "        \n",
        "        Args:\n",
        "            state: Current state\n",
        "            manager_hidden: Manager LSTM hidden state\n",
        "            worker_hidden: Worker LSTM hidden state\n",
        "            t: Current timestep\n",
        "        \"\"\"\n",
        "        batch_size = state.size(0)\n",
        "        \n",
        "        # Shared perception\n",
        "        z = self.perception(state)\n",
        "        \n",
        "        # Manager operates every c timesteps\n",
        "        if t % self.c == 0 or manager_hidden is None:\n",
        "            # Manager sets goal\n",
        "            if manager_hidden is None:\n",
        "                manager_hidden = (torch.zeros(1, batch_size, self.goal_dim),\n",
        "                                torch.zeros(1, batch_size, self.goal_dim))\n",
        "            \n",
        "            goal, manager_hidden = self.manager(z.unsqueeze(1), manager_hidden)\n",
        "            goal = F.normalize(goal, dim=-1)  # Normalize goal vector\n",
        "        else:\n",
        "            # Use previous goal\n",
        "            goal = torch.zeros(batch_size, 1, self.goal_dim)\n",
        "            if manager_hidden is not None:\n",
        "                goal = manager_hidden[0].transpose(0, 1)\n",
        "        \n",
        "        # Worker receives state + goal\n",
        "        worker_input = torch.cat([z, goal.squeeze(1)], dim=-1)\n",
        "        \n",
        "        if worker_hidden is None:\n",
        "            worker_hidden = (torch.zeros(1, batch_size, hidden_dim),\n",
        "                           torch.zeros(1, batch_size, hidden_dim))\n",
        "        \n",
        "        worker_out, worker_hidden = self.worker(worker_input.unsqueeze(1), worker_hidden)\n",
        "        \n",
        "        # Worker action\n",
        "        action_logits = self.worker_policy(worker_out.squeeze(1))\n",
        "        \n",
        "        # Value estimates\n",
        "        manager_value = self.manager_value(goal.squeeze(1))\n",
        "        worker_value = self.worker_value(worker_out.squeeze(1))\n",
        "        \n",
        "        return {\n",
        "            'action_logits': action_logits,\n",
        "            'goal': goal.squeeze(1),\n",
        "            'manager_value': manager_value,\n",
        "            'worker_value': worker_value,\n",
        "            'manager_hidden': manager_hidden,\n",
        "            'worker_hidden': worker_hidden\n",
        "        }\n",
        "\n",
        "\n",
        "class FeudalAgent:\n",
        "    \"\"\"\n",
        "    Feudal RL agent with manager-worker hierarchy.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, action_dim, goal_dim=16, c=10, lr=3e-4, gamma=0.99):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.goal_dim = goal_dim\n",
        "        self.c = c\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Networks\n",
        "        self.feudal_net = FeudalNet(state_dim, action_dim, goal_dim, c)\n",
        "        self.optimizer = optim.Adam(self.feudal_net.parameters(), lr=lr)\n",
        "        \n",
        "        # Hidden states\n",
        "        self.manager_hidden = None\n",
        "        self.worker_hidden = None\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.buffer = []\n",
        "        \n",
        "    def select_action(self, state, t=0):\n",
        "        \"\"\"Select action using feudal hierarchy.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = self.feudal_net(state_tensor, self.manager_hidden, self.worker_hidden, t)\n",
        "            \n",
        "            # Update hidden states\n",
        "            self.manager_hidden = output['manager_hidden']\n",
        "            self.worker_hidden = output['worker_hidden']\n",
        "            \n",
        "            # Sample action\n",
        "            action_probs = F.softmax(output['action_logits'], dim=-1)\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "            \n",
        "            return action, output['goal'].squeeze(0).numpy()\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, goal, manager_value, worker_value, done):\n",
        "        \"\"\"Store transition in buffer.\"\"\"\n",
        "        self.buffer.append({\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state,\n",
        "            'goal': goal,\n",
        "            'manager_value': manager_value,\n",
        "            'worker_value': worker_value,\n",
        "            'done': done\n",
        "        })\n",
        "    \n",
        "    def compute_manager_reward(self, goals, states, c):\n",
        "        \"\"\"\n",
        "        Compute manager reward based on transition embedding similarity.\n",
        "        Manager is rewarded for setting goals that align with state transitions.\n",
        "        \"\"\"\n",
        "        if len(goals) < 2:\n",
        "            return torch.zeros(1)\n",
        "        \n",
        "        # Compute state embeddings\n",
        "        state_tensors = torch.FloatTensor(states)\n",
        "        embeddings = self.feudal_net.perception(state_tensors)\n",
        "        \n",
        "        # Compute transition directions\n",
        "        transitions = embeddings[c:] - embeddings[:-c]\n",
        "        \n",
        "        # Compute cosine similarity between goals and transitions\n",
        "        goal_tensors = torch.FloatTensor(goals[:-c])\n",
        "        similarities = F.cosine_similarity(goal_tensors, transitions, dim=-1)\n",
        "        \n",
        "        return similarities.mean()\n",
        "    \n",
        "    def compute_worker_reward(self, goals, states, rewards, alpha=0.1):\n",
        "        \"\"\"\n",
        "        Compute worker reward: extrinsic + intrinsic.\n",
        "        Intrinsic reward encourages progress toward manager's goal.\n",
        "        \"\"\"\n",
        "        if len(goals) < 2:\n",
        "            return torch.FloatTensor(rewards)\n",
        "        \n",
        "        # Compute state embeddings\n",
        "        state_tensors = torch.FloatTensor(states)\n",
        "        embeddings = self.feudal_net.perception(state_tensors)\n",
        "        \n",
        "        # Compute progress toward goal\n",
        "        goal_tensors = torch.FloatTensor(goals[:-1])\n",
        "        transitions = embeddings[1:] - embeddings[:-1]\n",
        "        \n",
        "        # Intrinsic reward: progress toward goal\n",
        "        intrinsic_rewards = F.cosine_similarity(goal_tensors, transitions, dim=-1)\n",
        "        \n",
        "        # Combine extrinsic and intrinsic rewards\n",
        "        extrinsic_rewards = torch.FloatTensor(rewards[:-1])\n",
        "        total_rewards = extrinsic_rewards + alpha * intrinsic_rewards\n",
        "        \n",
        "        return total_rewards\n",
        "    \n",
        "    def update(self, batch_size=32):\n",
        "        \"\"\"Update feudal networks.\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        states = torch.FloatTensor([t['state'] for t in batch])\n",
        "        actions = torch.LongTensor([t['action'] for t in batch])\n",
        "        rewards = torch.FloatTensor([t['reward'] for t in batch])\n",
        "        next_states = torch.FloatTensor([t['next_state'] for t in batch])\n",
        "        goals = torch.FloatTensor([t['goal'] for t in batch])\n",
        "        manager_values = torch.FloatTensor([t['manager_value'] for t in batch])\n",
        "        worker_values = torch.FloatTensor([t['worker_value'] for t in batch])\n",
        "        dones = torch.BoolTensor([t['done'] for t in batch])\n",
        "        \n",
        "        # Forward pass\n",
        "        output = self.feudal_net(states)\n",
        "        \n",
        "        # Compute targets\n",
        "        with torch.no_grad():\n",
        "            next_output = self.feudal_net(next_states)\n",
        "            next_manager_values = next_output['manager_value']\n",
        "            next_worker_values = next_output['worker_value']\n",
        "            \n",
        "            manager_targets = rewards + self.gamma * next_manager_values * (~dones)\n",
        "            worker_targets = rewards + self.gamma * next_worker_values * (~dones)\n",
        "        \n",
        "        # Manager loss\n",
        "        manager_loss = F.mse_loss(output['manager_value'], manager_targets)\n",
        "        \n",
        "        # Worker loss\n",
        "        worker_loss = F.mse_loss(output['worker_value'], worker_targets)\n",
        "        \n",
        "        # Policy loss (simplified)\n",
        "        action_logits = output['action_logits']\n",
        "        log_probs = F.log_softmax(action_logits, dim=-1)\n",
        "        action_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        \n",
        "        # Advantage estimation\n",
        "        advantages = worker_targets - worker_values.detach()\n",
        "        policy_loss = -(action_probs * advantages).mean()\n",
        "        \n",
        "        # Total loss\n",
        "        total_loss = manager_loss + worker_loss + policy_loss\n",
        "        \n",
        "        # Update\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss.backward()\n",
        "        self.optimizer.step()\n",
        "        \n",
        "        return {\n",
        "            'manager_loss': manager_loss.item(),\n",
        "            'worker_loss': worker_loss.item(),\n",
        "            'policy_loss': policy_loss.item(),\n",
        "            'total_loss': total_loss.item()\n",
        "        }\n",
        "\n",
        "\n",
        "# Test Feudal Networks\n",
        "print(\"Testing Feudal Networks...\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = MultiRoomEnv(num_rooms=3, room_size=4)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = FeudalAgent(state_dim, action_dim, goal_dim=8, c=5)\n",
        "\n",
        "# Test action selection\n",
        "obs, _ = env.reset()\n",
        "print(f\"Initial observation: {obs}\")\n",
        "\n",
        "# Select action\n",
        "action, goal = agent.select_action(obs, t=0)\n",
        "print(f\"Selected action: {action}\")\n",
        "print(f\"Manager goal: {goal}\")\n",
        "\n",
        "# Test multiple steps\n",
        "for t in range(3):\n",
        "    action, goal = agent.select_action(obs, t=t)\n",
        "    print(f\"Step {t}: Action = {action}, Goal norm = {np.linalg.norm(goal):.3f}\")\n",
        "\n",
        "print(\"Feudal Networks test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Goal-Conditioned RL with Hindsight Experience Replay\n",
        "\n",
        "Goal-Conditioned RL trains policies to reach diverse goals. Hindsight Experience Replay (HER) improves sample efficiency by learning from failed trajectories by treating achieved states as goals.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GoalConditionedPolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    Universal Value Function Approximator (UVFA) for goal-conditioned RL.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, goal_dim, action_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.goal_dim = goal_dim\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Policy network (state + goal -> action)\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(state_dim + goal_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "        \n",
        "        # Q-value network (state + goal + action -> Q-value)\n",
        "        self.q_network = nn.Sequential(\n",
        "            nn.Linear(state_dim + goal_dim + action_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "        \n",
        "        # Value network (state + goal -> value)\n",
        "        self.value_network = nn.Sequential(\n",
        "            nn.Linear(state_dim + goal_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state, goal):\n",
        "        \"\"\"Forward pass through policy network.\"\"\"\n",
        "        x = torch.cat([state, goal], dim=-1)\n",
        "        return self.policy(x)\n",
        "    \n",
        "    def get_q_value(self, state, goal, action):\n",
        "        \"\"\"Get Q-value for state-goal-action tuple.\"\"\"\n",
        "        x = torch.cat([state, goal, action], dim=-1)\n",
        "        return self.q_network(x)\n",
        "    \n",
        "    def get_value(self, state, goal):\n",
        "        \"\"\"Get value for state-goal pair.\"\"\"\n",
        "        x = torch.cat([state, goal], dim=-1)\n",
        "        return self.value_network(x)\n",
        "\n",
        "\n",
        "class HERBuffer:\n",
        "    \"\"\"\n",
        "    Experience buffer with Hindsight Experience Replay support.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, capacity=100000):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = []\n",
        "        self.position = 0\n",
        "    \n",
        "    def add(self, state, action, reward, next_state, goal, achieved_goal, done):\n",
        "        \"\"\"Add transition to buffer.\"\"\"\n",
        "        if len(self.buffer) < self.capacity:\n",
        "            self.buffer.append(None)\n",
        "        \n",
        "        self.buffer[self.position] = {\n",
        "            'state': state,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state,\n",
        "            'goal': goal,\n",
        "            'achieved_goal': achieved_goal,\n",
        "            'done': done\n",
        "        }\n",
        "        \n",
        "        self.position = (self.position + 1) % self.capacity\n",
        "    \n",
        "    def sample(self, batch_size):\n",
        "        \"\"\"Sample batch from buffer.\"\"\"\n",
        "        return random.sample(self.buffer, batch_size)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "class HERAgent:\n",
        "    \"\"\"\n",
        "    Goal-conditioned RL agent with Hindsight Experience Replay.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, goal_dim, action_dim, lr=3e-4, gamma=0.99, \n",
        "                 her_ratio=0.8, target_update_freq=100):\n",
        "        self.state_dim = state_dim\n",
        "        self.goal_dim = goal_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        self.her_ratio = her_ratio\n",
        "        self.target_update_freq = target_update_freq\n",
        "        \n",
        "        # Networks\n",
        "        self.policy_net = GoalConditionedPolicy(state_dim, goal_dim, action_dim)\n",
        "        self.target_policy_net = GoalConditionedPolicy(state_dim, goal_dim, action_dim)\n",
        "        self.target_policy_net.load_state_dict(self.policy_net.state_dict())\n",
        "        \n",
        "        # Optimizers\n",
        "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.buffer = HERBuffer()\n",
        "        \n",
        "        # Update counter\n",
        "        self.update_count = 0\n",
        "        \n",
        "    def select_action(self, state, goal, epsilon=0.1):\n",
        "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        goal_tensor = torch.FloatTensor(goal).unsqueeze(0)\n",
        "        \n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.randint(self.action_dim)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            action_logits = self.policy_net(state_tensor, goal_tensor)\n",
        "            action_probs = F.softmax(action_logits, dim=-1)\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def store_transition(self, state, action, reward, next_state, goal, achieved_goal, done):\n",
        "        \"\"\"Store transition in buffer.\"\"\"\n",
        "        self.buffer.add(state, action, reward, next_state, goal, achieved_goal, done)\n",
        "    \n",
        "    def compute_reward(self, achieved_goal, desired_goal):\n",
        "        \"\"\"Compute reward based on goal achievement.\"\"\"\n",
        "        # Sparse reward: 1 if goal achieved, 0 otherwise\n",
        "        if np.array_equal(achieved_goal, desired_goal):\n",
        "            return 1.0\n",
        "        return 0.0\n",
        "    \n",
        "    def hindsight_experience_replay(self, trajectory, strategy='future'):\n",
        "        \"\"\"\n",
        "        Generate hindsight experiences from trajectory.\n",
        "        \n",
        "        Args:\n",
        "            trajectory: List of (state, action, reward, next_state, goal, achieved_goal, done)\n",
        "            strategy: 'future', 'final', or 'random'\n",
        "        \"\"\"\n",
        "        states, actions, rewards, next_states, goals, achieved_goals, dones = zip(*trajectory)\n",
        "        \n",
        "        # Add original experiences\n",
        "        for i in range(len(trajectory)):\n",
        "            self.buffer.add(states[i], actions[i], rewards[i], next_states[i], \n",
        "                          goals[i], achieved_goals[i], dones[i])\n",
        "        \n",
        "        # Add hindsight experiences\n",
        "        for t in range(len(trajectory)):\n",
        "            if strategy == 'future':\n",
        "                # Sample future achieved goal\n",
        "                future_indices = list(range(t, len(trajectory)))\n",
        "                if future_indices:\n",
        "                    future_t = np.random.choice(future_indices)\n",
        "                    new_goal = achieved_goals[future_t]\n",
        "            elif strategy == 'final':\n",
        "                # Use final achieved goal\n",
        "                new_goal = achieved_goals[-1]\n",
        "            elif strategy == 'random':\n",
        "                # Sample random achieved goal\n",
        "                new_goal = achieved_goals[np.random.randint(len(achieved_goals))]\n",
        "            \n",
        "            # Recompute rewards with new goal\n",
        "            new_rewards = []\n",
        "            for i in range(len(trajectory)):\n",
        "                new_reward = self.compute_reward(achieved_goals[i], new_goal)\n",
        "                new_rewards.append(new_reward)\n",
        "            \n",
        "            # Add modified experiences\n",
        "            for i in range(len(trajectory)):\n",
        "                self.buffer.add(states[i], actions[i], new_rewards[i], next_states[i],\n",
        "                              new_goal, achieved_goals[i], dones[i])\n",
        "    \n",
        "    def update(self, batch_size=32):\n",
        "        \"\"\"Update policy using DQN with HER.\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = self.buffer.sample(batch_size)\n",
        "        \n",
        "        states = torch.FloatTensor([t['state'] for t in batch])\n",
        "        actions = torch.LongTensor([t['action'] for t in batch])\n",
        "        rewards = torch.FloatTensor([t['reward'] for t in batch])\n",
        "        next_states = torch.FloatTensor([t['next_state'] for t in batch])\n",
        "        goals = torch.FloatTensor([t['goal'] for t in batch])\n",
        "        dones = torch.BoolTensor([t['done'] for t in batch])\n",
        "        \n",
        "        # Current Q-values\n",
        "        current_q_values = self.policy_net.get_q_value(states, goals, actions.unsqueeze(1).float())\n",
        "        \n",
        "        # Target Q-values\n",
        "        with torch.no_grad():\n",
        "            # Get next actions from target policy\n",
        "            next_action_logits = self.target_policy_net(next_states, goals)\n",
        "            next_actions = next_action_logits.argmax(dim=-1, keepdim=True)\n",
        "            \n",
        "            # Get target Q-values\n",
        "            next_q_values = self.target_policy_net.get_q_value(next_states, goals, next_actions.float())\n",
        "            target_q_values = rewards.unsqueeze(1) + self.gamma * next_q_values * (~dones).unsqueeze(1)\n",
        "        \n",
        "        # Q-learning loss\n",
        "        q_loss = F.mse_loss(current_q_values, target_q_values)\n",
        "        \n",
        "        # Update policy\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        q_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        \n",
        "        # Update target network\n",
        "        self.update_count += 1\n",
        "        if self.update_count % self.target_update_freq == 0:\n",
        "            self.target_policy_net.load_state_dict(self.policy_net.state_dict())\n",
        "        \n",
        "        return {'q_loss': q_loss.item()}\n",
        "\n",
        "\n",
        "# Test Goal-Conditioned RL with HER\n",
        "print(\"Testing Goal-Conditioned RL with HER...\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = MultiRoomEnv(num_rooms=3, room_size=4)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "goal_dim = 2  # Goal is (x, y) position\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = HERAgent(state_dim, goal_dim, action_dim)\n",
        "\n",
        "# Test action selection\n",
        "obs, _ = env.reset()\n",
        "goal = np.array([obs[3], obs[4]])  # Goal position from observation\n",
        "\n",
        "action = agent.select_action(obs, goal)\n",
        "print(f\"Selected action: {action}\")\n",
        "\n",
        "# Test reward computation\n",
        "achieved_goal = np.array([obs[0], obs[1]])  # Current position\n",
        "reward = agent.compute_reward(achieved_goal, goal)\n",
        "print(f\"Reward for current position: {reward}\")\n",
        "\n",
        "# Test HER\n",
        "trajectory = [\n",
        "    (obs, action, reward, obs, goal, achieved_goal, False)\n",
        "]\n",
        "agent.hindsight_experience_replay(trajectory, strategy='future')\n",
        "print(f\"Buffer size after HER: {len(agent.buffer)}\")\n",
        "\n",
        "print(\"Goal-Conditioned RL with HER test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Skill Discovery with DIAYN\n",
        "\n",
        "DIAYN (Diversity is All You Need) learns diverse skills without external rewards by maximizing mutual information between skills and states.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SkillConditionedPolicy(nn.Module):\n",
        "    \"\"\"\n",
        "    Policy conditioned on skill (latent variable).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, skill_dim, action_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.skill_dim = skill_dim\n",
        "        self.action_dim = action_dim\n",
        "        \n",
        "        # Policy network (state + skill -> action)\n",
        "        self.policy = nn.Sequential(\n",
        "            nn.Linear(state_dim + skill_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, action_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state, skill):\n",
        "        \"\"\"Forward pass through policy network.\"\"\"\n",
        "        x = torch.cat([state, skill], dim=-1)\n",
        "        return self.policy(x)\n",
        "\n",
        "\n",
        "class SkillDiscriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    Discriminator that predicts skill from state.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, skill_dim, hidden_dim=256):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.state_dim = state_dim\n",
        "        self.skill_dim = skill_dim\n",
        "        \n",
        "        # Discriminator network (state -> skill_logits)\n",
        "        self.discriminator = nn.Sequential(\n",
        "            nn.Linear(state_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, skill_dim)\n",
        "        )\n",
        "    \n",
        "    def forward(self, state):\n",
        "        \"\"\"Forward pass through discriminator network.\"\"\"\n",
        "        return self.discriminator(state)\n",
        "\n",
        "\n",
        "class DIAYNAgent:\n",
        "    \"\"\"\n",
        "    DIAYN agent for unsupervised skill discovery.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, state_dim, skill_dim, action_dim, lr=3e-4, gamma=0.99):\n",
        "        self.state_dim = state_dim\n",
        "        self.skill_dim = skill_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.gamma = gamma\n",
        "        \n",
        "        # Networks\n",
        "        self.policy = SkillConditionedPolicy(state_dim, skill_dim, action_dim)\n",
        "        self.discriminator = SkillDiscriminator(state_dim, skill_dim)\n",
        "        \n",
        "        # Optimizers\n",
        "        self.policy_optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n",
        "        self.discriminator_optimizer = optim.Adam(self.discriminator.parameters(), lr=lr)\n",
        "        \n",
        "        # Experience buffer\n",
        "        self.buffer = []\n",
        "        \n",
        "    def sample_skill(self):\n",
        "        \"\"\"Sample random skill (one-hot vector).\"\"\"\n",
        "        skill_id = np.random.randint(self.skill_dim)\n",
        "        skill = np.zeros(self.skill_dim)\n",
        "        skill[skill_id] = 1.0\n",
        "        return skill, skill_id\n",
        "    \n",
        "    def select_action(self, state, skill, epsilon=0.1):\n",
        "        \"\"\"Select action using skill-conditioned policy.\"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        skill_tensor = torch.FloatTensor(skill).unsqueeze(0)\n",
        "        \n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.randint(self.action_dim)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            action_logits = self.policy(state_tensor, skill_tensor)\n",
        "            action_probs = F.softmax(action_logits, dim=-1)\n",
        "            action = torch.multinomial(action_probs, 1).item()\n",
        "        \n",
        "        return action\n",
        "    \n",
        "    def compute_intrinsic_reward(self, state, skill_id):\n",
        "        \"\"\"\n",
        "        Compute intrinsic reward based on mutual information.\n",
        "        Reward = log p(skill|state) - log p(skill)\n",
        "        \"\"\"\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            skill_logits = self.discriminator(state_tensor)\n",
        "            skill_probs = F.softmax(skill_logits, dim=-1)\n",
        "            \n",
        "            # log p(skill|state)\n",
        "            log_p_skill_given_state = torch.log(skill_probs[0, skill_id] + 1e-8)\n",
        "            \n",
        "            # log p(skill) = log(1/num_skills)\n",
        "            log_p_skill = np.log(1.0 / self.skill_dim)\n",
        "            \n",
        "            # Intrinsic reward\n",
        "            intrinsic_reward = log_p_skill_given_state - log_p_skill\n",
        "        \n",
        "        return intrinsic_reward.item()\n",
        "    \n",
        "    def store_transition(self, state, skill_id, action, reward, next_state, done):\n",
        "        \"\"\"Store transition in buffer.\"\"\"\n",
        "        self.buffer.append({\n",
        "            'state': state,\n",
        "            'skill_id': skill_id,\n",
        "            'action': action,\n",
        "            'reward': reward,\n",
        "            'next_state': next_state,\n",
        "            'done': done\n",
        "        })\n",
        "    \n",
        "    def update_policy(self, batch_size=32):\n",
        "        \"\"\"Update policy using intrinsic rewards.\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        states = torch.FloatTensor([t['state'] for t in batch])\n",
        "        skills = torch.LongTensor([t['skill_id'] for t in batch])\n",
        "        actions = torch.LongTensor([t['action'] for t in batch])\n",
        "        rewards = torch.FloatTensor([t['reward'] for t in batch])\n",
        "        next_states = torch.FloatTensor([t['next_state'] for t in batch])\n",
        "        dones = torch.BoolTensor([t['done'] for t in batch])\n",
        "        \n",
        "        # Convert skills to one-hot\n",
        "        skill_one_hot = F.one_hot(skills, num_classes=self.skill_dim).float()\n",
        "        \n",
        "        # Policy loss (REINFORCE with intrinsic rewards)\n",
        "        action_logits = self.policy(states, skill_one_hot)\n",
        "        log_probs = F.log_softmax(action_logits, dim=-1)\n",
        "        action_log_probs = log_probs.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "        \n",
        "        # Policy gradient loss\n",
        "        policy_loss = -(action_log_probs * rewards).mean()\n",
        "        \n",
        "        # Update policy\n",
        "        self.policy_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.policy_optimizer.step()\n",
        "        \n",
        "        return {'policy_loss': policy_loss.item()}\n",
        "    \n",
        "    def update_discriminator(self, batch_size=32):\n",
        "        \"\"\"Update discriminator to predict skills from states.\"\"\"\n",
        "        if len(self.buffer) < batch_size:\n",
        "            return\n",
        "        \n",
        "        # Sample batch\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        \n",
        "        states = torch.FloatTensor([t['state'] for t in batch])\n",
        "        skills = torch.LongTensor([t['skill_id'] for t in batch])\n",
        "        \n",
        "        # Discriminator loss (cross-entropy)\n",
        "        skill_logits = self.discriminator(states)\n",
        "        discriminator_loss = F.cross_entropy(skill_logits, skills)\n",
        "        \n",
        "        # Update discriminator\n",
        "        self.discriminator_optimizer.zero_grad()\n",
        "        discriminator_loss.backward()\n",
        "        self.discriminator_optimizer.step()\n",
        "        \n",
        "        return {'discriminator_loss': discriminator_loss.item()}\n",
        "    \n",
        "    def update(self, batch_size=32):\n",
        "        \"\"\"Update both policy and discriminator.\"\"\"\n",
        "        policy_losses = self.update_policy(batch_size)\n",
        "        discriminator_losses = self.update_discriminator(batch_size)\n",
        "        \n",
        "        return {**policy_losses, **discriminator_losses}\n",
        "\n",
        "\n",
        "# Test DIAYN\n",
        "print(\"Testing DIAYN Skill Discovery...\")\n",
        "\n",
        "# Create environment and agent\n",
        "env = MultiRoomEnv(num_rooms=3, room_size=4)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "skill_dim = 4  # Number of skills to discover\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "agent = DIAYNAgent(state_dim, skill_dim, action_dim)\n",
        "\n",
        "# Test skill sampling\n",
        "skill, skill_id = agent.sample_skill()\n",
        "print(f\"Sampled skill: {skill}, ID: {skill_id}\")\n",
        "\n",
        "# Test action selection\n",
        "obs, _ = env.reset()\n",
        "action = agent.select_action(obs, skill)\n",
        "print(f\"Action from skill {skill_id}: {action}\")\n",
        "\n",
        "# Test intrinsic reward\n",
        "intrinsic_reward = agent.compute_intrinsic_reward(obs, skill_id)\n",
        "print(f\"Intrinsic reward: {intrinsic_reward:.3f}\")\n",
        "\n",
        "# Test updates\n",
        "agent.store_transition(obs, skill_id, action, 0.0, obs, False)\n",
        "losses = agent.update()\n",
        "print(f\"Update losses: {losses}\")\n",
        "\n",
        "print(\"DIAYN test completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Training and Evaluation Functions\n",
        "\n",
        "Let's implement training and evaluation functions for all hierarchical RL methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_option_critic(env, agent, num_episodes=1000, max_steps=100, \n",
        "                        epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995):\n",
        "    \"\"\"\n",
        "    Train Option-Critic agent.\n",
        "    \"\"\"\n",
        "    epsilon = epsilon_start\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in trange(num_episodes, desc=\"Training Option-Critic\"):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        \n",
        "        # Select initial option\n",
        "        current_option = None\n",
        "        option_steps = 0\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            if current_option is None or option_steps >= 10:  # Option termination\n",
        "                current_option, _ = agent.select_action(obs, epsilon=epsilon)\n",
        "                option_steps = 0\n",
        "            \n",
        "            # Get action from current option\n",
        "            action, should_terminate = agent.select_action(obs, current_option, epsilon=epsilon)\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Store transition\n",
        "            agent.store_transition(obs, current_option, action, reward, next_obs, terminated, truncated)\n",
        "            \n",
        "            # Update agent\n",
        "            if len(agent.buffer) >= 32:\n",
        "                loss_info = agent.update()\n",
        "                losses.append(loss_info)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "            \n",
        "            # Option termination\n",
        "            if should_terminate or done:\n",
        "                current_option = None\n",
        "                option_steps = 0\n",
        "            else:\n",
        "                option_steps += 1\n",
        "            \n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "        \n",
        "        # Decay epsilon\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "    \n",
        "    return episode_rewards, episode_lengths, losses\n",
        "\n",
        "\n",
        "def train_feudal_agent(env, agent, num_episodes=1000, max_steps=100):\n",
        "    \"\"\"\n",
        "    Train Feudal agent.\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in trange(num_episodes, desc=\"Training Feudal Agent\"):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        \n",
        "        # Reset hidden states\n",
        "        agent.manager_hidden = None\n",
        "        agent.worker_hidden = None\n",
        "        \n",
        "        trajectory = []\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action, goal = agent.select_action(obs, t=step)\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Get value estimates\n",
        "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                output = agent.feudal_net(state_tensor, agent.manager_hidden, agent.worker_hidden, step)\n",
        "                manager_value = output['manager_value'].item()\n",
        "                worker_value = output['worker_value'].item()\n",
        "            \n",
        "            # Store transition\n",
        "            agent.store_transition(obs, action, reward, next_obs, goal, manager_value, worker_value, done)\n",
        "            \n",
        "            trajectory.append((obs, action, reward, next_obs, goal, done))\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "            \n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Update agent\n",
        "        if len(agent.buffer) >= 32:\n",
        "            loss_info = agent.update()\n",
        "            losses.append(loss_info)\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "    \n",
        "    return episode_rewards, episode_lengths, losses\n",
        "\n",
        "\n",
        "def train_her_agent(env, agent, num_episodes=1000, max_steps=100, \n",
        "                   epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995):\n",
        "    \"\"\"\n",
        "    Train HER agent.\n",
        "    \"\"\"\n",
        "    epsilon = epsilon_start\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in trange(num_episodes, desc=\"Training HER Agent\"):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        \n",
        "        # Set goal (target position)\n",
        "        goal = np.array([obs[3], obs[4]])  # Goal position from observation\n",
        "        \n",
        "        trajectory = []\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action = agent.select_action(obs, goal, epsilon=epsilon)\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Compute achieved goal and reward\n",
        "            achieved_goal = np.array([next_obs[0], next_obs[1]])  # Current position\n",
        "            her_reward = agent.compute_reward(achieved_goal, goal)\n",
        "            \n",
        "            # Store transition\n",
        "            agent.store_transition(obs, action, her_reward, next_obs, goal, achieved_goal, done)\n",
        "            \n",
        "            trajectory.append((obs, action, her_reward, next_obs, goal, achieved_goal, done))\n",
        "            \n",
        "            episode_reward += her_reward\n",
        "            episode_length += 1\n",
        "            \n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Apply HER\n",
        "        agent.hindsight_experience_replay(trajectory, strategy='future')\n",
        "        \n",
        "        # Update agent\n",
        "        if len(agent.buffer) >= 32:\n",
        "            loss_info = agent.update()\n",
        "            losses.append(loss_info)\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "        \n",
        "        # Decay epsilon\n",
        "        epsilon = max(epsilon_end, epsilon * epsilon_decay)\n",
        "    \n",
        "    return episode_rewards, episode_lengths, losses\n",
        "\n",
        "\n",
        "def train_diayn_agent(env, agent, num_episodes=1000, max_steps=100):\n",
        "    \"\"\"\n",
        "    Train DIAYN agent.\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in trange(num_episodes, desc=\"Training DIAYN Agent\"):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        \n",
        "        # Sample skill\n",
        "        skill, skill_id = agent.sample_skill()\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action = agent.select_action(obs, skill)\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, _ = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Compute intrinsic reward\n",
        "            intrinsic_reward = agent.compute_intrinsic_reward(next_obs, skill_id)\n",
        "            \n",
        "            # Store transition\n",
        "            agent.store_transition(obs, skill_id, action, intrinsic_reward, next_obs, done)\n",
        "            \n",
        "            episode_reward += intrinsic_reward\n",
        "            episode_length += 1\n",
        "            \n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Update agent\n",
        "        if len(agent.buffer) >= 32:\n",
        "            loss_info = agent.update()\n",
        "            losses.append(loss_info)\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "    \n",
        "    return episode_rewards, episode_lengths, losses\n",
        "\n",
        "\n",
        "def evaluate_agent(env, agent, num_episodes=100, method='option_critic'):\n",
        "    \"\"\"\n",
        "    Evaluate agent performance.\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "    episode_lengths = []\n",
        "    success_rate = 0\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        \n",
        "        if method == 'option_critic':\n",
        "            current_option = None\n",
        "            option_steps = 0\n",
        "            \n",
        "            for step in range(100):  # Max steps\n",
        "                if current_option is None or option_steps >= 10:\n",
        "                    current_option, _ = agent.select_action(obs, epsilon=0.0)\n",
        "                    option_steps = 0\n",
        "                \n",
        "                action, should_terminate = agent.select_action(obs, current_option, epsilon=0.0)\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                \n",
        "                episode_reward += reward\n",
        "                episode_length += 1\n",
        "                \n",
        "                if should_terminate or done:\n",
        "                    current_option = None\n",
        "                    option_steps = 0\n",
        "                else:\n",
        "                    option_steps += 1\n",
        "                \n",
        "                if done:\n",
        "                    if reward > 0:  # Success\n",
        "                        success_rate += 1\n",
        "                    break\n",
        "        \n",
        "        elif method == 'feudal':\n",
        "            agent.manager_hidden = None\n",
        "            agent.worker_hidden = None\n",
        "            \n",
        "            for step in range(100):\n",
        "                action, goal = agent.select_action(obs, t=step)\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                \n",
        "                episode_reward += reward\n",
        "                episode_length += 1\n",
        "                \n",
        "                if done:\n",
        "                    if reward > 0:\n",
        "                        success_rate += 1\n",
        "                    break\n",
        "        \n",
        "        elif method == 'her':\n",
        "            goal = np.array([obs[3], obs[4]])\n",
        "            \n",
        "            for step in range(100):\n",
        "                action = agent.select_action(obs, goal, epsilon=0.0)\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                \n",
        "                achieved_goal = np.array([obs[0], obs[1]])\n",
        "                her_reward = agent.compute_reward(achieved_goal, goal)\n",
        "                episode_reward += her_reward\n",
        "                episode_length += 1\n",
        "                \n",
        "                if done:\n",
        "                    if her_reward > 0:\n",
        "                        success_rate += 1\n",
        "                    break\n",
        "        \n",
        "        elif method == 'diayn':\n",
        "            skill, skill_id = agent.sample_skill()\n",
        "            \n",
        "            for step in range(100):\n",
        "                action = agent.select_action(obs, skill, epsilon=0.0)\n",
        "                obs, reward, terminated, truncated, _ = env.step(action)\n",
        "                done = terminated or truncated\n",
        "                \n",
        "                intrinsic_reward = agent.compute_intrinsic_reward(obs, skill_id)\n",
        "                episode_reward += intrinsic_reward\n",
        "                episode_length += 1\n",
        "                \n",
        "                if done:\n",
        "                    break\n",
        "        \n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_lengths.append(episode_length)\n",
        "    \n",
        "    success_rate /= num_episodes\n",
        "    \n",
        "    return {\n",
        "        'mean_reward': np.mean(episode_rewards),\n",
        "        'std_reward': np.std(episode_rewards),\n",
        "        'mean_length': np.mean(episode_lengths),\n",
        "        'std_length': np.std(episode_lengths),\n",
        "        'success_rate': success_rate\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_training_results(results, title=\"Training Results\"):\n",
        "    \"\"\"\n",
        "    Plot training results for different methods.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Plot rewards\n",
        "    ax1 = axes[0, 0]\n",
        "    for method, data in results.items():\n",
        "        rewards = data['rewards']\n",
        "        # Smooth rewards\n",
        "        window = min(50, len(rewards) // 10)\n",
        "        if window > 1:\n",
        "            smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "            ax1.plot(smoothed, label=f\"{method} (smoothed)\")\n",
        "        else:\n",
        "            ax1.plot(rewards, label=method, alpha=0.3)\n",
        "    \n",
        "    ax1.set_title('Episode Rewards')\n",
        "    ax1.set_xlabel('Episode')\n",
        "    ax1.set_ylabel('Reward')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "    \n",
        "    # Plot episode lengths\n",
        "    ax2 = axes[0, 1]\n",
        "    for method, data in results.items():\n",
        "        lengths = data['lengths']\n",
        "        window = min(50, len(lengths) // 10)\n",
        "        if window > 1:\n",
        "            smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
        "            ax2.plot(smoothed, label=f\"{method} (smoothed)\")\n",
        "        else:\n",
        "            ax2.plot(lengths, label=method, alpha=0.3)\n",
        "    \n",
        "    ax2.set_title('Episode Lengths')\n",
        "    ax2.set_xlabel('Episode')\n",
        "    ax2.set_ylabel('Length')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "    \n",
        "    # Plot evaluation results\n",
        "    ax3 = axes[1, 0]\n",
        "    methods = list(results.keys())\n",
        "    mean_rewards = [results[method]['eval']['mean_reward'] for method in methods]\n",
        "    std_rewards = [results[method]['eval']['std_reward'] for method in methods]\n",
        "    \n",
        "    ax3.bar(methods, mean_rewards, yerr=std_rewards, capsize=5)\n",
        "    ax3.set_title('Evaluation Rewards')\n",
        "    ax3.set_ylabel('Mean Reward')\n",
        "    ax3.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot success rates\n",
        "    ax4 = axes[1, 1]\n",
        "    success_rates = [results[method]['eval']['success_rate'] for method in methods]\n",
        "    \n",
        "    ax4.bar(methods, success_rates)\n",
        "    ax4.set_title('Success Rates')\n",
        "    ax4.set_ylabel('Success Rate')\n",
        "    ax4.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"Training and evaluation functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training and Evaluation Functions\n",
        "\n",
        "def train_option_critic(env, agent, num_episodes, max_steps):\n",
        "    \"\"\"Train Option-Critic agent.\"\"\"\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in trange(num_episodes, desc=\"Training Option-Critic\"):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        current_option = None\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            if current_option is None:\n",
        "                # Select new option\n",
        "                current_option, _ = agent.select_action(obs)\n",
        "            \n",
        "            # Get action from current option\n",
        "            action, should_terminate = agent.select_action(obs, current_option)\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Store transition\n",
        "            agent.store_transition(obs, current_option, action, reward, next_obs, terminated, truncated)\n",
        "            \n",
        "            # Update agent\n",
        "            if len(agent.buffer) >= 32:\n",
        "                loss_info = agent.update()\n",
        "                losses.append(loss_info)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "            \n",
        "            # Check termination\n",
        "            if should_terminate or done:\n",
        "                current_option = None\n",
        "            \n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        rewards.append(episode_reward)\n",
        "        lengths.append(episode_length)\n",
        "    \n",
        "    return rewards, lengths, losses\n",
        "\n",
        "\n",
        "def train_feudal_agent(env, agent, num_episodes, max_steps):\n",
        "    \"\"\"Train Feudal agent.\"\"\"\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in trange(num_episodes, desc=\"Training Feudal\"):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        \n",
        "        # Reset hidden states\n",
        "        agent.manager_hidden = None\n",
        "        agent.worker_hidden = None\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action, goal = agent.select_action(obs, t=step)\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Get value estimates\n",
        "            state_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                output = agent.feudal_net(state_tensor, agent.manager_hidden, agent.worker_hidden, step)\n",
        "                manager_value = output['manager_value'].item()\n",
        "                worker_value = output['worker_value'].item()\n",
        "            \n",
        "            # Store transition\n",
        "            agent.store_transition(obs, action, reward, next_obs, goal, manager_value, worker_value, done)\n",
        "            \n",
        "            # Update agent\n",
        "            if len(agent.buffer) >= 32:\n",
        "                loss_info = agent.update()\n",
        "                losses.append(loss_info)\n",
        "            \n",
        "            episode_reward += reward\n",
        "            episode_length += 1\n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        rewards.append(episode_reward)\n",
        "        lengths.append(episode_length)\n",
        "    \n",
        "    return rewards, lengths, losses\n",
        "\n",
        "\n",
        "def train_her_agent(env, agent, num_episodes, max_steps):\n",
        "    \"\"\"Train HER agent.\"\"\"\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in trange(num_episodes, desc=\"Training HER\"):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        \n",
        "        # Set goal (target position)\n",
        "        goal = np.array([obs[3], obs[4]])  # Goal position from observation\n",
        "        \n",
        "        trajectory = []\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action = agent.select_action(obs, goal)\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Compute reward based on goal achievement\n",
        "            achieved_goal = np.array([next_obs[0], next_obs[1]])  # Current position\n",
        "            her_reward = agent.compute_reward(achieved_goal, goal)\n",
        "            \n",
        "            # Store transition\n",
        "            agent.store_transition(obs, action, her_reward, next_obs, goal, achieved_goal, done)\n",
        "            \n",
        "            # Add to trajectory for HER\n",
        "            trajectory.append((obs, action, her_reward, next_obs, goal, achieved_goal, done))\n",
        "            \n",
        "            # Update agent\n",
        "            if len(agent.buffer) >= 32:\n",
        "                loss_info = agent.update()\n",
        "                losses.append(loss_info)\n",
        "            \n",
        "            episode_reward += her_reward\n",
        "            episode_length += 1\n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Apply HER to trajectory\n",
        "        if len(trajectory) > 1:\n",
        "            agent.hindsight_experience_replay(trajectory, strategy='future')\n",
        "        \n",
        "        rewards.append(episode_reward)\n",
        "        lengths.append(episode_length)\n",
        "    \n",
        "    return rewards, lengths, losses\n",
        "\n",
        "\n",
        "def train_diayn_agent(env, agent, num_episodes, max_steps):\n",
        "    \"\"\"Train DIAYN agent.\"\"\"\n",
        "    rewards = []\n",
        "    lengths = []\n",
        "    losses = []\n",
        "    \n",
        "    for episode in trange(num_episodes, desc=\"Training DIAYN\"):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        episode_length = 0\n",
        "        \n",
        "        # Sample skill for this episode\n",
        "        skill, skill_id = agent.sample_skill()\n",
        "        \n",
        "        for step in range(max_steps):\n",
        "            # Select action\n",
        "            action = agent.select_action(obs, skill)\n",
        "            \n",
        "            # Execute action\n",
        "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            \n",
        "            # Compute intrinsic reward\n",
        "            intrinsic_reward = agent.compute_intrinsic_reward(next_obs, skill_id)\n",
        "            \n",
        "            # Store transition\n",
        "            agent.store_transition(obs, skill_id, action, intrinsic_reward, next_obs, done)\n",
        "            \n",
        "            # Update agent\n",
        "            if len(agent.buffer) >= 32:\n",
        "                loss_info = agent.update()\n",
        "                losses.append(loss_info)\n",
        "            \n",
        "            episode_reward += intrinsic_reward\n",
        "            episode_length += 1\n",
        "            obs = next_obs\n",
        "            \n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        rewards.append(episode_reward)\n",
        "        lengths.append(episode_length)\n",
        "    \n",
        "    return rewards, lengths, losses\n",
        "\n",
        "\n",
        "def evaluate_agent(env, agent, num_episodes, agent_type):\n",
        "    \"\"\"Evaluate agent performance.\"\"\"\n",
        "    rewards = []\n",
        "    successes = []\n",
        "    \n",
        "    for episode in range(num_episodes):\n",
        "        obs, _ = env.reset()\n",
        "        episode_reward = 0\n",
        "        success = False\n",
        "        \n",
        "        # Reset agent state if needed\n",
        "        if agent_type == 'feudal':\n",
        "            agent.manager_hidden = None\n",
        "            agent.worker_hidden = None\n",
        "        \n",
        "        for step in range(50):  # Max 50 steps for evaluation\n",
        "            if agent_type == 'option_critic':\n",
        "                if step == 0:\n",
        "                    current_option, _ = agent.select_action(obs, epsilon=0.0)\n",
        "                action, should_terminate = agent.select_action(obs, current_option, epsilon=0.0)\n",
        "                if should_terminate:\n",
        "                    current_option = None\n",
        "            elif agent_type == 'feudal':\n",
        "                action, goal = agent.select_action(obs, t=step)\n",
        "            elif agent_type == 'her':\n",
        "                goal = np.array([obs[3], obs[4]])\n",
        "                action = agent.select_action(obs, goal, epsilon=0.0)\n",
        "            elif agent_type == 'diayn':\n",
        "                # Use first skill for evaluation\n",
        "                skill = np.zeros(agent.skill_dim)\n",
        "                skill[0] = 1.0\n",
        "                action = agent.select_action(obs, skill, epsilon=0.0)\n",
        "            else:\n",
        "                action = env.action_space.sample()\n",
        "            \n",
        "            obs, reward, terminated, truncated, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            \n",
        "            if terminated:\n",
        "                success = True\n",
        "                break\n",
        "        \n",
        "        rewards.append(episode_reward)\n",
        "        successes.append(success)\n",
        "    \n",
        "    return {\n",
        "        'mean_reward': np.mean(rewards),\n",
        "        'std_reward': np.std(rewards),\n",
        "        'success_rate': np.mean(successes),\n",
        "        'rewards': rewards,\n",
        "        'successes': successes\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_results(results):\n",
        "    \"\"\"Plot training results.\"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    \n",
        "    # Plot rewards\n",
        "    ax = axes[0, 0]\n",
        "    for method, data in results.items():\n",
        "        rewards = data['rewards']\n",
        "        # Smooth rewards\n",
        "        window = max(1, len(rewards) // 20)\n",
        "        smoothed = np.convolve(rewards, np.ones(window)/window, mode='valid')\n",
        "        ax.plot(smoothed, label=method, alpha=0.8)\n",
        "    ax.set_title('Training Rewards')\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Reward')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    \n",
        "    # Plot episode lengths\n",
        "    ax = axes[0, 1]\n",
        "    for method, data in results.items():\n",
        "        lengths = data['lengths']\n",
        "        # Smooth lengths\n",
        "        window = max(1, len(lengths) // 20)\n",
        "        smoothed = np.convolve(lengths, np.ones(window)/window, mode='valid')\n",
        "        ax.plot(smoothed, label=method, alpha=0.8)\n",
        "    ax.set_title('Episode Lengths')\n",
        "    ax.set_xlabel('Episode')\n",
        "    ax.set_ylabel('Steps')\n",
        "    ax.legend()\n",
        "    ax.grid(True)\n",
        "    \n",
        "    # Plot evaluation results\n",
        "    ax = axes[1, 0]\n",
        "    methods = list(results.keys())\n",
        "    mean_rewards = [results[method]['eval']['mean_reward'] for method in methods]\n",
        "    ax.bar(methods, mean_rewards, alpha=0.7)\n",
        "    ax.set_title('Mean Evaluation Rewards')\n",
        "    ax.set_ylabel('Reward')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    # Plot success rates\n",
        "    ax = axes[1, 1]\n",
        "    success_rates = [results[method]['eval']['success_rate'] for method in methods]\n",
        "    ax.bar(methods, success_rates, alpha=0.7, color='green')\n",
        "    ax.set_title('Success Rates')\n",
        "    ax.set_ylabel('Success Rate')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "print(\"Training and evaluation functions defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Experiments and Comparison\n",
        "\n",
        "Let's run experiments comparing different hierarchical RL methods on the multi-room navigation task.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Run Experiments\n",
        "\n",
        "# Set experiment parameters\n",
        "NUM_EPISODES = 500  # Reduced for faster execution\n",
        "MAX_STEPS = 50\n",
        "EVAL_EPISODES = 50\n",
        "\n",
        "# Create environment\n",
        "env = MultiRoomEnv(num_rooms=3, room_size=4)\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "print(\"Starting Hierarchical RL Experiments...\")\n",
        "print(f\"Environment: {state_dim}D state, {action_dim} actions\")\n",
        "print(f\"Training: {NUM_EPISODES} episodes, {MAX_STEPS} max steps\")\n",
        "print(f\"Evaluation: {EVAL_EPISODES} episodes\")\n",
        "print()\n",
        "\n",
        "# Initialize agents\n",
        "agents = {}\n",
        "\n",
        "# Option-Critic\n",
        "print(\"Initializing Option-Critic...\")\n",
        "num_options = 3\n",
        "agents['Option-Critic'] = OptionLearningAgent(state_dim, num_options, action_dim)\n",
        "\n",
        "# Feudal Networks\n",
        "print(\"Initializing Feudal Networks...\")\n",
        "agents['Feudal'] = FeudalAgent(state_dim, action_dim, goal_dim=8, c=5)\n",
        "\n",
        "# HER\n",
        "print(\"Initializing HER...\")\n",
        "goal_dim = 2\n",
        "agents['HER'] = HERAgent(state_dim, goal_dim, action_dim)\n",
        "\n",
        "# DIAYN\n",
        "print(\"Initializing DIAYN...\")\n",
        "skill_dim = 4\n",
        "agents['DIAYN'] = DIAYNAgent(state_dim, skill_dim, action_dim)\n",
        "\n",
        "print(\"All agents initialized!\")\n",
        "print()\n",
        "\n",
        "# Training results storage\n",
        "results = {}\n",
        "\n",
        "# Train Option-Critic\n",
        "print(\"Training Option-Critic...\")\n",
        "rewards, lengths, losses = train_option_critic(env, agents['Option-Critic'], \n",
        "                                                NUM_EPISODES, MAX_STEPS)\n",
        "eval_results = evaluate_agent(env, agents['Option-Critic'], EVAL_EPISODES, 'option_critic')\n",
        "results['Option-Critic'] = {\n",
        "    'rewards': rewards,\n",
        "    'lengths': lengths,\n",
        "    'losses': losses,\n",
        "    'eval': eval_results\n",
        "}\n",
        "print(f\"Option-Critic - Mean Reward: {eval_results['mean_reward']:.2f}, \"\n",
        "      f\"Success Rate: {eval_results['success_rate']:.2f}\")\n",
        "print()\n",
        "\n",
        "# Train Feudal Networks\n",
        "print(\"Training Feudal Networks...\")\n",
        "rewards, lengths, losses = train_feudal_agent(env, agents['Feudal'], \n",
        "                                              NUM_EPISODES, MAX_STEPS)\n",
        "eval_results = evaluate_agent(env, agents['Feudal'], EVAL_EPISODES, 'feudal')\n",
        "results['Feudal'] = {\n",
        "    'rewards': rewards,\n",
        "    'lengths': lengths,\n",
        "    'losses': losses,\n",
        "    'eval': eval_results\n",
        "}\n",
        "print(f\"Feudal - Mean Reward: {eval_results['mean_reward']:.2f}, \"\n",
        "      f\"Success Rate: {eval_results['success_rate']:.2f}\")\n",
        "print()\n",
        "\n",
        "# Train HER\n",
        "print(\"Training HER...\")\n",
        "rewards, lengths, losses = train_her_agent(env, agents['HER'], \n",
        "                                          NUM_EPISODES, MAX_STEPS)\n",
        "eval_results = evaluate_agent(env, agents['HER'], EVAL_EPISODES, 'her')\n",
        "results['HER'] = {\n",
        "    'rewards': rewards,\n",
        "    'lengths': lengths,\n",
        "    'losses': losses,\n",
        "    'eval': eval_results\n",
        "}\n",
        "print(f\"HER - Mean Reward: {eval_results['mean_reward']:.2f}, \"\n",
        "      f\"Success Rate: {eval_results['success_rate']:.2f}\")\n",
        "print()\n",
        "\n",
        "# Train DIAYN\n",
        "print(\"Training DIAYN...\")\n",
        "rewards, lengths, losses = train_diayn_agent(env, agents['DIAYN'], \n",
        "                                            NUM_EPISODES, MAX_STEPS)\n",
        "eval_results = evaluate_agent(env, agents['DIAYN'], EVAL_EPISODES, 'diayn')\n",
        "results['DIAYN'] = {\n",
        "    'rewards': rewards,\n",
        "    'lengths': lengths,\n",
        "    'losses': losses,\n",
        "    'eval': eval_results\n",
        "}\n",
        "print(f\"DIAYN - Mean Reward: {eval_results['mean_reward']:.2f}, \"\n",
        "      f\"Success Rate: {eval_results['success_rate']:.2f}\")\n",
        "print()\n",
        "\n",
        "print(\"All experiments completed!\")\n",
        "\n",
        "# Plot results\n",
        "print(\"\\\\nPlotting results...\")\n",
        "plot_results(results)\n",
        "\n",
        "# Print summary\n",
        "print(\"\\\\n=== EXPERIMENT SUMMARY ===\")\n",
        "print(\"Method Comparison:\")\n",
        "for method, data in results.items():\n",
        "    eval_data = data['eval']\n",
        "    print(f\"{method:15} - Reward: {eval_data['mean_reward']:6.2f} ± {eval_data['std_reward']:5.2f}, \"\n",
        "          f\"Success: {eval_data['success_rate']:5.2f}\")\n",
        "\n",
        "print(\"\\\\nKey Insights:\")\n",
        "print(\"1. Hierarchical methods can learn complex behaviors through abstraction\")\n",
        "print(\"2. Different methods excel in different scenarios:\")\n",
        "print(\"   - Options: Good for temporal abstraction\")\n",
        "print(\"   - Feudal: Good for spatial abstraction\") \n",
        "print(\"   - HER: Good for sparse reward problems\")\n",
        "print(\"   - DIAYN: Good for skill discovery without rewards\")\n",
        "print(\"3. The choice of method depends on the specific problem structure\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# @title Plot Results\n",
        "\n",
        "# Plot training results\n",
        "plot_training_results(results, \"Hierarchical RL Methods Comparison\")\n",
        "\n",
        "# Print detailed results\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"EXPERIMENTAL RESULTS SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for method, data in results.items():\n",
        "    eval_data = data['eval']\n",
        "    print(f\"\\\\n{method}:\")\n",
        "    print(f\"  Mean Reward: {eval_data['mean_reward']:.3f} ± {eval_data['std_reward']:.3f}\")\n",
        "    print(f\"  Mean Length: {eval_data['mean_length']:.1f} ± {eval_data['std_length']:.1f}\")\n",
        "    print(f\"  Success Rate: {eval_data['success_rate']:.3f}\")\n",
        "\n",
        "print(\"\\\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Find best performing method\n",
        "best_method = max(results.keys(), key=lambda k: results[k]['eval']['success_rate'])\n",
        "print(f\"\\\\nBest performing method: {best_method}\")\n",
        "print(f\"Success rate: {results[best_method]['eval']['success_rate']:.3f}\")\n",
        "\n",
        "# Compare methods\n",
        "print(\"\\\\nMethod Comparison:\")\n",
        "print(\"- Option-Critic: Learns temporally extended actions (options)\")\n",
        "print(\"- Feudal Networks: Manager-worker hierarchy with goal communication\")\n",
        "print(\"- HER: Goal-conditioned RL with hindsight experience replay\")\n",
        "print(\"- DIAYN: Unsupervised skill discovery through mutual information\")\n",
        "\n",
        "print(\"\\\\nKey Insights:\")\n",
        "print(\"1. Hierarchical methods can learn complex behaviors through abstraction\")\n",
        "print(\"2. Different methods excel in different scenarios:\")\n",
        "print(\"   - Options: Good for temporal abstraction\")\n",
        "print(\"   - Feudal: Good for spatial abstraction\")\n",
        "print(\"   - HER: Good for sparse reward problems\")\n",
        "print(\"   - DIAYN: Good for skill discovery without rewards\")\n",
        "print(\"3. The choice of method depends on the specific problem structure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Analysis and Questions\n",
        "\n",
        "### Key Concepts Demonstrated\n",
        "\n",
        "1. **Temporal Abstraction**: Options framework allows agents to plan at multiple time scales\n",
        "2. **Spatial Abstraction**: Feudal hierarchies enable planning across different spatial scales\n",
        "3. **Goal-Conditioned Learning**: HER enables learning from failed trajectories\n",
        "4. **Skill Discovery**: DIAYN learns diverse skills without external rewards\n",
        "\n",
        "### Discussion Questions\n",
        "\n",
        "**Answer the following questions based on your experiments:**\n",
        "\n",
        "1. **Temporal Abstraction**: How does the Options framework help with long-horizon planning compared to flat RL? What are the trade-offs?\n",
        "\n",
        "2. **Hierarchical Communication**: In Feudal Networks, how does the manager-worker communication work? What happens when the communication is noisy or delayed?\n",
        "\n",
        "3. **Sample Efficiency**: Why is HER so effective for sparse reward problems? How does it compare to other exploration strategies?\n",
        "\n",
        "4. **Skill Discovery**: What kinds of skills does DIAYN discover? How could these skills be used for downstream tasks?\n",
        "\n",
        "5. **Method Selection**: Given a new hierarchical RL problem, how would you choose between these methods? What factors would influence your decision?\n",
        "\n",
        "### Extensions and Future Work\n",
        "\n",
        "- **Combining Methods**: How could you combine multiple hierarchical approaches?\n",
        "- **Automatic Hierarchy**: How could the hierarchy structure be learned automatically?\n",
        "- **Transfer Learning**: How could skills learned with DIAYN be transferred to new tasks?\n",
        "- **Real-World Applications**: What real-world problems would benefit from hierarchical RL?\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Hierarchical Reinforcement Learning provides powerful tools for solving complex, long-horizon tasks by decomposing them into simpler subtasks. The methods explored in this assignment demonstrate different approaches to temporal and spatial abstraction, each with their own strengths and applications.\n",
        "\n",
        "The key insight is that **abstraction enables agents to reason at multiple levels**, from high-level strategic planning to low-level tactical execution, making complex problems more tractable and enabling better sample efficiency and generalization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
