{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3YDSMAiek78"
      },
      "source": [
        "# Week 6 Workshop: Multi-Armed Bandits\n",
        "\n",
        "In this notebook, we will explore the core concepts of **multi-armed bandits**, focusing on:\n",
        "\n",
        "1. Definitions and properties (based on the recitation session).\n",
        "2. Action-value methods and uncertainty in bandits.\n",
        "3. Exploration-exploitation strategies:\n",
        "   - $\\epsilon$-greedy\n",
        "   - Optimistic initial values\n",
        "   - Upper Confidence Bound (UCB)\n",
        "   - Thompson Sampling\n",
        "4. Contextual bandits\n",
        "5. Real-world applications:\n",
        "   - Recommender Systems\n",
        "   - Search\n",
        "6. Summary and key takeaways\n",
        "\n",
        "We'll show **pure Python implementations** and **Gymnasium-based** implementations, so you can see how each algorithm might look in a more standardized RL environment setup.\n",
        "\n",
        "\n",
        "> **Note:** In typical RL frameworks, a bandit is treated as either a single-step environment (reset after each pull) or a multi-step environment with a fixed horizon. The examples below choose a **multi-step environment with a fixed horizon** to mimic `num_steps` pulls in a single episode.\n",
        "\n",
        "\n",
        "\n",
        "By the end of this notebook, you should:\n",
        "- Understand the **multi-armed bandit problem** and how it differs from full RL (non-associative property).\n",
        "- Implement various exploration methods ($\\epsilon$-greedy, UCB, etc.), both purely and via Gymnasium.\n",
        "- Get an introduction to **contextual bandits** and how they generalize the bandit paradigm.\n",
        "- See how these ideas apply to **recommender systems** and **search**."
      ],
      "id": "k3YDSMAiek78"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-ZQXoj6ek7-"
      },
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "We'll import:\n",
        "- `numpy` for numerical operations\n",
        "- `matplotlib` for plotting\n",
        "- `gymnasium` to demonstrate how to wrap the bandit environment in the standard Gym interface\n",
        "\n",
        "We also set a random seed for reproducibility."
      ],
      "id": "S-ZQXoj6ek7-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9e5RHUYek7_"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "x9e5RHUYek7_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llhVKi7Gek7_"
      },
      "source": [
        "## 2. Multi-Armed Bandits: Recap\n",
        "\n",
        "### 2.1 Definition of the Problem and Non-Associativity Property\n",
        "\n",
        "From the recitation session:\n",
        "\n",
        "A **multi-armed bandit (MAB)** is a fundamental reinforcement learning setting modeling **decision-making under uncertainty**. The problem is often described as having $k$ different slot machines (arms), each with an **unknown** distribution of rewards.\n",
        "\n",
        "- We have **$ k $** arms/actions.\n",
        "- Each arm $ a $ has an **unknown expected reward** $ q_*(a) $:\n",
        "  $$\n",
        "  q_*(a) = \\mathbb{E}[R \\mid A = a],\n",
        "  $$\n",
        "  where $ R $ is the reward for choosing action $ A $.\n",
        "- We want to **maximize the cumulative reward**:\n",
        "  $$\n",
        "  G = \\sum_{t=1}^{T} R_t.\n",
        "  $$\n",
        "- We **do not** know $ q_*(a) $ and must **estimate** these values over time.\n",
        "\n",
        "**Non-Associativity**:\n",
        "- Unlike a full RL problem, multi-armed bandits are **non-associative**—there are no distinct states or transitions.\n",
        "- The best action does not depend on any \"state\"; the challenge is figuring out which action yields the highest expected reward overall."
      ],
      "id": "llhVKi7Gek7_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRvHxXMfek7_"
      },
      "source": [
        "### 2.2 Action-Value Methods and Types\n",
        "\n",
        "From the recitation session:\n",
        "\n",
        "We define the **action-value** $ Q_t(a) $ as our estimate of $ q_*(a) $. Using **sample-average methods**:\n",
        "$$\n",
        "Q_t(a) = \\frac{1}{N_t(a)} \\sum_{i=1}^{N_t(a)} R_i,\n",
        "$$\n",
        "where $ N_t(a) $ is the number of times action $ a $ has been chosen up to time $ t $, and $ R_i $ are the observed rewards.\n",
        "\n",
        "**Incremental Update Rule** (to avoid storing entire histories):\n",
        "$$\n",
        "Q_{t+1}(a) = Q_t(a) + \\frac{1}{N_t(a)}(R_t - Q_t(a)).\n",
        "$$\n",
        "\n",
        "#### Constant Step-Size for Nonstationary Problems\n",
        "When rewards change over time, we use **constant step-size** $ \\alpha $:\n",
        "$$\n",
        "Q_{t+1}(a) = Q_t(a) + \\alpha (R_t - Q_t(a)).\n",
        "$$\n",
        "- Large $\\alpha$ gives more weight to recent data, useful for **nonstationary** environments."
      ],
      "id": "WRvHxXMfek7_"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCA-sLOXek8A"
      },
      "source": [
        "### 2.3 Exploration-Exploitation Dilemma and Uncertainty\n",
        "\n",
        "From the recitation session:\n",
        "\n",
        "A core challenge is balancing **exploration** and **exploitation**:\n",
        "- **Exploration**: try different actions to reduce uncertainty in their value estimates.\n",
        "- **Exploitation**: select the best-known action to maximize immediate reward.\n",
        "\n",
        "Mathematically, if an action is not selected often, $ N_t(a) $ remains small, so the **uncertainty** in its estimate is large. An agent must ensure all actions are sufficiently explored to accurately approximate $ q_*(a) $."
      ],
      "id": "lCA-sLOXek8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rdx_cQmek8A"
      },
      "source": [
        "## 3. Exploration in Bandits\n",
        "\n",
        "We now discuss the exploration strategies described in the recitation session, plus some additional ones and extra details. We'll first show **plain Python** implementations, then **Gymnasium-based** versions."
      ],
      "id": "3rdx_cQmek8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CjV7RzUWek8A"
      },
      "source": [
        "### 3.1 $\\epsilon$-Greedy\n",
        "\n",
        "From the recitation:\n",
        "\n",
        "> **$\\epsilon$-greedy**:\n",
        "> $$\n",
        "> A_t =\n",
        "> \\begin{cases}\n",
        "> \\arg\\max_a Q_t(a), & \\text{with probability } 1-\\epsilon \\\\\n",
        "> \\text{random } a, & \\text{with probability } \\epsilon.\n",
        "> \\end{cases}\n",
        "> $$\n",
        "\n",
        "- Simple to implement.\n",
        "- Guarantees each arm is sampled (with probability $\\epsilon$).\n",
        "- Can be inefficient if $\\epsilon$ is large because it spends too much time on suboptimal arms.\n",
        "\n",
        "#### 3.1.1 Plain Python: Bernoulli Bandit and $\\epsilon$-Greedy"
      ],
      "id": "CjV7RzUWek8A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-ZncU5qek8A",
        "outputId": "168b5725-d66c-4057-d52f-ea8f1497d2dc"
      },
      "source": [
        "class BernoulliBandit:\n",
        "    \"\"\"\n",
        "    A simple k-armed Bernoulli bandit.\n",
        "    Each arm i has a probability p[i] of returning reward=1.\n",
        "    \"\"\"\n",
        "    def __init__(self, p):\n",
        "        self.p = np.array(p)\n",
        "        self.k = len(p)\n",
        "\n",
        "    def step(self, action):\n",
        "        # Returns 1 with probability p[action], else 0\n",
        "        return 1 if np.random.rand() < self.p[action] else 0\n",
        "\n",
        "def epsilon_greedy(bandit, num_steps=1000, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Epsilon-greedy action selection.\n",
        "    bandit: an instance of BernoulliBandit\n",
        "    num_steps: total number of pulls\n",
        "    epsilon: exploration rate\n",
        "    \"\"\"\n",
        "    k = bandit.k\n",
        "    Q = np.zeros(k) # Estimated values of arms\n",
        "    N = np.zeros(k) # Count of pulls for each arm\n",
        "\n",
        "    rewards_history = []\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        # Explore vs Exploit\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(k) # Explore\n",
        "        else:\n",
        "            action = np.argmax(Q) # Exploit\n",
        "\n",
        "        reward = bandit.step(action)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action] # Update estimate\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo (plain Python)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "bandit = BernoulliBandit(p)\n",
        "rewards, Q_est = epsilon_greedy(bandit, num_steps=1000, epsilon=0.1)\n",
        "print(f\"Estimated Q-values: {Q_est}\")\n",
        "print(f\"Total reward: {rewards.sum()} out of 1000 pulls\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Q-values: [0.0625     0.36585366 0.80241493]\n",
            "Total reward: 749 out of 1000 pulls\n"
          ]
        }
      ],
      "id": "S-ZncU5qek8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuPNSFU_ek8A"
      },
      "source": [
        "#### 3.1.2 Gymnasium-Based Implementation: $\\epsilon$-Greedy\n",
        "\n",
        "Below, we wrap the Bernoulli bandit logic in a Gymnasium environment. In this approach:\n",
        "- We have a single episode of length `num_steps`.\n",
        "- The environment’s `action_space` is `Discrete(k)`.\n",
        "- Observations are trivial (we’ll just return a dummy array).\n",
        "- After each `step(action)`, we increment an internal step count and eventually set `done=True` when `num_steps` is reached.\n",
        "\n",
        "Then we run an $\\epsilon$-greedy policy in the **standard Gym loop**."
      ],
      "id": "XuPNSFU_ek8A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Jsu6xSyek8A",
        "outputId": "8e0c48f1-55c0-4d64-8408-2623640d3807"
      },
      "source": [
        "class BernoulliBanditEnv(gym.Env):\n",
        "    \"\"\"\n",
        "    Gymnasium environment for a k-armed Bernoulli bandit.\n",
        "    The episode length is fixed to num_steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, p, num_steps=1000):\n",
        "        super().__init__()\n",
        "        self.p = np.array(p)\n",
        "        self.k = len(p)\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        # Define action & observation space\n",
        "        self.action_space = spaces.Discrete(self.k)\n",
        "        # We'll return a dummy observation (e.g., shape (1,))\n",
        "        self.observation_space = spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
        "\n",
        "        self.current_step = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = 0\n",
        "        # Return a dummy observation\n",
        "        return np.array([0.0], dtype=np.float32), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 1 if np.random.rand() < self.p[action] else 0\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_steps)\n",
        "        # We don't have a meaningful observation, so just return a dummy\n",
        "        obs = np.array([0.0], dtype=np.float32)\n",
        "        info = {}\n",
        "        return obs, float(reward), done, False, info\n",
        "\n",
        "# Gym-based epsilon-greedy\n",
        "def epsilon_greedy_gym(env, epsilon=0.1):\n",
        "    k = env.action_space.n\n",
        "    Q = np.zeros(k)\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    step_count = 0\n",
        "\n",
        "    while not done:\n",
        "        step_count += 1\n",
        "        # Explore vs Exploit\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(k)\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action]\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo (Gym)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "env = BernoulliBanditEnv(p, num_steps=1000)\n",
        "gym_rewards, gym_Q = epsilon_greedy_gym(env, epsilon=0.1)\n",
        "print(f\"[GYM] Estimated Q-values: {gym_Q}\")\n",
        "print(f\"[GYM] Total reward: {gym_rewards.sum()} out of 1000 pulls\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GYM] Estimated Q-values: [0.0625     0.33962264 0.80294451]\n",
            "[GYM] Total reward: 731.0 out of 1000 pulls\n"
          ]
        }
      ],
      "id": "4Jsu6xSyek8A"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ja9YVPb7ek8B"
      },
      "source": [
        "### 3.2 Optimistic Initial Values\n",
        "\n",
        "From the recitation session:\n",
        "\n",
        "> **Optimistic Initial Values**: Initialize $ Q_1(a) $ to a high value (e.g., 5), so the agent is initially optimistic about all actions, forcing early exploration.\n",
        "\n",
        "#### 3.2.1 Plain Python: Optimistic Initial Values"
      ],
      "id": "ja9YVPb7ek8B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHhAIgDGek8B",
        "outputId": "6da5249e-11f4-48ea-b8cb-2a98311e444a"
      },
      "source": [
        "def optimistic_initial_values(bandit, num_steps=1000, initial_value=5.0):\n",
        "    k = bandit.k\n",
        "    Q = np.ones(k) * initial_value # Optimistic values\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        action = np.argmax(Q) # Always pick the best-known arm\n",
        "        reward = bandit.step(action)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action] # Update estimate\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo (plain Python)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "bandit = BernoulliBandit(p)\n",
        "rewards_opt, Q_opt = optimistic_initial_values(bandit, num_steps=1000, initial_value=5.0)\n",
        "print(f\"Estimated Q-values: {Q_opt}\")\n",
        "print(f\"Total reward: {rewards_opt.sum()} out of 1000 pulls\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Q-values: [0.         0.         0.80561122]\n",
            "Total reward: 804 out of 1000 pulls\n"
          ]
        }
      ],
      "id": "lHhAIgDGek8B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugpk3HSDek8B"
      },
      "source": [
        "#### 3.2.2 Gymnasium-Based Implementation: Optimistic Initial Values\n",
        "\n",
        "We’ll reuse `BernoulliBanditEnv`. We just need to set initial estimates to a high value in the Gym loop."
      ],
      "id": "ugpk3HSDek8B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4PFQC7gek8B",
        "outputId": "c4f48ed7-d5c8-4e19-b8c1-a3dc10755a09"
      },
      "source": [
        "def optimistic_initial_values_gym(env, initial_value=5.0):\n",
        "    k = env.action_space.n\n",
        "    Q = np.ones(k) * initial_value\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    step_count = 0\n",
        "\n",
        "    while not done:\n",
        "        step_count += 1\n",
        "        action = np.argmax(Q)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action]\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo (Gym)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "env = BernoulliBanditEnv(p, num_steps=1000)\n",
        "gym_rewards_opt, gym_Q_opt = optimistic_initial_values_gym(env, initial_value=5.0)\n",
        "print(f\"[GYM] Estimated Q-values: {gym_Q_opt}\")\n",
        "print(f\"[GYM] Total reward: {gym_rewards_opt.sum()} out of 1000 pulls\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GYM] Estimated Q-values: [0.11222445 0.         0.        ]\n",
            "[GYM] Total reward: 112.0 out of 1000 pulls\n"
          ]
        }
      ],
      "id": "Y4PFQC7gek8B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDMuq4azek8B"
      },
      "source": [
        "### 3.3 Upper Confidence Bound (UCB)\n",
        "\n",
        "From the recitation session:\n",
        "\n",
        "> **UCB**:\n",
        "> Improves on $\\epsilon$-greedy by considering uncertainty in action values:\n",
        "> $$\n",
        "> A_t = \\arg\\max_a \\left[ Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}} \\right].\n",
        "> $$\n",
        "> Where:\n",
        "> - $ Q_t(a) $ is the estimated action value.\n",
        "> - $ \\sqrt{\\frac{\\ln t}{N_t(a)}} $ is the confidence bound.\n",
        "> - $ c $ controls exploration.\n",
        "- The second term which is the **confidence bound**, shrinks as $ N_t(a) $ grows.\n",
        "- $ c $ is a hyperparameter controlling the degree of exploration.\n",
        "\n",
        "#### 3.3.1 Plain Python: UCB Implementation"
      ],
      "id": "NDMuq4azek8B"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb7GDCrVek8B",
        "outputId": "12d09130-c1a7-493a-a397-b6f95bc45a5c"
      },
      "source": [
        "def ucb(bandit, num_steps=1000, c=2.0):\n",
        "    k = bandit.k\n",
        "    Q = np.zeros(k)\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    for t in range(1, num_steps + 1):\n",
        "        if t <= k:\n",
        "            action = t - 1\n",
        "        else:\n",
        "            confidence = c * np.sqrt(np.log(t) / (N + 1e-9))\n",
        "            ucb_values = Q + confidence\n",
        "            action = np.argmax(ucb_values)\n",
        "\n",
        "        reward = bandit.step(action)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action] # Update estimate\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo (plain Python)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "bandit = BernoulliBandit(p)\n",
        "rewards_ucb, Q_ucb = ucb(bandit, num_steps=1000, c=2.0)\n",
        "print(f\"Estimated Q-values: {Q_ucb}\")\n",
        "print(f\"Total reward: {rewards_ucb.sum()} out of 1000 pulls\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Estimated Q-values: [0.15       0.33333333 0.80760626]\n",
            "Total reward: 750 out of 1000 pulls\n"
          ]
        }
      ],
      "id": "bb7GDCrVek8B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8B0P626bek8C"
      },
      "source": [
        "#### 3.3.2 Gymnasium-Based Implementation: UCB\n",
        "\n",
        "We again use `BernoulliBanditEnv` and incorporate the same UCB formula in the Gym loop."
      ],
      "id": "8B0P626bek8C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHGlqh3Tek8C",
        "outputId": "46f8d91a-e5f3-4265-ebc8-f1ba3a4c4c37"
      },
      "source": [
        "def ucb_gym(env, c=2.0):\n",
        "    k = env.action_space.n\n",
        "    Q = np.zeros(k)\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    t = 0\n",
        "\n",
        "    while not done:\n",
        "        t += 1\n",
        "        if t <= k:\n",
        "            action = t - 1\n",
        "        else:\n",
        "            confidence = c * np.sqrt(np.log(t) / (N + 1e-9))\n",
        "            ucb_values = Q + confidence\n",
        "            action = np.argmax(ucb_values)\n",
        "\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action]\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo (Gym)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "env = BernoulliBanditEnv(p, num_steps=1000)\n",
        "gym_rewards_ucb, gym_Q_ucb = ucb_gym(env, c=2.0)\n",
        "print(f\"[GYM] Estimated Q-values: {gym_Q_ucb}\")\n",
        "print(f\"[GYM] Total reward: {gym_rewards_ucb.sum()} out of 1000 pulls\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GYM] Estimated Q-values: [0.08333333 0.30769231 0.78642937]\n",
            "[GYM] Total reward: 730.0 out of 1000 pulls\n"
          ]
        }
      ],
      "id": "BHGlqh3Tek8C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woxin7q0ek8C"
      },
      "source": [
        "### 3.4 Thompson Sampling (Additional)\n",
        "\n",
        "**Thompson Sampling** is a popular approach that uses **Bayesian updating** of reward distributions.\n",
        "\n",
        "\n",
        "> [A Tutorial on Thompson Sampling, by Russo et al.](https://web.stanford.edu/~bvr/pubs/TS_Tutorial.pdf)\n",
        "\n",
        "\n",
        "\n",
        "1.  **A short explanation:**\n",
        "\n",
        "In the Thompson Sampling implementation, **α (alpha)** and **β (beta)** are the parameters of the **Beta distribution** used as a prior (and posterior) for each arm's probability of reward. In the context of a Bernoulli bandit:\n",
        "\n",
        "*   α (alpha) represents the count of \"successes\" (i.e. rewards of 1) plus one (initially, it starts at 1).\n",
        "*   β (beta) represents the count of \"failures\" (i.e. rewards of 0) plus one (also starting at 1).\n",
        "\n",
        "\n",
        "\n",
        "2.  **Core Idea** (Very Short):\n",
        "\n",
        "At each time step, for each **arm**, we sample a **probability** from its **Beta(α, β) distribution**. Then, we choose the arm with **the highest sampled probability**. This naturally balances exploration (arms with uncertain outcomes have **wider** Beta distributions, which might yield **high** samples) and exploitation (arms with many successes have higher mean values).\n",
        "\n",
        "In the code snippets below the main idea is, after pulling an arm:\n",
        "\n",
        "*   If a reward is received (i.e. reward = 1), the corresponding α is incremented.\n",
        "*   If no reward is received (reward = 0), the corresponding β is incremented.\n",
        "\n",
        "This Bayesian update refines our estimate of each arm's probability over time.\n",
        "\n",
        "#### 3.4.1 Plain Python: Thompson Sampling"
      ],
      "id": "woxin7q0ek8C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TVtdpl_ek8C",
        "outputId": "0dff594a-4222-4657-8f54-18b6ab03ad9e"
      },
      "source": [
        "def thompson_sampling(bandit, num_steps=1000):\n",
        "    k = bandit.k\n",
        "    alpha = np.ones(k)\n",
        "    beta = np.ones(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        # Sample from Beta distributions\n",
        "        sampled_q = np.random.beta(alpha, beta)\n",
        "        action = np.argmax(sampled_q)\n",
        "        reward = bandit.step(action)\n",
        "\n",
        "        if reward == 1:\n",
        "            alpha[action] += 1\n",
        "        else:\n",
        "            beta[action] += 1\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "    return np.array(rewards_history), alpha, beta\n",
        "\n",
        "# Demo (plain Python)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "bandit = BernoulliBandit(p)\n",
        "rewards_ts, alpha_ts, beta_ts = thompson_sampling(bandit, 1000)\n",
        "print(f\"Total reward with Thompson Sampling: {rewards_ts.sum()}\")\n",
        "print(f\"Final alpha: {alpha_ts}, final beta: {beta_ts}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward with Thompson Sampling: 803\n",
            "Final alpha: [  1.   2. 803.], final beta: [  5.   6. 189.]\n"
          ]
        }
      ],
      "id": "9TVtdpl_ek8C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3os1baIek8C"
      },
      "source": [
        "#### 3.4.2 Gymnasium-Based Implementation: Thompson Sampling\n",
        "\n",
        "We’ll maintain separate Beta($\\alpha$,$\\beta$) parameters for each arm, sampling from them in each environment step."
      ],
      "id": "i3os1baIek8C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZViPAFFFek8C",
        "outputId": "a9f86bbb-4822-4d21-faa1-ca3b97ab610a"
      },
      "source": [
        "def thompson_sampling_gym(env):\n",
        "    k = env.action_space.n\n",
        "    alpha = np.ones(k)\n",
        "    beta = np.ones(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        sampled_q = np.random.beta(alpha, beta)\n",
        "        action = np.argmax(sampled_q)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "        if reward == 1:\n",
        "            alpha[action] += 1\n",
        "        else:\n",
        "            beta[action] += 1\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), alpha, beta\n",
        "\n",
        "# Demo (Gym)\n",
        "p = [0.1, 0.3, 0.8]\n",
        "env = BernoulliBanditEnv(p, num_steps=1000)\n",
        "gym_rewards_ts, gym_alpha_ts, gym_beta_ts = thompson_sampling_gym(env)\n",
        "print(f\"[GYM] Total reward with Thompson Sampling: {gym_rewards_ts.sum()}\")\n",
        "print(f\"[GYM] Final alpha: {gym_alpha_ts}, final beta: {gym_beta_ts}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GYM] Total reward with Thompson Sampling: 795.0\n",
            "[GYM] Final alpha: [  1.   4. 793.], final beta: [  6.   5. 197.]\n"
          ]
        }
      ],
      "id": "ZViPAFFFek8C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGV9VVR2ek8C"
      },
      "source": [
        "## 4. Contextual Bandits\n",
        "\n",
        "In **contextual bandits**, each action’s reward depends on an observed **context** (features). This is more realistic (e.g., user profile for recommender systems).\n",
        "\n",
        "### 4.1 Contextual Bandit Example (Plain Python)\n",
        "Below, each arm has a weight vector. The environment returns a random context $ x $, and the probability of reward is $ \\sigma(w_a^T x) $, where $ \\sigma $ is the logistic function."
      ],
      "id": "VGV9VVR2ek8C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFhAGuYnek8C"
      },
      "source": [
        "class ContextualBanditEnv:\n",
        "    def __init__(self, weights):\n",
        "        \"\"\"\n",
        "        weights: array of shape (k, d).\n",
        "        Each row corresponds to an arm's weight vector.\n",
        "        \"\"\"\n",
        "        self.weights = weights\n",
        "        self.k = weights.shape[0]\n",
        "        self.d = weights.shape[1]\n",
        "\n",
        "    def get_context(self):\n",
        "        # Return a random context\n",
        "        return np.random.randn(self.d)\n",
        "\n",
        "    def step(self, action, context):\n",
        "        w = self.weights[action]\n",
        "        p = 1.0 / (1.0 + np.exp(- w.dot(context)))\n",
        "        return 1 if np.random.rand() < p else 0\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "hFhAGuYnek8C"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPpet22Jek8D"
      },
      "source": [
        "### 4.2 A Simple LinUCB-Like Approach (Plain Python)\n",
        "\n",
        "We assume each arm’s reward is linear in the context, plus a confidence bonus for exploration.\n",
        "\n",
        "**Core Idea:**\n",
        "\n",
        "The core idea of this LinUCB-like approach is to use a linear model for each arm that predicts the expected reward given a context, and then add a confidence bonus to that prediction to encourage exploration. In the code:\n",
        "\n",
        "*   For each arm, we maintain a matrix $A$ (which tracks the covariance of the contexts) and a vector $b$ (which accumulates the weighted rewards).\n",
        "*   The estimated parameter $θ$ for each arm is computed as $θ=A^{−1}b$.\n",
        "*   For a given context $x$, the predicted reward is $θ^Tx$, and the confidence bonus is $𝛼\\sqrt{x^TA^{-1}x}$.\n",
        "*   The arm chosen is the one that maximizes the sum of the predicted reward and the bonus.\n",
        "*   After taking an action and receiving a reward, the algorithm updates $A$ and $b$ for the selected arm.\n",
        "\n",
        "This method naturally balances exploitation (choosing the arm with the highest predicted reward) and exploration (choosing arms with high uncertainty)."
      ],
      "id": "LPpet22Jek8D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-RlSQrpek8D",
        "outputId": "3db27798-f7eb-4df0-8392-5095d5984e6a"
      },
      "source": [
        "def linucb(env, num_steps=1000, alpha=1.0):\n",
        "    k = env.k\n",
        "    d = env.d\n",
        "\n",
        "    A = [np.eye(d) for _ in range(k)]\n",
        "    b = [np.zeros(d) for _ in range(k)]\n",
        "    rewards_history = []\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        x = env.get_context()\n",
        "        p_t = []\n",
        "        for a in range(k):\n",
        "            A_inv = np.linalg.inv(A[a])\n",
        "            theta_a = A_inv.dot(b[a])\n",
        "            mean = theta_a.dot(x)\n",
        "            conf = alpha * np.sqrt(x.T.dot(A_inv).dot(x))\n",
        "            p_t.append(mean + conf)\n",
        "\n",
        "        action = np.argmax(p_t)\n",
        "        reward = env.step(action, x)\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        A[action] += np.outer(x, x)\n",
        "        b[action] += reward * x\n",
        "\n",
        "    return np.array(rewards_history)\n",
        "\n",
        "# Demo (plain Python)\n",
        "weights = np.array([\n",
        "    [2.0, -1.0],  # arm 0\n",
        "    [-1.0, 2.0],  # arm 1\n",
        "    [1.0, 1.0]    # arm 2\n",
        "])\n",
        "ctx_env = ContextualBanditEnv(weights)\n",
        "rewards_linucb = linucb(ctx_env, num_steps=1000, alpha=1.0)\n",
        "print(\"Total reward with LinUCB in contextual bandit:\", rewards_linucb.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward with LinUCB in contextual bandit: 727\n"
          ]
        }
      ],
      "id": "D-RlSQrpek8D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5oM__Ktek8D"
      },
      "source": [
        "### 4.3 Gymnasium-Based Contextual Bandit\n",
        "\n",
        "We can also wrap this contextual bandit logic in a Gym environment. Each step:\n",
        "1. We provide an observation = the current context.\n",
        "2. The agent chooses an action.\n",
        "3. We compute a reward.\n",
        "4. We sample a new context for the next step.\n",
        "5. The episode ends after `num_steps`."
      ],
      "id": "L5oM__Ktek8D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wA-m1BwKek8D"
      },
      "source": [
        "class ContextualBanditGymEnv(gym.Env):\n",
        "    def __init__(self, weights, num_steps=1000):\n",
        "        super().__init__()\n",
        "        self.weights = weights\n",
        "        self.k = weights.shape[0]\n",
        "        self.d = weights.shape[1]\n",
        "        self.num_steps = num_steps\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.k)\n",
        "        # Observation is a d-dimensional context\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-np.inf, high=np.inf, shape=(self.d,), dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = 0\n",
        "        # Sample an initial context\n",
        "        self.context = np.random.randn(self.d).astype(np.float32)\n",
        "        return self.context, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # Probability of reward = sigmoid(w_a . context)\n",
        "        w = self.weights[action]\n",
        "        p = 1.0 / (1.0 + np.exp(- w.dot(self.context)))\n",
        "        reward = 1.0 if np.random.rand() < p else 0.0\n",
        "\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_steps)\n",
        "        info = {}\n",
        "\n",
        "        # Sample next context\n",
        "        self.context = np.random.randn(self.d).astype(np.float32)\n",
        "        return self.context, reward, done, False, info"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "wA-m1BwKek8D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um37WE8pek8D"
      },
      "source": [
        "#### 4.3.1 Gymnasium LinUCB Demo\n",
        "We now adapt the **LinUCB** approach to use `ContextualBanditGymEnv`. We'll do a single episode of length `num_steps` and keep track of the contexts at each step."
      ],
      "id": "um37WE8pek8D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhnTV2TDek8D",
        "outputId": "4ca388bc-494a-4b44-bfd4-b1e948d3fb70"
      },
      "source": [
        "def linucb_gym(env, alpha=1.0):\n",
        "    k = env.action_space.n\n",
        "    d = env.observation_space.shape[0]\n",
        "    A = [np.eye(d) for _ in range(k)]\n",
        "    b = [np.zeros(d) for _ in range(k)]\n",
        "    rewards_history = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        x = obs  # current context\n",
        "        p_t = []\n",
        "        for a in range(k):\n",
        "            A_inv = np.linalg.inv(A[a])\n",
        "            theta_a = A_inv.dot(b[a])\n",
        "            mean = theta_a.dot(x)\n",
        "            conf = alpha * np.sqrt(x.T.dot(A_inv).dot(x))\n",
        "            p_t.append(mean + conf)\n",
        "\n",
        "        action = np.argmax(p_t)\n",
        "        new_obs, reward, done, truncated, info = env.step(action)\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        # Update\n",
        "        A[action] += np.outer(x, x)\n",
        "        b[action] += reward * x\n",
        "\n",
        "        obs = new_obs\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history)\n",
        "\n",
        "# Demo (Gym)\n",
        "weights = np.array([\n",
        "    [2.0, -1.0],  # arm 0\n",
        "    [-1.0, 2.0],  # arm 1\n",
        "    [1.0, 1.0]    # arm 2\n",
        "])\n",
        "env = ContextualBanditGymEnv(weights, num_steps=1000)\n",
        "gym_rewards_linucb = linucb_gym(env, alpha=1.0)\n",
        "print(\"[GYM] Total reward with LinUCB in contextual bandit:\", gym_rewards_linucb.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GYM] Total reward with LinUCB in contextual bandit: 715.0\n"
          ]
        }
      ],
      "id": "XhnTV2TDek8D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc0YcEOAek8D"
      },
      "source": [
        "## 5. Bandit Applications\n",
        "In this section, we provide **practical examples** for two major real-world applications of bandits:\n",
        "- Recommender Systems\n",
        "- Search\n",
        "\n",
        "We’ll show both **pure Python** and **Gymnasium** approaches."
      ],
      "id": "Rc0YcEOAek8D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gokDaD3Fek8D"
      },
      "source": [
        "### 5.1 Recommender Systems\n",
        "\n",
        "A recommender system often picks an item (action) to recommend for a particular user, receives feedback (reward), and aims to maximize user satisfaction (clicks, watch time, etc.).\n",
        "\n",
        "#### 5.1.1 Pure Python Example [EXTRA]\n",
        "We'll simulate a scenario where we have `k` items to recommend, and each user is described by a **context** vector. We'll keep it simple:\n",
        "- Each item has a weight vector.\n",
        "- The user context is sampled randomly.\n",
        "- The probability of a click (reward=1) is `sigmoid(w_item . context)`.\n",
        "We'll implement a minimal function to **run an $\\epsilon$-greedy** approach for demonstration."
      ],
      "id": "gokDaD3Fek8D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqbm4UcWek8D",
        "outputId": "e1366282-cae0-47fd-de7c-be0cfafbf6a0"
      },
      "source": [
        "class RecommenderSystemBandit:\n",
        "    \"\"\"\n",
        "    Pure Python bandit environment for a recommender system.\n",
        "    \"\"\"\n",
        "    def __init__(self, item_weights, context_dim=2, num_users=1000):\n",
        "        \"\"\"\n",
        "        item_weights: np.array of shape (k, context_dim)\n",
        "        context_dim: dimension of each user's feature vector\n",
        "        num_users: how many user interactions we simulate\n",
        "        \"\"\"\n",
        "        self.item_weights = item_weights\n",
        "        self.k = item_weights.shape[0]\n",
        "        self.context_dim = context_dim\n",
        "        self.num_users = num_users\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_context(self):\n",
        "        # Sample a random user feature vector\n",
        "        return np.random.randn(self.context_dim)\n",
        "\n",
        "    def step(self, action, user_context):\n",
        "        \"\"\"\n",
        "        Probability of reward = sigmoid(item_weights[action] . user_context)\n",
        "        \"\"\"\n",
        "        w = self.item_weights[action]\n",
        "        p = 1.0 / (1.0 + np.exp(- w.dot(user_context)))\n",
        "        reward = 1 if np.random.rand() < p else 0\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_users)\n",
        "        return reward, done\n",
        "\n",
        "def run_epsilon_greedy_recommender(env, epsilon=0.1):\n",
        "    k = env.k\n",
        "    Q = np.zeros(k)\n",
        "    # The number of times each 'action' has been selected,\n",
        "    # each element in N corresponds to a particular arm.\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    while True:\n",
        "        context = env.get_context()\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(k)\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "\n",
        "        reward, done = env.step(action, context)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action]\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo: RecommenderSystemBandit with k=3 items\n",
        "item_weights = np.array([\n",
        "    [1.5, 0.5],   # item 0\n",
        "    [-0.5, 1.2], # item 1\n",
        "    [0.8, 0.8]   # item 2\n",
        "])\n",
        "rec_env = RecommenderSystemBandit(item_weights, context_dim=2, num_users=1000)\n",
        "recomm_rewards, recomm_Q = run_epsilon_greedy_recommender(rec_env, epsilon=0.1)\n",
        "print(\"Recommender (pure Python) total reward:\", recomm_rewards.sum())\n",
        "print(\"Final Q estimates:\", recomm_Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommender (pure Python) total reward: 476\n",
            "Final Q estimates: [0.46927374 0.48042705 0.47592593]\n"
          ]
        }
      ],
      "id": "Jqbm4UcWek8D"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FPZRJCgnek8D"
      },
      "source": [
        "#### 5.1.2 Gymnasium Example [EXTRA]\n",
        "We create a Gym environment where each step corresponds to recommending an item for a randomly sampled user context. We’ll do a short **$\\epsilon$-greedy** run."
      ],
      "id": "FPZRJCgnek8D"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D8ZsZARTek8E",
        "outputId": "1c494e84-de6c-4334-c5d5-79ae6389ef87"
      },
      "source": [
        "class RecommenderSystemGymEnv(gym.Env):\n",
        "    def __init__(self, item_weights, context_dim=2, num_users=1000):\n",
        "        super().__init__()\n",
        "        self.item_weights = item_weights\n",
        "        self.k = item_weights.shape[0]\n",
        "        self.context_dim = context_dim\n",
        "        self.num_users = num_users\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.k)\n",
        "        # Observation is the user context\n",
        "        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(self.context_dim,), dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = 0\n",
        "        self.context = np.random.randn(self.context_dim).astype(np.float32)\n",
        "        return self.context, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        w = self.item_weights[action]\n",
        "        p = 1.0 / (1.0 + np.exp(- w.dot(self.context)))\n",
        "        reward = 1.0 if np.random.rand() < p else 0.0\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_users)\n",
        "        info = {}\n",
        "\n",
        "        # sample next user context\n",
        "        self.context = np.random.randn(self.context_dim).astype(np.float32)\n",
        "        return self.context, reward, done, False, info\n",
        "\n",
        "def run_epsilon_greedy_recommender_gym(env, epsilon=0.1):\n",
        "    k = env.action_space.n\n",
        "    Q = np.zeros(k)\n",
        "    N = np.zeros(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        if np.random.rand() < epsilon:\n",
        "            action = np.random.randint(k)\n",
        "        else:\n",
        "            action = np.argmax(Q)\n",
        "\n",
        "        new_obs, reward, done, truncated, info = env.step(action)\n",
        "        N[action] += 1\n",
        "        Q[action] += (reward - Q[action]) / N[action]\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        obs = new_obs\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), Q\n",
        "\n",
        "# Demo: RecommenderSystemGymEnv\n",
        "item_weights = np.array([\n",
        "    [1.5, 0.5],   # item 0\n",
        "    [-0.5, 1.2], # item 1\n",
        "    [0.8, 0.8]   # item 2\n",
        "])\n",
        "env_rec = RecommenderSystemGymEnv(item_weights, context_dim=2, num_users=1000)\n",
        "rec_gym_rewards, rec_gym_Q = run_epsilon_greedy_recommender_gym(env_rec, epsilon=0.1)\n",
        "print(\"[GYM Recommender] total reward:\", rec_gym_rewards.sum())\n",
        "print(\"[GYM Recommender] final Q estimates:\", rec_gym_Q)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GYM Recommender] total reward: 517.0\n",
            "[GYM Recommender] final Q estimates: [0.51923077 0.52070263 0.44680851]\n"
          ]
        }
      ],
      "id": "D8ZsZARTek8E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ2qVWXCek8E"
      },
      "source": [
        "### 5.2 Search\n",
        "\n",
        "A simplified \"search\" bandit scenario might involve multiple **ranking strategies** or **candidate documents**. We pick which strategy or document to display for a given query, and get a click/no-click reward. Below, we provide a toy model:\n",
        "- We have `k` different \"search strategies.\"\n",
        "- Each query is a context vector.\n",
        "- Probability of a click is a logistic function of the chosen strategy’s weight.\n",
        "\n",
        "#### 5.2.1 Pure Python Example [EXTRA]\n",
        "We'll do the same structure as the Recommender, but we’ll call it \"SearchBandit.\""
      ],
      "id": "QZ2qVWXCek8E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68PWkTktek8E",
        "outputId": "8e94d334-5f67-4a35-e9b3-c92688653a04"
      },
      "source": [
        "class SearchBandit:\n",
        "    \"\"\"\n",
        "    Pure Python environment to simulate 'search strategies' bandit.\n",
        "    \"\"\"\n",
        "    def __init__(self, strategy_weights, context_dim=2, num_queries=500):\n",
        "        self.strategy_weights = strategy_weights\n",
        "        self.k = strategy_weights.shape[0]\n",
        "        self.context_dim = context_dim\n",
        "        self.num_queries = num_queries\n",
        "        self.current_step = 0\n",
        "\n",
        "    def get_context(self):\n",
        "        return np.random.randn(self.context_dim)\n",
        "\n",
        "    def step(self, action, query_context):\n",
        "        w = self.strategy_weights[action]\n",
        "        p = 1.0 / (1.0 + np.exp(- w.dot(query_context)))\n",
        "        reward = 1 if np.random.rand() < p else 0\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_queries)\n",
        "        return reward, done\n",
        "\n",
        "def run_thompson_search(env):\n",
        "    \"\"\"\n",
        "    We'll demonstrate Thompson Sampling here.\n",
        "    \"\"\"\n",
        "    k = env.k\n",
        "    alpha = np.ones(k)\n",
        "    beta = np.ones(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    while True:\n",
        "        query_context = env.get_context()\n",
        "        sampled_q = np.random.beta(alpha, beta)\n",
        "        action = np.argmax(sampled_q)\n",
        "        reward, done = env.step(action, query_context)\n",
        "        if reward == 1:\n",
        "            alpha[action] += 1\n",
        "        else:\n",
        "            beta[action] += 1\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), alpha, beta\n",
        "\n",
        "# Demo: 3 search strategies\n",
        "strategy_weights = np.array([\n",
        "    [1.0, 1.0],   # strategy 0\n",
        "    [-0.5, 2.0], # strategy 1\n",
        "    [2.0, -0.2]  # strategy 2\n",
        "])\n",
        "search_env = SearchBandit(strategy_weights, context_dim=2, num_queries=500)\n",
        "search_rewards, alpha_s, beta_s = run_thompson_search(search_env)\n",
        "print(\"Search (pure Python) total reward:\", search_rewards.sum())\n",
        "print(\"Alpha:\", alpha_s, \"Beta:\", beta_s)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Search (pure Python) total reward: 264\n",
            "Alpha: [114.  68.  85.] Beta: [98. 69. 72.]\n"
          ]
        }
      ],
      "id": "68PWkTktek8E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB-sCusmek8E"
      },
      "source": [
        "#### 5.2.2 Gymnasium Example [EXTRA]\n",
        "We create a `SearchGymEnv` where each step is a query, and we choose among multiple strategies. We’ll reuse Thompson Sampling again."
      ],
      "id": "gB-sCusmek8E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei8Lza5Iek8E",
        "outputId": "1ce1c86a-b890-427a-b179-3ba63b18f89c"
      },
      "source": [
        "class SearchGymEnv(gym.Env):\n",
        "    def __init__(self, strategy_weights, context_dim=2, num_queries=500):\n",
        "        super().__init__()\n",
        "        self.strategy_weights = strategy_weights\n",
        "        self.k = strategy_weights.shape[0]\n",
        "        self.context_dim = context_dim\n",
        "        self.num_queries = num_queries\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.action_space = spaces.Discrete(self.k)\n",
        "        self.observation_space = spaces.Box(-np.inf, np.inf, shape=(context_dim,), dtype=np.float32)\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_step = 0\n",
        "        self.context = np.random.randn(self.context_dim).astype(np.float32)\n",
        "        return self.context, {}\n",
        "\n",
        "    def step(self, action):\n",
        "        w = self.strategy_weights[action]\n",
        "        p = 1.0 / (1.0 + np.exp(- w.dot(self.context)))\n",
        "        reward = 1.0 if np.random.rand() < p else 0.0\n",
        "        self.current_step += 1\n",
        "        done = (self.current_step >= self.num_queries)\n",
        "        info = {}\n",
        "\n",
        "        # get new query context\n",
        "        self.context = np.random.randn(self.context_dim).astype(np.float32)\n",
        "        return self.context, reward, done, False, info\n",
        "\n",
        "def run_thompson_search_gym(env):\n",
        "    k = env.action_space.n\n",
        "    alpha = np.ones(k)\n",
        "    beta = np.ones(k)\n",
        "    rewards_history = []\n",
        "\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        sampled_q = np.random.beta(alpha, beta)\n",
        "        action = np.argmax(sampled_q)\n",
        "        new_obs, reward, done, truncated, info = env.step(action)\n",
        "        if reward == 1:\n",
        "            alpha[action] += 1\n",
        "        else:\n",
        "            beta[action] += 1\n",
        "        rewards_history.append(reward)\n",
        "\n",
        "        obs = new_obs\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    return np.array(rewards_history), alpha, beta\n",
        "\n",
        "# Demo: SearchGymEnv\n",
        "strategy_weights = np.array([\n",
        "    [1.0, 1.0],   # strategy 0\n",
        "    [-0.5, 2.0], # strategy 1\n",
        "    [2.0, -0.2]  # strategy 2\n",
        "])\n",
        "env_search = SearchGymEnv(strategy_weights, context_dim=2, num_queries=500)\n",
        "search_gym_rewards, alpha_search_gym, beta_search_gym = run_thompson_search_gym(env_search)\n",
        "print(\"[GYM Search] total reward:\", search_gym_rewards.sum())\n",
        "print(\"[GYM Search] alpha:\", alpha_search_gym, \"beta:\", beta_search_gym)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[GYM Search] total reward: 268.0\n",
            "[GYM Search] alpha: [ 21.  65. 185.] beta: [ 22.  52. 161.]\n"
          ]
        }
      ],
      "id": "Ei8Lza5Iek8E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsVClz8Lek8E"
      },
      "source": [
        "## 6. Comparing the Algorithms\n",
        "\n",
        "Let’s do a quick empirical comparison among:\n",
        "- $\\epsilon$-greedy\n",
        "- Optimistic Initial Values\n",
        "- UCB\n",
        "- Thompson Sampling\n",
        "\n",
        "using the **plain Python** `BernoulliBandit` environment."
      ],
      "id": "hsVClz8Lek8E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "aGAYpKxtek8E",
        "outputId": "df7ef62e-d151-403a-ff84-69925e1bae88"
      },
      "source": [
        "def run_bandit_algorithms(p, num_steps=1000, runs=50):\n",
        "    \"\"\"\n",
        "    Compare multiple algorithms on the same Bernoulli bandit.\n",
        "    Returns average reward across runs for each algorithm.\n",
        "    \"\"\"\n",
        "    avg_rewards = {\n",
        "        'epsilon_greedy': np.zeros(num_steps),\n",
        "        'optimistic': np.zeros(num_steps),\n",
        "        'ucb': np.zeros(num_steps),\n",
        "        'thompson': np.zeros(num_steps)\n",
        "    }\n",
        "\n",
        "    for _ in range(runs):\n",
        "        # 1) Epsilon-Greedy\n",
        "        bandit = BernoulliBandit(p)\n",
        "        rew_eps, _ = epsilon_greedy(bandit, num_steps, epsilon=0.1)\n",
        "\n",
        "        # 2) Optimistic\n",
        "        bandit = BernoulliBandit(p)\n",
        "        rew_opt, _ = optimistic_initial_values(bandit, num_steps, initial_value=5.0)\n",
        "\n",
        "        # 3) UCB\n",
        "        bandit = BernoulliBandit(p)\n",
        "        rew_ucb, _ = ucb(bandit, num_steps, c=2.0)\n",
        "\n",
        "        # 4) Thompson Sampling\n",
        "        bandit = BernoulliBandit(p)\n",
        "        rew_ts, _, _ = thompson_sampling(bandit, num_steps)\n",
        "\n",
        "        avg_rewards['epsilon_greedy'] += rew_eps\n",
        "        avg_rewards['optimistic'] += rew_opt\n",
        "        avg_rewards['ucb'] += rew_ucb\n",
        "        avg_rewards['thompson'] += rew_ts\n",
        "\n",
        "    # Average across runs\n",
        "    for key in avg_rewards:\n",
        "        avg_rewards[key] /= runs\n",
        "\n",
        "    return avg_rewards\n",
        "\n",
        "p = [0.1, 0.3, 0.8]\n",
        "results = run_bandit_algorithms(p, num_steps=1000, runs=50)\n",
        "\n",
        "colors = [\"red\", \"blue\", \"green\", \"black\"]\n",
        "# Plot average reward over time\n",
        "plt.figure(figsize=(8,6))\n",
        "for idx, (label, rew) in enumerate(results.items()):\n",
        "    cum_mean = np.cumsum(rew) / (np.arange(len(rew)) + 1)\n",
        "    plt.plot(cum_mean, label=label, color=colors[idx])\n",
        "\n",
        "plt.xlabel(\"Steps\")\n",
        "plt.ylabel(\"Average Reward\")\n",
        "plt.title(\"Comparison of Bandit Algorithms\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "id": "aGAYpKxtek8E"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WijSNOdHek8F"
      },
      "source": [
        "## 7. Summary (Recitation Session + Additional)\n",
        "\n",
        "### From Recitation Session Recap\n",
        "\n",
        "| **Method**                  | **Exploration Type**               | **Pros**                               | **Cons**                               |\n",
        "|----------------------------|-------------------------------------|----------------------------------------|----------------------------------------|\n",
        "| **$\\epsilon$-greedy**    | Random exploration                  | Simple, ensures some exploration       | Wastes time on bad actions             |\n",
        "| **Optimistic Initial Values** | High initial estimates             | Systematic, encourages exploration     | Not as adaptive in nonstationary envs  |\n",
        "| **UCB**                    | Confidence-based exploration        | Theoretical guarantees (low regret)    | More complex, tricky to scale          |\n",
        "| **Thompson Sampling**      | Bayesian approach (Beta-Bernoulli)  | Natural exploration, strong performance| Implementation a bit more involved     |\n",
        "\n",
        "### Key Takeaways\n",
        "1. **Multi-armed bandits** are the simplest reinforcement learning setting but illustrate the **exploration-exploitation** dilemma clearly.\n",
        "2. **The Recitation Session** underscores the **non-associative** nature: no states, no transitions—just repeated action selection.\n",
        "3. Various **exploration strategies** exist:\n",
        "   - $\\epsilon$-greedy\n",
        "   - Optimistic initial values\n",
        "   - UCB\n",
        "   - Thompson Sampling (Bayesian)\n",
        "4. **Contextual bandits** extend MAB by including a **context** (features) to inform which arm to pull.\n",
        "5. Real-world **bandit applications** (recommender systems, search, etc.) can be tackled with the same techniques, using domain-specific reward definitions and contexts.\n",
        "\n",
        "### Next Steps for Enthusiats\n",
        "- Integrate more advanced methods for **nonstationary** bandits (e.g., sliding windows, constant step-size).\n",
        "- Explore **contextual bandits** in more complex environments (e.g., large-scale real data).\n",
        "- Consider bridging bandits to **full RL**: in full RL, each \"state\" becomes a context, but we also have transitions and possibly function approximation for large state/action spaces.\n"
      ],
      "id": "WijSNOdHek8F"
    }
  ]
}