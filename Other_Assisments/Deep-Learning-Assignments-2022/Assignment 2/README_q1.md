```markdown
# Assignment 2 — Question 1

Author: Mohammad taha majlesi - 810101504

Problem
-------
Build a multi-layer perceptron (MLP) from scratch using PyTorch tensors and train it on the Fashion-MNIST dataset. Implement forward and backward passes manually (no high-level nn.Module abstractions), and train with gradient descent.

Key files
---------
- `q1.ipynb` — notebook implementing the model, training loop, evaluation and plotting.
- `Figures/q1_*` — saved images (random samples, loss/accuracy curves, confusion matrix, random test images).

How it was implemented
----------------------
- Data: Fashion-MNIST, normalized and batched.
- Model: MLP with two hidden layers (128, 64) and ReLU activations. Final layer outputs logits for 10 classes.
- Training: simple gradient descent (or PyTorch optimizer if used in the notebook) for 10 epochs with cross-entropy loss.
- Evaluation: accuracy per epoch, confusion matrix on the test set, and a sample of predicted images.

How to run
----------
1. Create a virtual environment and install dependencies (example):

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. Open `q1.ipynb` in Jupyter and run the cells.

Reproducing plots
-----------------
- `Figures/q1_loss_acc_plots.png` — generated by plotting train/validation loss and accuracy per epoch.
- `Figures/q1_confusion_matrix.png` — computed from model predictions on the test set.

Notes and suggestions
---------------------
- The notebook includes a simple implementation for clarity; to improve speed or accuracy, use PyTorch `nn.Module` and built-in optimizers.
- Increasing training epochs, adding weight decay, or using batch normalization will likely improve final accuracy.

```
