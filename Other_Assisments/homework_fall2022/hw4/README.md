# Author: Taha Majlesi - 810101504, University of Tehran
# Homework 4: Model-Based Reinforcement Learning

**Author:** Taha Majlesi - 810101504, University of Tehran

## Overview

This assignment introduces model-based reinforcement learning, where agents learn a model of the environment's dynamics and use it for planning and policy improvement. You will implement model-based agents using neural network dynamics models, explore model predictive control (MPC), and investigate model-based policy optimization (MBPO) for sample-efficient learning.

## Learning Objectives

- Understand the principles of model-based RL
- Implement neural network dynamics models
- Develop model predictive control algorithms
- Analyze the trade-offs between model-based and model-free approaches
- Explore data augmentation and ensemble methods for robust modeling

## Key Concepts

- **Model-Based RL**: Learning a model of environment dynamics for planning
- **Dynamics Models**: Neural networks that predict next states and rewards
- **Model Predictive Control (MPC)**: Using the model for trajectory optimization
- **Model-Based Policy Optimization (MBPO)**: Combining model-based and model-free learning
- **Uncertainty Estimation**: Using ensembles for better exploration and robustness
- **Sample Efficiency**: Achieving good performance with fewer environment interactions

## Structure

- `cs285/`: RL codebase (model-based agents, dynamics models, MPC policies)
- `results/`: Experiment outputs
- `README.md`: This file
- `*.ipynb`: Notebooks for analysis

## Setup

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
2. (Optional) Use a virtual environment.

## Running Experiments

- Model-based MPC (Question 1):
  ```bash
  python cs285/scripts/run_hw4_mb.py --env_name cheetah-cs285-v0 --exp_name mb_test
  ```
- MBPO (Question 2):
  ```bash
  python cs285/scripts/run_hw4_mbpo.py --env_name cheetah-cs285-v0 --exp_name mbpo_test
  ```
- Logs generated by these scripts are stored in `cs285/data/` by default.

## Automated runner script

To bootstrap a virtual environment and execute the default MPC and MBPO experiments in one step, use the helper script:

```bash
chmod +x run_hw4.sh
./run_hw4.sh
```

The script creates (or reuses) `.venv/`, installs dependencies from `requirements.txt`, installs `cs285` in editable mode, and then launches both experiments with MuJoCo configured for headless rendering (`MUJOCO_GL=egl`).

Customize behavior with environment variables when invoking the script:

- `RUN_MB`, `RUN_MBPO` – set to `0` to skip either stage.
- `MB_ENV_NAME`, `MBPO_ENV_NAME` – choose among the provided CS285 benchmark environments.
- `MB_MPC_SAMPLING` – toggle between `random` and `cem`; combine with `MB_CEM_*` parameters to configure CEM.
- `MB_ADD_SL_NOISE=1` or `MBPO_ADD_SL_NOISE=1` – inject supervised-learning noise during dynamics training.
- `SAC_*` variables – adjust the SAC hyperparameters used inside MBPO (e.g. `SAC_INIT_TEMPERATURE=0.5`).
- `SKIP_INSTALL=1` – reuse an existing environment without reinstalling dependencies.
- `MB_EXTRA_FLAGS`, `MBPO_EXTRA_FLAGS` – append arbitrary CLI flags to the respective Python commands.

Examples:

Run only CEM-based MPC on the obstacles environment:

```bash
RUN_MBPO=0 MB_ENV_NAME=obstacles-cs285-v0 MB_MPC_SAMPLING=cem MB_CEM_ITERATIONS=6 MB_CEM_NUM_ELITES=10 ./run_hw4.sh
```

Reuse the environment and launch MBPO with a shorter rollout length:

```bash
SKIP_INSTALL=1 RUN_MB=0 MBPO_ENV_NAME=cheetah-cs285-v0 MBPO_ROLLOUT_LENGTH=5 ./run_hw4.sh
```

## Key Files

- `cs285/agents/`: Model-based agents (MB, MBPO)
- `cs285/models/`: Dynamics models
- `cs285/policies/`: MPC policies
- `cs285/infrastructure/`: Utilities and trainers

## Submission

- Submit code, results, and report as required.

## References

- Deisenroth, M., & Rasmussen, C. E. (2011). PILCO: A model-based and data-efficient approach to policy search. ICML.
- Janner, M., et al. (2019). When to trust your model: Model-based policy optimization. NeurIPS.
- Chua, K., et al. (2018). Deep reinforcement learning in a handful of trials using probabilistic dynamics models. NeurIPS.
- OpenAI Gymnasium documentation

## Complete the code

The following files have blanks to be filled with your solutions from homework 1. The relevant sections are marked with `TODO: get this from Piazza'.

- [infrastructure/rl_trainer.py](cs285/infrastructure/rl_trainer.py)
- [infrastructure/utils.py](cs285/infrastructure/utils.py)

You will then need to implement code in the following files:

- [agents/mb_agent.py](cs285/agents/mb_agent.py)
- [models/ff_model.py](cs285/models/ff_model.py)
- [policies/MPC_policy.py](cs285/policies/MPC_policy.py)
- [infrastructure/rl_trainer.py](cs285/infrastructure/rl_trainer.py)
- [agents/mbpo_agent.py](cs285/infrastructure/rl_trainer.py)

The relevant sections are marked with `TODO`.

You may also want to look through [scripts/run_hw4_mb.py](cs285/scripts/run_hw4_mb.py) and [scripts/run_hw4_mbpo.py](cs285/scripts/run_hw4_mbpo.py), though you will not need to edit this files beyond changing runtime arguments.

See the [assignment PDF](cs285_hw4.pdf) for more details on what files to edit.
