\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 4:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Advanced Methods in RL
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE Tahamaj Sadeghi } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large 401123456 } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}


\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Task} & \textbf{Points} \\
\hline
\text{Task 1: PPO} & 25 \\
\text{Task 2: DDPG} & 20 \\
\text{Task 3: SAC} & 25 \\
\text{Task 4: Comparison between SAC \& DDPG \& PPO} & 20 \\
\hline
\text{Clarity and Quality of Code} & 5 \\
\text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bonus 1: Writing your report in Latex } & 10 \\
\hline
\end{array}
\]

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 1: Proximal Policy Optimization (PPO) [25]}


\subsection{Question 1:} What is the role of the actor and critic networks in PPO, and how do they contribute to policy optimization? 

\textbf{Answer:}

The Actor-Critic architecture in PPO consists of two neural networks that work together to optimize the policy:

\textbf{Actor Network ($\pi_\theta(a|s)$):}
\begin{itemize}
    \item \textbf{Role:} Learns and represents the policy that maps states to action distributions
    \item \textbf{Output:} For continuous control, outputs mean $\mu_\theta(s)$ and standard deviation $\sigma_\theta(s)$ of a Gaussian distribution
    \item \textbf{Update:} Uses policy gradient with advantage estimates from the critic
    \item \textbf{Objective:} Maximizes the clipped surrogate objective:
    \[
    L^{CLIP}(\theta) = \mathbb{E}_t\left[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)\right]
    \]
    where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is the importance sampling ratio.
\end{itemize}

\textbf{Critic Network ($V_\phi(s)$):}
\begin{itemize}
    \item \textbf{Role:} Estimates the state-value function to provide baseline and advantage estimates
    \item \textbf{Output:} Scalar value $V_\phi(s)$ representing expected return from state $s$
    \item \textbf{Update:} Uses temporal difference learning to minimize value function error
    \item \textbf{Objective:} Minimizes the mean squared error:
    \[
    L^{VF}(\phi) = \mathbb{E}_t\left[(V_\phi(s_t) - V_t^{target})^2\right]
    \]
\end{itemize}

\textbf{Collaborative Contribution:}
\begin{enumerate}
    \item \textbf{Variance Reduction:} The critic provides a baseline that reduces the variance of policy gradient estimates
    \item \textbf{Advantage Estimation:} Uses Generalized Advantage Estimation (GAE) to compute advantages:
    \[
    \hat{A}_t = \sum_{k=0}^{\infty} (\gamma\lambda)^k \delta_{t+k}
    \]
    where $\delta_t = r_t + \gamma V_\phi(s_{t+1}) - V_\phi(s_t)$
    \item \textbf{Stable Updates:} The clipping mechanism prevents destructive policy updates while allowing multiple epochs of learning
\end{enumerate}

\subsection{Question 2:} PPO is known for maintaining a balance between exploration and exploitation during training. How does the stochastic nature of the actor network and the entropy term in the objective function contribute to this balance?

\textbf{Answer:}

PPO maintains exploration-exploitation balance through several mechanisms:

\textbf{1. Stochastic Policy Nature:}
\begin{itemize}
    \item The actor outputs a probability distribution over actions rather than deterministic actions
    \item For continuous control, uses Gaussian policy: $a \sim \mathcal{N}(\mu_\theta(s), \sigma_\theta^2(s))$
    \item This inherent randomness ensures exploration even without explicit exploration bonuses
\end{itemize}

\textbf{2. Entropy Regularization:}
The objective function includes an entropy bonus:
\[
L^{PPO}(\theta) = L^{CLIP}(\theta) - c_2 \cdot \mathbb{E}_t[\mathcal{H}(\pi_\theta(\cdot|s_t))]
\]
where $\mathcal{H}(\pi) = -\mathbb{E}_{a \sim \pi}[\log \pi(a|s)]$ is the policy entropy.

\textbf{3. Exploration Mechanisms:}
\begin{itemize}
    \item \textbf{High Entropy Early:} Initially, the policy has high entropy, encouraging exploration
    \item \textbf{Gradual Exploitation:} As training progresses, entropy naturally decreases as the policy becomes more deterministic
    \item \textbf{Entropy Coefficient ($c_2$):} Controls the exploration-exploitation tradeoff (typically 0.01)
\end{itemize}

\textbf{4. Clipping and Exploration:}
\begin{itemize}
    \item The clipping mechanism prevents the policy from becoming too deterministic too quickly
    \item By limiting policy updates, PPO maintains some stochasticity longer than vanilla policy gradients
    \item This gradual convergence allows for sustained exploration during learning
\end{itemize}

\textbf{5. Multiple Epochs Effect:}
\begin{itemize}
    \item PPO performs multiple epochs on the same batch of data
    \item This allows the policy to extract more information from each trajectory
    \item The stochastic nature ensures that even with multiple updates, exploration is maintained
\end{itemize}

\subsection{Question 3:} When analyzing the training results, what key indicators should be monitored to evaluate the performance of the PPO agent?

\textbf{Answer:}

Several key indicators should be monitored to evaluate PPO performance:

\textbf{1. Episode Return Trends:}
\begin{itemize}
    \item \textbf{Average Return:} Monitor the moving average of episode returns over the last 10-100 episodes
    \item \textbf{Convergence Speed:} Track how quickly the agent reaches target performance
    \item \textbf{Final Performance:} Evaluate the asymptotic performance after convergence
\end{itemize}

\textbf{2. Training Stability Metrics:}
\begin{itemize}
    \item \textbf{Variance Across Seeds:} Low standard deviation indicates stable training
    \item \textbf{Return Variance:} High variance in episode returns suggests unstable learning
    \item \textbf{Learning Curve Smoothness:} Smooth, monotonic improvement indicates stable training
\end{itemize}

\textbf{3. Policy Quality Indicators:}
\begin{itemize}
    \item \textbf{Policy Entropy:} Should decrease gradually from high to low values
    \item \textbf{Action Distribution:} Monitor if actions become too concentrated (over-exploitation)
    \item \textbf{KL Divergence:} Track KL divergence between old and new policies (should be small)
\end{itemize}

\textbf{4. Advantage and Value Function Quality:}
\begin{itemize}
    \item \textbf{Advantage Estimates:} Should be normalized and have reasonable magnitude
    \item \textbf{Value Function Accuracy:} Monitor value function loss and prediction accuracy
    \item \textbf{GAE Effectiveness:} Ensure GAE provides good bias-variance tradeoff
\end{itemize}

\textbf{5. Hyperparameter Sensitivity:}
\begin{itemize}
    \item \textbf{Clipping Ratio ($\epsilon$):} Monitor if clipping is frequently activated
    \item \textbf{Learning Rates:} Track if gradients are exploding or vanishing
    \item \textbf{Batch Size Effects:} Ensure sufficient batch size for stable updates
\end{itemize}

\textbf{6. Sample Efficiency:}
\begin{itemize}
    \item \textbf{Environment Steps:} Count total environment interactions needed for convergence
    \item \textbf{Update Efficiency:} Monitor how many policy updates are needed per episode
    \item \textbf{Data Reuse:} Track effectiveness of multiple epochs on same data
\end{itemize}

\textbf{7. Implementation-Specific Metrics:}
\begin{itemize}
    \item \textbf{Gradient Norms:} Should be bounded and not exploding
    \item \textbf{Policy Ratio Distribution:} Most ratios should be close to 1.0
    \item \textbf{Critic Loss:} Should decrease and stabilize over time
\end{itemize}



\newpage

\section{Task 2: Deep Deterministic Policy Gradient (DDPG) [20]}

\subsection{Question 1:}

What are the different types of noise used in DDPG for exploration, and how do they differ in terms of their behavior and impact on the learning process?

\vspace*{0.3cm}

\subsection{Question 2:}

What is the difference between PPO and DDPG regarding the use of past experiences?

\newpage

\section{Task 3: Soft Actor-Critic (SAC) [25]}

\subsection{Question 1:}
\textbf{Which algorithm performs better in the \texttt{HalfCheetah} environment? Why?}
\newline
Compare the performance of the PPO, DDPG, and SAC agents in terms of training stability, convergence speed, and overall accumulated reward. Based on your observations, which algorithm achieves better results in this environment?


\vspace*{0.3cm}

\subsection{Question 2:}
\textbf{How do the exploration strategies differ between PPO, DDPG, and SAC?}
\newline
Compare the exploration mechanisms used by each algorithm, such as deterministic vs. stochastic policies, entropy regularization, and noise injection. How do these strategies impact learning in environments with continuous action spaces?
\vspace*{0.3cm}

\subsection{Question 3:}
\textbf{What are the key advantages and disadvantages of each algorithm in terms of sample efficiency and stability?}
\newline
Discuss how PPO, DDPG, and SAC handle sample efficiency and training stability. Which algorithm is more sample-efficient, and which one is more stable during training? What trade-offs exist between these properties?

\vspace*{0.3cm}

\subsection{Question 3:}
\textbf{Which reinforcement learning algorithm—PPO, DDPG, or SAC—is the easiest to tune, and what are the most critical hyperparameters for ensuring stable training for each agent?}
\newline
How sensitive are PPO, DDPG, and SAC to hyperparameter choices, and which parameters have the most significant impact on stability?
What common tuning strategies can help improve performance and prevent instability in each algorithm?


\newpage

\section{Task 4: Comparison between SAC \& DDPG \& PPO [20]}

\subsection{Question 1:}
\textbf{Which algorithm performs better in the \texttt{HalfCheetah} environment? Why?}
\newline
Compare the performance of the PPO, DDPG, and SAC agents in terms of training stability, convergence speed, and overall accumulated reward. Based on your observations, which algorithm achieves better results in this environment?

\subsection{Question 2:}
\textbf{How do the exploration strategies differ between PPO, DDPG, and SAC?}
\newline
Compare the exploration mechanisms used by each algorithm, such as deterministic vs. stochastic policies, entropy regularization, and noise injection. How do these strategies impact learning in environments with continuous action spaces?

\subsection{Question 3:}
\textbf{What are the key advantages and disadvantages of each algorithm in terms of sample efficiency and stability?}
\newline
Discuss how PPO, DDPG, and SAC handle sample efficiency and training stability. Which algorithm is more sample-efficient, and which one is more stable during training? What trade-offs exist between these properties?

\subsection{Question 4:}
\textbf{Which reinforcement learning algorithm—PPO, DDPG, or SAC—is the easiest to tune, and what are the most critical hyperparameters for ensuring stable training for each agent?}
\newline
How sensitive are PPO, DDPG, and SAC to hyperparameter choices, and which parameters have the most significant impact on stability?
What common tuning strategies can help improve performance and prevent instability in each algorithm?


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}