\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL Course [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 7:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Value-Based Theory
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE [Full Name] } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large [Student Number] } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}


\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Section} & \textbf{Points} \\
\hline
\text{Positive Rewards} & 15 \\
 \text{General Rewards} & 10 \\
\text{Policy Turn} & 25 \\
% \hline
% \text{Clarity and Quality of Code} & 5 \\
% \text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bellman Operators} & 15 \\
\text{Bellman Residuals} & 35 \\
\hline
\text{Bonus 1: Writing your report in Latex} & 5 \\
\text{Bonus 2: Question 2.2.11} & 5 \\
\hline
\end{array}
\]

}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}


{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section*{Introduction}

This homework assignment explores the theoretical foundations of value-based reinforcement learning algorithms, focusing on two fundamental approaches: Value Iteration and Policy Iteration. These algorithms form the cornerstone of dynamic programming methods for solving Markov Decision Processes (MDPs) and provide the theoretical basis for many modern reinforcement learning techniques.

\textbf{Learning Objectives:}
\begin{itemize}
    \item Understand the mathematical foundations of Value Iteration and Policy Iteration
    \item Analyze convergence properties and computational complexity of these algorithms
    \item Explore the relationship between approximation errors and policy performance
    \item Develop intuition for when and why these algorithms work or fail
    \item Connect theoretical results to practical deep reinforcement learning applications
\end{itemize}

\textbf{Prerequisites:} This assignment assumes familiarity with:
\begin{itemize}
    \item Basic probability theory and Markov processes
    \item Linear algebra (vectors, matrices, norms)
    \item Real analysis (limits, convergence, contraction mappings)
    \item Fundamental concepts in reinforcement learning (MDPs, value functions, policies)
\end{itemize}

\textbf{Notation:} Throughout this assignment, we use standard reinforcement learning notation:
\begin{itemize}
    \item \( M = (S, A, R, P, \gamma) \) denotes a Markov Decision Process
    \item \( V^*(s) \) represents the optimal value function
    \item \( V^{\pi}(s) \) represents the value function for policy \( \pi \)
    \item \( \|v\| = \max_s |v(s)| \) denotes the infinity norm
    \item \( BV \) and \( B^{\pi}V \) represent Bellman operators
\end{itemize}

The problems are designed to build understanding progressively, starting with basic convergence proofs and advancing to sophisticated error analysis that connects to modern deep reinforcement learning challenges.

}}
}

\section{Iteration Family}
\label{sec:iteration_family}

Let \( M = (S, A, R, P, \gamma) \) be a finite Markov Decision Process (MDP) with the following components:
\begin{itemize}
    \item \textbf{State space:} \( S \) with \( |S| < \infty \) (finite number of states)
    \item \textbf{Action space:} \( A \) with \( |A| < \infty \) (finite number of actions)
    \item \textbf{Reward function:} \( R: S \times A \rightarrow \mathbb{R} \) with bounded rewards \( |R(s,a)| \leq R_{\text{max}} \) for all \( (s,a) \)
    \item \textbf{Transition probability:} \( P: S \times A \times S \rightarrow [0,1] \) where \( P(s'|s,a) \) denotes the probability of transitioning to state \( s' \) given action \( a \) in state \( s \)
    \item \textbf{Discount factor:} \( \gamma \in [0, 1) \) representing the importance of future rewards
\end{itemize}

In this section, we explore fundamental algorithms for solving MDPs: Value Iteration and Policy Iteration. These algorithms form the theoretical foundation for many reinforcement learning methods. We will examine their convergence properties, computational complexity, and theoretical guarantees.

\subsection{Positive Rewards}
\label{subsec:positive_rewards}

\textbf{Assumption:} Throughout this subsection, we assume that all rewards are non-negative, i.e., \( R(s, a) \geq 0 \) for all \( s \in S \) and \( a \in A \). This assumption simplifies the analysis and provides stronger convergence guarantees.

\textbf{Definition:} The optimal \( k \)-step value function \( V_k^*: S \rightarrow \mathbb{R} \) represents the maximum expected cumulative discounted reward achievable in exactly \( k \) steps:
\begin{equation}
V_k^*(s) = \max_{\pi} \mathbb{E}\left[\sum_{t=0}^{k-1} \gamma^t R(s_t, a_t) \Big| s_0 = s, \pi\right]
\end{equation}
where the expectation is taken over the trajectory induced by policy \( \pi \).

\begin{enumerate}
    \item \textbf{Upper Bound Derivation:}
    
    Derive an upper bound for the optimal \( k \)-step value function \( V_k^* \).

    \textbf{Hint:} Consider the maximum possible reward at each step and the geometric series formed by the discount factor.
    
    \textbf{Expected Solution Structure:}
    \begin{itemize}
        \item Use the boundedness of rewards: \( R(s,a) \leq R_{\text{max}} \)
        \item Apply the geometric series formula for \( \sum_{t=0}^{k-1} \gamma^t \)
        \item Consider the limit as \( k \rightarrow \infty \)
    \end{itemize}

    \item \textbf{Monotonicity and Convergence:}
    
    Prove that \( V_k^* \) is non-decreasing in \( k \). Specifically, show that:
    \[
    V_{k+1}^*(s) \geq V_k^*(s) \quad \forall s \in S, \forall k \geq 0
    \]
    
    Additionally, construct a policy \( \pi \) such that:
    \[
    V_{k+1}^\pi(s) \geq V_k^*(s) \quad \forall s \in S
    \]
    
    Use this monotonicity property to establish convergence of Value Iteration to a solution satisfying the Bellman optimality equation.
    
    \textbf{Key Steps:}
    \begin{itemize}
        \item Show that adding one more step cannot decrease the value
        \item Construct a policy that achieves the bound
        \item Apply the Monotone Convergence Theorem
        \item Verify that the limit satisfies the Bellman equation
    \end{itemize}

    \item \textbf{Optimality Proof:}
    
    By taking the limit \( k \rightarrow \infty \) in the Bellman equation, prove that the limiting value function \( V^* = \lim_{k \rightarrow \infty} V_k^* \) is indeed the optimal value function.
    
    \textbf{Requirements:}
    \begin{itemize}
        \item Show that \( V^* \) satisfies the Bellman optimality equation
        \item Prove that \( V^*(s) \geq V^\pi(s) \) for any policy \( \pi \) and state \( s \)
        \item Demonstrate that there exists a policy \( \pi^* \) such that \( V^{\pi^*} = V^* \)
    \end{itemize}
 \end{enumerate}
   \subsection{General Rewards}
\label{subsec:general_rewards}

\textbf{Relaxation of Assumptions:} We now remove the non-negativity constraint on rewards, allowing \( R(s, a) \) to be negative. However, we maintain the assumption that no terminating states exist, ensuring an infinite horizon problem.

\textbf{MDP Transformation:} Consider a new MDP \( \hat{M} = (S, A, \hat{R}, P, \gamma) \) where we add a constant reward \( r_0 \) to all rewards of the original MDP:
\begin{equation}
\hat{R}(s, a) = R(s, a) + r_0 \quad \forall (s, a) \in S \times A
\end{equation}

This transformation preserves the relative ordering of actions while shifting all rewards by a constant amount.
    
    \begin{enumerate}[resume*]
    \item \textbf{Convergence with Negative Rewards:}
    
    By deriving the optimal action and \( \hat{V}^*_k \) in terms of the original MDP's values and \( r_0 \), show that Value Iteration still converges to the optimal value function \( V^* \) (and optimal policy) of the original MDP even when rewards are negative.
    
    Additionally, compute the new optimal value function \( \hat{V}^* \) in terms of \( V^* \) and \( r_0 \).
    
    \textbf{Approach:}
    \begin{itemize}
        \item Show that the optimal policy remains unchanged under reward shifting
        \item Derive the relationship between \( \hat{V}^*_k \) and \( V^*_k \)
        \item Use induction to prove the general form
        \item Take the limit to establish convergence
    \end{itemize}
    
    \textbf{Expected Result:} You should find that \( \hat{V}^*(s) = V^*(s) + \frac{r_0}{1-\gamma} \).

    \item \textbf{Necessity of Non-Terminating Assumption:}
    
    Explain why the assumption of no terminating states is necessary for the above result to hold. Provide a concrete counterexample demonstrating how the relationship breaks down when terminating states are present.
    
    \textbf{Requirements:}
    \begin{itemize}
        \item Construct a simple MDP with terminating states
        \item Show that the additive relationship fails
        \item Explain the fundamental reason for the failure
        \item Discuss the implications for finite-horizon problems
    \end{itemize}
\end{enumerate}


% \lipsum[1-2]

\subsection{Policy Iteration}
\label{subsec:policy_iteration}

\textbf{Overview:} Policy Iteration is an alternative approach to solving MDPs that alternates between policy evaluation and policy improvement. Unlike Value Iteration, which directly updates value functions, Policy Iteration maintains an explicit policy and iteratively improves it.

\textbf{Algorithm Structure:}
\begin{enumerate}
    \item \textbf{Policy Evaluation:} Given a policy \( \pi_k \), compute its value function \( V^{\pi_k} \) by solving the system of linear equations:
    \begin{equation}
    V^{\pi_k}(s) = R(s, \pi_k(s)) + \gamma \sum_{s'} P(s'|s, \pi_k(s)) V^{\pi_k}(s')
    \end{equation}
    
    \item \textbf{Policy Improvement:} Extract a new policy \( \pi_{k+1} \) that is greedy with respect to \( V^{\pi_k} \):
    \begin{equation}
    \pi_{k+1}(s) = \arg\max_a \left[ R(s,a) + \gamma \sum_{s'} P(s'|s,a) V^{\pi_k}(s') \right]
    \end{equation}
    
    \item \textbf{Termination:} Stop when \( \pi_{k+1} = \pi_k \) (policy convergence).
\end{enumerate}
\begin{enumerate}[resume*]
\item \textbf{Policy Improvement Monotonicity:}
    
    Let \( \pi_k \) be the policy at iteration \( k \). Prove the following fundamental property of Policy Iteration:
    \[
    V^{\pi_{k+1}}(s) \geq V^{\pi_k}(s) \quad \forall s \in S
    \]
    with strict inequality for at least one state unless \( \pi_k \) is already optimal.
    
    Use the definition of the greedy policy and explain why policy improvement leads to a better or equal value function.
    
    \textbf{Proof Strategy:}
    \begin{itemize}
        \item Use the definition of greedy policy improvement
        \item Apply the Bellman equation for both policies
        \item Show that the improvement is non-negative
        \item Establish conditions for strict improvement
    \end{itemize}

    \item \textbf{Convergence to Optimality:}
    
    Prove that Policy Iteration always converges to the optimal policy in a finite MDP. Specifically, show that after a finite number of policy evaluations and improvements, the algorithm reaches a policy \( \pi^* \) that satisfies the Bellman optimality equation.
    
    You may use theorems discussed in class, but if a result was not proven, please provide a full justification.
    
    \textbf{Key Elements:}
    \begin{itemize}
        \item Finite policy space argument
        \item Monotonicity from previous part
        \item No cycling property
        \item Optimality at convergence
    \end{itemize}
    
    \item \textbf{Equivalence with Value Iteration:}
    
    Prove that Value Iteration and Policy Iteration both converge to the same optimal value function \( V^* \), even if the policies may differ. Explain how the policies can still be optimal despite possible differences.
    
    \textbf{Analysis Points:}
    \begin{itemize}
        \item Uniqueness of optimal value function
        \item Fixed point characterization
        \item Multiple optimal policies scenario
        \item Equivalence classes of optimal policies
    \end{itemize}

    \item \textbf{Computational Complexity Comparison:}
    
    Compare and contrast the computational cost of one step of Policy Iteration (i.e., full Policy Evaluation + Policy Improvement) versus one iteration of Value Iteration.
    
    \textbf{Considerations:}
    \begin{itemize}
        \item Policy Evaluation: Solving linear system vs. matrix-vector multiplication
        \item Policy Improvement: Action selection vs. value updates
        \item Overall complexity: Per-iteration vs. total convergence
        \item Practical trade-offs and when to use each method
    \end{itemize}

    \item \textbf{Undiscounted Case Analysis:}
    
    In the context of an MDP with an infinite horizon, analyze how both Value Iteration and Policy Iteration behave when the discount factor \( \gamma = 1 \).
    
    \textbf{Investigation Areas:}
    \begin{itemize}
        \item Convergence properties with \( \gamma = 1 \)
        \item Boundedness of value functions
        \item Existence of optimal policies
        \item Special cases (episodic tasks, average reward)
    \end{itemize}

% \item Let \( \pi_k \) be the policy at iteration \( k \). Prove that:  
% \[
% V^{\pi_{k+1}}(s) \geq V^{\pi_k}(s) \quad \forall s \in S,
% \]  
% with strict inequality unless \( \pi_k \) is optimal.

% \item Show that Policy Iteration terminates after finite number of iterations and the final policy \( \pi^* \) satisfies the Bellman optimality equation.   

% \item Prove that Value Iteration and Policy Iteration converge to the same \( V^* \), even if their policies differ.  

% \item Prove that Policy Iteration converges to the optimal policy in a finite MDP. Specifically, prove that after a finite number of policy evaluations and improvements, the algorithm will converge to a policy that satisfies the Bellman optimality equation.

% \item Compare and contrast computational cost of Policy Evaluation vs. Value Iteration.


% Show that if both algorithms are applied to the same MDP, they will converge to the same optimal value function \( V^* \) and an optimal policy (Which is not necessary the same. ) \( \pi^* \).


% \item What happens if $\gamma =1$ (undiscounted case) in both case of Horizen infinite or finite?







% \item 
% Let \( \hat{M} = (S, A, \hat{R}, P, \gamma) \), where \( |\hat{R}(s, a) - R(s, a)| \leq \epsilon \) for all \( s \in S \) and \( a \in A \). Besides the rewards, all other components of \( \hat{M} \) stay the same as in \( M \). Prove that 

% \[
% V^* - \hat{V}^* \leq \frac{\epsilon}{1 - \gamma}.
% \]

% Will \( M \) and \( \hat{M} \) have the same optimal policy? 

% \item 
% Now, let $\hat{M} = (S, A, \hat{R}, P, \gamma)$ where $\hat{R}(s, a) - R(s, a) = \epsilon$ for all $s \in S$ and $a \in A$ for some constant $\epsilon$. How are $V^*$ and $\hat{V}^*$ related? Express $\hat{V}^*$ in terms of $V^*$. Will $M$ and $\hat{M}$ have the same optimal policy? 


% \lipsum[1-2]
\end{enumerate}


% \lipsum[1-2]

% \subsection{Section 2}

% % \lipsum[1-2]
 


% \subsubsection{SubSection 1}

% \lipsum[1-2]

}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Bellman Operators and Residuals}
\label{sec:bellman_operators}

\textbf{Important Distinction:} Throughout this section, we distinguish between two types of value functions:
\begin{itemize}
    \item \textbf{Arbitrary value function} \( V \): A \( |S| \)-dimensional vector that need not correspond to any actual policy in the MDP
    \item \textbf{Policy value function} \( V^{\pi} \): A value function that is achieved by following a specific policy \( \pi \) in the MDP
\end{itemize}

\textbf{Example:} Consider an MDP with 2 states and only negative immediate rewards. The vector \( V = [1, 1] \) is a valid arbitrary value function, even though no policy can achieve these values. However, we cannot have \( V^{\pi} = [1, 1] \) for any policy \( \pi \).

This distinction is crucial for understanding approximation errors and convergence guarantees in reinforcement learning.

\subsection{Bellman Operators}
\label{subsec:bellman_operators}

\textbf{Definitions:} We study two fundamental Bellman operators:

\textbf{Optimal Bellman Operator:}
\begin{equation}
(BV)(s) = \max_a \left[ r(s, a) + \gamma \sum_{s' \in S} p(s'|s, a)V(s') \right]
\end{equation}

\textbf{Policy-Specific Bellman Operator:}
\begin{equation}
(B^{\pi}V)(s) = r(s, \pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s))V(s')
\end{equation}

\textbf{Norm:} Throughout this section, we use the infinity norm: \( \|v\| = \max_s |v(s)| \).

\textbf{Known Result:} The optimal Bellman operator \( B \) is a contraction mapping with contraction factor \( \gamma \), i.e., \( \|BV - BV'\| \leq \gamma\|V - V'\| \).

\begin{enumerate}
    \item \textbf{Contraction Property of Policy-Specific Operator:}
    
    Show that the policy-specific Bellman operator \( B^{\pi} \) satisfies the same contraction property as the optimal Bellman operator:
    \[
    \|B^{\pi}V - B^{\pi}V'\| \leq \gamma \|V - V'\| \quad \forall V, V'
    \]
    
    \textbf{Proof Strategy:}
    \begin{itemize}
        \item Use the definition of \( B^{\pi} \)
        \item Apply the triangle inequality
        \item Utilize the fact that probabilities sum to 1
        \item Take the maximum over all states
    \end{itemize}
    
    \item \textbf{Uniqueness of Fixed Point:}
    
    Prove that the fixed point of \( B^{\pi} \) is unique. Recall that a fixed point satisfies \( V = B^{\pi}V \). You may assume that a fixed point exists.
    
    \textbf{Hint:} Consider proof by contradiction. What happens if there were two distinct fixed points?
    
    \textbf{Alternative Approach:} Use the Banach Fixed Point Theorem since \( B^{\pi} \) is a contraction mapping.

    \item \textbf{Monotonicity Property:}
    
    Suppose that \( V \) and \( V' \) are vectors satisfying \( V(s) \leq V'(s) \) for all \( s \). Show that \( B^{\pi}V(s) \leq B^{\pi}V'(s) \) for all \( s \).
    
    \textbf{Note:} All inequalities are elementwise (pointwise).
    
    \textbf{Interpretation:} This property shows that the Bellman operator preserves the partial order on value functions, which is crucial for proving convergence of iterative algorithms.
\end{enumerate}

\subsection{Bellman Residuals} 
\label{subsec:bellman_residuals}

\textbf{Motivation:} In practice, we often work with approximate value functions that don't satisfy the Bellman equation exactly. Understanding how approximation errors affect policy performance is crucial for algorithm design and analysis.

\textbf{Greedy Policy Extraction:} Given an arbitrary value function \( V \), we can extract a greedy policy \( \pi \) using:
\begin{equation}
\pi(s) = \arg\max_a \left[ r(s, a) + \gamma \sum_{s' \in S} p(s'|s, a)V(s') \right]
\end{equation}

\textbf{Key Definitions:}
\begin{itemize}
    \item \textbf{Bellman residual:} \( (BV - V) \) - the difference between the Bellman update and the current value function
    \item \textbf{Bellman error magnitude:} \( \varepsilon = \|BV - V\| \) - the maximum absolute error across all states
\end{itemize}

\textbf{Goal:} We want to understand how the Bellman error magnitude relates to the performance of the greedy policy extracted from \( V \).

\begin{enumerate}[resume*]
    \item \textbf{Zero Bellman Error:}
    
    For what value function \( V \) does the Bellman error magnitude \( \|BV - V\| \) equal 0? Provide both the answer and a clear explanation.
    
    \textbf{Hint:} Think about what it means for a value function to satisfy the Bellman equation exactly.
    
    \item \textbf{Fundamental Performance Bounds:}
    
    Prove the following fundamental bounds for an arbitrary value function \( V \) and any policy \( \pi \):
    \[
    \|V - V^{\pi}\| \leq \frac{\|V - B^{\pi}V\|}{1 - \gamma}
    \]
    \[
    \|V - V^{*}\| \leq \frac{\|V - BV\|}{1 - \gamma}
    \]
    
    \textbf{Proof Strategy:}
    \begin{itemize}
        \item Use the contraction properties established in the previous subsection
        \item Consider iterative application of the Bellman operators
        \item Apply the triangle inequality and geometric series
        \item Use the fact that \( V^{\pi} \) and \( V^{*} \) are fixed points
    \end{itemize}
    
    \textbf{Interpretation:} These bounds show that the distance from an arbitrary value function to the true value function is controlled by the Bellman error, scaled by \( \frac{1}{1-\gamma} \).

    \item \textbf{Lower Bound on Greedy Policy Performance:}
    
    Let \( V \) be an arbitrary value function and \( \pi \) be the greedy policy extracted from \( V \). Let \( \varepsilon = \|BV - V\| \) be the Bellman error magnitude for \( V \). Prove the following for any state \( s \):
    \[
    V^{\pi}(s) \geq V^{*}(s) - \frac{2\varepsilon}{1 - \gamma}
    \]
    
    \textbf{Key Insight:} This result shows that reducing Bellman error directly improves policy quality, providing theoretical justification for algorithms that minimize Bellman residuals.

    \item \textbf{Practical Applications:}
    
    Give an example of a real-world application or domain where having a lower bound on \( V^{\pi}(s) \) would be useful. Explain why this bound is important in your chosen application.
    
    \textbf{Suggestions:} Consider safety-critical applications, resource allocation, or any scenario where performance guarantees are essential.

    \item \textbf{Uniqueness of Policy Performance:}
    
    Suppose we have another value function \( V' \) and extract its greedy policy \( \pi' \). If \( \|BV' - V'\| = \varepsilon = \|BV - V\| \), does the above lower bound imply that \( V^{\pi}(s) = V^{\pi'}(s) \) at any \( s \)?
    
    \textbf{Analysis:} Consider whether equal Bellman error magnitudes necessarily lead to equal policy performances.
 \end{enumerate}
\textbf{Special Case: Optimistic Value Functions}

Now we consider a special but important case where our approximate value function \( V \) satisfies \( V^{*} \leq V \) (i.e., \( V^{*}(s) \leq V(s) \) for all \( s \)). This occurs when our algorithm returns an "optimistic" value function that overestimates the true optimal values.

\textbf{Key Insight:} When \( V^{*} \leq V \), we can derive tighter bounds on policy performance, which is particularly useful for algorithms that maintain upper bounds on value functions.

\begin{enumerate}[resume*]
    \item \textbf{Tighter Bound with Optimistic Value Functions:}
    
    Using the same notation and setup as the previous parts, if \( V^{*} \leq V \), show that the following tighter bound holds for any state \( s \):
    \[
    V^{\pi}(s) \geq V^{*}(s) - \frac{\varepsilon}{1 - \gamma}
    \]
    
    \textbf{Recall:} For any policy \( \pi \), we have \( V^{\pi} \leq V^{*} \) (why?).
    
    \textbf{Comparison:} This bound is twice as tight as the general bound \( V^{\pi}(s) \geq V^{*}(s) - \frac{2\varepsilon}{1 - \gamma} \).

    \item \textbf{Sufficient Condition for Optimism:}
    
    It's not easy to verify that \( V^{*} \leq V \) holds because we often don't know \( V^{*} \) of the MDP. Show that if \( BV \leq V \) (pointwise), then \( V^{*} \leq V \).
    
    Note that this sufficient condition is much easier to check and does not require knowledge of \( V^{*} \).
    
    \textbf{Hint:} Try to apply induction. What is \( \lim_{n \to \infty} B^nV \)?
    
    \textbf{Practical Significance:} This result provides a practical way to verify that our value function is optimistic, which enables the use of tighter performance bounds.

    \item \textbf{Bonus: Even Tighter Bounds:}
    
    It is possible to make the bounds from the previous parts even tighter. Let \( V \) be an arbitrary value function and \( \pi \) be the greedy policy extracted from \( V \). Let \( \varepsilon = \|BV - V\| \) be the Bellman error magnitude for \( V \). Prove the following for any state \( s \):
    \[
    V^{\pi}(s) \geq V^{*}(s) - \frac{2\gamma \varepsilon}{1 - \gamma}
    \]
    
    Further, if \( V^{*} \leq V \), prove for any state \( s \):
    \[
    V^{\pi}(s) \geq V^{*}(s) - \frac{\gamma \varepsilon}{1 - \gamma}
    \]
    
    \textbf{Note:} These bounds incorporate an additional factor of \( \gamma \), providing even tighter guarantees for moderate discount factors.
\textbf{Interpretation and Practical Significance:}

The results from this section provide fundamental insights into the relationship between approximation errors and policy performance in reinforcement learning:

\begin{itemize}
    \item \textbf{Error Propagation:} Bellman errors propagate through the value function with a scaling factor of \( \frac{1}{1-\gamma} \), showing that approximation errors can be amplified significantly when \( \gamma \) is close to 1.
    
    \item \textbf{Policy Quality Guarantees:} The bounds show that reducing Bellman residuals directly improves policy quality, providing theoretical justification for algorithms that minimize Bellman errors.
    
    \item \textbf{Optimistic Algorithms:} When value functions are optimistic (overestimate true values), tighter performance bounds can be achieved, motivating algorithms that maintain upper bounds.
    
    \item \textbf{Practical Applications:} These bounds are crucial for safety-critical applications where performance guarantees are essential, such as autonomous systems, medical treatment planning, and financial portfolio management.
\end{itemize}

\textbf{Connection to Deep RL:} These theoretical results form the foundation for understanding why certain deep RL algorithms work well (e.g., DQN with target networks) and why others may fail (e.g., naive Q-learning with function approximation). The bounds help explain the importance of techniques like experience replay, target networks, and careful initialization in deep RL algorithms.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage

% {\fontfamily{lmss}\selectfont {\color{DarkBlue}

% \section{Part 1}

% % \lipsum[1-2]

% \subsection{Section 1}

% % \lipsum[1-2]

% \subsubsection{SubSection 1}

% % \lipsum[1-2]

% \subsubsection{SubSection 2}

% % \lipsum[1-2]

% \subsection{Section 2}

% % \lipsum[1-2]

% \subsubsection{SubSection 1}

% % \lipsum[1-2]

% }}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\begin{thebibliography}{9}

\bibitem{CS234}
Baesed on CS 234: Reinforcement Learning, Stanford University. Spring 2024.
\bibitem{Freepik}
\href{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon_16717130.htm}{Cover image designed by freepik}

\end{thebibliography}


}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}