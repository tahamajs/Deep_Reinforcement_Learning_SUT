\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL Course [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 7:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Value-Based Theory
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE [Full Name] } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large [Student Number] } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}


\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Section} & \textbf{Points} \\
\hline
\text{Positive Rewards} & 15 \\
 \text{General Rewards} & 10 \\
\text{Policy Turn} & 25 \\
% \hline
% \text{Clarity and Quality of Code} & 5 \\
% \text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bellman Operators} & 15 \\
\text{Bellman Residuals} & 35 \\
\hline
\text{Bonus 1: Writing your report in Latex} & 5 \\
\text{Bonus 2: Question 2.2.11} & 5 \\
\hline
\end{array}
\]

}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}


{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Iteration Family}
\label{sec:iteration_family}

Let \( M = (S, A, R, P, \gamma) \) be a finite Markov Decision Process (MDP) with the following components:
\begin{itemize}
    \item \textbf{State space:} \( S \) with \( |S| < \infty \) (finite number of states)
    \item \textbf{Action space:} \( A \) with \( |A| < \infty \) (finite number of actions)
    \item \textbf{Reward function:} \( R: S \times A \rightarrow \mathbb{R} \) with bounded rewards \( |R(s,a)| \leq R_{\text{max}} \) for all \( (s,a) \)
    \item \textbf{Transition probability:} \( P: S \times A \times S \rightarrow [0,1] \) where \( P(s'|s,a) \) denotes the probability of transitioning to state \( s' \) given action \( a \) in state \( s \)
    \item \textbf{Discount factor:} \( \gamma \in [0, 1) \) representing the importance of future rewards
\end{itemize}

In this section, we explore fundamental algorithms for solving MDPs: Value Iteration and Policy Iteration. These algorithms form the theoretical foundation for many reinforcement learning methods. We will examine their convergence properties, computational complexity, and theoretical guarantees.

\subsection{Positive Rewards}
\label{subsec:positive_rewards}

\textbf{Assumption:} Throughout this subsection, we assume that all rewards are non-negative, i.e., \( R(s, a) \geq 0 \) for all \( s \in S \) and \( a \in A \). This assumption simplifies the analysis and provides stronger convergence guarantees.

\textbf{Definition:} The optimal \( k \)-step value function \( V_k^*: S \rightarrow \mathbb{R} \) represents the maximum expected cumulative discounted reward achievable in exactly \( k \) steps:
\begin{equation}
V_k^*(s) = \max_{\pi} \mathbb{E}\left[\sum_{t=0}^{k-1} \gamma^t R(s_t, a_t) \Big| s_0 = s, \pi\right]
\end{equation}
where the expectation is taken over the trajectory induced by policy \( \pi \).

\begin{enumerate}
    \item \textbf{Upper Bound Derivation:}
    
    Derive an upper bound for the optimal \( k \)-step value function \( V_k^* \).
    
    \textbf{Hint:} Consider the maximum possible reward at each step and the geometric series formed by the discount factor.
    
    \textbf{Expected Solution Structure:}
    \begin{itemize}
        \item Use the boundedness of rewards: \( R(s,a) \leq R_{\text{max}} \)
        \item Apply the geometric series formula for \( \sum_{t=0}^{k-1} \gamma^t \)
        \item Consider the limit as \( k \rightarrow \infty \)
    \end{itemize}

    \item \textbf{Monotonicity and Convergence:}
    
    Prove that \( V_k^* \) is non-decreasing in \( k \). Specifically, show that:
    \[
    V_{k+1}^*(s) \geq V_k^*(s) \quad \forall s \in S, \forall k \geq 0
    \]
    
    Additionally, construct a policy \( \pi \) such that:
    \[
    V_{k+1}^\pi(s) \geq V_k^*(s) \quad \forall s \in S
    \]
    
    Use this monotonicity property to establish convergence of Value Iteration to a solution satisfying the Bellman optimality equation.
    
    \textbf{Key Steps:}
    \begin{itemize}
        \item Show that adding one more step cannot decrease the value
        \item Construct a policy that achieves the bound
        \item Apply the Monotone Convergence Theorem
        \item Verify that the limit satisfies the Bellman equation
    \end{itemize}

    \item \textbf{Optimality Proof:}
    
    By taking the limit \( k \rightarrow \infty \) in the Bellman equation, prove that the limiting value function \( V^* = \lim_{k \rightarrow \infty} V_k^* \) is indeed the optimal value function.
    
    \textbf{Requirements:}
    \begin{itemize}
        \item Show that \( V^* \) satisfies the Bellman optimality equation
        \item Prove that \( V^*(s) \geq V^\pi(s) \) for any policy \( \pi \) and state \( s \)
        \item Demonstrate that there exists a policy \( \pi^* \) such that \( V^{\pi^*} = V^* \)
    \end{itemize}
\end{enumerate}
\subsection{General Rewards}
\label{subsec:general_rewards}

\textbf{Relaxation of Assumptions:} We now remove the non-negativity constraint on rewards, allowing \( R(s, a) \) to be negative. However, we maintain the assumption that no terminating states exist, ensuring an infinite horizon problem.

\textbf{MDP Transformation:} Consider a new MDP \( \hat{M} = (S, A, \hat{R}, P, \gamma) \) where we add a constant reward \( r_0 \) to all rewards of the original MDP:
\begin{equation}
\hat{R}(s, a) = R(s, a) + r_0 \quad \forall (s, a) \in S \times A
\end{equation}

This transformation preserves the relative ordering of actions while shifting all rewards by a constant amount.

\begin{enumerate}[resume*]
    \item \textbf{Convergence with Negative Rewards:}
    
    By deriving the optimal action and \( \hat{V}^*_k \) in terms of the original MDP's values and \( r_0 \), show that Value Iteration still converges to the optimal value function \( V^* \) (and optimal policy) of the original MDP even when rewards are negative.
    
    Additionally, compute the new optimal value function \( \hat{V}^* \) in terms of \( V^* \) and \( r_0 \).
    
    \textbf{Approach:}
    \begin{itemize}
        \item Show that the optimal policy remains unchanged under reward shifting
        \item Derive the relationship between \( \hat{V}^*_k \) and \( V^*_k \)
        \item Use induction to prove the general form
        \item Take the limit to establish convergence
    \end{itemize}
    
    \textbf{Expected Result:} You should find that \( \hat{V}^*(s) = V^*(s) + \frac{r_0}{1-\gamma} \).

    \item \textbf{Necessity of Non-Terminating Assumption:}
    
    Explain why the assumption of no terminating states is necessary for the above result to hold. Provide a concrete counterexample demonstrating how the relationship breaks down when terminating states are present.
    
    \textbf{Requirements:}
    \begin{itemize}
        \item Construct a simple MDP with terminating states
        \item Show that the additive relationship fails
        \item Explain the fundamental reason for the failure
        \item Discuss the implications for finite-horizon problems
    \end{itemize}
\end{enumerate}


% \lipsum[1-2]

\subsection{Policy Turn}
In this part we want to dive into the mathematical proof of policy iteration.
\begin{enumerate}[resume*]
\item Let \( \pi_k \) be the policy at iteration \( k \). Prove the following:
    \[
    V^{\pi_{k+1}}(s) \geq V^{\pi_k}(s) \quad \forall s \in S,
    \]
    with strict inequality for at least one state unless \( \pi_k \) is already optimal. Use the definition of the greedy policy and explain why policy improvement leads to a better or equal value function.

    \item Prove that Policy Iteration always converges to the optimal policy in a finite MDP. Specifically, show that after a finite number of policy evaluations and improvements, the algorithm reaches a policy \( \pi^* \) that satisfies the Bellman optimality equation.  
    You may use theorems discussed in class, but if a result was not proven, please provide a full justification.
    
\item Prove that Value Iteration and Policy Iteration both converge to the same optimal value function \( V^* \), even if the policies may differ. How the policies are still optimal despite possible differences?

    \item Compare and contrast the computational cost of one step of Policy Iteration (i.e., full Policy Evaluation + Policy Improvement) versus one iteration of Value Iteration.  
    % Discuss in which scenarios each algorithm may be preferable in practice.

    \item In the context of a (MDP) with an infinite horizon, when the discount factor \( \gamma = 1 \), analyze how both Value Iteration and Policy Iteration behave.
    %     \item The MDP has a finite horizon
    % \end{itemize}
    % What challenges arise in this setting, and how does it differ from the discounted case?

% \item Let \( \pi_k \) be the policy at iteration \( k \). Prove that:  
% \[
% V^{\pi_{k+1}}(s) \geq V^{\pi_k}(s) \quad \forall s \in S,
% \]  
% with strict inequality unless \( \pi_k \) is optimal.

% \item Show that Policy Iteration terminates after finite number of iterations and the final policy \( \pi^* \) satisfies the Bellman optimality equation.   

% \item Prove that Value Iteration and Policy Iteration converge to the same \( V^* \), even if their policies differ.  

% \item Prove that Policy Iteration converges to the optimal policy in a finite MDP. Specifically, prove that after a finite number of policy evaluations and improvements, the algorithm will converge to a policy that satisfies the Bellman optimality equation.

% \item Compare and contrast computational cost of Policy Evaluation vs. Value Iteration.


% Show that if both algorithms are applied to the same MDP, they will converge to the same optimal value function \( V^* \) and an optimal policy (Which is not necessary the same. ) \( \pi^* \).


% \item What happens if $\gamma =1$ (undiscounted case) in both case of Horizen infinite or finite?







% \item 
% Let \( \hat{M} = (S, A, \hat{R}, P, \gamma) \), where \( |\hat{R}(s, a) - R(s, a)| \leq \epsilon \) for all \( s \in S \) and \( a \in A \). Besides the rewards, all other components of \( \hat{M} \) stay the same as in \( M \). Prove that 

% \[
% V^* - \hat{V}^* \leq \frac{\epsilon}{1 - \gamma}.
% \]

% Will \( M \) and \( \hat{M} \) have the same optimal policy? 

% \item 
% Now, let $\hat{M} = (S, A, \hat{R}, P, \gamma)$ where $\hat{R}(s, a) - R(s, a) = \epsilon$ for all $s \in S$ and $a \in A$ for some constant $\epsilon$. How are $V^*$ and $\hat{V}^*$ related? Express $\hat{V}^*$ in terms of $V^*$. Will $M$ and $\hat{M}$ have the same optimal policy? 


% \lipsum[1-2]
\end{enumerate}


% \lipsum[1-2]

% \subsection{Section 2}

% % \lipsum[1-2]
 


% \subsubsection{SubSection 1}

% \lipsum[1-2]

}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Bellman or Bellwoman}
\cite{CS234} Recall that a value function is a $|S|$-dimensional vector where $|S|$ is the number of states of the MDP. When we use the term $V$ in these expressions as an “arbitrary value function”, we mean that $V$ is an arbitrary $|S|$-dimensional vector which need not be aligned with the definition of the MDP at all. On the other hand, $V^{\pi}$ is a value function that is achieved by some policy $\pi$ in the MDP. For example, say the MDP has 2 states and only negative immediate rewards. $V = [1, 1]$ would be a valid choice for $V$ even though this value function can never be achieved by any policy $\pi$, but we can never have a $V^{\pi} = [1, 1]$. This distinction between $V$ and $V^{\pi}$ is important for this question and more broadly in reinforcement learning.
\subsection{Bellman Operators}
In the first part of this problem, we will explore some general and useful properties of the Bellman backup operator. We know that the Bellman backup operator $B$, defined below, is a contraction with the fixed point as $V^{\ast}$, the optimal value function of the MDP. The symbols have their usual meanings. $\gamma$ is the discount factor and $0 \leq \gamma < 1$. In all parts, $\|v\| = \max_s |v(s)|$ is the infinity norm of the vector.

\[
(BV)(s) = \max_a \left[ r(s, a) + \gamma \sum_{s' \in S} p(s'|s, a)V(s') \right]
\]

We also saw the contraction operator $B^{\pi}$ with the fixed point $V^{\pi}$, which is the Bellman backup operator for a particular policy given below:

\[
(B^{\pi}V)(s) = r(s, \pi(s)) + \gamma \sum_{s' \in S} p(s'|s, \pi(s))V(s')
\]

In this case, we’ll assume $\pi$ is deterministic, but it doesn’t have to be in general. You have seen that $\|BV - BV'\| \leq \gamma \|V - V'\|$ for two arbitrary value functions $V$ and $V'$.

\begin{enumerate}[]
    \item Show that the analogous inequality, $\|B^{\pi}V - B^{\pi}V'\| \leq \gamma \|V - V'\|$, holds. 
    % \hfill [3 pts]
    
    \item Prove that the fixed point for $B^{\pi}$ is unique. Recall that the fixed point is defined as $V$ satisfying $V = B^{\pi}V$. You may assume that a fixed point exists. 
    % \textit{Hint: Consider proof by contradiction.} \hfill [3 pts]
    
    \item Suppose that $V$ and $V'$ are vectors satisfying $V(s) \leq V'(s)$ for all $s$. Show that $B^{\pi}V(s) \leq B^{\pi}V'(s)$ for all $s$. 
    \textit{Note: all of these inequalities are elementwise.} 
    
    % \hfill [3 pts]
\end{enumerate}

\subsection{Bellman Residuals} 
We can extract a greedy policy $\pi$ from an arbitrary value function $V$ using the equation below:

\[
\pi(s) = \arg\max_a \left[ r(s, a) + \gamma \sum_{s' \in S} p(s'|s, a)V(s') \right]
\]

It is often helpful to know what the performance will be if we extract a greedy policy from an arbitrary value function. To see this, we introduce the notion of a Bellman residual.

Define the Bellman residual to be $(BV - V)$ and the Bellman error magnitude to be $\|BV - V\|$.

\begin{enumerate}[resume*]
    \item For what value function $V$ does the Bellman error magnitude $\|BV - V\|$ equal 0? Why?    
    % \hfill [2 pts]
    \item Prove the following statements for an arbitrary value function $V$ and any policy $\pi$. 
    % \textit{Hint: Try leveraging the triangle inequality by inserting a zero term.} 
    
    % \hfill [5 pts]
    \[
    \|V - V^{\pi}\| \leq \frac{\|V - B^{\pi}V\|}{1 - \gamma}\] \[
    \|V - V^{\ast}\| \leq \frac{\|V - BV\|}{1 - \gamma}
    \]
    \end{enumerate}
    % The result you proved in 5 will be useful in proving a bound on the policy performance in the next few parts. Given the Bellman residual, we will now try to derive a bound on the policy performance, $V^{\pi}$.

\begin{enumerate}[resume*]
    \item Let $V$ be an arbitrary value function and $\pi$ be the greedy policy extracted from $V$. Let $\varepsilon = \|BV - V\|$ be the Bellman error magnitude for $V$. Prove the following for any state $s$. 
    
    % \textit{Hint: Use the results from part (e).} \hfill [5 pts]
    \[
    V^{\pi}(s) \geq V^{\ast}(s) - \frac{2\varepsilon}{1 - \gamma}
    \]
    
    \item Give an example real-world application or domain where having a lower bound on $V^{\pi}(s)$ would be useful. 
    % \hfill [2 pts]
    
    \item Suppose we have another value function $V'$ and extract its greedy policy $\pi'$. $\|BV' - V'\| = \varepsilon = \|BV - V\|$. Does the above lower bound imply that $V^{\pi}(s) = V^{\pi'}(s)$ at any $s$? 
 \end{enumerate}
    Say $V \leq V'$ if $\forall s$, $V(s) \leq V'(s)$.
    % \hfill [2 pts]
\\
What if our algorithm returns a $V$ that satisfies $V^* \leq V$? I.e., it returns a value function that is better than the optimal value function of the MDP. Once again, remember that $V$ can be any vector, not necessarily achievable in the MDP, but we would still like to bound the performance of $V^{\pi}$ where $\pi$ is extracted from said $V$. We will show that if this condition is met, then we can achieve an even tighter bound on policy performance.
\begin{enumerate}[resume*]
    \item Using the same notation and setup as part 5, if $V^{\ast} \leq V$, show the following holds for any state $s$. \textit{Recall that for all $\pi$, $V^{\pi} \leq V^{\ast}$ (why?)} 
    % \hfill [5 pts]
    \[
    V^{\pi}(s) \geq V^{\ast}(s) - \frac{\varepsilon}{1 - \gamma}
    \]
\end{enumerate}

\paragraph{Intuition:} A useful way to interpret the results from parts (8) and (9) is based on the observation that a constant immediate reward of $r$ at every time-step leads to an overall discounted reward of 
\[
r + \gamma r + \gamma^2 r + \dots = \frac{r}{1 - \gamma}
\]
Thus, the above results say that a state value function $V$ with Bellman error magnitude $\varepsilon$ yields a greedy policy whose reward per step (on average), differs from optimal by at most $2\varepsilon$. So, if we develop an algorithm that reduces the Bellman residual, we’re also able to bound the performance of the policy extracted from the value function outputted by that algorithm, which is very useful!

% \paragraph{(Optional)} Try to prove the following if you’re interested. These parts will not be graded.

% \begin{enumerate}[label=(\alph*), start=10]
\begin{enumerate}[resume*]
    \item It’s not easy to show that the condition $V^{\ast} \leq V$ holds because we often don’t know $V^{\ast}$ of the MDP. Show that if $BV \leq V$ then $V^{\ast} \leq V$. Note that this sufficient condition is much easier
to check and does not require knowledge of $V^{\ast}$.

Hint: Try to apply induction. What is $\lim_{n \to \infty} B^nV$?
    
    \item (Bonus) It is possible to make the bounds from parts (9) and (10) tighter. Let $V$ be an arbitrary value function and $\pi$ be the greedy policy extracted from $V$. Let $\varepsilon = \|BV - V\|$ be the Bellman error magnitude for $V$. Prove the following for any state $s$:
    \[
    V^{\pi}(s) \geq V^{\ast}(s) - \frac{2\gamma \varepsilon}{1 - \gamma}
    \]
    Further, if $V^{\ast} \leq V$, prove for any state $s$
    \[
    V^{\pi}(s) \geq V^{\ast}(s) - \frac{\gamma \varepsilon}{1 - \gamma}
    \]
\end{enumerate}

\\
% \lipsum[1-2]

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \newpage

% {\fontfamily{lmss}\selectfont {\color{DarkBlue}

% \section{Part 1}

% % \lipsum[1-2]

% \subsection{Section 1}

% % \lipsum[1-2]

% \subsubsection{SubSection 1}

% % \lipsum[1-2]

% \subsubsection{SubSection 2}

% % \lipsum[1-2]

% \subsection{Section 2}

% % \lipsum[1-2]

% \subsubsection{SubSection 1}

% % \lipsum[1-2]

% }}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\begin{thebibliography}{9}

\bibitem{CS234}
Baesed on CS 234: Reinforcement Learning, Stanford University. Spring 2024.
\bibitem{Freepik}
\href{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon_16717130.htm}{Cover image designed by freepik}

\end{thebibliography}


}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}