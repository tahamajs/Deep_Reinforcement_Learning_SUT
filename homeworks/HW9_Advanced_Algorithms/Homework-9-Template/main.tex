\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL Course [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Solution for Homework [9]
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
[Advanced RL Algorithms]
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE [Full Name] } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large [Student Number] } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}


\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}
\noindent\rule{\textwidth}{1.5pt}

\section{Distributional Reinforcement Learning[40-points]}
\noindent\rule{\textwidth}{1.5pt}
\noindent\rule{\textwidth}{0.2pt}

\subsection{Theoretical Foundation[15-points]}
\subsubsection{a)[8-points]} Explain the fundamental difference between traditional value-based RL and distributional RL. Why is modeling the full return distribution beneficial?

\textbf{Answer:}

Traditional value-based reinforcement learning methods, such as DQN and Q-learning, focus on estimating the expected value of returns:

\begin{equation}
Q(s,a) = \mathbb{E}[R_t | s_t = s, a_t = a]
\end{equation}

In contrast, distributional RL models the entire probability distribution of returns rather than just the expectation:

\begin{equation}
Z(s,a) \text{ represents the full distribution of returns}
\end{equation}
\begin{equation}
Q(s,a) = \mathbb{E}[Z(s,a)]
\end{equation}

\textbf{Key Benefits:}
\begin{enumerate}
\item \textbf{Richer Representation}: Captures uncertainty and risk in returns
\item \textbf{Multi-Modal Returns}: Can represent multiple outcome scenarios
\item \textbf{Improved Learning}: Provides more informative learning signal
\item \textbf{Better Stability}: Reduces variance in value estimation
\item \textbf{Risk-Sensitive Policies}: Enables risk-aware decision making
\end{enumerate}

\subsubsection{b)[7-points]} Consider two actions with the same expected value but different distributions:
\begin{itemize}
\item Action A: Always returns 10 (deterministic)
\item Action B: Returns 0 or 20 with equal probability
\end{itemize}
Both have E[R] = 10, but how does distributional RL distinguish their risk profiles?

\textbf{Answer:}

Both actions have the same expected value $\mathbb{E}[R] = 10$, but distributional RL can distinguish their risk profiles:

\textbf{Action A (Deterministic):}
\begin{itemize}
\item Distribution: $\delta_{10}$ (point mass at 10)
\item Variance: $\text{Var}[R] = 0$
\item Risk: No uncertainty, guaranteed outcome
\end{itemize}

\textbf{Action B (Stochastic):}
\begin{itemize}
\item Distribution: $0.5 \cdot \delta_0 + 0.5 \cdot \delta_{20}$
\item Variance: $\text{Var}[R] = 100$
\item Risk: High uncertainty, potential for both loss and gain
\end{itemize}

\textbf{How Distributional RL Distinguishes:}
\begin{enumerate}
\item \textbf{Risk Assessment}: Action B has higher variance, indicating higher risk
\item \textbf{Tail Behavior}: Action B can produce extreme outcomes (0 or 20)
\item \textbf{Policy Selection}: Risk-averse agents might prefer Action A, risk-seeking agents might prefer Action B
\item \textbf{Conditional Value at Risk (CVaR)}: Can compute risk measures like CVaR$_{0.1}$ to assess worst-case scenarios
\end{enumerate}

This distinction is impossible with traditional value-based methods that only consider expected values.

\noindent\rule{\textwidth}{0.2pt}

\subsection{C51 Algorithm[15-points]}
\subsubsection{a)[8-points]} Describe the C51 algorithm in detail. How does it represent and update return distributions? Include the projection step.

\textbf{Answer:}

C51 (Categorical 51) discretizes the return distribution into a fixed number of atoms (typically 51).

\textbf{Architecture:}
\begin{itemize}
\item Network outputs probabilities for each atom per action
\item Output shape: [batch\_size, num\_actions, num\_atoms]
\item Support: V\_MIN to V\_MAX discretized into num\_atoms bins
\end{itemize}

\textbf{Distribution Representation:}
\begin{equation}
Z(s,a) \approx \sum_i p_i(s,a) \delta_{z_i} \text{ where } z_i \in [V_{\text{MIN}}, V_{\text{MAX}}]
\end{equation}

\textbf{Distributional Bellman Operator:}
\begin{equation}
T^\pi Z(s,a) = R(s,a) + \gamma Z(s', \pi(s'))
\end{equation}

\textbf{Projection Algorithm:}
The key innovation is projecting the Bellman-updated distribution back onto the fixed support:

\begin{enumerate}
\item \textbf{Compute Target Distribution:}
\begin{equation}
T_{z_j} = r + \gamma \cdot z_j
\end{equation}

\item \textbf{Project onto Support:}
For each atom $z_j$:
\begin{itemize}
\item Compute projected location: $b_j = \frac{T_{z_j} - V_{\text{MIN}}}{\Delta z}$
\item Distribute probability to neighboring atoms
\end{itemize}

\item \textbf{Loss Function:}
\begin{equation}
L = -\sum_i (p_{\text{target}})_i \log((p_{\text{current}})_i)
\end{equation}
Cross-entropy between target and current distributions
\end{enumerate}

\textbf{Implementation Details:}
\begin{verbatim}
class C51Network(nn.Module):
    def __init__(self, state_dim, action_dim, num_atoms=51):
        super().__init__()
        self.num_atoms = num_atoms
        self.v_min = -10
        self.v_max = 10
        self.delta_z = (self.v_max - self.v_min) / (num_atoms - 1)
        self.support = torch.linspace(self.v_min, self.v_max, num_atoms)

        self.network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim * num_atoms)
        )

    def forward(self, state):
        logits = self.network(state)
        logits = logits.view(-1, self.action_dim, self.num_atoms)
        probs = F.softmax(logits, dim=-1)
        return probs
\end{verbatim}

\textbf{Advantages:}
\begin{itemize}
\item More stable learning than DQN
\item Better performance on Atari games
\item Provides uncertainty estimates
\end{itemize}

\subsubsection{b)[7-points]} Implement the projection algorithm for C51. Show how to project the Bellman-updated distribution back onto the fixed support.

\textbf{Answer:}

\textbf{Projection Algorithm Implementation:}

\begin{verbatim}
def project_distribution(next_dist, rewards, dones, gamma, support):
    """
    Project T_z (distributional Bellman) onto support
    
    Args:
        next_dist: [batch_size, num_atoms] - next state distribution
        rewards: [batch_size] - immediate rewards
        dones: [batch_size] - episode termination flags
        gamma: discount factor
        support: [num_atoms] - support points
    """
    batch_size = rewards.shape[0]
    num_atoms = support.shape[0]
    v_min, v_max = support[0], support[-1]
    delta_z = (v_max - v_min) / (num_atoms - 1)
    
    # Compute projected values: r + γ * support
    proj_support = rewards.unsqueeze(-1) + \
                   gamma * (1 - dones.unsqueeze(-1)) * support
    
    # Clamp to valid range
    proj_support = proj_support.clamp(v_min, v_max)
    
    # Map to categorical distribution
    b = (proj_support - v_min) / delta_z
    l = b.floor().long()
    u = b.ceil().long()
    
    # Ensure indices are within bounds
    l = l.clamp(0, num_atoms - 1)
    u = u.clamp(0, num_atoms - 1)
    
    # Distribute probability
    projected_dist = torch.zeros_like(next_dist)
    
    for i in range(num_atoms):
        # Handle case where l == u (exact match)
        mask_lu = (l[:, i] == u[:, i])
        projected_dist[mask_lu, l[mask_lu, i]] += next_dist[mask_lu, i]
        
        # Handle case where l != u (interpolation)
        mask_diff = (l[:, i] != u[:, i])
        projected_dist[mask_diff, l[mask_diff, i]] += \
            next_dist[mask_diff, i] * (u[mask_diff, i] - b[mask_diff, i])
        projected_dist[mask_diff, u[mask_diff, i]] += \
            next_dist[mask_diff, i] * (b[mask_diff, i] - l[mask_diff, i])
    
    return projected_dist
\end{verbatim}

\textbf{Key Steps:}
\begin{enumerate}
\item \textbf{Compute Target Locations}: $T_{z_j} = r + \gamma \cdot z_j$
\item \textbf{Clamp to Support}: Ensure targets are within $[V_{\text{MIN}}, V_{\text{MAX}}]$
\item \textbf{Map to Indices}: Convert continuous values to discrete indices
\item \textbf{Distribute Probability}: Use linear interpolation to distribute probability mass
\end{enumerate}

\textbf{Mathematical Details:}
\begin{itemize}
\item For each atom $z_j$, compute $b_j = \frac{T_{z_j} - V_{\text{MIN}}}{\Delta z}$
\item Lower index: $l_j = \lfloor b_j \rfloor$
\item Upper index: $u_j = \lceil b_j \rceil$
\item Probability distribution:
\begin{align}
p_{\text{proj}}[l_j] &\leftarrow p_{\text{proj}}[l_j] + p_j \cdot (u_j - b_j) \\
p_{\text{proj}}[u_j] &\leftarrow p_{\text{proj}}[u_j] + p_j \cdot (b_j - l_j)
\end{align}
\end{itemize}

\noindent\rule{\textwidth}{0.2pt}

\subsection{Quantile Regression DQN[10-points]}
\subsubsection{a)[5-points]} Explain QR-DQN and how it differs from C51. What are the advantages of using quantile regression?

\textbf{Answer:}

\textbf{QR-DQN Overview:}

Unlike C51 which uses fixed locations (atoms) with learned probabilities, QR-DQN uses fixed probabilities (quantiles) with learned locations.

\textbf{Quantile Function:}
\begin{equation}
F^{-1}_Z(\tau) = \inf\{z : F_Z(z) \geq \tau\} \text{ where } \tau \in [0,1]
\end{equation}

\textbf{Key Differences from C51:}

\begin{center}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspect} & \textbf{C51} & \textbf{QR-DQN} \\
\hline
Support & Fixed locations & Learned locations \\
Probabilities & Learned & Fixed (uniform) \\
Loss & Cross-entropy & Quantile Huber loss \\
Flexibility & Fixed range & Adaptive range \\
\hline
\end{tabular}
\end{center}

\textbf{Advantages of QR-DQN:}
\begin{enumerate}
\item \textbf{Adaptive Support}: Automatically adjusts value range
\item \textbf{No Projection}: Simpler updates without distribution projection
\item \textbf{Better Tail Modeling}: Captures extreme values better
\item \textbf{Risk-Sensitive}: Easy to extract CVaR and other risk measures
\end{enumerate}

\textbf{Risk Metrics:}
\begin{verbatim}
def compute_cvar(quantiles, alpha=0.1):
    """Conditional Value at Risk"""
    num_quantiles = quantiles.shape[-1]
    cvar_quantiles = int(alpha * num_quantiles)
    return quantiles[..., :cvar_quantiles].mean(dim=-1)
\end{verbatim}

\subsubsection{b)[5-points]} Implement the quantile Huber loss function for QR-DQN.

\textbf{Answer:}

\textbf{Quantile Huber Loss Implementation:}

\begin{verbatim}
def quantile_huber_loss(quantiles, targets, taus, kappa=1.0):
    """
    Quantile Huber loss for QR-DQN
    
    Args:
        quantiles: [N, num_quantiles] - predicted quantiles
        targets: [N, num_quantiles] - target quantiles
        taus: [num_quantiles] - quantile fractions
        kappa: Huber loss threshold
    """
    td_errors = targets - quantiles
    
    # Huber loss
    huber_loss = torch.where(
        td_errors.abs() <= kappa,
        0.5 * td_errors.pow(2),
        kappa * (td_errors.abs() - 0.5 * kappa)
    )
    
    # Quantile loss
    quantile_loss = abs(taus - (td_errors < 0).float()) * huber_loss
    
    return quantile_loss.sum(dim=-1).mean()
\end{verbatim}

\textbf{Mathematical Formulation:}

The quantile Huber loss combines:
\begin{enumerate}
\item \textbf{Huber Loss}: Robust to outliers
\begin{equation}
L_\kappa(u) = \begin{cases}
\frac{1}{2}u^2 & \text{if } |u| \leq \kappa \\
\kappa(|u| - \frac{1}{2}\kappa) & \text{otherwise}
\end{cases}
\end{equation}

\item \textbf{Quantile Loss}: Asymmetric penalty
\begin{equation}
\rho_\tau(u) = u(\tau - \mathbf{1}_{u < 0})
\end{equation}

\item \textbf{Combined Loss}:
\begin{equation}
L(\theta) = \mathbb{E}_{(s,a,r,s')} \left[ \sum_{i=1}^N \rho_{\tau_i}(r + \gamma Q_{\tau_i}(s', a') - Q_{\tau_i}(s,a)) \right]
\end{equation}
\end{enumerate}

\textbf{Key Properties:}
\begin{itemize}
\item \textbf{Asymmetric}: Penalizes overestimation vs underestimation differently
\item \textbf{Robust}: Huber loss reduces sensitivity to outliers
\item \textbf{Multi-quantile}: Learns multiple quantiles simultaneously
\end{itemize}

\noindent\rule{\textwidth}{0.2pt}
}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}
\noindent\rule{\textwidth}{1.5pt}
\section{Rainbow DQN[50-points]}
\noindent\rule{\textwidth}{1.5pt}
\noindent\rule{\textwidth}{0.2pt}

\subsection{Rainbow Components[30-points]}
\subsubsection{a)[5-points]} List and briefly describe the six components that Rainbow DQN combines.

\textbf{Answer:}

Rainbow DQN integrates six orthogonal improvements to DQN:

\begin{enumerate}
\item \textbf{Double Q-Learning}: Reduces overestimation bias by decoupling action selection from evaluation
\item \textbf{Prioritized Experience Replay}: Samples important transitions more frequently based on TD error
\item \textbf{Dueling Networks}: Separates value and advantage streams for better generalization
\item \textbf{Multi-Step Returns}: Uses n-step bootstrapping for faster reward propagation
\item \textbf{Distributional RL}: Models full return distributions instead of just expectations
\item \textbf{Noisy Networks}: Adds parametric noise to network weights for state-dependent exploration
\end{enumerate}

\textbf{Synergies:}
\begin{itemize}
\item PER + n-step: Faster learning from important sequences
\item Dueling + Distributional: Better value decomposition
\item Noisy Nets + PER: Exploration prioritizes promising regions
\item Double Q + Distributional: Reduces bias in distributional targets
\end{itemize}

\subsubsection{b)[8-points]} Explain how Double Q-Learning reduces overestimation bias in DQN.

\textbf{Answer:}

\textbf{The Overestimation Problem:}

Standard DQN suffers from overestimation bias due to the max operator:

\begin{equation}
Q_{\text{target}} = r + \gamma \max_{a'} Q_{\text{target}}(s', a')
\end{equation}

\textbf{Why Overestimation Occurs:}
\begin{itemize}
\item Q-function approximation errors are typically positive
\item Max operator selects the most overestimated action
\item Policy exploits these overestimations
\item Leads to poor performance and instability
\end{itemize}

\textbf{Double Q-Learning Solution:}

Decouple action selection from evaluation using two networks:

\begin{verbatim}
# Standard DQN (problematic)
Q_target = r + γ * max_a' Q_target(s', a')

# Double DQN (solution)
a' = argmax_a' Q_online(s', a')  # Use online net for selection
Q_target = r + γ * Q_target(s', a')  # Use target net for evaluation
\end{verbatim}

\textbf{Mathematical Justification:}

Let $Q_1$ and $Q_2$ be two independent estimates of $Q^*$:

\begin{align}
\mathbb{E}[\max(Q_1, Q_2)] &\geq \max(\mathbb{E}[Q_1], \mathbb{E}[Q_2]) \\
\mathbb{E}[\min(Q_1, Q_2)] &\leq \min(\mathbb{E}[Q_1], \mathbb{E}[Q_2])
\end{align}

Since both networks overestimate, taking the minimum provides a more conservative estimate.

\textbf{Implementation:}

\begin{verbatim}
def double_q_update(states, actions, rewards, next_states, dones):
    with torch.no_grad():
        # Use online network for action selection
        next_actions = online_net(next_states).argmax(dim=1)
        
        # Use target network for evaluation
        next_q_values = target_net(next_states)[range(batch_size), next_actions]
        targets = rewards + gamma * (1 - dones) * next_q_values
    
    # Update online network
    current_q_values = online_net(states)[range(batch_size), actions]
    loss = F.mse_loss(current_q_values, targets)
    
    return loss
\end{verbatim}

\textbf{Benefits:}
\begin{itemize}
\item Reduces overestimation bias by ~25-30\%
\item More stable learning
\item Better final performance
\item Simple to implement
\end{itemize}

\subsubsection{c)[8-points]} Describe Prioritized Experience Replay. How does it improve sample efficiency?

\textbf{Answer:}

\textbf{Motivation:}

Standard experience replay samples transitions uniformly, but some transitions are more important for learning than others.

\textbf{Priority Metric:}

Use TD error magnitude as importance measure:

\begin{equation}
p_i = |\delta_i| + \epsilon
\end{equation}

where $\delta_i$ is the TD error for transition $i$.

\textbf{Sampling Probability:}

\begin{equation}
P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}
\end{equation}

where $\alpha$ controls the prioritization strength ($\alpha = 0$ gives uniform sampling).

\textbf{Implementation with SumTree:}

\begin{verbatim}
class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6, beta=0.4):
        self.alpha = alpha  # Priority exponent
        self.beta = beta    # Importance sampling correction
        self.tree = SumTree(capacity)
        self.max_priority = 1.0

    def add(self, experience, td_error=None):
        if td_error is None:
            priority = self.max_priority
        else:
            priority = (abs(td_error) + 1e-6) ** self.alpha
        
        self.tree.add(priority, experience)
        self.max_priority = max(self.max_priority, priority)

    def sample(self, batch_size):
        segment = self.tree.total() / batch_size
        priorities = []
        experiences = []
        indices = []

        for i in range(batch_size):
            s = random.uniform(segment * i, segment * (i + 1))
            idx, priority, experience = self.tree.get(s)
            priorities.append(priority)
            experiences.append(experience)
            indices.append(idx)

        # Importance sampling weights
        prob = np.array(priorities) / self.tree.total()
        weights = (len(self.tree) * prob) ** (-self.beta)
        weights /= weights.max()

        return experiences, weights, indices
\end{verbatim}

\textbf{Importance Sampling Correction:}

Since we're sampling non-uniformly, we need to correct for bias:

\begin{equation}
w_i = \left(\frac{1}{N} \cdot \frac{1}{P(i)}\right)^\beta
\end{equation}

where $\beta$ controls the correction strength.

\textbf{Benefits:}
\begin{itemize}
\item 2-3x sample efficiency improvement
\item Faster learning from important transitions
\item Better performance on sparse reward tasks
\item Works well with other improvements
\end{itemize}

\textbf{Trade-offs:}
\begin{itemize}
\item Increased computational cost (SumTree operations)
\item Need to tune $\alpha$ and $\beta$ parameters
\item Can be unstable if priorities change too rapidly
\end{itemize}

\subsubsection{d)[9-points]} Implement the Dueling Network architecture. Explain why mean subtraction is used in the combination.

\textbf{Answer:}

\textbf{Dueling Network Architecture:}

Separates value and advantage streams to better learn state values independently of action values.

\begin{verbatim}
class DuelingDQN(nn.Module):
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        super().__init__()

        # Shared feature extractor
        self.feature_layer = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )

        # Value stream: V(s)
        self.value_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)  # Single scalar output
        )

        # Advantage stream: A(s,a)
        self.advantage_stream = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)  # One per action
        )

    def forward(self, state):
        features = self.feature_layer(state)

        # Compute value and advantages
        value = self.value_stream(features)
        advantages = self.advantage_stream(features)

        # Combine using mean subtraction
        q_values = value + (advantages - advantages.mean(dim=-1, keepdim=True))

        return q_values
\end{verbatim}

\textbf{Why Mean Subtraction?}

\textbf{The Identifiability Problem:}

The decomposition $Q(s,a) = V(s) + A(s,a)$ is not unique:

\begin{align}
Q(s,a) &= V_1(s) + A_1(s,a) \\
&= V_2(s) + A_2(s,a)
\end{align}

where $V_2(s) = V_1(s) + c$ and $A_2(s,a) = A_1(s,a) - c$ for any constant $c$.

\textbf{Mean Subtraction Solution:}

Force advantages to have zero mean:

\begin{equation}
Q(s,a) = V(s) + \left(A(s,a) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s,a')\right)
\end{equation}

\textbf{Properties:}
\begin{itemize}
\item \textbf{Unique Decomposition}: $\bar{A}(s) = 0$ ensures uniqueness
\item \textbf{Value Interpretation}: $V(s) = \frac{1}{|\mathcal{A}|}\sum_a Q(s,a)$
\item \textbf{Advantage Interpretation}: $A(s,a) = Q(s,a) - V(s)$
\end{itemize}

\textbf{Alternative: Max Subtraction}

Some implementations use max instead of mean:

\begin{equation}
Q(s,a) = V(s) + \left(A(s,a) - \max_{a'} A(s,a')\right)
\end{equation}

This makes the greedy action have advantage 0, but can be less stable.

\textbf{Benefits of Dueling Architecture:}
\begin{itemize}
\item \textbf{Better Generalization}: Value stream learns state quality independently
\item \textbf{Faster Learning}: Value updated from every action
\item \textbf{More Stable}: Value provides baseline for Q-estimates
\item \textbf{Interpretable}: Can analyze state values vs action advantages
\end{itemize}

\textbf{Empirical Results:}
\begin{itemize}
\item +30\% improvement over standard DQN on Atari
\item Largest gains on games with many redundant actions
\item Particularly effective for continuous action requirements
\end{itemize}

\noindent\rule{\textwidth}{0.2pt}

\subsection{Integration and Implementation[20-points]}
\subsubsection{a)[10-points]} Show how to integrate all six Rainbow components in a single architecture.

\textbf{Answer:}

\textbf{Complete Rainbow DQN Architecture:}

\begin{verbatim}
class RainbowDQN(nn.Module):
    def __init__(self, state_dim, action_dim, num_atoms=51, n_steps=3):
        super().__init__()
        self.num_atoms = num_atoms
        self.n_steps = n_steps
        self.action_dim = action_dim

        # Feature extraction with noisy layers
        self.features = nn.Sequential(
            NoisyLinear(state_dim, 128),
            nn.ReLU()
        )

        # Dueling architecture with distributional RL
        self.value_stream = nn.Sequential(
            NoisyLinear(128, 128),
            nn.ReLU(),
            NoisyLinear(128, num_atoms)
        )

        self.advantage_stream = nn.Sequential(
            NoisyLinear(128, 128),
            nn.ReLU(),
            NoisyLinear(128, action_dim * num_atoms)
        )

        # Support for distributional RL
        self.register_buffer('support', torch.linspace(-10, 10, num_atoms))

    def forward(self, state):
        features = self.features(state)

        value = self.value_stream(features).view(-1, 1, self.num_atoms)
        advantage = self.advantage_stream(features).view(-1, self.action_dim, self.num_atoms)

        # Dueling combination
        q_atoms = value + (advantage - advantage.mean(dim=1, keepdim=True))

        # Distribution over atoms
        q_dist = F.softmax(q_atoms, dim=-1)

        return q_dist

    def reset_noise(self):
        for module in self.modules():
            if isinstance(module, NoisyLinear):
                module.reset_noise()

    def get_q_values(self, state):
        """Get Q-values from distribution"""
        q_dist = self.forward(state)
        q_values = (q_dist * self.support).sum(dim=-1)
        return q_values
\end{verbatim}

\textbf{Training with All Components:}

\begin{verbatim}
def train_rainbow(batch, priorities, is_weights):
    states, actions, rewards, next_states, dones = batch
    
    # Multi-step returns (n-step)
    n_step_rewards = compute_n_step_returns(rewards, gamma, n_steps)
    
    # Current distribution
    current_dist = model(states)[range(batch_size), actions]
    
    with torch.no_grad():
        # Double Q-learning: use online net for action selection
        next_q_values = model.get_q_values(next_states)
        next_actions = next_q_values.argmax(dim=1)
        
        # Target net for evaluation
        next_dist = target_model(next_states)[range(batch_size), next_actions]
        
        # Project distribution
        target_dist = project_distribution(next_dist, n_step_rewards, dones)
    
    # Cross-entropy loss
    loss = -(target_dist * torch.log(current_dist + 1e-8)).sum(dim=-1)
    
    # Importance sampling weights for prioritized replay
    loss = (loss * is_weights).mean()
    
    # Update priorities
    priorities = loss.detach()
    
    return loss, priorities
\end{verbatim}

\textbf{Component Integration Details:}

\begin{enumerate}
\item \textbf{Noisy Networks}: Replace linear layers with NoisyLinear for exploration
\item \textbf{Dueling Architecture}: Separate value and advantage streams
\item \textbf{Distributional RL}: Output probability distributions over atoms
\item \textbf{Double Q-Learning}: Use online net for action selection, target net for evaluation
\item \textbf{Multi-Step Returns}: Compute n-step targets for faster propagation
\item \textbf{Prioritized Replay}: Sample based on TD error magnitude
\end{enumerate}

\textbf{Synergies Between Components:}
\begin{itemize}
\item PER + n-step: Important sequences get higher priority
\item Dueling + Distributional: Better value decomposition with uncertainty
\item Noisy Nets + PER: Exploration focuses on promising regions
\item Double Q + Distributional: Reduces bias in distributional targets
\end{itemize}

\subsubsection{b)[10-points]} What are the main implementation challenges in Rainbow DQN? How can they be addressed?

\textbf{Answer:}

\textbf{Challenge 1: Memory Efficiency}

PER with distributional RL requires storing:
\begin{itemize}
\item States, actions, rewards
\item Priorities
\item N-step rollouts
\end{itemize}

\textbf{Solution:}
\begin{verbatim}
class EfficientPER:
    def __init__(self, capacity, n_step):
        self.n_step_buffer = deque(maxlen=n_step)
        self.priority_tree = SumTree(capacity)
    
    def add(self, transition):
        self.n_step_buffer.append(transition)
        if len(self.n_step_buffer) == self.n_step:
            n_step_transition = self._compute_n_step()
            self.priority_tree.add(n_step_transition)
\end{verbatim}

\textbf{Challenge 2: Computational Cost}

Rainbow is ~3-4x slower than DQN per step.

\textbf{Solutions:}
\begin{itemize}
\item Parallelize environment interactions
\item Use mixed precision training
\item Optimize projection operation with JIT compilation
\end{itemize}

\begin{verbatim}
@torch.jit.script
def fast_projection(next_dist, rewards, dones, gamma, support):
    """JIT-compiled projection for speed"""
    # Vectorized projection operation
    pass
\end{verbatim}

\textbf{Challenge 3: Hyperparameter Sensitivity}

Many interacting hyperparameters.

\textbf{Robust Configuration:}
\begin{verbatim}
RAINBOW_CONFIG = {
    'n_step': 3,
    'num_atoms': 51,
    'v_min': -10,
    'v_max': 10,
    'alpha': 0.6,  # PER priority exponent
    'beta_start': 0.4,  # IS weight
    'beta_frames': 100000,
    'sigma_init': 0.5,  # Noisy nets
    'target_update_freq': 8000,
}
\end{verbatim}

\textbf{Challenge 4: Stability}

Multiple components can interact unpredictably.

\textbf{Solutions:}
\begin{itemize}
\item Gradual annealing of beta in PER
\item Careful initialization of noisy layers
\item Monitor component-specific metrics
\end{itemize}

\begin{verbatim}
def train_rainbow(self, batch):
    # Monitor each component
    metrics = {
        'double_q_bias': ...,
        'per_weights': ...,
        'noisy_std': ...,
        'dueling_advantage': ...,
        'distributional_entropy': ...
    }
    return loss, metrics
\end{verbatim}

\textbf{Challenge 5: Debugging Complexity}

With 6 components, debugging becomes difficult.

\textbf{Solutions:}
\begin{itemize}
\item Ablation studies to isolate component effects
\item Component-specific logging
\item Gradual integration (add components one by one)
\item Unit tests for each component
\end{itemize}

\noindent\rule{\textwidth}{0.2pt}
}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}
\noindent\rule{\textwidth}{1.5pt}
\section{Twin Delayed DDPG (TD3)[40-points]}
\noindent\rule{\textwidth}{1.5pt}
\noindent\rule{\textwidth}{0.2pt}

\subsection{Core Innovations[20-points]}
\subsubsection{a)[7-points]} Explain the three key innovations in TD3 and why each is necessary.

\subsubsection{b)[6-points]} Why does taking the minimum of two Q-networks reduce overestimation?

\subsubsection{c)[7-points]} Describe target policy smoothing and its theoretical justification.

\noindent\rule{\textwidth}{0.2pt}

\subsection{Algorithm Implementation[20-points]}
\subsubsection{a)[10-points]} Provide complete pseudocode for TD3 and explain the key differences from DDPG.

\subsubsection{b)[10-points]} Analyze the contribution of each TD3 component through ablation studies.

\noindent\rule{\textwidth}{0.2pt}
}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}
\noindent\rule{\textwidth}{1.5pt}
\section{Trust Region Policy Optimization (TRPO)[35-points]}
\noindent\rule{\textwidth}{1.5pt}
\noindent\rule{\textwidth}{0.2pt}

\subsection{Trust Region Concept[15-points]}
\subsubsection{a)[8-points]} Explain the trust region concept in policy optimization. Why is it important?

\subsubsection{b)[7-points]} What is the natural policy gradient? How does it relate to TRPO?

\noindent\rule{\textwidth}{0.2pt}

\subsection{TRPO Algorithm[20-points]}
\subsubsection{a)[10-points]} Provide complete TRPO algorithm with all implementation details.

\subsubsection{b)[10-points]} Implement the conjugate gradient method for computing natural gradients.

\noindent\rule{\textwidth}{0.2pt}
}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}
\noindent\rule{\textwidth}{1.5pt}
\section{Advanced Value Functions[25-points]}
\noindent\rule{\textwidth}{1.5pt}
\noindent\rule{\textwidth}{0.2pt}

\subsection{Dueling Networks[15-points]}
\subsubsection{a)[8-points]} Explain the dueling network architecture. Why is it beneficial?

\subsubsection{b)[7-points]} Implement the dueling DQN and explain the mean subtraction technique.

\noindent\rule{\textwidth}{0.2pt}

\subsection{Retrace(λ)[10-points]}
\subsubsection{a)[5-points]} Explain the Retrace(λ) algorithm and its advantages for off-policy learning.

\subsubsection{b)[5-points]} Implement the Retrace(λ) target computation.

\noindent\rule{\textwidth}{0.2pt}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}