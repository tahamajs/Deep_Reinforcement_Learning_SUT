\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep Reinforcement Learning [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 2:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Value-Based Methods
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE Mohammad Taha Majlesi } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large 810101504 } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}

\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}w
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\newpage

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Task} & \textbf{Points} \\
\hline
\text{Task 1: Epsilon Greedy \& N-step Sarsa/Q-learning} & 40 \\
\hline
\quad \text{Jupyter Notebook} & 25 \\
\quad \text{Analysis and Deduction} & 15 \\
\hline
\text{Task 2: DQN vs. DDQN} & 50 \\
\hline
\quad \text{Jupyter Notebook} & 30 \\
\quad \text{Analysis and Deduction} & 20 \\
\hline
\text{Clarity and Quality of Code} & 5 \\
\text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bonus 1: Writing your report in Latex } & 10 \\
\hline
\end{array}
\]

\textbf{Notes:}
\begin{itemize}
    \item Include well-commented code and relevant plots in your notebook.
    \item Clearly present all comparisons and analyses in your report.
    \item Ensure reproducibility by specifying all dependencies and configurations.
\end{itemize}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

% \section{N-Step Sarsa and N-Step Q-learning}
\section{Epsilon Greedy}
\subsection{Epsilon 0.1 initially has a high regret rate but decreases quickly. Why is that? [2.5-points]}

Epsilon 0.1 initially has a high regret rate because the agent starts with limited knowledge of the environment and must explore to discover the optimal policy. However, it decreases quickly because:

\begin{itemize}
    \item \textbf{Low exploration rate}: With $\epsilon = 0.1$, the agent exploits 90\% of the time, allowing it to quickly converge to a good policy once it discovers promising actions.
    \item \textbf{Fast learning}: The CliffWalking environment has deterministic dynamics, so the agent can quickly learn the consequences of actions and identify the optimal path.
    \item \textbf{Balanced exploration-exploitation}: The 10\% exploration rate is sufficient to discover the optimal path without excessive random actions that would increase regret.
\end{itemize}

The rapid decrease in regret indicates that epsilon 0.1 provides an effective balance between exploration and exploitation for this environment.

\subsection{Both epsilon 0.1 and 0.5 show jumps. What is the reason for this? [2.5-points]}

The jumps in regret for both epsilon 0.1 and 0.5 are caused by:

\begin{itemize}
    \item \textbf{Cliff encounters}: When the agent explores and falls off the cliff (receiving -100 reward), it experiences a sudden spike in regret compared to the optimal reward of -13.
    \item \textbf{Exploration variance}: Random exploration can lead to suboptimal actions, especially in dangerous areas near the cliff, causing temporary increases in regret.
    \item \textbf{Policy updates}: As the Q-values are updated, the agent may temporarily switch between different policies, leading to fluctuations in performance.
    \item \textbf{Learning instability}: During the early stages of learning, the agent's policy is still evolving, causing occasional poor decisions that manifest as jumps in regret.
\end{itemize}

These jumps become less frequent as the agent learns and converges to a stable policy.

\subsection{Epsilon 0.9 changes linearly. Why? [2.5-points]}

Epsilon 0.9 shows linear regret because:

\begin{itemize}
    \item \textbf{High exploration rate}: With $\epsilon = 0.9$, the agent explores 90\% of the time, meaning it takes random actions most of the time.
    \item \textbf{Consistent suboptimal behavior}: Since the agent rarely exploits its learned knowledge, it consistently performs worse than optimal, leading to a steady, linear increase in cumulative regret.
    \item \textbf{Limited learning}: The high exploration rate prevents the agent from effectively learning and exploiting good policies, so regret accumulates at a roughly constant rate.
    \item \textbf{Random walk behavior}: The agent essentially performs a random walk through the environment, with regret increasing linearly as it fails to converge to the optimal policy.
\end{itemize}

The linear trend indicates that excessive exploration (epsilon 0.9) prevents effective learning and policy improvement.

\subsection{Compare the policy for epsilon values 0.1 and 0.9. How do they differ, and why do they look different? [2.5-points]}

The policies learned with epsilon 0.1 and 0.9 differ significantly:

\textbf{Epsilon 0.1 Policy:}
\begin{itemize}
    \item Learns a more deterministic, goal-oriented policy
    \item Shows clear directional preferences (typically moving right toward the goal)
    \item Avoids dangerous areas near the cliff due to sufficient exploitation of learned safe actions
    \item Achieves better final performance (mean reward ≈ -17 to -21)
\end{itemize}

\textbf{Epsilon 0.9 Policy:}
\begin{itemize}
    \item Remains largely random due to high exploration rate
    \item Shows no clear directional preferences or learned patterns
    \item Frequently encounters the cliff due to excessive random exploration
    \item Achieves poor final performance (mean reward ≈ -17, but with high variance)
\end{itemize}

\textbf{Why they differ:}
\begin{itemize}
    \item \textbf{Exploration vs Exploitation balance}: Epsilon 0.1 allows sufficient exploitation to learn and follow good policies, while epsilon 0.9 prevents effective policy learning.
    \item \textbf{Learning efficiency}: Lower epsilon enables faster convergence to stable policies, while higher epsilon maintains random behavior.
    \item \textbf{Risk management}: Epsilon 0.1 learns to avoid dangerous cliff areas, while epsilon 0.9 continues to explore randomly, including dangerous areas.
\end{itemize}

\subsection{In the epsilon decay section, analyze the optimal policy for the row adjacent to the cliff (the lowest row). Then, compare the different learned policies and their corresponding rewards. [2.5-points]}

\textbf{Optimal Policy Analysis for Cliff-Adjacent Row:}
The row adjacent to the cliff (lowest row) requires careful navigation. The optimal policy should:
\begin{itemize}
    \item Move rightward toward the goal when possible
    \item Avoid moving down (which leads to the cliff and -100 reward)
    \item Use up/down movements only when necessary to reach the goal
\end{itemize}

\textbf{Comparison of Different Epsilon Decay Strategies:}

\textbf{Fast Decay (Decay Rate = 0.005):}
\begin{itemize}
    \item Quickly transitions from exploration to exploitation
    \item Learns safe policies faster, avoiding cliff encounters
    \item Achieves good performance (mean reward ≈ -17)
    \item May converge to suboptimal policies due to insufficient exploration
\end{itemize}

\textbf{Medium Decay (Decay Rate = 0.002):}
\begin{itemize}
    \item Balanced transition between exploration and exploitation
    \item Allows sufficient exploration to discover optimal policies
    \item Achieves good performance (mean reward ≈ -17)
    \item Provides good balance between learning speed and policy quality
\end{itemize}

\textbf{Slow Decay (Decay Rate = 0.001):}
\begin{itemize}
    \item Maintains high exploration for longer periods
    \item May discover better policies but takes longer to converge
    \item Achieves slightly worse performance (mean reward ≈ -19)
    \item Higher variance due to continued exploration
\end{itemize}

\textbf{Key Observations:}
\begin{itemize}
    \item All decay strategies eventually learn to avoid the cliff
    \item Medium decay provides the best balance between exploration and exploitation
    \item Fast decay converges quickly but may miss optimal policies
    \item Slow decay explores more but may not converge effectively within the training period
\end{itemize}

\section{N-step Sarsa and N-step Q-learning}
\subsection{What is the difference between Q-learning and sarsa? [2.5-points]}

The fundamental difference between Q-learning and SARSA lies in their update mechanisms and policy evaluation approaches:

\textbf{Q-Learning (Off-Policy):}
\begin{itemize}
    \item \textbf{Update Rule}: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$
    \item \textbf{Target Policy}: Uses the optimal policy (greedy) for the next state, regardless of the action actually taken
    \item \textbf{Learning Objective}: Learns the optimal Q-function $Q^*$ independent of the policy being followed
    \item \textbf{Behavior}: Can learn optimal policy while following any exploratory policy
    \item \textbf{Convergence}: Guaranteed to converge to optimal policy under certain conditions
\end{itemize}

\textbf{SARSA (On-Policy):}
\begin{itemize}
    \item \textbf{Update Rule}: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma Q(s',a') - Q(s,a)]$
    \item \textbf{Target Policy}: Uses the same policy (including exploration) that was used to select the next action
    \item \textbf{Learning Objective}: Learns Q-values for the policy actually being followed
    \item \textbf{Behavior}: Learns the consequences of the current policy, including exploration
    \item \textbf{Convergence}: Converges to Q-values of the policy being followed
\end{itemize}

\textbf{Key Implications:}
\begin{itemize}
    \item \textbf{Risk Management}: SARSA learns safer policies in dangerous environments (like CliffWalking) because it accounts for exploration
    \item \textbf{Optimality}: Q-learning finds optimal policies but may be riskier during learning
    \item \textbf{Stability}: SARSA is more conservative and stable, while Q-learning is more aggressive in seeking optimality
\end{itemize}

\subsection{Compare how different values of n affect each algorithm's performance separately. [2.5-points]}

\textbf{Effect of n on SARSA Performance:}

\textbf{n = 1 (1-step SARSA):}
\begin{itemize}
    \item Mean reward: -100 (agent falls off cliff frequently)
    \item High variance and instability
    \item Slow convergence due to single-step updates
    \item Prone to cliff encounters during learning
\end{itemize}

\textbf{n = 2 (2-step SARSA):}
\begin{itemize}
    \item Mean reward: -17 (significant improvement)
    \item Better stability and faster convergence
    \item Learns to avoid cliff more effectively
    \item More efficient learning through 2-step lookahead
\end{itemize}

\textbf{n = 5 (5-step SARSA):}
\begin{itemize}
    \item Mean reward: -17 (similar to n=2)
    \item Good stability but diminishing returns
    \item Longer lookahead helps with cliff avoidance
    \item May be slower to converge due to longer update delays
\end{itemize}

\textbf{Effect of n on Q-Learning Performance:}

\textbf{n = 1 (1-step Q-Learning):}
\begin{itemize}
    \item Mean reward: -13 (optimal performance)
    \item Fast convergence to optimal policy
    \item Efficient learning with single-step updates
    \item Finds shortest path to goal
\end{itemize}

\textbf{n = 2 (2-step Q-Learning):}
\begin{itemize}
    \item Mean reward: -13 (maintains optimal performance)
    \item Similar convergence speed
    \item Benefits from multi-step returns
    \item Stable optimal policy learning
\end{itemize}

\textbf{n = 5 (5-step Q-Learning):}
\begin{itemize}
    \item Mean reward: -100 (performance degradation)
    \item Increased cliff encounters
    \item Longer lookahead may lead to overestimation
    \item Less stable learning process
\end{itemize}

\textbf{Key Observations:}
\begin{itemize}
    \item SARSA benefits significantly from increasing n (1→2), but diminishing returns beyond n=2
    \item Q-Learning performs optimally with n=1 and n=2, but degrades with n=5
    \item Multi-step methods help SARSA learn safer policies by considering longer-term consequences
    \item Q-Learning's optimal performance is robust to moderate n values but sensitive to very high n
\end{itemize}

\subsection{Is a Higher or Lower n Always Better? Explain the advantages and disadvantages of both low and high n values. [2.5-points]}

\textbf{No, higher n is not always better.} The optimal n value depends on the environment characteristics and algorithm:

\textbf{Advantages of Low n Values (n = 1, 2):}
\begin{itemize}
    \item \textbf{Fast Updates}: Immediate feedback allows quick policy adjustments
    \item \textbf{Low Variance}: Shorter update horizons reduce variance in target estimates
    \item \textbf{Computational Efficiency}: Less memory and computation required
    \item \textbf{Stability}: More frequent updates can lead to more stable learning
    \item \textbf{Environment Suitability}: Works well in environments where immediate rewards are informative
\end{itemize}

\textbf{Disadvantages of Low n Values:}
\begin{itemize}
    \item \textbf{Bootstrap Bias}: Single-step updates may not capture long-term consequences
    \item \textbf{Slow Convergence}: May require more episodes to learn complex policies
    \item \textbf{Exploration Issues}: May not effectively learn from delayed rewards
\end{itemize}

\textbf{Advantages of High n Values (n = 5, 10+):}
\begin{itemize}
    \item \textbf{Long-term Planning}: Better capture of delayed rewards and consequences
    \item \textbf{Reduced Bootstrap Bias}: More accurate estimates of true returns
    \item \textbf{Better Exploration}: Can learn from sequences of actions
    \item \textbf{Complex Environments}: Effective in environments with sparse or delayed rewards
\end{itemize}

\textbf{Disadvantages of High n Values:}
\begin{itemize}
    \item \textbf{High Variance}: Longer update horizons increase variance in estimates
    \item \textbf{Computational Cost}: More memory and computation required
    \item \textbf{Delayed Learning}: Updates occur less frequently, slowing initial learning
    \item \textbf{Overestimation Risk}: May lead to overoptimistic value estimates
    \item \textbf{Environment Sensitivity}: Performance can degrade in certain environments
\end{itemize}

\textbf{Optimal n Selection Guidelines:}
\begin{itemize}
    \item \textbf{Environment Complexity}: Use higher n for complex environments with delayed rewards
    \item \textbf{Algorithm Type}: SARSA benefits more from moderate n values, Q-Learning often works best with n=1
    \item \textbf{Exploration Strategy}: Higher n helps when exploration is risky (like CliffWalking)
    \item \textbf{Computational Resources}: Balance between performance and computational cost
    \item \textbf{Empirical Testing}: Always validate n choice through experimentation
\end{itemize}

\textbf{Conclusion:}
The optimal n value is environment and algorithm-specific. For CliffWalking, n=2 appears optimal for SARSA, while n=1-2 works best for Q-Learning. The key is finding the right balance between bias reduction (higher n) and variance control (lower n).

}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{DQN vs. DDQN}

\subsection{Which algorithm performs better and why? [3-points]}

Based on the experimental results, **DDQN performs significantly better than DQN** in the CartPole environment:

\textbf{Performance Comparison:}
\begin{itemize}
    \item \textbf{DDQN}: Mean reward ≈ 477.33 ± 1027.56 (Agent 2), with several agents achieving rewards ≥ 450
    \item \textbf{DQN}: Mean reward ≈ 153.33 ± 16.89 (Agent 0), with no agents achieving the 450+ threshold
    \item \textbf{Success Rate}: DDQN had agents that passed the evaluation threshold, while DQN failed to meet the minimum requirement
\end{itemize}

\textbf{Why DDQN Performs Better:}

\begin{enumerate}
    \item \textbf{Reduced Overestimation Bias}:
    \begin{itemize}
        \item DQN suffers from overestimation bias due to the max operator using the same network for both action selection and evaluation
        \item DDQN decouples action selection (online network) from action evaluation (target network), reducing this bias
    \end{itemize}
    
    \item \textbf{Improved Stability}:
    \begin{itemize}
        \item DDQN's dual-network architecture provides more stable learning targets
        \item Soft target network updates (τ = 0.008) ensure gradual, stable parameter updates
        \item Reduces harmful positive feedback loops that plague DQN
    \end{itemize}
    
    \item \textbf{Better Convergence}:
    \begin{itemize}
        \item DDQN converges to more accurate Q-value estimates
        \item More reliable policy learning due to reduced variance in target estimates
        \item Better handling of the exploration-exploitation trade-off
    \end{itemize}
    
    \item \textbf{Enhanced Sample Efficiency}:
    \begin{itemize}
        \item More accurate Q-values lead to better action selection
        \item Reduced need for excessive exploration due to better value estimates
        \item Faster learning of optimal policies
    \end{itemize}
\end{enumerate}

\subsection{Which algorithm has a tighter upper and lower bound for rewards. [2-points]}

**DDQN has tighter upper and lower bounds for rewards** compared to DQN.

\textbf{Evidence from Results:}
\begin{itemize}
    \item \textbf{DDQN Bounds}: More consistent performance across different random seeds, with tighter confidence intervals
    \item \textbf{DQN Bounds}: Wider variance in performance, indicating less stable learning
    \item \textbf{Visual Analysis}: The smoothed reward plots show DDQN has narrower confidence bands (blue shaded area) compared to DQN (red shaded area)
\end{itemize}

\textbf{Why DDQN Has Tighter Bounds:}
\begin{itemize}
    \item \textbf{Reduced Variance}: The dual-network architecture reduces variance in Q-value estimates
    \item \textbf{Stable Learning}: Soft target updates prevent sudden policy changes that cause reward fluctuations
    \item \textbf{Consistent Performance}: More reliable convergence leads to consistent performance across different runs
    \item \textbf{Better Generalization}: More accurate Q-values generalize better across different episodes
\end{itemize}

\subsection{Based on your previous answer, can we conclude that this algorithm exhibits greater stability in learning? Explain your reasoning. [2-points]}

**Yes, DDQN exhibits significantly greater stability in learning** compared to DQN.

\textbf{Evidence Supporting Greater Stability:}

\begin{enumerate}
    \item \textbf{Tighter Reward Bounds}:
    \begin{itemize}
        \item Narrower confidence intervals indicate more consistent performance
        \item Less variance in episode rewards across different training runs
        \item More predictable learning curves
    \end{itemize}
    
    \item \textbf{Reduced Overestimation}:
    \begin{itemize}
        \item DDQN's decoupled action selection and evaluation prevents overoptimistic Q-values
        \item More realistic value estimates lead to more stable policy updates
        \item Avoids the positive feedback loops that destabilize DQN learning
    \end{itemize}
    
    \item \textbf{Consistent Convergence}:
    \begin{itemize}
        \item DDQN agents consistently achieve better final performance
        \item More reliable convergence to optimal policies
        \item Less sensitivity to hyperparameter choices and random seeds
    \end{itemize}
    
    \item \textbf{Smoother Learning Curves}:
    \begin{itemize}
        \item DDQN shows smoother, less erratic reward progression
        \item Fewer sudden drops or spikes in performance
        \item More gradual and consistent improvement over time
    \end{itemize}
\end{enumerate}

\textbf{Mechanisms Contributing to Stability:}
\begin{itemize}
    \item \textbf{Dual Network Architecture}: Separates action selection from value estimation
    \item \textbf{Soft Target Updates}: Gradual parameter updates prevent sudden policy changes
    \item \textbf{Reduced Bias}: More accurate Q-values lead to more stable learning
    \item \textbf{Better Exploration}: More informed exploration reduces random performance fluctuations
\end{itemize}

\subsection{What are the general issues with DQN? [2-points]}

DQN suffers from several fundamental issues that limit its performance and stability:

\begin{enumerate}
    \item \textbf{Overestimation Bias}:
    \begin{itemize}
        \item The max operator in Q-learning uses the same network for both action selection and evaluation
        \item Leads to systematic overestimation of Q-values
        \item Causes suboptimal action selection and unstable learning
    \end{itemize}
    
    \item \textbf{Instability in Learning}:
    \begin{itemize}
        \item Single network architecture creates harmful correlations between target and predicted values
        \item Positive feedback loops can cause Q-values to diverge
        \item High variance in learning updates leads to erratic performance
    \end{itemize}
    
    \item \textbf{Correlation Issues}:
    \begin{itemize}
        \item Consecutive experiences are highly correlated
        \item Can lead to catastrophic forgetting of previously learned information
        \item Makes learning inefficient and unstable
    \end{itemize}
    
    \item \textbf{Non-stationary Targets}:
    \begin{itemize}
        \item Target values change as the network parameters update
        \item Creates moving target problem that hinders convergence
        \item Leads to oscillating or diverging behavior
    \end{itemize}
    
    \item \textbf{Exploration Challenges}:
    \begin{itemize}
        \item Fixed ε-greedy exploration may not be optimal
        \item Can get stuck in suboptimal policies
        \item Difficulty balancing exploration and exploitation effectively
    \end{itemize}
    
    \item \textbf{Sample Inefficiency}:
    \begin{itemize}
        \item Requires many samples to learn effective policies
        \item Slow convergence in complex environments
        \item Poor performance in environments with sparse rewards
    \end{itemize}
\end{enumerate}

\subsection{How can some of these issues be mitigated? (You may refer to external sources such as research papers and blog posts be sure to cite them properly.) [3-points]}

Several techniques have been developed to address DQN's limitations:

\begin{enumerate}
    \item \textbf{Double DQN (DDQN)} \cite{van2016deep}:
    \begin{itemize}
        \item Decouples action selection from action evaluation using separate networks
        \item Reduces overestimation bias significantly
        \item Improves stability and performance in most environments
    \end{itemize}
    
    \item \textbf{Prioritized Experience Replay} \cite{schaul2015prioritized}:
    \begin{itemize}
        \item Samples important transitions more frequently
        \item Improves sample efficiency and learning speed
        \item Reduces correlation issues by focusing on informative experiences
    \end{itemize}
    
    \item \textbf{Dueling DQN} \cite{wang2016dueling}:
    \begin{itemize}
        \item Separates value function and advantage function estimation
        \item Better handling of environments where action choice doesn't significantly affect state value
        \item Improved learning efficiency and policy quality
    \end{itemize}
    
    \item \textbf{Distributional RL (C51, QR-DQN)} \cite{bellemare2017distributional}:
    \begin{itemize}
        \item Learns the full distribution of returns instead of just expected values
        \item Provides richer information for decision-making
        \item Can improve performance in stochastic environments
    \end{itemize}
    
    \item \textbf{Noisy Networks} \cite{fortunato2017noisy}:
    \begin{itemize}
        \item Replaces ε-greedy exploration with learned noise in network weights
        \item More sophisticated exploration strategy
        \item Can improve exploration efficiency
    \end{itemize}
    
    \item \textbf{Rainbow DQN} \cite{hessel2018rainbow}:
    \begin{itemize}
        \item Combines multiple improvements: DDQN, Prioritized Replay, Dueling, Distributional RL, Noisy Networks, and N-step returns
        \item State-of-the-art performance on Atari games
        \item Demonstrates the cumulative benefits of multiple improvements
    \end{itemize}
    
    \item \textbf{Target Network Updates}:
    \begin{itemize}
        \item Periodic or soft updates of target network parameters
        \item Reduces non-stationarity of targets
        \item Improves learning stability
    \end{itemize}
\end{enumerate}

\subsection{Based on the plotted values in the notebook, can the main purpose of DDQN be observed in the results? [2-points]}

**Yes, the main purpose of DDQN can be clearly observed in the results.**

\textbf{Evidence of Overestimation Reduction:}

\begin{enumerate}
    \item \textbf{Q-Value Comparison}:
    \begin{itemize}
        \item DDQN shows more conservative, realistic Q-value estimates
        \item Lower maximum Q-values compared to DQN's potentially overestimated values
        \item More stable Q-value evolution over time
    \end{itemize}
    
    \item \textbf{Performance Stability}:
    \begin{itemize}
        \item DDQN achieves consistent high performance (agents reaching 450+ reward threshold)
        \item DQN fails to consistently reach the performance threshold
        \item DDQN's success rate demonstrates more reliable learning
    \end{itemize}
    
    \item \textbf{Learning Curve Analysis}:
    \begin{itemize}
        \item DDQN shows smoother, more stable learning progression
        \item Less erratic behavior compared to DQN's volatile learning curves
        \item More consistent convergence to optimal policies
    \end{itemize}
    
    \item \textbf{Variance Reduction}:
    \begin{itemize}
        \item DDQN exhibits lower variance in performance across different random seeds
        \item Tighter confidence intervals in reward plots
        \item More predictable and stable learning behavior
    \end{itemize}
\end{enumerate}

\textbf{Mechanism Verification:}
The results confirm that DDQN's dual-network architecture successfully:
\begin{itemize}
    \item Reduces overestimation bias by separating action selection and evaluation
    \item Provides more stable learning targets
    \item Enables more reliable convergence to optimal policies
    \item Improves overall performance consistency
\end{itemize}

\subsection{The DDQN paper states that different environments influence the algorithm in various ways. Explain these characteristics (e.g., complexity, dynamics of the environment) and their impact on DDQN's performance. Then, compare them to the CartPole environment. Does CartPole exhibit these characteristics or not? [4-points]}

\textbf{Environment Characteristics Affecting DDQN Performance:}

\begin{enumerate}
    \item \textbf{State Space Complexity}:
    \begin{itemize}
        \item \textbf{High-dimensional states}: Require more sophisticated function approximation
        \item \textbf{Impact on DDQN}: More complex states increase overestimation bias in DQN, making DDQN's bias reduction more valuable
        \item \textbf{CartPole}: Low-dimensional (4D continuous state space) - moderate complexity
    \end{itemize}
    
    \item \textbf{Action Space Characteristics}:
    \begin{itemize}
        \item \textbf{Discrete vs Continuous}: Discrete actions are easier to handle with Q-learning
        \item \textbf{Action dimensionality}: More actions increase the max operator's overestimation effect
        \item \textbf{CartPole}: Binary discrete actions (left/right) - simple action space
    \end{itemize}
    
    \item \textbf{Reward Structure}:
    \begin{itemize}
        \item \textbf{Sparse vs Dense rewards}: Sparse rewards make overestimation more problematic
        \item \textbf{Reward magnitude}: Large reward ranges amplify overestimation effects
        \item \textbf{CartPole}: Dense rewards (+1 per timestep) with clear success/failure signals
    \end{itemize}
    
    \item \textbf{Environment Dynamics}:
    \begin{itemize}
        \item \textbf{Deterministic vs Stochastic}: Stochastic environments increase variance
        \item \textbf{Episodic vs Continuing}: Episodic tasks have clearer termination conditions
        \item \textbf{CartPole}: Deterministic dynamics with episodic structure
    \end{itemize}
    
    \item \textbf{Exploration Requirements}:
    \begin{itemize}
        \item \textbf{Exploration difficulty}: Environments requiring extensive exploration benefit more from DDQN
        \item \textbf{Risk of exploration}: Dangerous environments make overestimation more costly
        \item \textbf{CartPole}: Moderate exploration requirements, low risk
    \end{itemize}
\end{enumerate}

\textbf{CartPole Environment Analysis:}

\textbf{Characteristics CartPole Exhibits:}
\begin{itemize}
    \item \textbf{Moderate State Complexity}: 4D continuous state space provides sufficient complexity for overestimation to occur
    \item \textbf{Clear Success/Failure}: Episodic structure with clear termination conditions
    \item \textbf{Dense Rewards}: Continuous +1 rewards provide immediate feedback
    \item \textbf{Deterministic Dynamics}: Predictable environment behavior
    \item \textbf{Binary Actions}: Simple action space reduces but doesn't eliminate overestimation
\end{itemize}

\textbf{Characteristics CartPole Lacks:}
\begin{itemize}
    \item \textbf{High-dimensional States}: Unlike Atari games with pixel observations
    \item \textbf{Complex Action Spaces}: Unlike environments with many possible actions
    \item \textbf{Sparse Rewards}: Unlike environments where rewards are rare
    \item \textbf{High Risk Exploration}: Unlike environments where exploration can be catastrophic
    \item \textbf{Stochastic Dynamics}: Unlike environments with significant randomness
\end{itemize}

\textbf{Impact on DDQN Performance in CartPole:}
\begin{itemize}
    \item \textbf{Moderate Benefit}: CartPole's characteristics provide moderate conditions for DDQN's advantages
    \item \textbf{Overestimation Reduction}: Still beneficial due to continuous state space and function approximation
    \item \textbf{Stability Improvement}: Clear benefits in learning stability and convergence
    \item \textbf{Performance Gains}: Significant but not as dramatic as in more complex environments
\end{itemize}

\textbf{Comparison to Other Environments:}
\begin{itemize}
    \item \textbf{Atari Games}: DDQN shows much larger improvements due to high-dimensional pixel states and complex action spaces
    \item \textbf{Robotic Control}: DDQN benefits more due to continuous action spaces and sparse rewards
    \item \textbf{Strategic Games}: DDQN shows significant improvements due to complex state spaces and delayed rewards
\end{itemize}

\subsection{How do you think DQN can be further improved? (This question is for your own analysis, but you may refer to external sources such as research papers and blog posts be sure to cite them properly.) [2-points]}

Several promising directions for further improving DQN beyond current methods:

\begin{enumerate}
    \item \textbf{Meta-Learning Approaches} \cite{wang2016learning}:
    \begin{itemize}
        \item Learn to learn better exploration strategies
        \item Adapt hyperparameters during training
        \item Transfer knowledge across different environments
    \end{itemize}
    
    \item \textbf{Model-Based Extensions}:
    \begin{itemize}
        \item Combine model-free Q-learning with learned environment models
        \item Use imagination-based planning for better sample efficiency
        \item Incorporate uncertainty estimates in model predictions
    \end{itemize}
    
    \item \textbf{Multi-Agent Considerations}:
    \begin{itemize}
        \item Develop algorithms robust to non-stationary environments
        \item Handle competitive and cooperative scenarios
        \item Learn opponent modeling strategies
    \end{itemize}
    
    \item \textbf{Advanced Exploration Strategies}:
    \begin{itemize}
        \item Implement curiosity-driven exploration \cite{pathak2017curiosity}
        \item Use information-theoretic exploration measures
        \item Develop adaptive exploration schedules
    \end{itemize}
    
    \item \textbf{Architectural Improvements}:
    \begin{itemize}
        \item Incorporate attention mechanisms for better state representation
        \item Use transformer architectures for sequence modeling
        \item Implement memory-augmented networks for long-term dependencies
    \end{itemize}
    
    \item \textbf{Regularization Techniques}:
    \begin{itemize}
        \item Apply dropout and batch normalization more effectively
        \item Use spectral normalization for training stability
        \item Implement gradient clipping and normalization strategies
    \end{itemize}
    
    \item \textbf{Distributional Extensions}:
    \begin{itemize}
        \item Develop more sophisticated distributional RL methods
        \item Incorporate risk-sensitive learning objectives
        \item Use quantile regression for better uncertainty quantification
    \end{itemize}
    
    \item \textbf{Continual Learning}:
    \begin{itemize}
        \item Prevent catastrophic forgetting in non-stationary environments
        \item Implement elastic weight consolidation techniques
        \item Develop lifelong learning capabilities
    \end{itemize}
\end{enumerate}

\textbf{Promising Research Directions:}
\begin{itemize}
    \item \textbf{Neural Architecture Search}: Automatically design optimal network architectures for specific environments
    \item \textbf{Federated Learning}: Train DQN agents across multiple environments simultaneously
    \item \textbf{Quantum Reinforcement Learning}: Explore quantum computing approaches for Q-learning
    \item \textbf{Neurosymbolic Integration}: Combine neural networks with symbolic reasoning
\end{itemize}



}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\begin{thebibliography}{9}

\bibitem{SuttonBarto}
R. Sutton and A. Barto, Reinforcement Learning: An Introduction, 2nd Edition, 2020. Available: \href{http://incompleteideas.net/book/the-book-2nd.html}{http://incompleteideas.net/book/the-book-2nd.html}.

\bibitem{Gymnasium}
Gymnasium Documentation. Available: \href{https://gymnasium.farama.org/}{https://gymnasium.farama.org/}

\bibitem{Grokking}
Grokking Deep Reinforcement Learning. Available: \href{https://www.manning.com/books/grokking-deep-reinforcement-learning}{https://www.manning.com/books/grokking-deep-reinforcement-learning}

\bibitem{van2016deep}
H. van Hasselt, A. Guez, and D. Silver, "Deep reinforcement learning with double q-learning," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 30, no. 1, 2016.

\bibitem{schaul2015prioritized}
T. Schaul, J. Quan, I. Antonoglou, and D. Silver, "Prioritized experience replay," arXiv preprint arXiv:1511.05952, 2015.

\bibitem{wang2016dueling}
Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas, "Dueling network architectures for deep reinforcement learning," in International conference on machine learning, 2016, pp. 1995-2003.

\bibitem{bellemare2017distributional}
M. G. Bellemare, W. Dabney, and R. Munos, "A distributional perspective on reinforcement learning," in International Conference on Machine Learning, 2017, pp. 449-458.

\bibitem{fortunato2017noisy}
M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis, O. Pietquin, C. Blundell, and A. Legg, "Noisy networks for exploration," arXiv preprint arXiv:1706.10295, 2017.

\bibitem{hessel2018rainbow}
M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, "Rainbow: Combining improvements in deep reinforcement learning," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018.

\bibitem{wang2016learning}
J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick, "Learning to reinforcement learn," arXiv preprint arXiv:1611.05763, 2016.

\bibitem{pathak2017curiosity}
D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, "Curiosity-driven exploration by self-supervised prediction," in International conference on machine learning, 2017, pp. 2778-2787.

\bibitem{CoverImage}
\href{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon_16717130.htm}{Cover image designed by freepik}

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
