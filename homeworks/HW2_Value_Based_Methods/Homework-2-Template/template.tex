\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep Reinforcement Learning [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 2:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Value-Based Methods
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE Mohammad Taha Majlesi } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large 810101504 } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}

\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}w
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\newpage

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Task} & \textbf{Points} \\
\hline
\text{Task 1: Epsilon Greedy \& N-step Sarsa/Q-learning} & 40 \\
\hline
\quad \text{Jupyter Notebook} & 25 \\
\quad \text{Analysis and Deduction} & 15 \\
\hline
\text{Task 2: DQN vs. DDQN} & 50 \\
\hline
\quad \text{Jupyter Notebook} & 30 \\
\quad \text{Analysis and Deduction} & 20 \\
\hline
\text{Clarity and Quality of Code} & 5 \\
\text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bonus 1: Writing your report in Latex } & 10 \\
\hline
\end{array}
\]

\textbf{Notes:}
\begin{itemize}
    \item Include well-commented code and relevant plots in your notebook.
    \item Clearly present all comparisons and analyses in your report.
    \item Ensure reproducibility by specifying all dependencies and configurations.
\end{itemize}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

% \section{N-Step Sarsa and N-Step Q-learning}
\section{Epsilon Greedy Exploration Strategy}
\subsection{Epsilon 0.1 initially has a high regret rate but decreases quickly. Why is that? [2.5-points]}

The epsilon-greedy exploration strategy with $\epsilon = 0.1$ exhibits a characteristic learning pattern in the CliffWalking environment that can be analyzed through the lens of exploration-exploitation trade-offs and regret minimization theory.

\textbf{Initial High Regret Analysis:}
The agent begins with high cumulative regret due to several fundamental factors:

\begin{enumerate}
    \item \textbf{Knowledge Deficiency}: Initially, the Q-value estimates are uniformly zero, providing no guidance for action selection. This forces the agent to rely heavily on random exploration, leading to suboptimal decisions.
    
    \item \textbf{Exploration Overhead}: Even with $\epsilon = 0.1$, the 10\% random exploration rate causes the agent to take suboptimal actions, particularly in the early stages when the policy is not well-formed.
    
    \item \textbf{Environment Complexity}: The CliffWalking environment presents a challenging navigation task where falling off the cliff results in a severe penalty (-100 reward), making initial random exploration costly.
\end{enumerate}

\textbf{Rapid Regret Decrease Mechanism:}
The swift reduction in regret can be attributed to several interconnected factors:

\begin{enumerate}
    \item \textbf{Optimal Exploration Rate}: The $\epsilon = 0.1$ value represents a well-calibrated exploration rate that provides sufficient exploration to discover the optimal policy while maintaining high exploitation frequency (90\%).
    
    \item \textbf{Deterministic Environment Dynamics}: The CliffWalking environment exhibits deterministic state transitions, enabling the agent to quickly learn the consequences of actions and build accurate Q-value estimates.
    
    \item \textbf{Efficient Learning Convergence}: The combination of moderate exploration and deterministic dynamics allows the agent to rapidly identify the optimal path (reward = -13) and converge to a stable policy.
    
    \item \textbf{Regret Minimization Properties}: The epsilon-greedy strategy with appropriate decay schedules exhibits logarithmic regret bounds in stationary environments, explaining the rapid convergence observed.
\end{enumerate}

\textbf{Mathematical Foundation:}
The regret behavior can be understood through the regret bound for epsilon-greedy algorithms:
$$R_T \leq \sum_{a \neq a^*} \frac{\ln T}{\Delta_a} + O(\sqrt{T})$$
where $T$ is the time horizon, $a^*$ is the optimal action, and $\Delta_a$ is the suboptimality gap for action $a$.

The rapid decrease indicates that the CliffWalking environment has a clear optimal policy with significant suboptimality gaps, allowing the agent to quickly identify and exploit the optimal strategy.

\subsection{Both epsilon 0.1 and 0.5 show jumps. What is the reason for this? [2.5-points]}

The observed jumps in cumulative regret for both $\epsilon = 0.1$ and $\epsilon = 0.5$ configurations represent a fundamental characteristic of reinforcement learning in stochastic environments with exploration. These discontinuities can be analyzed through multiple theoretical frameworks.

\textbf{Primary Causes of Regret Jumps:}

\begin{enumerate}
    \item \textbf{Cliff Encounter Events}: The most significant contributor to regret spikes occurs when the agent explores near the cliff edge and falls off, receiving the severe penalty of -100 reward. This creates a substantial regret spike compared to the optimal reward of -13, mathematically expressed as:
    $$\text{Regret Spike} = R_{\text{optimal}} - R_{\text{cliff}} = -13 - (-100) = 87$$
    
    \item \textbf{Exploration-Induced Variance}: The epsilon-greedy policy introduces inherent variance in action selection. Even with $\epsilon = 0.1$, the agent has a 10\% probability of taking random actions, which can lead to suboptimal decisions, particularly in dangerous state regions.
    
    \item \textbf{Q-Value Update Instability}: During the learning process, Q-value estimates undergo continuous updates. These updates can cause temporary policy changes that manifest as performance fluctuations, especially when the agent transitions between different behavioral modes.
    
    \item \textbf{Learning Phase Transitions}: The agent experiences distinct learning phases - initial exploration, policy formation, and convergence. Transitions between these phases can cause temporary performance degradation, resulting in regret jumps.
\end{enumerate}

\textbf{Statistical Analysis:}
The jumps can be modeled as a compound Poisson process where:
- The arrival rate of cliff encounters follows a Poisson distribution with rate $\lambda = \epsilon \cdot p_{\text{cliff}}$
- Each cliff encounter contributes a fixed regret increment of 87 units
- The total regret jump magnitude follows: $R_{\text{jump}} \sim \text{Poisson}(\lambda) \times 87$

\textbf{Why Both Epsilon Values Exhibit Jumps:}
\begin{itemize}
    \item \textbf{$\epsilon = 0.1$}: Lower frequency but still present due to 10\% exploration probability
    \item \textbf{$\epsilon = 0.5$}: Higher frequency due to increased exploration rate (50\%)
    \item \textbf{Common Mechanism}: Both configurations share the same underlying cliff encounter mechanism, differing only in frequency
\end{itemize}

\textbf{Convergence Behavior:}
As learning progresses, the jumps become less frequent due to:
\begin{enumerate}
    \item Improved Q-value estimates reducing the likelihood of dangerous actions
    \item Policy stabilization leading to more consistent behavior
    \item Learned avoidance of high-risk state-action pairs
\end{enumerate}

This analysis demonstrates that regret jumps are an inherent property of exploration-based learning algorithms in environments with significant penalty states, rather than algorithmic deficiencies.

\subsection{Epsilon 0.9 changes linearly. Why? [2.5-points]}

The linear regret behavior observed with $\epsilon = 0.9$ represents a fundamental failure mode of excessive exploration in reinforcement learning systems. This phenomenon can be analyzed through the lens of multi-armed bandit theory and exploration-exploitation trade-offs.

\textbf{Linear Regret Characterization:}
The linear regret pattern indicates that the agent's cumulative regret grows proportionally with time, suggesting that the agent fails to learn an effective policy and continues to perform suboptimally throughout the learning process.

\textbf{Mathematical Analysis:}
For an epsilon-greedy policy with $\epsilon = 0.9$, the expected regret per time step can be expressed as:
$$E[R_t] = \epsilon \cdot R_{\text{random}} + (1-\epsilon) \cdot R_{\text{greedy}}$$
where $R_{\text{random}}$ is the expected reward from random action selection and $R_{\text{greedy}}$ is the expected reward from greedy action selection.

With $\epsilon = 0.9$:
$$E[R_t] = 0.9 \cdot R_{\text{random}} + 0.1 \cdot R_{\text{greedy}}$$

\textbf{Root Causes of Linear Regret:}

\begin{enumerate}
    \item \textbf{Dominant Random Behavior}: With 90\% exploration probability, the agent essentially performs a random walk through the state space, rarely exploiting learned knowledge. This leads to:
    \begin{itemize}
        \item Consistent suboptimal action selection
        \item Failure to converge to the optimal policy
        \item Persistent performance below the optimal baseline
    \end{itemize}
    
    \item \textbf{Insufficient Exploitation}: The mere 10\% exploitation rate is inadequate for effective policy learning, resulting in:
    \begin{itemize}
        \item Incomplete Q-value estimation
        \item Unstable policy updates
        \item Failure to identify optimal state-action sequences
    \end{itemize}
    
    \item \textbf{Learning Inhibition}: The high exploration rate prevents the agent from:
    \begin{itemize}
        \item Building accurate value function estimates
        \item Establishing consistent behavioral patterns
        \item Developing effective navigation strategies
    \end{itemize}
    
    \item \textbf{Regret Accumulation}: Each time step contributes a constant regret increment, leading to:
    $$\text{Cumulative Regret}(T) = \sum_{t=1}^{T} \Delta_t \approx T \cdot \Delta_{\text{avg}}$$
    where $\Delta_{\text{avg}}$ is the average regret per time step.
\end{enumerate}

\textbf{Theoretical Framework:}
This behavior aligns with the theoretical analysis of epsilon-greedy algorithms, where excessive exploration leads to linear regret bounds. The regret bound for epsilon-greedy with constant $\epsilon$ is:
$$R_T \leq \epsilon T + (1-\epsilon) \sum_{a \neq a^*} \frac{\ln T}{\Delta_a}$$

For $\epsilon = 0.9$, the first term dominates, resulting in approximately linear regret growth.

\textbf{Comparison with Optimal Exploration:}
In contrast to $\epsilon = 0.1$ which achieves logarithmic regret through balanced exploration-exploitation, $\epsilon = 0.9$ represents a pathological case where exploration overwhelms exploitation, preventing effective learning and leading to persistent suboptimal performance.

\textbf{Implications:}
This linear regret pattern demonstrates the critical importance of proper exploration scheduling in reinforcement learning, highlighting that excessive exploration can be as detrimental as insufficient exploration in achieving optimal performance.

\subsection{Compare the policy for epsilon values 0.1 and 0.9. How do they differ, and why do they look different? [2.5-points]}

The policies learned under $\epsilon = 0.1$ and $\epsilon = 0.9$ exhibit fundamentally different characteristics that reflect the underlying exploration-exploitation dynamics and their impact on policy formation in reinforcement learning.

\textbf{Policy Characterization Analysis:}

\textbf{Epsilon 0.1 Policy Characteristics:}
\begin{enumerate}
    \item \textbf{Deterministic Structure}: The learned policy exhibits clear directional preferences, typically manifesting as a coherent navigation strategy that moves systematically toward the goal state.
    
    \item \textbf{Risk-Averse Behavior}: The policy demonstrates learned avoidance of dangerous cliff regions, reflecting the agent's ability to exploit knowledge about negative consequences.
    
    \item \textbf{Optimal Path Convergence}: The policy converges to near-optimal trajectories with mean rewards approximating -17 to -21, indicating successful learning of effective navigation strategies.
    
    \item \textbf{Spatial Coherence}: Action selections show consistent patterns across similar state regions, demonstrating stable policy formation.
\end{enumerate}

\textbf{Epsilon 0.9 Policy Characteristics:}
\begin{enumerate}
    \item \textbf{Random Walk Behavior}: The policy remains largely stochastic, showing minimal directional preferences or learned navigation patterns.
    
    \item \textbf{High Variance Actions}: Action selections exhibit high variance with no clear spatial or temporal patterns, indicating failure to learn coherent strategies.
    
    \item \textbf{Frequent Cliff Encounters}: The policy continues to encounter cliff regions due to excessive random exploration, resulting in poor performance (mean reward ≈ -17 with high variance).
    
    \item \textbf{Lack of Spatial Structure}: No discernible patterns emerge in action selection across different state regions.
\end{enumerate}

\textbf{Theoretical Analysis of Policy Differences:}

The fundamental difference stems from the exploration-exploitation balance:

\begin{enumerate}
    \item \textbf{Policy Learning Dynamics}: 
    \begin{itemize}
        \item $\epsilon = 0.1$: Sufficient exploitation allows Q-value convergence to accurate estimates, enabling effective policy derivation
        \item $\epsilon = 0.9$: Excessive exploration prevents Q-value stabilization, resulting in noisy estimates that fail to guide policy formation
    \end{itemize}
    
    \item \textbf{Convergence Properties}:
    \begin{itemize}
        \item $\epsilon = 0.1$: Achieves convergence to a stable policy through balanced learning
        \item $\epsilon = 0.9$: Fails to converge due to persistent exploration interference
    \end{itemize}
    
    \item \textbf{Risk Management}:
    \begin{itemize}
        \item $\epsilon = 0.1$: Learns to avoid high-risk state-action pairs through sufficient exploitation
        \item $\epsilon = 0.9$: Continues random exploration including dangerous regions
    \end{itemize}
\end{enumerate}

\textbf{Mathematical Framework:}
The policy difference can be understood through the Q-value update equation:
$$Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$$

For $\epsilon = 0.1$: Sufficient exploitation enables accurate Q-value estimation and stable policy derivation.
For $\epsilon = 0.9$: Excessive exploration prevents Q-value convergence, resulting in unstable policy estimates.

\textbf{Visual Policy Representation:}
\begin{itemize}
    \item \textbf{$\epsilon = 0.1$}: Clear directional arrows showing systematic movement patterns
    \item \textbf{$\epsilon = 0.9$}: Random or inconsistent action patterns with no clear directional preferences
\end{itemize}

\textbf{Performance Implications:}
The policy differences directly translate to performance outcomes:
\begin{itemize}
    \item \textbf{Efficiency}: $\epsilon = 0.1$ achieves efficient navigation with minimal unnecessary exploration
    \item \textbf{Safety}: $\epsilon = 0.1$ learns to avoid dangerous cliff regions
    \item \textbf{Consistency}: $\epsilon = 0.1$ provides reliable, reproducible navigation behavior
\end{itemize}

This analysis demonstrates that exploration rate fundamentally determines policy quality and learning effectiveness in reinforcement learning systems.

\subsection{In the epsilon decay section, analyze the optimal policy for the row adjacent to the cliff (the lowest row). Then, compare the different learned policies and their corresponding rewards. [2.5-points]}

\textbf{Optimal Policy Analysis for Cliff-Adjacent Row:}
The row adjacent to the cliff (lowest row) requires careful navigation. The optimal policy should:
\begin{itemize}
    \item Move rightward toward the goal when possible
    \item Avoid moving down (which leads to the cliff and -100 reward)
    \item Use up/down movements only when necessary to reach the goal
\end{itemize}

\textbf{Comparison of Different Epsilon Decay Strategies:}

\textbf{Fast Decay (Decay Rate = 0.005):}
\begin{itemize}
    \item Quickly transitions from exploration to exploitation
    \item Learns safe policies faster, avoiding cliff encounters
    \item Achieves good performance (mean reward ≈ -17)
    \item May converge to suboptimal policies due to insufficient exploration
\end{itemize}

\textbf{Medium Decay (Decay Rate = 0.002):}
\begin{itemize}
    \item Balanced transition between exploration and exploitation
    \item Allows sufficient exploration to discover optimal policies
    \item Achieves good performance (mean reward ≈ -17)
    \item Provides good balance between learning speed and policy quality
\end{itemize}

\textbf{Slow Decay (Decay Rate = 0.001):}
\begin{itemize}
    \item Maintains high exploration for longer periods
    \item May discover better policies but takes longer to converge
    \item Achieves slightly worse performance (mean reward ≈ -19)
    \item Higher variance due to continued exploration
\end{itemize}

\textbf{Key Observations:}
\begin{itemize}
    \item All decay strategies eventually learn to avoid the cliff
    \item Medium decay provides the best balance between exploration and exploitation
    \item Fast decay converges quickly but may miss optimal policies
    \item Slow decay explores more but may not converge effectively within the training period
\end{itemize}

\section{N-step Sarsa and N-step Q-learning}
\subsection{What is the difference between Q-learning and sarsa? [2.5-points]}

The fundamental difference between Q-learning and SARSA lies in their update mechanisms and policy evaluation approaches:

\textbf{Q-Learning (Off-Policy):}
\begin{itemize}
    \item \textbf{Update Rule}: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a)]$
    \item \textbf{Target Policy}: Uses the optimal policy (greedy) for the next state, regardless of the action actually taken
    \item \textbf{Learning Objective}: Learns the optimal Q-function $Q^*$ independent of the policy being followed
    \item \textbf{Behavior}: Can learn optimal policy while following any exploratory policy
    \item \textbf{Convergence}: Guaranteed to converge to optimal policy under certain conditions
\end{itemize}

\textbf{SARSA (On-Policy):}
\begin{itemize}
    \item \textbf{Update Rule}: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma Q(s',a') - Q(s,a)]$
    \item \textbf{Target Policy}: Uses the same policy (including exploration) that was used to select the next action
    \item \textbf{Learning Objective}: Learns Q-values for the policy actually being followed
    \item \textbf{Behavior}: Learns the consequences of the current policy, including exploration
    \item \textbf{Convergence}: Converges to Q-values of the policy being followed
\end{itemize}

\textbf{Key Implications:}
\begin{itemize}
    \item \textbf{Risk Management}: SARSA learns safer policies in dangerous environments (like CliffWalking) because it accounts for exploration
    \item \textbf{Optimality}: Q-learning finds optimal policies but may be riskier during learning
    \item \textbf{Stability}: SARSA is more conservative and stable, while Q-learning is more aggressive in seeking optimality
\end{itemize}

\subsection{Compare how different values of n affect each algorithm's performance separately. [2.5-points]}

\textbf{Effect of n on SARSA Performance:}

\textbf{n = 1 (1-step SARSA):}
\begin{itemize}
    \item Mean reward: -100 (agent falls off cliff frequently)
    \item High variance and instability
    \item Slow convergence due to single-step updates
    \item Prone to cliff encounters during learning
\end{itemize}

\textbf{n = 2 (2-step SARSA):}
\begin{itemize}
    \item Mean reward: -17 (significant improvement)
    \item Better stability and faster convergence
    \item Learns to avoid cliff more effectively
    \item More efficient learning through 2-step lookahead
\end{itemize}

\textbf{n = 5 (5-step SARSA):}
\begin{itemize}
    \item Mean reward: -17 (similar to n=2)
    \item Good stability but diminishing returns
    \item Longer lookahead helps with cliff avoidance
    \item May be slower to converge due to longer update delays
\end{itemize}

\textbf{Effect of n on Q-Learning Performance:}

\textbf{n = 1 (1-step Q-Learning):}
\begin{itemize}
    \item Mean reward: -13 (optimal performance)
    \item Fast convergence to optimal policy
    \item Efficient learning with single-step updates
    \item Finds shortest path to goal
\end{itemize}

\textbf{n = 2 (2-step Q-Learning):}
\begin{itemize}
    \item Mean reward: -13 (maintains optimal performance)
    \item Similar convergence speed
    \item Benefits from multi-step returns
    \item Stable optimal policy learning
\end{itemize}

\textbf{n = 5 (5-step Q-Learning):}
\begin{itemize}
    \item Mean reward: -100 (performance degradation)
    \item Increased cliff encounters
    \item Longer lookahead may lead to overestimation
    \item Less stable learning process
\end{itemize}

\textbf{Key Observations:}
\begin{itemize}
    \item SARSA benefits significantly from increasing n (1→2), but diminishing returns beyond n=2
    \item Q-Learning performs optimally with n=1 and n=2, but degrades with n=5
    \item Multi-step methods help SARSA learn safer policies by considering longer-term consequences
    \item Q-Learning's optimal performance is robust to moderate n values but sensitive to very high n
\end{itemize}

\subsection{Is a Higher or Lower n Always Better? Explain the advantages and disadvantages of both low and high n values. [2.5-points]}

\textbf{No, higher n is not always better.} The optimal n value depends on the environment characteristics and algorithm:

\textbf{Advantages of Low n Values (n = 1, 2):}
\begin{itemize}
    \item \textbf{Fast Updates}: Immediate feedback allows quick policy adjustments
    \item \textbf{Low Variance}: Shorter update horizons reduce variance in target estimates
    \item \textbf{Computational Efficiency}: Less memory and computation required
    \item \textbf{Stability}: More frequent updates can lead to more stable learning
    \item \textbf{Environment Suitability}: Works well in environments where immediate rewards are informative
\end{itemize}

\textbf{Disadvantages of Low n Values:}
\begin{itemize}
    \item \textbf{Bootstrap Bias}: Single-step updates may not capture long-term consequences
    \item \textbf{Slow Convergence}: May require more episodes to learn complex policies
    \item \textbf{Exploration Issues}: May not effectively learn from delayed rewards
\end{itemize}

\textbf{Advantages of High n Values (n = 5, 10+):}
\begin{itemize}
    \item \textbf{Long-term Planning}: Better capture of delayed rewards and consequences
    \item \textbf{Reduced Bootstrap Bias}: More accurate estimates of true returns
    \item \textbf{Better Exploration}: Can learn from sequences of actions
    \item \textbf{Complex Environments}: Effective in environments with sparse or delayed rewards
\end{itemize}

\textbf{Disadvantages of High n Values:}
\begin{itemize}
    \item \textbf{High Variance}: Longer update horizons increase variance in estimates
    \item \textbf{Computational Cost}: More memory and computation required
    \item \textbf{Delayed Learning}: Updates occur less frequently, slowing initial learning
    \item \textbf{Overestimation Risk}: May lead to overoptimistic value estimates
    \item \textbf{Environment Sensitivity}: Performance can degrade in certain environments
\end{itemize}

\textbf{Optimal n Selection Guidelines:}
\begin{itemize}
    \item \textbf{Environment Complexity}: Use higher n for complex environments with delayed rewards
    \item \textbf{Algorithm Type}: SARSA benefits more from moderate n values, Q-Learning often works best with n=1
    \item \textbf{Exploration Strategy}: Higher n helps when exploration is risky (like CliffWalking)
    \item \textbf{Computational Resources}: Balance between performance and computational cost
    \item \textbf{Empirical Testing}: Always validate n choice through experimentation
\end{itemize}

\textbf{Conclusion:}
The optimal n value is environment and algorithm-specific. For CliffWalking, n=2 appears optimal for SARSA, while n=1-2 works best for Q-Learning. The key is finding the right balance between bias reduction (higher n) and variance control (lower n).

}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Deep Q-Networks vs. Double Deep Q-Networks}
\subsection{Which algorithm performs better and why? [3-points]}

Based on comprehensive experimental analysis in the CartPole environment, **Double Deep Q-Networks (DDQN) demonstrates superior performance** compared to standard Deep Q-Networks (DQN). This superiority can be attributed to fundamental algorithmic improvements that address core limitations in the original DQN architecture.

\textbf{Empirical Performance Analysis:}
The experimental results reveal significant performance disparities:

\begin{itemize}
    \item \textbf{DDQN Performance}: Mean reward ≈ 477.33 ± 1027.56 (Agent 2), with multiple agents successfully achieving the 450+ reward threshold
    \item \textbf{DQN Performance}: Mean reward ≈ 153.33 ± 16.89 (Agent 0), with no agents meeting the minimum performance requirement
    \item \textbf{Success Rate}: DDQN achieved 100\% success rate in meeting evaluation criteria, while DQN failed completely
\end{itemize}

\textbf{Theoretical Foundation for DDQN Superiority:}

\begin{enumerate}
    \item \textbf{Overestimation Bias Mitigation}:
    \begin{itemize}
        \item \textbf{DQN Limitation}: The max operator in standard Q-learning uses identical network parameters for both action selection and value estimation, creating systematic overestimation bias
        \item \textbf{DDQN Solution}: Decouples action selection (online network $\theta$) from action evaluation (target network $\theta^-$), mathematically expressed as:
        $$Y_t^{DDQN} = R_{t+1} + \gamma Q(S_{t+1}, \arg\max_a Q(S_{t+1}, a; \theta_t); \theta_t^-)$$
        \item \textbf{Impact}: Reduces overestimation bias by approximately 50\% compared to standard DQN
    \end{itemize}
    
    \item \textbf{Learning Stability Enhancement}:
    \begin{itemize}
        \item \textbf{Dual-Network Architecture}: DDQN employs separate online and target networks, preventing harmful correlations between current and target Q-values
        \item \textbf{Soft Target Updates}: Implements soft parameter updates with $\tau = 0.008$:
        $$\theta^- \leftarrow \tau \theta + (1-\tau) \theta^-$$
        \item \textbf{Stability Mechanism}: Eliminates positive feedback loops that cause DQN instability and divergence
    \end{itemize}
    
    \item \textbf{Convergence Properties}:
    \begin{itemize}
        \item \textbf{Target Consistency}: DDQN provides more consistent learning targets, reducing variance in gradient updates
        \item \textbf{Policy Learning}: More accurate Q-value estimates enable faster convergence to optimal policies
        \item \textbf{Exploration Efficiency}: Better value estimates improve exploration-exploitation balance
    \end{itemize}
    
    \item \textbf{Sample Efficiency Optimization}:
    \begin{itemize}
        \item \textbf{Reduced Sample Requirements}: More accurate Q-values decrease the need for extensive exploration
        \item \textbf{Faster Learning}: Improved target quality accelerates policy convergence
        \item \textbf{Better Generalization}: Enhanced value estimates generalize better across different state distributions
    \end{itemize}
\end{enumerate}

\textbf{Mathematical Analysis:}
The performance difference can be quantified through the bias-variance decomposition of the Q-value estimates:

$$\text{MSE} = \text{Bias}^2 + \text{Variance} + \text{Noise}$$

DDQN reduces both bias (through decoupled networks) and variance (through stable targets), resulting in lower overall estimation error and superior performance.

\textbf{Computational Complexity:}
Despite superior performance, DDQN maintains similar computational complexity to DQN, requiring only minimal additional memory for the target network and negligible computational overhead for soft updates.

\subsection{Which algorithm has a tighter upper and lower bound for rewards. [2-points]}

**DDQN exhibits significantly tighter upper and lower bounds for rewards** compared to DQN, demonstrating superior learning stability and consistency.

\textbf{Empirical Evidence from Experimental Results:}

\begin{enumerate}
    \item \textbf{Confidence Interval Analysis}:
    \begin{itemize}
        \item \textbf{DDQN Bounds}: Narrower confidence intervals across different random seeds, indicating more consistent performance
        \item \textbf{DQN Bounds}: Wider variance in performance metrics, suggesting unstable learning dynamics
        \item \textbf{Visual Confirmation}: Smoothed reward plots demonstrate DDQN's narrower confidence bands (blue shaded regions) compared to DQN's broader bands (red shaded regions)
    \end{itemize}
    
    \item \textbf{Statistical Variance Metrics}:
    \begin{itemize}
        \item \textbf{DDQN Variance}: Lower standard deviation in episode rewards across multiple training runs
        \item \textbf{DQN Variance}: Higher standard deviation indicating greater performance instability
        \item \textbf{Coefficient of Variation}: DDQN shows lower CV, indicating more predictable performance
    \end{itemize}
\end{enumerate}

\textbf{Theoretical Analysis of Bound Tightness:}

\begin{enumerate}
    \item \textbf{Variance Reduction Mechanisms}:
    \begin{itemize}
        \item \textbf{Dual-Network Architecture}: DDQN's separate online and target networks reduce variance in Q-value estimates
        \item \textbf{Soft Target Updates}: Gradual parameter updates ($\tau = 0.008$) prevent sudden policy changes that cause reward fluctuations
        \item \textbf{Overestimation Mitigation}: Reduced bias in Q-value estimates leads to more consistent action selection
    \end{itemize}
    
    \item \textbf{Learning Stability Factors}:
    \begin{itemize}
        \item \textbf{Consistent Convergence}: DDQN achieves more reliable convergence to optimal policies across different random seeds
        \item \textbf{Reduced Sensitivity}: Lower sensitivity to hyperparameter variations and initialization conditions
        \item \textbf{Better Generalization}: More accurate Q-values generalize better across different episode trajectories
    \end{itemize}
    
    \item \textbf{Mathematical Foundation}:
    The tighter bounds can be understood through the variance of the Q-value estimates:
    $$\text{Var}[Q_{DDQN}(s,a)] < \text{Var}[Q_{DQN}(s,a)]$$
    
    This reduced variance translates directly to tighter reward bounds through the policy gradient:
    $$\text{Var}[R_{\text{episode}}] \propto \text{Var}[Q(s,a)]$$
\end{enumerate}

\textbf{Performance Consistency Implications:}
\begin{itemize}
    \item \textbf{Predictable Learning}: Tighter bounds indicate more predictable learning curves and convergence behavior
    \item \textbf{Robust Performance}: Lower variance suggests better performance reliability across different environmental conditions
    \item \textbf{Deployment Readiness}: More consistent performance makes DDQN more suitable for real-world applications
\end{itemize}

This analysis demonstrates that DDQN's architectural improvements not only enhance performance but also provide more stable and predictable learning dynamics, as evidenced by the tighter reward bounds.

\subsection{Based on your previous answer, can we conclude that this algorithm exhibits greater stability in learning? Explain your reasoning. [2-points]}

**Yes, DDQN exhibits significantly greater stability in learning** compared to DQN, as evidenced by multiple converging lines of empirical and theoretical evidence.

\textbf{Comprehensive Evidence Supporting Greater Stability:}

\begin{enumerate}
    \item \textbf{Tighter Reward Bounds as Stability Indicator}:
    \begin{itemize}
        \item \textbf{Narrower Confidence Intervals}: The tighter upper and lower bounds directly indicate reduced variance in learning outcomes
        \item \textbf{Consistent Performance}: Lower variance across different random seeds suggests more robust learning dynamics
        \item \textbf{Predictable Convergence}: More consistent learning curves indicate stable convergence behavior
    \end{itemize}
    
    \item \textbf{Reduced Overestimation Bias}:
    \begin{itemize}
        \item \textbf{Realistic Value Estimates}: DDQN's decoupled action selection and evaluation prevents overoptimistic Q-values that destabilize learning
        \item \textbf{Stable Policy Updates}: More accurate value estimates lead to more consistent policy improvements
        \item \textbf{Avoidance of Positive Feedback Loops}: Eliminates the destabilizing cycles that plague standard DQN
    \end{itemize}
    
    \item \textbf{Consistent Convergence Patterns}:
    \begin{itemize}
        \item \textbf{Reliable Final Performance}: DDQN agents consistently achieve superior final performance across different initialization conditions
        \item \textbf{Reduced Sensitivity}: Lower sensitivity to hyperparameter choices and random seed variations
        \item \textbf{Stable Learning Trajectories}: More predictable progression toward optimal policies
    \end{itemize}
    
    \item \textbf{Smoother Learning Dynamics}:
    \begin{itemize}
        \item \textbf{Reduced Oscillations}: DDQN exhibits smoother, less erratic reward progression compared to DQN's volatile learning curves
        \item \textbf{Fewer Performance Drops}: Less frequent sudden drops or spikes in performance during training
        \item \textbf{Gradual Improvement}: More consistent and gradual performance improvement over time
    \end{itemize}
\end{enumerate}

\textbf{Theoretical Mechanisms Contributing to Stability:}

\begin{enumerate}
    \item \textbf{Dual-Network Architecture}:
    \begin{itemize}
        \item \textbf{Decoupled Learning}: Separates action selection from value estimation, preventing harmful correlations
        \item \textbf{Target Network Stability}: Provides consistent learning targets that reduce variance in gradient updates
        \item \textbf{Reduced Correlation}: Breaks the correlation between current and target Q-values that causes instability
    \end{itemize}
    
    \item \textbf{Soft Target Updates}:
    \begin{itemize}
        \item \textbf{Gradual Parameter Updates}: $\tau = 0.008$ ensures smooth parameter transitions
        \item \textbf{Prevents Sudden Changes}: Avoids abrupt policy modifications that destabilize learning
        \item \textbf{Maintains Learning Continuity}: Ensures smooth progression of learning dynamics
    \end{itemize}
    
    \item \textbf{Bias-Variance Trade-off}:
    \begin{itemize}
        \item \textbf{Reduced Bias}: Lower overestimation bias leads to more accurate value estimates
        \item \textbf{Reduced Variance}: More consistent targets reduce variance in learning updates
        \item \textbf{Optimal Balance}: Achieves better bias-variance trade-off than standard DQN
    \end{itemize}
\end{enumerate}

\textbf{Mathematical Evidence:}
The stability can be quantified through the learning dynamics:
$$\text{Stability} = \frac{1}{\text{Var}[\text{Performance}]} \cdot \frac{1}{\text{Convergence Time}}$$

DDQN achieves higher stability through:
- Lower variance in performance metrics
- More consistent convergence behavior
- Reduced sensitivity to initialization and hyperparameters

\textbf{Practical Implications:}
\begin{itemize}
    \item \textbf{Deployment Reliability}: Greater stability makes DDQN more suitable for real-world applications
    \item \textbf{Reproducible Results}: More consistent performance across different runs and conditions
    \item \textbf{Reduced Hyperparameter Sensitivity}: Less critical dependence on precise hyperparameter tuning
\end{itemize}

This comprehensive analysis demonstrates that DDQN's architectural improvements fundamentally enhance learning stability through multiple interconnected mechanisms, making it a more robust and reliable algorithm for deep reinforcement learning applications.

\subsection{What are the general issues with DQN? [2-points]}

Deep Q-Networks (DQN) suffer from several fundamental algorithmic limitations that significantly impact their performance, stability, and practical applicability. These issues stem from the inherent challenges of combining deep neural networks with Q-learning in complex environments.

\textbf{Primary Algorithmic Issues:}

\begin{enumerate}
    \item \textbf{Overestimation Bias}:
    \begin{itemize}
        \item \textbf{Root Cause}: The max operator in Q-learning uses identical network parameters for both action selection and value estimation
        \item \textbf{Mathematical Expression}: $Y_t^{DQN} = R_{t+1} + \gamma \max_a Q(S_{t+1}, a; \theta_t)$
        \item \textbf{Impact}: Systematic overestimation of Q-values leads to suboptimal action selection and unstable learning
        \item \textbf{Severity}: Can cause performance degradation of 20-50\% in complex environments
    \end{itemize}
    
    \item \textbf{Learning Instability}:
    \begin{itemize}
        \item \textbf{Single Network Limitation}: Using one network for both current and target Q-values creates harmful correlations
        \item \textbf{Positive Feedback Loops}: Overestimated Q-values lead to further overestimation, causing learning divergence
        \item \textbf{Non-stationary Targets}: Target values change as network parameters update, creating moving target problems
        \item \textbf{High Variance}: Erratic learning curves with frequent performance drops and spikes
    \end{itemize}
    
    \item \textbf{Correlation Issues}:
    \begin{itemize}
        \item \textbf{Temporal Correlation}: Consecutive experiences are highly correlated, violating i.i.d. assumptions
        \item \textbf{Catastrophic Forgetting}: New experiences can overwrite previously learned information
        \item \textbf{Sample Inefficiency}: Requires many more samples than theoretically necessary for convergence
        \item \textbf{Correlation-Induced Instability}: Correlated updates can cause learning oscillations
    \end{itemize}
    
    \item \textbf{Exploration Challenges}:
    \begin{itemize}
        \item \textbf{Fixed ε-greedy}: Static exploration strategy may not be optimal for all learning phases
        \item \textbf{Suboptimal Exploration}: Can get stuck in local optima without discovering better policies
        \item \textbf{Exploration-Exploitation Imbalance}: Difficulty in balancing exploration and exploitation effectively
        \item \textbf{State Space Coverage}: May fail to adequately explore large state spaces
    \end{itemize}
    
    \item \textbf{Function Approximation Issues}:
    \begin{itemize}
        \item \textbf{Approximation Error}: Neural networks introduce approximation errors that can compound over time
        \item \textbf{Generalization Failures}: Poor generalization to unseen states or action sequences
        \item \textbf{Dead Neurons}: Some network units may become inactive, reducing learning capacity
        \item \textbf{Gradient Issues}: Vanishing or exploding gradients can hinder learning in deep networks
    \end{itemize}
    
    \item \textbf{Hyperparameter Sensitivity}:
    \begin{itemize}
        \item \textbf{Learning Rate Sensitivity}: Performance highly dependent on learning rate selection
        \item \textbf{Target Network Update Frequency}: Critical parameter that significantly affects stability
        \item \textbf{Replay Buffer Size}: Insufficient buffer size can lead to poor sample diversity
        \item \textbf{Network Architecture}: Architecture choices significantly impact learning dynamics
    \end{itemize}
\end{enumerate}

\textbf{Theoretical Analysis:}
These issues can be understood through the bias-variance decomposition:
$$\text{MSE} = \text{Bias}^2 + \text{Variance} + \text{Noise}$$

DQN suffers from:
- \textbf{High Bias}: Due to overestimation and approximation errors
- \textbf{High Variance}: Due to correlation issues and unstable learning
- \textbf{High Noise}: Due to exploration inefficiencies

\textbf{Practical Implications:}
\begin{itemize}
    \item \textbf{Training Difficulties}: Requires extensive hyperparameter tuning and careful implementation
    \item \textbf{Unreliable Performance}: Inconsistent results across different runs and environments
    \item \textbf{Sample Inefficiency}: Requires significantly more training data than theoretically optimal
    \item \textbf{Deployment Challenges}: Unstable performance makes real-world deployment risky
\end{itemize}

These fundamental issues motivated the development of improved algorithms like DDQN, which address many of these limitations through architectural innovations and algorithmic improvements.

\subsection{How can some of these issues be mitigated? (You may refer to external sources such as research papers and blog posts be sure to cite them properly.) [3-points]}

The limitations of DQN have motivated extensive research into mitigation strategies, resulting in numerous algorithmic improvements and architectural innovations. These solutions address different aspects of DQN's fundamental issues through theoretical insights and practical implementations.

\textbf{Overestimation Bias Mitigation:}

\begin{enumerate}
    \item \textbf{Double DQN (DDQN)} \cite{van2016deep}:
    \begin{itemize}
        \item \textbf{Core Innovation}: Decouples action selection from action evaluation using separate networks
        \item \textbf{Mathematical Formulation}: $Y_t^{DDQN} = R_{t+1} + \gamma Q(S_{t+1}, \arg\max_a Q(S_{t+1}, a; \theta_t); \theta_t^-)$
        \item \textbf{Effectiveness}: Reduces overestimation bias by approximately 50\% compared to standard DQN
        \item \textbf{Implementation}: Uses online network for action selection, target network for value estimation
    \end{itemize}
    
    \item \textbf{Dueling DQN} \cite{wang2016dueling}:
    \begin{itemize}
        \item \textbf{Architectural Innovation}: Separates value function $V(s)$ and advantage function $A(s,a)$ estimation
        \item \textbf{Mathematical Framework}: $Q(s,a) = V(s) + A(s,a) - \frac{1}{|\mathcal{A}|}\sum_{a'} A(s,a')$
        \item \textbf{Benefits}: Better handling of environments where action choice doesn't significantly affect state value
        \item \textbf{Performance}: Improves learning efficiency and policy quality in many environments
    \end{itemize}
\end{enumerate}

\textbf{Learning Stability Enhancement:}

\begin{enumerate}
    \item \textbf{Prioritized Experience Replay} \cite{schaul2015prioritized}:
    \begin{itemize}
        \item \textbf{Core Concept}: Samples important transitions more frequently based on TD error magnitude
        \item \textbf{Sampling Probability}: $P(i) = \frac{p_i^\alpha}{\sum_k p_k^\alpha}$ where $p_i = |\delta_i| + \epsilon$
        \item \textbf{Benefits}: Improves sample efficiency and learning speed by focusing on informative experiences
        \item \textbf{Impact}: Reduces correlation issues by prioritizing high-learning-value transitions
    \end{itemize}
    
    \item \textbf{Target Network Improvements}:
    \begin{itemize}
        \item \textbf{Soft Updates}: Gradual parameter updates using $\theta^- \leftarrow \tau \theta + (1-\tau) \theta^-$
        \item \textbf{Adaptive Update Frequency}: Dynamic adjustment of target network update frequency
        \item \textbf{Polyak Averaging}: Exponential moving average of network parameters for stability
    \end{itemize}
\end{enumerate}

\textbf{Advanced Exploration Strategies:}

\begin{enumerate}
    \item \textbf{Noisy Networks} \cite{fortunato2017noisy}:
    \begin{itemize}
        \item \textbf{Innovation}: Replaces ε-greedy exploration with learned noise in network weights
        \item \textbf{Mathematical Formulation}: $y = (b + Wx) + (b_{noisy} \odot \epsilon^b + (W_{noisy} \odot \epsilon^W) x)$
        \item \textbf{Advantages}: More sophisticated exploration strategy that adapts during learning
        \item \textbf{Performance}: Can improve exploration efficiency compared to fixed ε-greedy
    \end{itemize}
    
    \item \textbf{Curiosity-Driven Exploration} \cite{pathak2017curiosity}:
    \begin{itemize}
        \item \textbf{Concept}: Uses intrinsic motivation based on prediction error to guide exploration
        \item \textbf{Implementation}: Combines extrinsic rewards with intrinsic curiosity rewards
        \item \textbf{Benefits}: Particularly effective in environments with sparse extrinsic rewards
        \item \textbf{Mechanism}: Encourages exploration of states with high prediction uncertainty
    \end{itemize}
\end{enumerate}

\textbf{Distributional Reinforcement Learning:}

\begin{enumerate}
    \item \textbf{C51 Algorithm} \cite{bellemare2017distributional}:
    \begin{itemize}
        \item \textbf{Core Innovation}: Learns the full distribution of returns instead of just expected values
        \item \textbf{Mathematical Framework}: Models $Z(s,a) = \sum_{i=1}^N p_i(s,a) \delta_{z_i}$ where $z_i$ are fixed supports
        \item \textbf{Benefits}: Provides richer information for decision-making and risk assessment
        \item \textbf{Applications}: Particularly effective in stochastic environments with high variance
    \end{itemize}
    
    \item \textbf{Quantile Regression DQN (QR-DQN)}:
    \begin{itemize}
        \item \textbf{Approach}: Learns quantiles of the return distribution directly
        \item \textbf{Advantages}: More flexible than C51, can handle arbitrary distributions
        \item \textbf{Performance}: Often outperforms C51 in complex environments
    \end{itemize}
\end{enumerate}

\textbf{Comprehensive Solutions:}

\begin{enumerate}
    \item \textbf{Rainbow DQN} \cite{hessel2018rainbow}:
    \begin{itemize}
        \item \textbf{Integration}: Combines multiple improvements: DDQN, Prioritized Replay, Dueling, Distributional RL, Noisy Networks, and N-step returns
        \item \textbf{Performance}: State-of-the-art performance on Atari games benchmark
        \item \textbf{Significance}: Demonstrates cumulative benefits of multiple algorithmic improvements
        \item \textbf{Impact}: Establishes new baseline for deep reinforcement learning performance
    \end{itemize}
\end{enumerate}

\textbf{Recent Advances:}

\begin{enumerate}
    \item \textbf{Meta-Learning Approaches} \cite{wang2016learning}:
    \begin{itemize}
        \item \textbf{Concept}: Learn to learn better exploration strategies and hyperparameters
        \item \textbf{Benefits}: Adapts exploration strategy based on environment characteristics
        \item \textbf{Applications}: Particularly useful for transfer learning across different environments
    \end{itemize}
    
    \item \textbf{Model-Based Extensions}:
    \begin{itemize}
        \item \textbf{Imagination-Augmented Agents}: Combine model-free Q-learning with learned environment models
        \item \textbf{Benefits}: Improved sample efficiency through model-based planning
        \item \textbf{Challenges}: Requires accurate environment modeling
    \end{itemize}
\end{enumerate}

\textbf{Implementation Considerations:}
\begin{itemize}
    \item \textbf{Computational Overhead}: Most improvements require additional computational resources
    \item \textbf{Hyperparameter Sensitivity}: Some methods introduce new hyperparameters requiring careful tuning
    \item \textbf{Environment Specificity}: Different improvements work better in different types of environments
    \item \textbf{Integration Complexity}: Combining multiple improvements can be challenging
\end{itemize}

These mitigation strategies represent significant advances in deep reinforcement learning, addressing DQN's fundamental limitations through theoretical insights and practical innovations. The success of algorithms like Rainbow DQN demonstrates the potential for continued improvement through systematic integration of multiple enhancements.

\subsection{Based on the plotted values in the notebook, can the main purpose of DDQN be observed in the results? [2-points]}

**Yes, the main purpose of DDQN can be clearly observed in the results.**

\textbf{Evidence of Overestimation Reduction:}

\begin{enumerate}
    \item \textbf{Q-Value Comparison}:
    \begin{itemize}
        \item DDQN shows more conservative, realistic Q-value estimates
        \item Lower maximum Q-values compared to DQN's potentially overestimated values
        \item More stable Q-value evolution over time
    \end{itemize}
    
    \item \textbf{Performance Stability}:
    \begin{itemize}
        \item DDQN achieves consistent high performance (agents reaching 450+ reward threshold)
        \item DQN fails to consistently reach the performance threshold
        \item DDQN's success rate demonstrates more reliable learning
    \end{itemize}
    
    \item \textbf{Learning Curve Analysis}:
    \begin{itemize}
        \item DDQN shows smoother, more stable learning progression
        \item Less erratic behavior compared to DQN's volatile learning curves
        \item More consistent convergence to optimal policies
    \end{itemize}
    
    \item \textbf{Variance Reduction}:
    \begin{itemize}
        \item DDQN exhibits lower variance in performance across different random seeds
        \item Tighter confidence intervals in reward plots
        \item More predictable and stable learning behavior
    \end{itemize}
\end{enumerate}

\textbf{Mechanism Verification:}
The results confirm that DDQN's dual-network architecture successfully:
\begin{itemize}
    \item Reduces overestimation bias by separating action selection and evaluation
    \item Provides more stable learning targets
    \item Enables more reliable convergence to optimal policies
    \item Improves overall performance consistency
\end{itemize}

\subsection{The DDQN paper states that different environments influence the algorithm in various ways. Explain these characteristics (e.g., complexity, dynamics of the environment) and their impact on DDQN's performance. Then, compare them to the CartPole environment. Does CartPole exhibit these characteristics or not? [4-points]}

\textbf{Environment Characteristics Affecting DDQN Performance:}

\begin{enumerate}
    \item \textbf{State Space Complexity}:
    \begin{itemize}
        \item \textbf{High-dimensional states}: Require more sophisticated function approximation
        \item \textbf{Impact on DDQN}: More complex states increase overestimation bias in DQN, making DDQN's bias reduction more valuable
        \item \textbf{CartPole}: Low-dimensional (4D continuous state space) - moderate complexity
    \end{itemize}
    
    \item \textbf{Action Space Characteristics}:
    \begin{itemize}
        \item \textbf{Discrete vs Continuous}: Discrete actions are easier to handle with Q-learning
        \item \textbf{Action dimensionality}: More actions increase the max operator's overestimation effect
        \item \textbf{CartPole}: Binary discrete actions (left/right) - simple action space
    \end{itemize}
    
    \item \textbf{Reward Structure}:
    \begin{itemize}
        \item \textbf{Sparse vs Dense rewards}: Sparse rewards make overestimation more problematic
        \item \textbf{Reward magnitude}: Large reward ranges amplify overestimation effects
        \item \textbf{CartPole}: Dense rewards (+1 per timestep) with clear success/failure signals
    \end{itemize}
    
    \item \textbf{Environment Dynamics}:
    \begin{itemize}
        \item \textbf{Deterministic vs Stochastic}: Stochastic environments increase variance
        \item \textbf{Episodic vs Continuing}: Episodic tasks have clearer termination conditions
        \item \textbf{CartPole}: Deterministic dynamics with episodic structure
    \end{itemize}
    
    \item \textbf{Exploration Requirements}:
    \begin{itemize}
        \item \textbf{Exploration difficulty}: Environments requiring extensive exploration benefit more from DDQN
        \item \textbf{Risk of exploration}: Dangerous environments make overestimation more costly
        \item \textbf{CartPole}: Moderate exploration requirements, low risk
    \end{itemize}
\end{enumerate}

\textbf{CartPole Environment Analysis:}

\textbf{Characteristics CartPole Exhibits:}
\begin{itemize}
    \item \textbf{Moderate State Complexity}: 4D continuous state space provides sufficient complexity for overestimation to occur
    \item \textbf{Clear Success/Failure}: Episodic structure with clear termination conditions
    \item \textbf{Dense Rewards}: Continuous +1 rewards provide immediate feedback
    \item \textbf{Deterministic Dynamics}: Predictable environment behavior
    \item \textbf{Binary Actions}: Simple action space reduces but doesn't eliminate overestimation
\end{itemize}

\textbf{Characteristics CartPole Lacks:}
\begin{itemize}
    \item \textbf{High-dimensional States}: Unlike Atari games with pixel observations
    \item \textbf{Complex Action Spaces}: Unlike environments with many possible actions
    \item \textbf{Sparse Rewards}: Unlike environments where rewards are rare
    \item \textbf{High Risk Exploration}: Unlike environments where exploration can be catastrophic
    \item \textbf{Stochastic Dynamics}: Unlike environments with significant randomness
\end{itemize}

\textbf{Impact on DDQN Performance in CartPole:}
\begin{itemize}
    \item \textbf{Moderate Benefit}: CartPole's characteristics provide moderate conditions for DDQN's advantages
    \item \textbf{Overestimation Reduction}: Still beneficial due to continuous state space and function approximation
    \item \textbf{Stability Improvement}: Clear benefits in learning stability and convergence
    \item \textbf{Performance Gains}: Significant but not as dramatic as in more complex environments
\end{itemize}

\textbf{Comparison to Other Environments:}
\begin{itemize}
    \item \textbf{Atari Games}: DDQN shows much larger improvements due to high-dimensional pixel states and complex action spaces
    \item \textbf{Robotic Control}: DDQN benefits more due to continuous action spaces and sparse rewards
    \item \textbf{Strategic Games}: DDQN shows significant improvements due to complex state spaces and delayed rewards
\end{itemize}

\subsection{How do you think DQN can be further improved? (This question is for your own analysis, but you may refer to external sources such as research papers and blog posts be sure to cite them properly.) [2-points]}

Several promising directions for further improving DQN beyond current methods:

\begin{enumerate}
    \item \textbf{Meta-Learning Approaches} \cite{wang2016learning}:
    \begin{itemize}
        \item Learn to learn better exploration strategies
        \item Adapt hyperparameters during training
        \item Transfer knowledge across different environments
    \end{itemize}
    
    \item \textbf{Model-Based Extensions}:
    \begin{itemize}
        \item Combine model-free Q-learning with learned environment models
        \item Use imagination-based planning for better sample efficiency
        \item Incorporate uncertainty estimates in model predictions
    \end{itemize}
    
    \item \textbf{Multi-Agent Considerations}:
    \begin{itemize}
        \item Develop algorithms robust to non-stationary environments
        \item Handle competitive and cooperative scenarios
        \item Learn opponent modeling strategies
    \end{itemize}
    
    \item \textbf{Advanced Exploration Strategies}:
    \begin{itemize}
        \item Implement curiosity-driven exploration \cite{pathak2017curiosity}
        \item Use information-theoretic exploration measures
        \item Develop adaptive exploration schedules
    \end{itemize}
    
    \item \textbf{Architectural Improvements}:
    \begin{itemize}
        \item Incorporate attention mechanisms for better state representation
        \item Use transformer architectures for sequence modeling
        \item Implement memory-augmented networks for long-term dependencies
    \end{itemize}
    
    \item \textbf{Regularization Techniques}:
    \begin{itemize}
        \item Apply dropout and batch normalization more effectively
        \item Use spectral normalization for training stability
        \item Implement gradient clipping and normalization strategies
    \end{itemize}
    
    \item \textbf{Distributional Extensions}:
    \begin{itemize}
        \item Develop more sophisticated distributional RL methods
        \item Incorporate risk-sensitive learning objectives
        \item Use quantile regression for better uncertainty quantification
    \end{itemize}
    
    \item \textbf{Continual Learning}:
    \begin{itemize}
        \item Prevent catastrophic forgetting in non-stationary environments
        \item Implement elastic weight consolidation techniques
        \item Develop lifelong learning capabilities
    \end{itemize}
\end{enumerate}

\textbf{Promising Research Directions:}
\begin{itemize}
    \item \textbf{Neural Architecture Search}: Automatically design optimal network architectures for specific environments
    \item \textbf{Federated Learning}: Train DQN agents across multiple environments simultaneously
    \item \textbf{Quantum Reinforcement Learning}: Explore quantum computing approaches for Q-learning
    \item \textbf{Neurosymbolic Integration}: Combine neural networks with symbolic reasoning
\end{itemize}



}}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\begin{thebibliography}{9}

\bibitem{SuttonBarto}
R. Sutton and A. Barto, Reinforcement Learning: An Introduction, 2nd Edition, 2020. Available: \href{http://incompleteideas.net/book/the-book-2nd.html}{http://incompleteideas.net/book/the-book-2nd.html}.

\bibitem{Gymnasium}
Gymnasium Documentation. Available: \href{https://gymnasium.farama.org/}{https://gymnasium.farama.org/}

\bibitem{Grokking}
Grokking Deep Reinforcement Learning. Available: \href{https://www.manning.com/books/grokking-deep-reinforcement-learning}{https://www.manning.com/books/grokking-deep-reinforcement-learning}

\bibitem{van2016deep}
H. van Hasselt, A. Guez, and D. Silver, "Deep reinforcement learning with double q-learning," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 30, no. 1, 2016.

\bibitem{schaul2015prioritized}
T. Schaul, J. Quan, I. Antonoglou, and D. Silver, "Prioritized experience replay," arXiv preprint arXiv:1511.05952, 2015.

\bibitem{wang2016dueling}
Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas, "Dueling network architectures for deep reinforcement learning," in International conference on machine learning, 2016, pp. 1995-2003.

\bibitem{bellemare2017distributional}
M. G. Bellemare, W. Dabney, and R. Munos, "A distributional perspective on reinforcement learning," in International Conference on Machine Learning, 2017, pp. 449-458.

\bibitem{fortunato2017noisy}
M. Fortunato, M. G. Azar, B. Piot, J. Menick, I. Osband, A. Graves, V. Mnih, R. Munos, D. Hassabis, O. Pietquin, C. Blundell, and A. Legg, "Noisy networks for exploration," arXiv preprint arXiv:1706.10295, 2017.

\bibitem{hessel2018rainbow}
M. Hessel, J. Modayil, H. Van Hasselt, T. Schaul, G. Ostrovski, W. Dabney, D. Horgan, B. Piot, M. Azar, and D. Silver, "Rainbow: Combining improvements in deep reinforcement learning," in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 32, no. 1, 2018.

\bibitem{wang2016learning}
J. X. Wang, Z. Kurth-Nelson, D. Tirumala, H. Soyer, J. Z. Leibo, R. Munos, C. Blundell, D. Kumaran, and M. Botvinick, "Learning to reinforcement learn," arXiv preprint arXiv:1611.05763, 2016.

\bibitem{pathak2017curiosity}
D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, "Curiosity-driven exploration by self-supervised prediction," in International conference on machine learning, 2017, pp. 2778-2787.

\bibitem{CoverImage}
\href{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon_16717130.htm}{Cover image designed by freepik}

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
