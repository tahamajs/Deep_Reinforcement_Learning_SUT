{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d18f69eb",
   "metadata": {},
   "source": [
    "# Meta-Reinforcement Learning with MAML\n",
    "\n",
    "In this assignment notebook, we will implement **Model-Agnostic Meta-Learning (MAML)** from scratch for a custom `HalfCheetahBackward` environment. The goal is to learn policy parameters that can quickly adapt to new tasks in this case, running the HalfCheetah agent **backward** with just a few gradient steps. Each section below provides context and key steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMA9RHEIqikL"
   },
   "outputs": [],
   "source": [
    "!pip -q install gymnasium[mujoco]\n",
    "!pip install imageio -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26141088",
   "metadata": {},
   "source": [
    "## Environment & Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U7gHgXr0xh6W",
    "outputId": "f0e29da5-6926-4c3c-e730-9eded019b690"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from collections import namedtuple, deque\n",
    "import imageio\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592e4063",
   "metadata": {},
   "source": [
    "\n",
    "This is necessary for running the HalfCheetah env on colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKMYUz6fxd4o",
    "outputId": "a6682905-e453-491e-ca65-368ca260305a"
   },
   "outputs": [],
   "source": [
    "%env MUJOCO_GL=egl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0871967",
   "metadata": {},
   "source": [
    "This code displays a saved mp4 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zh-QCtmGx6NP"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def show_video(path):\n",
    "    mp4 = open(path, 'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(\"\"\"\n",
    "    <video width=400 controls>\n",
    "          <source src=\"%s\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\" % data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d05f9",
   "metadata": {},
   "source": [
    "Run this code to get started with the HalfCheetah environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c9AGb7Uur7Df"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"HalfCheetah-v5\", render_mode=\"rgb_array\")\n",
    "obs, info = env.reset()\n",
    "frames = []\n",
    "\n",
    "for _ in range(100):\n",
    "    frames.append(env.render())\n",
    "    action = env.action_space.sample()  # Random action\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    if terminated or truncated:\n",
    "        obs, info = env.reset()\n",
    "    \n",
    "env.close()\n",
    "imageio.mimsave('./HalfCheetah.mp4', frames, fps=20)\n",
    "show_video('./HalfCheetah.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48731171",
   "metadata": {},
   "source": [
    "We modify the HalfCheetah environment by creating a wrapper on top it to reward the model for moving backwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRzn-YjCBamz"
   },
   "outputs": [],
   "source": [
    "class HalfCheetahBackward(gym.Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.env = gym.make(\"HalfCheetah-v5\", render_mode=\"rgb_array\")\n",
    "        self.forward_reward_weight = 1.0\n",
    "        self.ctrl_cost_weight = 0.05\n",
    "\n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        return self.env.reset(seed=seed, options=options)[0]\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, _, done, tr, info = self.env.step(action)\n",
    "        reward =  -1 * self.forward_reward_weight * info[\"reward_forward\"] + self.ctrl_cost_weight * info[\"reward_ctrl\"]\n",
    "        return obs, reward, done, tr, info\n",
    "\n",
    "    def render(self):\n",
    "        return self.env.render()\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dca1173",
   "metadata": {},
   "source": [
    "## Gaussian Policy Network\n",
    "We parameterize our policy π_θ(a|s) as a multivariate Gaussian:\n",
    "\\[\n",
    "$μ_θ(s) = f_θ(s), \\quad Σ_θ = \\text{diag}(\\exp(2φ))$\n",
    "\\]\n",
    "Here, `mean_head` outputs μ_θ(s) and `log_std` (φ) is a learned vector of log-standard deviations. Sampling and computing log-probabilities from this distribution is essential for the policy gradient update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U4eoMc7EERWx"
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_sizes=(64,64)):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        prev_size = obs_dim\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([nn.Linear(prev_size, hidden_size), nn.ReLU()])\n",
    "            prev_size = hidden_size\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.mean_head = nn.Linear(prev_size, act_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(act_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.net(x)\n",
    "        mean = self.mean_head(h)\n",
    "        std = torch.exp(self.log_std)\n",
    "        return mean, std\n",
    "\n",
    "    def get_action(self, obs):\n",
    "        mean, std = self(obs)\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        return action, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf23f6d",
   "metadata": {},
   "source": [
    "## Trajectory Collection & Return Computation\n",
    "The `rollout` function runs the policy in the environment for up to `max_steps`, storing:\n",
    "- Observations **s_t**\n",
    "- Actions **a_t** sampled from π_θ\n",
    "- Log-probabilities **log π_θ(a_t|s_t)**\n",
    "- Rewards **r_t**\n",
    "After the episode, we compute discounted returns:\n",
    "\\[\n",
    "$G_t = \\sum_{k=0}^{T-t} γ^k r_{t+k}$\n",
    "\\]\n",
    "These returns serve as our baselines for policy gradient estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HMaGZXOTPrR"
   },
   "outputs": [],
   "source": [
    "def rollout(env, policy, max_steps=200, gamma=0.99):\n",
    "    obs = env.reset()\n",
    "    obs_buf, logp_buf, ret_buf = [], [], []\n",
    "    rewards = []\n",
    "    for _ in range(max_steps):\n",
    "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "        action, dist = policy.get_action(obs_tensor)\n",
    "        action_np = action.squeeze(0).detach().numpy()\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        \n",
    "        obs_buf.append(obs_tensor.squeeze(0))\n",
    "        logp_buf.append(log_prob)\n",
    "        \n",
    "        obs, reward, terminated, truncated, info = env.step(action_np)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    ret_buf = torch.tensor(returns, dtype=torch.float32)\n",
    "    return obs_buf, logp_buf, ret_buf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete this section to evaluate the model and save a sample trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, env, num_episodes=10, max_episode_len=200 ,path=\"./Final_Evaluation.mp4\", no_video=False):\n",
    "    frames = []\n",
    "    total_rewards = 0\n",
    "    for episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(max_episode_len):\n",
    "            if not no_video and episode == 0:  # Record only first episode\n",
    "                frames.append(env.render())\n",
    "            \n",
    "            obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action, _ = model.get_action(obs_tensor)\n",
    "                action_np = action.squeeze(0).detach().numpy()\n",
    "            \n",
    "            obs, reward, terminated, truncated, info = env.step(action_np)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        total_rewards += episode_reward\n",
    "    \n",
    "    mean_rewards = total_rewards / num_episodes\n",
    "    print(f\"Mean Reward: {mean_rewards}\")\n",
    "    if not no_video:\n",
    "        imageio.mimsave(path, frames, fps=20)\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2bff13",
   "metadata": {},
   "source": [
    "## Model-Agnostic Meta-Learning (MAML)\n",
    "MAML seeks initial parameters $\\theta$ that can adapt to a new task with only a few gradient steps. For each task Tᵢ, we perform an **inner loop** update:\n",
    "\n",
    "\\[\n",
    "$\\theta'_i = \\theta - α \\nabla_\\theta \\mathcal{L}_{T_i}(\\theta)$\n",
    "\\]\n",
    "\n",
    "Then, the **meta-objective** (outer loop) minimizes the post-adaptation loss (optimize for the initial parameter $\\theta$):\n",
    "\n",
    "\\[\n",
    "$\\min_\\theta \\sum_i \\mathcal{L}_{T_i}(\\theta'_i) = \\sum_i \\mathcal{L}_{T_i}\\bigl(\\theta - α \\nabla_\\theta \\mathcal{L}_{T_i}(\\theta)\\bigr)$\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ueGeJCWiChP"
   },
   "outputs": [],
   "source": [
    "class MAML:\n",
    "    def __init__(\n",
    "        self,\n",
    "        task_env_cls,\n",
    "        inner_lr,\n",
    "        outer_lr,\n",
    "        inner_steps,\n",
    "        meta_batch_size,\n",
    "        max_episode_len=200,\n",
    "        gamma=0.99\n",
    "    ):\n",
    "        self.task_env_cls = task_env_cls\n",
    "        self.inner_lr = inner_lr\n",
    "        self.inner_steps = inner_steps\n",
    "        self.meta_batch_size = meta_batch_size\n",
    "        self.max_episode_len = max_episode_len\n",
    "        self.gamma = gamma\n",
    "        self.loss_history   = []\n",
    "        self.reward_history = []\n",
    "\n",
    "        # Get dimensions from a sample environment\n",
    "        sample_env = task_env_cls()\n",
    "        obs_dim = sample_env.observation_space.shape[0]\n",
    "        act_dim = sample_env.action_space.shape[0]\n",
    "        sample_env.close()\n",
    "\n",
    "        self.meta_policy = Policy(obs_dim, act_dim).to(device)\n",
    "        self.meta_opt = optim.Adam(self.meta_policy.parameters(), lr=outer_lr, weight_decay=1e-4)\n",
    "\n",
    "    def inner_update(self, env):\n",
    "        obs_s, logp_s, ret_s = rollout(env, self.meta_policy, self.max_episode_len, self.gamma)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        obs_s = torch.stack(obs_s).to(device)\n",
    "        logp_s = torch.stack(logp_s).to(device)\n",
    "        ret_s = ret_s.to(device)\n",
    "        \n",
    "        # Policy gradient loss (negative because we want to maximize)\n",
    "        pg_loss = -(logp_s * ret_s).mean()\n",
    "\n",
    "        l2_reg = 1e-4  # L2 regularization coefficient\n",
    "        l2_loss = sum(p.pow(2.0).sum() for p in self.meta_policy.parameters())\n",
    "        loss_s = pg_loss + l2_reg * l2_loss\n",
    "\n",
    "        grads = torch.autograd.grad(loss_s, self.meta_policy.parameters(), create_graph=True)\n",
    "        return grads\n",
    "\n",
    "    def adapt_policy(self, grads):\n",
    "        adapted = Policy(self.meta_policy.net[0].in_features, self.meta_policy.mean_head.out_features)\n",
    "        adapted.load_state_dict(self.meta_policy.state_dict())\n",
    "        \n",
    "        # Update weights using gradients\n",
    "        for param, grad in zip(adapted.parameters(), grads):\n",
    "            param.data = param.data - self.inner_lr * grad\n",
    "        \n",
    "        return adapted\n",
    "\n",
    "    def meta_step(self):\n",
    "        total_meta_loss = 0.0\n",
    "        total_reward = 0.0\n",
    "\n",
    "        for _ in range(self.meta_batch_size):\n",
    "            env = self.task_env_cls()\n",
    "\n",
    "            grads = self.inner_update(env)\n",
    "\n",
    "            # Get adapted policy (use grads to update weights)\n",
    "            adapted = self.adapt_policy(grads)\n",
    "\n",
    "            obs_q, logp_q, ret_q = rollout(env, adapted, self.max_episode_len, self.gamma)\n",
    "            \n",
    "            # Convert to tensors\n",
    "            obs_q = torch.stack(obs_q).to(device)\n",
    "            logp_q = torch.stack(logp_q).to(device)\n",
    "            ret_q = ret_q.to(device)\n",
    "            \n",
    "            # Meta loss (negative because we want to maximize)\n",
    "            loss_q = -(logp_q * ret_q).mean()\n",
    "            total_meta_loss += loss_q\n",
    "\n",
    "            total_reward += ret_q.mean().item()\n",
    "\n",
    "            env.close()\n",
    "\n",
    "        meta_loss = total_meta_loss / self.meta_batch_size\n",
    "        self.meta_opt.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        self.meta_opt.step()\n",
    "\n",
    "        self.loss_history.append(meta_loss.item())\n",
    "        self.reward_history.append(total_reward / self.meta_batch_size)\n",
    "\n",
    "        return meta_loss.item()\n",
    "\n",
    "    def train(self, meta_iters=501):\n",
    "        for it in tqdm(range(1, meta_iters)):\n",
    "            loss = self.meta_step()\n",
    "            if it % 10 == 0:\n",
    "                print(f\"\\t[Iter {it}]\\tloss={loss:.3f},\\treward={self.reward_history[-1]:.3f}\")\n",
    "        self.plot_metrics()\n",
    "        return self.meta_policy\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        iters = range(1, len(self.loss_history) + 1)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(iters, self.loss_history, label=\"Loss\")\n",
    "        plt.plot(iters, self.reward_history, label=\"Avg Query Reward\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.legend()\n",
    "        plt.title(\"Training Progress\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9590b37",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "The `train()` function instantiates the `MAML` class with hyperparameters and executes the meta-training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHFWgSSWEu9q"
   },
   "outputs": [],
   "source": [
    "def train(inner_lr, outer_lr, inner_steps, meta_batch_size, max_episode_len, gamma=0.99):\n",
    "    maml = MAML(\n",
    "        task_env_cls=HalfCheetahBackward,\n",
    "        inner_lr=inner_lr,\n",
    "        outer_lr=outer_lr,\n",
    "        inner_steps=inner_steps,\n",
    "        meta_batch_size=meta_batch_size,\n",
    "        max_episode_len=max_episode_len,\n",
    "        gamma=gamma\n",
    "    )\n",
    "    meta_policy = maml.train(meta_iters=500)\n",
    "    return meta_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ1Qzy6UE3q4",
    "outputId": "c1b024da-825c-4f93-b6f8-40ae1eb58439"
   },
   "outputs": [],
   "source": [
    "policy = train(inner_lr=0.01,\n",
    "               outer_lr=0.001,\n",
    "               inner_steps=1,\n",
    "               meta_batch_size=4,\n",
    "               max_episode_len=200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qV7KclmSU1mT"
   },
   "outputs": [],
   "source": [
    "env = HalfCheetahBackward()\n",
    "evaluate(policy, env, num_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e89M4g-uocwu"
   },
   "outputs": [],
   "source": [
    "show_video('./Final_Evaluation.mp4')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
