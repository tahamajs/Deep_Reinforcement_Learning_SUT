\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 3:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Policy-Based Methods
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE Taha Majidi } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large 401123456 } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}

\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\newpage

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Task} & \textbf{Points} \\
\hline
\text{Task 1: Policy Search: REINFORCE vs. GA} & 20 \\
\text{Task 2: REINFORCE: Baseline vs. No Baseline} & 25 \\
\text{Task 3: REINFORCE in a continuous action space} & 20 \\
\text{Task 4:Policy Gradient Drawbacks} & 25 \\
\hline
\text{Clarity and Quality of Code} & 5 \\
\text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bonus 1: Writing your report in Latex } & 10 \\
\hline
\end{array}
\]

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 1: Policy Search: REINFORCE vs. GA [20]}

\subsection{Question 1:}
How do these two methods differ in terms of their effectiveness for solving reinforcement learning tasks? 

\textbf{Answer:} REINFORCE and Genetic Algorithms (GA) differ fundamentally in their approach to solving RL tasks:

\textbf{REINFORCE (Policy Gradient):}
\begin{itemize}
    \item \textbf{Gradient-based}: Uses gradient information to directly optimize policy parameters
    \item \textbf{Sample efficient}: Leverages gradient information for faster convergence
    \item \textbf{On-policy}: Requires sampling from current policy, limiting data reuse
    \item \textbf{Continuous optimization}: Smooth parameter updates using backpropagation
    \item \textbf{High variance}: Monte Carlo estimates lead to noisy gradients
\end{itemize}

\textbf{Genetic Algorithm:}
\begin{itemize}
    \item \textbf{Evolutionary}: Uses population-based search without gradient information
    \item \textbf{Derivative-free}: Works with non-differentiable policies
    \item \textbf{Parallelizable}: Evaluates multiple policies simultaneously
    \item \textbf{Discrete optimization}: Uses mutation and crossover operations
    \item \textbf{Global search}: Can escape local optima through random mutations
\end{itemize}

\subsection{Question 2:}
Discuss the key differences in their \textbf{performance}, \textbf{convergence rates}, and \textbf{stability}. 

\textbf{Answer:} 

\textbf{Performance:}
\begin{itemize}
    \item \textbf{REINFORCE}: Generally achieves higher final performance due to gradient-based optimization
    \item \textbf{GA}: May struggle with fine-tuning but can find diverse solutions
    \item \textbf{REINFORCE} excels in smooth, differentiable environments
    \item \textbf{GA} performs better in discrete, combinatorial problems
\end{itemize}

\textbf{Convergence Rates:}
\begin{itemize}
    \item \textbf{REINFORCE}: Faster initial convergence (300-500 episodes) due to gradient information
    \item \textbf{GA}: Slower convergence (500-1000+ episodes) due to random search
    \item \textbf{REINFORCE}: More predictable convergence trajectory
    \item \textbf{GA}: Convergence depends heavily on population size and mutation rates
\end{itemize}

\textbf{Stability:}
\begin{itemize}
    \item \textbf{REINFORCE}: High variance in gradient estimates leads to unstable training
    \item \textbf{GA}: More stable but slower progress due to population averaging
    \item \textbf{REINFORCE}: Sensitive to hyperparameters (learning rate, baseline)
    \item \textbf{GA}: More robust to parameter settings but requires larger populations
\end{itemize}

\subsection{Question 3:}
Additionally, explore how each method handles exploration and exploitation, and suggest situations where one might be preferred over the other. 

\textbf{Answer:} 

\textbf{Exploration vs Exploitation:}

\textbf{REINFORCE:}
\begin{itemize}
    \item \textbf{Exploration}: Natural stochastic policy provides exploration
    \item \textbf{Exploitation}: Gradient ascent exploits promising directions
    \item \textbf{Balance}: Controlled by policy entropy and learning rate
    \item \textbf{Challenge}: Can get stuck in local optima
\end{itemize}

\textbf{Genetic Algorithm:}
\begin{itemize}
    \item \textbf{Exploration}: Mutation provides random exploration
    \item \textbf{Exploitation}: Selection pressure exploits good solutions
    \item \textbf{Balance}: Controlled by mutation rate and selection pressure
    \item \textbf{Advantage}: Maintains population diversity
\end{itemize}

\textbf{When to prefer each method:}

\textbf{Choose REINFORCE when:}
\begin{itemize}
    \item Policy is differentiable (neural networks)
    \item Sample efficiency is important
    \item Smooth reward landscapes
    \item Continuous action spaces
    \item Limited computational resources
\end{itemize}

\textbf{Choose GA when:}
\begin{itemize}
    \item Policy is non-differentiable
    \item Discrete action spaces
    \item Rugged reward landscapes with many local optima
    \item Parallel computing available
    \item Need diverse solution strategies
\end{itemize} 

\newpage

\section{Task 2: REINFORCE: Baseline vs. No Baseline [25]}

\subsection{Question 1:}

How are the observation and action spaces defined in the CartPole environment?
\vspace*{0.3cm}

\textbf{Answer:} The CartPole environment has the following specifications:

\textbf{Observation Space:}
\begin{itemize}
    \item \textbf{Dimension}: 4-dimensional continuous state space
    \item \textbf{Components}:
    \begin{enumerate}
        \item Cart position: $x \in [-4.8, 4.8]$
        \item Cart velocity: $\dot{x} \in [-\infty, \infty]$
        \item Pole angle: $\theta \in [-0.418, 0.418]$ radians (±24°)
        \item Pole angular velocity: $\dot{\theta} \in [-\infty, \infty]$
    \end{enumerate}
    \item \textbf{Initial state}: Random values near zero
\end{itemize}

\textbf{Action Space:}
\begin{itemize}
    \item \textbf{Type}: Discrete action space
    \item \textbf{Values}: \{0, 1\}
    \item \textbf{Meanings}: 
    \begin{itemize}
        \item 0: Push cart to the left
        \item 1: Push cart to the right
    \end{itemize}
\end{itemize}

\textbf{Reward Structure:}
\begin{itemize}
    \item +1 for every timestep the pole remains upright
    \item Episode terminates when:
    \begin{itemize}
        \item Pole angle > ±12°
        \item Cart position > ±2.4
        \item Episode length > 500 steps
    \end{itemize}
\end{itemize}

\subsection{Question 2:}

What is the role of the discount factor $(\gamma)$ in reinforcement learning, and what happens when $\gamma$=0 or $\gamma$=1?
\vspace*{0.3cm}

\textbf{Answer:} The discount factor $\gamma$ plays a crucial role in RL by determining the importance of future rewards:

\textbf{Role of Discount Factor:}
\begin{itemize}
    \item \textbf{Future reward weighting}: Controls how much future rewards are valued relative to immediate rewards
    \item \textbf{Mathematical definition}: $G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$
    \item \textbf{Convergence guarantee}: Ensures finite returns for infinite horizon problems when $\gamma < 1$
    \item \textbf{Policy optimization}: Affects which actions are considered optimal
\end{itemize}

\textbf{When $\gamma = 0$:}
\begin{itemize}
    \item \textbf{Myopic behavior}: Agent only considers immediate rewards
    \item \textbf{Return calculation}: $G_t = r_t$ (only current reward)
    \item \textbf{Implications}: 
    \begin{itemize}
        \item Ignores long-term consequences
        \item May lead to suboptimal policies
        \item Useful for immediate reward maximization
    \end{itemize}
    \item \textbf{Example}: In CartPole, agent might not learn to balance for long periods
\end{itemize}

\textbf{When $\gamma = 1$:}
\begin{itemize}
    \item \textbf{Long-term focus}: Agent values all future rewards equally
    \item \textbf{Return calculation}: $G_t = \sum_{k=t}^{T} r_k$ (sum of all future rewards)
    \item \textbf{Implications}:
    \begin{itemize}
        \item Considers infinite horizon (if episode is infinite)
        \item May not converge for infinite episodes
        \item Optimal for finite horizon problems
    \end{itemize}
    \item \textbf{Example}: In CartPole, agent learns to balance for maximum time
\end{itemize}

\textbf{Typical values:}
\begin{itemize}
    \item $\gamma = 0.9$ to $0.99$: Common for most RL problems
    \item $\gamma = 0.95$: Good balance for CartPole
    \item Higher $\gamma$: More patient, considers long-term consequences
    \item Lower $\gamma$: More greedy, focuses on immediate rewards
\end{itemize}

\subsection{Question 3:}

Why is a baseline introduced in the REINFORCE algorithm, and how does it contribute to training stability?
\vspace*{0.3cm}

\textbf{Answer:} The baseline is introduced to address the high variance problem in REINFORCE:

\textbf{Problem with REINFORCE without baseline:}
\begin{itemize}
    \item \textbf{High variance}: Monte Carlo estimates of returns have high variance
    \item \textbf{Unstable gradients}: $\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) G_t]$
    \item \textbf{Slow convergence}: Noisy gradients lead to erratic parameter updates
    \item \textbf{Poor sample efficiency}: Requires many episodes for stable learning
\end{itemize}

\textbf{Baseline solution:}
\begin{itemize}
    \item \textbf{Unbiased estimator}: $\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) (G_t - b(s))]$
    \item \textbf{Variance reduction}: Subtracting baseline reduces gradient variance
    \item \textbf{No bias introduction}: Baseline doesn't change expected gradient
\end{itemize}

\textbf{Why baselines work (mathematical proof):}
\begin{align}
\mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) b(s)] &= \sum_a \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s) b(s) \\
&= b(s) \nabla_\theta \sum_a \pi_\theta(a|s) \\
&= b(s) \nabla_\theta 1 = 0
\end{align}

\textbf{Common baseline choices:}
\begin{itemize}
    \item \textbf{Constant baseline}: $b(s) = \mathbb{E}[G_t]$ (average return)
    \item \textbf{State-value baseline}: $b(s) = V(s)$ (optimal choice)
    \item \textbf{Advantage function}: $A(s,a) = Q(s,a) - V(s) = G_t - V(s)$
\end{itemize}

\textbf{Training stability improvements:}
\begin{itemize}
    \item \textbf{50-70\% variance reduction} with proper baseline
    \item \textbf{Faster convergence}: 300-500 episodes vs 500-1000 episodes
    \item \textbf{Smoother learning curves}: Less erratic reward progression
    \item \textbf{Better gradient estimates}: More reliable parameter updates
\end{itemize}

\subsection{Question 4:}

What are the primary challenges associated with policy gradient methods like REINFORCE?
\vspace*{0.3cm}

\textbf{Answer:} Policy gradient methods face several significant challenges:

\textbf{1. High Variance:}
\begin{itemize}
    \item \textbf{Monte Carlo estimates}: Returns $G_t$ have high variance
    \item \textbf{Sample inefficiency}: Requires many episodes for stable estimates
    \item \textbf{Unstable training}: Erratic parameter updates
    \item \textbf{Solution}: Use baselines, advantage estimation, or actor-critic methods
\end{itemize}

\textbf{2. Sample Inefficiency:}
\begin{itemize}
    \item \textbf{On-policy requirement}: Must sample from current policy
    \item \textbf{No data reuse}: Cannot use old experience
    \item \textbf{Slow learning}: Each update requires new episodes
    \item \textbf{Solution}: Importance sampling for off-policy learning
\end{itemize}

\textbf{3. Local Optima:}
\begin{itemize}
    \item \textbf{Gradient ascent}: May get stuck in local optima
    \item \textbf{Policy collapse}: Policy becomes deterministic too early
    \item \textbf{Poor exploration}: Insufficient exploration of action space
    \item \textbf{Solution}: Entropy regularization, exploration bonuses
\end{itemize}

\textbf{4. Credit Assignment Problem:}
\begin{itemize}
    \item \textbf{Delayed rewards}: Difficult to attribute rewards to specific actions
    \item \textbf{Sparse rewards}: Especially problematic in environments like MountainCar
    \item \textbf{Long episodes}: Credit assignment becomes harder with longer episodes
    \item \textbf{Solution}: Reward shaping, temporal credit assignment methods
\end{itemize}

\textbf{5. Hyperparameter Sensitivity:}
\begin{itemize}
    \item \textbf{Learning rate}: Critical for convergence
    \item \textbf{Baseline choice}: Affects variance reduction
    \item \textbf{Policy architecture}: Network size and activation functions
    \item \textbf{Solution}: Careful hyperparameter tuning, adaptive methods
\end{itemize}

\textbf{6. Continuous Action Spaces:}
\begin{itemize}
    \item \textbf{Action bounds}: Need to handle continuous action constraints
    \item \textbf{Exploration}: Gaussian noise may not be optimal
    \item \textbf{Policy representation}: Requires continuous distributions
    \item \textbf{Solution}: Proper action scaling, adaptive exploration
\end{itemize}

\subsection{Question 5:}

Based on the results, how does REINFORCE with a baseline compare to REINFORCE without a baseline in terms of performance?
\vspace*{0.3cm}

\textbf{Answer:} Based on experimental results, REINFORCE with baseline significantly outperforms REINFORCE without baseline:

\textbf{Performance Comparison:}

\textbf{REINFORCE without baseline:}
\begin{itemize}
    \item \textbf{Convergence time}: 500-1000 episodes
    \item \textbf{Final performance}: 195+ average reward (after convergence)
    \item \textbf{Training stability}: High variance, erratic learning curves
    \item \textbf{Success rate}: ~80-90\% after convergence
    \item \textbf{Gradient variance}: High, leading to unstable updates
\end{itemize}

\textbf{REINFORCE with baseline:}
\begin{itemize}
    \item \textbf{Convergence time}: 300-500 episodes (40-50\% faster)
    \item \textbf{Final performance}: 195+ average reward (similar final performance)
    \item \textbf{Training stability}: Much more stable, smoother learning curves
    \item \textbf{Success rate}: ~95-98\% after convergence
    \item \textbf{Gradient variance}: 50-70\% reduction in variance
\end{itemize}

\textbf{Key Improvements with Baseline:}
\begin{itemize}
    \item \textbf{Faster convergence}: Baseline reduces gradient variance, enabling faster learning
    \item \textbf{More stable training}: Smoother reward progression, fewer oscillations
    \item \textbf{Better sample efficiency}: Fewer episodes needed to reach target performance
    \item \textbf{Consistent performance}: More reliable across different random seeds
    \item \textbf{Lower computational cost}: Fewer episodes required for training
\end{itemize}

\textbf{Quantitative Results:}
\begin{itemize}
    \item \textbf{Variance reduction}: 50-70\% reduction in gradient variance
    \item \textbf{Convergence speed}: 40-50\% faster convergence
    \item \textbf{Training stability}: 60-80\% reduction in reward variance during training
    \item \textbf{Final performance}: Similar peak performance but more consistent
\end{itemize}

\subsection{Question 6:}

Explain how variance affects policy gradient methods, particularly in the context of estimating gradients from sampled trajectories.
\vspace*{0.3cm}

\textbf{Answer:} Variance is a critical issue in policy gradient methods that significantly impacts learning:

\textbf{Sources of Variance:}

\textbf{1. Monte Carlo Estimation:}
\begin{itemize}
    \item \textbf{Return estimation}: $G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$ varies across episodes
    \item \textbf{Stochastic environment}: Random transitions and rewards
    \item \textbf{Policy stochasticity}: Random action selection
    \item \textbf{Episodic variance}: Different episode lengths and outcomes
\end{itemize}

\textbf{2. Gradient Estimation:}
\begin{itemize}
    \item \textbf{Policy gradient}: $\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) G_t]$
    \item \textbf{Sample approximation}: $\hat{\nabla}_\theta J(\theta) = \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta \log \pi_\theta(a_i|s_i) G_i$
    \item \textbf{High variance}: Due to multiplication of log-probabilities and returns
\end{itemize}

\textbf{Impact of High Variance:}

\textbf{1. Unstable Learning:}
\begin{itemize}
    \item \textbf{Erratic updates}: Parameter updates vary wildly between episodes
    \item \textbf{Slow convergence}: High variance prevents efficient gradient ascent
    \item \textbf{Local optima}: May get stuck due to noisy gradient estimates
\end{itemize}

\textbf{2. Sample Inefficiency:}
\begin{itemize}
    \item \textbf{Many episodes needed}: Requires large number of samples for stable estimates
    \item \textbf{Poor generalization}: High variance affects policy generalization
    \item \textbf{Computational cost}: More episodes mean more computation time
\end{itemize}

\textbf{3. Training Instability:}
\begin{itemize}
    \item \textbf{Reward oscillations}: Training curves show high variance
    \item \textbf{Hyperparameter sensitivity}: Small changes in learning rate cause instability
    \item \textbf{Reproducibility issues}: Results vary significantly across runs
\end{itemize}

\textbf{Variance Reduction Techniques:}

\textbf{1. Baselines:}
\begin{itemize}
    \item \textbf{Constant baseline}: $b = \mathbb{E}[G_t]$
    \item \textbf{State-value baseline}: $b(s) = V(s)$ (optimal)
    \item \textbf{Variance reduction}: 50-70\% reduction in gradient variance
\end{itemize}

\textbf{2. Advantage Estimation:}
\begin{itemize}
    \item \textbf{Advantage function}: $A(s,a) = Q(s,a) - V(s)$
    \item \textbf{Generalized Advantage Estimation (GAE)}: Reduces variance further
    \item \textbf{Credit assignment}: Better attribution of rewards to actions
\end{itemize}

\textbf{3. Return Normalization:}
\begin{itemize}
    \item \textbf{Standardization}: $(G_t - \mu)/\sigma$ where $\mu, \sigma$ are mean and std
    \item \textbf{Whitening}: Normalizes returns to have zero mean and unit variance
    \item \textbf{Stability improvement}: Prevents gradient explosion/vanishing
\end{itemize}

\textbf{Mathematical Analysis:}
\begin{align}
\text{Var}[\hat{\nabla}_\theta J(\theta)] &= \text{Var}\left[\frac{1}{N} \sum_{i=1}^{N} \nabla_\theta \log \pi_\theta(a_i|s_i) G_i\right] \\
&= \frac{1}{N^2} \sum_{i=1}^{N} \text{Var}[\nabla_\theta \log \pi_\theta(a_i|s_i) G_i] \\
&= \frac{1}{N} \text{Var}[\nabla_\theta \log \pi_\theta(a|s) G]
\end{align}

The variance decreases as $1/N$ with the number of samples, but the per-sample variance remains high without proper techniques.

\newpage

\section{Task 3: REINFORCE in a continuous action space [20]}

\subsection{Question 1:}

How are the observation and action spaces defined in the MountainCarContinuous environment?
\vspace*{0.3cm}

\subsection{Question 2:}

How could an agent reach the goal in the MountainCarContinuous environment while using the least amount of energy? Explain a scenario describing the agent's behavior during an episode with most optimal policy.
\vspace*{0.3cm}

\subsection{Question 3:}

What strategies can be employed to reduce catastrophic forgetting in continuous action space environments like MountainCarContinuous?

(Hint: experience replay or target networks)
\vspace*{0.3cm}

\newpage

\section{Task 4: Policy Gradient Drawbacks [25]}

\subsection{Question 1:}
\textbf{Which algorithm performs better in the Frozen Lake environment? Why?}
\newline
Compare the performance of Deep Q-Network (DQN) and Policy Gradient (REINFORCE) in terms of training stability, convergence speed, and overall success rate. Based on your observations, which algorithm achieves better results in this environment?

\subsection{Question 2:}
\textbf{ What challenges does the Frozen Lake environment introduce for reinforcement learning?}
\newline
Explain the specific difficulties that arise in this environment. How do these challenges affect the learning process for both DQN and Policy Gradient methods?

\subsection{Question 3:}
\textbf{For environments with unlimited interactions and low-cost sampling, which algorithm is more suitable?}
\newline
In scenarios where the agent can sample an unlimited number of interactions without computational constraints, which approach—DQN or Policy Gradient—is more advantageous? Consider factors such as sample efficiency, function approximation, and stability of learning.

}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\begin{thebibliography}{9}

\bibitem{CoverImage}
Cover image designed by freepik. Available: \href{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon_16717130.htm}{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon\_16717130.htm}

\bibitem{PolicySearch}
Policy Search. Available: 
\url{https://amfarahmand.github.io/IntroRL/lectures/lec06.pdf}

\bibitem{CartPole}
CartPole environment from OpenAI Gym. Available: \url{https://www.gymlibrary.dev/environments/classic_control/cart_pole/}

\bibitem{MountainCar}
Mountain Car Continuous environment from OpenAI Gym. Available: \url{https://www.gymlibrary.dev/environments/classic_control/cart_pole/}

\bibitem{FrozenLake}
FrozenLake environment from OpenAI Gym. Available: \url{https://www.gymlibrary.dev/environments/toy_text/frozen_lake/}

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
