\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 3:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Policy-Based Methods
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE Taha Majlesi } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large 810101504 } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}

\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\newpage

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Task} & \textbf{Points} \\
\hline
\text{Task 1: Policy Search: REINFORCE vs. GA} & 20 \\
\text{Task 2: REINFORCE: Baseline vs. No Baseline} & 25 \\
\text{Task 3: REINFORCE in a continuous action space} & 20 \\
\text{Task 4:Policy Gradient Drawbacks} & 25 \\
\hline
\text{Clarity and Quality of Code} & 5 \\
\text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bonus 1: Writing your report in Latex } & 10 \\
\hline
\end{array}
\]

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 1: Policy Search: REINFORCE vs. GA [20]}

\subsection{Question 1:}
How do these two methods differ in terms of their effectiveness for solving reinforcement learning tasks? 

\textbf{Answer:} REINFORCE and Genetic Algorithms (GA) differ fundamentally in their approach to solving RL tasks:

\textbf{REINFORCE (Policy Gradient):}
\begin{itemize}
    \item \textbf{Gradient-based}: Uses gradient information to directly optimize policy parameters
    \item \textbf{Sample efficient}: Leverages gradient information for faster convergence
    \item \textbf{On-policy}: Requires sampling from current policy, limiting data reuse
    \item \textbf{Continuous optimization}: Smooth parameter updates using backpropagation
    \item \textbf{High variance}: Monte Carlo estimates lead to noisy gradients
\end{itemize}

\textbf{Genetic Algorithm:}
\begin{itemize}
    \item \textbf{Evolutionary}: Uses population-based search without gradient information
    \item \textbf{Derivative-free}: Works with non-differentiable policies
    \item \textbf{Parallelizable}: Evaluates multiple policies simultaneously
    \item \textbf{Discrete optimization}: Uses mutation and crossover operations
    \item \textbf{Global search}: Can escape local optima through random mutations
\end{itemize}

\subsection{Question 2:}
Discuss the key differences in their \textbf{performance}, \textbf{convergence rates}, and \textbf{stability}. 

\textbf{Answer:} 

\textbf{Performance:}
\begin{itemize}
    \item \textbf{REINFORCE}: Generally achieves higher final performance due to gradient-based optimization
    \item \textbf{GA}: May struggle with fine-tuning but can find diverse solutions
    \item \textbf{REINFORCE} excels in smooth, differentiable environments
    \item \textbf{GA} performs better in discrete, combinatorial problems
\end{itemize}

\textbf{Convergence Rates:}
\begin{itemize}
    \item \textbf{REINFORCE}: Faster initial convergence (300-500 episodes) due to gradient information
    \item \textbf{GA}: Slower convergence (500-1000+ episodes) due to random search
    \item \textbf{REINFORCE}: More predictable convergence trajectory
    \item \textbf{GA}: Convergence depends heavily on population size and mutation rates
\end{itemize}

\textbf{Stability:}
\begin{itemize}
    \item \textbf{REINFORCE}: High variance in gradient estimates leads to unstable training
    \item \textbf{GA}: More stable but slower progress due to population averaging
    \item \textbf{REINFORCE}: Sensitive to hyperparameters (learning rate, baseline)
    \item \textbf{GA}: More robust to parameter settings but requires larger populations
\end{itemize}

\subsection{Question 3:}
Additionally, explore how each method handles exploration and exploitation, and suggest situations where one might be preferred over the other. 

\textbf{Answer:} 

\textbf{Exploration vs Exploitation:}

\textbf{REINFORCE:}
\begin{itemize}
    \item \textbf{Exploration}: Natural stochastic policy provides exploration
    \item \textbf{Exploitation}: Gradient ascent exploits promising directions
    \item \textbf{Balance}: Controlled by policy entropy and learning rate
    \item \textbf{Challenge}: Can get stuck in local optima
\end{itemize}

\textbf{Genetic Algorithm:}
\begin{itemize}
    \item \textbf{Exploration}: Mutation provides random exploration
    \item \textbf{Exploitation}: Selection pressure exploits good solutions
    \item \textbf{Balance}: Controlled by mutation rate and selection pressure
    \item \textbf{Advantage}: Maintains population diversity
\end{itemize}

\textbf{When to prefer each method:}

\textbf{Choose REINFORCE when:}
\begin{itemize}
    \item Policy is differentiable (neural networks)
    \item Sample efficiency is important
    \item Smooth reward landscapes
    \item Continuous action spaces
    \item Limited computational resources
\end{itemize}

\textbf{Choose GA when:}
\begin{itemize}
    \item Policy is non-differentiable
    \item Discrete action spaces
    \item Rugged reward landscapes with many local optima
    \item Parallel computing available
    \item Need diverse solution strategies
\end{itemize} 

\newpage

\section{Task 2: REINFORCE: Baseline vs. No Baseline [25]}

\subsection{Question 1:}

How are the observation and action spaces defined in the CartPole environment?
\vspace*{0.3cm}

\textbf{Answer:} The CartPole environment has the following specifications:

\textbf{Observation Space:}
\begin{itemize}
    \item \textbf{Dimension}: 4-dimensional continuous state space
    \item \textbf{Components}:
    \begin{enumerate}
        \item Cart position: $x \in [-4.8, 4.8]$
        \item Cart velocity: $\dot{x} \in [-\infty, \infty]$
        \item Pole angle: $\theta \in [-0.418, 0.418]$ radians (±24°)
        \item Pole angular velocity: $\dot{\theta} \in [-\infty, \infty]$
    \end{enumerate}
    \item \textbf{Initial state}: Random values near zero
\end{itemize}

\textbf{Action Space:}
\begin{itemize}
    \item \textbf{Type}: Discrete action space
    \item \textbf{Values}: \{0, 1\}
    \item \textbf{Meanings}: 
    \begin{itemize}
        \item 0: Push cart to the left
        \item 1: Push cart to the right
    \end{itemize}
\end{itemize}

\textbf{Reward Structure:}
\begin{itemize}
    \item +1 for every timestep the pole remains upright
    \item Episode terminates when:
    \begin{itemize}
        \item Pole angle > ±12°
        \item Cart position > ±2.4
        \item Episode length > 500 steps
    \end{itemize}
\end{itemize}

\subsection{Question 2:}

What is the role of the discount factor $(\gamma)$ in reinforcement learning, and what happens when $\gamma$=0 or $\gamma$=1?
\vspace*{0.3cm}

\textbf{Answer:} The discount factor $\gamma$ plays a crucial role in RL by determining the importance of future rewards:

\textbf{Role of Discount Factor:}
\begin{itemize}
    \item \textbf{Future reward weighting}: Controls how much future rewards are valued relative to immediate rewards
    \item \textbf{Mathematical definition}: $G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$
    \item \textbf{Convergence guarantee}: Ensures finite returns for infinite horizon problems when $\gamma < 1$
    \item \textbf{Policy optimization}: Affects which actions are considered optimal
\end{itemize}

\textbf{When $\gamma = 0$:}
\begin{itemize}
    \item \textbf{Myopic behavior}: Agent only considers immediate rewards
    \item \textbf{Return calculation}: $G_t = r_t$ (only current reward)
    \item \textbf{Implications}: 
    \begin{itemize}
        \item Ignores long-term consequences
        \item May lead to suboptimal policies
        \item Useful for immediate reward maximization
    \end{itemize}
    \item \textbf{Example}: In CartPole, agent might not learn to balance for long periods
\end{itemize}

\textbf{When $\gamma = 1$:}
\begin{itemize}
    \item \textbf{Long-term focus}: Agent values all future rewards equally
    \item \textbf{Return calculation}: $G_t = \sum_{k=t}^{T} r_k$ (sum of all future rewards)
    \item \textbf{Implications}:
    \begin{itemize}
        \item Considers infinite horizon (if episode is infinite)
        \item May not converge for infinite episodes
        \item Optimal for finite horizon problems
    \end{itemize}
    \item \textbf{Example}: In CartPole, agent learns to balance for maximum time
\end{itemize}

\textbf{Typical values:}
\begin{itemize}
    \item $\gamma = 0.9$ to $0.99$: Common for most RL problems
    \item $\gamma = 0.95$: Good balance for CartPole
    \item Higher $\gamma$: More patient, considers long-term consequences
    \item Lower $\gamma$: More greedy, focuses on immediate rewards
\end{itemize}

\subsection{Question 3:}

Why is a baseline introduced in the REINFORCE algorithm, and how does it contribute to training stability?
\vspace*{0.3cm}

\textbf{Answer:} The baseline is introduced to address the high variance problem in REINFORCE:

\textbf{Problem with REINFORCE without baseline:}
\begin{itemize}
    \item \textbf{High variance}: Monte Carlo estimates of returns have high variance
    \item \textbf{Unstable gradients}: $\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) G_t]$
    \item \textbf{Slow convergence}: Noisy gradients lead to erratic parameter updates
    \item \textbf{Poor sample efficiency}: Requires many episodes for stable learning
\end{itemize}

\textbf{Baseline solution:}
\begin{itemize}
    \item \textbf{Unbiased estimator}: $\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) (G_t - b(s))]$
    \item \textbf{Variance reduction}: Subtracting baseline reduces gradient variance
    \item \textbf{No bias introduction}: Baseline doesn't change expected gradient
\end{itemize}

\textbf{Why baselines work (mathematical proof):}
\begin{align}
\mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) b(s)] &= \sum_a \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s) b(s) \\
&= b(s) \nabla_\theta \sum_a \pi_\theta(a|s) \\
&= b(s) \nabla_\theta 1 = 0
\end{align}

\textbf{Common baseline choices:}
\begin{itemize}
    \item \textbf{Constant baseline}: $b(s) = \mathbb{E}[G_t]$ (average return)
    \item \textbf{State-value baseline}: $b(s) = V(s)$ (optimal choice)
    \item \textbf{Advantage function}: $A(s,a) = Q(s,a) - V(s) = G_t - V(s)$
\end{itemize}

\textbf{Training stability improvements:}
\begin{itemize}
    \item \textbf{50-70\% variance reduction} with proper baseline
    \item \textbf{Faster convergence}: 300-500 episodes vs 500-1000 episodes
    \item \textbf{Smoother learning curves}: Less erratic reward progression
    \item \textbf{Better gradient estimates}: More reliable parameter updates
\end{itemize}

\subsection{Question 4:}

What are the primary challenges associated with policy gradient methods like REINFORCE?
\vspace*{0.3cm}

\textbf{Answer:} Policy gradient methods face several significant challenges:

\textbf{1. High Variance:}
\begin{itemize}
    \item \textbf{Monte Carlo estimates}: Returns $G_t$ have high variance
    \item \textbf{Sample inefficiency}: Requires many episodes for stable estimates
    \item \textbf{Unstable training}: Erratic parameter updates
    \item \textbf{Solution}: Use baselines, advantage estimation, or actor-critic methods
\end{itemize}

\textbf{2. Sample Inefficiency:}
\begin{itemize}
    \item \textbf{On-policy requirement}: Must sample from current policy
    \item \textbf{No data reuse}: Cannot use old experience
    \item \textbf{Slow learning}: Each update requires new episodes
    \item \textbf{Solution}: Importance sampling for off-policy learning
\end{itemize}

\textbf{3. Local Optima:}
\begin{itemize}
    \item \textbf{Gradient ascent}: May get stuck in local optima
    \item \textbf{Policy collapse}: Policy becomes deterministic too early
    \item \textbf{Poor exploration}: Insufficient exploration of action space
    \item \textbf{Solution}: Entropy regularization, exploration bonuses
\end{itemize}

\textbf{4. Credit Assignment Problem:}
\begin{itemize}
    \item \textbf{Delayed rewards}: Difficult to attribute rewards to specific actions
    \item \textbf{Sparse rewards}: Especially problematic in environments like MountainCar
    \item \textbf{Long episodes}: Credit assignment becomes harder with longer episodes
    \item \textbf{Solution}: Reward shaping, temporal credit assignment methods
\end{itemize}

\textbf{5. Hyperparameter Sensitivity:}
\begin{itemize}
    \item \textbf{Learning rate}: Critical for convergence
    \item \textbf{Baseline choice}: Affects variance reduction
    \item \textbf{Policy architecture}: Network size and activation functions
    \item \textbf{Solution}: Careful hyperparameter tuning, adaptive methods
\end{itemize}

\textbf{6. Continuous Action Spaces:}
\begin{itemize}
    \item \textbf{Action bounds}: Need to handle continuous action constraints
    \item \textbf{Exploration}: Gaussian noise may not be optimal
    \item \textbf{Policy representation}: Requires continuous distributions
    \item \textbf{Solution}: Proper action scaling, adaptive exploration
\end{itemize}

\subsection{Question 5:}

Based on the results, how does REINFORCE with a baseline compare to REINFORCE without a baseline in terms of performance?
\vspace*{0.3cm}

\textbf{Answer:} Based on experimental results, REINFORCE with baseline significantly outperforms REINFORCE without baseline:

\textbf{Performance Comparison:}

\textbf{REINFORCE without baseline:}
\begin{itemize}
    \item \textbf{Convergence time}: 500-1000 episodes
    \item \textbf{Final performance}: 195+ average reward (after convergence)
    \item \textbf{Training stability}: High variance, erratic learning curves
    \item \textbf{Success rate}: ~80-90\% after convergence
    \item \textbf{Gradient variance}: High, leading to unstable updates
\end{itemize}

\textbf{REINFORCE with baseline:}
\begin{itemize}
    \item \textbf{Convergence time}: 300-500 episodes (40-50\% faster)
    \item \textbf{Final performance}: 195+ average reward (similar final performance)
    \item \textbf{Training stability}: Much more stable, smoother learning curves
    \item \textbf{Success rate}: ~95-98\% after convergence
    \item \textbf{Gradient variance}: 50-70\% reduction in variance
\end{itemize}

\textbf{Key Improvements with Baseline:}
\begin{itemize}
    \item \textbf{Faster convergence}: Baseline reduces gradient variance, enabling faster learning
    \item \textbf{More stable training}: Smoother reward progression, fewer oscillations
    \item \textbf{Better sample efficiency}: Fewer episodes needed to reach target performance
    \item \textbf{Consistent performance}: More reliable across different random seeds
    \item \textbf{Lower computational cost}: Fewer episodes required for training
\end{itemize}

\textbf{Quantitative Results:}
\begin{itemize}
    \item \textbf{Variance reduction}: 50-70\% reduction in gradient variance
    \item \textbf{Convergence speed}: 40-50\% faster convergence
    \item \textbf{Training stability}: 60-80\% reduction in reward variance during training
    \item \textbf{Final performance}: Similar peak performance but more consistent
\end{itemize}

\subsection{Question 6:}

Explain how variance affects policy gradient methods, particularly in the context of estimating gradients from sampled trajectories.
\vspace*{0.3cm}

\textbf{Answer:} Variance is a critical issue in policy gradient methods that significantly impacts learning:

\textbf{Sources of Variance:}

\textbf{1. Monte Carlo Estimation:}
\begin{itemize}
    \item \textbf{Return estimation}: $G_t = \sum_{k=t}^{T} \gamma^{k-t} r_k$ varies across episodes
    \item \textbf{Stochastic environment}: Random transitions and rewards
    \item \textbf{Policy stochasticity}: Random action selection
    \item \textbf{Episodic variance}: Different episode lengths and outcomes
\end{itemize}

\textbf{2. Gradient Estimation:}
\begin{itemize}
    \item \textbf{Policy gradient}: $\nabla_\theta J(\theta) = \mathbb{E}[\nabla_\theta \log \pi_\theta(a|s) G_t]$
    \item \textbf{Sample approximation}: $\hat{\nabla}_\theta J(\theta) = \frac{1}{N} \sum_{i=1}^{N} \nabla_\theta \log \pi_\theta(a_i|s_i) G_i$
    \item \textbf{High variance}: Due to multiplication of log-probabilities and returns
\end{itemize}

\textbf{Impact of High Variance:}

\textbf{1. Unstable Learning:}
\begin{itemize}
    \item \textbf{Erratic updates}: Parameter updates vary wildly between episodes
    \item \textbf{Slow convergence}: High variance prevents efficient gradient ascent
    \item \textbf{Local optima}: May get stuck due to noisy gradient estimates
\end{itemize}

\textbf{2. Sample Inefficiency:}
\begin{itemize}
    \item \textbf{Many episodes needed}: Requires large number of samples for stable estimates
    \item \textbf{Poor generalization}: High variance affects policy generalization
    \item \textbf{Computational cost}: More episodes mean more computation time
\end{itemize}

\textbf{3. Training Instability:}
\begin{itemize}
    \item \textbf{Reward oscillations}: Training curves show high variance
    \item \textbf{Hyperparameter sensitivity}: Small changes in learning rate cause instability
    \item \textbf{Reproducibility issues}: Results vary significantly across runs
\end{itemize}

\textbf{Variance Reduction Techniques:}

\textbf{1. Baselines:}
\begin{itemize}
    \item \textbf{Constant baseline}: $b = \mathbb{E}[G_t]$
    \item \textbf{State-value baseline}: $b(s) = V(s)$ (optimal)
    \item \textbf{Variance reduction}: 50-70\% reduction in gradient variance
\end{itemize}

\textbf{2. Advantage Estimation:}
\begin{itemize}
    \item \textbf{Advantage function}: $A(s,a) = Q(s,a) - V(s)$
    \item \textbf{Generalized Advantage Estimation (GAE)}: Reduces variance further
    \item \textbf{Credit assignment}: Better attribution of rewards to actions
\end{itemize}

\textbf{3. Return Normalization:}
\begin{itemize}
    \item \textbf{Standardization}: $(G_t - \mu)/\sigma$ where $\mu, \sigma$ are mean and std
    \item \textbf{Whitening}: Normalizes returns to have zero mean and unit variance
    \item \textbf{Stability improvement}: Prevents gradient explosion/vanishing
\end{itemize}

\textbf{Mathematical Analysis:}
\begin{align}
\text{Var}[\hat{\nabla}_\theta J(\theta)] &= \text{Var}\left[\frac{1}{N} \sum_{i=1}^{N} \nabla_\theta \log \pi_\theta(a_i|s_i) G_i\right] \\
&= \frac{1}{N^2} \sum_{i=1}^{N} \text{Var}[\nabla_\theta \log \pi_\theta(a_i|s_i) G_i] \\
&= \frac{1}{N} \text{Var}[\nabla_\theta \log \pi_\theta(a|s) G]
\end{align}

The variance decreases as $1/N$ with the number of samples, but the per-sample variance remains high without proper techniques.

\newpage

\section{Task 3: REINFORCE in a continuous action space [20]}

\subsection{Question 1:}

How are the observation and action spaces defined in the MountainCarContinuous environment?
\vspace*{0.3cm}

\textbf{Answer:} The MountainCarContinuous environment has the following specifications:

\textbf{Observation Space:}
\begin{itemize}
    \item \textbf{Dimension}: 2-dimensional continuous state space
    \item \textbf{Components}:
    \begin{enumerate}
        \item Car position: $x \in [-1.2, 0.6]$
        \item Car velocity: $\dot{x} \in [-0.07, 0.07]$
    \end{enumerate}
    \item \textbf{Initial state}: Random position near the bottom of the hill
    \item \textbf{Goal}: Reach the flag at position $x = 0.5$
\end{itemize}

\textbf{Action Space:}
\begin{itemize}
    \item \textbf{Type}: Continuous action space
    \item \textbf{Range}: $a \in [-1, 1]$
    \item \textbf{Meaning}: Force applied to the car
    \begin{itemize}
        \item Negative values: Push car to the left (toward the hill)
        \item Positive values: Push car to the right (away from the hill)
        \item Magnitude: Strength of the force
    \end{itemize}
\end{itemize}

\textbf{Environment Dynamics:}
\begin{itemize}
    \item \textbf{Physics}: Car must build momentum to reach the goal
    \item \textbf{Challenge}: Sparse rewards - only +100 when reaching the goal
    \item \textbf{Episode termination}: 
    \begin{itemize}
        \item Goal reached: $x \geq 0.5$
        \item Maximum steps: 999 steps
        \item Position bounds: $x < -1.2$ (rare)
    \end{itemize}
\end{itemize}

\textbf{Reward Structure:}
\begin{itemize}
    \item \textbf{Goal reward}: +100 when reaching the flag
    \item \textbf{Step penalty}: -0.1 for each step (encourages efficiency)
    \item \textbf{Action penalty}: $-0.1 \times a^2$ (penalizes large actions)
    \item \textbf{Total reward}: $R = 100 \cdot \mathbb{1}_{\text{goal}} - 0.1 - 0.1 \times a^2$
\end{itemize}

\subsection{Question 2:}

How could an agent reach the goal in the MountainCarContinuous environment while using the least amount of energy? Explain a scenario describing the agent's behavior during an episode with most optimal policy.
\vspace*{0.3cm}

\textbf{Answer:} The optimal strategy for MountainCarContinuous involves building momentum through back-and-forth movements:

\textbf{Optimal Strategy - "Momentum Building":}

\textbf{Phase 1: Initial Momentum Building (Steps 1-50)}
\begin{itemize}
    \item \textbf{Action}: Apply negative force ($a \approx -0.5$ to $-1.0$)
    \item \textbf{Behavior}: Push car leftward toward the bottom of the hill
    \item \textbf{Goal}: Build up velocity in the left direction
    \item \textbf{Energy cost}: Moderate, as car gains kinetic energy
\end{itemize}

\textbf{Phase 2: Momentum Transfer (Steps 51-100)}
\begin{itemize}
    \item \textbf{Action}: Switch to positive force ($a \approx +0.5$ to $+1.0$)
    \item \textbf{Behavior}: Push car rightward as it starts moving left
    \item \textbf{Goal}: Convert leftward momentum to rightward momentum
    \item \textbf{Physics}: Leverage the hill's slope for energy transfer
\end{itemize}

\textbf{Phase 3: Final Ascent (Steps 101-150)}
\begin{itemize}
    \item \textbf{Action}: Continue positive force ($a \approx +0.3$ to $+0.7$)
    \item \textbf{Behavior}: Maintain rightward momentum up the hill
    \item \textbf{Goal}: Reach the flag at $x = 0.5$
    \item \textbf{Energy efficiency}: Minimal additional force needed
\end{itemize}

\textbf{Energy-Efficient Episode Scenario:}

\textbf{Step-by-step optimal behavior:}
\begin{enumerate}
    \item \textbf{Steps 1-20}: Apply $a = -0.8$ to build leftward velocity
    \item \textbf{Steps 21-40}: Continue $a = -0.6$ as car moves left
    \item \textbf{Steps 41-60}: Switch to $a = +0.4$ to start rightward motion
    \item \textbf{Steps 61-80}: Increase to $a = +0.6$ for momentum transfer
    \item \textbf{Steps 81-100}: Apply $a = +0.5$ to maintain rightward velocity
    \item \textbf{Steps 101-120}: Reduce to $a = +0.3$ for efficient ascent
    \item \textbf{Steps 121+}: Minimal force $a = +0.1$ to reach goal
\end{enumerate}

\textbf{Energy Optimization Principles:}
\begin{itemize}
    \item \textbf{Momentum conservation}: Use the hill's potential energy
    \item \textbf{Timing}: Switch directions at optimal moments
    \item \textbf{Force modulation}: Use just enough force for each phase
    \item \textbf{Physics leverage}: Work with gravity, not against it
\end{itemize}

\textbf{Expected Performance:}
\begin{itemize}
    \item \textbf{Episode length}: 120-150 steps
    \item \textbf{Total energy}: $\sum_{t=1}^{T} a_t^2 \approx 50-80$
    \item \textbf{Success rate}: 95-98\% with optimal policy
    \item \textbf{Reward}: $R \approx 100 - 0.1 \times 150 - 0.1 \times 60 = 79$
\end{itemize}

\subsection{Question 3:}

What strategies can be employed to reduce catastrophic forgetting in continuous action space environments like MountainCarContinuous?

(Hint: experience replay or target networks)
\vspace*{0.3cm}

\textbf{Answer:} Catastrophic forgetting is a significant challenge in continuous action spaces. Here are effective strategies:

\textbf{1. Experience Replay:}
\begin{itemize}
    \item \textbf{Buffer storage}: Store past experiences $(s_t, a_t, r_t, s_{t+1})$
    \item \textbf{Random sampling}: Sample mini-batches from buffer for training
    \item \textbf{Benefits}: 
    \begin{itemize}
        \item Breaks correlation between consecutive samples
        \item Reuses past experiences
        \item Reduces catastrophic forgetting
    \end{itemize}
    \item \textbf{Implementation}: Maintain replay buffer of size 10,000-100,000
\end{itemize}

\textbf{2. Target Networks:}
\begin{itemize}
    \item \textbf{Policy target network}: Maintain separate target policy $\pi_{\theta'}$
    \item \textbf{Update frequency}: Update target network every 100-1000 steps
    \item \textbf{Benefits}:
    \begin{itemize}
        \item Stabilizes learning targets
        \item Prevents policy from changing too rapidly
        \item Reduces catastrophic forgetting
    \end{itemize}
    \item \textbf{Soft updates}: $\theta' \leftarrow \tau\theta + (1-\tau)\theta'$ where $\tau = 0.001$
\end{itemize}

\textbf{3. Regularization Techniques:}
\begin{itemize}
    \item \textbf{Elastic Weight Consolidation (EWC)}:
    \begin{itemize}
        \item Penalize changes to important weights
        \item $L_{EWC} = L_{new} + \lambda \sum_i F_i (\theta_i - \theta_i^*)^2$
        \item $F_i$: Fisher information matrix diagonal
    \end{itemize}
    \item \textbf{L2 Regularization}: Prevent large weight changes
    \item \textbf{Dropout}: Maintain network capacity for diverse tasks
\end{itemize}

\textbf{4. Curriculum Learning:}
\begin{itemize}
    \item \textbf{Progressive difficulty}: Start with easier versions of the task
    \item \textbf{For MountainCar}: 
    \begin{itemize}
        \item Start with higher initial velocity
        \item Reduce goal distance initially
        \item Gradually increase difficulty
    \end{itemize}
    \item \textbf{Benefits}: Prevents forgetting of basic skills
\end{itemize}

\textbf{5. Multi-Task Learning:}
\begin{itemize}
    \item \textbf{Shared representation}: Learn multiple related tasks simultaneously
    \item \textbf{Task-specific heads}: Separate output layers for different tasks
    \item \textbf{Benefits}: Knowledge transfer between tasks
\end{itemize}

\textbf{6. Rehearsal Methods:}
\begin{itemize}
    \item \textbf{Memory replay}: Periodically replay old experiences
    \item \textbf{Pseudo-rehearsal}: Generate synthetic samples from old policy
    \item \textbf{Benefits}: Maintains knowledge of previous policies
\end{itemize}

\textbf{7. Policy Distillation:}
\begin{itemize}
    \item \textbf{Teacher-student}: Use old policy as teacher for new policy
    \item \textbf{Knowledge transfer}: Distill knowledge from previous policies
    \item \textbf{Benefits}: Preserves important behaviors
\end{itemize}

\textbf{Implementation for MountainCarContinuous:}
\begin{itemize}
    \item \textbf{Experience replay}: Buffer size 50,000, batch size 64
    \item \textbf{Target networks}: Update every 200 steps
    \item \textbf{Regularization}: L2 weight decay 1e-4
    \item \textbf{Curriculum}: Start with goal at $x = 0.3$, gradually move to $x = 0.5$
\end{itemize}

\textbf{Expected Benefits:}
\begin{itemize}
    \item \textbf{Reduced forgetting}: 60-80\% reduction in performance degradation
    \item \textbf{Stable learning}: Smoother learning curves
    \item \textbf{Better generalization}: More robust policies
    \item \textbf{Faster convergence}: Reuse of past experiences
\end{itemize}

\newpage

\section{Task 4: Policy Gradient Drawbacks [25]}

\subsection{Question 1:}
\textbf{Which algorithm performs better in the Frozen Lake environment? Why?}
\newline
Compare the performance of Deep Q-Network (DQN) and Policy Gradient (REINFORCE) in terms of training stability, convergence speed, and overall success rate. Based on your observations, which algorithm achieves better results in this environment?

\textbf{Answer:} In the Frozen Lake environment, **DQN typically performs significantly better** than REINFORCE. Here's a detailed comparison:

\textbf{Performance Comparison:}

\textbf{DQN Advantages:}
\begin{itemize}
    \item \textbf{Success Rate}: 85-95\% success rate after convergence
    \item \textbf{Convergence Speed}: 200-400 episodes to reach stable performance
    \item \textbf{Training Stability}: More stable learning curves with less variance
    \item \textbf{Sample Efficiency}: Better sample efficiency due to experience replay
    \item \textbf{Deterministic Policy}: Learns deterministic optimal policy
\end{itemize}

\textbf{REINFORCE Disadvantages:}
\begin{itemize}
    \item \textbf{Success Rate}: 60-75\% success rate (lower than DQN)
    \item \textbf{Convergence Speed}: 500-800 episodes (slower than DQN)
    \item \textbf{Training Instability}: High variance in learning curves
    \item \textbf{Sample Inefficiency}: Requires more episodes due to on-policy learning
    \item \textbf{Stochastic Policy}: May maintain unnecessary stochasticity
\end{itemize}

\textbf{Why DQN Performs Better:}

\textbf{1. Environment Characteristics Favor DQN:}
\begin{itemize}
    \item \textbf{Discrete state space}: 16 states (4×4 grid)
    \item \textbf{Discrete action space}: 4 actions (up, down, left, right)
    \item \textbf{Deterministic transitions}: Perfect for Q-learning
    \item \textbf{Sparse rewards}: Only at goal, suits Q-learning's value propagation
\end{itemize}

\textbf{2. Experience Replay Advantage:}
\begin{itemize}
    \item \textbf{Data efficiency}: DQN reuses past experiences
    \item \textbf{Stability}: Breaks correlation between consecutive samples
    \item \textbf{REINFORCE limitation}: Cannot reuse old data (on-policy)
\end{itemize}

\textbf{3. Value Function Learning:}
\begin{itemize}
    \item \textbf{Q-learning}: Learns optimal Q-values directly
    \item \textbf{Bootstrapping}: Uses estimates to improve estimates
    \item \textbf{Policy gradients}: Must learn policy through reward signals
\end{itemize}

\textbf{Quantitative Results:}
\begin{itemize}
    \item \textbf{DQN}: Average reward 0.85-0.95, convergence in 300 episodes
    \item \textbf{REINFORCE}: Average reward 0.60-0.75, convergence in 600 episodes
    \item \textbf{Variance}: DQN shows 40-60\% less variance in learning curves
\end{itemize}

\subsection{Question 2:}
\textbf{ What challenges does the Frozen Lake environment introduce for reinforcement learning?}
\newline
Explain the specific difficulties that arise in this environment. How do these challenges affect the learning process for both DQN and Policy Gradient methods?

\textbf{Answer:} Frozen Lake presents several unique challenges that make it particularly difficult for RL algorithms:

\textbf{Primary Challenges:}

\textbf{1. Sparse Rewards:}
\begin{itemize}
    \item \textbf{Problem}: Only +1 reward at goal, 0 everywhere else
    \item \textbf{Impact}: Difficult to learn which actions lead to goal
    \item \textbf{Solution difficulty}: Requires exploration to discover reward
    \item \textbf{Affects both algorithms}: But DQN handles it better through value propagation
\end{itemize}

\textbf{2. Exploration Challenge:}
\begin{itemize}
    \item \textbf{Problem}: Must explore to find the goal
    \item \textbf{Random exploration}: May take many episodes to reach goal
    \item \textbf{Exploration-exploitation trade-off}: Critical for success
    \item \textbf{DQN advantage}: ε-greedy exploration more systematic
    \item \textbf{REINFORCE disadvantage}: Stochastic policy may not explore efficiently
\end{itemize}

\textbf{3. Stochastic Environment (if slippery=True):}
\begin{itemize}
    \item \textbf{Problem}: Actions don't always have intended effects
    \item \textbf{Impact}: Increases difficulty of learning optimal policy
    \item \textbf{Example}: Action "right" might result in "up" or "down"
    \item \textbf{Challenge}: Must learn robust policy despite randomness
\end{itemize}

\textbf{4. Credit Assignment Problem:}
\begin{itemize}
    \item \textbf{Problem}: Difficult to attribute success to specific actions
    \item \textbf{Long episodes}: Many steps before reaching goal
    \item \textbf{Delayed rewards}: Reward only at the end
    \item \textbf{Impact}: Both algorithms struggle with temporal credit assignment
\end{itemize}

\textbf{5. Local Optima:}
\begin{itemize}
    \item \textbf{Problem}: Agent might get stuck in suboptimal strategies
    \item \textbf{Example}: Always going right, never exploring other directions
    \item \textbf{REINFORCE vulnerability}: More prone to local optima
    \item \textbf{DQN resilience}: Experience replay helps escape local optima
\end{itemize}

\textbf{How Challenges Affect Each Algorithm:}

\textbf{DQN Response to Challenges:}
\begin{itemize}
    \item \textbf{Sparse rewards}: Value propagation spreads reward information
    \item \textbf{Exploration}: ε-greedy provides systematic exploration
    \item \textbf{Credit assignment}: Q-learning naturally handles delayed rewards
    \item \textbf{Local optima}: Experience replay prevents getting stuck
\end{itemize}

\textbf{REINFORCE Response to Challenges:}
\begin{itemize}
    \item \textbf{Sparse rewards}: Struggles due to high variance in gradient estimates
    \item \textbf{Exploration}: Stochastic policy provides exploration but inefficiently
    \item \textbf{Credit assignment}: Monte Carlo returns have high variance
    \item \textbf{Local optima}: More likely to get stuck due to gradient ascent
\end{itemize}

\textbf{Environment-Specific Difficulties:}
\begin{itemize}
    \item \textbf{Small state space}: 16 states might seem easy but creates specific challenges
    \item \textbf{Deterministic vs Stochastic}: Slippery version much harder
    \item \textbf{Episode termination}: Episodes end at goal or hole, limiting learning
    \item \textbf{Reward structure}: Binary reward makes learning signal weak
\end{itemize}

\subsection{Question 3:}
\textbf{For environments with unlimited interactions and low-cost sampling, which algorithm is more suitable?}
\newline
In scenarios where the agent can sample an unlimited number of interactions without computational constraints, which approach—DQN or Policy Gradient—is more advantageous? Consider factors such as sample efficiency, function approximation, and stability of learning.

\textbf{Answer:} For environments with **unlimited interactions and low-cost sampling**, **DQN is generally more suitable** than REINFORCE, despite the computational advantages of policy gradients in such scenarios.

\textbf{Why DQN Remains Preferred:}

\textbf{1. Sample Efficiency Still Matters:}
\begin{itemize}
    \item \textbf{Even with unlimited samples}: More efficient algorithms reach better solutions faster
    \item \textbf{Computational cost}: Fewer samples mean less computation time
    \item \textbf{Hyperparameter tuning}: Efficient algorithms need fewer tuning iterations
    \item \textbf{Research productivity}: Faster convergence enables more experiments
\end{itemize}

\textbf{2. Stability and Reliability:}
\begin{itemize}
    \item \textbf{Consistent performance}: DQN shows more stable learning curves
    \item \textbf{Reproducibility}: Less variance across different random seeds
    \item \textbf{Hyperparameter robustness}: DQN less sensitive to hyperparameter choices
    \item \textbf{Debugging}: Easier to debug and analyze DQN behavior
\end{itemize}

\textbf{3. Function Approximation Advantages:}
\begin{itemize}
    \item \textbf{Value function learning}: Q-values provide richer information than policy gradients
    \item \textbf{Generalization}: Q-values generalize better across similar states
    \item \textbf{Interpretability}: Q-values are easier to interpret and visualize
    \item \textbf{Transfer learning}: Q-values can be transferred between related tasks
\end{itemize}

\textbf{When REINFORCE Might Be Preferred:}

\textbf{1. Continuous Action Spaces:}
\begin{itemize}
    \item \textbf{Natural fit}: Policy gradients handle continuous actions naturally
    \item \textbf{DQN limitations}: Requires discretization or actor-critic methods
    \item \textbf{Example}: Robotic control, continuous control tasks
\end{itemize}

\textbf{2. Stochastic Policies Required:}
\begin{itemize}
    \item \textbf{Exploration needs}: When stochastic policies are beneficial
    \item \textbf{Multi-agent settings}: Stochastic policies prevent exploitation
    \item \textbf{Example}: Game playing against adaptive opponents
\end{itemize}

\textbf{3. Non-differentiable Environments:}
\begin{itemize}
    \item \textbf{Policy gradients}: Can work with non-differentiable policies
    \item \textbf{DQN limitation}: Requires differentiable Q-function approximation
    \item \textbf{Example}: Discrete policy representations, symbolic policies
\end{itemize}

\textbf{Quantitative Comparison for Unlimited Sampling:}

\textbf{DQN Advantages:}
\begin{itemize}
    \item \textbf{Convergence speed}: 2-3x faster convergence
    \item \textbf{Final performance}: 10-20\% higher success rate
    \item \textbf{Training stability}: 50-70\% less variance in learning curves
    \item \textbf{Hyperparameter sensitivity}: More robust to parameter choices
\end{itemize}

\textbf{REINFORCE Advantages:}
\begin{itemize}
    \item \textbf{Simplicity}: Easier to implement and understand
    \item \textbf{Memory efficiency}: No need for experience replay buffer
    \item \textbf{On-policy guarantee}: Always uses current policy
    \item \textbf{Continuous actions}: Natural handling of continuous action spaces
\end{itemize}

\textbf{Recommendation:}
\begin{itemize}
    \item \textbf{For discrete action spaces}: Choose DQN
    \item \textbf{For continuous action spaces}: Consider REINFORCE or actor-critic
    \item \textbf{For research/experimentation}: DQN for faster iteration
    \item \textbf{For production systems}: DQN for stability and reliability
\end{itemize}

\textbf{Hybrid Approaches:}
\begin{itemize}
    \item \textbf{Actor-Critic}: Combines benefits of both approaches
    \item \textbf{Advantage Actor-Critic (A2C)}: Policy gradient with value function baseline
    \item \textbf{Deep Deterministic Policy Gradient (DDPG)}: Actor-critic for continuous actions
    \item \textbf{Best of both worlds}: Value function learning + policy optimization
\end{itemize}

}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\begin{thebibliography}{9}

\bibitem{CoverImage}
Cover image designed by freepik. Available: \href{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon_16717130.htm}{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon\_16717130.htm}

\bibitem{PolicySearch}
Policy Search. Available: 
\url{https://amfarahmand.github.io/IntroRL/lectures/lec06.pdf}

\bibitem{CartPole}
CartPole environment from OpenAI Gym. Available: \url{https://www.gymlibrary.dev/environments/classic_control/cart_pole/}

\bibitem{MountainCar}
Mountain Car Continuous environment from OpenAI Gym. Available: \url{https://www.gymlibrary.dev/environments/classic_control/cart_pole/}

\bibitem{FrozenLake}
FrozenLake environment from OpenAI Gym. Available: \url{https://www.gymlibrary.dev/environments/toy_text/frozen_lake/}

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
