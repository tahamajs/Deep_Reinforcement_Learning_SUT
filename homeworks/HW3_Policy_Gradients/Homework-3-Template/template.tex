\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 3:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Policy-Based Methods
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE Taha Majidi } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large 401123456 } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}

\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\newpage

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Task} & \textbf{Points} \\
\hline
\text{Task 1: Policy Search: REINFORCE vs. GA} & 20 \\
\text{Task 2: REINFORCE: Baseline vs. No Baseline} & 25 \\
\text{Task 3: REINFORCE in a continuous action space} & 20 \\
\text{Task 4:Policy Gradient Drawbacks} & 25 \\
\hline
\text{Clarity and Quality of Code} & 5 \\
\text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bonus 1: Writing your report in Latex } & 10 \\
\hline
\end{array}
\]

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 1: Policy Search: REINFORCE vs. GA [20]}

\subsection{Question 1:}
How do these two methods differ in terms of their effectiveness for solving reinforcement learning tasks? 

\textbf{Answer:} REINFORCE and Genetic Algorithms (GA) differ fundamentally in their approach to solving RL tasks:

\textbf{REINFORCE (Policy Gradient):}
\begin{itemize}
    \item \textbf{Gradient-based}: Uses gradient information to directly optimize policy parameters
    \item \textbf{Sample efficient}: Leverages gradient information for faster convergence
    \item \textbf{On-policy}: Requires sampling from current policy, limiting data reuse
    \item \textbf{Continuous optimization}: Smooth parameter updates using backpropagation
    \item \textbf{High variance}: Monte Carlo estimates lead to noisy gradients
\end{itemize}

\textbf{Genetic Algorithm:}
\begin{itemize}
    \item \textbf{Evolutionary}: Uses population-based search without gradient information
    \item \textbf{Derivative-free}: Works with non-differentiable policies
    \item \textbf{Parallelizable}: Evaluates multiple policies simultaneously
    \item \textbf{Discrete optimization}: Uses mutation and crossover operations
    \item \textbf{Global search}: Can escape local optima through random mutations
\end{itemize}

\subsection{Question 2:}
Discuss the key differences in their \textbf{performance}, \textbf{convergence rates}, and \textbf{stability}. 

\textbf{Answer:} 

\textbf{Performance:}
\begin{itemize}
    \item \textbf{REINFORCE}: Generally achieves higher final performance due to gradient-based optimization
    \item \textbf{GA}: May struggle with fine-tuning but can find diverse solutions
    \item \textbf{REINFORCE} excels in smooth, differentiable environments
    \item \textbf{GA} performs better in discrete, combinatorial problems
\end{itemize}

\textbf{Convergence Rates:}
\begin{itemize}
    \item \textbf{REINFORCE}: Faster initial convergence (300-500 episodes) due to gradient information
    \item \textbf{GA}: Slower convergence (500-1000+ episodes) due to random search
    \item \textbf{REINFORCE}: More predictable convergence trajectory
    \item \textbf{GA}: Convergence depends heavily on population size and mutation rates
\end{itemize}

\textbf{Stability:}
\begin{itemize}
    \item \textbf{REINFORCE}: High variance in gradient estimates leads to unstable training
    \item \textbf{GA}: More stable but slower progress due to population averaging
    \item \textbf{REINFORCE}: Sensitive to hyperparameters (learning rate, baseline)
    \item \textbf{GA}: More robust to parameter settings but requires larger populations
\end{itemize}

\subsection{Question 3:}
Additionally, explore how each method handles exploration and exploitation, and suggest situations where one might be preferred over the other. 

\textbf{Answer:} 

\textbf{Exploration vs Exploitation:}

\textbf{REINFORCE:}
\begin{itemize}
    \item \textbf{Exploration}: Natural stochastic policy provides exploration
    \item \textbf{Exploitation}: Gradient ascent exploits promising directions
    \item \textbf{Balance}: Controlled by policy entropy and learning rate
    \item \textbf{Challenge}: Can get stuck in local optima
\end{itemize}

\textbf{Genetic Algorithm:}
\begin{itemize}
    \item \textbf{Exploration}: Mutation provides random exploration
    \item \textbf{Exploitation}: Selection pressure exploits good solutions
    \item \textbf{Balance}: Controlled by mutation rate and selection pressure
    \item \textbf{Advantage}: Maintains population diversity
\end{itemize}

\textbf{When to prefer each method:}

\textbf{Choose REINFORCE when:}
\begin{itemize}
    \item Policy is differentiable (neural networks)
    \item Sample efficiency is important
    \item Smooth reward landscapes
    \item Continuous action spaces
    \item Limited computational resources
\end{itemize}

\textbf{Choose GA when:}
\begin{itemize}
    \item Policy is non-differentiable
    \item Discrete action spaces
    \item Rugged reward landscapes with many local optima
    \item Parallel computing available
    \item Need diverse solution strategies
\end{itemize} 

\newpage

\section{Task 2: REINFORCE: Baseline vs. No Baseline [25]}

\subsection{Question 1:}

How are the observation and action spaces defined in the CartPole environment?
\vspace*{0.3cm}

\subsection{Question 2:}

What is the role of the discount factor $(\gamma)$ in reinforcement learning, and what happens when $\gamma$=0 or $\gamma$=1?
\vspace*{0.3cm}

\subsection{Question 3:}

Why is a baseline introduced in the REINFORCE algorithm, and how does it contribute to training stability?
\vspace*{0.3cm}

\subsection{Question 4:}

What are the primary challenges associated with policy gradient methods like REINFORCE?
\vspace*{0.3cm}

\subsection{Question 5:}

Based on the results, how does REINFORCE with a baseline compare to REINFORCE without a baseline in terms of performance?
\vspace*{0.3cm}

\subsection{Question 6:}

Explain how variance affects policy gradient methods, particularly in the context of estimating gradients from sampled trajectories.

\newpage

\section{Task 3: REINFORCE in a continuous action space [20]}

\subsection{Question 1:}

How are the observation and action spaces defined in the MountainCarContinuous environment?
\vspace*{0.3cm}

\subsection{Question 2:}

How could an agent reach the goal in the MountainCarContinuous environment while using the least amount of energy? Explain a scenario describing the agent's behavior during an episode with most optimal policy.
\vspace*{0.3cm}

\subsection{Question 3:}

What strategies can be employed to reduce catastrophic forgetting in continuous action space environments like MountainCarContinuous?

(Hint: experience replay or target networks)
\vspace*{0.3cm}

\newpage

\section{Task 4: Policy Gradient Drawbacks [25]}

\subsection{Question 1:}
\textbf{Which algorithm performs better in the Frozen Lake environment? Why?}
\newline
Compare the performance of Deep Q-Network (DQN) and Policy Gradient (REINFORCE) in terms of training stability, convergence speed, and overall success rate. Based on your observations, which algorithm achieves better results in this environment?

\subsection{Question 2:}
\textbf{ What challenges does the Frozen Lake environment introduce for reinforcement learning?}
\newline
Explain the specific difficulties that arise in this environment. How do these challenges affect the learning process for both DQN and Policy Gradient methods?

\subsection{Question 3:}
\textbf{For environments with unlimited interactions and low-cost sampling, which algorithm is more suitable?}
\newline
In scenarios where the agent can sample an unlimited number of interactions without computational constraints, which approach—DQN or Policy Gradient—is more advantageous? Consider factors such as sample efficiency, function approximation, and stability of learning.

}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\begin{thebibliography}{9}

\bibitem{CoverImage}
Cover image designed by freepik. Available: \href{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon_16717130.htm}{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon\_16717130.htm}

\bibitem{PolicySearch}
Policy Search. Available: 
\url{https://amfarahmand.github.io/IntroRL/lectures/lec06.pdf}

\bibitem{CartPole}
CartPole environment from OpenAI Gym. Available: \url{https://www.gymlibrary.dev/environments/classic_control/cart_pole/}

\bibitem{MountainCar}
Mountain Car Continuous environment from OpenAI Gym. Available: \url{https://www.gymlibrary.dev/environments/classic_control/cart_pole/}

\bibitem{FrozenLake}
FrozenLake environment from OpenAI Gym. Available: \url{https://www.gymlibrary.dev/environments/toy_text/frozen_lake/}

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
