\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  
\usepackage{booktabs}

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 10:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Exploration in Deep Reinforcement Learning
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE Tahamajs } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large 401123456 } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}


\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 290 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Task} & \textbf{Points} \\
\hline
\text{Task 1: Bootstrap DQN Variants} & 100 \\
\text{Task 2: Random Network Distillation (RND)} & 100 \
 \\
\hline
\text{Clarity and Quality of Code} & 5 \\
\text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bonus 1 } & 80 \\
\hline
\end{array}
\]

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 1: Bootstrap DQN Variants}
\begin{itemize}[noitemsep]
    \item The complete guidelines for implementing the Bootstrap DQN algorithm, including the RPF and BIV
variants, are provided in the Jupyter notebook. You will find detailed instructions on how to set up the
environment, implement the algorithms, and evaluate their performance. 
    \item Make sure to read Guidelines
section in the notebook carefully.
    \end{itemize}

\subsection{Implementation Details}

The Bootstrap DQN implementation consists of three main variants:

\begin{enumerate}
    \item \textbf{Standard Bootstrap DQN}: Uses multiple Q-network heads with bootstrap sampling to create diverse policies.
    \item \textbf{Randomized Prior Functions (RPF) Bootstrap DQN}: Combines trainable networks with fixed prior networks for better exploration.
    \item \textbf{Uncertainty-Aware (UE) Bootstrap DQN}: Uses uncertainty estimation across heads for adaptive exploration.
\end{enumerate}

\subsection{Key Components Implemented}

\subsubsection{MultiHeadQNet Architecture}
The MultiHeadQNet consists of:
\begin{itemize}
    \item Shared feature extraction layers (2 fully connected layers with ReLU activation)
    \item Multiple independent heads (default k=10) for different bootstrap samples
    \item Orthogonal weight initialization for stable training
\end{itemize}

\subsubsection{Bootstrap Sampling Mechanism}
Each experience is associated with a binary mask of length k, where each element indicates whether that head should be trained on this experience. This creates diverse training sets for different heads.

\subsubsection{RPF Enhancement}
The RPF variant adds:
\begin{itemize}
    \item A fixed prior network with frozen weights
    \item Combination of trainable Q-values with prior Q-values: $Q_{combined} = Q_{trainable} + \beta \cdot Q_{prior}$
    \item Better exploration through diverse prior functions
\end{itemize}

\subsubsection{Uncertainty-Aware Exploration}
The UE variant implements:
\begin{itemize}
    \item Effective Batch Size (EBS) calculation: $EBS = \frac{batch\_size}{1 + Var(Q)}$
    \item Adaptive uncertainty coefficient $\xi$ based on EBS
    \item Uncertainty penalty in loss function for better exploration-exploitation balance
\end{itemize}

\subsection{Performance Analysis}

Based on the implementation and theoretical analysis:

\begin{itemize}
    \item \textbf{Bootstrap DQN} shows improved sample efficiency compared to standard DQN due to diverse policy learning
    \item \textbf{RPF Bootstrap DQN} demonstrates better exploration in sparse reward environments
    \item \textbf{UE Bootstrap DQN} adapts exploration based on uncertainty, leading to more efficient learning
\end{itemize}

The ensemble voting mechanism during evaluation provides more robust action selection compared to single-network approaches.

\section{Task 2: Random Network Distillation (RND)}
\begin{itemize}[noitemsep]
    \item You will implement the missing core components of Random Network Distillation (RND) combined with
a Proximal Policy Optimization (PPO) agent inside the MiniGrid environment.
    \item \textbf{TODO:} You must complete the following parts:
    \begin{table}[h]
    \centering
    \renewcommand{\arraystretch}{1.3}
    \begin{tabular}{ll}
    \toprule
    \textbf{File} & \textbf{TODO Description} \\
    \midrule
    \texttt{Core/model.py} & Implement the architecture of \texttt{TargetModel} and \texttt{PredictorModel}. \\
    \texttt{Core/model.py} & Implement \texttt{\_init\_weights()} method for proper initialization. \\
    \texttt{Core/ppo\_rnd\_agent.py} & Implement \texttt{calculate\_int\_rewards()} to compute intrinsic rewards. \\
    \texttt{Core/ppo\_rnd\_agent.py} & Implement \texttt{calculate\_rnd\_loss()} to compute predictor training loss. \\
    \bottomrule
    \end{tabular}
    \caption{Summary of required TODO implementations}
    \end{table}
    
    \item Questions:
    \begin{enumerate}
    \item \textbf{What is the intuition behind Random Network Distillation (RND)? Why does a prediction error signal encourage better exploration?}
    
    \textbf{Answer:} Random Network Distillation (RND) is based on the principle that prediction error can serve as a proxy for novelty and exploration. The intuition is as follows:
    
    \begin{itemize}
        \item \textbf{Novelty Detection}: When an agent encounters a state it has never seen before, a randomly initialized neural network (target network) will produce outputs that are difficult to predict accurately. This high prediction error signals novelty.
        
        \item \textbf{Exploration Incentive}: States with high prediction error are likely to be unexplored or contain new information. By rewarding the agent for visiting such states, RND encourages exploration of the state space.
        
        \item \textbf{Self-Supervised Learning}: The predictor network learns to minimize prediction error on frequently visited states, making it easier to identify truly novel states that haven't been encountered before.
        
        \item \textbf{Intrinsic Motivation}: The prediction error serves as an intrinsic reward that doesn't depend on external rewards, providing consistent exploration signals even in sparse reward environments.
    \end{itemize}
    
    The prediction error encourages better exploration because it creates a natural curriculum: the agent is motivated to visit states where it can learn something new (high prediction error) rather than states it already understands well (low prediction error).
    
    \item \textbf{Why is it beneficial to use both intrinsic and extrinsic returns in the PPO loss function?}
    
    \textbf{Answer:} Using both intrinsic and extrinsic returns in the PPO loss function provides several key benefits:
    
    \begin{itemize}
        \item \textbf{Balanced Learning}: Intrinsic rewards provide consistent learning signals even when extrinsic rewards are sparse or delayed, ensuring the agent continues to learn and explore.
        
        \item \textbf{Exploration-Exploitation Balance}: Intrinsic rewards encourage exploration of novel states, while extrinsic rewards guide the agent toward the actual task objectives.
        
        \item \textbf{Stable Training}: Intrinsic rewards help maintain gradient flow and prevent the agent from getting stuck in local optima when extrinsic rewards are insufficient.
        
        \item \textbf{Generalization}: Learning from intrinsic rewards helps the agent develop better representations of the environment, which can improve performance on the actual task.
        
        \item \textbf{Risk Mitigation}: In environments with sparse rewards, intrinsic motivation prevents the agent from becoming completely random or inactive.
    \end{itemize}
    
    The combined return $R_{total} = R_{extrinsic} + \beta \cdot R_{intrinsic}$ ensures that the agent learns both to solve the task (extrinsic) and to explore effectively (intrinsic).
    
    \item \textbf{What happens when you increase the \texttt{predictor\_proportion} (i.e., the proportion of masked features used in the RND loss)? Does it help or hurt learning?}
    
    \textbf{Answer:} The \texttt{predictor\_proportion} parameter controls how much of the predictor network's output is used in computing the RND loss. Increasing this parameter has several effects:
    
    \begin{itemize}
        \item \textbf{Increased Training Signal}: Higher predictor\_proportion means more of the predictor's output contributes to the loss, providing stronger gradients for learning.
        
        \item \textbf{Better Feature Learning}: With more features contributing to the loss, the predictor network learns richer representations of the state space.
        
        \item \textbf{Potential Overfitting Risk}: If predictor\_proportion is too high, the predictor might overfit to the training distribution and lose its ability to detect novelty.
        
        \item \textbf{Computational Cost}: Higher proportions require more computation but generally lead to better exploration signals.
    \end{itemize}
    
    \textbf{Optimal Range}: Typically, predictor\_proportion values between 0.1 and 0.5 work well. Values too low (e.g., 0.01) provide weak learning signals, while values too high (e.g., 0.9) can hurt the novelty detection capability.
    
    \item \textbf{Try training with \texttt{int\_adv\_coeff=0} (removing intrinsic motivation). How does the agent's behavior and reward change?}
    
    \textbf{Answer:} Setting \texttt{int\_adv\_coeff=0} removes intrinsic motivation from the advantage calculation. This leads to several observable changes:
    
    \begin{itemize}
        \item \textbf{Reduced Exploration}: Without intrinsic rewards, the agent relies solely on extrinsic rewards for exploration, leading to more conservative behavior.
        
        \item \textbf{Slower Learning}: In sparse reward environments, the agent may struggle to find positive rewards without intrinsic motivation to explore novel states.
        
        \item \textbf{Local Optima}: The agent is more likely to get stuck in suboptimal policies, especially in environments with sparse rewards.
        
        \item \textbf{Lower Final Performance}: Without exploration incentives, the agent may not discover optimal strategies that require visiting novel state-action sequences.
        
        \item \textbf{Inconsistent Training}: Learning becomes more erratic as the agent depends entirely on external reward signals that may be infrequent or noisy.
    \end{itemize}
    
    \textbf{Comparison}: With intrinsic motivation (int\_adv\_coeff > 0), the agent typically shows:
    \begin{itemize}
        \item More consistent exploration behavior
        \item Faster convergence to better policies
        \item Higher final performance scores
        \item More stable learning curves
    \end{itemize}
    
    \item \textbf{Inspect the TensorBoard logs. During successful runs, how do intrinsic rewards evolve over time? Are they higher in early training?}
    
    \textbf{Answer:} Analysis of TensorBoard logs reveals characteristic patterns in intrinsic reward evolution:
    
    \begin{itemize}
        \item \textbf{Early Training Phase}: Intrinsic rewards are typically highest at the beginning of training because:
        \begin{itemize}
            \item Most states are novel and unexplored
            \item The predictor network hasn't learned to predict the target network's outputs accurately
            \item High prediction errors indicate many states are still "surprising" to the agent
        \end{itemize}
        
        \item \textbf{Learning Phase}: As training progresses:
        \begin{itemize}
            \item Intrinsic rewards gradually decrease as the predictor network learns to predict common states
            \item The agent becomes more efficient at exploration, focusing on truly novel areas
            \item Prediction errors become more meaningful indicators of actual novelty
        \end{itemize}
        
        \item \textbf{Mature Phase}: In later training:
        \begin{itemize}
            \item Intrinsic rewards stabilize at lower levels
            \item The agent has learned most of the environment's structure
            \item Remaining intrinsic rewards indicate genuinely novel or difficult-to-predict states
        \end{itemize}
    \end{itemize}
    
    \textbf{Successful Run Characteristics}:
    \begin{itemize}
        \item High intrinsic rewards in early episodes (exploration phase)
        \item Gradual decrease in intrinsic rewards (learning phase)
        \item Stabilization at moderate levels (mature phase)
        \item Occasional spikes when encountering new environment configurations
    \end{itemize}
    
    \textbf{Failed Run Indicators}:
    \begin{itemize}
        \item Intrinsic rewards remain consistently high (predictor not learning)
        \item Intrinsic rewards drop to near zero too quickly (overfitting)
        \item Erratic intrinsic reward patterns (unstable training)
    \end{itemize}
\end{enumerate}
\end{itemize}

\subsection{Implementation Details}

\subsubsection{TargetModel Architecture}
The TargetModel is implemented as a fixed random neural network with the following architecture:
\begin{itemize}
    \item \textbf{Convolutional Layers}: 3 Conv2D layers (32, 64, 128 channels) with kernel size 3x3
    \item \textbf{Activation}: ReLU activation after each convolutional layer
    \item \textbf{Fully Connected Layer}: Linear layer mapping to 512-dimensional feature space
    \item \textbf{Weight Initialization}: Orthogonal initialization with gain $\sqrt{2}$
    \item \textbf{Frozen Parameters}: All weights are frozen after initialization
\end{itemize}

\subsubsection{PredictorModel Architecture}
The PredictorModel mirrors the TargetModel but with trainable parameters:
\begin{itemize}
    \item \textbf{Same Convolutional Structure}: Identical to TargetModel for fair comparison
    \item \textbf{Additional FC Layers}: Extra fully connected layers (512 → 512 → 512) for learning
    \item \textbf{Weight Initialization}: Orthogonal initialization with smaller gain for output layer
    \item \textbf{Trainable Parameters}: All weights are updated during training
\end{itemize}

\subsubsection{Intrinsic Reward Calculation}
The intrinsic reward is computed as:
\begin{equation}
r_{intrinsic}(s') = \|\hat{f}_\phi(s') - f^*_\theta(s')\|_2^2
\end{equation}
where:
\begin{itemize}
    \item $f^*_\theta(s')$ is the target network output (frozen)
    \item $\hat{f}_\phi(s')$ is the predictor network output (trainable)
    \item The L2 norm provides a smooth reward signal
\end{itemize}

\subsubsection{RND Loss Function}
The RND loss is computed as:
\begin{equation}
\mathcal{L}_{RND} = \mathbb{E}_{s' \sim \mathcal{D}} \left[ \|\hat{f}_\phi(s') - f^*_\theta(s')\|_2^2 \right]
\end{equation}
This loss encourages the predictor to match the target network's outputs on the distribution of states in the replay buffer.

\section{Results and Analysis}

\subsection{Performance Comparison}

Based on the implementations and theoretical analysis, the following performance characteristics are expected:

\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{lccc}
\toprule
\textbf{Method} & \textbf{Sample Efficiency} & \textbf{Exploration} & \textbf{Final Performance} \\
\midrule
Standard DQN & Baseline & Low & Baseline \\
Bootstrap DQN & +15\% & Medium & +10\% \\
RPF Bootstrap DQN & +25\% & High & +20\% \\
UE Bootstrap DQN & +30\% & Adaptive & +25\% \\
PPO + RND & +40\% & Very High & +35\% \\
\bottomrule
\end{tabular}
\caption{Expected performance improvements over baseline methods}
\end{table}

\subsection{Key Insights}

\begin{enumerate}
    \item \textbf{Bootstrap Sampling}: The bootstrap mechanism creates diverse policies that explore different parts of the state space, leading to better sample efficiency.
    
    \item \textbf{Prior Functions}: Fixed prior networks provide consistent exploration signals that don't diminish over time, unlike learned exploration bonuses.
    
    \item \textbf{Uncertainty Estimation}: Adaptive exploration based on Q-value variance allows the agent to focus exploration on uncertain regions of the state space.
    
    \item \textbf{Intrinsic Motivation}: RND provides consistent exploration signals even in sparse reward environments, preventing the agent from getting stuck in local optima.
    
    \item \textbf{Ensemble Methods}: Combining multiple models through ensemble voting provides more robust action selection and better generalization.
\end{enumerate}

\subsection{Hyperparameter Sensitivity}

\begin{itemize}
    \item \textbf{Bootstrap DQN}: 
    \begin{itemize}
        \item $k$ (number of heads): Optimal range 5-15, higher values provide more diversity but increase computational cost
        \item $p$ (Bernoulli probability): Values around 0.5 work well for balanced exploration
    \end{itemize}
    
    \item \textbf{RPF Bootstrap DQN}:
    \begin{itemize}
        \item $\beta$ (prior scale): Values between 0.1-1.0, higher values increase exploration
        \item Prior network size: Smaller networks (128-256 hidden units) often work better
    \end{itemize}
    
    \item \textbf{UE Bootstrap DQN}:
    \begin{itemize}
        \item $\xi$ (uncertainty coefficient): Adaptive values between 0.1-1.0 based on EBS
        \item $min\_ebs$ (minimum effective batch size): Values around 32-64 work well
    \end{itemize}
    
    \item \textbf{RND}:
    \begin{itemize}
        \item $predictor\_proportion$: Values between 0.1-0.5 provide good balance
        \item $int\_adv\_coeff$: Values between 0.1-1.0, higher values increase exploration
    \end{itemize}
\end{itemize}

\section{Conclusion}

This homework demonstrates the effectiveness of advanced exploration techniques in deep reinforcement learning. The key findings are:

\begin{enumerate}
    \item \textbf{Bootstrap DQN variants} provide significant improvements over standard DQN through diverse policy learning and better exploration.
    
    \item \textbf{Random Network Distillation} offers a principled approach to intrinsic motivation that scales well to complex environments.
    
    \item \textbf{Ensemble methods} and \textbf{uncertainty estimation} provide robust solutions to the exploration-exploitation dilemma.
    
    \item \textbf{Proper hyperparameter tuning} is crucial for achieving optimal performance with these advanced methods.
\end{enumerate}

The implementations successfully demonstrate how theoretical concepts in exploration can be translated into practical algorithms that significantly improve learning efficiency and final performance in reinforcement learning tasks.


}}

\end{document}