{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G29aFzYJF4Tt"
      },
      "source": [
        "<div align=\"center\">\n",
        "    <img src=\"https://www.sharif.ir/documents/20124/0/logo-fa-IR.png/4d9b72bc-494b-ed5a-d3bb-e7dfd319aec8?t=1609608338755\" alt=\"Logo\" width=\"200\">\n",
        "    <p><b> Reinforcement Learning Course, Dr. Rohban</b></p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydDCIkmkF4Tv"
      },
      "source": [
        "> - Full Name: Taha Majlesi\n",
        "> - Student ID: 810101504"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBaiqMVHM2bP"
      },
      "source": [
        "# Random Network Distillation (RND) with PPO - Homework Project\n",
        "\n",
        "  \n",
        "\n",
        "---\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "## 1. Introduction: Random Network Distillation (RND)\n",
        "\n",
        "A common way of doing exploration is to visit states with a large prediction error of some quantity, for instance, the TD error or even random functions.  \n",
        "The RND algorithm ([Exploration by Random Network Distillation](https://arxiv.org/abs/1810.12894)) aims at encouraging exploration by asking the exploration policy to more frequently undertake transitions where the prediction error of a random neural network function is high.\n",
        "\n",
        "Formally, let $f^*_\\theta(s')$ be a randomly chosen vector-valued function represented by a neural network.  \n",
        "RND trains another neural network, $\\hat{f}_\\phi(s')$, to match the predictions of $f^*_\\theta(s')$ under the distribution of datapoints in the buffer, as shown below:\n",
        "\n",
        "$$\n",
        "\\phi^* = \\arg\\min_\\phi \\mathbb{E}_{s,a,s'\\sim\\mathcal{D}} \\left[ \\left\\| \\hat{f}_\\phi(s') - f^*_\\theta(s') \\right\\| \\right]\n",
        "$$\n",
        "\n",
        "If a transition $(s, a, s')$ is in the distribution of the data buffer, the prediction error $\\mathcal{E}_\\phi(s')$ is expected to be small.  \n",
        "On the other hand, for all unseen state-action tuples, it is expected to be large.\n",
        "\n",
        "In practice, RND uses two critics:\n",
        "- an exploitation critic $Q_R(s,a)$, which estimates returns based on the true rewards,\n",
        "- and an exploration critic $Q_E(s,a)$, which estimates returns based on the exploration bonuses.\n",
        "\n",
        "To stabilize training, prediction errors are normalized before being used.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. What You Will Implement\n",
        "\n",
        "  \n",
        "\n",
        "You will implement the missing core components of Random Network Distillation (RND) combined with a Proximal Policy Optimization (PPO) agent inside the MiniGrid environment.\n",
        "\n",
        "  \n",
        "\n",
        "Specifically, you will:\n",
        "\n",
        "  \n",
        "\n",
        "- Complete the architecture of TargetModel and PredictorModel.\n",
        "\n",
        "  \n",
        "\n",
        "- Complete the initialization of weights for these models.\n",
        "\n",
        "  \n",
        "\n",
        "- Implement the intrinsic reward calculation (prediction error).\n",
        "\n",
        "  \n",
        "\n",
        "- Implement the RND loss calculation.\n",
        "\n",
        "  \n",
        "\n",
        "You will complete TODO sections inside two main files:\n",
        "\n",
        "  \n",
        "\n",
        "    Core/ppo_rnd_agent.py\n",
        "    Core/model.py\n",
        "\n",
        "  \n",
        "\n",
        "---\n",
        "\n",
        "  \n",
        "\n",
        "## 3. Project Structure\n",
        "\n",
        "\n",
        "```\n",
        "RND_PPO_Project/\n",
        " ‚îú‚îÄ‚îÄ main.py               # Main training loop and evaluation\n",
        " ‚îú‚îÄ‚îÄrequirements.txt       # Python dependencies               \n",
        " ‚îú‚îÄ‚îÄ Core/\n",
        " ‚îÇ    ‚îî‚îÄ‚îÄ ppo_rnd_agent.py         # Agent logic (policy + RND + training)\n",
        " ‚îÇ    ‚îî‚îÄ‚îÄ model.py         # Model architectures (policy, predictor, target)\n",
        " ‚îú‚îÄ‚îÄ Common/\n",
        " ‚îÇ    ‚îú‚îÄ‚îÄ config.py        # Hyperparameters and argument parsing\n",
        " ‚îÇ    ‚îú‚îÄ‚îÄ utils.py         # Utilities (normalization, helper functions)\n",
        " ‚îÇ    ‚îú‚îÄ‚îÄ logger.py        # Tensorboard logger\n",
        " ‚îÇ    ‚îî‚îÄ‚îÄ play.py          # Evaluation / Play script\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Modules Explanation\n",
        "\n",
        "| Module        | Description |\n",
        "|---------------|-------------|\n",
        "| `ppo_rnd_agent.py`    | **Core agent logic.** This file contains the PPO algorithm implementation and also handles the RND intrinsic reward mechanism. It manages action selection, GAE (Generalized Advantage Estimation), reward normalization, and model training. <br>‚û°Ô∏è You will modify this file to implement the intrinsic reward and RND loss functions. |\n",
        "| `model.py`    | **Neural network architectures.** This defines the structure of the policy network (used for action selection) and the two RND networks ‚Äî Target and Predictor. These networks process observations and output value estimates and policy distributions. <br>‚û°Ô∏è You will define the structure of the `TargetModel` and `PredictorModel` classes here and implement proper initialization. |\n",
        "| `utils.py`    | **Support utilities.** This includes helper functions like setting random seeds for reproducibility, maintaining running mean and variance for normalization, and a few decorators. It helps the rest of the codebase stay clean and modular. |\n",
        "| `config.py`   | **Experiment settings.** It defines all training hyperparameters (learning rate, batch size, gamma, etc.) and parses command-line flags such as `--train_from_scratch` or `--do_test`. This ensures experiments are configurable without touching main code. |\n",
        "| `logger.py`   | **Logging training metrics.** Records performance data like losses, episode rewards, and value function explained variances into TensorBoard. This helps you visually inspect whether the agent is learning or not. |\n",
        "| `play.py`     | **Evaluation module.** This file runs a trained agent in the environment without further learning. It resets the environment, feeds observations through the trained policy, and executes actions until the episode terminates. |\n",
        "| `runner.py`     | **Parallel environment interaction.** Runs a Gym environment in a separate process using torch.multiprocessing. It communicates with the main process to exchange observations and actions, enabling parallel experience collection. Supports episode reset and optional rendering. |\n",
        "| `main.py`     | **Project entry point.** Orchestrates the full experiment ‚Äî sets up environment, models, logger, and executes training or testing depending on the flag. This is where everything comes together. |\n",
        "\n",
        "---\n",
        "\n",
        "## 5. TODO Parts (Your Tasks)\n",
        "\n",
        "You must complete the following parts:\n",
        "\n",
        "| File | TODO Description |\n",
        "| :--- | :--- |\n",
        "| `Core/model.py` | Implement the architecture of `TargetModel` and `PredictorModel`. |\n",
        "| `Core/model.py` | Implement `_init_weights()` method for proper initialization. |\n",
        "| `Core/ppo_rnd_agent.py` | Implement `calculate_int_rewards()` to compute intrinsic rewards. |\n",
        "| `Core/ppo_rnd_agent.py` | Implement `calculate_rnd_loss()` to compute predictor training loss. |\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Corrected imports - fixing the import issues\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "from abc import ABC\n",
        "\n",
        "# Add the project directory to Python path\n",
        "project_dir = os.getcwd()\n",
        "if project_dir not in sys.path:\n",
        "    sys.path.append(project_dir)\n",
        "\n",
        "# Import project modules with correct names\n",
        "from Core.model import PolicyModel, TargetModel, PredictorModel\n",
        "from Core.ppo_rnd_agent import Brain\n",
        "from Common.config import get_params  # Fixed: was get_config\n",
        "from Common.utils import RunningMeanStd\n",
        "from Common.logger import Logger\n",
        "from Common.runner import Runner\n",
        "from Common.play import Play  # Fixed: was play\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho_lAcI7F4Tv"
      },
      "source": [
        "# Setup Code\n",
        "Before getting started we need to run some boilerplate code to set up our environment. You'll need to rerun this setup code each time you start the notebook.\n",
        "\n",
        "First, run this cell load the [autoreload](https://ipython.readthedocs.io/en/stable/config/extensions/autoreload.html?highlight=autoreload) extension. This allows us to edit `.py` source files, and re-import them into the notebook for a seamless editing and debugging experience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import all necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import sys\n",
        "from abc import ABC\n",
        "\n",
        "# Add the project directory to Python path\n",
        "project_dir = os.getcwd()\n",
        "if project_dir not in sys.path:\n",
        "    sys.path.append(project_dir)\n",
        "\n",
        "# Import project modules\n",
        "from Core.model import PolicyModel, TargetModel, PredictorModel\n",
        "from Core.ppo_rnd_agent import Brain\n",
        "from Common.config import get_config\n",
        "from Common.utils import RunningMeanStd\n",
        "from Common.logger import Logger\n",
        "from Common.runner import Runner\n",
        "from Common.play import play\n",
        "\n",
        "print(\"All imports successful!\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTx3_X1zF4Tw"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkR7h2lPF4Tw"
      },
      "source": [
        "#### In the following cell you are going to direct to your gooledrive if you are using GooleColab which is preferable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OY7wnz1YGS5z"
      },
      "outputs": [],
      "source": [
        "# ----------------------------\n",
        "# 1. Set up project directory (for Google Colab users)\n",
        "# ----------------------------\n",
        "# Uncomment the following lines if you're using Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Go to the Project directory\n",
        "# ----------------------------\n",
        "import os\n",
        "\n",
        "# For Google Colab users, uncomment and fill in your path:\n",
        "# GOOGLE_DRIVE_PATH_AFTER_MYDRIVE = 'your/path/here'\n",
        "# GOOGLE_DRIVE_PATH = os.path.join('drive', 'My Drive', GOOGLE_DRIVE_PATH_AFTER_MYDRIVE)\n",
        "# os.chdir(GOOGLE_DRIVE_PATH)\n",
        "\n",
        "# For local users, the current directory should already be correct\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(\"Project files:\")\n",
        "print(os.listdir('.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA-iCbPSj5rW"
      },
      "source": [
        "\n",
        "## 1. Install dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required dependencies\n",
        "# Uncomment the following line if running in Google Colab or if packages are missing\n",
        "# !pip install -r requirements.txt\n",
        "\n",
        "# Check if required packages are available\n",
        "try:\n",
        "    import gym\n",
        "    import minigrid\n",
        "    print(\"‚úÖ Gym and MiniGrid are available\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå Gym or MiniGrid not found. Please install them:\")\n",
        "    print(\"pip install gym==0.19.0\")\n",
        "    print(\"pip install minigrid\")\n",
        "\n",
        "try:\n",
        "    import torch\n",
        "    print(f\"‚úÖ PyTorch {torch.__version__} is available\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå PyTorch not found. Please install it:\")\n",
        "    print(\"pip install torch>=1.6.0\")\n",
        "\n",
        "try:\n",
        "    import numpy as np\n",
        "    print(f\"‚úÖ NumPy {np.__version__} is available\")\n",
        "except ImportError:\n",
        "    print(\"‚ùå NumPy not found. Please install it:\")\n",
        "    print(\"pip install numpy>=1.19.2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Complete Implementation\n",
        "\n",
        "The following sections show the complete implementation of all TODO parts that were required for the RND algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 TargetModel Implementation\n",
        "\n",
        "The TargetModel is a fixed random neural network that serves as the target for the PredictorModel to learn from. It consists of 3 convolutional layers followed by a fully connected layer that outputs 512-dimensional features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TargetModel Implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from abc import ABC\n",
        "\n",
        "class TargetModel(nn.Module, ABC):\n",
        "    def __init__(self, state_shape):\n",
        "        super(TargetModel, self).__init__()\n",
        "        c, w, h = state_shape\n",
        "        \n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(c, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # Calculate flattened size after conv layers\n",
        "        # For MiniGrid 7x7 input, after 3 conv layers with padding=1, size remains 7x7\n",
        "        flatten_size = 128 * 7 * 7\n",
        "        \n",
        "        # Fully connected layer to produce 512-dimensional features\n",
        "        self.encoded_features = nn.Linear(flatten_size, 512)\n",
        "        \n",
        "        self._init_weights()  # Call this after defining layers\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize all layers with orthogonal weights\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
        "                layer.bias.data.zero_()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Normalize input to [0, 1] range\n",
        "        x = inputs / 255.0\n",
        "        \n",
        "        # Pass through convolutional layers with ReLU activations\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        \n",
        "        # Flatten and pass through fully connected layer\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        encoded_features = self.encoded_features(x)\n",
        "        \n",
        "        return encoded_features\n",
        "\n",
        "# Test the TargetModel\n",
        "print(\"Testing TargetModel...\")\n",
        "state_shape = (3, 7, 7)  # RGB channels, 7x7 grid\n",
        "target_model = TargetModel(state_shape)\n",
        "print(f\"TargetModel created successfully!\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in target_model.parameters())}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(1, 3, 7, 7)\n",
        "output = target_model(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output mean: {output.mean().item():.4f}, std: {output.std().item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 PredictorModel Implementation\n",
        "\n",
        "The PredictorModel is a trainable neural network that learns to predict the output of the TargetModel. It has the same convolutional architecture as the TargetModel but includes additional fully connected layers for learning the mapping.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PredictorModel Implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from abc import ABC\n",
        "\n",
        "class PredictorModel(nn.Module, ABC):\n",
        "    def __init__(self, state_shape):\n",
        "        super(PredictorModel, self).__init__()\n",
        "        c, w, h = state_shape\n",
        "        \n",
        "        # Convolutional layers (same as TargetModel)\n",
        "        self.conv1 = nn.Conv2d(c, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        \n",
        "        # Calculate flattened size after conv layers\n",
        "        flatten_size = 128 * 7 * 7\n",
        "        \n",
        "        # Additional fully connected layers for prediction\n",
        "        self.fc1 = nn.Linear(flatten_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 512)\n",
        "        \n",
        "        # Final output layer to match TargetModel output dimension\n",
        "        self.encoded_features = nn.Linear(512, 512)\n",
        "        \n",
        "        self._init_weights()  # Call this after defining layers\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Initialize all layers with orthogonal weights\n",
        "        for layer in self.modules():\n",
        "            if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
        "                if layer == self.encoded_features:\n",
        "                    # Use smaller gain for final output layer to slow learning\n",
        "                    nn.init.orthogonal_(layer.weight, gain=np.sqrt(0.01))\n",
        "                else:\n",
        "                    nn.init.orthogonal_(layer.weight, gain=np.sqrt(2))\n",
        "                layer.bias.data.zero_()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # Normalize input to [0, 1] range\n",
        "        x = inputs / 255.0\n",
        "        \n",
        "        # Pass through convolutional layers with ReLU activations\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.relu(self.conv3(x))\n",
        "        \n",
        "        # Flatten and pass through fully connected layers\n",
        "        x = x.view(x.size(0), -1)  # Flatten\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        \n",
        "        # Final encoded features\n",
        "        encoded_features = self.encoded_features(x)\n",
        "        \n",
        "        return encoded_features\n",
        "\n",
        "# Test the PredictorModel\n",
        "print(\"Testing PredictorModel...\")\n",
        "state_shape = (3, 7, 7)  # RGB channels, 7x7 grid\n",
        "predictor_model = PredictorModel(state_shape)\n",
        "print(f\"PredictorModel created successfully!\")\n",
        "print(f\"Model parameters: {sum(p.numel() for p in predictor_model.parameters())}\")\n",
        "\n",
        "# Test forward pass\n",
        "test_input = torch.randn(1, 3, 7, 7)\n",
        "output = predictor_model(test_input)\n",
        "print(f\"Input shape: {test_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Output mean: {output.mean().item():.4f}, std: {output.std().item():.4f}\")\n",
        "\n",
        "# Test prediction error calculation\n",
        "target_output = target_model(test_input)\n",
        "prediction_error = torch.mean((output - target_output) ** 2, dim=1)\n",
        "print(f\"Prediction error: {prediction_error.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.3 Intrinsic Reward Calculation\n",
        "\n",
        "The intrinsic reward is computed as the prediction error between the TargetModel and PredictorModel. States with high prediction error (unseen states) receive higher intrinsic rewards, encouraging exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Intrinsic Reward Calculation Implementation\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def calculate_int_rewards(target_model, predictor_model, next_obs, state_rms, device, batch=True):\n",
        "    \"\"\"\n",
        "    Calculate intrinsic rewards based on prediction error between target and predictor models.\n",
        "    \n",
        "    Args:\n",
        "        target_model: Fixed random neural network\n",
        "        predictor_model: Trainable neural network\n",
        "        next_obs: Next observations (numpy array)\n",
        "        state_rms: Running mean and std for state normalization\n",
        "        device: PyTorch device\n",
        "        batch: Whether observations are batched\n",
        "    \n",
        "    Returns:\n",
        "        int_reward: Intrinsic rewards as numpy array\n",
        "    \"\"\"\n",
        "    if not batch:\n",
        "        next_obs = np.expand_dims(next_obs, axis=0)\n",
        "\n",
        "    # Normalize observations\n",
        "    norm_obs = np.clip(\n",
        "        (next_obs - state_rms.mean) / (state_rms.var**0.5), -5, 5\n",
        "    ).astype(np.float32)\n",
        "    norm_obs = torch.tensor(norm_obs).to(device)\n",
        "\n",
        "    # Get target features (fixed random network)\n",
        "    with torch.no_grad():\n",
        "        target_features = target_model(norm_obs)\n",
        "\n",
        "    # Get predicted features (trainable network)\n",
        "    pred_features = predictor_model(norm_obs)\n",
        "\n",
        "    # Compute squared error between predicted and target features\n",
        "    prediction_error = torch.mean((pred_features - target_features) ** 2, dim=1)\n",
        "\n",
        "    # Convert to numpy array\n",
        "    int_reward = prediction_error.cpu().numpy()\n",
        "\n",
        "    return int_reward\n",
        "\n",
        "# Test intrinsic reward calculation\n",
        "print(\"Testing intrinsic reward calculation...\")\n",
        "test_obs = np.random.rand(5, 3, 7, 7) * 255  # Batch of 5 observations\n",
        "\n",
        "# Create mock state_rms\n",
        "class MockStateRMS:\n",
        "    def __init__(self):\n",
        "        self.mean = np.zeros((3, 7, 7))\n",
        "        self.var = np.ones((3, 7, 7))\n",
        "\n",
        "state_rms = MockStateRMS()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Calculate intrinsic rewards\n",
        "int_rewards = calculate_int_rewards(target_model, predictor_model, test_obs, state_rms, device)\n",
        "print(f\"Input observations shape: {test_obs.shape}\")\n",
        "print(f\"Intrinsic rewards shape: {int_rewards.shape}\")\n",
        "print(f\"Intrinsic rewards: {int_rewards}\")\n",
        "print(f\"Mean intrinsic reward: {int_rewards.mean():.4f}\")\n",
        "print(f\"Std intrinsic reward: {int_rewards.std():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 RND Loss Calculation\n",
        "\n",
        "The RND loss is computed during training to update the PredictorModel. It uses a dropout mask to randomly select a fraction of samples for training, which helps stabilize learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RND Loss Calculation Implementation\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def calculate_rnd_loss(target_model, predictor_model, obs, state_rms, device, predictor_proportion=0.25):\n",
        "    \"\"\"\n",
        "    Calculate RND loss for training the predictor model.\n",
        "    \n",
        "    Args:\n",
        "        target_model: Fixed random neural network\n",
        "        predictor_model: Trainable neural network\n",
        "        obs: Observations (torch tensor)\n",
        "        state_rms: Running mean and std for state normalization\n",
        "        device: PyTorch device\n",
        "        predictor_proportion: Fraction of samples to use for training (dropout mask)\n",
        "    \n",
        "    Returns:\n",
        "        final_loss: Scalar loss value\n",
        "    \"\"\"\n",
        "    # Normalize observations\n",
        "    norm_obs = np.clip(\n",
        "        (obs.cpu().numpy() - state_rms.mean) / (state_rms.var**0.5), -5, 5\n",
        "    ).astype(np.float32)\n",
        "    norm_obs = torch.tensor(norm_obs).to(device)\n",
        "\n",
        "    # Get target features (fixed random network)\n",
        "    with torch.no_grad():\n",
        "        target = target_model(norm_obs)\n",
        "\n",
        "    # Get predicted features (trainable network)\n",
        "    pred = predictor_model(norm_obs)\n",
        "\n",
        "    # Compute squared error between predicted and target features\n",
        "    loss = torch.mean((pred - target) ** 2, dim=1)\n",
        "\n",
        "    # Apply dropout mask using predictor_proportion\n",
        "    # This randomly selects a fraction of samples for training the predictor\n",
        "    mask = torch.rand_like(loss) < predictor_proportion\n",
        "    masked_loss = loss * mask.float()\n",
        "\n",
        "    # Compute final loss as mean of masked losses\n",
        "    final_loss = torch.mean(masked_loss)\n",
        "\n",
        "    return final_loss\n",
        "\n",
        "# Test RND loss calculation\n",
        "print(\"Testing RND loss calculation...\")\n",
        "test_obs_tensor = torch.randn(10, 3, 7, 7) * 255  # Batch of 10 observations\n",
        "\n",
        "# Calculate RND loss\n",
        "rnd_loss = calculate_rnd_loss(target_model, predictor_model, test_obs_tensor, state_rms, device)\n",
        "print(f\"Input observations shape: {test_obs_tensor.shape}\")\n",
        "print(f\"RND loss: {rnd_loss.item():.4f}\")\n",
        "\n",
        "# Test with different predictor proportions\n",
        "for prop in [0.1, 0.25, 0.5, 1.0]:\n",
        "    loss = calculate_rnd_loss(target_model, predictor_model, test_obs_tensor, state_rms, device, prop)\n",
        "    print(f\"RND loss (proportion={prop}): {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.5 Key Implementation Details\n",
        "\n",
        "**Architecture Design:**\n",
        "- **TargetModel**: Fixed random network with 3 conv layers (32‚Üí64‚Üí128 channels) + FC layer (512 features)\n",
        "- **PredictorModel**: Same conv architecture + 2 additional FC layers (512‚Üí512‚Üí512) for learning\n",
        "- **Weight Initialization**: Orthogonal initialization with gain=‚àö2 for most layers, gain=‚àö0.01 for final layer\n",
        "\n",
        "**Intrinsic Reward Mechanism:**\n",
        "- Computes MSE between TargetModel and PredictorModel outputs\n",
        "- Higher prediction error = higher intrinsic reward = more exploration\n",
        "- Uses state normalization for stable training\n",
        "\n",
        "**Training Strategy:**\n",
        "- Dropout mask randomly selects 25% of samples for predictor training\n",
        "- Combines extrinsic and intrinsic rewards in advantage calculation\n",
        "- PPO updates both policy and predictor networks simultaneously\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Complete RND Training Demonstration\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Create models\n",
        "state_shape = (3, 7, 7)\n",
        "target_model = TargetModel(state_shape)\n",
        "predictor_model = PredictorModel(state_shape)\n",
        "\n",
        "# Set target model to eval mode (no gradients)\n",
        "target_model.eval()\n",
        "for param in target_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Create optimizer for predictor model only\n",
        "optimizer = optim.Adam(predictor_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 100\n",
        "batch_size = 32\n",
        "predictor_proportion = 0.25\n",
        "\n",
        "# Mock state normalization\n",
        "class MockStateRMS:\n",
        "    def __init__(self):\n",
        "        self.mean = np.zeros((3, 7, 7))\n",
        "        self.var = np.ones((3, 7, 7))\n",
        "\n",
        "state_rms = MockStateRMS()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Move models to device\n",
        "target_model = target_model.to(device)\n",
        "predictor_model = predictor_model.to(device)\n",
        "\n",
        "# Training loop\n",
        "losses = []\n",
        "prediction_errors = []\n",
        "\n",
        "print(\"Starting RND training demonstration...\")\n",
        "for epoch in range(num_epochs):\n",
        "    # Generate random observations (simulating environment states)\n",
        "    obs_batch = torch.randn(batch_size, 3, 7, 7).to(device) * 255\n",
        "    \n",
        "    # Calculate RND loss\n",
        "    loss = calculate_rnd_loss(target_model, predictor_model, obs_batch, state_rms, device, predictor_proportion)\n",
        "    \n",
        "    # Backward pass\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Calculate prediction error for monitoring\n",
        "    with torch.no_grad():\n",
        "        norm_obs = torch.randn(batch_size, 3, 7, 7).to(device) * 255\n",
        "        target_features = target_model(norm_obs)\n",
        "        pred_features = predictor_model(norm_obs)\n",
        "        prediction_error = torch.mean((pred_features - target_features) ** 2, dim=1).mean()\n",
        "    \n",
        "    losses.append(loss.item())\n",
        "    prediction_errors.append(prediction_error.item())\n",
        "    \n",
        "    if epoch % 20 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss.item():.4f}, Prediction Error = {prediction_error.item():.4f}\")\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Plot training progress\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses)\n",
        "plt.title('RND Loss Over Time')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(prediction_errors)\n",
        "plt.title('Prediction Error Over Time')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Prediction Error')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "print(f\"Final prediction error: {prediction_errors[-1]:.4f}\")\n",
        "print(f\"Loss reduction: {losses[0] - losses[-1]:.4f}\")\n",
        "print(f\"Prediction error reduction: {prediction_errors[0] - prediction_errors[-1]:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.4 RND Loss Calculation\n",
        "\n",
        "The RND loss is computed during training to update the PredictorModel. It uses a dropout mask to randomly select a fraction of samples for training, which helps stabilize learning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cj6IeGRij9D2"
      },
      "outputs": [],
      "source": [
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKK1I8xkQ-gX"
      },
      "source": [
        "\n",
        "## 7. Student Instructions (Updated)\n",
        "\n",
        "> **All TODO sections have been completed!** The following files now contain the full implementation:\n",
        "- `Core/ppo_rnd_agent.py` - Complete intrinsic reward and RND loss implementation\n",
        "- `Core/model.py` - Complete TargetModel and PredictorModel architectures\n",
        "\n",
        "> **What was implemented:**\n",
        "1. **TargetModel**: Fixed random network with 3 conv layers + FC layer (512 features)\n",
        "2. **PredictorModel**: Same conv architecture + 2 additional FC layers for learning\n",
        "3. **Intrinsic Rewards**: MSE between target and predictor outputs\n",
        "4. **RND Loss**: Training loss with dropout mask for predictor updates\n",
        "\n",
        "You can now proceed to train the agent with the complete implementation!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4F01w4apP5oo"
      },
      "source": [
        "## 8. Train the Agent\n",
        "\n",
        "Now that all implementations are complete, let's train the RND agent from scratch!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42IgzWrukAjM"
      },
      "outputs": [],
      "source": [
        "# Train the RND Agent\n",
        "# This will use the actual project implementation\n",
        "\n",
        "# First, let's check if we can import the project modules\n",
        "try:\n",
        "    from Core.ppo_rnd_agent import Brain\n",
        "    from Common.config import get_config\n",
        "    print(\"‚úÖ Successfully imported project modules\")\n",
        "    \n",
        "    # Get configuration\n",
        "    config = get_config()\n",
        "    print(f\"Configuration loaded: {config}\")\n",
        "    \n",
        "    # Create the brain (agent)\n",
        "    brain = Brain(**config)\n",
        "    print(\"‚úÖ Brain (RND agent) created successfully\")\n",
        "    \n",
        "    # Check if we can run training\n",
        "    print(\"Ready to train! Run the following command in terminal:\")\n",
        "    print(\"python main.py --train_from_scratch\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing project modules: {e}\")\n",
        "    print(\"Make sure you're in the correct directory and all files are present\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error creating brain: {e}\")\n",
        "    print(\"Check your configuration and dependencies\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As19xqS3kZP4"
      },
      "source": [
        "\n",
        "## 9. Visualize Training Logs\n",
        "\n",
        "Launch TensorBoard to monitor your training progress and analyze the RND agent's performance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "riwgtJAUkh56"
      },
      "outputs": [],
      "source": [
        "# Visualize Training Logs with TensorBoard\n",
        "import os\n",
        "import subprocess\n",
        "import webbrowser\n",
        "from threading import Timer\n",
        "\n",
        "def open_tensorboard():\n",
        "    \"\"\"Open TensorBoard in the browser\"\"\"\n",
        "    try:\n",
        "        # Start TensorBoard\n",
        "        log_dir = \"Logs\"  # Default log directory\n",
        "        if not os.path.exists(log_dir):\n",
        "            print(f\"‚ùå Log directory '{log_dir}' not found.\")\n",
        "            print(\"Make sure you've run training first to generate logs.\")\n",
        "            return\n",
        "        \n",
        "        print(f\"üìä Starting TensorBoard with log directory: {log_dir}\")\n",
        "        print(\"TensorBoard will be available at: http://localhost:6006\")\n",
        "        \n",
        "        # Start TensorBoard process\n",
        "        process = subprocess.Popen([\n",
        "            \"tensorboard\", \n",
        "            \"--logdir\", log_dir,\n",
        "            \"--port\", \"6006\",\n",
        "            \"--host\", \"0.0.0.0\"\n",
        "        ], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        \n",
        "        # Open browser after a short delay\n",
        "        def open_browser():\n",
        "            webbrowser.open(\"http://localhost:6006\")\n",
        "        \n",
        "        Timer(2.0, open_browser).start()\n",
        "        \n",
        "        print(\"‚úÖ TensorBoard started successfully!\")\n",
        "        print(\"Press Ctrl+C to stop TensorBoard\")\n",
        "        \n",
        "    except FileNotFoundError:\n",
        "        print(\"‚ùå TensorBoard not found. Please install it:\")\n",
        "        print(\"pip install tensorboard\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error starting TensorBoard: {e}\")\n",
        "\n",
        "# For Google Colab users, use the magic command instead\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    print(\"üîç Google Colab detected. Using magic command for TensorBoard...\")\n",
        "    print(\"Run the following cell to start TensorBoard:\")\n",
        "    print(\"%load_ext tensorboard\")\n",
        "    print(\"%tensorboard --logdir Logs\")\n",
        "except ImportError:\n",
        "    print(\"üñ•Ô∏è  Local environment detected.\")\n",
        "    print(\"Run the following to start TensorBoard:\")\n",
        "    print(\"tensorboard --logdir Logs --port 6006\")\n",
        "    print(\"Then open http://localhost:6006 in your browser\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nBRTZdHkknG"
      },
      "source": [
        "\n",
        "## 10. Summary\n",
        "\n",
        "**Congratulations!** You have successfully implemented the complete Random Network Distillation (RND) algorithm with PPO. \n",
        "\n",
        "**Key Components Implemented:**\n",
        "1. ‚úÖ **TargetModel**: Fixed random neural network for generating target features\n",
        "2. ‚úÖ **PredictorModel**: Trainable network that learns to predict target features  \n",
        "3. ‚úÖ **Intrinsic Rewards**: Prediction error-based exploration bonuses\n",
        "4. ‚úÖ **RND Loss**: Training objective with dropout masking for stability\n",
        "\n",
        "**How RND Works:**\n",
        "- The TargetModel generates random features for each state\n",
        "- The PredictorModel learns to predict these features for seen states\n",
        "- Unseen states have high prediction error ‚Üí high intrinsic reward ‚Üí more exploration\n",
        "- This encourages the agent to visit novel states and improve exploration\n",
        "\n",
        "**Expected Results:**\n",
        "- The agent should show improved exploration in the MiniGrid environment\n",
        "- Intrinsic rewards should decrease over time as the agent explores more states\n",
        "- The agent should learn to solve the environment more efficiently than standard PPO\n",
        "\n",
        "Good luck with your training! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Testing and Summary\n",
        "print(\"üéâ RND Implementation Complete!\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Test all components\n",
        "print(\"\\nüìã Testing All Components:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# 1. Test TargetModel\n",
        "try:\n",
        "    target_model = TargetModel((3, 7, 7))\n",
        "    test_input = torch.randn(1, 3, 7, 7)\n",
        "    target_output = target_model(test_input)\n",
        "    print(f\"‚úÖ TargetModel: Input {test_input.shape} ‚Üí Output {target_output.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå TargetModel failed: {e}\")\n",
        "\n",
        "# 2. Test PredictorModel\n",
        "try:\n",
        "    predictor_model = PredictorModel((3, 7, 7))\n",
        "    predictor_output = predictor_model(test_input)\n",
        "    print(f\"‚úÖ PredictorModel: Input {test_input.shape} ‚Üí Output {predictor_output.shape}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå PredictorModel failed: {e}\")\n",
        "\n",
        "# 3. Test Intrinsic Reward Calculation\n",
        "try:\n",
        "    mock_state_rms = type('MockStateRMS', (), {\n",
        "        'mean': np.zeros((3, 7, 7)),\n",
        "        'var': np.ones((3, 7, 7))\n",
        "    })()\n",
        "    \n",
        "    int_rewards = calculate_int_rewards(\n",
        "        target_model, predictor_model, \n",
        "        test_input.numpy(), mock_state_rms, \n",
        "        torch.device('cpu')\n",
        "    )\n",
        "    print(f\"‚úÖ Intrinsic Rewards: {int_rewards.shape} rewards calculated\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Intrinsic Rewards failed: {e}\")\n",
        "\n",
        "# 4. Test RND Loss Calculation\n",
        "try:\n",
        "    rnd_loss = calculate_rnd_loss(\n",
        "        target_model, predictor_model,\n",
        "        test_input, mock_state_rms,\n",
        "        torch.device('cpu')\n",
        "    )\n",
        "    print(f\"‚úÖ RND Loss: {rnd_loss.item():.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå RND Loss failed: {e}\")\n",
        "\n",
        "print(\"\\nüöÄ Ready for Training!\")\n",
        "print(\"-\" * 30)\n",
        "print(\"To start training, run:\")\n",
        "print(\"python main.py --train_from_scratch\")\n",
        "print(\"\\nTo test a trained model, run:\")\n",
        "print(\"python main.py --do_test\")\n",
        "\n",
        "print(\"\\nüìä Key Features Implemented:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"1. ‚úÖ TargetModel: Fixed random neural network (512 features)\")\n",
        "print(\"2. ‚úÖ PredictorModel: Trainable network with additional FC layers\")\n",
        "print(\"3. ‚úÖ Intrinsic Rewards: Prediction error-based exploration bonuses\")\n",
        "print(\"4. ‚úÖ RND Loss: Training objective with dropout masking\")\n",
        "print(\"5. ‚úÖ Complete PPO integration with RND exploration\")\n",
        "\n",
        "print(\"\\nüéØ Expected Results:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"‚Ä¢ Improved exploration in MiniGrid environment\")\n",
        "print(\"‚Ä¢ Decreasing intrinsic rewards over time\")\n",
        "print(\"‚Ä¢ Better sample efficiency compared to standard PPO\")\n",
        "print(\"‚Ä¢ Successful completion of MiniGrid tasks\")\n",
        "\n",
        "print(\"\\nüìà Monitoring Training:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"‚Ä¢ Use TensorBoard to visualize training progress\")\n",
        "print(\"‚Ä¢ Watch for decreasing RND loss and prediction errors\")\n",
        "print(\"‚Ä¢ Monitor episode rewards and success rates\")\n",
        "print(\"‚Ä¢ Check intrinsic vs extrinsic reward balance\")\n",
        "\n",
        "print(\"\\nüîß Troubleshooting:\")\n",
        "print(\"-\" * 30)\n",
        "print(\"‚Ä¢ If training fails: Check dependencies and GPU memory\")\n",
        "print(\"‚Ä¢ If no exploration: Verify intrinsic reward calculation\")\n",
        "print(\"‚Ä¢ If slow learning: Adjust learning rates and batch sizes\")\n",
        "print(\"‚Ä¢ If unstable: Check gradient clipping and normalization\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"üéâ Implementation Complete! Happy Training! üöÄ\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
