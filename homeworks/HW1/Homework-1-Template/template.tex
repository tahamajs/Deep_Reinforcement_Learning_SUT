\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 1:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Introduction to RL
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE [Full Name] } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large [Student Number] } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}

\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\subsection*{Grading}

The grading will be based on the following criteria, with a total of 100 points:

\[
\begin{array}{|l|l|}
\hline
\textbf{Task} & \textbf{Points} \\
\hline
\text{Task 1: Solving Predefined Environments} & 45 \\
\text{Task 2: Creating Custom Environments} & 45 \\

\hline
\text{Clarity and Quality of Code} & 5 \\
\text{Clarity and Quality of Report} & 5 \\
\hline
\text{Bonus 1: Writing a wrapper for a known env} & 10 \\
\text{Bonus 2: Implementing pygame env } & 20 \\
\text{Bonus 3: Writing your report in Latex } & 10 \\
\hline
\end{array}
\]

\textbf{Notes:}
\begin{itemize}
    \item Include well-commented code and relevant plots in your notebook.
    \item Clearly present all comparisons and analyses in your report.
    \item Ensure reproducibility by specifying all dependencies and configurations.
\end{itemize}

}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 1: Solving Predefined Environments [45-points]}

In this task, we explore reinforcement learning by training agents on predefined environments using Stable-Baselines3. We implement and compare different RL algorithms including A2C, PPO, DQN, and Q-Learning across multiple environments.

\subsection{Environment Selection and Implementation}

We selected four environments for comprehensive analysis:

\begin{itemize}
    \item \textbf{CartPole-v1}: A classic control problem where the agent must balance a pole on a cart
    \item \textbf{FrozenLake-v1}: A gridworld navigation problem with slippery surfaces
    \item \textbf{Taxi-v3}: A more complex navigation problem requiring pickup and dropoff
    \item \textbf{FlappyBird-v0}: A custom environment simulating the popular Flappy Bird game
\end{itemize}

\subsection{CartPole Environment Analysis}

\subsubsection{Environment Setup and Training}

The CartPole environment was trained using A2C and PPO algorithms:

\begin{verbatim}
# A2C Training
from stable_baselines3 import A2C, PPO, DQN
import gymnasium as gym

env = gym.make("CartPole-v1")
model = A2C("MlpPolicy", env, verbose=1, 
           tensorboard_log="./a2c_CartPole_tensorboard/")
model.learn(total_timesteps=500_000)
model.save("a2c_cartpole")

# PPO Training
model = PPO("MlpPolicy", env, verbose=1, 
           tensorboard_log="./PPO_CartPole_tensorboard/")
model.learn(total_timesteps=500_000)
model.save("ppo_cartpole")
\end{verbatim}

\subsubsection{Hyperparameter Analysis}

We conducted hyperparameter tuning experiments with different learning rates and discount factors:

\begin{verbatim}
learning_rates = [0.0001, 0.01]  
gammas = [0.95, 0.99]

for lr in learning_rates:
    for gamma in gammas:
        model = PPO("MlpPolicy", env, verbose=0,
                   learning_rate=lr, gamma=gamma,
                   tensorboard_log=f'./PPO_CartPole_tensorboard/ppo_lr{lr}_gamma{gamma}')
        model.learn(total_timesteps=200_000)
\end{verbatim}

\subsubsection{Reward Wrapping}

We implemented a reward wrapper to modify the reward function:

\begin{verbatim}
from gymnasium import RewardWrapper

class DoubledReward(RewardWrapper):
    def reward(self, reward):
        return reward * 2

wrapped_env = DoubledReward(env)
model = PPO("MlpPolicy", wrapped_env, verbose=0, 
           tensorboard_log="./PPO_CartPole_tensorboard/reward_wrapped_env_ppo")
model.learn(total_timesteps=500_000)
\end{verbatim}

\subsection{FrozenLake Environment Analysis}

\subsubsection{Algorithm Comparison}

For FrozenLake, we compared DQN and A2C algorithms:

\begin{verbatim}
env = gym.make("FrozenLake-v1", is_slippery=True)

# DQN Training
model = DQN("MlpPolicy", env, verbose=0, 
           tensorboard_log="./DQN_FrozenLake_tensorboard/")
model.learn(total_timesteps=500_000)

# A2C Training  
model = A2C("MlpPolicy", env, verbose=0, 
           tensorboard_log="./A2C_FrozenLake_tensorboard/")
model.learn(total_timesteps=500_000)
\end{verbatim}

\subsubsection{Custom Reward Function}

We implemented a modified reward function for FrozenLake:

\begin{verbatim}
class ModifiedFrozenLakeReward(RewardWrapper):
    def reward(self, reward):
        if reward == 0:            
            return -0.01
        elif reward == 1:           
            return 2
        else:
            return reward

wrapped_env = ModifiedFrozenLakeReward(env)
model = DQN("MlpPolicy", wrapped_env, verbose=0, 
           tensorboard_log="./DQN_FrozenLake_tensorboard/reward_wrapped_env_DQN")
model.learn(total_timesteps=500_000)
\end{verbatim}

\subsection{Taxi Environment Analysis}

\subsubsection{Q-Learning Implementation}

For the Taxi environment, we implemented a custom Q-Learning algorithm as it proved more effective than DQN and PPO:

\begin{verbatim}
def qlearning_train(taxi_env, learning_rate, discount_factor):
    writer = SummaryWriter(f"Qlearning_Taxi/taxi_experiment_lr_{learning_rate}_{discount_factor}")
    
    num_states = taxi_env.observation_space.n
    num_actions = taxi_env.action_space.n
    q_values = np.zeros((num_states, num_actions))
    
    alpha = learning_rate
    gamma = discount_factor
    eps = 1.0
    decay = 0.005
    episodes = 5000
    max_steps_per_ep = 99
    
    episode_rewards = []
    
    for ep in range(episodes):
        state, _ = taxi_env.reset()
        total_reward = 0
    
        for step in range(max_steps_per_ep):
            if random.random() < eps:
                chosen_action = taxi_env.action_space.sample()
            else:
                chosen_action = np.argmax(q_values[state, :])
            
            next_state, reward, done, _, _ = taxi_env.step(chosen_action)
            
            q_values[state, chosen_action] += alpha * (
                reward + gamma * np.max(q_values[next_state, :]) - q_values[state, chosen_action]
            )
    
            state = next_state
            total_reward += reward
    
            if done:
                break
    
        eps = 1 / (1 + decay * ep)
        episode_rewards.append(total_reward)
        
        avg_reward = np.mean(episode_rewards[-100:]) if ep >= 100 else np.mean(episode_rewards)
        writer.add_scalar("Mean_Episode_Reward", avg_reward, ep)
    
    return q_values
\end{verbatim}

\subsubsection{Agent Visualization}

We implemented a simulation function to visualize the trained agent:

\begin{verbatim}
def simulate_agent(environment, q_matrix, num_runs=5, max_steps_per_run=100):
    for run in range(num_runs):
        current_state, _ = environment.reset()
        finished = False
        
        for step in range(max_steps_per_run):            
            print(environment.render())
            sleep(0.5)  
            
            chosen_action = np.argmax(q_matrix[current_state, :])
            next_state, reward, finished, _, _ = environment.step(chosen_action)
            current_state = next_state

            if finished:
                print(f"Run completed in {step + 1} steps.\n")
                break
\end{verbatim}

\subsection{FlappyBird Environment Analysis}

\subsubsection{Environment Setup}

We used a third-party FlappyBird implementation:

\begin{verbatim}
!pip install flappy-bird-gymnasium

import flappy_bird_gymnasium
env = gymnasium.make("FlappyBird-v0", render_mode="rgb_array", use_lidar=True)

# Environment properties
print(f"Action Space: {env.action_space}")  # Discrete(2)
print(f"Observation Space: {env.observation_space}")  # Box(0.0, 1.0, (180,), float64)
\end{verbatim}

\subsubsection{Algorithm Training}

We trained A2C and PPO agents on FlappyBird:

\begin{verbatim}
# A2C Training
model = A2C("MlpPolicy", env, verbose=0, 
           tensorboard_log="./A2C_FlappyBird_tensorboard/")
model.learn(total_timesteps=5_000_000)

# PPO Training
model = PPO("MlpPolicy", env, verbose=0, 
           tensorboard_log="./PPO_mlp_FlappyBird_tensorboard/")
model.learn(total_timesteps=5_000_000)
\end{verbatim}

\subsection{Results and Analysis}

\subsubsection{Performance Comparison}

Through TensorBoard logging, we observed the following performance characteristics:

\begin{itemize}
    \item \textbf{CartPole}: Both A2C and PPO achieved stable performance, with PPO showing more consistent learning curves
    \item \textbf{FrozenLake}: DQN outperformed A2C due to the discrete nature of the environment
    \item \textbf{Taxi}: Q-Learning proved most effective, achieving optimal solutions in fewer episodes
    \item \textbf{FlappyBird}: Both A2C and PPO struggled with the high-dimensional observation space, requiring extensive training
\end{itemize}

\subsubsection{Hyperparameter Impact}

Our hyperparameter analysis revealed:

\begin{itemize}
    \item Higher learning rates (0.01) led to faster initial learning but less stable convergence
    \item Lower learning rates (0.0001) provided more stable learning but slower convergence
    \item Higher discount factors (0.99) were beneficial for long-term planning tasks
    \item Lower discount factors (0.95) worked better for immediate reward tasks
\end{itemize}

\subsubsection{Reward Wrapping Effects}

Custom reward functions significantly impacted learning:

\begin{itemize}
    \item Doubled rewards in CartPole accelerated learning
    \item Modified FrozenLake rewards improved exploration by penalizing non-progress
    \item Custom Taxi rewards with higher penalties for illegal actions improved policy quality
\end{itemize}

}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 2: Creating Custom Environments [45-points]}


}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 3: Pygame for RL environment [20-points]}


}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\begin{thebibliography}{9}

\bibitem{SuttonBarto}
R. Sutton and A. Barto, \textit{Reinforcement Learning: An Introduction}, 2nd Edition, 2020. Available online: \url{http://incompleteideas.net/book/the-book-2nd.html}


\bibitem{StableBaselines3}
A. Raffin et al., "Stable Baselines3: Reliable Reinforcement Learning Implementations," GitHub Repository, 2020. Available: \url{https://github.com/DLR-RM/stable-baselines3}.

\bibitem{Gymnasium}
Gymnasium Documentation. Available: \url{https://gymnasium.farama.org/}.

\bibitem{Pygame}
Pygame Documentation. Available: \url{https://www.pygame.org/docs/}.

\bibitem{CS285}
CS 285: Deep Reinforcement Learning, UC Berkeley, Pieter Abbeel. Course material available: \url{http://rail.eecs.berkeley.edu/deeprlcourse/}.

\bibitem{Freepik}
\href{https://www.freepik.com/free-vector/cute-artificial-intelligence-robot-isometric-icon_16717130.htm}{Cover image designed by freepik}

\end{thebibliography}


}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}