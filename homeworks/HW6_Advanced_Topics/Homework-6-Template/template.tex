\documentclass[12pt]{article}
\usepackage[a4paper, top=2.5cm, bottom=2.5cm, left=1.5cm, right=1.5cm]{geometry}
\usepackage{amsmath, amsfonts, amssymb, mathtools}
\usepackage{fancyhdr, setspace, parskip}
\usepackage{graphicx, caption, subfig, array, multirow}
\usepackage{hyperref, enumitem, cancel}
\usepackage[T1]{fontenc}
\usepackage{tgtermes}
\usepackage[dvipsnames]{xcolor}
\usepackage{tocloft}
\usepackage{titlesec}
\usepackage{lipsum}  

\definecolor{DarkBlue}{RGB}{10, 0, 80}

% Hyperlink setup
\hypersetup{
    colorlinks=true,
    linkcolor=DarkBlue,
    filecolor=BrickRed,      
    urlcolor=RoyalBlue,
}


% Header and footer customization
\fancyhead{}
\fancyhead[L]{
{\fontfamily{lmss}{\color{DarkBlue}
\textbf{\leftmark}
}}
}
\fancyhead[R]{
{\fontfamily{ppl}\selectfont {\color{DarkBlue}
{Deep RL [Spring 2025]}
}}
}

\fancyfoot{}
\fancyfoot[C]{
{\fontfamily{lmss}{\color{BrickRed}
\textbf{\thepage}
}}
}

\renewcommand{\sectionmark}[1]{ \markboth{\thesection\quad #1}{} }

\renewcommand{\headrule}{{\color{BrickRed}\hrule width\headwidth height 0.5pt}}
\renewcommand{\footrulewidth}{0pt}


% Table of Contents customizations
\renewcommand{\cftsecafterpnum}{\vskip6pt}
\renewcommand{\cftsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsubsubsecafterpnum}{\vskip3pt}
\renewcommand{\cftsecfont}{\sffamily\large}
\renewcommand{\cftsubsecfont}{\sffamily}
\renewcommand{\cftsubsubsecfont}{\sffamily}
% \renewcommand{\cftsecdotsep}{1}
\renewcommand{\cftsubsecdotsep}{1}
\renewcommand{\cftsubsubsecdotsep}{1}


% Section title styles
\titleformat*{\section}{\LARGE\bfseries\color{DarkBlue}}
\titleformat*{\subsection}{\Large\bfseries\color{DarkBlue}}
\titleformat*{\subsubsection}{\large\bfseries\color{DarkBlue}}

\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

% Start of the document
\pagestyle{fancy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\pagenumbering{gobble}
\thispagestyle{plain}

\begin{center}

\vspace*{-1.5cm}
\begin{figure}[!h]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/cover-std.png}
\end{figure}

{
\fontfamily{ppl}

{\color{DarkBlue} {\fontsize{30}{50} \textbf{
Deep Reinforcement Learning
}}}

{\color{DarkBlue} {\Large
Professor Mohammad Hossein Rohban
}}
}


\vspace{20pt}

{
\fontfamily{lmss}


{\color{RedOrange}
{\Large
Homework 6:
}\\
}
{\color{BrickRed}
\rule{12cm}{0.5pt}

{\Huge
Advanced Topics in Deep RL
}
\rule{12cm}{0.5pt}
}

\vspace{10pt}

{\color{RoyalPurple} { \small By:} } \\
\vspace{10pt}

{\color{Blue} { \LARGE Taha Majidi } } \\
\vspace{5pt}
{\color{RoyalBlue} { \Large 401123456 } }


\vspace*{\fill}
\begin{center}
\begin{tabular}{ccc}
    \includegraphics[width=0.14\linewidth]{figs/sharif-logo.png} & \includegraphics[width=0.14\linewidth]{figs/riml-logo.png} & \includegraphics[width=0.14\linewidth]{figs/dlr-logo.png} \\
\end{tabular}
\end{center}


\vspace*{-.25cm}

{\color{YellowOrange} {
\rule{10cm}{0.5pt} \\
\vspace{2pt}
\large Spring 2025}
}}
\vspace*{-1cm}

\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\pagenumbering{gobble}
\thispagestyle{plain}
{\fontfamily{lmss}\selectfont {\color{BrickRed} \textbf{\tableofcontents} }}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\subsection*{Overview}

This assignment explores advanced topics in Deep Reinforcement Learning, including:

\begin{itemize}
\item \textbf{Curiosity-Driven Learning}: Intrinsic motivation and exploration bonuses
\item \textbf{Reward Shaping}: Designing auxiliary rewards for better learning
\item \textbf{Transfer Learning}: Applying knowledge across related tasks
\item \textbf{Multi-Task RL}: Training single agents for multiple tasks
\item \textbf{Meta-Learning}: Learning to learn quickly from few samples
\item \textbf{Exploration Strategies}: Advanced exploration techniques
\end{itemize}

\subsection*{Learning Objectives}

\begin{enumerate}
\item Understand intrinsic motivation mechanisms in RL
\item Implement curiosity-driven exploration methods
\item Design effective reward shaping strategies
\item Apply transfer learning across related tasks
\item Implement multi-task learning approaches
\item Understand meta-learning principles and applications
\item Compare different exploration strategies
\end{enumerate}

}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\pagenumbering{arabic}

{\fontfamily{lmss}\selectfont {\color{DarkBlue}

\section{Task 1: Curiosity-Driven Learning}

\subsection{Problem Formulation}

In sparse reward environments, agents struggle with exploration because they rarely receive extrinsic rewards. Curiosity-driven learning addresses this by adding intrinsic motivation based on prediction error or novelty.

\subsection{Intrinsic Curiosity Module (ICM)}

The ICM uses a forward model to predict next states and computes intrinsic rewards based on prediction error:

\begin{equation}
r_{intrinsic} = \eta \| \hat{f}(s_t, a_t) - s_{t+1} \|^2
\end{equation}

where $\hat{f}$ is the learned forward model and $\eta$ is a scaling factor.

\subsection{Implementation}

\begin{verbatim}
class ICM(nn.Module):
    def __init__(self, state_dim, action_dim):
        super().__init__()
        # Feature encoder
        self.encoder = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
        
        # Forward model: predict next state features
        self.forward_model = nn.Sequential(
            nn.Linear(64 + action_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
        
        # Inverse model: predict action from states
        self.inverse_model = nn.Sequential(
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
    
    def forward(self, state, action, next_state):
        # Encode states
        phi_s = self.encoder(state)
        phi_s_next = self.encoder(next_state)
        
        # Forward model loss
        phi_s_next_pred = self.forward_model(
            torch.cat([phi_s, action], dim=-1)
        )
        forward_loss = F.mse_loss(phi_s_next_pred, phi_s_next.detach())
        
        # Inverse model loss
        action_pred = self.inverse_model(
            torch.cat([phi_s, phi_s_next], dim=-1)
        )
        inverse_loss = F.cross_entropy(action_pred, action)
        
        # Intrinsic reward
        intrinsic_reward = forward_loss.detach()
        
        return intrinsic_reward, forward_loss, inverse_loss
\end{verbatim}

\subsection{Questions and Answers}

\textbf{Q: What insight does the oracle reward provide?}

\textbf{A:} The oracle reward provides an upper bound on what any learning algorithm can achieve. It tells us the maximum possible average reward per step, which helps us evaluate how close our algorithms are to optimal performance. The gap between oracle performance and actual algorithm performance represents the "regret" - the cost of learning. This benchmark is crucial for understanding whether our algorithms are performing well or if there's significant room for improvement.

\textbf{Q: Why is the oracle considered "cheating" in a practical sense?}

\textbf{A:} The oracle is considered "cheating" because it has perfect knowledge of the true reward probabilities, which is never available in real-world scenarios. In practice, we must learn these probabilities through exploration and experience. The oracle represents an idealized scenario that helps us understand the theoretical limits but doesn't reflect the realistic constraints of bandit problems where we must balance exploration and exploitation without prior knowledge of arm qualities.

\section{Task 2: Random Network Distillation (RND)}

\subsection{Key Idea}

RND uses prediction error of a random network as exploration bonus. The algorithm trains a predictor network to match the output of a fixed random target network.

\subsection{Why It Works}

\begin{itemize}
\item Frequently visited states → predictor learns well → low error → low bonus
\item Novel states → high prediction error → high bonus → exploration
\end{itemize}

\subsection{Implementation}

\begin{verbatim}
class RND(nn.Module):
    def __init__(self, state_dim):
        super().__init__()
        # Fixed random target network
        self.target = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
        # Freeze target
        for param in self.target.parameters():
            param.requires_grad = False
        
        # Trainable predictor network
        self.predictor = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64)
        )
    
    def forward(self, state):
        target_features = self.target(state)
        predicted_features = self.predictor(state)
        
        # Intrinsic reward = prediction error
        intrinsic_reward = F.mse_loss(
            predicted_features, 
            target_features.detach(),
            reduction='none'
        ).mean(dim=-1)
        
        return intrinsic_reward
\end{verbatim}

\subsection{Advantages}

\begin{itemize}
\item No dynamics model needed
\item Computationally efficient
\item Works in high-dimensional spaces
\item Used successfully in Montezuma's Revenge
\end{itemize}

\subsection{Questions and Answers}

\textbf{Q: Why is the reward of the random agent generally lower and highly variable?}

\textbf{A:} The random agent's reward is lower because it doesn't learn from experience and continues to select suboptimal arms with equal probability. The high variability comes from the stochastic nature of random selection - sometimes it gets lucky and selects good arms, other times it selects poor arms. Without any learning mechanism, it cannot improve its performance over time or exploit knowledge about which arms are better.

\textbf{Q: How might you improve a random agent without using any learning mechanism?}

\textbf{A:} Without learning, you could improve a random agent by using domain knowledge or heuristics. For example, you could bias the random selection toward arms that historically perform better, use weighted random selection based on prior beliefs, or implement a simple rule-based strategy that doesn't require learning from rewards. However, these approaches still rely on some form of external information or assumptions about the problem.

\section{Task 3: Multi-Task Reinforcement Learning}

\subsection{Goal}

Train a single agent to perform multiple tasks simultaneously, leveraging shared representations and transfer learning.

\subsection{Approaches}

\subsubsection{Multi-Head Networks}

\begin{verbatim}
class MultiTaskPolicy(nn.Module):
    def __init__(self, state_dim, action_dim, num_tasks):
        super().__init__()
        # Shared trunk
        self.shared = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU()
        )
        
        # Task-specific heads
        self.heads = nn.ModuleList([
            nn.Linear(256, action_dim) 
            for _ in range(num_tasks)
        ])
    
    def forward(self, state, task_id):
        features = self.shared(state)
        logits = self.heads[task_id](features)
        return F.softmax(logits, dim=-1)
\end{verbatim}

\subsubsection{Task Conditioning}

\begin{verbatim}
class TaskConditionedPolicy(nn.Module):
    def __init__(self, state_dim, action_dim, task_embedding_dim):
        super().__init__()
        # Task embedding
        self.task_embedding = nn.Embedding(num_tasks, task_embedding_dim)
        
        # Conditioned policy
        self.policy = nn.Sequential(
            nn.Linear(state_dim + task_embedding_dim, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        )
    
    def forward(self, state, task_id):
        task_emb = self.task_embedding(task_id)
        x = torch.cat([state, task_emb], dim=-1)
        return self.policy(x)
\end{verbatim}

\subsection{Benefits}

\begin{itemize}
\item Transfer learning across tasks
\item Improved sample efficiency
\item Better generalization
\item Single model deployment
\end{itemize}

\subsection{Questions and Answers}

\textbf{Q: Why might the early exploration phase lead to high fluctuations in the reward curve?}

\textbf{A:} The early exploration phase leads to high fluctuations because the agent is randomly selecting arms without any knowledge of their true performance. During this phase, the agent might get lucky and select good arms (high rewards) or unlucky and select poor arms (low rewards). The randomness in arm selection combined with the stochastic nature of rewards creates high variance in the observed rewards.

\textbf{Q: What are the trade-offs of using a fixed exploration phase?}

\textbf{A:} The main trade-offs are: (1) \textbf{Too short exploration}: May miss the best arm, leading to suboptimal long-term performance. (2) \textbf{Too long exploration}: Wastes time on suboptimal arms, delaying exploitation of the best arm. (3) \textbf{Fixed duration}: Doesn't adapt to the difficulty of the problem - some bandit problems require more exploration than others. The optimal exploration duration depends on the gap between the best and second-best arms and the variance in rewards.

\section{Task 4: Meta-Reinforcement Learning}

\subsection{Problem}

Can an agent learn to learn? Can it quickly adapt to new tasks with few samples?

\subsection{Model-Agnostic Meta-Learning (MAML)}

MAML finds initial parameters $\theta$ such that after $K$ gradient steps on new task $T_i$, performance is maximized:

\begin{equation}
\theta^* = \arg\min_\theta \sum_i L_{T_i}(\theta - \alpha\nabla_\theta L_{T_i}(\theta))
\end{equation}

\subsection{Algorithm Implementation}

\begin{verbatim}
def maml_meta_train(tasks, meta_lr=0.001, inner_lr=0.01, inner_steps=5):
    # Initialize meta-parameters
    meta_params = initialize_policy()
    
    for meta_iteration in range(N):
        meta_gradient = 0
        
        for task in sample_tasks(tasks):
            # Inner loop: adapt to task
            params = meta_params.clone()
            
            for k in range(inner_steps):
                # Sample batch from task
                batch = task.sample()
                
                # Compute task loss and gradient
                loss = compute_loss(params, batch)
                grad = torch.autograd.grad(loss, params)
                
                # Inner update
                params = params - inner_lr * grad
            
            # Outer loop: meta-gradient
            test_batch = task.sample()
            test_loss = compute_loss(params, test_batch)
            meta_grad = torch.autograd.grad(test_loss, meta_params)
            
            meta_gradient += meta_grad
        
        # Meta-update
        meta_params = meta_params - meta_lr * meta_gradient
    
    return meta_params
\end{verbatim}

\subsection{Applications}

\begin{itemize}
\item Few-shot RL: Learn from few samples in new task
\item Fast adaptation to new environments
\item Robotic manipulation with varied objects
\end{itemize}

\subsection{Questions and Answers}

\textbf{Q: Why does UCB learn slowly even after many steps?}

\textbf{A:} UCB's apparent slow learning has deep theoretical and practical reasons: (1) \textbf{Conservative Exploration Bonus}: The bonus term $\sqrt{2 \log t / n_a}$ decreases slowly and is designed for asymptotic optimality, not finite-time performance. (2) \textbf{Logarithmic Sample Allocation}: UCB pulls suboptimal arms approximately $n_i(T) \approx 8 \log(T) / \Delta_i^2$ times, where $\Delta_i$ is the gap to optimal arm. For small gaps, this requires many pulls. (3) \textbf{Gap-Dependent Behavior}: When several arms have probabilities close to optimal, UCB continues exploring the second-best arm for many rounds.

\textbf{Q: Under what conditions might an explore-first strategy outperform UCB?}

\textbf{A:} Explore-first strategies can outperform UCB in several scenarios: (1) \textbf{Finite Horizon with Large Gaps}: When T is small and gaps are large, explore-first can quickly identify and exploit the best arm. (2) \textbf{Known Horizon}: When T is known, can optimize max_ex = O(T^(2/3)) to achieve regret O(T^(2/3)). (3) \textbf{Implementation Simplicity}: Explore-first is easier to implement correctly without careful tuning of exploration constants. (4) \textbf{Computational Constraints}: After exploration, O(1) time per step vs UCB's O(K) computation. (5) \textbf{Prior Knowledge}: If approximate gaps are known, can set max_ex to be sufficient with high confidence.

\section{Task 5: Hierarchical Reinforcement Learning}

\subsection{Key Idea}

Learn policies at multiple time scales, enabling efficient exploration and transfer of skills.

\subsection{Options Framework}

An option is defined as:
\begin{equation}
\text{Option} = (\text{Initiation Set}, \text{Policy}, \text{Termination Condition})
\end{equation}

\textbf{Example}: "Navigate to door" is an option composed of:
\begin{itemize}
\item Low-level actions (move forward, turn)
\item Termination: reached door
\end{itemize}

\subsection{Feudal Networks}

\begin{itemize}
\item \textbf{Manager}: Sets goals for Worker
\item \textbf{Worker}: Achieves goals set by Manager
\item \textbf{Manager reward}: extrinsic environment reward
\item \textbf{Worker reward}: intrinsic reward for achieving sub-goals
\end{itemize}

\subsection{Implementation}

\begin{verbatim}
class HierarchicalAgent:
    def __init__(self, state_dim, action_dim, num_options):
        self.manager = Manager(state_dim, num_options)
        self.worker = Worker(state_dim, action_dim)
        self.current_option = None
        self.option_steps = 0
        
    def get_action(self, state):
        if self.current_option is None or self.should_terminate():
            # Manager selects new option
            self.current_option = self.manager.select_option(state)
            self.option_steps = 0
            
        # Worker executes option
        action = self.worker.get_action(state, self.current_option)
        self.option_steps += 1
        
        return action
        
    def should_terminate(self):
        # Termination condition
        return self.option_steps >= self.max_option_steps
\end{verbatim}

\subsection{Questions and Answers}

\textbf{Q: Why does a high ε value result in lower immediate rewards?}

\textbf{A:} A high ε value results in lower immediate rewards because the agent frequently chooses random actions instead of exploiting the best-known arm. Even when the agent has identified a good arm, it continues to explore randomly with probability ε, missing opportunities to earn higher rewards from the optimal arm. The expected per-step reward becomes: E[R] = (1-ε) × R_best + ε × R_random, where R_random is typically much lower than R_best.

\textbf{Q: What benefits might there be in decaying ε over time?}

\textbf{A:} Decaying ε over time provides several benefits: (1) \textbf{Early exploration}: High ε initially allows thorough exploration of all arms. (2) \textbf{Gradual exploitation}: As ε decreases, the agent increasingly exploits the best-known arm. (3) \textbf{Adaptive balance}: The algorithm automatically transitions from exploration-heavy to exploitation-heavy behavior. (4) \textbf{Better convergence}: This approach often leads to faster convergence to optimal performance compared to fixed ε values. With ε_t = 1/t, the algorithm can achieve O(log T) regret, matching UCB's theoretical performance.

\section{Task 6: Advanced Exploration Strategies}

\subsection{Problem}

Standard exploration methods (ε-greedy, UCB) may be insufficient for complex environments with sparse rewards or high-dimensional state spaces.

\subsection{Exploration Methods}

\subsubsection{Count-Based Exploration}

Use visitation counts to encourage exploration of rarely visited states:

\begin{equation}
r_{explore}(s) = \frac{\beta}{\sqrt{N(s)}}
\end{equation}

where $N(s)$ is the number of times state $s$ has been visited.

\subsubsection{Disagreement-Based Exploration}

Use ensemble of models and explore states where models disagree:

\begin{equation}
r_{explore}(s) = \text{Var}(\{\hat{f}_i(s)\}_{i=1}^K)
\end{equation}

where $\hat{f}_i$ are different forward models.

\subsubsection{Information Gain}

Explore states that maximize information gain about the environment:

\begin{equation}
r_{explore}(s) = H(\theta) - H(\theta | s, a)
\end{equation}

where $H(\theta)$ is the entropy of model parameters.

\subsection{Implementation}

\begin{verbatim}
class AdvancedExploration:
    def __init__(self, state_dim, method='count'):
        self.method = method
        self.state_counts = {}
        self.models = [ForwardModel(state_dim) for _ in range(5)]
        
    def get_exploration_bonus(self, state):
        if self.method == 'count':
            count = self.state_counts.get(tuple(state), 0)
            return self.beta / np.sqrt(count + 1)
            
        elif self.method == 'disagreement':
            predictions = [model.predict(state) for model in self.models]
            return np.var(predictions)
            
        elif self.method == 'information_gain':
            # Simplified information gain
            uncertainty = self.compute_uncertainty(state)
            return uncertainty
            
    def update(self, state, action, next_state):
        # Update state counts
        state_key = tuple(state)
        self.state_counts[state_key] = self.state_counts.get(state_key, 0) + 1
        
        # Update models
        for model in self.models:
            model.update(state, action, next_state)
\end{verbatim}

\subsection{Questions and Answers}

\textbf{Q: How does LinUCB leverage context to outperform classical bandit algorithms?}

\textbf{A:} LinUCB leverages context by learning a linear relationship between context features and arm rewards. Instead of treating each arm independently, it uses the context to make more informed decisions. This allows the algorithm to generalize across similar contexts and make better predictions about which arm to choose for a given context, leading to more efficient exploration and better long-term performance. The key advantage is sample efficiency: classical bandits need O(K log T) samples to distinguish K arms, while LinUCB needs O(d log T) samples where d is the context dimension.

\textbf{Q: What is the role of the α parameter in LinUCB, and how does it affect the exploration bonus?}

\textbf{A:} The α parameter controls the exploration-exploitation trade-off in LinUCB. It determines how much uncertainty to add to the reward estimates when computing the upper confidence bound. Higher α values lead to more aggressive exploration (larger bonus), while lower α values favor exploitation (smaller bonus). The parameter balances the trade-off between exploring uncertain arms and exploiting arms with high estimated rewards. The exploration bonus is α√(x^T A^{-1} x), where the uncertainty term depends on the confidence ellipsoid in the direction of the context vector.

\section{Task 7: Comprehensive Analysis and Comparison}

\subsection{Algorithm Comparison}

\begin{table}[h]
\centering
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{Method} & \textbf{Regret} & \textbf{Assumptions} & \textbf{Pros} & \textbf{Cons} \\
\hline
Curiosity (ICM) & O(T) & Forward model & Sparse rewards & Model complexity \\
RND & O(T) & None & Simple, efficient & No dynamics \\
Multi-Task & O(log T) & Related tasks & Transfer learning & Task similarity \\
Meta-Learning & O(log T) & Task distribution & Fast adaptation & Complex training \\
Hierarchical & O(T^(2/3)) & Option structure & Skill reuse & Design complexity \\
Advanced Exploration & O(log T) & State visitation & Better exploration & Computational cost \\
\hline
\end{tabular}
\caption{Comparison of Advanced RL Methods}
\end{table}

\subsection{Performance Analysis}

\subsubsection{Sample Efficiency}

\begin{itemize}
\item \textbf{Curiosity-driven methods}: Excellent for sparse reward environments
\item \textbf{Multi-task learning}: Leverages shared representations across tasks
\item \textbf{Meta-learning}: Fast adaptation to new tasks with few samples
\item \textbf{Hierarchical RL}: Efficient exploration through skill reuse
\end{itemize}

\subsubsection{Computational Complexity}

\begin{itemize}
\item \textbf{ICM}: O(d²) per step (forward model training)
\item \textbf{RND}: O(d) per step (simple prediction)
\item \textbf{MAML}: O(K × d²) per meta-update (K inner steps)
\item \textbf{Hierarchical}: O(log K) per step (option selection)
\end{itemize}

\subsection{Questions and Answers}

\textbf{Q: Under what conditions might an explore-first strategy outperform UCB?}

\textbf{A:} Explore-first strategies can outperform UCB in several scenarios: (1) \textbf{Finite time horizons}: UCB's theoretical optimality is asymptotic, but in finite time, aggressive early exploitation can yield higher cumulative rewards. (2) \textbf{Clear arm separation}: When there's a significant gap between the best and second-best arms, explore-first can quickly identify and exploit the optimal arm. (3) \textbf{Short exploration periods}: If the optimal exploration duration is known and short, explore-first avoids UCB's conservative exploration bonus. (4) \textbf{High variance environments}: UCB's confidence intervals may be too conservative when rewards are highly variable, causing unnecessary exploration.

\textbf{Q: How do design choices affect short-term vs. long-term performance?}

\textbf{A:} \textbf{Short-term performance}: Explore-first strategies can excel by quickly identifying good arms and exploiting them aggressively. UCB's conservative exploration bonus may delay exploitation, leading to lower initial rewards. \textbf{Long-term performance}: UCB's theoretical guarantees ensure optimal asymptotic performance, while explore-first may plateau at suboptimal levels if exploration was insufficient. The key trade-off is between aggressive early exploitation (better short-term) versus conservative exploration (better long-term).

\textbf{Q: Impact of extending the exploration phase (e.g., 20 vs. 5 steps)}

\textbf{A:} Increasing the exploration phase from 5 to 20 steps has several impacts: (1) \textbf{Better arm identification}: With 20 steps, the agent has more opportunities to sample each arm and build more accurate estimates of their true reward probabilities. (2) \textbf{Reduced risk of premature commitment}: The agent is less likely to commit to a suboptimal arm based on limited data. (3) \textbf{Delayed exploitation}: The agent starts exploiting the best arm later, which can hurt short-term performance. (4) \textbf{Improved long-term performance}: Better arm identification typically leads to higher rewards once exploitation begins.

\textbf{Q: Discussion on why ExpFstAg might sometimes outperform UCB in practice}

\textbf{A:} ExpFstAg can outperform UCB in practice despite UCB's theoretical optimality for several reasons: (1) \textbf{Finite time horizons}: UCB's optimality is asymptotic, but in finite time, aggressive early exploitation can yield higher cumulative rewards. (2) \textbf{Hyperparameter sensitivity}: UCB's exploration bonus may be too conservative for the specific problem, while a well-tuned ExpFstAg can find the optimal exploration duration. (3) \textbf{Early exploitation advantage}: ExpFstAg can quickly identify and exploit the best arm, while UCB continues exploring conservatively. (4) \textbf{Problem-specific optimization}: ExpFstAg can be tuned for specific problem characteristics (arm separation, variance, time horizon).

\section{Task 8: Deep-Dive Theoretical Questions}

\subsection{Finite-Horizon Regret and Asymptotic Guarantees}

\textbf{Question}: Many algorithms (e.g., UCB) are analyzed using asymptotic (long-term) regret bounds. In a finite-horizon scenario (say, 500–1000 steps), explain intuitively why an algorithm that is asymptotically optimal may still yield poor performance. What trade-offs arise between aggressive early exploration and cautious long-term learning?

\textbf{Answer}: Asymptotic optimality guarantees the form of regret (e.g., O(log T)) but says nothing about constants. This creates a fundamental tension in finite-horizon problems:

\textbf{1. Asymptotic vs. Finite-Time Regret}

The regret of UCB is bounded as:
\begin{equation}
R(T) \leq \sum_{i: \Delta_i > 0} \frac{8 \log T}{\Delta_i} + O(\Delta_i)
\end{equation}

where $\Delta_i = \mu^* - \mu_i$ is the gap for suboptimal arm $i$.

\textbf{Analysis}:
\begin{itemize}
\item For $T \to \infty$: The log T term dominates, regret is O(log T) ✓ optimal
\item For finite T=500: The constant 8 matters significantly!
  \begin{itemize}
  \item If $\Delta_i = 0.1$ (small gap), then $8 \log(500) / 0.01 = 8 \times 6.21 / 0.01 = 4968$
  \item Need ~5000 samples to be confident, but T=500 total!
  \item UCB will continue exploring, incurring regret
  \end{itemize}
\end{itemize}

\textbf{2. Exploration Conservatism}

UCB's exploration bonus: $\sqrt{2 \log t / n_a}$

At t=500, $n_a=50$:
\begin{equation}
\text{bonus} = \sqrt{2 \times 6.21 / 50} \approx 0.498
\end{equation}

This is huge! Almost 50\% of the reward scale. This bonus is designed for worst-case guarantees:
\begin{itemize}
\item Must work for arbitrary (unknown) gaps $\Delta_i$
\item Must work for arbitrary (unknown) horizon T
\item Result: Over-explores in "easy" problems with large gaps
\end{itemize}

\textbf{3. Trade-off: Aggressive vs. Cautious}

\textbf{Aggressive Exploration} (e.g., Explore-First with max_ex=20):
\begin{itemize}
\item Pros: Quick identification of best arm (if gaps are large), Fast transition to exploitation, High finite-time reward
\item Cons: No recovery from mistakes, Fails if exploration insufficient (small gaps, unlucky samples), Linear or O(T^(2/3)) regret asymptotically
\end{itemize}

\textbf{Cautious Exploration} (e.g., UCB):
\begin{itemize}
\item Pros: Provable O(log T) regret, Works for any gap $\Delta$ (instance-optimal), Recovers from unlucky samples
\item Cons: Slow finite-time convergence, Over-explores when gaps are large, High constants in regret bound
\end{itemize}

\subsection{Hyperparameter Sensitivity and Exploration-Exploitation Balance}

\textbf{Question}: Consider the impact of hyperparameters such as ε in ε-greedy, the exploration constant in UCB, and the α parameter in LinUCB. Explain intuitively how slight mismatches in these parameters can lead to either under-exploration (missing the best arm) or over-exploration (wasting pulls on suboptimal arms). How would you design a self-adaptive mechanism to balance this trade-off in practice?

\textbf{Answer}: Hyperparameter sensitivity is one of the most critical practical challenges in bandit algorithms:

\textbf{1. Epsilon (ε) in ε-Greedy}

Fixed ε regret: $R(T) \approx \varepsilon \times T \times \Delta + O(K \log T / \varepsilon)$

Optimal ε minimizes regret:
\begin{equation}
\frac{d}{d\varepsilon} [\varepsilon T \Delta + K \log T / \varepsilon] = 0
\end{equation}
\begin{equation}
T \Delta - K \log T / \varepsilon^2 = 0
\end{equation}
\begin{equation}
\varepsilon^* = \sqrt{\frac{K \log T}{T \Delta}}
\end{equation}

\textbf{Effects of Mismatch}:
\begin{itemize}
\item \textbf{ε too small} (e.g., ε=0.01 when ε*=0.1):
  \begin{itemize}
  \item Under-exploration: May miss best arm entirely
  \item Probability of not finding best arm in T steps: $P(\text{miss}) \approx \exp(-\varepsilon T / K) = \exp(-0.01 \times 500 / 10) = \exp(-0.5) \approx 0.61$
  \item 61\% chance of missing best arm!
  \end{itemize}
\item \textbf{ε too large} (e.g., ε=0.5 when ε*=0.1):
  \begin{itemize}
  \item Over-exploration: Wastes 50\% of samples on random actions
  \item Regret: $R(T) \approx 0.5 \times 500 \times 0.4 = 100$ (vs. optimal ≈ 30)
  \item 3× worse than optimal!
  \end{itemize}
\end{itemize}

\textbf{2. Self-Adaptive Mechanisms}

\textbf{Approach 1: Reward-Based Adaptation}

\begin{verbatim}
class AdaptiveEpsGreedy:
    def __init__(self, n_act, eps_init=0.1, window=100):
        self.n_act = n_act
        self.eps = eps_init
        self.reward_history = []
        self.window = window

    def update_eps(self, reward):
        self.reward_history.append(reward)

        if len(self.reward_history) > self.window:
            # Compute recent vs. historical performance
            recent_mean = np.mean(self.reward_history[-self.window:])
            historical_mean = np.mean(self.reward_history)

            # If recent improving → decrease exploration
            if recent_mean > historical_mean:
                self.eps *= 0.95  # decay
            # If recent declining → increase exploration
            else:
                self.eps *= 1.05  # grow

            # Clip to reasonable range
            self.eps = np.clip(self.eps, 0.01, 0.5)
\end{verbatim}

\textbf{Approach 2: Variance-Based Adaptation}

\begin{verbatim}
class VarianceAdaptiveUCB:
    def compute_bonus(self, a):
        # Empirical variance of arm a
        var_a = np.var(self.reward_history[a])

        # Adaptive exploration constant
        c_adaptive = max(0.5, min(5.0, var_a * 10))

        # UCB bonus
        bonus = np.sqrt(c_adaptive * np.log(self.t) / (self.act_counts[a] + 1e-5))

        return bonus
\end{verbatim}

\subsection{Context Incorporation and Overfitting in LinUCB}

\textbf{Question}: LinUCB uses context features to estimate arm rewards, assuming a linear relation. Intuitively, why might this linear assumption hurt performance when the true relationship is complex or when the context is high-dimensional and noisy? Under what conditions can adding context lead to worse performance than classical (context-free) UCB?

\textbf{Answer}: Context incorporation in LinUCB is a double-edged sword: it can dramatically improve performance when used correctly, but hurt when misapplied.

\textbf{1. The Linear Assumption}

LinUCB assumes: $E[r | x, a] = \theta_a^T x$

This is a strong assumption that rarely holds exactly in practice.

\textbf{When Linear Assumption Fails}:

\textbf{Example 1: Non-linear Relationships}
\begin{equation}
\text{True reward: } r = \sin(\theta_a^T x) + \text{noise}
\end{equation}
\begin{equation}
\text{LinUCB prediction: } \hat{r} = \theta_a^T x
\end{equation}
\begin{equation}
\text{Error: } |\sin(\theta_a^T x) - \theta_a^T x| \text{ can be large!}
\end{equation}

\textbf{Example 2: Interaction Effects}
\begin{equation}
\text{True reward: } r = x_1 \times x_2 \text{ (interaction between features)}
\end{equation}
\begin{equation}
\text{LinUCB: } \hat{r} = \theta_1 x_1 + \theta_2 x_2 \text{ (no interaction term)}
\end{equation}

Can never model multiplicative interactions!

\textbf{2. Curse of Dimensionality}

LinUCB complexity scales with feature dimension d.

\textbf{Sample Complexity}: Need ~O(d log T) samples to learn $\theta_a$ accurately.

\textbf{Problem}: If d is large (say, d > 100), requires many samples:
\begin{itemize}
\item d=10: Need ~100 samples per arm
\item d=100: Need ~1000 samples per arm
\item d=1000: Need ~10000 samples per arm!
\end{itemize}

But we might only have T=10000 total samples for K=10 arms.

\textbf{3. When Context Hurts Performance}

LinUCB can be worse than context-free UCB when:

\textbf{Condition 1: Uninformative Features}

If context features are uncorrelated with rewards:
\begin{itemize}
\item LinUCB wastes capacity learning meaningless $\theta_a$
\item UCB directly estimates mean rewards (more efficient)
\end{itemize}

\textbf{Condition 2: High Dimensionality}

When d > T / K:
\begin{itemize}
\item Not enough samples per arm to learn d parameters
\item LinUCB estimates unreliable
\item UCB estimates reliable (only K parameters total)
\end{itemize}

\textbf{Condition 3: Model Misspecification}

When true reward is non-linear:
\begin{itemize}
\item LinUCB's linear approximation is systematically wrong
\item Errors compound over time
\item UCB's non-parametric approach is more robust
\end{itemize}

\textbf{4. Mitigation Strategies}

\textbf{Strategy 1: Regularization}

Add L2 penalty:
\begin{equation}
\theta_a = \arg\min_\theta \|X_a \theta - r_a\|^2 + \lambda\|\theta\|^2
\end{equation}

This is already done in LinUCB (A_a = X^T X + λI with λ=1), but may need stronger regularization for noisy features.

\textbf{Strategy 2: Feature Selection}

Remove uninformative features:
\begin{verbatim}
def select_features(X, y, k=20):
    # Compute mutual information
    mi_scores = mutual_info_regression(X, y)

    # Select top-k features
    top_k_indices = np.argsort(mi_scores)[-k:]

    return X[:, top_k_indices], top_k_indices
\end{verbatim}

\textbf{Strategy 3: Dimensionality Reduction}

PCA or autoencoders:
\begin{verbatim}
from sklearn.decomposition import PCA

pca = PCA(n_components=20)
X_reduced = pca.fit_transform(X)  # 100 → 20 dimensions
\end{verbatim}

\subsection{Adaptive Strategy Selection}

\textbf{Question}: Imagine designing a hybrid bandit agent that can switch between an explore-first strategy and UCB based on observed performance. What signals (e.g., variance of reward estimates, stabilization of Q-values, or sudden drops in reward) might indicate that a switch is warranted? Provide an intuitive justification for how and why such a meta-strategy might outperform either strategy alone in a finite-time setting.

\textbf{Answer}: A hybrid agent can leverage the strengths of both strategies while mitigating their weaknesses.

\textbf{1. Switching Signals}

\textbf{Signal 1: Reward Variance Stabilization}
\begin{itemize}
\item High variance → Need more exploration → Use UCB
\item Low variance → Can exploit → Use explore-first
\item Threshold: $\text{Var}(R_t) < \sigma_{threshold}$
\end{itemize}

\textbf{Signal 2: Q-value Convergence}
\begin{itemize}
\item Q-values changing rapidly → Still learning → Use UCB
\item Q-values stable → Ready to exploit → Use explore-first
\item Measure: $\|\Delta Q_t\| < \epsilon_{convergence}$
\end{itemize}

\textbf{Signal 3: Performance Plateau}
\begin{itemize}
\item Reward increasing → Strategy working → Continue current
\item Reward plateauing → Switch strategies
\item Measure: $\frac{dR}{dt} < \epsilon_{plateau}$
\end{itemize}

\textbf{Signal 4: Confidence in Best Arm}
\begin{itemize}
\item High confidence → Use explore-first
\item Low confidence → Use UCB
\item Measure: $\text{Confidence} = \frac{\max(Q) - \text{second\_max}(Q)}{\text{std}(Q)}$
\end{itemize}

\textbf{2. Meta-Strategy Implementation}

\begin{verbatim}
class HybridBandit:
    def __init__(self, n_act):
        self.n_act = n_act
        self.strategy = 'ucb'  # Start with UCB
        self.ucb_agent = UCBAgent(n_act)
        self.expfst_agent = ExpFstAgent(n_act, max_ex=20)
        
        # Performance tracking
        self.reward_history = []
        self.q_history = []
        self.switch_threshold = 0.1
        
    def get_action(self):
        # Monitor performance signals
        if self.should_switch_strategy():
            self.switch_strategy()
            
        # Use current strategy
        if self.strategy == 'ucb':
            return self.ucb_agent.get_action()
        else:
            return self.expfst_agent.get_action()
            
    def should_switch_strategy(self):
        if len(self.reward_history) < 50:
            return False
            
        # Signal 1: Reward variance
        recent_var = np.var(self.reward_history[-20:])
        if recent_var < self.switch_threshold and self.strategy == 'ucb':
            return True
            
        # Signal 2: Q-value convergence
        if len(self.q_history) > 10:
            q_change = np.mean(np.abs(np.diff(self.q_history[-10:])))
            if q_change < 0.01 and self.strategy == 'ucb':
                return True
                
        return False
        
    def switch_strategy(self):
        if self.strategy == 'ucb':
            # Transfer Q-values to explore-first
            self.expfst_agent.Q = self.ucb_agent.Q.copy()
            self.strategy = 'expfst'
        else:
            self.strategy = 'ucb'
\end{verbatim}

\textbf{3. Why Meta-Strategy Outperforms}

\textbf{Finite-Time Advantages}:
\begin{itemize}
\item \textbf{Early exploration}: UCB ensures thorough initial exploration
\item \textbf{Fast exploitation}: Switch to explore-first when confident
\item \textbf{Adaptive}: Responds to problem characteristics
\item \textbf{Robust}: Falls back to UCB if explore-first fails
\end{itemize}

\textbf{Theoretical Justification}:
\begin{itemize}
\item UCB provides worst-case guarantees
\item Explore-first provides best-case performance
\item Meta-strategy achieves: $\min(\text{UCB\_regret}, \text{ExpFst\_regret})$
\item In practice: Often achieves near-optimal performance
\end{itemize}

\textbf{4. Practical Considerations}

\textbf{Switching Frequency}:
\begin{itemize}
\item Too frequent → Instability, no convergence
\item Too rare → Miss opportunities
\item Recommendation: Switch at most once per 100 steps
\end{itemize}

\textbf{Transfer Learning}:
\begin{itemize}
\item Transfer Q-values between strategies
\item Maintain exploration counts
\item Preserve learned confidence bounds
\end{itemize}

\textbf{Performance Monitoring}:
\begin{itemize}
\item Track cumulative regret
\item Monitor exploration efficiency
\item Detect strategy failures early
\end{itemize}

This hybrid approach demonstrates how combining multiple strategies can achieve better practical performance than any single method alone, especially in finite-time scenarios where theoretical guarantees may not be sufficient.

}}

\end{document}