\section{Policy Gradient Theorem}

In this section, we present a comprehensive analysis of the Policy Gradient Theorem, which forms the theoretical foundation for policy-based reinforcement learning algorithms. The Policy Gradient Theorem provides a direct method for computing gradients of the expected return with respect to policy parameters, enabling gradient-based optimization of stochastic policies in continuous action spaces.

\subsection{Mathematical Framework and Notations}

Consider a finite Markov Decision Process (MDP) defined by the tuple $(\mathcal{S}, \mathcal{A}, P, r, \gamma)$, where:
\begin{itemize}
    \item $\mathcal{S}$ is the finite state space
    \item $\mathcal{A}$ is the finite action space  
    \item $P(s'|s, a)$ is the transition probability function representing the probability of transitioning from state $s$ to state $s'$ when taking action $a$
    \item $r: \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ is the bounded reward function, where $r(s, a)$ denotes the immediate reward for taking action $a$ in state $s$
    \item $\gamma \in [0, 1)$ is the discount factor that determines the importance of future rewards
    \item $s_0 \in \mathcal{S}$ is the initial state distribution
\end{itemize}

A parametrized stochastic policy $\pi_{\theta}: \mathcal{S} \times \mathcal{A} \rightarrow [0, 1]$ maps states to probability distributions over actions, where $\theta \in \mathbb{R}^d$ represents the policy parameters. This policy induces a distribution over trajectories $\tau = (s_t, a_t, r_t)_{t=0}^{\infty}$, where:
\begin{itemize}
    \item $s_0$ is sampled from the initial state distribution
    \item For each timestep $t \geq 0$: $a_t \sim \pi_{\theta}(\cdot|s_t)$ and $s_{t+1} \sim P(\cdot|s_t, a_t)$
\end{itemize}

The state value function $V^{\pi_\theta}: \mathcal{S} \rightarrow \mathbb{R}$ and state-action value function $Q^{\pi_\theta}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ are defined through the Bellman equations:
\begin{align}
    V^{\pi_\theta}(s) &= \mathbb{E}_{a\sim \pi_\theta(\cdot|s)} [Q^{\pi_\theta}(s,a)] \label{eq:value_function}\\
    Q^{\pi_\theta}(s,a) &= r(s, a) + \gamma\mathbb{E}_{s'\sim P(\cdot|s, a)} [V^{\pi_\theta}(s')] \label{eq:q_function}
\end{align}

The discounted state visitation distribution $d^{\pi}_{s_0}: \mathcal{S} \rightarrow [0, 1]$ represents the normalized probability of visiting each state when following policy $\pi$ starting from state $s_0$:
\begin{align}
    d^{\pi}_{s_0}(s) = (1-\gamma)\sum_{t = 0}^{\infty}\gamma^t \Pr^{\pi}(s_t = s|s_0) \label{eq:state_visitation}
\end{align}
where $\Pr^{\pi}(s_t = s|s_0)$ denotes the probability of being in state $s$ at time $t$ when following policy $\pi$ starting from state $s_0$. The factor $(1-\gamma)$ ensures that $d^{\pi}_{s_0}$ is a proper probability distribution, i.e., $\sum_{s \in \mathcal{S}} d^{\pi}_{s_0}(s) = 1$.

\subsection{Proving the Policy Gradient Theorem}

The objective function of our RL problem is defined as $J(\theta) = V^{\pi_\theta}(s_0)$. The policy gradient method uses the gradient ascent algorithm to optimize $\theta$. This can be done by the direct differentiation of the objective function.

a) Prove the following identity, which is known as the Policy Gradient Theorem:

\begin{align}\label{policy_grad}
    \nabla_\theta J(\theta) = \frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\theta \log \pi_\theta (a|s) Q^{\pi_\theta} (s, a)]
\end{align}

\textbf{Solution:}

We will prove the Policy Gradient Theorem by directly differentiating the objective function $J(\theta) = V^{\pi_\theta}(s_0)$.

\textbf{Step 1: Express the objective in terms of trajectories}

The value function can be written as:
\begin{align}
J(\theta) = V^{\pi_\theta}(s_0) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right]
\end{align}

where $\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \ldots)$ is a trajectory sampled from policy $\pi_\theta$.

\textbf{Step 2: Differentiate with respect to policy parameters}

Taking the gradient:
\begin{align}
\nabla_\theta J(\theta) &= \nabla_\theta \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right] \\
&= \sum_{\tau} P(\tau|\theta) \nabla_\theta \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right] + \sum_{\tau} \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right] \nabla_\theta P(\tau|\theta)
\end{align}

The first term is zero since rewards don't depend on $\theta$. For the second term:
\begin{align}
\nabla_\theta P(\tau|\theta) &= \nabla_\theta \left[ \prod_{t=0}^{\infty} \pi_\theta(a_t|s_t) P(s_{t+1}|s_t, a_t) \right] \\
&= P(\tau|\theta) \nabla_\theta \left[ \sum_{t=0}^{\infty} \log \pi_\theta(a_t|s_t) \right] \\
&= P(\tau|\theta) \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t)
\end{align}

\textbf{Step 3: Combine and rearrange}

Substituting back:
\begin{align}
\nabla_\theta J(\theta) &= \sum_{\tau} P(\tau|\theta) \left[ \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right] \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \right] \\
&= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \left( \sum_{t=0}^{\infty} \gamma^t r(s_t, a_t) \right) \left( \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \right) \right]
\end{align}

\textbf{Step 4: Use the log-derivative trick}

We can rewrite this as:
\begin{align}
\nabla_\theta J(\theta) &= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \left( \sum_{k=t}^{\infty} \gamma^k r(s_k, a_k) \right) \right] \\
&= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \gamma^t \left( \sum_{k=0}^{\infty} \gamma^k r(s_{t+k}, a_{t+k}) \right) \right] \\
&= \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^{\infty} \nabla_\theta \log \pi_\theta(a_t|s_t) \gamma^t Q^{\pi_\theta}(s_t, a_t) \right]
\end{align}

\textbf{Step 5: Convert to state-action expectation}

\begin{align}
\nabla_\theta J(\theta) &= \sum_{t=0}^{\infty} \gamma^t \mathbb{E}_{s_t \sim d^{\pi_\theta}_{s_0,t}} \mathbb{E}_{a_t \sim \pi_\theta(.|s_t)} \left[ \nabla_\theta \log \pi_\theta(a_t|s_t) Q^{\pi_\theta}(s_t, a_t) \right]
\end{align}

where $d^{\pi_\theta}_{s_0,t}(s) = P(s_t = s | s_0, \pi_\theta)$ is the state visitation probability at time $t$.

\textbf{Step 6: Use discounted state visitation distribution}

The discounted state visitation distribution is:
\begin{align}
d^{\pi_\theta}_{s_0}(s) = (1-\gamma) \sum_{t=0}^{\infty} \gamma^t d^{\pi_\theta}_{s_0,t}(s)
\end{align}

Therefore:
\begin{align}
\sum_{t=0}^{\infty} \gamma^t d^{\pi_\theta}_{s_0,t}(s) = \frac{d^{\pi_\theta}_{s_0}(s)}{1-\gamma}
\end{align}

\textbf{Step 7: Final result}

Substituting:
\begin{align}
\nabla_\theta J(\theta) &= \sum_{t=0}^{\infty} \gamma^t \sum_s d^{\pi_\theta}_{s_0,t}(s) \sum_a \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s, a) \\
&= \sum_s \left( \sum_{t=0}^{\infty} \gamma^t d^{\pi_\theta}_{s_0,t}(s) \right) \sum_a \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s, a) \\
&= \sum_s \frac{d^{\pi_\theta}_{s_0}(s)}{1-\gamma} \sum_a \pi_\theta(a|s) \nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s, a) \\
&= \frac{1}{1-\gamma} \mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}} \mathbb{E}_{a \sim \pi_\theta(.|s)} \left[ \nabla_\theta \log \pi_\theta(a|s) Q^{\pi_\theta}(s, a) \right]
\end{align}

This completes the proof of the Policy Gradient Theorem. $\square$


\subsection{Compatible Function Approximation Theorem}

Now, consider the case in which $Q^{\pi_\theta}$ is approximated by a learned function approximator. If the approximation is sufficiently good, we might hope to use it in place of $Q^{\pi_\theta}$ in equation \ref{policy_grad}. If we use the function approximator $Q_{\phi}(s, a)$, the convergence of our method is not necessarily maintained due to the fact that our gradient will not be exact anymore. The following theorem provides sufficient conditions for our function approximator so that our gradient using the approximator remains exact.

\begin{theorem}\label{thm:compatible_approximator}
    (Compatible Function Approximation). If the following two conditions are satisfied for any function approximator with parameter $\phi$:
    \begin{enumerate}
        \item Critic gradient is compatible with the Actor score function, i.e., 
        \begin{align*}
            \nabla_{\phi} Q_\phi(s, a) = \nabla_\theta \log \pi_\theta (a|s)
        \end{align*}
        \item Critic parameters $\phi$ minimize the following mean-squared error\footnote{Assume that the mean-squared error has only one critical point which corresponds to its minimum.}: 
        \begin{align*}
            \epsilon = \mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [(Q^{\pi_\theta} (s, a) - Q_\phi(s, a))^2]
        \end{align*}
    \end{enumerate}
    Then, the policy gradient using critic $Q_\phi(s, a)$ is exact, i.e., 
    \begin{align*}
            \nabla_\theta J(\theta) = \frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\theta \log \pi_\theta (a|s) Q_\phi (s, a)]
    \end{align*}
\end{theorem}

b) Prove theorem \ref{thm:compatible_approximator}.

\textbf{Solution:}

We need to prove that under the two conditions stated in the theorem, the policy gradient using the function approximator $Q_\phi(s,a)$ is exact.

\textbf{Given conditions:}
\begin{enumerate}
    \item $\nabla_{\phi} Q_\phi(s, a) = \nabla_\theta \log \pi_\theta (a|s)$ (compatibility condition)
    \item $\phi$ minimizes $\epsilon = \mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [(Q^{\pi_\theta} (s, a) - Q_\phi(s, a))^2]$
\end{enumerate}

\textbf{Goal:} Show that
\begin{align}
\nabla_\theta J(\theta) = \frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\theta \log \pi_\theta (a|s) Q_\phi (s, a)]
\end{align}

\textbf{Proof:}

From the Policy Gradient Theorem (part a), we have:
\begin{align}
\nabla_\theta J(\theta) = \frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\theta \log \pi_\theta (a|s) Q^{\pi_\theta} (s, a)]
\end{align}

We want to show that we can replace $Q^{\pi_\theta}(s,a)$ with $Q_\phi(s,a)$ without changing the gradient.

\textbf{Step 1: Express the difference}

Let $\Delta(s,a) = Q^{\pi_\theta}(s,a) - Q_\phi(s,a)$ be the approximation error. Then:
\begin{align}
\nabla_\theta J(\theta) &= \frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\theta \log \pi_\theta (a|s) (Q_\phi(s,a) + \Delta(s,a))] \\
&= \frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\theta \log \pi_\theta (a|s) Q_\phi(s,a)] \\
&\quad + \frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\theta \log \pi_\theta (a|s) \Delta(s,a)]
\end{align}

\textbf{Step 2: Show the error term is zero}

We need to show that:
\begin{align}
\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\theta \log \pi_\theta (a|s) \Delta(s,a)] = 0
\end{align}

\textbf{Step 3: Use the compatibility condition}

From condition 1, we have $\nabla_{\phi} Q_\phi(s, a) = \nabla_\theta \log \pi_\theta (a|s)$.

Since $\phi$ minimizes the MSE (condition 2), we have:
\begin{align}
\nabla_\phi \epsilon = 0
\end{align}

Computing the gradient:
\begin{align}
\nabla_\phi \epsilon &= \nabla_\phi \mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [(Q^{\pi_\theta} (s, a) - Q_\phi(s, a))^2] \\
&= \mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\phi (Q^{\pi_\theta} (s, a) - Q_\phi(s, a))^2] \\
&= \mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [2(Q^{\pi_\theta} (s, a) - Q_\phi(s, a)) \nabla_\phi Q_\phi(s, a)] \\
&= 2\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\Delta(s,a) \nabla_\phi Q_\phi(s, a)]
\end{align}

Since $\nabla_\phi \epsilon = 0$ and $\nabla_\phi Q_\phi(s, a) = \nabla_\theta \log \pi_\theta (a|s)$ (compatibility condition), we have:
\begin{align}
\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\Delta(s,a) \nabla_\theta \log \pi_\theta (a|s)] = 0
\end{align}

\textbf{Step 4: Conclusion}

Therefore, the error term in Step 1 is zero, and we have:
\begin{align}
\nabla_\theta J(\theta) = \frac{1}{1-\gamma}\mathbb{E}_{s \sim d^{\pi_\theta}_{s_0}}\mathbb{E}_{a \sim \pi_\theta(.|s)} [\nabla_\theta \log \pi_\theta (a|s) Q_\phi (s, a)]
\end{align}

This proves that the policy gradient using the function approximator $Q_\phi(s,a)$ is exact under the stated conditions. $\square$

\textbf{Interpretation:}

The compatibility condition ensures that the critic's gradient direction matches the actor's score function direction. The MSE minimization condition ensures that the critic provides unbiased estimates. Together, these conditions guarantee that the approximate policy gradient equals the true policy gradient.