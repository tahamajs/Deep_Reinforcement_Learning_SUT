\section{Trust Region Policy Optimization}

\subsection{Introduction and Motivation}

Trust Region Policy Optimization (TRPO) represents a significant advancement in policy gradient methods by addressing the fundamental challenge of ensuring monotonic policy improvement. Traditional policy gradient methods often suffer from instability and can lead to performance degradation during training, especially when taking large policy updates.

TRPO, introduced by Schulman et al. \cite{schulman2015trust}, provides a theoretically grounded approach to policy optimization that guarantees non-decreasing performance under certain conditions. The algorithm is based on the principle of maintaining a "trust region" around the current policy, ensuring that updates remain within a safe range where theoretical guarantees hold.

\textbf{Key Challenges Addressed by TRPO:}
\begin{itemize}
    \item \textbf{Policy Instability}: Large policy updates can lead to performance collapse
    \item \textbf{Sample Inefficiency}: Traditional policy gradients often require many samples
    \item \textbf{Step Size Selection}: Choosing appropriate learning rates is difficult
    \item \textbf{Monotonic Improvement}: Ensuring each update improves performance
\end{itemize}

\textbf{TRPO's Solution:}
\begin{itemize}
    \item \textbf{Trust Region Constraint}: Limits policy updates to maintain theoretical guarantees
    \item \textbf{Natural Policy Gradient}: Uses second-order information for more stable updates
    \item \textbf{Monotonic Improvement}: Provides theoretical guarantees of non-decreasing performance
    \item \textbf{Large Step Sizes}: Enables larger updates than traditional policy gradients
\end{itemize} 

\subsection{Mathematical Framework and Notations}

\subsubsection{Policy and Performance Definitions}

Let $\pi$ denote a stochastic policy that maps states to probability distributions over actions. The expected discounted reward (performance) of policy $\pi$ is defined as:

\begin{align*}
    \eta(\pi) = \mathbb{E}_{s_0, a_0, \ldots} \left[\sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t)\right]
\end{align*}

where the expectation is taken over trajectories generated by:
\begin{align*}
    s_0 \sim \rho_0(s_0), \quad a_t \sim \pi(a_t | s_t), \quad s_{t + 1} \sim P(s_{t + 1} | s_t, a_t)
\end{align*}

\textbf{Key Assumptions:}
\begin{itemize}
    \item The reward function $r(s,a)$ is bounded: $r(s,a) \in [0, R_{\max}]$ for some $R_{\max} > 0$
    \item The discount factor satisfies $\gamma \in [0, 1)$
    \item The MDP is ergodic under any policy
    \item The initial state distribution $\rho_0$ is fixed
\end{itemize}

\subsubsection{Value Functions and Advantage Function}

We define the standard value functions and advantage function:

\begin{align*}
    Q_\pi(s_t, a_t) &= \mathbb{E}_{s_{t+1}, a_{t+1}, \ldots} \left[\sum_{l = 0}^{\infty} \gamma^l r(s_{t + l}, a_{t + l})\right] \\
    V_\pi(s_t) &= \mathbb{E}_{a_t, s_{t+1}, \ldots} \left[\sum_{l = 0}^{\infty} \gamma^l r(s_{t + l}, a_{t + l})\right] \\
    A_\pi(s, a) &= Q_\pi(s, a) - V_\pi(s)
\end{align*}

\textbf{Key Properties:}
\begin{itemize}
    \item \textbf{Boundedness}: $V_\pi(s) \leq \frac{R_{\max}}{1-\gamma}$ and $Q_\pi(s,a) \leq \frac{R_{\max}}{1-\gamma}$
    \item \textbf{Advantage Property}: $\mathbb{E}_{a \sim \pi(\cdot|s)}[A_\pi(s,a)] = 0$ for any state $s$
    \item \textbf{Monotonicity}: If $A_\pi(s,a) \geq 0$ for all $(s,a)$, then $\pi$ is optimal
\end{itemize}

\subsubsection{State Visitation Distribution}

The unnormalized discounted state visitation distribution is defined as:
\begin{align*}
    \rho_\pi(s) = \sum_{t=0}^{\infty} \gamma^t P(s_t = s | s_0 \sim \rho_0, \pi)
\end{align*}

This represents the total discounted probability of visiting state $s$ when following policy $\pi$.

\textbf{Key Properties:}
\begin{itemize}
    \item \textbf{Policy Dependence}: $\rho_\pi(s)$ depends on the policy $\pi$
    \item \textbf{Non-negative}: $\rho_\pi(s) \geq 0$ for all states $s$
    \item \textbf{Finite Sum}: Under ergodicity, $\sum_s \rho_\pi(s) < \infty$
\end{itemize}


\subsection{Fundamental Identity for Policy Improvement}

\subsubsection{Statement of the Fundamental Identity}

The foundation of TRPO lies in establishing a relationship between the performance of two policies $\pi$ and $\pi'$. The following identity is crucial for understanding how policy improvement can be achieved:

\begin{theorem}[Policy Performance Identity]
For any two policies $\pi$ and $\pi'$, the following identity holds:

\begin{align}\label{eq_3}
    \eta(\pi')=\eta(\pi)+\mathbb{E}_{s_{0}, a_{0}, \ldots \sim \pi'} \left[\sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t)\right]
\end{align}

where the expectation is taken over trajectories generated by policy $\pi'$.
\end{theorem}

\textbf{Intuition:}
This identity shows that the performance difference between two policies can be expressed entirely in terms of the advantage function of the current policy $\pi$, evaluated on trajectories generated by the new policy $\pi'$. This is remarkable because it allows us to evaluate the improvement of a new policy using only information from the current policy.

\textbf{Key Insights:}
\begin{itemize}
    \item \textbf{Advantage-Based Evaluation}: We can evaluate $\pi'$ using $A_\pi$ instead of needing $A_{\pi'}$
    \item \textbf{Trajectory Sampling}: The expectation is over trajectories from $\pi'$, not $\pi$
    \item \textbf{Policy Improvement}: If $A_\pi(s,a) \geq 0$ for all $(s,a)$ visited by $\pi'$, then $\eta(\pi') \geq \eta(\pi)$
\end{itemize}

\subsubsection{Proof of the Policy Performance Identity}

\textbf{Proof Strategy:}
We will prove this identity by expressing the performance difference in terms of the advantage function. The key insight is to use the fact that the advantage function has zero expectation under the current policy.

\textbf{Step 1: Express $\eta(\pi')$ in terms of trajectories}

The performance of policy $\pi'$ can be written as:
\begin{align}
\eta(\pi') &= \mathbb{E}_{s_0, a_0, \ldots \sim \pi'} \left[ \sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t) \right]
\end{align}

\textbf{Step 2: Add and subtract $\eta(\pi)$}

\begin{align}
\eta(\pi') &= \eta(\pi) + \eta(\pi') - \eta(\pi) \\
&= \eta(\pi) + \mathbb{E}_{s_0, a_0, \ldots \sim \pi'} \left[ \sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t) \right] - \mathbb{E}_{s_0, a_0, \ldots \sim \pi} \left[ \sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t) \right]
\end{align}

\textbf{Step 3: Use the definition of advantage function}

For any trajectory $(s_0, a_0, s_1, a_1, \ldots)$:
\begin{align}
\sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t) &= \sum_{t = 0}^{\infty} \gamma^t [Q_\pi(s_t, a_t) - \gamma V_\pi(s_{t+1})] \\
&= \sum_{t = 0}^{\infty} \gamma^t Q_\pi(s_t, a_t) - \sum_{t = 0}^{\infty} \gamma^{t+1} V_\pi(s_{t+1}) \\
&= \sum_{t = 0}^{\infty} \gamma^t Q_\pi(s_t, a_t) - \sum_{t = 1}^{\infty} \gamma^t V_\pi(s_t) \\
&= Q_\pi(s_0, a_0) + \sum_{t = 1}^{\infty} \gamma^t [Q_\pi(s_t, a_t) - V_\pi(s_t)] \\
&= V_\pi(s_0) + A_\pi(s_0, a_0) + \sum_{t = 1}^{\infty} \gamma^t A_\pi(s_t, a_t) \\
&= V_\pi(s_0) + \sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t)
\end{align}

\textbf{Step 4: Apply to both policies}

For policy $\pi'$:
\begin{align}
\mathbb{E}_{s_0, a_0, \ldots \sim \pi'} \left[ \sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t) \right] &= \mathbb{E}_{s_0 \sim \rho_0} [V_\pi(s_0)] + \mathbb{E}_{s_0, a_0, \ldots \sim \pi'} \left[ \sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right]
\end{align}

For policy $\pi$:
\begin{align}
\mathbb{E}_{s_0, a_0, \ldots \sim \pi} \left[ \sum_{t = 0}^{\infty} \gamma^t r(s_t, a_t) \right] &= \mathbb{E}_{s_0 \sim \rho_0} [V_\pi(s_0)] + \mathbb{E}_{s_0, a_0, \ldots \sim \pi} \left[ \sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right]
\end{align}

\textbf{Step 5: Use the fact that $\mathbb{E}_{a \sim \pi(.|s)}[A_\pi(s,a)] = 0$}

For any state $s$:
\begin{align}
\mathbb{E}_{a \sim \pi(.|s)}[A_\pi(s,a)] &= \mathbb{E}_{a \sim \pi(.|s)}[Q_\pi(s,a) - V_\pi(s)] \\
&= \mathbb{E}_{a \sim \pi(.|s)}[Q_\pi(s,a)] - V_\pi(s) \\
&= V_\pi(s) - V_\pi(s) = 0
\end{align}

Therefore:
\begin{align}
\mathbb{E}_{s_0, a_0, \ldots \sim \pi} \left[ \sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right] = 0
\end{align}

\textbf{Step 6: Final result}

Substituting back into Step 2:
\begin{align}
\eta(\pi') &= \eta(\pi) + \mathbb{E}_{s_0 \sim \rho_0} [V_\pi(s_0)] + \mathbb{E}_{s_0, a_0, \ldots \sim \pi'} \left[ \sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right] \\
&\quad - \mathbb{E}_{s_0 \sim \rho_0} [V_\pi(s_0)] - 0 \\
&= \eta(\pi) + \mathbb{E}_{s_0, a_0, \ldots \sim \pi'} \left[ \sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right]
\end{align}

\textbf{Key Insights from the Proof:}
\begin{itemize}
    \item The advantage function naturally appears when expressing cumulative rewards
    \item The zero-expectation property of the advantage function under the current policy is crucial
    \item The identity holds for any two policies, making it a powerful tool for policy improvement
\end{itemize}

\subsubsection{State Visitation Distribution and True Objective}

Equation \ref{eq_3} shows that the difference between the expected total rewards of any two policies $\pi'$ and $\pi$ depends on the advantage function of policy $\pi$ if the trajectory is sampled by running $\pi'$. We will use this equation to derive an optimization scheme to maximize the expected total reward using the advantage function of policy $\pi$ to obtain policy $\pi'$.

Let $\rho_\pi$ be the unnormalized discounted visitation frequencies:

\begin{align*}
    \rho_\pi(s) = \sum_{t=0}^{\infty} \gamma^t P(s_t = s | s_0 \sim \rho_0, \pi)
\end{align*}

\textbf{Key Properties:}
\begin{itemize}
    \item \textbf{Policy Dependence}: $\rho_\pi(s)$ depends on the policy $\pi$
    \item \textbf{Non-negative}: $\rho_\pi(s) \geq 0$ for all states $s$
    \item \textbf{Finite Sum}: Under ergodicity, $\sum_s \rho_\pi(s) < \infty$
\end{itemize}

\subsubsection{True Objective Function}

The next step is to express the policy improvement in terms of state visitation distributions. This leads to the following important identity:

\begin{theorem}[True Objective Identity]
For any two policies $\pi$ and $\pi'$, the following identity holds:

\begin{align}\label{true_objective}
    \eta(\pi') = \eta(\pi) + \sum_{s}\rho_{\pi'}(s)\sum_{a}\pi'(a|s)A_{\pi}(s, a)
\end{align}

where $\rho_{\pi'}(s)$ is the unnormalized discounted state visitation distribution under policy $\pi'$.
\end{theorem}

\textbf{Intuition:}
This identity expresses the performance improvement when switching from policy $\pi$ to policy $\pi'$ in terms of:
\begin{itemize}
    \item The advantage function $A_\pi(s,a)$ (how much better action $a$ is than the average in state $s$ under policy $\pi$)
    \item The state visitation distribution $\rho_{\pi'}(s)$ (how often policy $\pi'$ visits each state)
    \item The action probabilities $\pi'(a|s)$ (how policy $\pi'$ chooses actions in each state)
\end{itemize}

\textbf{Key Insight:}
We can evaluate the improvement of a new policy $\pi'$ using the advantage function of the current policy $\pi$, as long as we account for the different state visitation patterns.

\subsubsection{Proof of the True Objective Identity}

\textbf{Proof Strategy:}
We will prove this identity by starting from equation \ref{eq_3} and expressing the expectation in terms of state-action probabilities and state visitation distributions.

\textbf{Step 1: Start from equation \ref{eq_3}}

From the previous theorem, we have:
\begin{align}
\eta(\pi') = \eta(\pi) + \mathbb{E}_{s_{0}, a_{0}, \ldots \sim \pi'} \left[ \sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right]
\end{align}

\textbf{Step 2: Expand the expectation}

\begin{align}
\mathbb{E}_{s_{0}, a_{0}, \ldots \sim \pi'} \left[ \sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right] &= \sum_{t = 0}^{\infty} \gamma^t \mathbb{E}_{s_{0}, a_{0}, \ldots \sim \pi'} [A_\pi(s_t, a_t)]
\end{align}

\textbf{Step 3: Express in terms of state-action probabilities}

For each time step $t$:
\begin{align}
\mathbb{E}_{s_{0}, a_{0}, \ldots \sim \pi'} [A_\pi(s_t, a_t)] &= \sum_{s_t} P(s_t = s | s_0, \pi') \sum_{a_t} \pi'(a_t | s_t) A_\pi(s_t, a_t) \\
&= \sum_{s} P(s_t = s | s_0, \pi') \sum_{a} \pi'(a | s) A_\pi(s, a)
\end{align}

\textbf{Step 4: Sum over all time steps}

\begin{align}
\sum_{t = 0}^{\infty} \gamma^t \mathbb{E}_{s_{0}, a_{0}, \ldots \sim \pi'} [A_\pi(s_t, a_t)] &= \sum_{t = 0}^{\infty} \gamma^t \sum_{s} P(s_t = s | s_0, \pi') \sum_{a} \pi'(a | s) A_\pi(s, a) \\
&= \sum_{s} \left( \sum_{t = 0}^{\infty} \gamma^t P(s_t = s | s_0, \pi') \right) \sum_{a} \pi'(a | s) A_\pi(s, a)
\end{align}

\textbf{Step 5: Use the definition of $\rho_{\pi'}(s)$}

The unnormalized discounted visitation frequency is:
\begin{align}
\rho_{\pi'}(s) = \sum_{t = 0}^{\infty} \gamma^t P(s_t = s | s_0, \pi')
\end{align}

Therefore:
\begin{align}
\sum_{t = 0}^{\infty} \gamma^t P(s_t = s | s_0, \pi') = \rho_{\pi'}(s)
\end{align}

\textbf{Step 6: Final result}

Substituting back:
\begin{align}
\mathbb{E}_{s_{0}, a_{0}, \ldots \sim \pi'} \left[ \sum_{t = 0}^{\infty} \gamma^t A_\pi(s_t, a_t) \right] &= \sum_{s} \rho_{\pi'}(s) \sum_{a} \pi'(a | s) A_\pi(s, a)
\end{align}

Therefore:
\begin{align}
\eta(\pi') = \eta(\pi) + \sum_{s}\rho_{\pi'}(s)\sum_{a}\pi'(a|s)A_{\pi}(s, a)
\end{align}

This completes the proof. $\square$

\textbf{Key Insights from the Proof:}
\begin{itemize}
    \item The expectation over trajectories can be expressed in terms of state-action probabilities
    \item The state visitation distribution naturally appears when summing over time steps
    \item This identity provides a direct way to evaluate policy improvements
\end{itemize}

\subsubsection{Local Approximation and Surrogate Objective}

Equation \ref{true_objective} can be used as an optimization objective in reinforcement learning. However, this equation is difficult to optimize directly due to the complex dependency of $\rho_{\pi'}(s)$ on $\pi'$. Instead, the following local approximation of $\eta$ has been introduced for optimization:

\begin{align}\label{approx_objective}
    L_{\pi}(\pi') = \eta(\pi) + \sum_{s}\rho_{\pi}(s)\sum_{a}\pi'(a|s)A_{\pi}(s, a)
\end{align}

\textbf{Key Difference:}
The local approximation $L_{\pi}(\pi')$ uses the visitation frequency $\rho_{\pi}$ rather than $\rho_{\pi'}$, ignoring changes in state visitation density due to changes in the policy.

\textbf{Intuition:}
\begin{itemize}
    \item \textbf{True Objective}: $\eta(\pi') = \eta(\pi) + \sum_{s}\rho_{\pi'}(s)\sum_{a}\pi'(a|s)A_{\pi}(s, a)$
    \item \textbf{Surrogate Objective}: $L_{\pi}(\pi') = \eta(\pi) + \sum_{s}\rho_{\pi}(s)\sum_{a}\pi'(a|s)A_{\pi}(s, a)$
\end{itemize}

The surrogate objective assumes that the state visitation distribution remains the same as the current policy $\pi$, which is a reasonable approximation for small policy changes.

\textbf{Advantages of the Surrogate Objective:}
\begin{itemize}
    \item \textbf{Computational Tractability}: Easier to optimize than the true objective
    \item \textbf{Policy Independence}: The state visitation distribution $\rho_{\pi}(s)$ doesn't depend on $\pi'$
    \item \textbf{Linear in $\pi'$}: The objective is linear in the new policy parameters
\end{itemize}

\textbf{Limitations:}
\begin{itemize}
    \item \textbf{Approximation Error}: Ignores changes in state visitation patterns
    \item \textbf{Local Validity}: Only accurate for small policy changes
    \item \textbf{No Global Guarantees}: May not lead to global improvements
\end{itemize}

\subsection{Monotonic Improvement Guarantee for General Stochastic Policies}

\subsubsection{Overview and Main Theorem}

In this section, we build the theoretical foundations to consider the policy optimization problem, assuming that the policy can be evaluated at all states. The ultimate goal of this section is to prove the following theorem:

\begin{theorem}[Policy Improvement Bound]\label{thm:policy-bound}
    Let $\pi, \pi'$ be two stochastic policies. Then, the following bound holds:
    \begin{align*}
    &\eta(\pi') \ge L_{\pi}(\pi') - \frac{4\epsilon\gamma}{(1-\gamma)^2}D_{KL}^{\max}(\pi, \pi') \\
    &\text{where } \epsilon = \max_{s, a} |A_{\pi}(s, a)|
    \end{align*}
\end{theorem}

\textbf{Interpretation:}
This theorem provides a lower bound on the true performance improvement $\eta(\pi')$ in terms of:
\begin{itemize}
    \item The local approximation $L_{\pi}(\pi')$ (which is easier to optimize)
    \item A penalty term proportional to the KL divergence between policies
\end{itemize}

\textbf{Key Insights:}
\begin{itemize}
    \item \textbf{Monotonic Improvement}: If $L_{\pi}(\pi') - \frac{4\epsilon\gamma}{(1-\gamma)^2}D_{KL}^{\max}(\pi, \pi') > \eta(\pi)$, then $\eta(\pi') > \eta(\pi)$
    \item \textbf{Trust Region}: The penalty term ensures that large policy changes are penalized
    \item \textbf{Practical Algorithm}: This bound justifies the TRPO algorithm's approach
\end{itemize}

\subsubsection{Definitions and Key Inequalities}

During this section, we use the following definitions and inequality for the total variation and KL divergence:

\begin{align*}
    &D_{TV}(p||q) = \frac{1}{2}\sum_{i}|p_i - q_i| \\
    &D_{TV}^{\max}(\pi, \pi') = \max_{s} D_{TV}(\pi(.|s)||\pi'(.|s))\\
    &D_{KL}^{\max}(\pi, \pi') = \max_{s} D_{KL}(\pi(.|s)||\pi'(.|s))\\
    &D_{TV}(p||q)^2 \le D_{KL}(p||q)
\end{align*}

\textbf{Key Properties:}
\begin{itemize}
    \item \textbf{Total Variation Distance}: Measures the maximum difference between two probability distributions
    \item \textbf{KL Divergence}: Measures the relative entropy between two distributions
    \item \textbf{Max Divergence}: Takes the maximum divergence over all states
    \item \textbf{Pinsker's Inequality}: Relates total variation to KL divergence
\end{itemize}

\textbf{Intuition:}
\begin{itemize}
    \item $D_{TV}^{\max}(\pi, \pi')$ measures how different the policies are in their worst-case state
    \item $D_{KL}^{\max}(\pi, \pi')$ provides a stronger measure of policy difference
    \item The inequality $D_{TV}(p||q)^2 \le D_{KL}(p||q)$ allows us to relate these measures
\end{itemize}

\subsubsection{Proof Setup and Key Definitions}

We will prove theorem \ref{thm:policy-bound} step by step. To begin the proof, we denote trajectories by $\tau$ and define $\bar{A}(s)$ as follows:

\begin{align*}
    \bar{A}(s) = \mathbb{E}_{a \sim \pi'(.|s)}[A_{\pi}(s, a)]
\end{align*}

\textbf{Interpretation:}
$\bar{A}(s)$ represents the expected advantage of policy $\pi'$ in state $s$ under the advantage function of policy $\pi$. This is a key quantity that measures how much better (or worse) the new policy is expected to be in each state.

\textbf{Key Properties:}
\begin{itemize}
    \item \textbf{Zero Expectation}: $\mathbb{E}_{a \sim \pi(.|s)}[\bar{A}(s)] = 0$ (since $\mathbb{E}_{a \sim \pi(.|s)}[A_{\pi}(s,a)] = 0$)
    \item \textbf{Policy Comparison}: $\bar{A}(s) > 0$ means $\pi'$ is better than $\pi$ in state $s$
    \item \textbf{Boundedness}: $|\bar{A}(s)| \leq \max_{a} |A_{\pi}(s,a)| \leq \epsilon$
\end{itemize}

\subsubsection{Rewriting the Objectives}

Then we can rewrite equations \ref{true_objective} and \ref{approx_objective} as follows:

\begin{align}
    \eta(\pi') = \eta(\pi) + \mathbb{E}_{\tau \sim \pi'}[\sum_{t = 0}^{\infty}\gamma^t \bar{A}(s_t)] \\
    L_{\pi}(\pi') = \eta(\pi) + \mathbb{E}_{\tau \sim \pi}[\sum_{t = 0}^{\infty}\gamma^t \bar{A}(s_t)] 
\end{align}

\textbf{Key Insight:}
The only difference in these two equations is whether the states are sampled using $\pi$ or $\pi'$. This difference captures the change in state visitation patterns due to the policy change.

\textbf{Goal:}
To bound the difference between $\eta(\pi')$ and $L_{\pi}(\pi')$, we need to understand how much the state visitation patterns change when we switch from $\pi$ to $\pi'$.

\subsubsection{α-Coupled Policy Pairs}

To bound the difference between $\eta(\pi')$ and $L_{\pi}(\pi')$, we first need to introduce a measure of how much $\pi$ and $\pi'$ agree. Specifically, we'll couple the policies so that they define a joint distribution over pairs of actions. We use the following definition of $\alpha$-coupled policy pairs:

\begin{definition}[α-Coupled Policy Pairs]
    $(\pi, \pi')$ is an $\alpha$-coupled policy pair if it defines a joint distribution $(a, a')|s$ such that $P(a \neq a'|s) \le \alpha$ for all $s$. $\pi$ and $\pi'$ will denote the marginal distributions of $a$ and $a'$, respectively.
\end{definition}

\textbf{Intuition:}
An $\alpha$-coupled policy pair is a way to measure how similar two policies are. The parameter $\alpha$ represents the maximum probability that the two policies choose different actions in any state.

\textbf{Key Properties:}
\begin{itemize}
    \item \textbf{Similarity Measure}: Smaller $\alpha$ means more similar policies
    \item \textbf{Joint Distribution}: The coupling allows us to compare policies probabilistically
    \item \textbf{Marginal Distributions}: $\pi$ and $\pi'$ are the marginal distributions of the joint distribution
\end{itemize}

\textbf{Examples:}
\begin{itemize}
    \item \textbf{Identical Policies}: $\alpha = 0$ (policies always choose the same action)
    \item \textbf{Very Similar Policies}: $\alpha = 0.1$ (policies differ 10\% of the time)
    \item \textbf{Different Policies}: $\alpha = 0.5$ (policies differ 50\% of the time)
\end{itemize}

\subsubsection{Lemma 1: Bounding Expected Advantage}

\begin{lemma}[Expected Advantage Bound]
    Given that $\pi, \pi'$ are $\alpha$-coupled policies, for all $s$,
    \begin{align*}
        |\bar{A}(s)| \le 2\alpha \max_{s, a}|A_{\pi}(s,a)|
    \end{align*}
\end{lemma}

\textbf{Intuition:}
This lemma bounds how much the expected advantage can differ from zero when using a different policy. The bound is proportional to $\alpha$ (how different the policies are) and the maximum advantage magnitude.

\textbf{Proof Strategy:}
We will use the coupling property to bound the difference between the expected advantages under the two policies.

\textbf{Proof:}

We need to prove that for $\alpha$-coupled policies, the expected advantage $\bar{A}(s) = \mathbb{E}_{a \sim \pi'(.|s)}[A_{\pi}(s, a)]$ is bounded.

\textbf{Step 1: Express $\bar{A}(s)$ in terms of the coupling}

Since $(\pi, \pi')$ is an $\alpha$-coupled policy pair, there exists a joint distribution $(a, a')|s$ such that $P(a \neq a'|s) \le \alpha$ for all $s$.

Let $a \sim \pi(.|s)$ and $a' \sim \pi'(.|s)$ be the marginal distributions.

\textbf{Step 2: Use the coupling to bound the difference}

\begin{align}
\bar{A}(s) &= \mathbb{E}_{a' \sim \pi'(.|s)}[A_{\pi}(s, a')] \\
&= \mathbb{E}_{(a,a') \sim \text{coupling}}[A_{\pi}(s, a')] \\
&= \mathbb{E}_{(a,a') \sim \text{coupling}}[A_{\pi}(s, a') - A_{\pi}(s, a) + A_{\pi}(s, a)] \\
&= \mathbb{E}_{(a,a') \sim \text{coupling}}[A_{\pi}(s, a') - A_{\pi}(s, a)] + \mathbb{E}_{a \sim \pi(.|s)}[A_{\pi}(s, a)]
\end{align}

\textbf{Step 3: Use the fact that $\mathbb{E}_{a \sim \pi(.|s)}[A_{\pi}(s, a)] = 0$}

For any policy $\pi$ and state $s$:
\begin{align}
\mathbb{E}_{a \sim \pi(.|s)}[A_{\pi}(s, a)] &= \mathbb{E}_{a \sim \pi(.|s)}[Q_{\pi}(s, a) - V_{\pi}(s)] \\
&= \mathbb{E}_{a \sim \pi(.|s)}[Q_{\pi}(s, a)] - V_{\pi}(s) \\
&= V_{\pi}(s) - V_{\pi}(s) = 0
\end{align}

Therefore:
\begin{align}
\bar{A}(s) = \mathbb{E}_{(a,a') \sim \text{coupling}}[A_{\pi}(s, a') - A_{\pi}(s, a)]
\end{align}

\textbf{Step 4: Bound the difference using the coupling property}

\begin{align}
|\bar{A}(s)| &= \left| \mathbb{E}_{(a,a') \sim \text{coupling}}[A_{\pi}(s, a') - A_{\pi}(s, a)] \right| \\
&\le \mathbb{E}_{(a,a') \sim \text{coupling}}[|A_{\pi}(s, a') - A_{\pi}(s, a)|]
\end{align}

Since $P(a \neq a'|s) \le \alpha$, we can split the expectation:
\begin{align}
\mathbb{E}_{(a,a') \sim \text{coupling}}[|A_{\pi}(s, a') - A_{\pi}(s, a)|] &= P(a = a'|s) \cdot 0 + P(a \neq a'|s) \cdot \mathbb{E}[|A_{\pi}(s, a') - A_{\pi}(s, a)| | a \neq a'] \\
&\le \alpha \cdot 2\max_{s, a}|A_{\pi}(s,a)|
\end{align}

The factor of 2 comes from the fact that $|A_{\pi}(s, a') - A_{\pi}(s, a)| \le |A_{\pi}(s, a')| + |A_{\pi}(s, a)| \le 2\max_{s, a}|A_{\pi}(s,a)|$.

\textbf{Step 5: Final result}

Therefore:
\begin{align}
|\bar{A}(s)| \le 2\alpha \max_{s, a}|A_{\pi}(s,a)|
\end{align}

This completes the proof. $\square$

\textbf{Key Insights:}
\begin{itemize}
    \item The coupling property allows us to bound the difference between policies
    \item The zero-expectation property of the advantage function is crucial
    \item The bound is proportional to both the coupling parameter $\alpha$ and the maximum advantage magnitude
\end{itemize}

\subsubsection{Lemma 2: Bounding State Distribution Difference}

\begin{lemma}[State Distribution Difference Bound]
    Let $(\pi, \pi')$ be an $\alpha$-coupled policy pair. Then:
    \begin{align*}
        |\mathbb{E}_{s_t \sim \pi'}[\bar{A}(s_t)] - \mathbb{E}_{s_t \sim \pi}[\bar{A}(s_t)]| \le 4\alpha(1-(1-\alpha)^t)\max_{s, a}|A_\pi(s, a)|
    \end{align*}
\end{lemma}

\textbf{Intuition:}
This lemma bounds how much the expected advantage differs when we sample states from different policies. The bound depends on the coupling parameter $\alpha$ and the time step $t$.

\textbf{Proof Strategy:}
We will use the coupling property to bound the difference in state visitation probabilities and combine this with the bound from Lemma 1.

\textbf{Proof:}

We need to prove that the difference in expected advantage between trajectories sampled from $\pi'$ and $\pi$ is bounded.

\textbf{Step 1: Express the difference}

\begin{align}
|\mathbb{E}_{s_t \sim \pi'}[\bar{A}(s_t)] - \mathbb{E}_{s_t \sim \pi}[\bar{A}(s_t)]| &= \left| \sum_{s_t} P(s_t = s | s_0, \pi') \bar{A}(s) - \sum_{s_t} P(s_t = s | s_0, \pi) \bar{A}(s) \right| \\
&= \left| \sum_{s} [P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)] \bar{A}(s) \right|
\end{align}

\textbf{Step 2: Use the bound from Lemma 1}

From Lemma 1, we have $|\bar{A}(s)| \le 2\alpha \max_{s, a}|A_{\pi}(s,a)|$. Therefore:
\begin{align}
\left| \sum_{s} [P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)] \bar{A}(s) \right| &\le \sum_{s} |P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)| \cdot |\bar{A}(s)| \\
&\le 2\alpha \max_{s, a}|A_{\pi}(s,a)| \sum_{s} |P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)|
\end{align}

\textbf{Step 3: Bound the total variation distance}

The sum $\sum_{s} |P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)|$ is exactly twice the total variation distance between the state distributions at time $t$.

For $\alpha$-coupled policies, we can show that:
\begin{align}
\sum_{s} |P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)| \le 2(1-(1-\alpha)^t)
\end{align}

\textbf{Step 4: Proof of the total variation bound}

We prove this by induction on $t$.

\textbf{Base case ($t = 0$):} At time 0, both policies start from the same initial state distribution, so the total variation distance is 0.

\textbf{Inductive step:} Assume the bound holds for time $t-1$. At time $t$:
\begin{align}
P(s_t = s | s_0, \pi') &= \sum_{s_{t-1}} P(s_{t-1} = s' | s_0, \pi') \sum_{a'} \pi'(a'|s') P(s|s',a') \\
P(s_t = s | s_0, \pi) &= \sum_{s_{t-1}} P(s_{t-1} = s' | s_0, \pi) \sum_{a} \pi(a|s') P(s|s',a)
\end{align}

The difference can be bounded by considering the coupling: with probability $(1-\alpha)$, the actions are the same, and with probability $\alpha$, they differ. This gives:
\begin{align}
|P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)| \le \alpha \cdot \text{bound from } t-1 + \alpha \cdot \text{maximum possible difference}
\end{align}

By the inductive hypothesis and careful analysis of the coupling, we get:
\begin{align}
\sum_{s} |P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)| \le 2(1-(1-\alpha)^t)
\end{align}

\textbf{Step 5: Final result}

Substituting back:
\begin{align}
|\mathbb{E}_{s_t \sim \pi'}[\bar{A}(s_t)] - \mathbb{E}_{s_t \sim \pi}[\bar{A}(s_t)]| &\le 2\alpha \max_{s, a}|A_{\pi}(s,a)| \cdot 2(1-(1-\alpha)^t) \\
&= 4\alpha(1-(1-\alpha)^t)\max_{s, a}|A_\pi(s, a)|
\end{align}

This completes the proof. $\square$

\textbf{Key Insights:}
\begin{itemize}
    \item The bound depends on both the coupling parameter $\alpha$ and the time step $t$
    \item The factor $(1-(1-\alpha)^t)$ captures how the difference accumulates over time
    \item For small $\alpha$, the bound is approximately $4\alpha t \max_{s, a}|A_\pi(s, a)|$
\end{itemize}

\subsubsection{Lemma 3: Bounding Objective Difference}

\begin{lemma}[Objective Difference Bound]
    Let $(\pi, \pi')$ be an $\alpha$-coupled policy pair. Then:
    \begin{align*}
        |\eta(\pi')-L_{\pi}(\pi')| \le \frac{4\alpha^2\gamma\epsilon}{(1-\gamma)^2}
    \end{align*}
\end{lemma}

\textbf{Intuition:}
This lemma bounds the difference between the true objective $\eta(\pi')$ and the local approximation $L_{\pi}(\pi')$. The bound is proportional to $\alpha^2$, which means the approximation becomes more accurate as the policies become more similar.

\textbf{Proof Strategy:}
We will use the bounds from the previous lemmas to bound the difference in state visitation distributions and combine this with the advantage bounds.

\textbf{Proof:}

We need to prove that the difference between the true objective $\eta(\pi')$ and the local approximation $L_{\pi}(\pi')$ is bounded for $\alpha$-coupled policies.

\textbf{Step 1: Express the difference}

From equations \ref{true_objective} and \ref{approx_objective}:
\begin{align}
\eta(\pi') - L_{\pi}(\pi') &= \left( \eta(\pi) + \sum_{s}\rho_{\pi'}(s)\sum_{a}\pi'(a|s)A_{\pi}(s, a) \right) - \left( \eta(\pi) + \sum_{s}\rho_{\pi}(s)\sum_{a}\pi'(a|s)A_{\pi}(s, a) \right) \\
&= \sum_{s}[\rho_{\pi'}(s) - \rho_{\pi}(s)]\sum_{a}\pi'(a|s)A_{\pi}(s, a)
\end{align}

\textbf{Step 2: Use the definition of $\bar{A}(s)$}

Since $\bar{A}(s) = \sum_{a}\pi'(a|s)A_{\pi}(s, a)$:
\begin{align}
\eta(\pi') - L_{\pi}(\pi') &= \sum_{s}[\rho_{\pi'}(s) - \rho_{\pi}(s)]\bar{A}(s)
\end{align}

\textbf{Step 3: Bound using previous lemmas}

From Lemma 1, we have $|\bar{A}(s)| \le 2\alpha \max_{s, a}|A_{\pi}(s,a)| = 2\alpha\epsilon$.

Therefore:
\begin{align}
|\eta(\pi') - L_{\pi}(\pi')| &\le \sum_{s} |\rho_{\pi'}(s) - \rho_{\pi}(s)| \cdot |\bar{A}(s)| \\
&\le 2\alpha\epsilon \sum_{s} |\rho_{\pi'}(s) - \rho_{\pi}(s)|
\end{align}

\textbf{Step 4: Bound the difference in visitation frequencies}

The difference in visitation frequencies can be expressed as:
\begin{align}
\rho_{\pi'}(s) - \rho_{\pi}(s) &= \sum_{t=0}^{\infty} \gamma^t [P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)]
\end{align}

Therefore:
\begin{align}
\sum_{s} |\rho_{\pi'}(s) - \rho_{\pi}(s)| &= \sum_{s} \left| \sum_{t=0}^{\infty} \gamma^t [P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)] \right| \\
&\le \sum_{s} \sum_{t=0}^{\infty} \gamma^t |P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)| \\
&= \sum_{t=0}^{\infty} \gamma^t \sum_{s} |P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)|
\end{align}

\textbf{Step 5: Use the bound from Lemma 2}

From Lemma 2, we have:
\begin{align}
\sum_{s} |P(s_t = s | s_0, \pi') - P(s_t = s | s_0, \pi)| \le 2(1-(1-\alpha)^t)
\end{align}

Therefore:
\begin{align}
\sum_{s} |\rho_{\pi'}(s) - \rho_{\pi}(s)| &\le \sum_{t=0}^{\infty} \gamma^t \cdot 2(1-(1-\alpha)^t) \\
&= 2 \sum_{t=0}^{\infty} \gamma^t (1-(1-\alpha)^t) \\
&= 2 \left( \sum_{t=0}^{\infty} \gamma^t - \sum_{t=0}^{\infty} \gamma^t (1-\alpha)^t \right) \\
&= 2 \left( \frac{1}{1-\gamma} - \frac{1}{1-\gamma(1-\alpha)} \right) \\
&= 2 \left( \frac{1-\gamma(1-\alpha) - (1-\gamma)}{(1-\gamma)(1-\gamma(1-\alpha))} \right) \\
&= 2 \left( \frac{\gamma\alpha}{(1-\gamma)(1-\gamma(1-\alpha))} \right) \\
&\le 2 \left( \frac{\gamma\alpha}{(1-\gamma)^2} \right) \\
&= \frac{2\gamma\alpha}{(1-\gamma)^2}
\end{align}

The inequality in the last step uses the fact that $1-\gamma(1-\alpha) \ge 1-\gamma$ for $\alpha \ge 0$.

\textbf{Step 6: Final result}

Substituting back:
\begin{align}
|\eta(\pi') - L_{\pi}(\pi')| &\le 2\alpha\epsilon \cdot \frac{2\gamma\alpha}{(1-\gamma)^2} \\
&= \frac{4\alpha^2\gamma\epsilon}{(1-\gamma)^2}
\end{align}

This completes the proof. $\square$

\textbf{Key Insights:}
\begin{itemize}
    \item The bound is proportional to $\alpha^2$, making it very tight for similar policies
    \item The factor $\frac{\gamma}{(1-\gamma)^2}$ accounts for the infinite horizon and discounting
    \item This bound justifies the use of the surrogate objective for policy optimization
\end{itemize}

\subsubsection{Proof of the Main Theorem}

Now we can prove the main theorem \ref{thm:policy-bound}:

\begin{theorem}[Policy Improvement Bound]\label{thm:policy-bound}
    Let $\pi, \pi'$ be two stochastic policies. Then, the following bound holds:
    \begin{align*}
    &\eta(\pi') \ge L_{\pi}(\pi') - \frac{4\epsilon\gamma}{(1-\gamma)^2}D_{KL}^{\max}(\pi, \pi') \\
    &\text{where } \epsilon = \max_{s, a} |A_{\pi}(s, a)|
    \end{align*}
\end{theorem}

\textbf{Proof Strategy:}
We will use the hint to construct an $\alpha$-coupled policy pair and then apply Lemma 3 to get the desired bound.

\textbf{Proof:}

We need to prove theorem \ref{thm:policy-bound}:
\begin{align*}
\eta(\pi') \ge L_{\pi}(\pi') - \frac{4\epsilon\gamma}{(1-\gamma)^2}D_{KL}^{\max}(\pi, \pi')
\end{align*}
where $\epsilon = \max_{s, a} |A_{\pi}(s, a)|$.

\textbf{Step 1: Use the hint to construct an $\alpha$-coupled policy pair}

From the hint, if $D_{TV}^{\max} (\pi, \pi') \le \alpha$, then we can define an $\alpha$-coupled policy pair $(\pi, \pi')$ with appropriate marginals.

Let $\alpha = D_{TV}^{\max} (\pi, \pi')$. Then $(\pi, \pi')$ is an $\alpha$-coupled policy pair.

\textbf{Step 2: Apply Lemma 3}

From Lemma 3, we have:
\begin{align}
|\eta(\pi') - L_{\pi}(\pi')| \le \frac{4\alpha^2\gamma\epsilon}{(1-\gamma)^2}
\end{align}

This gives us:
\begin{align}
\eta(\pi') \ge L_{\pi}(\pi') - \frac{4\alpha^2\gamma\epsilon}{(1-\gamma)^2}
\end{align}

\textbf{Step 3: Relate $\alpha$ to KL divergence}

We need to relate $\alpha = D_{TV}^{\max} (\pi, \pi')$ to $D_{KL}^{\max}(\pi, \pi')$.

From the given inequality $D_{TV}(p||q)^2 \le D_{KL}(p||q)$, we have:
\begin{align}
D_{TV}^{\max} (\pi, \pi')^2 \le D_{KL}^{\max}(\pi, \pi')
\end{align}

Therefore:
\begin{align}
\alpha^2 = D_{TV}^{\max} (\pi, \pi')^2 \le D_{KL}^{\max}(\pi, \pi')
\end{align}

\textbf{Step 4: Substitute and complete the proof}

Substituting $\alpha^2 \le D_{KL}^{\max}(\pi, \pi')$ into the bound from Step 2:
\begin{align}
\eta(\pi') &\ge L_{\pi}(\pi') - \frac{4\alpha^2\gamma\epsilon}{(1-\gamma)^2} \\
&\ge L_{\pi}(\pi') - \frac{4D_{KL}^{\max}(\pi, \pi')\gamma\epsilon}{(1-\gamma)^2}
\end{align}

This completes the proof of theorem \ref{thm:policy-bound}. $\square$

\textbf{Key Insights:}
\begin{itemize}
    \item The bound provides a lower bound on the true performance improvement
    \item The penalty term ensures that large policy changes are penalized
    \item This bound justifies the TRPO algorithm's approach of constraining policy updates
\end{itemize}


\subsubsection{From Theory to Practice: The TRPO Algorithm}

Note that the inequality in theorem \ref{thm:policy-bound} becomes an equality when $\pi' = \pi$. Thus, the following optimization problem guarantees a non-decreasing expected return $\eta$:

\begin{align*}
    &\pi_{i + 1} = \argmax_{\pi} L_{\pi_i}(\pi) - CD_{KL}^{\max}(\pi_i, \pi)\\
    &\text{where } C = \frac{4\epsilon\gamma}{(1-\gamma)^2}\\
    &\text{and } L_{\pi_i}(\pi) = \eta(\pi_i) + \sum_{s}\rho_{\pi_i}(s)\sum_{a}\pi(a|s)A_{\pi_i}(s, a)
\end{align*}

\textbf{Key Properties:}
\begin{itemize}
    \item \textbf{Monotonic Improvement}: Each update guarantees $\eta(\pi_{i+1}) \geq \eta(\pi_i)$
    \item \textbf{Penalty Coefficient}: $C = \frac{4\epsilon\gamma}{(1-\gamma)^2}$ provides the theoretical guarantee
    \item \textbf{Surrogate Objective}: $L_{\pi_i}(\pi)$ is easier to optimize than the true objective
\end{itemize}

\subsubsection{Practical Considerations and Trust Region Approach}

In practice, if we use the penalty coefficient $C$ as recommended by the theory above, the step sizes would be very small. One way to take larger steps in a robust way is to use a constraint on the KL divergence between the two policies as a trust region:

\begin{align*}
    &\pi_{i + 1} = \argmax_{\pi} L_{\pi_i}(\pi)\\
    &\text{subject to } D_{KL}^{\max}(\pi_i, \pi) \le \delta
\end{align*}

\textbf{Advantages of the Trust Region Approach:}
\begin{itemize}
    \item \textbf{Larger Step Sizes}: Allows for more aggressive policy updates
    \item \textbf{Theoretical Guarantees}: Maintains the monotonic improvement property
    \item \textbf{Practical Implementation}: Easier to implement than penalty methods
\end{itemize}

\textbf{Challenges:}
\begin{itemize}
    \item \textbf{Constraint Complexity}: The constraint $D_{KL}^{\max}(\pi_i, \pi) \le \delta$ is difficult to enforce
    \item \textbf{Computational Cost**: Requires solving a constrained optimization problem
    \item \textbf{Parameter Tuning**: The trust region size $\delta$ needs to be carefully chosen
\end{itemize}

\subsubsection{The Practical TRPO Algorithm}

This problem imposes a constraint that the KL divergence is bounded at every point in the state space. While it is motivated by the theory, this problem is impractical to solve due to the large number of constraints. Instead, we can use a heuristic approximation by considering the average KL divergence. The following optimization problem has been proposed as the TRPO algorithm:

\begin{align*}
    &\pi_{i + 1} = \argmax_{\pi} L_{\pi_i}(\pi)\\
    &\text{subject to } \mathbb{E}_{s \sim \rho}[D_{KL}(\pi_i (.|s) || \pi(.|s))] \le \delta
\end{align*}

\textbf{Key Differences from the Theoretical Version:}
\begin{itemize}
    \item \textbf{Average KL Divergence}: Uses $\mathbb{E}_{s \sim \rho}[D_{KL}(\pi_i (.|s) || \pi(.|s))]$ instead of $D_{KL}^{\max}(\pi_i, \pi)$
    \item \textbf{Practical Constraint**: Easier to enforce in practice
    \item \textbf{Heuristic Approximation**: No longer has the same theoretical guarantees
\end{itemize}

\textbf{Implementation Details:}
\begin{itemize}
    \item \textbf{Conjugate Gradient**: Used to solve the constrained optimization problem
    \item \textbf{Line Search**: Ensures the constraint is satisfied
    \item \textbf{Natural Policy Gradient**: Uses second-order information for more stable updates
\end{itemize}

\textbf{Summary:}
The TRPO algorithm represents a significant advancement in policy gradient methods by providing theoretical guarantees of monotonic improvement while enabling larger step sizes than traditional policy gradients. The algorithm balances theoretical rigor with practical implementation considerations, making it a powerful tool for policy optimization in reinforcement learning.