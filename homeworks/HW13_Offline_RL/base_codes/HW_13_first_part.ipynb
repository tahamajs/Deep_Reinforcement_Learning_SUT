{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b869fc",
   "metadata": {},
   "source": [
    "# **Homework 13: Multi-Agent Reinforcement Learning**\n",
    "\n",
    "#### **Course:** Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ade3a8",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: Nash Equilibrium (Theory)\n",
    "\n",
    "A Nash Equilibrium (NE) represents a state where no player can improve their outcome by unilaterally changing their strategy. For our games, we'll focus on finding the mixed-strategy NE, where players choose their actions probabilistically.\n",
    "\n",
    "### 1.1 Standard Rock-Scissors-Paper\n",
    "\n",
    "Given the standard RSP payoff matrix:\n",
    "\n",
    "| Player 1 | Rock | Scissors | Paper |\n",
    "| :--- | :--: | :---: | :---: |\n",
    "| **Rock** | 0, 0 | 1, -1 | -1, 1 |\n",
    "| **Scissors**| -1, 1 | 0, 0 | 1, -1 |\n",
    "| **Paper** | 1, -1 | -1, 1 | 0, 0 |\n",
    "\n",
    "\n",
    "**Your Task:** Analytically derive the mixed-strategy Nash Equilibrium for this game. Show the steps for setting up the indifference equations for Player 1 and solving for Player 2's equilibrium strategy probabilities $(q_R, q_S, q_P)$. (find the Mixed Nash equilibrium of the game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae8ca8",
   "metadata": {},
   "source": [
    "### 1.2 Modified Rock-Scissors-Paper\n",
    "\n",
    "Now, consider the modified RSP game where the stakes are higher:\n",
    "\n",
    "| Player 1 | Rock | Scissors | Paper |\n",
    "| :--- | :--: | :---: | :---: |\n",
    "| **Rock** | 0, 0 | 1, -1 | -2, 2 |\n",
    "| **Scissors**| -1, 1 | 0, 0 | 3, -3 |\n",
    "| **Paper** | 2, -2 | -3, 3 | 0, 0 |\n",
    "\n",
    "\n",
    "**Your Task:** Like pervious one Derive the mixed-strategy Nash Equilibrium for this modified game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3956a",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Learning by Observation - Fictitious Play (Implementation)\n",
    "\n",
    "Fictitious Play is an intuitive learning algorithm where each agent models its opponent as playing a stationary strategy defined by the historical frequency of their past actions. The agent then plays a **best response** to this belief.\n",
    "\n",
    "### 2.1 Implementation\n",
    "\n",
    "**Your Task:** Implement the `simulate_fictitious_play` function below. It should take the payoff matrices for both players and the number of iterations as input. At each step, each player should choose the action that maximizes their expected payoff given the history of the opponent's plays.\n",
    "\n",
    "**Algorithm:** At each time step $t > 0$, Player $i$ forms a belief that their opponent ($-i$) will play each action $a'$ with a probability equal to its historical frequency. The agent then chooses an action $a_i^*$ that is a best response to this belief.\n",
    "\n",
    "Let $C_{t-1}(a_{-i})$ be the count of times opponent $-i$ has played action $a_{-i}$ up to step $t-1$. Player $i$'s best response is:\n",
    "$$a_{i,t}^* = \\arg\\max_{a_i \\in A_i} \\sum_{a_{-i} \\in A_{-i}} u_i(a_i, a_{-i}) \\cdot \\frac{C_{t-1}(a_{-i})}{t-1}$$\n",
    "\n",
    "**Note on Tie-Breaking:** If multiple actions yield the same maximal expected payoff, your agent should choose one of these best responses uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_fictitious_play(A, B, iterations):\n",
    "    \"\"\"\n",
    "    Simulates Fictitious Play for two players in a normal-form game.\n",
    "\n",
    "    Args:\n",
    "        A (np.ndarray): Payoff matrix for Player 1.\n",
    "        B (np.ndarray): Payoff matrix for Player 2.\n",
    "        iterations (int): The number of rounds to play.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - p1_freq_history (np.ndarray): History of Player 1's action frequencies.\n",
    "            - p2_freq_history (np.ndarray): History of Player 2's action frequencies.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    # Initialize action counts and history lists\n",
    "    # Loop for the specified number of iterations\n",
    "    # On iteration 0, use a tie-breaking rule (e.g., play action 0)\n",
    "    # On subsequent iterations, calculate best response to opponent's historical frequencies\n",
    "    # Update action counts\n",
    "    # Periodically record the current action frequencies for plotting\n",
    "    \n",
    "    pass # Replace with your implementation\n",
    "\n",
    "# --- Payoff Matrices ---\n",
    "A_std = np.array([[0, 1, -1], [-1, 0, 1], [1, -1, 0]])\n",
    "A_mod = np.array([[0, 1, -2], [-1, 0, 3], [2, -3, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11ca0c",
   "metadata": {},
   "source": [
    "### 2.2 Analysis\n",
    "\n",
    "**Your Task:**\n",
    "1.  Run your simulation for **1,000,000 iterations** on both the **standard** and **modified** RSP games.\n",
    "2.  Generate two plots, one for each game. Each plot should show the evolution of Players action frequencies over time and include horizontal lines indicating the theoretical NE probabilities you calculated in Problem 1.\n",
    "3.  **Analyze your results:** Do the action frequencies converge? If so, do they converge to the Nash Equilibrium? Explain the observed behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be41da",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Fictitious Play with Exploration (Implementation)\n",
    "\n",
    "Our Fictitious Play agent is purely exploitative. In Reinforcement Learning, we know the importance of the **exploration-exploitation tradeoff**. Let's create an $\\epsilon$-greedy version of Fictitious Play.\n",
    "\n",
    "### 3.1 Implementation\n",
    "\n",
    "**Your Task:** Create a new function, `simulate_epsilon_greedy_fp`. This function should be similar to your Fictitious Play implementation but include an `epsilon` parameter. At each step, with probability `epsilon`, the agent should choose a random action (explore). With probability `1-epsilon`, it should play the best response (exploit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_epsilon_greedy_fp(A, B, iterations, epsilon):\n",
    "    \"\"\"\n",
    "    Simulates epsilon-greedy Fictitious Play.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    pass # Replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7024d5",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Analysis\n",
    "\n",
    "**Your Task:**\n",
    "1.  Run the `simulate_epsilon_greedy_fp` function on the **modified** RSP game for **1,000,000 iterations** with three different `epsilon` values: `0.01`, `0.1`, and `0.3`.\n",
    "2.  Plot the results for each simulation.\n",
    "3.  **Analyze your results:** How does `epsilon` affect the learning dynamics? Does the agent's strategy still converge to the NE? If not, to what does it converge? Discuss the impact of exploration in this multi-agent context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43626e0a",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Learning from \"What If\" - Regret Matching (Implementation & Theory)\n",
    "\n",
    "Regret Matching is a powerful no-regret learning algorithm. Instead of playing a best response to history, an agent's probability of choosing an action is proportional to the positive **regret** for not having chosen that action in the past. The key property of regret matching is that the **average strategy** over time converges to a Nash Equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9351241",
   "metadata": {},
   "source": [
    "### 4.1 Implementation\n",
    "\n",
    "**Your Task:** Implement the `simulate_regret_matching` function below.\n",
    "\n",
    "**Algorithm:** Regret Matching works in two steps. First, update the cumulative regrets. Second, determine the next round's strategy.\n",
    "\n",
    "1.  **Regret Calculation:** After playing action $a_i$ against opponent's action $a_{-i}$, the cumulative regret $R_t(s)$ for *not* having played action $s \\in A_i$ is updated as follows:\n",
    "    $$R_t(s) = R_{t-1}(s) + u_i(s, a_{-i}) - u_i(a_i, a_{-i})$$\n",
    "\n",
    "2.  **Strategy Calculation:** The probability of playing action $s$ in the next round is proportional to its positive cumulative regret, $R_t^+(s) = \\max(0, R_t(s))$.\n",
    "    $$p_{t+1}(s) = \\frac{R_t^+(s)}{\\sum_{s' \\in A_i} R_t^+(s')}$$\n",
    "    If the sum of positive regrets is zero, play uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d97ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_regret_matching(A, B, iterations):\n",
    "    \"\"\"\n",
    "    Simulates Regret Matching for two players.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - p1_avg_strat_hist (np.ndarray): History of Player 1's average strategy.\n",
    "            - p1_inst_strat_hist (np.ndarray): History of Player 1's instantaneous strategy.\n",
    "    \"\"\"\n",
    "    num_actions = A.shape[0]\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    # Initialize regrets, strategy sums, and history lists\n",
    "    # Loop for iterations\n",
    "    # Calculate the current strategy based on positive regrets\n",
    "    #   (If sum of positive regrets is 0, play uniformly random)\n",
    "    # Store the instantaneous strategy and add to the strategy sum\n",
    "    # Choose actions based on the current strategies\n",
    "    # Update regrets for ALL actions based on the outcome\n",
    "    # Periodically record the average and instantaneous strategies\n",
    "    \n",
    "    pass # Replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdfb79",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Analysis\n",
    "\n",
    "**Your Task:**\n",
    "1.  Run your simulation for the **modified** RSP game for **1,000,000 iterations**.\n",
    "2.  Generate a single figure with two subplots:\n",
    "    * **Subplot 1:** Plot the **instantaneous strategy** of Player 1 over time.\n",
    "    * **Subplot 2:** Plot the **average strategy** of Player 1 over time. Include horizontal lines for the NE.\n",
    "3.  **Analyze your results:** Compare the two plots. Which one converges to the Nash Equilibrium? \\\n",
    "                              (Bonus): Explain why this is the expected theoretical outcome for Regret Matching algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
