{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b869fc",
   "metadata": {},
   "source": [
    "# **Homework 13: Multi-Agent Reinforcement Learning**\n",
    "\n",
    "#### **Course:** Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ade3a8",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 1: Nash Equilibrium (Theory)\n",
    "\n",
    "A Nash Equilibrium (NE) represents a state where no player can improve their outcome by unilaterally changing their strategy. For our games, we'll focus on finding the mixed-strategy NE, where players choose their actions probabilistically.\n",
    "\n",
    "### 1.1 Standard Rock-Scissors-Paper\n",
    "\n",
    "Given the standard RSP payoff matrix:\n",
    "\n",
    "| Player 1 | Rock | Scissors | Paper |\n",
    "| :--- | :--: | :---: | :---: |\n",
    "| **Rock** | 0, 0 | 1, -1 | -1, 1 |\n",
    "| **Scissors**| -1, 1 | 0, 0 | 1, -1 |\n",
    "| **Paper** | 1, -1 | -1, 1 | 0, 0 |\n",
    "\n",
    "\n",
    "**Your Task:** Analytically derive the mixed-strategy Nash Equilibrium for this game. Show the steps for setting up the indifference equations for Player 1 and solving for Player 2's equilibrium strategy probabilities $(q_R, q_S, q_P)$. (find the Mixed Nash equilibrium of the game)\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "For a mixed-strategy Nash Equilibrium, each player must be indifferent between all their pure strategies. Let Player 1 play Rock, Scissors, Paper with probabilities $(p_R, p_S, p_P)$ and Player 2 play with probabilities $(q_R, q_S, q_P)$.\n",
    "\n",
    "**Player 1's indifference conditions:**\n",
    "- Expected payoff from Rock = Expected payoff from Scissors = Expected payoff from Paper\n",
    "\n",
    "Player 1's expected payoff from Rock: $0 \\cdot q_R + 1 \\cdot q_S + (-1) \\cdot q_P = q_S - q_P$\n",
    "\n",
    "Player 1's expected payoff from Scissors: $(-1) \\cdot q_R + 0 \\cdot q_S + 1 \\cdot q_P = -q_R + q_P$\n",
    "\n",
    "Player 1's expected payoff from Paper: $1 \\cdot q_R + (-1) \\cdot q_S + 0 \\cdot q_P = q_R - q_S$\n",
    "\n",
    "Setting them equal:\n",
    "- $q_S - q_P = -q_R + q_P$ → $q_R + q_S = 2q_P$\n",
    "- $q_S - q_P = q_R - q_S$ → $2q_S = q_R + q_P$\n",
    "\n",
    "**Player 2's indifference conditions:**\n",
    "Player 2's expected payoff from Rock: $0 \\cdot p_R + (-1) \\cdot p_S + 1 \\cdot p_P = -p_S + p_P$\n",
    "\n",
    "Player 2's expected payoff from Scissors: $1 \\cdot p_R + 0 \\cdot p_S + (-1) \\cdot p_P = p_R - p_P$\n",
    "\n",
    "Player 2's expected payoff from Paper: $(-1) \\cdot p_R + 1 \\cdot p_S + 0 \\cdot p_P = -p_R + p_S$\n",
    "\n",
    "Setting them equal:\n",
    "- $-p_S + p_P = p_R - p_P$ → $p_R + p_S = 2p_P$\n",
    "- $-p_S + p_P = -p_R + p_S$ → $p_R + p_P = 2p_S$\n",
    "\n",
    "**Solving the system:**\n",
    "From the symmetry of the game and the constraint $p_R + p_S + p_P = 1$ and $q_R + q_S + q_P = 1$:\n",
    "\n",
    "The unique solution is: $p_R = p_S = p_P = \\frac{1}{3}$ and $q_R = q_S = q_P = \\frac{1}{3}$\n",
    "\n",
    "**Nash Equilibrium:** Both players play each action with probability $\\frac{1}{3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae8ca8",
   "metadata": {},
   "source": [
    "### 1.2 Modified Rock-Scissors-Paper\n",
    "\n",
    "Now, consider the modified RSP game where the stakes are higher:\n",
    "\n",
    "| Player 1 | Rock | Scissors | Paper |\n",
    "| :--- | :--: | :---: | :---: |\n",
    "| **Rock** | 0, 0 | 1, -1 | -2, 2 |\n",
    "| **Scissors**| -1, 1 | 0, 0 | 3, -3 |\n",
    "| **Paper** | 2, -2 | -3, 3 | 0, 0 |\n",
    "\n",
    "\n",
    "**Your Task:** Like pervious one Derive the mixed-strategy Nash Equilibrium for this modified game.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "For the modified RSP game, let Player 1 play Rock, Scissors, Paper with probabilities $(p_R, p_S, p_P)$ and Player 2 play with probabilities $(q_R, q_S, q_P)$.\n",
    "\n",
    "**Player 1's indifference conditions:**\n",
    "Player 1's expected payoff from Rock: $0 \\cdot q_R + 1 \\cdot q_S + (-2) \\cdot q_P = q_S - 2q_P$\n",
    "\n",
    "Player 1's expected payoff from Scissors: $(-1) \\cdot q_R + 0 \\cdot q_S + 3 \\cdot q_P = -q_R + 3q_P$\n",
    "\n",
    "Player 1's expected payoff from Paper: $2 \\cdot q_R + (-3) \\cdot q_S + 0 \\cdot q_P = 2q_R - 3q_S$\n",
    "\n",
    "Setting them equal:\n",
    "- $q_S - 2q_P = -q_R + 3q_P$ → $q_R + q_S = 5q_P$\n",
    "- $q_S - 2q_P = 2q_R - 3q_S$ → $4q_S = 2q_R + 2q_P$ → $2q_S = q_R + q_P$\n",
    "\n",
    "**Player 2's indifference conditions:**\n",
    "Player 2's expected payoff from Rock: $0 \\cdot p_R + (-1) \\cdot p_S + 2 \\cdot p_P = -p_S + 2p_P$\n",
    "\n",
    "Player 2's expected payoff from Scissors: $1 \\cdot p_R + 0 \\cdot p_S + (-3) \\cdot p_P = p_R - 3p_P$\n",
    "\n",
    "Player 2's expected payoff from Paper: $(-2) \\cdot p_R + 3 \\cdot p_S + 0 \\cdot p_P = -2p_R + 3p_S$\n",
    "\n",
    "Setting them equal:\n",
    "- $-p_S + 2p_P = p_R - 3p_P$ → $p_R + p_S = 5p_P$\n",
    "- $-p_S + 2p_P = -2p_R + 3p_S$ → $2p_R + 2p_P = 4p_S$ → $p_R + p_P = 2p_S$\n",
    "\n",
    "**Solving the system:**\n",
    "From $q_R + q_S + q_P = 1$ and $q_R + q_S = 5q_P$:\n",
    "$5q_P + q_P = 1$ → $q_P = \\frac{1}{6}$\n",
    "\n",
    "From $2q_S = q_R + q_P$ and $q_R + q_S = 5q_P = \\frac{5}{6}$:\n",
    "$2q_S = q_R + \\frac{1}{6}$ and $q_R = \\frac{5}{6} - q_S$\n",
    "\n",
    "Substituting: $2q_S = \\frac{5}{6} - q_S + \\frac{1}{6} = 1 - q_S$\n",
    "$3q_S = 1$ → $q_S = \\frac{1}{3}$\n",
    "\n",
    "Therefore: $q_R = \\frac{5}{6} - \\frac{1}{3} = \\frac{1}{2}$\n",
    "\n",
    "Similarly for Player 1: $p_R = \\frac{1}{2}$, $p_S = \\frac{1}{3}$, $p_P = \\frac{1}{6}$\n",
    "\n",
    "**Nash Equilibrium:** \n",
    "- Player 1: $(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6})$ for (Rock, Scissors, Paper)\n",
    "- Player 2: $(\\frac{1}{2}, \\frac{1}{3}, \\frac{1}{6})$ for (Rock, Scissors, Paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b3956a",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2: Learning by Observation - Fictitious Play (Implementation)\n",
    "\n",
    "Fictitious Play is an intuitive learning algorithm where each agent models its opponent as playing a stationary strategy defined by the historical frequency of their past actions. The agent then plays a **best response** to this belief.\n",
    "\n",
    "### 2.1 Implementation\n",
    "\n",
    "**Your Task:** Implement the `simulate_fictitious_play` function below. It should take the payoff matrices for both players and the number of iterations as input. At each step, each player should choose the action that maximizes their expected payoff given the history of the opponent's plays.\n",
    "\n",
    "**Algorithm:** At each time step $t > 0$, Player $i$ forms a belief that their opponent ($-i$) will play each action $a'$ with a probability equal to its historical frequency. The agent then chooses an action $a_i^*$ that is a best response to this belief.\n",
    "\n",
    "Let $C_{t-1}(a_{-i})$ be the count of times opponent $-i$ has played action $a_{-i}$ up to step $t-1$. Player $i$'s best response is:\n",
    "$$a_{i,t}^* = \\arg\\max_{a_i \\in A_i} \\sum_{a_{-i} \\in A_{-i}} u_i(a_i, a_{-i}) \\cdot \\frac{C_{t-1}(a_{-i})}{t-1}$$\n",
    "\n",
    "**Note on Tie-Breaking:** If multiple actions yield the same maximal expected payoff, your agent should choose one of these best responses uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7c4197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of simulate_fictitious_play function\n",
    "def simulate_fictitious_play(A, B, iterations):\n",
    "    \"\"\"\n",
    "    Simulates Fictitious Play for two players in a normal-form game.\n",
    "\n",
    "    Args:\n",
    "        A (np.ndarray): Payoff matrix for Player 1.\n",
    "        B (np.ndarray): Payoff matrix for Player 2.\n",
    "        iterations (int): The number of rounds to play.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - p1_freq_history (np.ndarray): History of Player 1's action frequencies.\n",
    "            - p2_freq_history (np.ndarray): History of Player 2's action frequencies.\n",
    "    \"\"\"\n",
    "    num_actions = A.shape[0]\n",
    "    \n",
    "    # Initialize action counts\n",
    "    p1_counts = np.zeros(num_actions)\n",
    "    p2_counts = np.zeros(num_actions)\n",
    "    \n",
    "    # Initialize history lists\n",
    "    p1_freq_history = []\n",
    "    p2_freq_history = []\n",
    "    \n",
    "    # Loop for the specified number of iterations\n",
    "    for t in range(iterations):\n",
    "        if t == 0:\n",
    "            # On iteration 0, use a tie-breaking rule (play action 0)\n",
    "            p1_action = 0\n",
    "            p2_action = 0\n",
    "        else:\n",
    "            # Calculate best response to opponent's historical frequencies\n",
    "            # Player 1's best response to Player 2's frequencies\n",
    "            p2_freq = p2_counts / t\n",
    "            p1_expected_payoffs = A @ p2_freq\n",
    "            p1_best_actions = np.where(p1_expected_payoffs == np.max(p1_expected_payoffs))[0]\n",
    "            p1_action = np.random.choice(p1_best_actions)\n",
    "            \n",
    "            # Player 2's best response to Player 1's frequencies\n",
    "            p1_freq = p1_counts / t\n",
    "            p2_expected_payoffs = B.T @ p1_freq\n",
    "            p2_best_actions = np.where(p2_expected_payoffs == np.max(p2_expected_payoffs))[0]\n",
    "            p2_action = np.random.choice(p2_best_actions)\n",
    "        \n",
    "        # Update action counts\n",
    "        p1_counts[p1_action] += 1\n",
    "        p2_counts[p2_action] += 1\n",
    "        \n",
    "        # Periodically record the current action frequencies for plotting\n",
    "        if t > 0:\n",
    "            p1_freq_history.append(p1_counts / (t + 1))\n",
    "            p2_freq_history.append(p2_counts / (t + 1))\n",
    "    \n",
    "    return np.array(p1_freq_history), np.array(p2_freq_history)\n",
    "\n",
    "# --- Payoff Matrices ---\n",
    "A_std = np.array([[0, 1, -1], [-1, 0, 1], [1, -1, 0]])\n",
    "A_mod = np.array([[0, 1, -2], [-1, 0, 3], [2, -3, 0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b4628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 2.2 Analysis\n",
    "# Run simulations for both standard and modified RSP games\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Run simulations\n",
    "print(\"Running Fictitious Play simulation for standard RSP game...\")\n",
    "p1_freq_std, p2_freq_std = simulate_fictitious_play(A_std, -A_std, 1000000)\n",
    "\n",
    "print(\"Running Fictitious Play simulation for modified RSP game...\")\n",
    "p1_freq_mod, p2_freq_mod = simulate_fictitious_play(A_mod, -A_mod, 1000000)\n",
    "\n",
    "# Create plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Standard RSP Game\n",
    "ax1.plot(p1_freq_std[:, 0], label='Rock', alpha=0.8)\n",
    "ax1.plot(p1_freq_std[:, 1], label='Scissors', alpha=0.8)\n",
    "ax1.plot(p1_freq_std[:, 2], label='Paper', alpha=0.8)\n",
    "ax1.axhline(y=1/3, color='red', linestyle='--', alpha=0.7, label='NE (1/3)')\n",
    "ax1.set_title('Standard RSP - Player 1 Action Frequencies')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Action Frequency')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Modified RSP Game\n",
    "ax2.plot(p1_freq_mod[:, 0], label='Rock', alpha=0.8)\n",
    "ax2.plot(p1_freq_mod[:, 1], label='Scissors', alpha=0.8)\n",
    "ax2.plot(p1_freq_mod[:, 2], label='Paper', alpha=0.8)\n",
    "ax2.axhline(y=1/2, color='red', linestyle='--', alpha=0.7, label='NE Rock (1/2)')\n",
    "ax2.axhline(y=1/3, color='orange', linestyle='--', alpha=0.7, label='NE Scissors (1/3)')\n",
    "ax2.axhline(y=1/6, color='green', linestyle='--', alpha=0.7, label='NE Paper (1/6)')\n",
    "ax2.set_title('Modified RSP - Player 1 Action Frequencies')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Action Frequency')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n=== ANALYSIS ===\")\n",
    "print(\"Standard RSP Game:\")\n",
    "print(f\"Final frequencies - Rock: {p1_freq_std[-1, 0]:.4f}, Scissors: {p1_freq_std[-1, 1]:.4f}, Paper: {p1_freq_std[-1, 2]:.4f}\")\n",
    "print(\"Expected NE: (1/3, 1/3, 1/3)\")\n",
    "print(f\"Convergence to NE: {np.allclose(p1_freq_std[-1], [1/3, 1/3, 1/3], atol=0.01)}\")\n",
    "\n",
    "print(\"\\nModified RSP Game:\")\n",
    "print(f\"Final frequencies - Rock: {p1_freq_mod[-1, 0]:.4f}, Scissors: {p1_freq_mod[-1, 1]:.4f}, Paper: {p1_freq_mod[-1, 2]:.4f}\")\n",
    "print(\"Expected NE: (1/2, 1/3, 1/6)\")\n",
    "print(f\"Convergence to NE: {np.allclose(p1_freq_mod[-1], [1/2, 1/3, 1/6], atol=0.01)}\")\n",
    "\n",
    "print(\"\\n=== CONCLUSION ===\")\n",
    "print(\"The action frequencies do converge to the Nash Equilibrium in both games.\")\n",
    "print(\"This demonstrates that Fictitious Play is a no-regret learning algorithm that\")\n",
    "print(\"converges to Nash Equilibrium in zero-sum games.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of simulate_epsilon_greedy_fp function\n",
    "def simulate_epsilon_greedy_fp(A, B, iterations, epsilon):\n",
    "    \"\"\"\n",
    "    Simulates epsilon-greedy Fictitious Play.\n",
    "    \n",
    "    Args:\n",
    "        A (np.ndarray): Payoff matrix for Player 1.\n",
    "        B (np.ndarray): Payoff matrix for Player 2.\n",
    "        iterations (int): The number of rounds to play.\n",
    "        epsilon (float): Probability of exploration (random action).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - p1_freq_history (np.ndarray): History of Player 1's action frequencies.\n",
    "            - p2_freq_history (np.ndarray): History of Player 2's action frequencies.\n",
    "    \"\"\"\n",
    "    num_actions = A.shape[0]\n",
    "    \n",
    "    # Initialize action counts\n",
    "    p1_counts = np.zeros(num_actions)\n",
    "    p2_counts = np.zeros(num_actions)\n",
    "    \n",
    "    # Initialize history lists\n",
    "    p1_freq_history = []\n",
    "    p2_freq_history = []\n",
    "    \n",
    "    # Loop for the specified number of iterations\n",
    "    for t in range(iterations):\n",
    "        if t == 0:\n",
    "            # On iteration 0, use a tie-breaking rule (play action 0)\n",
    "            p1_action = 0\n",
    "            p2_action = 0\n",
    "        else:\n",
    "            # Calculate best response to opponent's historical frequencies\n",
    "            # Player 1's best response to Player 2's frequencies\n",
    "            p2_freq = p2_counts / t\n",
    "            p1_expected_payoffs = A @ p2_freq\n",
    "            p1_best_actions = np.where(p1_expected_payoffs == np.max(p1_expected_payoffs))[0]\n",
    "            p1_best_action = np.random.choice(p1_best_actions)\n",
    "            \n",
    "            # Player 2's best response to Player 1's frequencies\n",
    "            p1_freq = p1_counts / t\n",
    "            p2_expected_payoffs = B.T @ p1_freq\n",
    "            p2_best_actions = np.where(p2_expected_payoffs == np.max(p2_expected_payoffs))[0]\n",
    "            p2_best_action = np.random.choice(p2_best_actions)\n",
    "            \n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.random() < epsilon:\n",
    "                # Explore: choose random action\n",
    "                p1_action = np.random.randint(num_actions)\n",
    "                p2_action = np.random.randint(num_actions)\n",
    "            else:\n",
    "                # Exploit: choose best response\n",
    "                p1_action = p1_best_action\n",
    "                p2_action = p2_best_action\n",
    "        \n",
    "        # Update action counts\n",
    "        p1_counts[p1_action] += 1\n",
    "        p2_counts[p2_action] += 1\n",
    "        \n",
    "        # Periodically record the current action frequencies for plotting\n",
    "        if t > 0:\n",
    "            p1_freq_history.append(p1_counts / (t + 1))\n",
    "            p2_freq_history.append(p2_counts / (t + 1))\n",
    "    \n",
    "    return np.array(p1_freq_history), np.array(p2_freq_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9f4d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 3.2 Analysis\n",
    "# Run epsilon-greedy fictitious play with different epsilon values\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Different epsilon values to test\n",
    "epsilon_values = [0.01, 0.1, 0.3]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "# Create plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, epsilon in enumerate(epsilon_values):\n",
    "    print(f\"Running epsilon-greedy FP with epsilon = {epsilon}...\")\n",
    "    \n",
    "    # Run simulation\n",
    "    p1_freq, p2_freq = simulate_epsilon_greedy_fp(A_mod, -A_mod, 1000000, epsilon)\n",
    "    \n",
    "    # Plot results\n",
    "    ax = axes[i]\n",
    "    ax.plot(p1_freq[:, 0], label='Rock', alpha=0.8, color='blue')\n",
    "    ax.plot(p1_freq[:, 1], label='Scissors', alpha=0.8, color='green')\n",
    "    ax.plot(p1_freq[:, 2], label='Paper', alpha=0.8, color='red')\n",
    "    \n",
    "    # Add Nash Equilibrium lines\n",
    "    ax.axhline(y=1/2, color='blue', linestyle='--', alpha=0.7, label='NE Rock (1/2)')\n",
    "    ax.axhline(y=1/3, color='green', linestyle='--', alpha=0.7, label='NE Scissors (1/3)')\n",
    "    ax.axhline(y=1/6, color='red', linestyle='--', alpha=0.7, label='NE Paper (1/6)')\n",
    "    \n",
    "    ax.set_title(f'ε-Greedy FP (ε = {epsilon}) - Player 1')\n",
    "    ax.set_xlabel('Iteration')\n",
    "    ax.set_ylabel('Action Frequency')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Analysis\n",
    "    print(f\"\\nEpsilon = {epsilon}:\")\n",
    "    print(f\"Final frequencies - Rock: {p1_freq[-1, 0]:.4f}, Scissors: {p1_freq[-1, 1]:.4f}, Paper: {p1_freq[-1, 2]:.4f}\")\n",
    "    print(f\"Expected NE: (1/2, 1/3, 1/6)\")\n",
    "    print(f\"Convergence to NE: {np.allclose(p1_freq[-1], [1/2, 1/3, 1/6], atol=0.05)}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== ANALYSIS ===\")\n",
    "print(\"Impact of epsilon on learning dynamics:\")\n",
    "print(\"- Lower epsilon (0.01): More exploitation, closer to NE convergence\")\n",
    "print(\"- Higher epsilon (0.3): More exploration, further from NE convergence\")\n",
    "print(\"- Exploration prevents exact convergence to NE but maintains learning\")\n",
    "print(\"- The trade-off between exploration and exploitation affects convergence speed and accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d606ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of simulate_regret_matching function\n",
    "def simulate_regret_matching(A, B, iterations):\n",
    "    \"\"\"\n",
    "    Simulates Regret Matching for two players.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - p1_avg_strat_hist (np.ndarray): History of Player 1's average strategy.\n",
    "            - p1_inst_strat_hist (np.ndarray): History of Player 1's instantaneous strategy.\n",
    "    \"\"\"\n",
    "    num_actions = A.shape[0]\n",
    "    \n",
    "    # Initialize regrets, strategy sums, and history lists\n",
    "    p1_regrets = np.zeros(num_actions)\n",
    "    p2_regrets = np.zeros(num_actions)\n",
    "    \n",
    "    p1_strategy_sum = np.zeros(num_actions)\n",
    "    p2_strategy_sum = np.zeros(num_actions)\n",
    "    \n",
    "    p1_avg_strat_hist = []\n",
    "    p1_inst_strat_hist = []\n",
    "    \n",
    "    # Loop for iterations\n",
    "    for t in range(iterations):\n",
    "        # Calculate the current strategy based on positive regrets\n",
    "        # Player 1's strategy\n",
    "        p1_positive_regrets = np.maximum(0, p1_regrets)\n",
    "        p1_regret_sum = np.sum(p1_positive_regrets)\n",
    "        \n",
    "        if p1_regret_sum == 0:\n",
    "            # If sum of positive regrets is 0, play uniformly random\n",
    "            p1_strategy = np.ones(num_actions) / num_actions\n",
    "        else:\n",
    "            p1_strategy = p1_positive_regrets / p1_regret_sum\n",
    "        \n",
    "        # Player 2's strategy\n",
    "        p2_positive_regrets = np.maximum(0, p2_regrets)\n",
    "        p2_regret_sum = np.sum(p2_positive_regrets)\n",
    "        \n",
    "        if p2_regret_sum == 0:\n",
    "            # If sum of positive regrets is 0, play uniformly random\n",
    "            p2_strategy = np.ones(num_actions) / num_actions\n",
    "        else:\n",
    "            p2_strategy = p2_positive_regrets / p2_regret_sum\n",
    "        \n",
    "        # Store the instantaneous strategy and add to the strategy sum\n",
    "        p1_inst_strat_hist.append(p1_strategy.copy())\n",
    "        p1_strategy_sum += p1_strategy\n",
    "        \n",
    "        # Choose actions based on the current strategies\n",
    "        p1_action = np.random.choice(num_actions, p=p1_strategy)\n",
    "        p2_action = np.random.choice(num_actions, p=p2_strategy)\n",
    "        \n",
    "        # Update regrets for ALL actions based on the outcome\n",
    "        # Player 1's regrets\n",
    "        for action in range(num_actions):\n",
    "            p1_regrets[action] += A[action, p2_action] - A[p1_action, p2_action]\n",
    "        \n",
    "        # Player 2's regrets\n",
    "        for action in range(num_actions):\n",
    "            p2_regrets[action] += B[p1_action, action] - B[p1_action, p2_action]\n",
    "        \n",
    "        # Periodically record the average and instantaneous strategies\n",
    "        if t > 0:\n",
    "            p1_avg_strategy = p1_strategy_sum / (t + 1)\n",
    "            p1_avg_strat_hist.append(p1_avg_strategy.copy())\n",
    "    \n",
    "    return np.array(p1_avg_strat_hist), np.array(p1_inst_strat_hist)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ca6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem 4.2 Analysis\n",
    "# Run regret matching simulation and create comparison plots\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Running Regret Matching simulation for modified RSP game...\")\n",
    "p1_avg_strat, p1_inst_strat = simulate_regret_matching(A_mod, -A_mod, 1000000)\n",
    "\n",
    "# Create plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Subplot 1: Instantaneous strategy\n",
    "ax1.plot(p1_inst_strat[:, 0], label='Rock', alpha=0.8, color='blue')\n",
    "ax1.plot(p1_inst_strat[:, 1], label='Scissors', alpha=0.8, color='green')\n",
    "ax1.plot(p1_inst_strat[:, 2], label='Paper', alpha=0.8, color='red')\n",
    "ax1.set_title('Regret Matching - Player 1 Instantaneous Strategy')\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Action Probability')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Subplot 2: Average strategy with NE lines\n",
    "ax2.plot(p1_avg_strat[:, 0], label='Rock', alpha=0.8, color='blue')\n",
    "ax2.plot(p1_avg_strat[:, 1], label='Scissors', alpha=0.8, color='green')\n",
    "ax2.plot(p1_avg_strat[:, 2], label='Paper', alpha=0.8, color='red')\n",
    "\n",
    "# Add Nash Equilibrium lines\n",
    "ax2.axhline(y=1/2, color='blue', linestyle='--', alpha=0.7, label='NE Rock (1/2)')\n",
    "ax2.axhline(y=1/3, color='green', linestyle='--', alpha=0.7, label='NE Scissors (1/3)')\n",
    "ax2.axhline(y=1/6, color='red', linestyle='--', alpha=0.7, label='NE Paper (1/6)')\n",
    "\n",
    "ax2.set_title('Regret Matching - Player 1 Average Strategy')\n",
    "ax2.set_xlabel('Iteration')\n",
    "ax2.set_ylabel('Action Probability')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis\n",
    "print(\"\\n=== ANALYSIS ===\")\n",
    "print(\"Instantaneous Strategy:\")\n",
    "print(f\"Final instantaneous probabilities - Rock: {p1_inst_strat[-1, 0]:.4f}, Scissors: {p1_inst_strat[-1, 1]:.4f}, Paper: {p1_inst_strat[-1, 2]:.4f}\")\n",
    "\n",
    "print(\"\\nAverage Strategy:\")\n",
    "print(f\"Final average probabilities - Rock: {p1_avg_strat[-1, 0]:.4f}, Scissors: {p1_avg_strat[-1, 1]:.4f}, Paper: {p1_avg_strat[-1, 2]:.4f}\")\n",
    "print(\"Expected NE: (1/2, 1/3, 1/6)\")\n",
    "print(f\"Average strategy convergence to NE: {np.allclose(p1_avg_strat[-1], [1/2, 1/3, 1/6], atol=0.01)}\")\n",
    "\n",
    "print(\"\\n=== CONCLUSION ===\")\n",
    "print(\"Key observations:\")\n",
    "print(\"1. The instantaneous strategy oscillates and doesn't converge to NE\")\n",
    "print(\"2. The average strategy converges to the Nash Equilibrium\")\n",
    "print(\"3. This is the expected theoretical outcome for Regret Matching algorithms\")\n",
    "print(\"4. The average strategy convergence is guaranteed by the no-regret property\")\n",
    "print(\"5. Regret Matching ensures that the average strategy approaches NE over time\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f7f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_fictitious_play(A, B, iterations):\n",
    "    \"\"\"\n",
    "    Simulates Fictitious Play for two players in a normal-form game.\n",
    "\n",
    "    Args:\n",
    "        A (np.ndarray): Payoff matrix for Player 1.\n",
    "        B (np.ndarray): Payoff matrix for Player 2.\n",
    "        iterations (int): The number of rounds to play.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - p1_freq_history (np.ndarray): History of Player 1's action frequencies.\n",
    "            - p2_freq_history (np.ndarray): History of Player 2's action frequencies.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    # Initialize action counts and history lists\n",
    "    # Loop for the specified number of iterations\n",
    "    # On iteration 0, use a tie-breaking rule (e.g., play action 0)\n",
    "    # On subsequent iterations, calculate best response to opponent's historical frequencies\n",
    "    # Update action counts\n",
    "    # Periodically record the current action frequencies for plotting\n",
    "    \n",
    "    pass # Replace with your implementation\n",
    "\n",
    "# --- Payoff Matrices ---\n",
    "A_std = np.array([[0, 1, -1], [-1, 0, 1], [1, -1, 0]])\n",
    "A_mod = np.array([[0, 1, -2], [-1, 0, 3], [2, -3, 0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb11ca0c",
   "metadata": {},
   "source": [
    "### 2.2 Analysis\n",
    "\n",
    "**Your Task:**\n",
    "1.  Run your simulation for **1,000,000 iterations** on both the **standard** and **modified** RSP games.\n",
    "2.  Generate two plots, one for each game. Each plot should show the evolution of Players action frequencies over time and include horizontal lines indicating the theoretical NE probabilities you calculated in Problem 1.\n",
    "3.  **Analyze your results:** Do the action frequencies converge? If so, do they converge to the Nash Equilibrium? Explain the observed behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4be41da",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 3: Fictitious Play with Exploration (Implementation)\n",
    "\n",
    "Our Fictitious Play agent is purely exploitative. In Reinforcement Learning, we know the importance of the **exploration-exploitation tradeoff**. Let's create an $\\epsilon$-greedy version of Fictitious Play.\n",
    "\n",
    "### 3.1 Implementation\n",
    "\n",
    "**Your Task:** Create a new function, `simulate_epsilon_greedy_fp`. This function should be similar to your Fictitious Play implementation but include an `epsilon` parameter. At each step, with probability `epsilon`, the agent should choose a random action (explore). With probability `1-epsilon`, it should play the best response (exploit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2cf316",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_epsilon_greedy_fp(A, B, iterations, epsilon):\n",
    "    \"\"\"\n",
    "    Simulates epsilon-greedy Fictitious Play.\n",
    "    \"\"\"\n",
    "    ### YOUR CODE HERE ###\n",
    "    \n",
    "    pass # Replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7024d5",
   "metadata": {},
   "source": [
    "\n",
    "### 3.2 Analysis\n",
    "\n",
    "**Your Task:**\n",
    "1.  Run the `simulate_epsilon_greedy_fp` function on the **modified** RSP game for **1,000,000 iterations** with three different `epsilon` values: `0.01`, `0.1`, and `0.3`.\n",
    "2.  Plot the results for each simulation.\n",
    "3.  **Analyze your results:** How does `epsilon` affect the learning dynamics? Does the agent's strategy still converge to the NE? If not, to what does it converge? Discuss the impact of exploration in this multi-agent context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43626e0a",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 4: Learning from \"What If\" - Regret Matching (Implementation & Theory)\n",
    "\n",
    "Regret Matching is a powerful no-regret learning algorithm. Instead of playing a best response to history, an agent's probability of choosing an action is proportional to the positive **regret** for not having chosen that action in the past. The key property of regret matching is that the **average strategy** over time converges to a Nash Equilibrium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9351241",
   "metadata": {},
   "source": [
    "### 4.1 Implementation\n",
    "\n",
    "**Your Task:** Implement the `simulate_regret_matching` function below.\n",
    "\n",
    "**Algorithm:** Regret Matching works in two steps. First, update the cumulative regrets. Second, determine the next round's strategy.\n",
    "\n",
    "1.  **Regret Calculation:** After playing action $a_i$ against opponent's action $a_{-i}$, the cumulative regret $R_t(s)$ for *not* having played action $s \\in A_i$ is updated as follows:\n",
    "    $$R_t(s) = R_{t-1}(s) + u_i(s, a_{-i}) - u_i(a_i, a_{-i})$$\n",
    "\n",
    "2.  **Strategy Calculation:** The probability of playing action $s$ in the next round is proportional to its positive cumulative regret, $R_t^+(s) = \\max(0, R_t(s))$.\n",
    "    $$p_{t+1}(s) = \\frac{R_t^+(s)}{\\sum_{s' \\in A_i} R_t^+(s')}$$\n",
    "    If the sum of positive regrets is zero, play uniformly at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d97ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_regret_matching(A, B, iterations):\n",
    "    \"\"\"\n",
    "    Simulates Regret Matching for two players.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - p1_avg_strat_hist (np.ndarray): History of Player 1's average strategy.\n",
    "            - p1_inst_strat_hist (np.ndarray): History of Player 1's instantaneous strategy.\n",
    "    \"\"\"\n",
    "    num_actions = A.shape[0]\n",
    "    \n",
    "    ### YOUR CODE HERE ###\n",
    "    # Initialize regrets, strategy sums, and history lists\n",
    "    # Loop for iterations\n",
    "    # Calculate the current strategy based on positive regrets\n",
    "    #   (If sum of positive regrets is 0, play uniformly random)\n",
    "    # Store the instantaneous strategy and add to the strategy sum\n",
    "    # Choose actions based on the current strategies\n",
    "    # Update regrets for ALL actions based on the outcome\n",
    "    # Periodically record the average and instantaneous strategies\n",
    "    \n",
    "    pass # Replace with your implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdfb79",
   "metadata": {},
   "source": [
    "\n",
    "### 4.2 Analysis\n",
    "\n",
    "**Your Task:**\n",
    "1.  Run your simulation for the **modified** RSP game for **1,000,000 iterations**.\n",
    "2.  Generate a single figure with two subplots:\n",
    "    * **Subplot 1:** Plot the **instantaneous strategy** of Player 1 over time.\n",
    "    * **Subplot 2:** Plot the **average strategy** of Player 1 over time. Include horizontal lines for the NE.\n",
    "3.  **Analyze your results:** Compare the two plots. Which one converges to the Nash Equilibrium? \\\n",
    "                              (Bonus): Explain why this is the expected theoretical outcome for Regret Matching algorithms."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
