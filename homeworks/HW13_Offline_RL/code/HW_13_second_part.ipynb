{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9PAzBIOgimq"
      },
      "source": [
        "# Assignment: Implementing MADDPG with the TorchRL Toolkit\n",
        "In this task, you’ll implement the MADDPG algorithm — a method used to train multiple agents to learn and collaborate effectively.\n",
        "\n",
        "To make things smoother, we’ll be using TorchRL, a library that simplifies building and training RL agents.\n",
        "\n",
        "The assignment has two main goals:\n",
        "\n",
        "1. Help you understand the key ideas behind MADDPG, especially the idea of centralized training (agents learn together) and decentralized execution (they act independently).\n",
        "\n",
        "2. Introduce you to important TorchRL.\n",
        "\n",
        "We’ve already set up the basic structure for you. Your job is to complete the missing pieces marked as TODOs. Before each coding step, we’ll explain what the MADDPG concept is and how to apply it using the right TorchRL tools."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5sNEpyyr2qw"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip3 install torchrl\n",
        "!pip3 install vmas\n",
        "!pip3 install tqdm\n",
        "!apt-get update -y\n",
        "!apt-get install -y x11-utils xvfb python3-opengl libgl1-mesa-glx libglu1-mesa\n",
        "!pip install pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7WYX8P0YGGy"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndJCvEKxX7IT"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import tempfile\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from matplotlib import pyplot as plt\n",
        "import pyvirtualdisplay\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from tensordict import TensorDictBase\n",
        "from tensordict.nn import TensorDictModule, TensorDictSequential\n",
        "from torch import multiprocessing\n",
        "from torchrl.collectors import SyncDataCollector\n",
        "from torchrl.data import LazyMemmapStorage, RandomSampler, ReplayBuffer\n",
        "from torchrl.envs import (\n",
        "    check_env_specs,\n",
        "    RewardSum,\n",
        "    TransformedEnv,\n",
        "    VmasEnv,\n",
        ")\n",
        "from torchrl.modules import (\n",
        "    AdditiveGaussianModule,\n",
        "    MLP,\n",
        "    ProbabilisticActor,\n",
        "    TanhDelta,\n",
        ")\n",
        "from torchrl.objectives import DDPGLoss, SoftUpdate, ValueEstimators\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- Setup Virtual Display ---\n",
        "try:\n",
        "    display = pyvirtualdisplay.Display(visible=False, size=(1400, 900))\n",
        "    display.start()\n",
        "    print(\"Virtual display started.\")\n",
        "except Exception as e:\n",
        "    print(f\"Could not start virtual display: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTDTHy_qYbZj"
      },
      "source": [
        "##Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLxkb6VXYKuu"
      },
      "outputs": [],
      "source": [
        "# General\n",
        "seed = 0\n",
        "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
        "device = torch.device(0) if torch.cuda.is_available() and not is_fork else torch.device(\"cpu\")\n",
        "torch.manual_seed(seed)\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Vmas Environment\n",
        "scenario_name = \"navigation\"\n",
        "n_agents = 3\n",
        "max_steps = 100  # Episode steps before done\n",
        "\n",
        "# Sampling\n",
        "frames_per_batch = 2000\n",
        "n_iters = 1500\n",
        "total_frames = frames_per_batch * n_iters\n",
        "num_vmas_envs = frames_per_batch // max_steps\n",
        "\n",
        "# Replay Buffer\n",
        "memory_size = 2_000_000\n",
        "\n",
        "# Training\n",
        "n_optimiser_steps = 10\n",
        "train_batch_size = 512\n",
        "lr = 3e-4\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "# DDPG Algorithm\n",
        "gamma = 0.99\n",
        "polyak_tau = 0.005"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHe9QGlZYflY"
      },
      "source": [
        "##Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8yKRbusYkIP"
      },
      "outputs": [],
      "source": [
        "# Each agent is in its own group, so group_name == agent_name\n",
        "custom_group_map = {f\"agent_{i}\": [f\"agent_{i}\"] for i in range(n_agents)}\n",
        "\n",
        "# Create the vectorized Vmas environment\n",
        "env = VmasEnv(\n",
        "    scenario=scenario_name,\n",
        "    num_envs=num_vmas_envs,\n",
        "    continuous_actions=True,\n",
        "    max_steps=max_steps,\n",
        "    device=device,\n",
        "    n_agents=n_agents,\n",
        "    group_map=custom_group_map,\n",
        ")\n",
        "\n",
        "# Wrap the environment to sum rewards for each agent group\n",
        "env = TransformedEnv(\n",
        "    env,\n",
        "    RewardSum(\n",
        "        in_keys=env.reward_keys,\n",
        "        reset_keys=[\"_reset\"] * len(env.group_map.keys()),\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Print environment specs\n",
        "print(f\"group_map: {env.group_map}\")\n",
        "print(\"action_spec:\", env.full_action_spec)\n",
        "print(\"reward_spec:\", env.full_reward_spec)\n",
        "print(\"done_spec:\", env.full_done_spec)\n",
        "print(\"observation_spec:\", env.observation_spec)\n",
        "\n",
        "check_env_specs(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7LQITA1g1cP"
      },
      "source": [
        "##Part 1 : Decentralized Actor\n",
        "\n",
        "**1a. MADDPG Concept: The Agent's Brain**\n",
        "\n",
        "In MADDPG, each agent has its own independent \"actor\" network. This network takes the agent's observation and decides which action to take. It's the \"decentralized execution\" part of the algorithm.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Implement the Actor Network using torchrl.modules.MLP. This will be a standard PyTorch nn.Module that serves as the brain for a single agent.\n",
        "\n",
        "**1b. TorchRL vs TensorDictModule**\n",
        "\n",
        "A standard nn.Module doesn't know how to interact with TorchRL's data structures. We need to wrap it with a TensorDictModule. This wrapper acts as an adapter, telling your MLP which data \"key\" to read its input from in the TensorDict and which \"key\" to write its output to.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Wrap your AgentMLP in a TensorDictModule."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxXuM9FgYn8g"
      },
      "source": [
        "##Policy Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SkGNtr-YznQ"
      },
      "outputs": [],
      "source": [
        "# Part 1: Create the Actor Network using torchrl.modules.MLP\n",
        "policy_modules = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    agent_modules = {}\n",
        "    for agent in agents:\n",
        "        ### TODO: PART 1a ###\n",
        "        # Create an actor network using `torchrl.modules.MLP`.\n",
        "        # - `in_features`: The dimension of the agent's observation.\n",
        "        # - `out_features`: The dimension of the agent's action.\n",
        "        # - `num_cells`: A list defining the hidden layer sizes (e.g., [256, 256]).\n",
        "        # - `activation_class`: The activation function (e.g., nn.ReLU).\n",
        "        ### YOUR CODE HERE ###\n",
        "        obs_dim = env.observation_spec[agent, \"observation\"].shape[-1]\n",
        "        action_dim = env.full_action_spec[agent, \"action\"].shape[-1]\n",
        "        agent_modules[agent] = MLP(\n",
        "            in_features=obs_dim,\n",
        "            out_features=action_dim,\n",
        "            num_cells=[256, 256],\n",
        "            activation_class=nn.ReLU\n",
        "        )\n",
        "\n",
        "    ### TODO: PART 1b ###\n",
        "    # Wrap the MLP actor in a TensorDictModule to handle I/O.\n",
        "    # - The input should be the agent's observation: `(agent, \"observation\")`.\n",
        "    # - The output should be the action parameters: `(agent, \"param\")`.\n",
        "    ### YOUR CODE HERE ###\n",
        "    agent_policy_modules = {}\n",
        "    for agent in agents:\n",
        "        agent_policy_modules[agent] = TensorDictModule(\n",
        "            agent_modules[agent],\n",
        "            in_keys=[(agent, \"observation\")],\n",
        "            out_keys=[(agent, \"param\")]\n",
        "        )\n",
        "    policy_modules[group] = TensorDictSequential(*agent_policy_modules.values())\n",
        "\n",
        "\n",
        "# Create Probabilistic Policies\n",
        "policies = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    agent_policies = []\n",
        "    for agent in agents:\n",
        "        agent_policies.append(\n",
        "            ProbabilisticActor(\n",
        "                module=policy_modules[group],\n",
        "                spec=env.full_action_spec[agent, \"action\"],\n",
        "                in_keys=[(agent, \"param\")],\n",
        "                out_keys=[(agent, \"action\")],\n",
        "                distribution_class=TanhDelta,\n",
        "                distribution_kwargs={\n",
        "                    \"low\": env.full_action_spec_unbatched[agent, \"action\"].space.low,\n",
        "                    \"high\": env.full_action_spec_unbatched[agent, \"action\"].space.high,\n",
        "                },\n",
        "                return_log_prob=False,\n",
        "            )\n",
        "        )\n",
        "    policies[group] = TensorDictSequential(*agent_policies)\n",
        "\n",
        "# Create Target Policies for DDPG\n",
        "target_policies = copy.deepcopy(policies)\n",
        "\n",
        "# Create Exploration Policies: An AdditiveGaussianModule is appended to the policy to add noise for exploration\n",
        "exploration_policies = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    first_actor = None\n",
        "    for module in policies[group].modules():\n",
        "        if isinstance(module, ProbabilisticActor):\n",
        "            first_actor = module\n",
        "            break\n",
        "    if first_actor is None:\n",
        "        raise RuntimeError(\"No ProbabilisticActor found in policies[group]\")\n",
        "\n",
        "    exploration_policy = TensorDictSequential(\n",
        "        policies[group],\n",
        "        AdditiveGaussianModule(\n",
        "            spec=first_actor.spec,\n",
        "            annealing_num_steps=total_frames // 3,\n",
        "            action_key=(group, \"action\"),\n",
        "            sigma_init=0.5,\n",
        "            sigma_end=0.05,\n",
        "        ),\n",
        "    )\n",
        "    exploration_policies[group] = exploration_policy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI4b4gE2iXaV"
      },
      "source": [
        "##Part 2: The Centralized Critic\n",
        "\n",
        "**2a. MADDPG Concept: Using Global Information**\n",
        "\n",
        "One of the key strengths of the MADDPG algorithm lies in its use of a centralized critic during training. Unlike the decentralized actors, which only have access to their individual observations, the centralized critic has access to the observations and actions of all agents. This broader perspective enables it to more accurately evaluate the quality of joint actions taken by the agents, which in turn leads to more stable and cooperative learning dynamics.\n",
        "\n",
        "Your task:\n",
        "\n",
        "Implement the CentralizedCritic using torchrl.modules.MLP. This network takes observations and actions of all agents, and return a scalar value representing the estimated state-action value. It's the \"centralized training\" part of the algorithm.\n",
        "\n",
        "**2b. TorchRL Tool: Assembling Inputs with TensorDictModule**\n",
        "\n",
        "How do we collect data from many different keys and feed it as one tensor to our critic? TensorDictModule can do more than just wrap a network; it can also perform operations. We can give it a list of in_keys and a lambda function to tell it how to combine them.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "Create a TensorDictModule that gathers all observations and actions and concatenates them into a single tensor for the critic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpcuMBejirJc"
      },
      "source": [
        "##Critic Network Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqaa0g_LlLmJ"
      },
      "outputs": [],
      "source": [
        "# Part 2: Create the Centralized Critic using torchrl.modules.MLP\n",
        "critics = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    ### TODO: PART 2a ###\n",
        "    # Create the centralized critic network using `torchrl.modules.MLP`.\n",
        "    # - First, calculate `in_features`.\n",
        "    # - `out_features` should be 1, as the critic outputs a single Q-value.\n",
        "    ### YOUR CODE HERE ###\n",
        "    agent_critic_modules = {}\n",
        "    for agent in agents:\n",
        "        # Calculate total input features: all observations + all actions\n",
        "        total_obs_dim = sum(env.observation_spec[other_agent, \"observation\"].shape[-1] \n",
        "                           for other_agent in env.group_map.keys())\n",
        "        total_action_dim = sum(env.full_action_spec[other_agent, \"action\"].shape[-1] \n",
        "                              for other_agent in env.group_map.keys())\n",
        "        critic_in_features = total_obs_dim + total_action_dim\n",
        "        \n",
        "        agent_critic_modules[agent] = MLP(\n",
        "            in_features=critic_in_features,\n",
        "            out_features=1,\n",
        "            num_cells=[256, 256],\n",
        "            activation_class=nn.ReLU\n",
        "        )\n",
        "\n",
        "    ### TODO: PART 2b ###\n",
        "    # Wire up the critic. This involves creating a `cat_module` that\n",
        "    # concatenates all agent observations and actions into a single tensor.\n",
        "    ### YOUR CODE HERE ###\n",
        "    agent_critic_tdmodules = {}\n",
        "    for agent in agents:\n",
        "        # 1. Define the `cat_inputs` list for concatenation.\n",
        "        cat_inputs = []\n",
        "        # Add all observations\n",
        "        for other_agent in env.group_map.keys():\n",
        "            cat_inputs.append((other_agent, \"observation\"))\n",
        "        # Add all actions\n",
        "        for other_agent in env.group_map.keys():\n",
        "            cat_inputs.append((other_agent, \"action\"))\n",
        "\n",
        "        # 2. Create the `cat_module` using TensorDictModule and a lambda function.\n",
        "        cat_module = TensorDictModule(\n",
        "            lambda *tensors: torch.cat(tensors, dim=-1),\n",
        "            in_keys=cat_inputs,\n",
        "            out_keys=[(agent, \"obs_actions\")]\n",
        "        )\n",
        "\n",
        "        critic_module = TensorDictModule(\n",
        "            agent_critic_modules[agent],\n",
        "            in_keys=[(agent, \"obs_actions\")], # Must match cat_module's out_key\n",
        "            out_keys=[(agent, \"state_action_value\")],\n",
        "        )\n",
        "        agent_critic_tdmodules[agent] = TensorDictSequential(cat_module, critic_module)\n",
        "    critics[group] = TensorDictSequential(*agent_critic_tdmodules.values())\n",
        "\n",
        "print(\"Model and policy structure ready for review.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voH8phhhllef"
      },
      "source": [
        "##Part 3: The Learning Algorithm\n",
        "\n",
        "**3a. MADDPG Concept: The Update Step**\n",
        "\n",
        "The critic learns by comparing its Q-value prediction to a \"target\" value calculated from the reward and the next state's value. The actor then learns by performing gradient ascent to find actions that the critic scores highly.\n",
        "\n",
        "**3b. TorchRL Tool: DDPGLoss**\n",
        "\n",
        "This high-level module encapsulates the entire loss calculation for both the actor and the critic. You provide your networks, and it computes the gradients. Your only job is to tell it which keys to use for its calculations. You must also supply the actions from the target policies, as these are used to calculate the target Q-value, which is a key part of the DDPG algorithm.\n",
        "\n",
        "Your Task:\n",
        "\n",
        "1.   Configure the DDPGLoss module with the correct keys.\n",
        "2.   In the main training loop, provide the target actions needed for the loss calculation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3Q2kJQmZF5V"
      },
      "source": [
        "##Replay Buffer, Losses, and Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7n5mZTqNY8Ww"
      },
      "outputs": [],
      "source": [
        "# Part 3\n",
        "# Shared Replay Buffer\n",
        "shared_replay_buffer = ReplayBuffer(\n",
        "    storage=LazyMemmapStorage(memory_size, scratch_dir=tempfile.TemporaryDirectory().name),\n",
        "    sampler=RandomSampler(),\n",
        "    batch_size=train_batch_size,\n",
        ")\n",
        "if device.type != \"cpu\":\n",
        "    shared_replay_buffer.append_transform(lambda x: x.to(device))\n",
        "\n",
        "# DDPG Losses\n",
        "losses = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    loss_module = DDPGLoss(\n",
        "        actor_network=policies[group],\n",
        "        value_network=critics[group],\n",
        "        delay_value=True,\n",
        "        delay_actor=True,\n",
        "        loss_function=\"l2\",\n",
        "    )\n",
        "    ### TODO: PART 3a ###\n",
        "    # Use `loss_module.set_keys(...)` to map the tensor names to what the\n",
        "    # loss function expects. You must map \"reward\", \"done\", \"terminated\",\n",
        "    # and the output of your critic: \"state_action_value\".\n",
        "    ### YOUR CODE HERE ###\n",
        "    loss_module.set_keys(\n",
        "        reward=(group, \"reward\"),\n",
        "        done=(group, \"done\"),\n",
        "        terminated=(group, \"terminated\"),\n",
        "        value=(group, \"state_action_value\")\n",
        "    )\n",
        "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
        "    losses[group] = loss_module\n",
        "\n",
        "# Target Network Updaters and Optimizers\n",
        "target_updaters = {group: SoftUpdate(loss, tau=polyak_tau) for group, loss in losses.items()}\n",
        "optimisers = {\n",
        "    group: {\n",
        "        \"loss_actor\": torch.optim.Adam(loss.actor_network_params.flatten_keys().values(), lr=1e-4),\n",
        "        \"loss_value\": torch.optim.Adam(loss.value_network_params.flatten_keys().values(), lr=3e-4),\n",
        "    }\n",
        "    for group, loss in losses.items()\n",
        "}\n",
        "print(\"Losses, optimizers, and replay buffer are ready.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvEy6IURZMd9"
      },
      "source": [
        "##Data Collection and Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6m1yO0iZQQb"
      },
      "outputs": [],
      "source": [
        "# Data Collection and Training\n",
        "collector = SyncDataCollector(\n",
        "    env,\n",
        "    TensorDictSequential(*exploration_policies.values()),\n",
        "    device=device,\n",
        "    frames_per_batch=frames_per_batch,\n",
        "    total_frames=total_frames,\n",
        ")\n",
        "\n",
        "\n",
        "def process_batch(batch: TensorDictBase) -> TensorDictBase:\n",
        "    for group in env.group_map.keys():\n",
        "        keys = list(batch.keys(True, True))\n",
        "        group_shape = batch.get_item_shape(group)\n",
        "        nested_done_key = (\"next\", group, \"done\")\n",
        "        nested_terminated_key = (\"next\", group, \"terminated\")\n",
        "        if nested_done_key not in keys:\n",
        "            batch.set(\n",
        "                nested_done_key,\n",
        "                batch.get((\"next\", \"done\")).unsqueeze(-1).expand((*group_shape, 1)),\n",
        "            )\n",
        "        if nested_terminated_key not in keys:\n",
        "            batch.set(\n",
        "                nested_terminated_key,\n",
        "                batch.get((\"next\", \"terminated\"))\n",
        "                .unsqueeze(-1)\n",
        "                .expand((*group_shape, 1)),\n",
        "            )\n",
        "    return batch\n",
        "\n",
        "# Training Loop\n",
        "episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
        "pbar = tqdm(total=n_iters, desc=\"Training Progress\")\n",
        "\n",
        "for iteration, batch in enumerate(collector):\n",
        "    current_frames = batch.numel()\n",
        "    batch = process_batch(batch)\n",
        "    shared_replay_buffer.extend(batch.reshape(-1))\n",
        "\n",
        "    for group in env.group_map.keys():\n",
        "        for _ in range(n_optimiser_steps):\n",
        "            subdata = shared_replay_buffer.sample()\n",
        "\n",
        "            # --- Part 3b: Compute Target Actions for the Critic's Loss ---\n",
        "            with torch.no_grad():\n",
        "                next_td = subdata.get(\"next\")\n",
        "                ### TODO: PART 3b ###\n",
        "                # The DDPG loss needs to know what the *target* policies would do in\n",
        "                # the next state. Loop through all agent groups, run their `target_policies`\n",
        "                # on `next_td`, and store the resulting action under the key `(\"next\", other_group, \"action\")`.\n",
        "                ### YOUR CODE HERE ###\n",
        "                for other_group in env.group_map.keys():\n",
        "                    next_td = target_policies[other_group](next_td)\n",
        "\n",
        "\n",
        "            loss_vals = losses[group](subdata)\n",
        "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
        "                loss = loss_vals[loss_name]\n",
        "                optimiser = optimisers[group][loss_name]\n",
        "                loss.backward()\n",
        "                params = optimiser.param_groups[0][\"params\"]\n",
        "                torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
        "                optimiser.step()\n",
        "                optimiser.zero_grad()\n",
        "            target_updaters[group].step()\n",
        "        exploration_policies[group][-1].step(current_frames)\n",
        "\n",
        "    for group in env.group_map.keys():\n",
        "        episode_reward_mean = (\n",
        "            batch.get((\"next\", group, \"episode_reward\"))[\n",
        "                batch.get((\"next\", group, \"done\"))\n",
        "            ]\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        episode_reward_mean_map[group].append(episode_reward_mean)\n",
        "\n",
        "    reward_strings = [\n",
        "        f\"{group}: {episode_reward_mean_map[group][-1]:.2f}\"\n",
        "        for group in env.group_map.keys()\n",
        "    ]\n",
        "    description = (\n",
        "        f\"Iter [{iteration+1}/{n_iters}] | Rewards: \" + \" | \".join(reward_strings)\n",
        "    )\n",
        "    pbar.set_description(description)\n",
        "    pbar.refresh()\n",
        "\n",
        "pbar.close()\n",
        "collector.shutdown()\n",
        "print(\"\\nTraining finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_eoTvo8ZZlS"
      },
      "source": [
        "##Plotting Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOuRkqpMZU_A"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(n_agents, 1, figsize=(10, 8), sharex=True)\n",
        "if n_agents == 1:\n",
        "    axs = [axs]\n",
        "for i, group in enumerate(env.group_map.keys()):\n",
        "    axs[i].plot(episode_reward_mean_map[group], label=f\"Episode reward mean {group}\")\n",
        "    axs[i].set_ylabel(\"Reward\")\n",
        "    axs[i].legend()\n",
        "axs[-1].set_xlabel(\"Training iterations\")\n",
        "fig.suptitle(\"Training Rewards\")\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MW6GGlkZjsr"
      },
      "source": [
        "##Evaluation and Rendering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWsThylfZmQG"
      },
      "outputs": [],
      "source": [
        "print(\"Starting evaluation and rendering...\")\n",
        "\n",
        "# Create a single environment for rendering\n",
        "render_env = VmasEnv(\n",
        "    scenario=scenario_name,\n",
        "    num_envs=1,\n",
        "    continuous_actions=True,\n",
        "    max_steps=max_steps,\n",
        "    device=device,\n",
        "    n_agents=n_agents,\n",
        "    group_map=custom_group_map,\n",
        ")\n",
        "\n",
        "td = render_env.reset()\n",
        "frames = []\n",
        "\n",
        "# Rollout Loop\n",
        "with torch.no_grad():\n",
        "    for _ in range(max_steps):\n",
        "        # 1. Run policies to get actions\n",
        "        for group in render_env.group_map.keys():\n",
        "            td = policies[group](td)\n",
        "\n",
        "        # 2. Step the environment\n",
        "        td_next = render_env.step(td)\n",
        "\n",
        "        # 3. Use the new observation for the next policy call\n",
        "        td = td_next.get(\"next\").clone()\n",
        "\n",
        "        # 4. Reset if the episode terminated\n",
        "        if td_next.get(\"done\").item():\n",
        "            td = render_env.reset()\n",
        "\n",
        "        # 5. Render the frame and append to list\n",
        "        frame = render_env.render(mode=\"rgb_array\")\n",
        "        frames.append(Image.fromarray(frame))\n",
        "\n",
        "# Save the rollout as a GIF\n",
        "gif_path = f\"{scenario_name}_evaluation.gif\"\n",
        "frames[0].save(\n",
        "    gif_path,\n",
        "    save_all=True,\n",
        "    append_images=frames[1:],\n",
        "    duration=100,\n",
        "    loop=0,\n",
        ")\n",
        "print(f\"✅ Saved animation as {gif_path}\")\n",
        "\n",
        "# To display the GIF in a Jupyter notebook, you can use the following:\n",
        "# from IPython.display import Image as IPImage\n",
        "# IPImage(filename=gif_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAqmEiTZQE5n"
      },
      "source": [
        "##Part 4\n",
        "\n",
        "Based on your understanding of the differences between MADDPG and IDDPG, modify the necessary sections of your MADDPG implementation to convert it into its independent variant (IDDPG). Run the modified code, clearly explain the changes you made and the rationale behind each modification, and finally analyze and discuss the differences you observe between the performance of MADDPG and IDDPG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Part 4: IDDPG Implementation and Analysis\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"PART 4: IMPLEMENTING IDDPG (Independent DDPG)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "\"\"\"\n",
        "IDDPG vs MADDPG Key Differences:\n",
        "1. MADDPG: Centralized critic uses all agents' observations and actions\n",
        "2. IDDPG: Each agent has its own independent critic that only uses its own observation and action\n",
        "\n",
        "The main change is in the critic network - instead of using global information,\n",
        "each agent's critic only sees its own observation and action.\n",
        "\"\"\"\n",
        "\n",
        "# Create IDDPG Critics (Independent Critics)\n",
        "iddpg_critics = {}\n",
        "for group, agents in env.group_map.items():\n",
        "    agent_critic_modules = {}\n",
        "    for agent in agents:\n",
        "        # IDDPG: Only use this agent's observation and action\n",
        "        obs_dim = env.observation_spec[agent, \"observation\"].shape[-1]\n",
        "        action_dim = env.full_action_spec[agent, \"action\"].shape[-1]\n",
        "        critic_in_features = obs_dim + action_dim\n",
        "        \n",
        "        agent_critic_modules[agent] = MLP(\n",
        "            in_features=critic_in_features,\n",
        "            out_features=1,\n",
        "            num_cells=[256, 256],\n",
        "            activation_class=nn.ReLU\n",
        "        )\n",
        "\n",
        "    agent_critic_tdmodules = {}\n",
        "    for agent in agents:\n",
        "        # IDDPG: Only concatenate this agent's observation and action\n",
        "        cat_inputs = [(agent, \"observation\"), (agent, \"action\")]\n",
        "        \n",
        "        cat_module = TensorDictModule(\n",
        "            lambda *tensors: torch.cat(tensors, dim=-1),\n",
        "            in_keys=cat_inputs,\n",
        "            out_keys=[(agent, \"obs_actions\")]\n",
        "        )\n",
        "\n",
        "        critic_module = TensorDictModule(\n",
        "            agent_critic_modules[agent],\n",
        "            in_keys=[(agent, \"obs_actions\")],\n",
        "            out_keys=[(agent, \"state_action_value\")],\n",
        "        )\n",
        "        agent_critic_tdmodules[agent] = TensorDictSequential(cat_module, critic_module)\n",
        "    iddpg_critics[group] = TensorDictSequential(*agent_critic_tdmodules.values())\n",
        "\n",
        "print(\"IDDPG critics created successfully!\")\n",
        "print(\"\\nKey Changes Made:\")\n",
        "print(\"1. Each critic now only uses its own agent's observation and action\")\n",
        "print(\"2. Input dimension reduced from global state to local state\")\n",
        "print(\"3. No information sharing between agents' critics\")\n",
        "\n",
        "# Create IDDPG Losses\n",
        "iddpg_losses = {}\n",
        "for group, _agents in env.group_map.items():\n",
        "    loss_module = DDPGLoss(\n",
        "        actor_network=policies[group],\n",
        "        value_network=iddpg_critics[group],\n",
        "        delay_value=True,\n",
        "        delay_actor=True,\n",
        "        loss_function=\"l2\",\n",
        "    )\n",
        "    loss_module.set_keys(\n",
        "        reward=(group, \"reward\"),\n",
        "        done=(group, \"done\"),\n",
        "        terminated=(group, \"terminated\"),\n",
        "        value=(group, \"state_action_value\")\n",
        "    )\n",
        "    loss_module.make_value_estimator(ValueEstimators.TD0, gamma=gamma)\n",
        "    iddpg_losses[group] = loss_module\n",
        "\n",
        "# Create IDDPG Target Networks\n",
        "iddpg_target_critics = copy.deepcopy(iddpg_critics)\n",
        "\n",
        "# Create IDDPG Optimizers\n",
        "iddpg_optimisers = {\n",
        "    group: {\n",
        "        \"loss_actor\": torch.optim.Adam(loss.actor_network_params.flatten_keys().values(), lr=1e-4),\n",
        "        \"loss_value\": torch.optim.Adam(loss.value_network_params.flatten_keys().values(), lr=3e-4),\n",
        "    }\n",
        "    for group, loss in iddpg_losses.items()\n",
        "}\n",
        "\n",
        "# Create IDDPG Target Updaters\n",
        "iddpg_target_updaters = {group: SoftUpdate(loss, tau=polyak_tau) for group, loss in iddpg_losses.items()}\n",
        "\n",
        "print(\"\\nIDDPG setup complete! Ready for training comparison.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IDDPG Training Comparison\n",
        "print(\"Starting IDDPG training for comparison...\")\n",
        "\n",
        "# Create a new replay buffer for IDDPG\n",
        "iddpg_replay_buffer = ReplayBuffer(\n",
        "    storage=LazyMemmapStorage(memory_size, scratch_dir=tempfile.TemporaryDirectory().name),\n",
        "    sampler=RandomSampler(),\n",
        "    batch_size=train_batch_size,\n",
        ")\n",
        "if device.type != \"cpu\":\n",
        "    iddpg_replay_buffer.append_transform(lambda x: x.to(device))\n",
        "\n",
        "# IDDPG Training Loop (shorter for comparison)\n",
        "iddpg_episode_reward_mean_map = {group: [] for group in env.group_map.keys()}\n",
        "iddpg_pbar = tqdm(total=500, desc=\"IDDPG Training Progress\")  # Shorter training for comparison\n",
        "\n",
        "for iteration, batch in enumerate(collector):\n",
        "    if iteration >= 500:  # Stop early for comparison\n",
        "        break\n",
        "        \n",
        "    current_frames = batch.numel()\n",
        "    batch = process_batch(batch)\n",
        "    iddpg_replay_buffer.extend(batch.reshape(-1))\n",
        "\n",
        "    for group in env.group_map.keys():\n",
        "        for _ in range(n_optimiser_steps):\n",
        "            subdata = iddpg_replay_buffer.sample()\n",
        "\n",
        "            # Compute Target Actions for IDDPG (same as MADDPG)\n",
        "            with torch.no_grad():\n",
        "                next_td = subdata.get(\"next\")\n",
        "                for other_group in env.group_map.keys():\n",
        "                    next_td = target_policies[other_group](next_td)\n",
        "\n",
        "            loss_vals = iddpg_losses[group](subdata)\n",
        "            for loss_name in [\"loss_actor\", \"loss_value\"]:\n",
        "                loss = loss_vals[loss_name]\n",
        "                optimiser = iddpg_optimisers[group][loss_name]\n",
        "                loss.backward()\n",
        "                params = optimiser.param_groups[0][\"params\"]\n",
        "                torch.nn.utils.clip_grad_norm_(params, max_grad_norm)\n",
        "                optimiser.step()\n",
        "                optimiser.zero_grad()\n",
        "            iddpg_target_updaters[group].step()\n",
        "        exploration_policies[group][-1].step(current_frames)\n",
        "\n",
        "    for group in env.group_map.keys():\n",
        "        episode_reward_mean = (\n",
        "            batch.get((\"next\", group, \"episode_reward\"))[\n",
        "                batch.get((\"next\", group, \"done\"))\n",
        "            ]\n",
        "            .mean()\n",
        "            .item()\n",
        "        )\n",
        "        iddpg_episode_reward_mean_map[group].append(episode_reward_mean)\n",
        "\n",
        "    reward_strings = [\n",
        "        f\"{group}: {iddpg_episode_reward_mean_map[group][-1]:.2f}\"\n",
        "        for group in env.group_map.keys()\n",
        "    ]\n",
        "    description = (\n",
        "        f\"IDDPG Iter [{iteration+1}/500] | Rewards: \" + \" | \".join(reward_strings)\n",
        "    )\n",
        "    iddpg_pbar.set_description(description)\n",
        "    iddpg_pbar.refresh()\n",
        "\n",
        "iddpg_pbar.close()\n",
        "print(\"\\nIDDPG training finished.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Performance Comparison Analysis\n",
        "print(\"=\"*80)\n",
        "print(\"PERFORMANCE COMPARISON: MADDPG vs IDDPG\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Plot comparison\n",
        "fig, axs = plt.subplots(n_agents, 1, figsize=(12, 10), sharex=True)\n",
        "if n_agents == 1:\n",
        "    axs = [axs]\n",
        "\n",
        "for i, group in enumerate(env.group_map.keys()):\n",
        "    # Plot MADDPG results\n",
        "    axs[i].plot(episode_reward_mean_map[group], label=f\"MADDPG {group}\", alpha=0.8, linewidth=2)\n",
        "    \n",
        "    # Plot IDDPG results (only first 500 iterations for fair comparison)\n",
        "    iddpg_rewards = iddpg_episode_reward_mean_map[group][:500]\n",
        "    axs[i].plot(iddpg_rewards, label=f\"IDDPG {group}\", alpha=0.8, linewidth=2)\n",
        "    \n",
        "    axs[i].set_ylabel(\"Episode Reward\")\n",
        "    axs[i].legend()\n",
        "    axs[i].grid(True, alpha=0.3)\n",
        "\n",
        "axs[-1].set_xlabel(\"Training Iterations\")\n",
        "fig.suptitle(\"MADDPG vs IDDPG Performance Comparison\", fontsize=16)\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()\n",
        "\n",
        "# Calculate final performance metrics\n",
        "print(\"\\nFINAL PERFORMANCE METRICS:\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for group in env.group_map.keys():\n",
        "    maddpg_final = episode_reward_mean_map[group][-1]\n",
        "    iddpg_final = iddpg_episode_reward_mean_map[group][-1]\n",
        "    \n",
        "    print(f\"{group}:\")\n",
        "    print(f\"  MADDPG Final Reward: {maddpg_final:.3f}\")\n",
        "    print(f\"  IDDPG Final Reward:  {iddpg_final:.3f}\")\n",
        "    print(f\"  Difference: {maddpg_final - iddpg_final:.3f}\")\n",
        "    print()\n",
        "\n",
        "# Calculate average performance over last 100 iterations\n",
        "print(\"AVERAGE PERFORMANCE (Last 100 iterations):\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for group in env.group_map.keys():\n",
        "    maddpg_avg = np.mean(episode_reward_mean_map[group][-100:])\n",
        "    iddpg_avg = np.mean(iddpg_episode_reward_mean_map[group][-100:])\n",
        "    \n",
        "    print(f\"{group}:\")\n",
        "    print(f\"  MADDPG Average: {maddpg_avg:.3f}\")\n",
        "    print(f\"  IDDPG Average:  {iddpg_avg:.3f}\")\n",
        "    print(f\"  Improvement: {((maddpg_avg - iddpg_avg) / abs(iddpg_avg) * 100):.1f}%\")\n",
        "    print()\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ANALYSIS AND DISCUSSION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\"\"\n",
        "KEY DIFFERENCES BETWEEN MADDPG AND IDDPG:\n",
        "\n",
        "1. INFORMATION SHARING:\n",
        "   - MADDPG: Centralized critic has access to all agents' observations and actions\n",
        "   - IDDPG: Each agent's critic only sees its own observation and action\n",
        "\n",
        "2. COOPERATION CAPABILITY:\n",
        "   - MADDPG: Can learn coordinated strategies through global information\n",
        "   - IDDPG: Limited to individual learning without coordination\n",
        "\n",
        "3. COMPUTATIONAL COMPLEXITY:\n",
        "   - MADDPG: Higher input dimension for critics (all observations + actions)\n",
        "   - IDDPG: Lower input dimension (only own observation + action)\n",
        "\n",
        "4. TRAINING STABILITY:\n",
        "   - MADDPG: More stable due to centralized training\n",
        "   - IDDPG: May be less stable due to non-stationary environment\n",
        "\n",
        "EXPECTED PERFORMANCE DIFFERENCES:\n",
        "\n",
        "1. MADDPG should generally perform better in cooperative tasks because:\n",
        "   - Agents can learn coordinated strategies\n",
        "   - Centralized critic provides better value estimates\n",
        "   - More stable training dynamics\n",
        "\n",
        "2. IDDPG may struggle with:\n",
        "   - Learning coordinated behaviors\n",
        "   - Non-stationary environment issues\n",
        "   - Suboptimal local policies\n",
        "\n",
        "3. However, IDDPG has advantages:\n",
        "   - Lower computational requirements\n",
        "   - More scalable to larger numbers of agents\n",
        "   - No need for centralized information during execution\n",
        "\n",
        "CONCLUSION:\n",
        "The performance comparison will show whether the centralized training approach\n",
        "of MADDPG provides significant benefits over the independent learning approach\n",
        "of IDDPG in this multi-agent navigation task.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
