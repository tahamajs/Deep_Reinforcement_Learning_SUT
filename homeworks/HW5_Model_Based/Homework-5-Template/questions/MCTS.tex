\section{Task 1: Monte Carlo Tree Search}

\subsection{Task Overview}
This notebook implements a \textbf{MuZero-inspired reinforcement learning (RL) framework}, integrating \textbf{planning, learning, and model-based approaches}. The primary objective is to develop an RL agent that can learn from \textbf{environment interactions} and improve decision-making using \textbf{Monte Carlo Tree Search (MCTS)}.

The key components of this implementation include:

\subsubsection{Representation, Dynamics, and Prediction Networks}
\begin{itemize}
    \item Transform raw observations into \textbf{latent hidden states}.
    \item Simulate \textbf{future state transitions} and predict \textbf{rewards}.
    \item Output \textbf{policy distributions} (probability of actions) and \textbf{value estimates} (expected returns).
\end{itemize}

\subsubsection{Search Algorithms}
\begin{itemize}
    \item \textbf{Monte Carlo Tree Search (MCTS)}: A structured search algorithm that simulates future decisions and \textbf{backpropagates values} to guide action selection.
    \item \textbf{Naive Depth Search}: A simpler approach that expands all actions up to a fixed depth, evaluating rewards.
\end{itemize}

\subsubsection{Buffer Replay (Experience Memory)}
\begin{itemize}
    \item Stores entire \textbf{trajectories} (state-action-reward sequences).
    \item Samples \textbf{mini-batches} of past experiences for training.
    \item Enables \textbf{n-step return calculations} for updating value estimates.
\end{itemize}

\subsubsection{Agent}
\begin{itemize}
    \item Integrates \textbf{search algorithms} and \textbf{deep networks} to infer actions.
    \item Uses a \textbf{latent state representation} instead of raw observations.
    \item Selects actions using \textbf{MCTS, Naive Search, or Direct Policy Inference}.
\end{itemize}

\subsubsection{Training Loop}
\begin{enumerate}
    \item \textbf{Step 1}: Collects trajectories through environment interaction.
    \item \textbf{Step 2}: Stores experiences in the \textbf{replay buffer}.
    \item \textbf{Step 3}: Samples sub-trajectories for \textbf{model updates}.
    \item \textbf{Step 4}: Unrolls the learned model \textbf{over multiple steps}.
    \item \textbf{Step 5}: Computes \textbf{loss functions} (policy, value, and reward prediction errors).
    \item \textbf{Step 6}: Updates the neural network parameters.
\end{enumerate}

\textbf{Sections to be Implemented}

The notebook contains several placeholders (\texttt{TODO}) for missing implementations.
\subsection{Questions}

\subsubsection{MCTS Fundamentals}
\begin{itemize}
    \item \textbf{What are the four main phases of MCTS (Selection, Expansion, Simulation, Backpropagation), and what is the conceptual purpose of each phase?}
    
    \textbf{Answer:} MCTS operates through four distinct phases that work together to build an efficient search tree:
    
    \begin{enumerate}
        \item \textbf{Selection}: Starting from the root, traverse the tree using a tree policy (typically UCB1) until reaching a leaf node. The purpose is to balance exploration of promising paths with exploitation of known good moves. The UCB1 formula guides this selection:
        \[UCB1(s,a) = Q(s,a) + c\sqrt{\frac{\ln N(s)}{N(s,a)}}\]
        where the first term promotes exploitation and the second term encourages exploration of less-visited actions.
        
        \item \textbf{Expansion}: When reaching a leaf node that is not terminal, add one or more child nodes to expand the tree. This phase grows the search tree incrementally, focusing computational resources on promising regions of the state space.
        
        \item \textbf{Simulation (Rollout)}: From the newly expanded node (or leaf if terminal), play out a complete game using a default policy (often random) until reaching a terminal state. This provides an estimate of the value of the position without requiring perfect evaluation functions.
        
        \item \textbf{Backpropagation}: Propagate the simulation result back up the path taken during selection, updating visit counts and value estimates for all nodes in the path. This ensures that promising moves accumulate higher values and visit counts over time.
    \end{enumerate}
    
    \item \textbf{How does MCTS balance exploration and exploitation in its node selection strategy (i.e., how does the UCB formula address this balance)?}
    
    \textbf{Answer:} MCTS achieves the exploration-exploitation balance through the UCB1 formula:
    \[UCB1(s,a) = Q(s,a) + c\sqrt{\frac{\ln N(s)}{N(s,a)}}\]
    
    \begin{itemize}
        \item \textbf{Exploitation Term} $Q(s,a)$: Represents the average value of taking action $a$ from state $s$. Higher values indicate better-performing actions that should be exploited more frequently.
        
        \item \textbf{Exploration Term} $c\sqrt{\frac{\ln N(s)}{N(s,a)}}$: Encourages exploration of less-visited actions. The term grows as:
        \begin{itemize}
            \item $N(s)$ increases (more visits to the parent state)
            \item $N(s,a)$ decreases (fewer visits to this specific action)
        \end{itemize}
        
        \item \textbf{Exploration Constant} $c$: Controls the balance between exploration and exploitation. Typical values:
        \begin{itemize}
            \item $c = \sqrt{2}$: Theoretical optimum for UCB1
            \item $c < \sqrt{2}$: More exploitative behavior
            \item $c > \sqrt{2}$: More exploratory behavior
        \end{itemize}
    \end{itemize}
    
    This balance ensures that MCTS doesn't get stuck in local optima while still focusing computational resources on promising regions of the search space.
\end{itemize}

\subsubsection{Tree Policy and Rollouts}
\begin{itemize} 
   \item \textbf{Why do we run multiple simulations from each node rather than a single simulation?}
   
   \textbf{Answer:} Running multiple simulations is crucial for several reasons:
   
   \begin{enumerate}
       \item \textbf{Statistical Reliability}: A single simulation provides only one sample from a potentially noisy distribution. Multiple simulations allow us to:
       \begin{itemize}
           \item Estimate the true expected value more accurately
           \item Reduce variance in value estimates
           \item Build confidence in the quality of different moves
       \end{itemize}
       
       \item \textbf{Law of Large Numbers}: As the number of simulations increases, the sample mean converges to the true expected value. This is particularly important in games with:
       \begin{itemize}
           \item Stochastic elements (random events, opponent moves)
           \item Complex position evaluation
           \item Multiple possible outcomes
       \end{itemize}
       
       \item \textbf{UCB1 Requirements}: The UCB1 formula assumes that we have multiple samples to compute reliable statistics. The visit count $N(s,a)$ and average value $Q(s,a)$ become more meaningful with more simulations.
       
       \item \textbf{Tree Growth}: Multiple simulations allow the tree to grow more systematically, exploring different branches and building a more comprehensive view of the game tree.
   \end{enumerate}
   
   \item \textbf{What role do random rollouts (or simulated playouts) play in estimating the value of a position?}
   
   \textbf{Answer:} Random rollouts serve as a crucial evaluation mechanism in MCTS:
   
   \begin{enumerate}
       \item \textbf{Heuristic-Free Evaluation}: Random rollouts provide position evaluation without requiring hand-crafted evaluation functions. This is particularly valuable in:
       \begin{itemize}
           \item Complex games where evaluation functions are difficult to design
           \item New domains where expert knowledge is limited
           \item Situations where perfect evaluation is computationally infeasible
       \end{itemize}
       
       \item \textbf{Monte Carlo Estimation}: Each rollout represents a random sample from the distribution of possible game outcomes. By averaging many rollouts, we approximate the true expected value:
       \[V(s) \approx \frac{1}{N} \sum_{i=1}^{N} R_i\]
       where $R_i$ is the outcome of the $i$-th rollout and $N$ is the number of simulations.
       
       \item \textbf{Computational Efficiency}: Random rollouts are:
       \begin{itemize}
           \item Fast to execute (no complex evaluation)
           \item Parallelizable (multiple rollouts can run simultaneously)
           \item Scalable to different game complexities
       \end{itemize}
       
       \item \textbf{Unbiased Estimation}: Random rollouts provide unbiased estimates of position values, assuming the default policy (random play) is unbiased. This contrasts with heuristic evaluation functions that may introduce systematic biases.
       
       \item \textbf{Adaptive Evaluation}: The quality of rollout-based evaluation improves automatically as the tree grows, since more promising positions receive more simulations and thus more accurate estimates.
   \end{enumerate}
\end{itemize}


\subsubsection{Integration with Neural Networks}
\begin{itemize}
    \item In the context of Neural MCTS (e.g., AlphaGo-style approaches), how are policy networks and value networks incorporated into the search procedure?  
    \item What is the role of the policy network’s output (“prior probabilities”) in the Expansion phase, and how does it influence which moves get explored?
\end{itemize}


\subsubsection{Backpropagation and Node Statistics}
\begin{itemize}
    \item During backpropagation, how do we update node visit counts and value estimates?  
    \item Why is it important to aggregate results carefully (e.g., averaging or summing outcomes) when multiple simulations pass through the same node?
\end{itemize}


\subsubsection{Hyperparameters and Practical Considerations}
\begin{itemize}
    \item How does the exploration constant (often denoted \( c_{puct} \) or \( c \)) in the UCB formula affect the search behavior, and how would you tune it?  
    \item In what ways can the “temperature” parameter (if used) shape the final move selection, and why might you lower the temperature as training progresses?
\end{itemize}


\subsubsection{Comparisons to Other Methods}
\begin{itemize}
    \item How does MCTS differ from classical minimax search or alpha-beta pruning in handling deep or complex game trees?  
    \item What unique advantages does MCTS provide when the state space is extremely large or when an accurate heuristic evaluation function is not readily available?
\end{itemize}