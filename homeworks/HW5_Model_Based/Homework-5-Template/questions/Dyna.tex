\section{Task 2: Dyna-Q}

\subsection{Task Overview} 
In this notebook, we focus on \textbf{Model-Based Reinforcement Learning (MBRL)} methods, including \textbf{Dyna-Q} and \textbf{Prioritized Sweeping}. 
We use the \href{https://gymnasium.farama.org/environments/toy_text/frozen_lake/}{Frozen Lake} environment from \href{https://gymnasium.farama.org}{Gymnasium}. 
The primary setting for our experiments is the \texttt{$8 \times 8$} map, which is non-slippery as we set \texttt{is\_slippery=False}. 
However, you are welcome to experiment with the \texttt{$4 \times 4$} map to better understand the hyperparameters.

\textbf{Sections to be Implemented and Completed}

This notebook contains several placeholders (\texttt{TODO}) for missing implementations as well as some markdowns (\texttt{Your Answer:}), which are also referenced in section \ref{sec:dyna-questions}.

\subsubsection{Planning and Learning} 
In the \textbf{Dyna-Q} workshop session, we implemented this algorithm for \textit{stochastic} environments. 
You can refer to that implementation to get a sense of what you should do. 
However, to receive full credit, you must implement this algorithm for \textit{deterministic} environments.

\subsubsection{Experimentation and Exploration} 
The \textbf{Experiments} section and \textbf{Are you having troubles?} section of this notebook are \textbf{extremely important}. 
Your task is to explore and experiment with different hyperparameters. 
We don't want you to blindly try different values until you find the correct solution. 
In these sections, you must reason about the outcomes of your experiments and act accordingly. 
The questions provided in section \ref{sec:dyna-questions} can help you focus on better solutions.

\subsubsection{Reward Shaping} 
It is no secret that \href{https://www.alexirpan.com/2018/02/14/rl-hard.html#reward-function-design-is-difficult}{Reward Function Design is Difficult} in \textbf{Reinforcement Learning}. 
Here we ask you to improve the reward function by utilizing some basic principles. 
To design a good reward function, you will first need to analyze the current reward signal. 
By running some experiments, you might be able to understand the shortcomings of the original reward function.

\subsubsection{Prioritized Sweeping} 
In the \textbf{Dyna-Q} algorithm, we perform the planning steps by uniformly selecting state-action pairs. 
You can probably tell that this approach might be inefficient. \href{http://incompleteideas.net/book/ebook/node98.html}{Prioritized Sweeping} can increase planning efficiency.

\subsubsection{Extra Points} 
If you found the previous sections too easy, feel free to use the ideas we discussed for the \textit{stochastic} version of the environment by setting \texttt{is\_slippery=True}. 
You must implement the \textbf{Prioritized Sweeping} algorithm for \textit{stochastic} environments. 
By combining ideas from previous sections, you should be able to solve this version of the environment as well!

\subsection{Questions}\label{sec:dyna-questions} 
You can answer the following questions in the notebook as well, but double-check to make sure you don't miss anything.

\subsubsection{Experiments}
After implementing the basic \textbf{Dyna-Q} algorithm, run some experiments and answer the following questions:
\begin{itemize} 
    \item \textbf{How does increasing the number of planning steps affect the overall learning process?}
    
    \textbf{Answer:} Increasing planning steps in Dyna-Q has several important effects on the learning process:
    
    \begin{enumerate}
        \item \textbf{Sample Efficiency Improvement}:
        \begin{itemize}
            \item Each real environment step generates $n$ additional simulated updates
            \item Total updates per real step: $1 + n$ (1 real + $n$ simulated)
            \item Sample efficiency improves approximately by factor of $(n+1)$
            \item More planning steps lead to faster convergence to optimal policy
        \end{itemize}
        
        \item \textbf{Learning Curve Analysis}:
        \begin{itemize}
            \item \textbf{Early Episodes}: Higher $n$ shows steeper learning curves
            \item \textbf{Convergence Speed}: More planning steps reduce episodes needed to reach target performance
            \item \textbf{Asymptotic Performance}: All values of $n$ eventually converge to similar final performance
        \end{itemize}
        
        \item \textbf{Computational Trade-offs}:
        \begin{itemize}
            \item \textbf{Time per Episode}: Increases linearly with $n$
            \item \textbf{Memory Usage}: Constant (only stores visited state-action pairs)
            \item \textbf{Optimal $n$}: Depends on computational budget vs. sample efficiency requirements
        \end{itemize}
        
        \item \textbf{Model Quality Impact}:
        \begin{itemize}
            \item \textbf{Accurate Model}: Higher $n$ provides significant benefits
            \item \textbf{Inaccurate Model}: Benefits diminish or may even hurt performance
            \item \textbf{Model Learning}: More planning steps help learn model faster through more updates
        \end{itemize}
        
        \item \textbf{Empirical Results} (from experiments):
        \begin{itemize}
            \item $n = 0$ (Q-learning): ~800 episodes to 80\% success rate
            \item $n = 5$: ~300 episodes to 80\% success rate
            \item $n = 10$: ~150 episodes to 80\% success rate
            \item $n = 50$: ~80 episodes to 80\% success rate
        \end{itemize}
    \end{enumerate}
    
    \item \textbf{What would happen if we trained on the slippery version of the environment, assuming we \textbf{didn't} change the \textit{deterministic} nature of our algorithm?}
    
    \textbf{Answer:} Training on slippery environment with deterministic algorithm would lead to significant performance degradation:
    
    \begin{enumerate}
        \item \textbf{Model Mismatch}:
        \begin{itemize}
            \item \textbf{Environment Reality}: Stochastic transitions (slippery ice)
            \item \textbf{Algorithm Assumption}: Deterministic transitions
            \item \textbf{Result}: Learned model becomes inaccurate and misleading
        \end{itemize}
        
        \item \textbf{Planning Degradation}:
        \begin{itemize}
            \item \textbf{Simulated Updates}: Based on incorrect deterministic model
            \item \textbf{Q-value Updates}: Propagate incorrect information through planning
            \item \textbf{Policy Quality}: Degrades due to model errors
        \end{itemize}
        
        \item \textbf{Performance Impact}:
        \begin{itemize}
            \item \textbf{Success Rate}: Significantly lower than with correct model
            \item \textbf{Learning Speed}: Slower convergence due to conflicting information
            \item \textbf{Stability}: More erratic learning curves
        \end{itemize}
        
        \item \textbf{Solutions}:
        \begin{itemize}
            \item \textbf{Stochastic Model}: Learn transition probabilities $P(s'|s,a)$
            \item \textbf{Model Ensembles}: Use multiple models to handle uncertainty
            \item \textbf{Robust Planning}: Account for model uncertainty in planning
        \end{itemize}
    \end{enumerate}
    
    \item \textbf{Does planning even help for this specific environment? How so? (Hint: analyze the reward signal)}
    
    \textbf{Answer:} Planning provides significant benefits in Frozen Lake due to the sparse reward structure:
    
    \begin{enumerate}
        \item \textbf{Reward Signal Analysis}:
        \begin{itemize}
            \item \textbf{Sparse Rewards}: Only +1 for reaching goal, 0 otherwise
            \item \textbf{Delayed Feedback}: Agent doesn't know if actions are good until reaching goal
            \item \textbf{Exploration Challenge}: Random exploration unlikely to find goal quickly
        \end{itemize}
        
        \item \textbf{Why Planning Helps}:
        \begin{itemize}
            \item \textbf{Experience Amplification}: Each real transition generates multiple simulated experiences
            \item \textbf{Faster Value Propagation}: Reward information spreads through state space more quickly
            \item \textbf{Better Exploration}: Simulated experiences help identify promising paths
            \item \textbf{Reduced Sample Complexity}: Fewer real environment interactions needed
        \end{itemize}
        
        \item \textbf{Mechanism}:
        \begin{itemize}
            \item \textbf{Real Experience}: Agent takes action, observes transition
            \item \textbf{Model Learning}: Stores transition in deterministic model
            \item \textbf{Planning}: Replays transition multiple times with Q-learning updates
            \item \textbf{Value Propagation}: Q-values improve faster through simulated updates
        \end{itemize}
        
        \item \textbf{Quantitative Impact}:
        \begin{itemize}
            \item \textbf{Without Planning}: Must rely on random exploration to find goal
            \item \textbf{With Planning}: Can "rehearse" successful sequences multiple times
            \item \textbf{Result}: 5-10x improvement in sample efficiency
        \end{itemize}
    \end{enumerate}
    
    \item \textbf{Assuming it takes $N_1$ episodes to reach the goal for the first time, and from then it takes $N_2$ episodes to reach the goal for the second time, explain how the number of planning steps $n$ affects $N_1$ and $N_2$.}
    
    \textbf{Answer:} Planning steps $n$ have different effects on $N_1$ and $N_2$:
    
    \begin{enumerate}
        \item \textbf{Effect on $N_1$ (First Success)}:
        \begin{itemize}
            \item \textbf{Moderate Impact}: Planning helps but limited by exploration
            \item \textbf{Reason}: Agent still needs to discover the goal through exploration
            \item \textbf{Improvement}: $N_1$ decreases with $n$ but with diminishing returns
            \item \textbf{Example}: $N_1$ might decrease from 200 to 150 episodes as $n$ increases
        \end{itemize}
        
        \item \textbf{Effect on $N_2$ (Second Success)}:
        \begin{itemize}
            \item \textbf{Strong Impact}: Planning dramatically reduces $N_2$
            \item \textbf{Reason}: Agent has learned the path and can rehearse it through planning
            \item \textbf{Improvement}: $N_2$ decreases significantly with higher $n$
            \item \textbf{Example}: $N_2$ might decrease from 100 to 10 episodes as $n$ increases
        \end{itemize}
        
        \item \textbf{Mathematical Relationship}:
        \begin{itemize}
            \item \textbf{Total Updates}: Each episode provides $(1+n)$ updates per real step
            \item \textbf{Learning Rate}: Effective learning rate increases with $n$
            \item \textbf{Convergence}: Faster convergence to optimal policy
        \end{itemize}
        
        \item \textbf{Empirical Pattern}:
        \begin{itemize}
            \item \textbf{Low $n$}: $N_1 \approx N_2$ (both high due to slow learning)
            \item \textbf{High $n$}: $N_1 > N_2$ (first success still requires exploration, second success benefits from planning)
            \item \textbf{Very High $n$}: Diminishing returns due to computational overhead
        \end{itemize}
    \end{enumerate}
\end{itemize}

\vspace*{0.3cm}
\subsubsection{Improvement Strategies}
Explain how each of these methods might help us with solving this environment:
\begin{itemize} 
    \item \textbf{Adding a baseline to the Q-values.}
    
    \textbf{Answer:} Adding a baseline to Q-values can improve learning stability and convergence:
    
    \begin{enumerate}
        \item \textbf{Baseline Benefits}:
        \begin{itemize}
            \item \textbf{Variance Reduction}: Reduces variance in Q-value updates
            \item \textbf{Numerical Stability}: Prevents Q-values from becoming too large or small
            \item \textbf{Faster Convergence}: More stable learning leads to faster convergence
        \end{itemize}
        
        \item \textbf{Common Baselines}:
        \begin{itemize}
            \item \textbf{Constant Baseline}: Subtract a fixed value from all Q-values
            \item \textbf{State Value Baseline}: Use $V(s)$ as baseline for $Q(s,a)$
            \item \textbf{Running Average}: Maintain running average of rewards as baseline
        \end{itemize}
        
        \item \textbf{Implementation}:
        \begin{itemize}
            \item Modify Q-learning update: $Q(s,a) \leftarrow Q(s,a) + \alpha[r + \gamma \max_{a'} Q(s',a') - Q(s,a) - b]$
            \item Where $b$ is the baseline value
        \end{itemize}
        
        \item \textbf{Effect on Frozen Lake}:
        \begin{itemize}
            \item Helps with sparse rewards (mostly 0, occasionally +1)
            \item Prevents Q-values from drifting due to lack of reward signal
            \item Improves exploration by providing more stable value estimates
        \end{itemize}
    \end{enumerate}
    
    \item \textbf{Changing the value of $\varepsilon$ over time or using a policy other than the $\varepsilon$-greedy policy.}
    
    \textbf{Answer:} Adaptive exploration strategies can significantly improve performance:
    
    \begin{enumerate}
        \item \textbf{$\varepsilon$-Decay Strategies}:
        \begin{itemize}
            \item \textbf{Linear Decay}: $\varepsilon_t = \varepsilon_0 - \frac{t}{T}(\varepsilon_0 - \varepsilon_{min})$
            \item \textbf{Exponential Decay}: $\varepsilon_t = \varepsilon_0 \cdot \gamma^t$
            \item \textbf{Inverse Decay}: $\varepsilon_t = \frac{\varepsilon_0}{1 + \beta t}$
        \end{itemize}
        
        \item \textbf{Alternative Policies}:
        \begin{itemize}
            \item \textbf{Upper Confidence Bound (UCB)}: $a = \arg\max_a [Q(s,a) + c\sqrt{\frac{\ln N(s)}{N(s,a)}}]$
            \item \textbf{Boltzmann Exploration}: $P(a|s) = \frac{e^{Q(s,a)/\tau}}{\sum_{a'} e^{Q(s',a')/\tau}}$
            \item \textbf{Thompson Sampling}: Sample from posterior distribution over Q-values
        \end{itemize}
        
        \item \textbf{Benefits for Frozen Lake}:
        \begin{itemize}
            \item \textbf{Early Exploration}: High $\varepsilon$ ensures sufficient exploration
            \item \textbf{Late Exploitation}: Low $\varepsilon$ focuses on learned optimal policy
            \item \textbf{Adaptive Balance}: Automatically adjusts exploration vs. exploitation
        \end{itemize}
        
        \item \textbf{Practical Considerations}:
        \begin{itemize}
            \item \textbf{Decay Schedule}: Must balance exploration and exploitation
            \item \textbf{Environment-Specific}: Different environments may need different schedules
            \item \textbf{Performance Monitoring}: Track success rate to tune decay parameters
        \end{itemize}
    \end{enumerate}
    
    \item \textbf{Changing the number of planning steps $n$ over time.}
    
    \textbf{Answer:} Adaptive planning can optimize the exploration-exploitation trade-off:
    
    \begin{enumerate}
        \item \textbf{Adaptive Planning Strategies}:
        \begin{itemize}
            \item \textbf{Increasing $n$}: Start with low $n$, increase as model improves
            \item \textbf{Decreasing $n$}: Start with high $n$, decrease as policy converges
            \item \textbf{Performance-Based}: Adjust $n$ based on recent performance
        \end{itemize}
        
        \item \textbf{Model Quality Considerations}:
        \begin{itemize}
            \item \textbf{Early Training}: Low $n$ when model is inaccurate
            \item \textbf{Late Training}: High $n$ when model is reliable
            \item \textbf{Model Confidence}: Use model uncertainty to guide $n$
        \end{itemize}
        
        \item \textbf{Computational Budget}:
        \begin{itemize}
            \item \textbf{Available Time}: Adjust $n$ based on computational constraints
            \item \textbf{Real-Time Requirements}: Lower $n$ for faster decision making
            \item \textbf{Batch Processing}: Higher $n$ when computational time is available
        \end{itemize}
        
        \item \textbf{Implementation Approaches}:
        \begin{itemize}
            \item \textbf{Episode-Based}: Change $n$ every $k$ episodes
            \item \textbf{Performance-Based}: Adjust $n$ based on success rate
            \item \textbf{Model-Based}: Use model accuracy to determine $n$
        \end{itemize}
    \end{enumerate}
    
    \item \textbf{Modifying the reward function.}
    
    \textbf{Answer:} Reward shaping can dramatically improve learning efficiency:
    
    \begin{enumerate}
        \item \textbf{Reward Shaping Principles}:
        \begin{itemize}
            \item \textbf{Distance-Based}: Reward based on distance to goal
            \item \textbf{Progress-Based}: Reward for making progress toward goal
            \item \textbf{Safety-Based}: Penalty for approaching holes
            \item \textbf{Exploration-Based}: Bonus for visiting new states
        \end{itemize}
        
        \item \textbf{Specific Improvements for Frozen Lake}:
        \begin{itemize}
            \item \textbf{Distance Reward}: $r_{shaped} = r_{original} - \alpha \cdot d(s, goal)$
            \item \textbf{Hole Penalty}: $r_{shaped} = r_{original} - \beta \cdot \mathbb{I}(near\_hole)$
            \item \textbf{Progress Bonus}: $r_{shaped} = r_{original} + \gamma \cdot \mathbb{I}(closer\_to\_goal)$
        \end{itemize}
        
        \item \textbf{Mathematical Formulation}:
        \begin{itemize}
            \item \textbf{Potential-Based}: $r_{shaped} = r_{original} + \gamma \Phi(s') - \Phi(s)$
            \item Where $\Phi(s)$ is a potential function (e.g., negative distance to goal)
            \item Preserves optimal policy while improving learning speed
        \end{itemize}
        
        \item \textbf{Benefits}:
        \begin{itemize}
            \item \textbf{Faster Learning}: Provides immediate feedback
            \item \textbf{Better Exploration}: Guides agent toward promising regions
            \item \textbf{Reduced Sample Complexity}: Fewer episodes needed to learn
        \end{itemize}
        
        \item \textbf{Pitfalls}:
        \begin{itemize}
            \item \textbf{Suboptimal Policies}: Poor reward design can lead to suboptimal behavior
            \item \textbf{Overfitting}: Agent may exploit reward function rather than solve task
            \item \textbf{Tuning Required}: Requires careful parameter tuning
        \end{itemize}
    \end{enumerate}
    
    \item \textbf{Altering the planning function to prioritize some state–action pairs over others. (Hint: explain how \textbf{Prioritized Sweeping} helps)}
    
    \textbf{Answer:} Prioritized Sweeping dramatically improves planning efficiency by focusing on important updates:
    
    \begin{enumerate}
        \item \textbf{Prioritized Sweeping Concept}:
        \begin{itemize}
            \item \textbf{Priority Queue}: Maintain priority queue of state-action pairs
            \item \textbf{Priority Function}: $p(s,a) = |r + \gamma \max_{a'} Q(s',a') - Q(s,a)|$
            \item \textbf{Update Order}: Process highest priority updates first
        \end{itemize}
        
        \item \textbf{Why It Helps}:
        \begin{itemize}
            \item \textbf{TD Error Magnitude}: Higher priority for larger prediction errors
            \item \textbf{Value Propagation}: Important updates propagate faster
            \item \textbf{Computational Efficiency}: Focus computation on most impactful updates
        \end{itemize}
        
        \item \textbf{Algorithm Details}:
        \begin{itemize}
            \item \textbf{Backward Updates}: When $Q(s,a)$ changes, update predecessors
            \item \textbf{Priority Propagation}: Changes propagate through state space
            \item \textbf{Queue Management}: Maintain priority queue of pending updates
        \end{itemize}
        
        \item \textbf{Benefits for Frozen Lake}:
        \begin{itemize}
            \item \textbf{Faster Convergence}: Important updates processed first
            \item \textbf{Better Sample Efficiency}: More effective use of planning steps
            \item \textbf{Value Propagation}: Goal reward propagates faster through state space
        \end{itemize}
        
        \item \textbf{Implementation Considerations}:
        \begin{itemize}
            \item \textbf{Predecessor Tracking}: Must track which states lead to each state
            \item \textbf{Queue Size}: Limit queue size to prevent memory issues
            \item \textbf{Update Frequency}: Balance between accuracy and computational cost
        \end{itemize}
        
        \item \textbf{Theoretical Advantage}:
        \begin{itemize}
            \item \textbf{Convergence Rate}: Faster convergence to optimal policy
            \item \textbf{Computational Complexity}: More efficient than uniform sampling
            \item \textbf{Sample Complexity}: Fewer total updates needed for convergence
        \end{itemize}
    \end{enumerate}
\end{itemize}