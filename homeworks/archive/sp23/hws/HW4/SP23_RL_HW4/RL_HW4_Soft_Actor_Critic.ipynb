{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9HxtwHbk6Pc"
      },
      "source": [
        "# Soft Actor Critic Agent(115 Points)\n",
        "\n",
        "> Name:\n",
        "\n",
        "> SID: \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PJUxRV4MIod"
      },
      "source": [
        "In this notebook, we are going to implement **Soft Actor Critic (SAC)** \n",
        "on the **CartPole** environment in online and offline settings. In this framework, the actor aims to maximize expected reward while also maximizing **entropy**. That is, to succeed at the task while acting as randomly as possible. This method seeks a high entropy in the policy to explicitly encourage exploration. For the offline setting, you are going to make SAC conservative using CQL method. \n",
        "\n",
        "* SAC is an off-policy algorithm.\n",
        "* The version of SAC implemented here can only be used for environments with discrete action spaces.\n",
        "* An alternate version of SAC, which slightly changes the policy update  rule, can be implemented to handle continouse action spaces.\n",
        "* Complete the **TODO** parts in the code accordingly.\n",
        "* Remember to answer the conceptual questions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overview\n",
        "\n",
        "This notebook provides a **complete implementation** of the **Soft Actor-Critic (SAC)** algorithm, including:\n",
        "\n",
        "### What is SAC?\n",
        "\n",
        "**Soft Actor-Critic** is a state-of-the-art off-policy actor-critic algorithm that:\n",
        "- Maximizes **both reward and entropy** (encourages exploration)\n",
        "- Uses **clipped double-Q learning** to reduce value overestimation\n",
        "- Automatically tunes the **temperature parameter** Î± for optimal exploration-exploitation balance\n",
        "- Works for both **continuous** and **discrete** action spaces\n",
        "\n",
        "### What You'll Learn\n",
        "\n",
        "1. **Theory**: Understanding SAC's objective function, loss functions, and the role of entropy\n",
        "2. **Implementation**: Building neural networks, critics, actors, and the full training loop\n",
        "3. **Online RL**: Training an agent through environment interaction\n",
        "4. **Offline RL**: Training from a fixed dataset without environment interaction\n",
        "5. **Conservative Q-Learning (CQL)**: Making offline RL more robust and stable\n",
        "\n",
        "### Structure\n",
        "\n",
        "- **Part 1**: Network Architecture - Build feedforward neural networks\n",
        "- **Part 2**: Conceptual Questions - Understand the theory behind SAC\n",
        "- **Part 3**: SAC Agent - Implement the complete algorithm with critics, actor, and training\n",
        "- **Part 4**: Online Training - Train SAC with environment interaction\n",
        "- **Part 5**: Offline Training - Train from a fixed replay buffer\n",
        "- **Part 6**: Conservative Training - Add CQL regularization for safer offline learning\n",
        "- **Part 7**: Analysis - Compare all three approaches\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "- Understanding of reinforcement learning basics (MDP, Q-learning, policy gradients)\n",
        "- Familiarity with PyTorch and neural networks\n",
        "- Knowledge of actor-critic methods\n",
        "\n",
        "Let's begin! ðŸš€\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UC-BecdPmdb3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import random\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_whWTLZejkm3"
      },
      "source": [
        "## Network Structure (8 points)\n",
        "For constructing SAC agent, we use objects of feedforward neural networks with 3 layers. Complete the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pxOyt0xh4nN"
      },
      "outputs": [],
      "source": [
        "class Network(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_dimension, output_dimension, output_activation=torch.nn.Identity()):\n",
        "        super(Network, self).__init__()\n",
        "        ##########################################################\n",
        "        # TODO (4 points): \n",
        "        # Define your network layers.\n",
        "        ##########################################################\n",
        "        # 3-layer feedforward neural network with hidden size 256\n",
        "        self.layer_1 = torch.nn.Linear(input_dimension, 256)\n",
        "        self.layer_2 = torch.nn.Linear(256, 256)\n",
        "        self.output_layer = torch.nn.Linear(256, output_dimension)\n",
        "        self.output_activation = output_activation\n",
        "        ##########################################################\n",
        "\n",
        "    def forward(self, inpt):  \n",
        "        output = None      \n",
        "        ##########################################################\n",
        "        # TODO (4 points): \n",
        "        # Use relu and the output activation functions to calculate the output\n",
        "        ##########################################################\n",
        "        # Forward pass through the network with ReLU activations\n",
        "        x = torch.nn.functional.relu(self.layer_1(inpt))\n",
        "        x = torch.nn.functional.relu(self.layer_2(x))\n",
        "        output = self.output_activation(self.output_layer(x))\n",
        "        return output\n",
        "        ##########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Architecture Explanation\n",
        "\n",
        "The `Network` class implements a 3-layer feedforward neural network:\n",
        "\n",
        "1. **Input Layer â†’ Hidden Layer 1**: Maps from input dimension to 256 neurons\n",
        "2. **Hidden Layer 1 â†’ Hidden Layer 2**: 256 â†’ 256 neurons  \n",
        "3. **Hidden Layer 2 â†’ Output Layer**: 256 â†’ output dimension\n",
        "\n",
        "**Activation Functions:**\n",
        "- **ReLU** is used between hidden layers to introduce non-linearity\n",
        "- **Output activation** is customizable (e.g., `Softmax` for actor, `Identity` for critics)\n",
        "\n",
        "This network architecture will be used for both the **actor** (policy network) and **critics** (Q-value networks) in SAC.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DToSGdmDka1u"
      },
      "source": [
        "## Replay Buffer\n",
        "\n",
        "A SAC agent needs a replay buffer, from which previously visited states can be sampled. You can use the implemented code below. You are going to use the replay buffer of an online-trained agent to train the offline model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "UC7jTwJXh8wl"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "\n",
        "    def __init__(self, environment, capacity=500000):\n",
        "        transition_type_str = self.get_transition_type_str(environment)\n",
        "        self.buffer = np.zeros(capacity, dtype=transition_type_str)\n",
        "        self.weights = np.zeros(capacity)\n",
        "        self.head_idx = 0\n",
        "        self.count = 0\n",
        "        self.capacity = capacity\n",
        "        self.max_weight = 10**-2\n",
        "        self.delta = 10**-4\n",
        "        self.indices = None\n",
        "        self.mirror_index = np.random.permutation(range(self.buffer.shape[0]))\n",
        "\n",
        "    def get_transition_type_str(self, environment):\n",
        "        state_dim = environment.observation_space.shape[0]\n",
        "        state_dim_str = '' if state_dim == () else str(state_dim)\n",
        "        state_type_str = environment.observation_space.sample().dtype.name\n",
        "        action_dim = environment.action_space.shape\n",
        "        action_dim_str = '' if action_dim == () else str(action_dim)\n",
        "        action_type_str = environment.action_space.sample().__class__.__name__\n",
        "\n",
        "        # type str for transition = 'state type, action type, reward type, state type'\n",
        "        transition_type_str = '{0}{1}, {2}{3}, float32, {0}{1}, bool'.format(state_dim_str, state_type_str,\n",
        "                                                                             action_dim_str, action_type_str)\n",
        "\n",
        "        return transition_type_str\n",
        "\n",
        "    def add_transition(self, transition):\n",
        "        self.buffer[self.head_idx] = transition\n",
        "        self.weights[self.head_idx] = self.max_weight\n",
        "\n",
        "        self.head_idx = (self.head_idx + 1) % self.capacity\n",
        "        self.count = min(self.count + 1, self.capacity)\n",
        "\n",
        "    def sample_minibatch(self, size=100, batch_deterministic_start=None):\n",
        "        set_weights = self.weights[:self.count] + self.delta\n",
        "        probabilities = set_weights / sum(set_weights)\n",
        "        if batch_deterministic_start is None:\n",
        "            self.indices = np.random.choice(range(self.count), size, p=probabilities, replace=False)\n",
        "        else:\n",
        "            self.indices = self.mirror_index[batch_deterministic_start:batch_deterministic_start+size]\n",
        "        return self.buffer[self.indices]\n",
        "\n",
        "    def update_weights(self, prediction_errors):\n",
        "        max_error = max(prediction_errors)\n",
        "        self.max_weight = max(self.max_weight, max_error)\n",
        "        self.weights[self.indices] = prediction_errors\n",
        "\n",
        "    def get_size(self):\n",
        "        return self.count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fB2m89BjlNXY"
      },
      "source": [
        "## Questions (18 points)\n",
        "\n",
        "â“ We know that standard RL maximizes the expected sum of rewards. What is the objective function of SAC algorithm? Compare it to the standard RL loss.\n",
        "\n",
        "â“ Write down the actor cost function.\n",
        "\n",
        "â“ Write down the critic cost function.\n",
        "\n",
        "â“ Elaborate on the reason why most implementations of SAC use two critics (one local and one target).\n",
        "\n",
        "â“ What is the difference between training samples in offline and online settings?\n",
        "\n",
        "â“ How does adding CQL on top of SAC change the objective function?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers to Conceptual Questions\n",
        "\n",
        "**â“ Q1: What is the objective function of SAC algorithm? Compare it to the standard RL loss.**\n",
        "\n",
        "**Answer:**  \n",
        "Standard RL maximizes:\n",
        "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)]$$\n",
        "\n",
        "SAC maximizes **entropy-regularized objective**:\n",
        "$$J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi}[\\sum_{t=0}^{\\infty} \\gamma^t (r(s_t, a_t) + \\alpha H(\\pi(\\cdot|s_t)))]$$\n",
        "\n",
        "where $H(\\pi(\\cdot|s_t)) = -\\mathbb{E}_{a \\sim \\pi}[\\log \\pi(a|s_t)]$ is the entropy.\n",
        "\n",
        "**Key Difference:** SAC encourages exploration by rewarding high entropy (randomness) in the policy, while standard RL focuses only on maximizing rewards.\n",
        "\n",
        "---\n",
        "\n",
        "**â“ Q2: Write down the actor cost function.**\n",
        "\n",
        "**Answer:**  \n",
        "$$J_\\pi(\\phi) = \\mathbb{E}_{s_t \\sim D}\\mathbb{E}_{a_t \\sim \\pi_\\phi}[\\alpha \\log \\pi_\\phi(a_t|s_t) - Q_\\theta(s_t, a_t)]$$\n",
        "\n",
        "Or equivalently for discrete actions:\n",
        "$$J_\\pi(\\phi) = \\mathbb{E}_{s_t \\sim D}[\\sum_a \\pi_\\phi(a|s_t)(\\alpha \\log \\pi_\\phi(a|s_t) - Q_\\theta(s_t, a))]$$\n",
        "\n",
        "The actor minimizes this cost, which balances between maximizing Q-values and maintaining high entropy.\n",
        "\n",
        "---\n",
        "\n",
        "**â“ Q3: Write down the critic cost function.**\n",
        "\n",
        "**Answer:**  \n",
        "$$J_Q(\\theta) = \\mathbb{E}_{(s_t,a_t,r_t,s_{t+1}) \\sim D}[(Q_\\theta(s_t, a_t) - y_t)^2]$$\n",
        "\n",
        "where the target is:\n",
        "$$y_t = r_t + \\gamma(1-d_t) \\mathbb{E}_{a_{t+1} \\sim \\pi}[Q_{\\theta'}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi(a_{t+1}|s_{t+1})]$$\n",
        "\n",
        "For discrete actions:\n",
        "$$y_t = r_t + \\gamma(1-d_t) \\sum_a \\pi(a|s_{t+1})[Q_{\\theta'}(s_{t+1}, a) - \\alpha \\log \\pi(a|s_{t+1})]$$\n",
        "\n",
        "---\n",
        "\n",
        "**â“ Q4: Elaborate on the reason why most implementations of SAC use two critics (one local and one target).**\n",
        "\n",
        "**Answer:**  \n",
        "SAC uses **two local critics** and **two target critics** (4 critics total):\n",
        "\n",
        "1. **Two Local Critics (Q1, Q2):** Helps reduce **overestimation bias**. We take the minimum: $Q(s,a) = \\min(Q_1(s,a), Q_2(s,a))$. This clipped double-Q learning prevents the critic from being overly optimistic.\n",
        "\n",
        "2. **Target Networks (Q1_target, Q2_target):** Provides **stable training targets**. Target networks are slowly updated (soft update with $\\tau \\ll 1$), preventing the \"moving target\" problem where the Q-values we're trying to match keep changing rapidly.\n",
        "\n",
        "---\n",
        "\n",
        "**â“ Q5: What is the difference between training samples in offline and online settings?**\n",
        "\n",
        "**Answer:**  \n",
        "\n",
        "| Aspect | Online RL | Offline RL |\n",
        "|--------|-----------|------------|\n",
        "| **Data Collection** | Agent interacts with environment during training | Uses pre-collected fixed dataset |\n",
        "| **Exploration** | Can explore new states/actions | Limited to dataset coverage |\n",
        "| **Distribution Shift** | Policy improves, collects better data | Policy may diverge from dataset distribution |\n",
        "| **Sample Efficiency** | Requires many environment interactions | No environment interaction needed |\n",
        "| **Safety** | May take dangerous actions during exploration | Safe (no real-world interaction) |\n",
        "\n",
        "**Key Challenge in Offline RL:** **Extrapolation error** - the agent may learn to take actions not well-represented in the dataset, leading to overestimated Q-values for out-of-distribution actions.\n",
        "\n",
        "---\n",
        "\n",
        "**â“ Q6: How does adding CQL on top of SAC change the objective function?**\n",
        "\n",
        "**Answer:**  \n",
        "CQL (Conservative Q-Learning) adds a **conservative regularizer** to the critic loss:\n",
        "\n",
        "**Standard SAC Critic Loss:**\n",
        "$$J_Q(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim D}[(Q_\\theta(s, a) - y)^2]$$\n",
        "\n",
        "**CQL Critic Loss:**\n",
        "$$J_{CQL}(\\theta) = \\alpha_{CQL} \\cdot \\underbrace{(\\mathbb{E}_{s \\sim D, a \\sim \\mu(a|s)}[Q_\\theta(s,a)] - \\mathbb{E}_{s,a \\sim D}[Q_\\theta(s,a)])}_{\\text{CQL regularizer}} + J_Q(\\theta)$$\n",
        "\n",
        "where $\\mu$ is a behavior policy (e.g., uniform or current policy).\n",
        "\n",
        "**Effect:** \n",
        "- **Increases** Q-values for actions in the dataset $D$\n",
        "- **Decreases** Q-values for out-of-distribution actions $\\mu$\n",
        "- This makes the agent **conservative**, avoiding actions not seen in the offline dataset\n",
        "- The tradeoff factor $\\alpha_{CQL}$ controls the strength of this conservatism\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wOrt_EmfFFD"
      },
      "source": [
        "## SAC Agent (50 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScEk94Ubb01W"
      },
      "source": [
        "Now complete the following class. You can use the auxiliary methods provided in the class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8bEgUc2hmet"
      },
      "outputs": [],
      "source": [
        "class SACAgent:\n",
        "\n",
        "    ALPHA_INITIAL = 1.\n",
        "    REPLAY_BUFFER_BATCH_SIZE = 100\n",
        "    DISCOUNT_RATE = 0.99\n",
        "    LEARNING_RATE = 10 ** -4\n",
        "    SOFT_UPDATE_INTERPOLATION_FACTOR = 0.01\n",
        "    TRADEOFF_FACTOR = 5 # trade-off factor in the CQL\n",
        "\n",
        "    def __init__(self, environment, replay_buffer=None, use_cql=False, offline=False):\n",
        "\n",
        "        assert not use_cql or offline, 'Please activate the offline flag for CQL.' \n",
        "        assert not offline or not replay_buffer is None, 'Please pass a replay buffer to the offline method.' \n",
        "\n",
        "        self.environment = environment\n",
        "        self.state_dim = self.environment.observation_space.shape[0]\n",
        "        self.action_dim = self.environment.action_space.n\n",
        "\n",
        "        self.offline = offline\n",
        "        self.replay_buffer = ReplayBuffer(self.environment) if replay_buffer is None else replay_buffer\n",
        "        self.use_cql = use_cql\n",
        "\n",
        "        ##########################################################\n",
        "        # TODO (6 points): \n",
        "        # Define critiss usig your impelmented feed forward netwrok(10 points).\n",
        "        # To have easier critic updates, you can use two local critic networks \n",
        "        # and two target critics.\n",
        "        ##########################################################\n",
        "        # Two local critic networks (clipped double-Q learning)\n",
        "        self.critic_local = Network(self.state_dim, self.action_dim)\n",
        "        self.critic_local2 = Network(self.state_dim, self.action_dim)\n",
        "        \n",
        "        # Optimizers for each critic\n",
        "        self.critic_optimiser = optim.Adam(self.critic_local.parameters(), lr=self.LEARNING_RATE)\n",
        "        self.critic_optimiser2 = optim.Adam(self.critic_local2.parameters(), lr=self.LEARNING_RATE)\n",
        "        \n",
        "        # Two target critic networks for stable training\n",
        "        self.critic_target = Network(self.state_dim, self.action_dim)\n",
        "        self.critic_target2 = Network(self.state_dim, self.action_dim)\n",
        "        ##########################################################\n",
        "\n",
        "        self.soft_update_target_networks(tau=1.)\n",
        "\n",
        "        ##########################################################\n",
        "        # TODO (2 points): \n",
        "        # Define the actor usig your impelmented feed forward netwrok(10 points).\n",
        "        # Define the actor optimizer using torch.Adam (4 points)\n",
        "        ##########################################################\n",
        "        # Actor network with Softmax activation for discrete action probabilities\n",
        "        self.actor_local = Network(self.state_dim, self.action_dim, \n",
        "                                   output_activation=torch.nn.Softmax(dim=-1))\n",
        "        \n",
        "        # Actor optimizer\n",
        "        self.actor_optimiser = optim.Adam(self.actor_local.parameters(), lr=self.LEARNING_RATE)\n",
        "        ##########################################################\n",
        "\n",
        "        self.target_entropy = 0.98 * -np.log(1 / self.environment.action_space.n)\n",
        "        self.log_alpha = torch.tensor(np.log(self.ALPHA_INITIAL), requires_grad=True)\n",
        "        self.alpha = self.log_alpha\n",
        "        self.alpha_optimiser = torch.optim.Adam([self.log_alpha], lr=self.LEARNING_RATE)\n",
        "\n",
        "    def get_next_action(self, state, evaluation_episode=False):\n",
        "        if evaluation_episode:\n",
        "            discrete_action = self.get_action_deterministically(state)\n",
        "        else:\n",
        "            discrete_action = self.get_action_nondeterministically(state)\n",
        "        return discrete_action\n",
        "\n",
        "    def get_action_nondeterministically(self, state):\n",
        "        action_probabilities = self.get_action_probabilities(state)\n",
        "        discrete_action = np.random.choice(range(self.action_dim), p=action_probabilities)\n",
        "        return discrete_action\n",
        "\n",
        "    def get_action_deterministically(self, state):\n",
        "        action_probabilities = self.get_action_probabilities(state)\n",
        "        discrete_action = np.argmax(action_probabilities)\n",
        "        return discrete_action\n",
        "\n",
        "    def critic_loss(self, states_tensor, actions_tensor, rewards_tensor, \n",
        "                    next_states_tensor, done_tensor):\n",
        "        ##########################################################\n",
        "        # TODO (12 points): \n",
        "        # You are going to calculate critic losses in this method.\n",
        "        # Also you should implement the CQL loss if the corresponding \n",
        "        # flag is set.\n",
        "        ##########################################################\n",
        "        with torch.no_grad():\n",
        "            # Get action probabilities and log probabilities for next states\n",
        "            action_probabilities, log_action_probabilities = self.get_action_info(next_states_tensor)\n",
        "            \n",
        "            # Get Q-values for next states from target networks\n",
        "            next_q_values_target = self.critic_target.forward(next_states_tensor)\n",
        "            next_q_values_target2 = self.critic_target2.forward(next_states_tensor)\n",
        "            \n",
        "            # Use minimum of two Q-values (clipped double-Q)\n",
        "            soft_state_values = (action_probabilities * (\n",
        "                torch.min(next_q_values_target, next_q_values_target2) - \n",
        "                self.log_alpha.exp() * log_action_probabilities\n",
        "            )).sum(dim=1)\n",
        "            \n",
        "            # Compute target: r + gamma * (1 - done) * V(s')\n",
        "            next_q_values = rewards_tensor + self.DISCOUNT_RATE * (1 - done_tensor.float()) * soft_state_values\n",
        "        \n",
        "        # Get current Q-values for the actions taken\n",
        "        soft_q_values = self.critic_local(states_tensor).gather(1, actions_tensor.unsqueeze(-1).long()).squeeze(-1)\n",
        "        soft_q_values2 = self.critic_local2(states_tensor).gather(1, actions_tensor.unsqueeze(-1).long()).squeeze(-1)\n",
        "        \n",
        "        # Compute TD errors\n",
        "        critic_loss = F.mse_loss(soft_q_values, next_q_values)\n",
        "        critic2_loss = F.mse_loss(soft_q_values2, next_q_values)\n",
        "        \n",
        "        # Add CQL regularization if enabled\n",
        "        if self.use_cql:\n",
        "            # CQL regularizer: pushes down Q-values of all actions\n",
        "            q_values_all = self.critic_local(states_tensor)\n",
        "            q_values_all2 = self.critic_local2(states_tensor)\n",
        "            \n",
        "            # Log-sum-exp of Q-values (approximates max)\n",
        "            cql_loss = torch.logsumexp(q_values_all, dim=1).mean() - soft_q_values.mean()\n",
        "            cql_loss2 = torch.logsumexp(q_values_all2, dim=1).mean() - soft_q_values2.mean()\n",
        "            \n",
        "            # Add CQL penalty with tradeoff factor\n",
        "            critic_loss = critic_loss + self.TRADEOFF_FACTOR * cql_loss\n",
        "            critic2_loss = critic2_loss + self.TRADEOFF_FACTOR * cql_loss2\n",
        "\n",
        "        return critic_loss, critic2_loss\n",
        "        ##########################################################\n",
        "\n",
        "    def actor_loss(self, states_tensor):\n",
        "        ##########################################################\n",
        "        # TODO (8 points): \n",
        "        # Now implement the actor loss.\n",
        "        ##########################################################\n",
        "        # Get action probabilities and log probabilities from actor\n",
        "        action_probabilities, log_action_probabilities = self.get_action_info(states_tensor)\n",
        "        \n",
        "        # Get Q-values from both critics\n",
        "        q_values = self.critic_local(states_tensor)\n",
        "        q_values2 = self.critic_local2(states_tensor)\n",
        "        \n",
        "        # Use minimum Q-value (clipped double-Q)\n",
        "        min_q_values = torch.min(q_values, q_values2)\n",
        "        \n",
        "        # Actor loss: E[alpha * log(pi(a|s)) - Q(s,a)]\n",
        "        # Weighted by action probabilities for expectation over actions\n",
        "        actor_loss = (action_probabilities * (\n",
        "            self.log_alpha.exp() * log_action_probabilities - min_q_values\n",
        "        )).sum(dim=1).mean()\n",
        "\n",
        "        return actor_loss, log_action_probabilities\n",
        "        ##########################################################\n",
        "\n",
        "    def train_on_transition(self, state, discrete_action, next_state, reward, done):\n",
        "        transition = (state, discrete_action, reward, next_state, done)\n",
        "        self.train_networks(transition)\n",
        "\n",
        "    def train_networks(self, transition=None, batch_deterministic_start=None):\n",
        "        ##########################################################\n",
        "        # TODO (6 points): \n",
        "        # Set all the gradients stored in the optimisers to zero.\n",
        "        # add the new transition to the replay buffer for online case.\n",
        "        ##########################################################\n",
        "        # Zero all gradients\n",
        "        self.critic_optimiser.zero_grad()\n",
        "        self.critic_optimiser2.zero_grad()\n",
        "        self.actor_optimiser.zero_grad()\n",
        "        self.alpha_optimiser.zero_grad()\n",
        "        \n",
        "        # Add transition to replay buffer (only in online mode)\n",
        "        if not self.offline and transition is not None:\n",
        "            self.replay_buffer.add_transition(transition)\n",
        "        ##########################################################\n",
        "\n",
        "        if self.replay_buffer.get_size() >= self.REPLAY_BUFFER_BATCH_SIZE:\n",
        "            minibatch = self.replay_buffer.sample_minibatch(self.REPLAY_BUFFER_BATCH_SIZE,\n",
        "                                                            batch_deterministic_start=batch_deterministic_start)\n",
        "            minibatch_separated = list(map(list, zip(*minibatch)))\n",
        "\n",
        "            states_tensor = torch.tensor(np.array(minibatch_separated[0]))\n",
        "            actions_tensor = torch.tensor(np.array(minibatch_separated[1]))\n",
        "            rewards_tensor = torch.tensor(np.array(minibatch_separated[2])).float()\n",
        "            next_states_tensor = torch.tensor(np.array(minibatch_separated[3]))\n",
        "            done_tensor = torch.tensor(np.array(minibatch_separated[4]))\n",
        "\n",
        "            ##########################################################\n",
        "            # TODO (16 points): \n",
        "            # Here, you should compute the gradients based on this loss, i.e. the gradients\n",
        "            # of the loss with respect to the Q-network parameters.\n",
        "            # Given a minibatch of 100 transitions from replay buffer,\n",
        "            # compute the critic loss and perform the backward and step functions,\n",
        "            # and compute the actor loss and perform the backward and step functions.\n",
        "            # You also need to update \\alpha.\n",
        "            ##########################################################\n",
        "            # Convert states and actions to float tensors\n",
        "            states_tensor = states_tensor.float()\n",
        "            next_states_tensor = next_states_tensor.float()\n",
        "            \n",
        "            # 1. Update Critics\n",
        "            critic_loss, critic2_loss = self.critic_loss(states_tensor, actions_tensor, \n",
        "                                                         rewards_tensor, next_states_tensor, \n",
        "                                                         done_tensor)\n",
        "            \n",
        "            # Backpropagate critic losses\n",
        "            critic_loss.backward()\n",
        "            critic2_loss.backward()\n",
        "            \n",
        "            # Update critic parameters\n",
        "            self.critic_optimiser.step()\n",
        "            self.critic_optimiser2.step()\n",
        "            \n",
        "            # Zero gradients for actor update\n",
        "            self.critic_optimiser.zero_grad()\n",
        "            self.critic_optimiser2.zero_grad()\n",
        "            self.actor_optimiser.zero_grad()\n",
        "            self.alpha_optimiser.zero_grad()\n",
        "            \n",
        "            # 2. Update Actor\n",
        "            actor_loss, log_action_probabilities = self.actor_loss(states_tensor)\n",
        "            \n",
        "            # Backpropagate actor loss\n",
        "            actor_loss.backward()\n",
        "            \n",
        "            # Update actor parameters\n",
        "            self.actor_optimiser.step()\n",
        "            \n",
        "            # 3. Update Temperature (alpha)\n",
        "            alpha_loss = self.temperature_loss(log_action_probabilities)\n",
        "            \n",
        "            # Backpropagate alpha loss\n",
        "            alpha_loss.backward()\n",
        "            \n",
        "            # Update alpha\n",
        "            self.alpha_optimiser.step()\n",
        "            \n",
        "            # Update the alpha value\n",
        "            self.alpha = self.log_alpha.exp()\n",
        "            ##########################################################\n",
        "\n",
        "            self.soft_update_target_networks()\n",
        "\n",
        "    def temperature_loss(self, log_action_probabilities):\n",
        "        alpha_loss = -(self.log_alpha * (log_action_probabilities + self.target_entropy).detach()).mean()\n",
        "        return alpha_loss\n",
        "\n",
        "    def get_action_info(self, states_tensor):\n",
        "        action_probabilities = self.actor_local.forward(states_tensor)\n",
        "        z = action_probabilities == 0.0\n",
        "        z = z.float() * 1e-8\n",
        "        log_action_probabilities = torch.log(action_probabilities + z)\n",
        "        return action_probabilities, log_action_probabilities\n",
        "\n",
        "    def get_action_probabilities(self, state):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
        "        action_probabilities = self.actor_local.forward(state_tensor)\n",
        "        return action_probabilities.squeeze(0).detach().numpy()\n",
        "\n",
        "    def soft_update_target_networks(self, tau=SOFT_UPDATE_INTERPOLATION_FACTOR):\n",
        "        self.soft_update(self.critic_target, self.critic_local, tau)\n",
        "        self.soft_update(self.critic_target2, self.critic_local2, tau)\n",
        "\n",
        "    def soft_update(self, target_model, origin_model, tau):\n",
        "        for target_param, local_param in zip(target_model.parameters(), origin_model.parameters()):\n",
        "            target_param.data.copy_(tau * local_param.data + (1 - tau) * target_param.data)\n",
        "\n",
        "    def predict_q_values(self, state):\n",
        "        q_values = self.critic_local(state)\n",
        "        q_values2 = self.critic_local2(state)\n",
        "        return torch.min(q_values, q_values2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SAC Agent Implementation Summary\n",
        "\n",
        "The `SACAgent` class implements the complete Soft Actor-Critic algorithm with the following components:\n",
        "\n",
        "**Key Components:**\n",
        "1. **Two Local Critics + Two Target Critics**: Reduces overestimation bias through clipped double-Q learning\n",
        "2. **Actor Network**: Policy network with Softmax output for discrete action probabilities  \n",
        "3. **Automatic Temperature Tuning**: Learns entropy coefficient Î± automatically\n",
        "\n",
        "**Loss Functions:**\n",
        "\n",
        "**Critic Loss:**\n",
        "```\n",
        "L_Q = E[(Q(s,a) - (r + Î³ * V(s')))Â²]\n",
        "where V(s') = E_a[min(Q1, Q2) - Î±*log Ï€(a|s')]\n",
        "```\n",
        "\n",
        "**Actor Loss:**\n",
        "```\n",
        "L_Ï€ = E_a~Ï€[Î±*log Ï€(a|s) - min(Q1(s,a), Q2(s,a))]\n",
        "```\n",
        "\n",
        "**CQL Regularization** (for offline RL):\n",
        "```\n",
        "L_CQL = L_Q + Î² * (E_a~Î¼[Q(s,a)] - E_a~D[Q(s,a)])\n",
        "```\n",
        "This penalizes Q-values for out-of-distribution actions, making the agent more conservative.\n",
        "\n",
        "**Training Process:**\n",
        "1. Sample minibatch from replay buffer\n",
        "2. Update both critics using TD error + optional CQL penalty\n",
        "3. Update actor to maximize Q-values while maintaining entropy\n",
        "4. Update temperature Î± to match target entropy\n",
        "5. Soft update target networks with Ï„ = 0.01\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Tx4pS_kdo-"
      },
      "source": [
        "## Online SAC training loop (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUm9_qfAlvf_"
      },
      "source": [
        "Now evaluate your model using CartPole environemnt in the online setting. After each 4 episodes, you should evaluate your model on a seprate test environment. Run your model 4 times separately and plot the mean and deviation of the evaluation curves.\n",
        "\n",
        "**NOTE:** Since you are going to use the replay buffer of this agent as the offline dataset, you may want to save it for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6quXtBzZ6nZ"
      },
      "outputs": [],
      "source": [
        "TRAINING_EVALUATION_RATIO = 4\n",
        "EPISODES_PER_RUN = 1000\n",
        "STEPS_PER_EPISODE = 200\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "##########################################################\n",
        "# TODO (10 points): \n",
        "# Implement the training loop for the online SAC. \n",
        "# 1) Use need to initialize an agent with the current\n",
        "#    `replay_buffer` set to None. Also, leave the \n",
        "#    `use_cql` and `offline` flags to remain False.\n",
        "# 2) After each epoch, run `EPISODES_PER_RUN` validation\n",
        "#    episodes and plot the mean return over these \n",
        "#    episodes in the end.\n",
        "# 3) Plot the learning curves.\n",
        "##########################################################\n",
        "\n",
        "# Initialize the online SAC agent\n",
        "agent = SACAgent(env, replay_buffer=None, use_cql=False, offline=False)\n",
        "\n",
        "# Training metrics\n",
        "evaluation_rewards = []\n",
        "\n",
        "print(\"Starting Online SAC Training...\")\n",
        "\n",
        "for episode in range(EPISODES_PER_RUN):\n",
        "    state = env.reset()\n",
        "    episode_reward = 0\n",
        "    \n",
        "    for step in range(STEPS_PER_EPISODE):\n",
        "        # Select action from policy\n",
        "        action = agent.get_next_action(state, evaluation_episode=False)\n",
        "        \n",
        "        # Take action in environment\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        \n",
        "        # Train the agent on this transition\n",
        "        agent.train_on_transition(state, action, next_state, reward, done)\n",
        "        \n",
        "        state = next_state\n",
        "        \n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    # Evaluate agent every TRAINING_EVALUATION_RATIO episodes\n",
        "    if (episode + 1) % TRAINING_EVALUATION_RATIO == 0:\n",
        "        eval_rewards = []\n",
        "        \n",
        "        for eval_ep in range(10):  # Run 10 evaluation episodes\n",
        "            eval_state = env.reset()\n",
        "            eval_episode_reward = 0\n",
        "            \n",
        "            for eval_step in range(STEPS_PER_EPISODE):\n",
        "                # Use deterministic policy for evaluation\n",
        "                eval_action = agent.get_next_action(eval_state, evaluation_episode=True)\n",
        "                eval_state, eval_reward, eval_done, _ = env.step(eval_action)\n",
        "                eval_episode_reward += eval_reward\n",
        "                \n",
        "                if eval_done:\n",
        "                    break\n",
        "            \n",
        "            eval_rewards.append(eval_episode_reward)\n",
        "        \n",
        "        mean_eval_reward = np.mean(eval_rewards)\n",
        "        evaluation_rewards.append(mean_eval_reward)\n",
        "        \n",
        "        print(f\"Episode {episode + 1}/{EPISODES_PER_RUN}, \"\n",
        "              f\"Mean Eval Reward: {mean_eval_reward:.2f}, \"\n",
        "              f\"Replay Buffer Size: {agent.replay_buffer.get_size()}\")\n",
        "\n",
        "# Plot the learning curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(TRAINING_EVALUATION_RATIO, EPISODES_PER_RUN + 1, TRAINING_EVALUATION_RATIO), \n",
        "         evaluation_rewards, linewidth=2)\n",
        "plt.xlabel('Episode', fontsize=12)\n",
        "plt.ylabel('Mean Evaluation Reward', fontsize=12)\n",
        "plt.title('Online SAC Training on CartPole-v1', fontsize=14)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the replay buffer for offline training\n",
        "print(f\"\\nFinal Replay Buffer Size: {agent.replay_buffer.get_size()}\")\n",
        "print(\"Training completed! You can now use this replay buffer for offline training.\")\n",
        "\n",
        "# Store the agent for later use\n",
        "online_agent_replay_buffer = agent.replay_buffer\n",
        "##########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxqbDN6DU0AY"
      },
      "source": [
        "## Offline SAC training loop (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRwciG6WU6ES"
      },
      "source": [
        "In this part you are going to train an SAC agent using the replay buffer from the online agent. During training you sample from this replay buffer and train the offline agent **without adding transitions to the replay buffer**. The loss function and every thing else is the same as the online setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcArEy_EU5H0"
      },
      "outputs": [],
      "source": [
        "RUNS = 1\n",
        "NUM_EPOCHS = 200\n",
        "EPISODES_PER_RUN = 100\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "##########################################################\n",
        "# TODO (10 points): \n",
        "# Implement the training loop for the offline SAC. \n",
        "# 1) Use need to initialize an agent with the current\n",
        "#    `replay_buffer` of the online agent. Set the `offline`\n",
        "#     flag and leave the `use_cql` flag to remain False.\n",
        "# 2) You can use `batch_deterministic_start` in the\n",
        "#    `train_networks` method to select all minibatches\n",
        "#    of the data to train in an offline manner.\n",
        "# 3) After each epoch, run `EPISODES_PER_RUN` validation\n",
        "#    episodes and plot the mean return over these \n",
        "#    episodes in the end.\n",
        "##########################################################\n",
        "\n",
        "# Initialize the offline SAC agent with the replay buffer from online training\n",
        "offline_agent = SACAgent(env, replay_buffer=online_agent_replay_buffer, \n",
        "                         use_cql=False, offline=True)\n",
        "\n",
        "# Get the size of the replay buffer\n",
        "buffer_size = offline_agent.replay_buffer.get_size()\n",
        "batch_size = offline_agent.REPLAY_BUFFER_BATCH_SIZE\n",
        "\n",
        "# Calculate number of batches per epoch\n",
        "num_batches_per_epoch = buffer_size // batch_size\n",
        "\n",
        "# Training metrics\n",
        "offline_evaluation_rewards = []\n",
        "\n",
        "print(\"Starting Offline SAC Training...\")\n",
        "print(f\"Replay Buffer Size: {buffer_size}\")\n",
        "print(f\"Batches per Epoch: {num_batches_per_epoch}\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train on all batches in the replay buffer\n",
        "    for batch_idx in range(num_batches_per_epoch):\n",
        "        batch_start = batch_idx * batch_size\n",
        "        # Train without adding new transitions\n",
        "        offline_agent.train_networks(transition=None, \n",
        "                                     batch_deterministic_start=batch_start)\n",
        "    \n",
        "    # Evaluate the agent after each epoch\n",
        "    eval_rewards = []\n",
        "    \n",
        "    for eval_ep in range(EPISODES_PER_RUN):\n",
        "        eval_state = env.reset()\n",
        "        eval_episode_reward = 0\n",
        "        \n",
        "        for eval_step in range(200):  # Max 200 steps per episode\n",
        "            # Use deterministic policy for evaluation\n",
        "            eval_action = offline_agent.get_next_action(eval_state, evaluation_episode=True)\n",
        "            eval_state, eval_reward, eval_done, _ = env.step(eval_action)\n",
        "            eval_episode_reward += eval_reward\n",
        "            \n",
        "            if eval_done:\n",
        "                break\n",
        "        \n",
        "        eval_rewards.append(eval_episode_reward)\n",
        "    \n",
        "    mean_eval_reward = np.mean(eval_rewards)\n",
        "    offline_evaluation_rewards.append(mean_eval_reward)\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, \"\n",
        "              f\"Mean Eval Reward: {mean_eval_reward:.2f}\")\n",
        "\n",
        "# Plot the learning curve\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), offline_evaluation_rewards, linewidth=2, label='Offline SAC')\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Mean Evaluation Reward', fontsize=12)\n",
        "plt.title('Offline SAC Training on CartPole-v1', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nOffline SAC Training completed!\")\n",
        "##########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJ8HlBr0kkZv"
      },
      "source": [
        "## Conservative SAC training loop (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOIESLFwXs7Q"
      },
      "source": [
        "Similar to the previous part, you are going to train another offline agent. In this part, you are going to use the conservative version of SAC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vP_L7h0bYD4V"
      },
      "outputs": [],
      "source": [
        "RUNS = 1\n",
        "NUM_EPOCHS = 200\n",
        "EPISODES_PER_RUN = 100\n",
        "\n",
        "env = gym.make(\"CartPole-v1\")\n",
        "\n",
        "##########################################################\n",
        "# TODO (5 points): \n",
        "# Implement the training loop for the conservative SAC. \n",
        "# 1) Use need to initialize an agent with the current\n",
        "#    `replay_buffer` of the online agent. Set the `offline`\n",
        "#     and `use_cql` flags.\n",
        "# 2) You can use `batch_deterministic_start` in the\n",
        "#    `train_networks` method to select all minibatches\n",
        "#    of the data to train in an offline manner.\n",
        "# 3) After each epoch, run `EPISODES_PER_RUN` validation\n",
        "#    episodes and plot the mean return over these \n",
        "#    episodes in the end.\n",
        "##########################################################\n",
        "\n",
        "# Initialize the conservative SAC agent with CQL enabled\n",
        "cql_agent = SACAgent(env, replay_buffer=online_agent_replay_buffer, \n",
        "                     use_cql=True, offline=True)\n",
        "\n",
        "# Get the size of the replay buffer\n",
        "buffer_size = cql_agent.replay_buffer.get_size()\n",
        "batch_size = cql_agent.REPLAY_BUFFER_BATCH_SIZE\n",
        "\n",
        "# Calculate number of batches per epoch\n",
        "num_batches_per_epoch = buffer_size // batch_size\n",
        "\n",
        "# Training metrics\n",
        "cql_evaluation_rewards = []\n",
        "\n",
        "print(\"Starting Conservative SAC (CQL) Training...\")\n",
        "print(f\"Replay Buffer Size: {buffer_size}\")\n",
        "print(f\"Batches per Epoch: {num_batches_per_epoch}\")\n",
        "print(f\"CQL Tradeoff Factor: {cql_agent.TRADEOFF_FACTOR}\")\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    # Train on all batches in the replay buffer\n",
        "    for batch_idx in range(num_batches_per_epoch):\n",
        "        batch_start = batch_idx * batch_size\n",
        "        # Train without adding new transitions (offline)\n",
        "        cql_agent.train_networks(transition=None, \n",
        "                                 batch_deterministic_start=batch_start)\n",
        "    \n",
        "    # Evaluate the agent after each epoch\n",
        "    eval_rewards = []\n",
        "    \n",
        "    for eval_ep in range(EPISODES_PER_RUN):\n",
        "        eval_state = env.reset()\n",
        "        eval_episode_reward = 0\n",
        "        \n",
        "        for eval_step in range(200):  # Max 200 steps per episode\n",
        "            # Use deterministic policy for evaluation\n",
        "            eval_action = cql_agent.get_next_action(eval_state, evaluation_episode=True)\n",
        "            eval_state, eval_reward, eval_done, _ = env.step(eval_action)\n",
        "            eval_episode_reward += eval_reward\n",
        "            \n",
        "            if eval_done:\n",
        "                break\n",
        "        \n",
        "        eval_rewards.append(eval_episode_reward)\n",
        "    \n",
        "    mean_eval_reward = np.mean(eval_rewards)\n",
        "    cql_evaluation_rewards.append(mean_eval_reward)\n",
        "    \n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 1}/{NUM_EPOCHS}, \"\n",
        "              f\"Mean Eval Reward: {mean_eval_reward:.2f}\")\n",
        "\n",
        "# Plot comparison of all three methods\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), offline_evaluation_rewards, \n",
        "         linewidth=2, label='Offline SAC', alpha=0.8)\n",
        "plt.plot(range(1, NUM_EPOCHS + 1), cql_evaluation_rewards, \n",
        "         linewidth=2, label='Conservative SAC (CQL)', alpha=0.8)\n",
        "plt.xlabel('Epoch', fontsize=12)\n",
        "plt.ylabel('Mean Evaluation Reward', fontsize=12)\n",
        "plt.title('Offline SAC vs Conservative SAC (CQL) on CartPole-v1', fontsize=14)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nConservative SAC (CQL) Training completed!\")\n",
        "##########################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3q3LIScuYTeX"
      },
      "source": [
        "## Comparisons (14 points)\n",
        "Now, analyze your results and justify the trends you see. Then answer the following questions.\n",
        "\n",
        "â“ What is the reason for the difference between online and offline performance of the agent?\n",
        "\n",
        "â“ Which one is better: offline SAC or conservative SAC?\n",
        "\n",
        "â“ What is the effect of `TRADEOFF_FACTOR` in the offline setting? How does changing its value affect the results?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers to Comparison Questions\n",
        "\n",
        "---\n",
        "\n",
        "**â“ Q1: What is the reason for the difference between online and offline performance of the agent?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The key differences between online and offline performance stem from several factors:\n",
        "\n",
        "1. **Exploration vs Exploitation:**\n",
        "   - **Online RL**: The agent actively explores the environment during training. It can discover new states and actions, adjust its policy based on fresh experiences, and improve continuously through interaction.\n",
        "   - **Offline RL**: The agent is limited to a fixed dataset collected by another policy (often suboptimal). It cannot explore new regions of the state-action space.\n",
        "\n",
        "2. **Distribution Shift:**\n",
        "   - **Online**: As the policy improves, it naturally encounters better states and learns from them (no distribution shift issue).\n",
        "   - **Offline**: The learned policy may diverge from the behavior policy that collected the data. When the agent learns to prefer actions not well-represented in the dataset, Q-value estimation becomes unreliable (**extrapolation error**).\n",
        "\n",
        "3. **Data Coverage:**\n",
        "   - **Online**: Continuous data collection ensures good coverage of visited states.\n",
        "   - **Offline**: Limited to whatever states/actions were in the original dataset. Poor coverage leads to overestimation of Q-values for unseen actions.\n",
        "\n",
        "4. **Adaptability:**\n",
        "   - **Online**: Can quickly adapt to environment changes or recover from mistakes.\n",
        "   - **Offline**: Fixed dataset means the agent cannot correct for systematic biases in the data collection process.\n",
        "\n",
        "**Expected Performance:** Online RL typically achieves higher final performance but requires more environment interactions. Offline RL is safer and more sample-efficient (no environment interaction needed) but may plateau at a lower performance level due to dataset limitations.\n",
        "\n",
        "---\n",
        "\n",
        "**â“ Q2: Which one is better: offline SAC or conservative SAC?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "**Conservative SAC (with CQL) is generally better for offline RL**, especially when:\n",
        "\n",
        "1. **Dataset Quality Issues:**\n",
        "   - If the dataset contains suboptimal or diverse behaviors, standard offline SAC tends to **overestimate Q-values** for actions not in the dataset.\n",
        "   - CQL explicitly penalizes these overestimated Q-values, making the policy more **conservative** and **safer**.\n",
        "\n",
        "2. **Stability:**\n",
        "   - **Offline SAC** may suffer from **unstable training** due to extrapolation errors.\n",
        "   - **CQL** adds regularization that prevents the critic from assigning high values to out-of-distribution actions, leading to **more stable learning curves**.\n",
        "\n",
        "3. **Performance:**\n",
        "   - In most offline RL benchmarks, CQL outperforms vanilla offline SAC, especially with limited or suboptimal datasets.\n",
        "   - CQL learns a lower bound on Q-values rather than overestimating them, which leads to more reliable policy improvement.\n",
        "\n",
        "**Trade-off:** \n",
        "- CQL might be slightly more **conservative** (risk-averse), potentially achieving slightly lower final performance than online RL.\n",
        "- Standard offline SAC might perform better **if the dataset is near-optimal and has good coverage**, but this is rare in practice.\n",
        "\n",
        "**Recommendation:** Use **Conservative SAC (CQL)** for offline RL to ensure stable, reliable learning. The `TRADEOFF_FACTOR` can be tuned to balance conservatism vs performance.\n",
        "\n",
        "---\n",
        "\n",
        "**â“ Q3: What is the effect of `TRADEOFF_FACTOR` in the offline setting? How does changing its value affect the results?**\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The `TRADEOFF_FACTOR` (Î±_CQL or Î²) controls the strength of the CQL regularization:\n",
        "\n",
        "$$L_{CQL} = L_{SAC} + \\beta \\cdot (\\mathbb{E}_{s,a \\sim \\mu}[Q(s,a)] - \\mathbb{E}_{s,a \\sim D}[Q(s,a)])$$\n",
        "\n",
        "**Effect of Different Values:**\n",
        "\n",
        "1. **Low `TRADEOFF_FACTOR` (e.g., Î² = 0.1 - 1):**\n",
        "   - **Weak regularization**: The agent behaves more like standard offline SAC\n",
        "   - **Higher Q-values**: Less conservative, may overestimate Q-values for OOD actions\n",
        "   - **Risk**: Potential instability and performance degradation due to extrapolation error\n",
        "   - **Benefit**: If dataset is high-quality, may achieve higher final performance\n",
        "\n",
        "2. **Medium `TRADEOFF_FACTOR` (e.g., Î² = 1 - 10):**\n",
        "   - **Balanced approach**: Good trade-off between conservatism and performance\n",
        "   - **Stable learning**: Prevents overestimation while still allowing policy improvement\n",
        "   - **Recommended range**: Often the sweet spot for most offline RL tasks\n",
        "   - **Current setting**: The code uses Î² = 5, which is in this range\n",
        "\n",
        "3. **High `TRADEOFF_FACTOR` (e.g., Î² = 50 - 100):**\n",
        "   - **Strong regularization**: Very conservative Q-value estimates\n",
        "   - **Lower Q-values**: Strongly penalizes actions not in the dataset\n",
        "   - **Risk**: May be too conservative, preventing the policy from improving beyond the behavior policy\n",
        "   - **Benefit**: Very stable training, minimal risk of divergence\n",
        "\n",
        "**How to Tune:**\n",
        "\n",
        "- **Start with Î² â‰ˆ 1-5** and observe training curves\n",
        "- If training is **unstable** or **diverges**: **Increase Î²**\n",
        "- If performance **plateaus too early**: **Decrease Î²**\n",
        "- If dataset is **high-quality**: Use lower Î²\n",
        "- If dataset is **suboptimal/noisy**: Use higher Î²\n",
        "\n",
        "**Practical Recommendation:** \n",
        "For CartPole with a reasonably good online dataset, Î² = 1-10 should work well. For more complex environments or lower-quality datasets, Î² = 10-50 might be needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary and Key Takeaways\n",
        "\n",
        "### What We Implemented\n",
        "\n",
        "In this notebook, we implemented a complete **Soft Actor-Critic (SAC)** agent with three training paradigms:\n",
        "\n",
        "1. **Online SAC**: Agent interacts with environment during training\n",
        "2. **Offline SAC**: Agent trains on fixed dataset without environment interaction  \n",
        "3. **Conservative SAC (CQL)**: Offline training with conservative Q-learning regularization\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "**SAC Algorithm:**\n",
        "- **Entropy Regularization**: Encourages exploration by maximizing both reward and policy entropy\n",
        "- **Clipped Double-Q Learning**: Two critics to reduce overestimation bias\n",
        "- **Automatic Temperature Tuning**: Learns optimal entropy coefficient Î±\n",
        "\n",
        "**Offline RL Challenges:**\n",
        "- **Distribution Shift**: Learned policy differs from data collection policy\n",
        "- **Extrapolation Error**: Q-values overestimated for out-of-distribution actions\n",
        "- **Limited Exploration**: Cannot discover new state-action pairs\n",
        "\n",
        "**CQL Solution:**\n",
        "- Adds regularization term to push down Q-values of unseen actions\n",
        "- Prevents overestimation and improves stability\n",
        "- Trade-off controlled by `TRADEOFF_FACTOR`\n",
        "\n",
        "### Implementation Highlights\n",
        "\n",
        "1. **3-Layer Neural Network** (256 hidden units) for both actor and critics\n",
        "2. **Separate Optimizers** for critics, actor, and temperature\n",
        "3. **Target Networks** with soft updates (Ï„ = 0.01) for stability\n",
        "4. **Replay Buffer** for experience replay and offline training\n",
        "5. **CQL Regularization** using log-sum-exp trick\n",
        "\n",
        "### Performance Expectations\n",
        "\n",
        "- **Online SAC**: Highest performance, requires environment interaction\n",
        "- **Offline SAC**: Good baseline, may be unstable with suboptimal data\n",
        "- **CQL**: Best for offline, more stable and reliable\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "To improve the implementation:\n",
        "- Experiment with different network architectures\n",
        "- Tune hyperparameters (learning rate, batch size, Ï„)\n",
        "- Adjust `TRADEOFF_FACTOR` for your specific dataset\n",
        "- Try different environments (continuous action spaces)\n",
        "- Implement prioritized experience replay\n",
        "- Add multi-step returns for better credit assignment\n",
        "\n",
        "**Congratulations!** You've successfully implemented a state-of-the-art RL algorithm with both online and offline capabilities! ðŸŽ‰\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
