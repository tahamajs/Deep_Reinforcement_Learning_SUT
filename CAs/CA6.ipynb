{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b916fc1",
   "metadata": {},
   "source": [
    "# CA6: Policy Gradient Methods\n",
    "\n",
    "## Session 6: From Value-Based to Policy-Based Reinforcement Learning\n",
    "\n",
    "### Overview\n",
    "\n",
    "Welcome to Session 6 of the Deep Reinforcement Learning course. In the previous sessions, we focused on **value-based methods** like Deep Q-Networks (DQN) and its variants. These methods learn a value function and then derive a policy from it.\n",
    "\n",
    "In this session, we will explore a fundamentally different approach: **Policy Gradient (PG) methods**. Instead of learning a value function, we will directly parameterize and optimize the policy itself. This approach has several advantages, particularly in continuous action spaces and stochastic environments.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this session, you will be ableto:\n",
    "- Understand the theoretical foundations of Policy Gradient methods.\n",
    "- Differentiate between value-based and policy-based approaches.\n",
    "- Implement the REINFORCE algorithm from scratch.\n",
    "- Understand and mitigate the high variance problem in policy gradients.\n",
    "- Implement Actor-Critic methods, including A2C.\n",
    "- Analyze the trade-offs between different policy gradient algorithms.\n",
    "\n",
    "### Notebook Structure\n",
    "\n",
    "- **Part 1: Introduction to Policy Gradient Methods**: Theory, advantages, and mathematical foundations.\n",
    "- **Part 2: The REINFORCE Algorithm**: Derivation and implementation of the simplest policy gradient method.\n",
    "- **Part 3: Actor-Critic Methods**: Introducing a critic to reduce variance.\n",
    "- **Part 4: Advanced Actor-Critic (A2C/A3C)**: State-of-the-art policy gradient methods.\n",
    "- **Part 5: Practical Exercises and Assignments**: Hands-on coding exercises.\n",
    "- **Part 6: Theoretical Questions and Answers**: Deep dive into the theory.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Introduction to Policy Gradient Methods\n",
    "\n",
    "### 1.1 What are Policy Gradient Methods?\n",
    "\n",
    "Policy Gradient methods are a class of algorithms in reinforcement learning that directly learn a parameterized policy, denoted as **π_θ(a|s)**. The policy is typically represented by a neural network with parameters **θ**. The goal is to adjust the parameters **θ** to maximize the expected total reward.\n",
    "\n",
    "**Key Idea**: Instead of learning the values of state-action pairs, we directly learn the probability of taking each action in each state. The learning process involves updating the policy parameters in the direction of the gradient of the expected return.\n",
    "\n",
    "### 1.2 Value-Based vs. Policy-Based Methods\n",
    "\n",
    "| Feature | Value-Based Methods (e.g., DQN) | Policy-Based Methods (e.g., REINFORCE) |\n",
    "| :--- | :--- | :--- |\n",
    "| **What is Learned** | Learns a value function Q(s,a). | Learns a policy π(a|s). |\n",
    "| **Policy** | Implicitly derived from the value function (e.g., ε-greedy). | Explicitly represented and learned. |\n",
    "| **Action Space** | Primarily for discrete action spaces. | Can handle both discrete and continuous action spaces naturally. |\n",
    "| **Policy Type** | Typically deterministic (or ε-greedy). | Can learn stochastic policies. |\n",
    "| **Optimization** | Minimizes a TD error (e.g., MSE). | Maximizes an objective function (expected return) via gradient ascent. |\n",
    "\n",
    "### 1.3 Advantages and Disadvantages of Policy Gradient Methods\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "1.  **Continuous Action Spaces**: PG methods can naturally handle continuous action spaces by outputting the parameters of a probability distribution (e.g., mean and standard deviation for a Gaussian distribution).\n",
    "2.  **Stochastic Policies**: They can learn truly stochastic policies, which is beneficial in environments where the optimal policy is stochastic (e.g., rock-paper-scissors).\n",
    "3.  **Simpler Action Selection**: Once the policy is learned, selecting an action is a simple matter of sampling from the policy's output distribution. There's no need for a complex maximization step over Q-values.\n",
    "4.  **Better Convergence Properties**: In some cases, PG methods have better convergence properties than value-based methods, which can suffer from instabilities due to bootstrapping.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "1.  **High Variance**: The policy gradient estimate is often very noisy, leading to high variance during training. This can make learning slow and unstable.\n",
    "2.  **Sample Inefficiency**: Basic PG methods are often less sample-efficient than their value-based counterparts, requiring more interactions with the environment to learn.\n",
    "3.  **Local Optima**: The optimization process can get stuck in local optima, as it's performing gradient ascent on a potentially complex, non-convex landscape.\n",
    "\n",
    "### 1.4 Mathematical Foundations: The Policy Objective Function\n",
    "\n",
    "The goal of policy gradient methods is to find the optimal policy parameters **θ*** that maximize the expected total reward. We define an objective function, **J(θ)**, which represents the quality of the policy.\n",
    "\n",
    "For an episodic environment, the objective function is the expected return from the starting state distribution:\n",
    "\n",
    "**J(θ) = E_τ∼π_θ [R(τ)] = E_τ∼π_θ [Σ_{t=0}^{T} r_t]**\n",
    "\n",
    "Where:\n",
    "- **τ** is a trajectory (or episode) sampled from the policy **π_θ**.\n",
    "- **R(τ)** is the total reward of the trajectory.\n",
    "\n",
    "The core of policy gradient methods is to update the policy parameters **θ** using gradient ascent:\n",
    "\n",
    "**θ_{k+1} = θ_k + α ∇_θ J(θ_k)**\n",
    "\n",
    "Where **α** is the learning rate and **∇_θ J(θ)** is the policy gradient. The main challenge is to compute this gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cf0c3d",
   "metadata": {},
   "source": [
    "### 1.5 The Policy Gradient Theorem\n",
    "\n",
    "The core of policy gradient methods is the **Policy Gradient Theorem**, which provides a way to compute the gradient of the objective function **J(θ)** without needing to know the dynamics of the environment. The theorem provides an analytical expression for the policy gradient that we can estimate from samples.\n",
    "\n",
    "The theorem states that the gradient of the objective function is given by:\n",
    "\n",
    "**∇_θ J(θ) = E_τ∼π_θ [ (Σ_{t=0}^{T} ∇_θ log π_θ(a_t|s_t)) (Σ_{t=0}^{T} r(s_t, a_t)) ]**\n",
    "\n",
    "This form is often simplified to:\n",
    "\n",
    "**∇_θ J(θ) = E_τ∼π_θ [ Σ_{t=0}^{T} G_t ∇_θ log π_θ(a_t|s_t) ]**\n",
    "\n",
    "Where:\n",
    "- **G_t = Σ_{k=t}^{T} r_k** is the **return** (the sum of rewards from time step *t* to the end of the episode).\n",
    "- **∇_θ log π_θ(a_t|s_t)** is the gradient of the log-probability of taking action *a_t* in state *s_t*. This term tells us how to change the policy parameters to make the action *a_t* more or less likely.\n",
    "\n",
    "**Intuition:**\n",
    "- If the return **G_t** is high, we want to increase the probability of taking action **a_t** in state **s_t**.\n",
    "- If the return **G_t** is low (or negative), we want to decrease the probability of taking action **a_t** in state **s_t**.\n",
    "\n",
    "This theorem is powerful because it allows us to estimate the gradient using trajectories sampled from the environment, without needing to know the transition probabilities **P(s'|s,a)**.\n",
    "\n",
    "---\n",
    "\n",
    "## Part 2: The REINFORCE Algorithm (Monte-Carlo Policy Gradient)\n",
    "\n",
    "The REINFORCE algorithm, also known as Monte-Carlo Policy Gradient, is one of the most fundamental policy gradient algorithms. It directly applies the Policy Gradient Theorem by estimating the gradient from a batch of complete episodes.\n",
    "\n",
    "### 2.1 The REINFORCE Update Rule\n",
    "\n",
    "The REINFORCE algorithm collects a set of trajectories by running the current policy **π_θ** in the environment. Then, for each time step *t* in each trajectory, it computes the return **G_t** and uses it to update the policy parameters.\n",
    "\n",
    "The update rule for a single trajectory is:\n",
    "\n",
    "**θ ← θ + α Σ_{t=0}^{T} G_t ∇_θ log π_θ(a_t|s_t)**\n",
    "\n",
    "In practice, we collect a batch of trajectories and average the gradients over the batch.\n",
    "\n",
    "### 2.2 Algorithm: REINFORCE\n",
    "\n",
    "1.  **Initialize** the policy network **π_θ** with random parameters **θ**.\n",
    "2.  **Loop forever** (for each episode):\n",
    "    a. **Generate** an episode (a trajectory) by running the policy **π_θ**:\n",
    "       τ = (s_0, a_0, r_1, s_1, a_1, r_2, ..., s_T, a_T, r_{T+1})\n",
    "    b. **For each time step** t = 0, 1, ..., T:\n",
    "       i. **Compute** the return **G_t = Σ_{k=t+1}^{T+1} r_k**.\n",
    "    c. **Update** the policy parameters **θ**:\n",
    "       **θ ← θ + α Σ_{t=0}^{T} G_t ∇_θ log π_θ(a_t|s_t)**\n",
    "\n",
    "### 2.3 The High Variance Problem\n",
    "\n",
    "A major drawback of the REINFORCE algorithm is the high variance of the gradient estimate. The return **G_t** can vary significantly depending on the trajectory, even for the same state-action pair. This high variance can make the learning process slow and unstable.\n",
    "\n",
    "**Sources of Variance:**\n",
    "1.  **Stochasticity in the Environment**: The environment's transition function and reward function can be stochastic.\n",
    "2.  **Stochasticity in the Policy**: The policy itself is stochastic, leading to different actions and trajectories.\n",
    "\n",
    "In the next sections, we will explore techniques to reduce this variance, such as using a baseline and the Actor-Critic architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820cac0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.4 Implementation of REINFORCE\n",
    "\n",
    "Let's implement the REINFORCE algorithm to solve the `CartPole-v1` environment from OpenAI Gym.\n",
    "\n",
    "First, we need to set up our environment and import the necessary libraries.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up the environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "gamma = 0.99\n",
    "num_episodes = 1000\n",
    "```\n",
    "\n",
    "Next, we define our policy network. For a discrete action space, the network will output logits for each action, which we can convert to probabilities using a softmax function.\n",
    "\n",
    "```python\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "policy_net = PolicyNetwork(state_size, action_size)\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "```\n",
    "\n",
    "Now, we can write the main training loop for the REINFORCE algorithm.\n",
    "\n",
    "```python\n",
    "def train_reinforce():\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        \n",
    "        # Generate an episode\n",
    "        while True:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            logits = policy_net(state_tensor)\n",
    "            action_dist = Categorical(logits=logits)\n",
    "            action = action_dist.sample()\n",
    "            \n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(sum(rewards))\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = []\n",
    "        G_t = 0\n",
    "        for r in reversed(rewards):\n",
    "            G_t = r + gamma * G_t\n",
    "            returns.insert(0, G_t)\n",
    "        \n",
    "        returns = torch.tensor(returns)\n",
    "        # Normalize returns for stability\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-9)\n",
    "        \n",
    "        # Compute policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)\n",
    "        \n",
    "        # Update policy\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}, Average Reward: {np.mean(all_rewards[-100:])}\")\n",
    "            \n",
    "    return all_rewards\n",
    "\n",
    "# Train the agent\n",
    "rewards_history = train_reinforce()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_history)\n",
    "plt.title('REINFORCE Training on CartPole-v1')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This implementation demonstrates the core concepts of the REINFORCE algorithm. Note the normalization of returns, which is a common trick to stabilize training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98dbd9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: Actor-Critic Methods - Reducing Variance\n",
    "\n",
    "As we've seen, the REINFORCE algorithm suffers from high variance because the return **G_t** can be very noisy. Actor-Critic methods address this by introducing a **critic**, which learns a value function to assist the **actor** (the policy).\n",
    "\n",
    "### 3.1 Introducing a Baseline\n",
    "\n",
    "A simple way to reduce the variance of the policy gradient is to subtract a **baseline** from the return **G_t**. The baseline, **b(s_t)**, is a function of the state **s_t**. The policy gradient with a baseline is:\n",
    "\n",
    "**∇_θ J(θ) = E_τ∼π_θ [ Σ_{t=0}^{T} (G_t - b(s_t)) ∇_θ log π_θ(a_t|s_t) ]**\n",
    "\n",
    "This modification does not change the expected value of the gradient, so it doesn't introduce bias. However, a well-chosen baseline can significantly reduce the variance.\n",
    "\n",
    "**What is a good baseline?**\n",
    "A good baseline is the **state-value function, V(s_t)**. The term **A(s_t, a_t) = G_t - V(s_t)** is an estimate of the **advantage function**, which measures how much better than average it is to take action *a_t* in state *s_t*.\n",
    "\n",
    "Using the advantage function, the policy gradient becomes:\n",
    "\n",
    "**∇_θ J(θ) = E_τ∼π_θ [ Σ_{t=0}^{T} A(s_t, a_t) ∇_θ log π_θ(a_t|s_t) ]**\n",
    "\n",
    "### 3.2 The Actor-Critic Architecture\n",
    "\n",
    "This leads us to the Actor-Critic architecture, which consists of two components:\n",
    "\n",
    "1.  **The Actor**: A policy network **π_θ(a|s)** that controls how the agent acts.\n",
    "2.  **The Critic**: A value network **V_φ(s)** that estimates the state-value function **V(s)**.\n",
    "\n",
    "**How it works:**\n",
    "- The **actor** decides which action to take.\n",
    "- The **critic** evaluates the action by computing the advantage function.\n",
    "- The **actor** updates its policy in the direction suggested by the critic.\n",
    "- The **critic** updates its value function to be more accurate.\n",
    "\n",
    "**The Actor-Critic Update Cycle:**\n",
    "\n",
    "1.  **Actor Update (Policy Update)**:\n",
    "    - The actor uses the critic's value estimate to compute the advantage:\n",
    "      **A(s_t, a_t) ≈ r_t + γV_φ(s_{t+1}) - V_φ(s_t)**\n",
    "    - The actor's parameters **θ** are updated using this advantage:\n",
    "      **θ ← θ + α_actor A(s_t, a_t) ∇_θ log π_θ(a_t|s_t)**\n",
    "\n",
    "2.  **Critic Update (Value Update)**:\n",
    "    - The critic's parameters **φ** are updated to minimize the error between its value estimate and the observed return (TD error):\n",
    "      **δ_t = r_t + γV_φ(s_{t+1}) - V_φ(s_t)**\n",
    "      **φ ← φ + α_critic δ_t ∇_φ V_φ(s_t)**\n",
    "\n",
    "This approach is more stable and sample-efficient than REINFORCE because it uses the critic's value estimates to reduce variance and provide more informative updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.3 Implementation of a Basic Actor-Critic Algorithm\n",
    "\n",
    "Let's implement a simple one-step Actor-Critic algorithm. We'll need two networks: one for the actor (policy) and one for the critic (value function).\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make('CartPole-v1')\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Hyperparameters\n",
    "actor_lr = 0.001\n",
    "critic_lr = 0.005\n",
    "gamma = 0.99\n",
    "num_episodes = 1000\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        logits = self.fc2(x)\n",
    "        return logits\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        value = self.fc2(x)\n",
    "        return value\n",
    "\n",
    "actor = Actor(state_size, action_size)\n",
    "critic = Critic(state_size)\n",
    "actor_optimizer = optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "critic_optimizer = optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "\n",
    "def train_actor_critic():\n",
    "    all_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            # Actor selects an action\n",
    "            logits = actor(state_tensor)\n",
    "            action_dist = Categorical(logits=logits)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            \n",
    "            # Environment step\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            episode_reward += reward\n",
    "            \n",
    "            # Critic evaluates the state\n",
    "            value = critic(state_tensor)\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            next_value = critic(next_state_tensor)\n",
    "            \n",
    "            if done:\n",
    "                next_value = torch.tensor([0.0])\n",
    "            \n",
    "            # Compute advantage and TD error\n",
    "            advantage = reward + gamma * next_value - value\n",
    "            td_error = advantage\n",
    "            \n",
    "            # Actor update\n",
    "            actor_loss = -log_prob * advantage.detach()\n",
    "            actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            actor_optimizer.step()\n",
    "            \n",
    "            # Critic update\n",
    "            critic_loss = td_error.pow(2)\n",
    "            critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            critic_optimizer.step()\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(episode_reward)\n",
    "        if (episode + 1) % 100 == 0:\n",
    "            print(f\"Episode {episode+1}, Average Reward: {np.mean(all_rewards[-100:])}\")\n",
    "            \n",
    "    return all_rewards\n",
    "\n",
    "# Train the agent\n",
    "ac_rewards_history = train_actor_critic()\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ac_rewards_history)\n",
    "plt.title('Actor-Critic Training on CartPole-v1')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "This implementation shows a basic Actor-Critic setup. The actor and critic are updated at each time step, making it more sample-efficient than the episodic updates of REINFORCE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fdd657",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 4: Advanced Actor-Critic Methods (A2C and A3C)\n",
    "\n",
    "The basic Actor-Critic algorithm can be improved further. Two popular and powerful extensions are the **Advantage Actor-Critic (A2C)** and the **Asynchronous Advantage Actor-Critic (A3C)**.\n",
    "\n",
    "### 4.1 Advantage Actor-Critic (A2C)\n",
    "\n",
    "A2C is a synchronous, deterministic version of A3C. It waits for all actors to finish their segment of experience before updating the global network, which can be more efficient on GPUs.\n",
    "\n",
    "The core idea of A2C is to use a more sophisticated advantage function estimate. Instead of the one-step TD error, A2C often uses an **n-step return** to compute the advantage.\n",
    "\n",
    "**n-step Advantage:**\n",
    "**A(s_t, a_t) ≈ (Σ_{i=0}^{n-1} γ^i r_{t+i}) + γ^n V(s_{t+n}) - V(s_t)**\n",
    "\n",
    "This provides a better trade-off between bias and variance.\n",
    "\n",
    "### 4.2 Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "A3C is a parallel version of the Actor-Critic algorithm. It uses multiple actors, each with its own copy of the environment, to collect experience in parallel.\n",
    "\n",
    "**How A3C works:**\n",
    "1.  **Global Network**: There is a single global network with parameters **θ** (for the actor) and **φ** (for the critic).\n",
    "2.  **Worker Actors**: Multiple worker threads are created. Each worker has its own copy of the actor and critic networks and its own environment.\n",
    "3.  **Parallel Experience Collection**: Each worker interacts with its environment for a fixed number of steps, collecting a trajectory of experience.\n",
    "4.  **Asynchronous Updates**: After collecting experience, each worker computes the gradients for the actor and critic and updates the global network asynchronously.\n",
    "\n",
    "**Advantages of A3C:**\n",
    "- **Decorrelated Experience**: Because the workers are exploring different parts of the state space, their experiences are more decorrelated, which stabilizes training.\n",
    "- **Faster Training**: Parallelism allows for much faster training times.\n",
    "- **No Replay Buffer**: A3C does not require an experience replay buffer, which saves memory.\n",
    "\n",
    "### 4.3 A2C/A3C Implementation Details\n",
    "\n",
    "A common implementation detail for A2C and A3C is to have the actor and critic share the initial layers of their networks. This can improve learning efficiency as both components can benefit from the shared feature representation.\n",
    "\n",
    "**Entropy Regularization:**\n",
    "To encourage exploration, a term is often added to the actor's loss function that penalizes the policy for being too deterministic. This is the **entropy** of the policy.\n",
    "\n",
    "**Actor Loss with Entropy Regularization:**\n",
    "**L_actor = - (A(s_t, a_t) ∇_θ log π_θ(a_t|s_t) + β H(π_θ(·|s_t)))**\n",
    "\n",
    "Where:\n",
    "- **H(π_θ(·|s_t))** is the entropy of the policy distribution.\n",
    "- **β** is a hyperparameter that controls the strength of the entropy regularization.\n",
    "\n",
    "This encourages the policy to maintain some randomness, which helps with exploration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
