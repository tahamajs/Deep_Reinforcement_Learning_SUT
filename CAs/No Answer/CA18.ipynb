{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e72e3b80",
   "metadata": {},
   "source": [
    "# CA18: Advanced Deep Reinforcement Learning - Comprehensive Exercise\n",
    "\n",
    "## Course: Deep Reinforcement Learning\n",
    "## Assignment: CA18 - Advanced RL Paradigms Implementation and Analysis\n",
    "## Date: July 2025\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Learning Objectives\n",
    "\n",
    "By the end of this comprehensive exercise, you will:\n",
    "\n",
    "1. **Master Advanced RL Paradigms**: Understand and implement 5 cutting-edge RL approaches\n",
    "2. **Theoretical Foundations**: Grasp the mathematical principles underlying each method\n",
    "3. **Practical Implementation**: Build working systems from scratch using PyTorch\n",
    "4. **Performance Analysis**: Compare and evaluate different approaches scientifically\n",
    "5. **Integration Skills**: Combine multiple paradigms for enhanced performance\n",
    "6. **Real-world Applications**: Apply techniques to practical scenarios\n",
    "\n",
    "## üéØ Exercise Structure\n",
    "\n",
    "This exercise covers **5 major advanced RL paradigms**:\n",
    "\n",
    "### **Part I: World Models and Imagination-Augmented Agents**\n",
    "- Theory: Model-based RL, recurrent state space models, planning\n",
    "- Implementation: RSSM, world model, MPC planner, imagination-augmented agent\n",
    "- Exercise: Build and evaluate a planning-based RL agent\n",
    "\n",
    "### **Part II: Multi-Agent Deep Reinforcement Learning**\n",
    "- Theory: Game theory, coordination, communication, MARL algorithms\n",
    "- Implementation: MADDPG, communication networks, multi-agent environments\n",
    "- Exercise: Create cooperative and competitive multi-agent systems\n",
    "\n",
    "### **Part III: Causal Reinforcement Learning**\n",
    "- Theory: Causality, interventions, counterfactual reasoning, causal discovery\n",
    "- Implementation: Causal graphs, PC algorithm, causal mechanisms\n",
    "- Exercise: Build causally-aware RL agents for robust decision making\n",
    "\n",
    "### **Part IV: Quantum-Enhanced Reinforcement Learning**\n",
    "- Theory: Quantum computing, variational quantum circuits, quantum advantage\n",
    "- Implementation: Quantum gates, VQC, quantum policy networks\n",
    "- Exercise: Explore quantum speedups in RL problems\n",
    "\n",
    "### **Part V: Federated Reinforcement Learning**\n",
    "- Theory: Distributed learning, privacy preservation, communication efficiency\n",
    "- Implementation: FedAvg-RL, differential privacy, secure aggregation\n",
    "- Exercise: Build privacy-preserving collaborative RL systems\n",
    "\n",
    "### **Part VI: Integration and Analysis**\n",
    "- Comparative analysis of all methods\n",
    "- Hybrid approaches combining multiple paradigms\n",
    "- Real-world application scenarios\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- **Mathematical Background**: Linear algebra, probability theory, calculus\n",
    "- **Programming Skills**: Python, PyTorch, NumPy, Matplotlib\n",
    "- **RL Knowledge**: Basic RL concepts (MDP, policy gradient, value functions)\n",
    "- **Deep Learning**: Neural networks, backpropagation, optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Let's Begin!\n",
    "\n",
    "This comprehensive exercise will take you through the most advanced techniques in modern Deep Reinforcement Learning. Each section builds upon previous knowledge while introducing cutting-edge concepts that represent the future of AI.\n",
    "\n",
    "**Ready to explore the frontiers of artificial intelligence? Let's dive in!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7298315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Setup Complete!\n",
      "Device: cpu\n",
      "PyTorch version: 2.4.1\n",
      "NumPy version: 1.24.3\n",
      "Ready to explore advanced Deep Reinforcement Learning! ü§ñ\n"
     ]
    }
   ],
   "source": [
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import deque, defaultdict\nimport random\nimport time\nimport copy\nimport warnings\nfrom typing import List, Dict, Tuple, Optional, Union, Any\nfrom abc import ABC, abstractmethod\nimport networkx as nx\nfrom scipy import stats\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cluster import KMeans\nimport gym\nimport math\nimport cmath\nfrom scipy.linalg import expm\nfrom itertools import combinations, permutations\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\nplt.style.use('default')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\nwarnings.filterwarnings('ignore')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"üöÄ Setup Complete!\")\nprint(f\"Device: {device}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\nprint(\"Ready to explore advanced Deep Reinforcement Learning! ü§ñ\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9ef2a",
   "metadata": {},
   "source": [
    "# Part I: World Models and Imagination-Augmented Agents\n",
    "\n",
    "## üåç Theoretical Foundation\n",
    "\n",
    "### Introduction to World Models\n",
    "\n",
    "**World Models** represent a paradigm shift in reinforcement learning, moving from model-free to model-based approaches that learn internal representations of the environment. This approach was popularized by Ha and Schmidhuber (2018) and has revolutionized how we think about sample efficiency and planning in RL.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "#### 1. Model-Based Reinforcement Learning\n",
    "\n",
    "Traditional model-free RL learns policies directly from experience:\n",
    "- **Pro**: No need to model environment dynamics\n",
    "- **Con**: Sample inefficient, cannot plan ahead\n",
    "\n",
    "Model-based RL learns a model of the environment:\n",
    "- **Pro**: Can plan using learned model, more sample efficient  \n",
    "- **Con**: Model errors can compound, more complex\n",
    "\n",
    "#### 2. Recurrent State Space Models (RSSM)\n",
    "\n",
    "The RSSM is the heart of world models, consisting of:\n",
    "\n",
    "**Deterministic Path**: $h_t = f_\\theta(h_{t-1}, a_{t-1})$\n",
    "- Encodes deterministic aspects of state evolution\n",
    "- Uses RNN/LSTM/GRU to maintain temporal consistency\n",
    "\n",
    "**Stochastic Path**: $s_t \\sim p(s_t | h_t)$  \n",
    "- Models stochastic aspects and uncertainty\n",
    "- Typically Gaussian: $s_t \\sim \\mathcal{N}(\\mu_\\phi(h_t), \\sigma_\\phi(h_t))$\n",
    "\n",
    "**Combined State**: $z_t = [h_t, s_t]$\n",
    "- Combines deterministic and stochastic components\n",
    "- Provides rich representation for planning\n",
    "\n",
    "#### 3. Three-Component Architecture\n",
    "\n",
    "**1. Representation Model (Encoder)**\n",
    "$$h_t = f_\\theta(h_{t-1}, a_{t-1}, o_t)$$\n",
    "- Encodes observations into internal state\n",
    "- Maintains temporal consistency\n",
    "\n",
    "**2. Transition Model**  \n",
    "$$\\hat{s}_{t+1}, \\hat{h}_{t+1} = g_\\phi(s_t, h_t, a_t)$$\n",
    "- Predicts next state from current state and action\n",
    "- Enables forward simulation\n",
    "\n",
    "**3. Observation Model (Decoder)**\n",
    "$$\\hat{o}_t = d_\\psi(s_t, h_t)$$\n",
    "- Reconstructs observations from internal state\n",
    "- Ensures representation quality\n",
    "\n",
    "#### 4. Imagination-Augmented Agents (I2A)\n",
    "\n",
    "I2A extends world models by using \"imagination\" for policy learning:\n",
    "\n",
    "**Imagination Rollouts**:\n",
    "- Use world model to simulate future trajectories\n",
    "- Generate imagined experiences: $\\tau^{imagine} = \\{(s_t^i, a_t^i, r_t^i)\\}_{t=0}^H$\n",
    "\n",
    "**Imagination Encoder**:\n",
    "- Process imagined trajectories into useful features\n",
    "- Extract planning-relevant information\n",
    "\n",
    "**Policy Network**:\n",
    "- Combines real observations with imagination features  \n",
    "- Makes decisions using both current state and future projections\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### State Space Model\n",
    "\n",
    "The world model learns a latent state space representation:\n",
    "\n",
    "$$p(s_{1:T}, o_{1:T} | a_{1:T}) = \\prod_{t=1}^T p(s_t | s_{t-1}, a_{t-1}) p(o_t | s_t)$$\n",
    "\n",
    "Where:\n",
    "- $s_t$: latent state at time $t$\n",
    "- $o_t$: observation at time $t$  \n",
    "- $a_t$: action at time $t$\n",
    "\n",
    "#### Training Objectives\n",
    "\n",
    "**1. Reconstruction Loss**:\n",
    "$$\\mathcal{L}_{recon} = \\mathbb{E}_{(o,a) \\sim \\mathcal{D}}[||o - \\hat{o}||^2]$$\n",
    "\n",
    "**2. KL Regularization**:\n",
    "$$\\mathcal{L}_{KL} = \\mathbb{E}_{s \\sim q_\\phi}[D_{KL}(q_\\phi(s|o,h) || p(s|h))]$$\n",
    "\n",
    "**3. Prediction Loss**:\n",
    "$$\\mathcal{L}_{pred} = \\mathbb{E}_{(s,a,s') \\sim \\mathcal{D}}[||s' - \\hat{s}'||^2]$$\n",
    "\n",
    "**Total Loss**:\n",
    "$$\\mathcal{L}_{world} = \\mathcal{L}_{recon} + \\beta \\mathcal{L}_{KL} + \\lambda \\mathcal{L}_{pred}$$\n",
    "\n",
    "### Planning Algorithms\n",
    "\n",
    "#### 1. Model Predictive Control (MPC)\n",
    "\n",
    "MPC uses the world model for online planning:\n",
    "\n",
    "1. **Rollout**: Simulate $H$-step trajectories using world model\n",
    "2. **Evaluate**: Score trajectories using reward predictions  \n",
    "3. **Execute**: Take first action of best trajectory\n",
    "4. **Replan**: Repeat process at next timestep\n",
    "\n",
    "**MPC Objective**:\n",
    "$$a^* = \\arg\\max_a \\sum_{h=1}^H \\gamma^h r(s_h, a_h)$$\n",
    "\n",
    "where $(s_h, a_h)$ come from world model rollouts.\n",
    "\n",
    "#### 2. Cross Entropy Method (CEM)\n",
    "\n",
    "CEM is a population-based optimization method:\n",
    "\n",
    "1. **Sample**: Generate action sequence population\n",
    "2. **Evaluate**: Score sequences using world model\n",
    "3. **Select**: Keep top-performing sequences\n",
    "4. **Update**: Fit distribution to elite sequences\n",
    "5. **Repeat**: Iterate until convergence\n",
    "\n",
    "### Advantages and Applications\n",
    "\n",
    "**Advantages**:\n",
    "- **Sample Efficiency**: Learn from imagined experiences\n",
    "- **Planning Capability**: Look ahead before acting\n",
    "- **Transfer Learning**: World models can transfer across tasks\n",
    "- **Interpretability**: Can visualize agent's internal world understanding\n",
    "\n",
    "**Applications**:\n",
    "- **Robotics**: Sample-efficient robot learning\n",
    "- **Game Playing**: Strategic planning in complex games  \n",
    "- **Autonomous Driving**: Safe planning with uncertainty\n",
    "- **Finance**: Portfolio optimization with market models\n",
    "\n",
    "### Key Research Papers\n",
    "\n",
    "1. **World Models** (Ha & Schmidhuber, 2018)\n",
    "2. **PlaNet** (Hafner et al., 2019)  \n",
    "3. **DreamerV1** (Hafner et al., 2020)\n",
    "4. **DreamerV2** (Hafner et al., 2021)\n",
    "5. **I2A** (Weber et al., 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "113977f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ World Models Implementation Complete!\n",
      "Components implemented:\n",
      "- RSSMCore: Recurrent state space model with deterministic/stochastic components\n",
      "- WorldModel: Complete world model with encoder/decoder and predictors\n",
      "- MPCPlanner: Cross-entropy method planner for action sequence optimization\n",
      "- ImaginationAugmentedAgent: I2A-style agent combining model-free and imagination\n"
     ]
    }
   ],
   "source": [
    "class RSSMCore(nn.Module):\n    def __init__(self, state_dim: int = 30, hidden_dim: int = 200, \n                 action_dim: int = 2, embed_dim: int = 1024):\n        super().__init__()\n        self.state_dim = state_dim\n        self.hidden_dim = hidden_dim\n        self.action_dim = action_dim\n        self.embed_dim = embed_dim\n        self.rnn = nn.GRUCell(state_dim + action_dim, hidden_dim)\n        self.prior_net = nn.Sequential(\n            nn.Linear(hidden_dim, state_dim * 2),\n        )\n        self.posterior_net = nn.Sequential(\n            nn.Linear(hidden_dim + embed_dim, state_dim * 2),\n        )\n    def initial_state(self, batch_size: int) -> Dict[str, torch.Tensor]:\n        return {\n            'hidden': torch.zeros(batch_size, self.hidden_dim, device=device),\n            'stoch': torch.zeros(batch_size, self.state_dim, device=device)\n        }\n    def observe(self, embed: torch.Tensor, action: torch.Tensor, \n                state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        hidden = self.rnn(\n            torch.cat([state['stoch'], action], dim=1), \n            state['hidden']\n        )\n        posterior_input = torch.cat([hidden, embed], dim=1)\n        posterior_params = self.posterior_net(posterior_input)\n        posterior_mean, posterior_logstd = posterior_params.chunk(2, dim=1)\n        posterior_std = torch.exp(posterior_logstd)\n        stoch = posterior_mean + posterior_std * torch.randn_like(posterior_std)\n        prior_params = self.prior_net(hidden)\n        prior_mean, prior_logstd = prior_params.chunk(2, dim=1)\n        prior_std = torch.exp(prior_logstd)\n        return {\n            'hidden': hidden,\n            'stoch': stoch,\n            'prior_mean': prior_mean,\n            'prior_std': prior_std,\n            'posterior_mean': posterior_mean,\n            'posterior_std': posterior_std\n        }\n    def imagine(self, action: torch.Tensor, \n                state: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        hidden = self.rnn(\n            torch.cat([state['stoch'], action], dim=1),\n            state['hidden']\n        )\n        prior_params = self.prior_net(hidden)\n        prior_mean, prior_logstd = prior_params.chunk(2, dim=1)\n        prior_std = torch.exp(prior_logstd)\n        stoch = prior_mean + prior_std * torch.randn_like(prior_std)\n        return {\n            'hidden': hidden,\n            'stoch': stoch,\n            'prior_mean': prior_mean,\n            'prior_std': prior_std\n        }\nclass WorldModel(nn.Module):\n    def __init__(self, obs_dim: int, action_dim: int, state_dim: int = 30,\n                 hidden_dim: int = 200, embed_dim: int = 1024):\n        super().__init__()\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.state_dim = state_dim\n        self.hidden_dim = hidden_dim\n        self.embed_dim = embed_dim\n        self.encoder = nn.Sequential(\n            nn.Linear(obs_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(), \n            nn.Linear(512, embed_dim),\n        )\n        self.rssm = RSSMCore(state_dim, hidden_dim, action_dim, embed_dim)\n        self.decoder = nn.Sequential(\n            nn.Linear(state_dim + hidden_dim, 512),\n            nn.ReLU(),\n            nn.Linear(512, 256), \n            nn.ReLU(),\n            nn.Linear(256, obs_dim),\n        )\n        self.reward_model = nn.Sequential(\n            nn.Linear(state_dim + hidden_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1),\n        )\n        self.continue_model = nn.Sequential(\n            nn.Linear(state_dim + hidden_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1),\n            nn.Sigmoid()\n        )\n    def encode(self, obs: torch.Tensor) -> torch.Tensor:\n        return self.encoder(obs)\n    def decode(self, state: Dict[str, torch.Tensor]) -> torch.Tensor:\n        state_concat = torch.cat([state['stoch'], state['hidden']], dim=1)\n        return self.decoder(state_concat)\n    def predict_reward(self, state: Dict[str, torch.Tensor]) -> torch.Tensor:\n        state_concat = torch.cat([state['stoch'], state['hidden']], dim=1)\n        return self.reward_model(state_concat)\n    def predict_continue(self, state: Dict[str, torch.Tensor]) -> torch.Tensor:\n        state_concat = torch.cat([state['stoch'], state['hidden']], dim=1)\n        return self.continue_model(state_concat)\n    def observe_sequence(self, obs_seq: torch.Tensor, \n                        action_seq: torch.Tensor) -> Dict[str, torch.Tensor]:\n        batch_size, seq_len = obs_seq.shape[:2]\n        state = self.rssm.initial_state(batch_size)\n        states = []\n        reconstructions = []\n        rewards = []\n        continues = []\n        kl_losses = []\n        for t in range(seq_len):\n            embed = self.encode(obs_seq[:, t])\n            if t == 0:\n                action = torch.zeros(batch_size, self.action_dim, device=device)\n            else:\n                action = action_seq[:, t-1]\n            state = self.rssm.observe(embed, action, state)\n            states.append(state)\n            reconstruction = self.decode(state)\n            reward = self.predict_reward(state)\n            continue_prob = self.predict_continue(state)\n            reconstructions.append(reconstruction)\n            rewards.append(reward)\n            continues.append(continue_prob)\n            if 'posterior_mean' in state:\n                kl = self._kl_divergence(\n                    state['posterior_mean'], state['posterior_std'],\n                    state['prior_mean'], state['prior_std']\n                )\n                kl_losses.append(kl)\n        return {\n            'states': states,\n            'reconstructions': torch.stack(reconstructions, dim=1),\n            'rewards': torch.stack(rewards, dim=1),\n            'continues': torch.stack(continues, dim=1),\n            'kl_losses': torch.stack(kl_losses, dim=1) if kl_losses else None\n        }\n    def imagine_sequence(self, initial_state: Dict[str, torch.Tensor],\n                        actions: torch.Tensor) -> Dict[str, torch.Tensor]:\n        batch_size, seq_len = actions.shape[:2]\n        state = {k: v.clone() for k, v in initial_state.items()}\n        states = [state]\n        rewards = []\n        continues = []\n        for t in range(seq_len):\n            state = self.rssm.imagine(actions[:, t], state)\n            states.append(state)\n            reward = self.predict_reward(state)\n            continue_prob = self.predict_continue(state)\n            rewards.append(reward)\n            continues.append(continue_prob)\n        return {\n            'states': states[1:],\n            'rewards': torch.stack(rewards, dim=1),\n            'continues': torch.stack(continues, dim=1)\n        }\n    def _kl_divergence(self, mean1: torch.Tensor, std1: torch.Tensor,\n                      mean2: torch.Tensor, std2: torch.Tensor) -> torch.Tensor:\n        var1 = std1.pow(2)\n        var2 = std2.pow(2)\n        kl = (var1 / var2 + (mean2 - mean1).pow(2) / var2 + \n              torch.log(std2 / std1) - 1).sum(dim=1, keepdim=True)\n        return 0.5 * kl\nclass MPCPlanner:\n    def __init__(self, world_model: WorldModel, action_dim: int,\n                 horizon: int = 12, n_candidates: int = 1000, \n                 n_iterations: int = 10, n_elite: int = 100):\n        self.world_model = world_model\n        self.action_dim = action_dim  \n        self.horizon = horizon\n        self.n_candidates = n_candidates\n        self.n_iterations = n_iterations\n        self.n_elite = n_elite\n        self.action_min = -1.0\n        self.action_max = 1.0\n    def plan(self, initial_state: Dict[str, torch.Tensor]) -> torch.Tensor:\n        batch_size = initial_state['hidden'].shape[0]\n        mean = torch.zeros(batch_size, self.horizon, self.action_dim, device=device)\n        std = torch.ones(batch_size, self.horizon, self.action_dim, device=device)\n        for iteration in range(self.n_iterations):\n            noise = torch.randn(batch_size, self.n_candidates, self.horizon, \n                              self.action_dim, device=device)\n            mean_expanded = mean.unsqueeze(1).expand(-1, self.n_candidates, -1, -1)\n            std_expanded = std.unsqueeze(1).expand(-1, self.n_candidates, -1, -1)\n            actions = mean_expanded + std_expanded * noise\n            actions = torch.clamp(actions, self.action_min, self.action_max)\n            returns = self._evaluate_sequences(initial_state, actions)\n            _, elite_indices = torch.topk(returns, self.n_elite, dim=1)\n            for b in range(batch_size):\n                elite_actions = actions[b, elite_indices[b]]\n                mean[b] = elite_actions.mean(dim=0)\n                std[b] = elite_actions.std(dim=0) + 1e-6\n        final_noise = torch.randn(batch_size, 1, self.horizon, \n                                self.action_dim, device=device)\n        final_actions = mean.unsqueeze(1) + std.unsqueeze(1) * final_noise\n        final_actions = torch.clamp(final_actions, self.action_min, self.action_max)\n        return final_actions[:, 0, 0]\n    def _evaluate_sequences(self, initial_state: Dict[str, torch.Tensor],\n                           actions: torch.Tensor) -> torch.Tensor:\n        batch_size, n_candidates = actions.shape[:2]\n        expanded_state = {}\n        for key, value in initial_state.items():\n            expanded_state[key] = value.unsqueeze(1).expand(\n                -1, n_candidates, -1\n            ).reshape(batch_size * n_candidates, -1)\n        actions_flat = actions.reshape(batch_size * n_candidates, self.horizon, -1)\n        with torch.no_grad():\n            imagined = self.world_model.imagine_sequence(expanded_state, actions_flat)\n        rewards = imagined['rewards']\n        continues = imagined['continues']\n        gamma = 0.99\n        returns = torch.zeros(batch_size * n_candidates, device=device)\n        for t in range(self.horizon):\n            discount = gamma ** t\n            continue_discount = torch.prod(continues[:, :t+1], dim=1) if t > 0 else continues[:, 0]\n            returns += discount * continue_discount.squeeze() * rewards[:, t].squeeze()\n        returns = returns.reshape(batch_size, n_candidates)\n        return returns\nclass ImaginationAugmentedAgent(nn.Module):\n    def __init__(self, obs_dim: int, action_dim: int, world_model: WorldModel,\n                 planner: MPCPlanner, hidden_dim: int = 256):\n        super().__init__()\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.world_model = world_model\n        self.planner = planner\n        self.model_free_policy = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()\n        )\n        self.value_function = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(), \n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.imagination_encoder = nn.Sequential(\n            nn.Linear(world_model.state_dim + world_model.hidden_dim + 1, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU()\n        )\n        self.combined_policy = nn.Sequential(\n            nn.Linear(action_dim + 64, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()\n        )\n    def forward(self, obs: torch.Tensor, use_imagination: bool = True) -> Dict[str, torch.Tensor]:\n        batch_size = obs.shape[0]\n        mf_action = self.model_free_policy(obs)\n        value = self.value_function(obs)\n        if not use_imagination:\n            return {\n                'action': mf_action,\n                'value': value,\n                'imagination_features': None\n            }\n        with torch.no_grad():\n            embed = self.world_model.encode(obs)\n            initial_state = self.world_model.rssm.initial_state(batch_size)\n            dummy_action = torch.zeros(batch_size, self.action_dim, device=device)\n            current_state = self.world_model.rssm.observe(embed, dummy_action, initial_state)\n        imagination_features = self._generate_imagination_features(current_state)\n        combined_input = torch.cat([mf_action, imagination_features], dim=1)\n        final_action = self.combined_policy(combined_input)\n        return {\n            'action': final_action,\n            'value': value,\n            'imagination_features': imagination_features,\n            'mf_action': mf_action\n        }\n    def _generate_imagination_features(self, state: Dict[str, torch.Tensor]) -> torch.Tensor:\n        batch_size = state['hidden'].shape[0]\n        horizon = 5\n        imagination_actions = torch.randn(batch_size, horizon, self.action_dim, device=device)\n        imagination_actions = torch.clamp(imagination_actions, -1, 1)\n        with torch.no_grad():\n            imagined = self.world_model.imagine_sequence(state, imagination_actions)\n        features = []\n        for t in range(horizon):\n            state_t = imagined['states'][t]\n            reward_t = imagined['rewards'][:, t]\n            state_concat = torch.cat([state_t['stoch'], state_t['hidden'], reward_t], dim=1)\n            step_features = self.imagination_encoder(state_concat)\n            features.append(step_features)\n        imagination_features = torch.stack(features, dim=1).mean(dim=1)\n        return imagination_features\nprint(\"‚úÖ World Models Implementation Complete!\")\nprint(\"Components implemented:\")\nprint(\"- RSSMCore: Recurrent state space model with deterministic/stochastic components\")\nprint(\"- WorldModel: Complete world model with encoder/decoder and predictors\")  \nprint(\"- MPCPlanner: Cross-entropy method planner for action sequence optimization\")\nprint(\"- ImaginationAugmentedAgent: I2A-style agent combining model-free and imagination\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149e9348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Exercise 1: World Models Training and Evaluation\n",
      "======================================================================\n",
      "Environment: 4D state, 2D action\n",
      "Collecting 50 episodes of random data...\n",
      "Episode 0/50\n",
      "Episode 20/50\n",
      "Episode 40/50\n",
      "Collected 50 episodes\n",
      "Created 54 training batches\n",
      "World model parameters: 601,830\n",
      "Training world model for 30 epochs...\n",
      "Epoch 0: Total=2.8761, Recon=0.4948, KL=0.6107, Reward=2.3202\n",
      "Epoch 10: Total=0.2671, Recon=0.1020, KL=-0.6128, Reward=0.2264\n",
      "Epoch 20: Total=0.4155, Recon=0.0885, KL=-0.7578, Reward=0.4028\n",
      "Evaluating MPC planning for 10 episodes...\n",
      "Episode 0: Reward=-28.49, Length=200\n",
      "Episode 5: Reward=-35.22, Length=200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 279\u001b[0m\n\u001b[1;32m    269\u001b[0m planner \u001b[38;5;241m=\u001b[39m MPCPlanner(\n\u001b[1;32m    270\u001b[0m     world_model\u001b[38;5;241m=\u001b[39mworld_model,\n\u001b[1;32m    271\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39menv\u001b[38;5;241m.\u001b[39maction_dim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m     n_elite\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m\n\u001b[1;32m    276\u001b[0m )\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# Evaluate planning performance\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m planning_rewards \u001b[38;5;241m=\u001b[39m evaluate_world_model_planning(env, world_model, planner, n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[1;32m    282\u001b[0m fig, ((ax1, ax2), (ax3, ax4)) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "Cell \u001b[0;32mIn[3], line 214\u001b[0m, in \u001b[0;36mevaluate_world_model_planning\u001b[0;34m(env, world_model, planner, n_episodes)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Plan action using MPC\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 214\u001b[0m     action_tensor \u001b[38;5;241m=\u001b[39m planner\u001b[38;5;241m.\u001b[39mplan(state)\n\u001b[1;32m    215\u001b[0m     action \u001b[38;5;241m=\u001b[39m action_tensor\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    217\u001b[0m \u001b[38;5;66;03m# Execute action\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 318\u001b[0m, in \u001b[0;36mMPCPlanner.plan\u001b[0;34m(self, initial_state)\u001b[0m\n\u001b[1;32m    315\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_min, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_max)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;66;03m# Evaluate action sequences\u001b[39;00m\n\u001b[0;32m--> 318\u001b[0m returns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_sequences(initial_state, actions)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Select elite sequences\u001b[39;00m\n\u001b[1;32m    321\u001b[0m _, elite_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtopk(returns, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_elite, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 356\u001b[0m, in \u001b[0;36mMPCPlanner._evaluate_sequences\u001b[0;34m(self, initial_state, actions)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Imagine sequence\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 356\u001b[0m     imagined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mworld_model\u001b[38;5;241m.\u001b[39mimagine_sequence(expanded_state, actions_flat)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Calculate returns\u001b[39;00m\n\u001b[1;32m    359\u001b[0m rewards \u001b[38;5;241m=\u001b[39m imagined[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrewards\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# [batch*candidates, horizon, 1]\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 250\u001b[0m, in \u001b[0;36mWorldModel.imagine_sequence\u001b[0;34m(self, initial_state, actions)\u001b[0m\n\u001b[1;32m    247\u001b[0m states\u001b[38;5;241m.\u001b[39mappend(state)\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# Predict reward and continuation\u001b[39;00m\n\u001b[0;32m--> 250\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_reward(state)\n\u001b[1;32m    251\u001b[0m continue_prob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_continue(state)\n\u001b[1;32m    253\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "Cell \u001b[0;32mIn[2], line 166\u001b[0m, in \u001b[0;36mWorldModel.predict_reward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict reward from state\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m state_concat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstoch\u001b[39m\u001b[38;5;124m'\u001b[39m], state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden\u001b[39m\u001b[38;5;124m'\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward_model(state_concat)\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonProject1/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonProject1/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/pythonProject1/lib/python3.11/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def create_world_model_environment():\n    class ContinuousControlEnv:\n        def __init__(self, state_dim=4, action_dim=2):\n            self.state_dim = state_dim\n            self.action_dim = action_dim\n            self.max_steps = 200\n            self.reset()\n        def reset(self):\n            self.state = np.random.uniform(-1, 1, self.state_dim)\n            self.steps = 0\n            return self.state.copy()\n        def step(self, action):\n            action = np.clip(action, -1, 1)\n            next_state = np.zeros_like(self.state)\n            next_state[0] = self.state[0] + 0.1 * action[0] + 0.05 * np.sin(self.state[1])\n            next_state[1] = self.state[1] + 0.1 * action[1] + 0.02 * self.state[0] * self.state[2]\n            next_state[2] = 0.9 * self.state[2] + 0.1 * np.tanh(action[0] + action[1])\n            next_state[3] = 0.95 * self.state[3] + 0.1 * np.random.normal(0, 0.1)\n            next_state += np.random.normal(0, 0.02, self.state_dim)\n            reward = -np.sum(next_state**2) - 0.01 * np.sum(action**2)\n            self.steps += 1\n            done = self.steps >= self.max_steps or np.linalg.norm(next_state) > 3\n            self.state = next_state\n            return next_state.copy(), reward, done, {}\n    return ContinuousControlEnv()\ndef collect_random_data(env, n_episodes=100):\n    data = {\n        'observations': [],\n        'actions': [],\n        'rewards': [],\n        'dones': []\n    }\n    print(f\"Collecting {n_episodes} episodes of random data...\")\n    for episode in range(n_episodes):\n        obs = env.reset()\n        episode_obs = [obs]\n        episode_actions = []\n        episode_rewards = []\n        episode_dones = []\n        while True:\n            action = np.random.uniform(-1, 1, env.action_dim)\n            next_obs, reward, done, _ = env.step(action)\n            episode_obs.append(next_obs)\n            episode_actions.append(action)\n            episode_rewards.append(reward)\n            episode_dones.append(done)\n            if done:\n                break\n        data['observations'].append(np.array(episode_obs))\n        data['actions'].append(np.array(episode_actions))\n        data['rewards'].append(np.array(episode_rewards))\n        data['dones'].append(np.array(episode_dones))\n        if episode % 20 == 0:\n            print(f\"Episode {episode}/{n_episodes}\")\n    return data\ndef create_training_batches(data, batch_size=32, seq_length=20):\n    batches = []\n    for episode_obs, episode_actions, episode_rewards in zip(\n        data['observations'], data['actions'], data['rewards']\n    ):\n        episode_length = len(episode_actions)\n        for start_idx in range(0, episode_length - seq_length + 1, seq_length // 2):\n            end_idx = start_idx + seq_length\n            batch_obs = episode_obs[start_idx:end_idx+1]\n            batch_actions = episode_actions[start_idx:end_idx]\n            batch_rewards = episode_rewards[start_idx:end_idx]\n            batches.append({\n                'observations': torch.FloatTensor(batch_obs).to(device),\n                'actions': torch.FloatTensor(batch_actions).to(device),\n                'rewards': torch.FloatTensor(batch_rewards).unsqueeze(-1).to(device)\n            })\n    grouped_batches = []\n    for i in range(0, len(batches), batch_size):\n        batch_group = batches[i:i+batch_size]\n        if len(batch_group) == batch_size:\n            obs_batch = torch.stack([b['observations'] for b in batch_group])\n            action_batch = torch.stack([b['actions'] for b in batch_group])\n            reward_batch = torch.stack([b['rewards'] for b in batch_group])\n            grouped_batches.append({\n                'observations': obs_batch,\n                'actions': action_batch,\n                'rewards': reward_batch\n            })\n    return grouped_batches\ndef train_world_model(world_model, batches, n_epochs=50, lr=1e-3):\n    optimizer = torch.optim.Adam(world_model.parameters(), lr=lr)\n    losses = {'total': [], 'reconstruction': [], 'kl': [], 'reward': []}\n    print(f\"Training world model for {n_epochs} epochs...\")\n    for epoch in range(n_epochs):\n        epoch_losses = {'total': 0, 'reconstruction': 0, 'kl': 0, 'reward': 0}\n        for batch_idx, batch in enumerate(batches):\n            obs_seq = batch['observations']\n            action_seq = batch['actions']\n            reward_seq = batch['rewards']\n            output = world_model.observe_sequence(obs_seq[:, :-1], action_seq)\n            recon_loss = F.mse_loss(\n                output['reconstructions'], \n                obs_seq[:, 1:]\n            )\n            kl_loss = output['kl_losses'].mean() if output['kl_losses'] is not None else 0\n            reward_loss = F.mse_loss(output['rewards'], reward_seq)\n            total_loss = recon_loss + 0.1 * kl_loss + reward_loss\n            optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(world_model.parameters(), 1.0)\n            optimizer.step()\n            epoch_losses['total'] += total_loss.item()\n            epoch_losses['reconstruction'] += recon_loss.item()\n            epoch_losses['kl'] += kl_loss.item() if isinstance(kl_loss, torch.Tensor) else kl_loss\n            epoch_losses['reward'] += reward_loss.item()\n        for key in epoch_losses:\n            epoch_losses[key] /= len(batches)\n            losses[key].append(epoch_losses[key])\n        if epoch % 10 == 0:\n            print(f\"Epoch {epoch}: Total={epoch_losses['total']:.4f}, \"\n                  f\"Recon={epoch_losses['reconstruction']:.4f}, \"\n                  f\"KL={epoch_losses['kl']:.4f}, \"\n                  f\"Reward={epoch_losses['reward']:.4f}\")\n    return losses\ndef evaluate_world_model_planning(env, world_model, planner, n_episodes=10):\n    print(f\"Evaluating MPC planning for {n_episodes} episodes...\")\n    episode_rewards = []\n    episode_lengths = []\n    for episode in range(n_episodes):\n        obs = env.reset()\n        episode_reward = 0\n        episode_length = 0\n        state = world_model.rssm.initial_state(1)\n        while True:\n            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n            embed = world_model.encode(obs_tensor)\n            dummy_action = torch.zeros(1, env.action_dim).to(device)\n            state = world_model.rssm.observe(embed, dummy_action, state)\n            with torch.no_grad():\n                action_tensor = planner.plan(state)\n                action = action_tensor.cpu().numpy()[0]\n            next_obs, reward, done, _ = env.step(action)\n            episode_reward += reward\n            episode_length += 1\n            obs = next_obs\n            if done:\n                break\n        episode_rewards.append(episode_reward)\n        episode_lengths.append(episode_length)\n        if episode % 5 == 0:\n            print(f\"Episode {episode}: Reward={episode_reward:.2f}, Length={episode_length}\")\n    print(f\"Average reward: {np.mean(episode_rewards):.2f} ¬± {np.std(episode_rewards):.2f}\")\n    print(f\"Average length: {np.mean(episode_lengths):.1f} ¬± {np.std(episode_lengths):.1f}\")\n    return episode_rewards\nprint(\"üöÄ Starting Exercise 1: World Models Training and Evaluation\")\nprint(\"=\"*70)\nenv = create_world_model_environment()\nprint(f\"Environment: {env.state_dim}D state, {env.action_dim}D action\")\nrandom_data = collect_random_data(env, n_episodes=50)\nprint(f\"Collected {len(random_data['observations'])} episodes\")\ntraining_batches = create_training_batches(random_data, batch_size=16, seq_length=15)\nprint(f\"Created {len(training_batches)} training batches\")\nworld_model = WorldModel(\n    obs_dim=env.state_dim,\n    action_dim=env.action_dim,\n    state_dim=20,\n    hidden_dim=100,\n    embed_dim=256\n).to(device)\nprint(f\"World model parameters: {sum(p.numel() for p in world_model.parameters()):,}\")\ntraining_losses = train_world_model(world_model, training_batches, n_epochs=30)\nplanner = MPCPlanner(\n    world_model=world_model,\n    action_dim=env.action_dim,\n    horizon=8,\n    n_candidates=500,\n    n_iterations=5,\n    n_elite=50\n)\nplanning_rewards = evaluate_world_model_planning(env, world_model, planner, n_episodes=10)\nfig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\nax1.plot(training_losses['total'], label='Total Loss')\nax1.plot(training_losses['reconstruction'], label='Reconstruction')\nax1.plot(training_losses['reward'], label='Reward Prediction')\nax1.set_title('World Model Training Losses')\nax1.set_xlabel('Epoch')\nax1.set_ylabel('Loss')\nax1.legend()\nax1.grid(True)\nax2.plot(training_losses['kl'])\nax2.set_title('KL Divergence Loss')\nax2.set_xlabel('Epoch')\nax2.set_ylabel('KL Loss')\nax2.grid(True)\nax3.bar(range(len(planning_rewards)), planning_rewards, alpha=0.7)\nax3.set_title('MPC Planning Episode Rewards')\nax3.set_xlabel('Episode')\nax3.set_ylabel('Total Reward')\nax3.grid(True)\nax4.hist(planning_rewards, bins=5, alpha=0.7, edgecolor='black')\nax4.axvline(np.mean(planning_rewards), color='red', linestyle='--', \n           label=f'Mean: {np.mean(planning_rewards):.2f}')\nax4.set_title('Reward Distribution')\nax4.set_xlabel('Episode Reward')\nax4.set_ylabel('Frequency')\nax4.legend()\nax4.grid(True)\nplt.tight_layout()\nplt.show()\nprint(\"\\n‚úÖ Exercise 1 Complete!\")\nprint(\"Key learnings:\")\nprint(\"- World models can learn environment dynamics from observation sequences\")\nprint(\"- MPC planning uses learned models for lookahead decision making\")\nprint(\"- RSSM balances deterministic and stochastic state evolution\")\nprint(\"- Imagination enables sample-efficient learning through internal simulation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f2979",
   "metadata": {},
   "source": [
    "# Part II: Multi-Agent Deep Reinforcement Learning\n",
    "\n",
    "## üë• Theoretical Foundation\n",
    "\n",
    "### Introduction to Multi-Agent RL\n",
    "\n",
    "**Multi-Agent Reinforcement Learning (MARL)** extends single-agent RL to environments with multiple learning agents. This creates fundamentally new challenges due to **non-stationarity** - each agent's environment changes as other agents learn and adapt their policies.\n",
    "\n",
    "### Core Challenges in MARL\n",
    "\n",
    "#### 1. Non-Stationarity Problem\n",
    "- **Single-Agent RL**: Environment is stationary (fixed transition dynamics)\n",
    "- **Multi-Agent RL**: Environment is non-stationary (other agents change their behavior)\n",
    "- **Consequence**: Standard RL convergence guarantees no longer hold\n",
    "\n",
    "#### 2. Credit Assignment Problem\n",
    "- **Challenge**: Which agent is responsible for team success/failure?\n",
    "- **Example**: In cooperative tasks, global reward must be decomposed\n",
    "- **Solutions**: Difference rewards, counterfactual reasoning, attention mechanisms\n",
    "\n",
    "#### 3. Scalability Issues\n",
    "- **Joint Action Space**: Grows exponentially with number of agents\n",
    "- **Joint Observation Space**: Exponential growth in state complexity\n",
    "- **Communication**: Bandwidth limitations, partial observability\n",
    "\n",
    "#### 4. Coordination vs Competition\n",
    "- **Cooperative**: Agents share common objectives (team sports, rescue operations)\n",
    "- **Competitive**: Agents have opposing objectives (adversarial games, auctions)\n",
    "- **Mixed-Motive**: Combination of cooperation and competition (negotiation, markets)\n",
    "\n",
    "### Game Theoretic Foundations\n",
    "\n",
    "#### Nash Equilibrium\n",
    "A strategy profile where no agent can unilaterally improve by changing strategy:\n",
    "\n",
    "$$\\pi^*_i \\in \\arg\\max_{\\pi_i} J_i(\\pi_i, \\pi^*_{-i})$$\n",
    "\n",
    "where $\\pi^*_{-i}$ represents the strategies of all agents except $i$.\n",
    "\n",
    "#### Solution Concepts\n",
    "1. **Nash Equilibrium**: Stable but not necessarily optimal\n",
    "2. **Pareto Optimal**: Efficient outcomes that cannot be improved for all agents\n",
    "3. **Correlated Equilibrium**: Allows for coordination through external signals\n",
    "4. **Stackelberg Equilibrium**: Leader-follower dynamics\n",
    "\n",
    "### MARL Algorithm Categories\n",
    "\n",
    "#### 1. Independent Learning (IL)\n",
    "Each agent treats others as part of the environment:\n",
    "- **Pros**: Simple, scalable, no communication needed\n",
    "- **Cons**: No convergence guarantees, ignores other agents' adaptation\n",
    "- **Examples**: Independent Q-learning, Independent Actor-Critic\n",
    "\n",
    "#### 2. Joint Action Learning (JAL)  \n",
    "Agents learn joint action-value functions:\n",
    "- **Pros**: Can achieve coordination, theoretically sound\n",
    "- **Cons**: Exponential complexity in number of agents\n",
    "- **Examples**: Multi-Agent Q-learning, Nash-Q learning\n",
    "\n",
    "#### 3. Agent Modeling (AM)\n",
    "Agents maintain models of other agents:\n",
    "- **Pros**: Handles non-stationarity explicitly\n",
    "- **Cons**: Computational overhead, modeling errors\n",
    "- **Examples**: MAAC, MADDPG with opponent modeling\n",
    "\n",
    "#### 4. Communication-Based\n",
    "Agents can exchange information:\n",
    "- **Pros**: Direct coordination, shared knowledge\n",
    "- **Cons**: Communication overhead, protocol design\n",
    "- **Examples**: CommNet, I2C, TarMAC\n",
    "\n",
    "### Deep MARL Algorithms\n",
    "\n",
    "#### 1. Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "\n",
    "**Key Idea**: Centralized training, decentralized execution\n",
    "- **Training**: Critics have access to all agents' observations and actions\n",
    "- **Execution**: Actors only use local observations\n",
    "\n",
    "**Actor Update**: \n",
    "$$\\nabla_{\\theta_i} J_i = \\mathbb{E}[\\nabla_{\\theta_i} \\mu_i(o_i) \\nabla_{a_i} Q_i^{\\mu}(x, a_1, ..., a_N)|_{a_i=\\mu_i(o_i)}]$$\n",
    "\n",
    "**Critic Update**:\n",
    "$$Q_i^{\\mu}(x, a_1, ..., a_N) = \\mathbb{E}[r_i + \\gamma Q_i^{\\mu'}(x', a'_1, ..., a'_N)]$$\n",
    "\n",
    "where $x$ is the global state and $a_i$ are individual actions.\n",
    "\n",
    "#### 2. Multi-Agent Actor-Critic (MAAC)\n",
    "\n",
    "Extends single-agent AC to multi-agent setting:\n",
    "- **Centralized Critic**: Uses global information during training\n",
    "- **Decentralized Actors**: Use only local observations\n",
    "- **Attention Mechanism**: Selectively focus on relevant agents\n",
    "\n",
    "#### 3. Counterfactual Multi-Agent Policy Gradient (COMA)\n",
    "\n",
    "Addresses credit assignment through counterfactual reasoning:\n",
    "\n",
    "**Counterfactual Advantage**:\n",
    "$$A_i(s, a) = Q(s, a) - \\sum_{a'_i} \\pi_i(a'_i|o_i) Q(s, (a_{-i}, a'_i))$$\n",
    "\n",
    "This measures how much better the taken action is compared to marginalizing over all possible actions.\n",
    "\n",
    "### Communication in MARL\n",
    "\n",
    "#### 1. Communication Protocols\n",
    "- **Broadcast**: All-to-all communication\n",
    "- **Targeted**: Agent-specific messages\n",
    "- **Hierarchical**: Tree-structured communication\n",
    "\n",
    "#### 2. Communication Learning\n",
    "- **What to Communicate**: Message content learning\n",
    "- **When to Communicate**: Communication scheduling\n",
    "- **Who to Communicate With**: Network topology learning\n",
    "\n",
    "#### 3. Differentiable Communication\n",
    "\n",
    "**Gumbel-Softmax Trick** for discrete communication:\n",
    "$$\\text{softmax}\\left(\\frac{\\log(\\pi_i) + G_i}{\\tau}\\right)$$\n",
    "\n",
    "where $G_i$ are Gumbel random variables and $\\tau$ is temperature.\n",
    "\n",
    "### Cooperative Multi-Agent RL\n",
    "\n",
    "#### 1. Team Reward Structure\n",
    "- **Global Reward**: Same reward for all agents\n",
    "- **Local Rewards**: Individual agent rewards\n",
    "- **Shaped Rewards**: Carefully designed to promote cooperation\n",
    "\n",
    "#### 2. Value Decomposition Methods\n",
    "\n",
    "**VDN (Value Decomposition Networks)**:\n",
    "$$Q_{tot}(s, a) = \\sum_{i=1}^n Q_i(s_i, a_i)$$\n",
    "\n",
    "**QMIX**: Monotonic value decomposition\n",
    "$$\\frac{\\partial Q_{tot}}{\\partial Q_i} \\geq 0$$\n",
    "\n",
    "#### 3. Policy Gradient Methods\n",
    "- **Multi-Agent Policy Gradient (MAPG)**\n",
    "- **Trust Region Methods**: MADDPG-TR\n",
    "- **Proximal Policy Optimization**: MAPPO\n",
    "\n",
    "### Competitive Multi-Agent RL\n",
    "\n",
    "#### 1. Self-Play Training\n",
    "Agents learn by playing against copies of themselves:\n",
    "- **Advantages**: Always improving opponents, no human data needed\n",
    "- **Challenges**: Exploitability, strategy diversity\n",
    "\n",
    "#### 2. Population-Based Training\n",
    "Maintain population of diverse strategies:\n",
    "- **League Play**: Different skill levels and strategies\n",
    "- **Diversity Metrics**: Behavioral diversity, policy diversity\n",
    "- **Meta-Game Analysis**: Strategy effectiveness matrix\n",
    "\n",
    "#### 3. Adversarial Training\n",
    "- **Minimax Objective**: $\\min_{\\pi_1} \\max_{\\pi_2} J(\\pi_1, \\pi_2)$\n",
    "- **Nash-AC**: Nash equilibrium seeking\n",
    "- **PSRO**: Policy Space Response Oracles\n",
    "\n",
    "### Theoretical Guarantees\n",
    "\n",
    "#### 1. Convergence Results\n",
    "- **Independent Learning**: Generally no convergence guarantees\n",
    "- **Joint Action Learning**: Convergence to Nash under restrictive assumptions\n",
    "- **Two-Timescale Algorithms**: Convergence through different learning rates\n",
    "\n",
    "#### 2. Sample Complexity\n",
    "Multi-agent sample complexity often exponentially worse than single-agent due to:\n",
    "- Larger state-action spaces\n",
    "- Non-stationarity\n",
    "- Coordination requirements\n",
    "\n",
    "#### 3. Regret Bounds\n",
    "**Multi-Agent Regret**: \n",
    "$$R_i(T) = \\max_{\\pi_i} \\sum_{t=1}^T J_i(\\pi_i, \\pi_{-i}^t) - \\sum_{t=1}^T J_i(\\pi_i^t, \\pi_{-i}^t)$$\n",
    "\n",
    "### Applications\n",
    "\n",
    "#### 1. Robotics\n",
    "- **Multi-Robot Systems**: Coordination and task allocation\n",
    "- **Swarm Robotics**: Large-scale coordination\n",
    "- **Human-Robot Interaction**: Mixed human-AI teams\n",
    "\n",
    "#### 2. Autonomous Vehicles\n",
    "- **Traffic Management**: Intersection control, highway merging\n",
    "- **Platooning**: Vehicle following and coordination\n",
    "- **Mixed Autonomy**: Human and autonomous vehicles\n",
    "\n",
    "#### 3. Game Playing\n",
    "- **Real-Time Strategy Games**: StarCraft, Dota\n",
    "- **Board Games**: Multi-player poker, diplomacy\n",
    "- **Sports Simulation**: Team coordination\n",
    "\n",
    "#### 4. Economics and Finance\n",
    "- **Algorithmic Trading**: Multi-agent market making\n",
    "- **Auction Design**: Bidding strategies\n",
    "- **Resource Allocation**: Cloud computing, network resources\n",
    "\n",
    "### Key Research Papers\n",
    "\n",
    "1. **MADDPG** (Lowe et al., 2017)\n",
    "2. **COMA** (Foerster et al., 2018)\n",
    "3. **QMIX** (Rashid et al., 2018)\n",
    "4. **CommNet** (Sukhbaatar et al., 2016)\n",
    "5. **OpenAI Five** (OpenAI, 2019)\n",
    "6. **AlphaStar** (Vinyals et al., 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014f01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentReplayBuffer:\n    def __init__(self, capacity: int, n_agents: int, obs_dim: int, action_dim: int):\n        self.capacity = capacity\n        self.n_agents = n_agents\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.observations = np.zeros((capacity, n_agents, obs_dim))\n        self.actions = np.zeros((capacity, n_agents, action_dim))\n        self.rewards = np.zeros((capacity, n_agents, 1))\n        self.next_observations = np.zeros((capacity, n_agents, obs_dim))\n        self.dones = np.zeros((capacity, n_agents, 1))\n        self.ptr = 0\n        self.size = 0\n    def add(self, obs: np.ndarray, actions: np.ndarray, rewards: np.ndarray,\n            next_obs: np.ndarray, dones: np.ndarray):\n        self.observations[self.ptr] = obs\n        self.actions[self.ptr] = actions\n        self.rewards[self.ptr] = rewards.reshape(self.n_agents, 1)\n        self.next_observations[self.ptr] = next_obs\n        self.dones[self.ptr] = dones.reshape(self.n_agents, 1)\n        self.ptr = (self.ptr + 1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n    def sample(self, batch_size: int) -> Dict[str, torch.Tensor]:\n        indices = np.random.choice(self.size, batch_size, replace=False)\n        return {\n            'observations': torch.FloatTensor(self.observations[indices]).to(device),\n            'actions': torch.FloatTensor(self.actions[indices]).to(device),\n            'rewards': torch.FloatTensor(self.rewards[indices]).to(device),\n            'next_observations': torch.FloatTensor(self.next_observations[indices]).to(device),\n            'dones': torch.FloatTensor(self.dones[indices]).to(device)\n        }\nclass Actor(nn.Module):\n    def __init__(self, obs_dim: int, action_dim: int, hidden_dim: int = 256):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()\n        )\n    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n        return self.network(obs)\nclass Critic(nn.Module):\n    def __init__(self, total_obs_dim: int, total_action_dim: int, \n                 hidden_dim: int = 256):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(total_obs_dim + total_action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def forward(self, obs: torch.Tensor, actions: torch.Tensor) -> torch.Tensor:\n        x = torch.cat([obs, actions], dim=-1)\n        return self.network(x)\nclass AttentionCritic(nn.Module):\n    def __init__(self, obs_dim: int, action_dim: int, n_agents: int,\n                 hidden_dim: int = 256, attention_dim: int = 64):\n        super().__init__()\n        self.n_agents = n_agents\n        self.attention_dim = attention_dim\n        self.query_net = nn.Linear(obs_dim + action_dim, attention_dim)\n        self.key_net = nn.Linear(obs_dim + action_dim, attention_dim)\n        self.value_net = nn.Linear(obs_dim + action_dim, attention_dim)\n        self.critic_net = nn.Sequential(\n            nn.Linear(attention_dim + obs_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def forward(self, observations: torch.Tensor, actions: torch.Tensor,\n                agent_idx: int) -> torch.Tensor:\n        batch_size = observations.shape[0]\n        obs_act = torch.cat([observations, actions], dim=-1)\n        own_obs_act = obs_act[:, agent_idx]\n        queries = self.query_net(own_obs_act).unsqueeze(1)\n        keys = self.key_net(obs_act)\n        values = self.value_net(obs_act)\n        attention_scores = torch.bmm(queries, keys.transpose(1, 2))\n        attention_weights = F.softmax(attention_scores / np.sqrt(self.attention_dim), dim=-1)\n        attended_values = torch.bmm(attention_weights, values).squeeze(1)\n        critic_input = torch.cat([attended_values, own_obs_act], dim=-1)\n        return self.critic_net(critic_input)\nclass CommunicationNetwork(nn.Module):\n    def __init__(self, obs_dim: int, message_dim: int, hidden_dim: int = 128):\n        super().__init__()\n        self.message_dim = message_dim\n        self.message_encoder = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, message_dim)\n        )\n        self.message_processor = nn.Sequential(\n            nn.Linear(obs_dim + message_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, obs_dim)\n        )\n    def generate_message(self, obs: torch.Tensor) -> torch.Tensor:\n        return self.message_encoder(obs)\n    def process_messages(self, obs: torch.Tensor, \n                        messages: torch.Tensor) -> torch.Tensor:\n        avg_message = messages.mean(dim=1)\n        combined = torch.cat([obs, avg_message], dim=-1)\n        enhanced_obs = self.message_processor(combined)\n        return enhanced_obs\nclass MADDPGAgent:\n    def __init__(self, agent_idx: int, obs_dim: int, action_dim: int,\n                 n_agents: int, lr_actor: float = 1e-3, lr_critic: float = 1e-3,\n                 use_attention: bool = False, use_communication: bool = False):\n        self.agent_idx = agent_idx\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.n_agents = n_agents\n        self.use_attention = use_attention\n        self.use_communication = use_communication\n        self.actor = Actor(obs_dim, action_dim).to(device)\n        self.actor_target = Actor(obs_dim, action_dim).to(device)\n        if use_attention:\n            self.critic = AttentionCritic(obs_dim, action_dim, n_agents).to(device)\n            self.critic_target = AttentionCritic(obs_dim, action_dim, n_agents).to(device)\n        else:\n            total_obs_dim = obs_dim * n_agents\n            total_action_dim = action_dim * n_agents\n            self.critic = Critic(total_obs_dim, total_action_dim).to(device)\n            self.critic_target = Critic(total_obs_dim, total_action_dim).to(device)\n        if use_communication:\n            self.comm_network = CommunicationNetwork(obs_dim, message_dim=32).to(device)\n        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr_actor)\n        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr_critic)\n        if use_communication:\n            self.comm_optimizer = torch.optim.Adam(self.comm_network.parameters(), lr=lr_actor)\n        self.hard_update(self.actor_target, self.actor)\n        self.hard_update(self.critic_target, self.critic)\n        self.noise_std = 0.2\n        self.noise_decay = 0.995\n        self.min_noise = 0.01\n    def act(self, obs: torch.Tensor, messages: torch.Tensor = None,\n            explore: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:\n        if self.use_communication and messages is not None:\n            obs = self.comm_network.process_messages(obs, messages)\n        action = self.actor(obs)\n        if explore:\n            noise = torch.randn_like(action) * self.noise_std\n            action = torch.clamp(action + noise, -1, 1)\n        message = None\n        if self.use_communication:\n            message = self.comm_network.generate_message(obs)\n        return action, message\n    def update(self, batch: Dict[str, torch.Tensor], other_actors: List[nn.Module],\n               gamma: float = 0.99, tau: float = 0.01):\n        obs = batch['observations']\n        actions = batch['actions']\n        rewards = batch['rewards']\n        next_obs = batch['next_observations']\n        dones = batch['dones']\n        batch_size = obs.shape[0]\n        with torch.no_grad():\n            next_actions = torch.zeros_like(actions)\n            for i, actor in enumerate(other_actors):\n                if i == self.agent_idx:\n                    next_actions[:, i] = self.actor_target(next_obs[:, i])\n                else:\n                    next_actions[:, i] = actor(next_obs[:, i])\n            if self.use_attention:\n                target_q = self.critic_target(next_obs, next_actions, self.agent_idx)\n            else:\n                next_obs_flat = next_obs.view(batch_size, -1)\n                next_actions_flat = next_actions.view(batch_size, -1)\n                target_q = self.critic_target(next_obs_flat, next_actions_flat)\n            target_q = rewards[:, self.agent_idx] + gamma * (1 - dones[:, self.agent_idx]) * target_q\n        if self.use_attention:\n            current_q = self.critic(obs, actions, self.agent_idx)\n        else:\n            obs_flat = obs.view(batch_size, -1)\n            actions_flat = actions.view(batch_size, -1)\n            current_q = self.critic(obs_flat, actions_flat)\n        critic_loss = F.mse_loss(current_q, target_q)\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)\n        self.critic_optimizer.step()\n        current_actions = actions.clone()\n        current_actions[:, self.agent_idx] = self.actor(obs[:, self.agent_idx])\n        if self.use_attention:\n            actor_loss = -self.critic(obs, current_actions, self.agent_idx).mean()\n        else:\n            obs_flat = obs.view(batch_size, -1)\n            current_actions_flat = current_actions.view(batch_size, -1)\n            actor_loss = -self.critic(obs_flat, current_actions_flat).mean()\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n        self.actor_optimizer.step()\n        self.soft_update(self.actor_target, self.actor, tau)\n        self.soft_update(self.critic_target, self.critic, tau)\n        self.noise_std = max(self.noise_std * self.noise_decay, self.min_noise)\n        return {\n            'critic_loss': critic_loss.item(),\n            'actor_loss': actor_loss.item(),\n            'q_value': current_q.mean().item(),\n            'noise_std': self.noise_std\n        }\n    def soft_update(self, target: nn.Module, source: nn.Module, tau: float):\n        for target_param, param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n    def hard_update(self, target: nn.Module, source: nn.Module):\n        target.load_state_dict(source.state_dict())\nclass MultiAgentEnvironment:\n    def __init__(self, n_agents: int = 3, obs_dim: int = 6, action_dim: int = 2,\n                 env_type: str = 'cooperative'):\n        self.n_agents = n_agents\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.env_type = env_type\n        self.max_steps = 200\n        self.reset()\n    def reset(self) -> np.ndarray:\n        self.agent_states = np.random.uniform(-2, 2, (self.n_agents, self.obs_dim))\n        self.steps = 0\n        return self.get_observations()\n    def get_observations(self) -> np.ndarray:\n        observations = np.zeros((self.n_agents, self.obs_dim))\n        for i in range(self.n_agents):\n            obs = self.agent_states[i].copy()\n            obs += np.random.normal(0, 0.1, self.obs_dim)\n            observations[i] = obs\n        return observations\n    def step(self, actions: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict]:\n        actions = np.clip(actions, -1, 1)\n        for i in range(self.n_agents):\n            if self.obs_dim >= 4:\n                self.agent_states[i, 2:4] += 0.1 * actions[i, :2]\n                self.agent_states[i, 2:4] *= 0.9\n                self.agent_states[i, :2] += 0.1 * self.agent_states[i, 2:4]\n            else:\n                self.agent_states[i, :2] += 0.1 * actions[i, :2]\n            self.agent_states[i] += np.random.normal(0, 0.02, self.obs_dim)\n        rewards = self.compute_rewards()\n        self.steps += 1\n        dones = np.array([self.steps >= self.max_steps] * self.n_agents)\n        for i in range(self.n_agents):\n            if np.linalg.norm(self.agent_states[i, :2]) > 5:\n                dones[i] = True\n        observations = self.get_observations()\n        return observations, rewards, dones, {}\n    def compute_rewards(self) -> np.ndarray:\n        rewards = np.zeros(self.n_agents)\n        if self.env_type == 'cooperative':\n            center = np.mean(self.agent_states[:, :2], axis=0)\n            for i in range(self.n_agents):\n                center_reward = -np.linalg.norm(self.agent_states[i, :2])\n                cohesion_reward = 0\n                for j in range(self.n_agents):\n                    if i != j:\n                        dist = np.linalg.norm(self.agent_states[i, :2] - self.agent_states[j, :2])\n                        cohesion_reward += -0.1 * dist\n                rewards[i] = center_reward + 0.5 * cohesion_reward\n        elif self.env_type == 'competitive':\n            target = np.array([0, 0])\n            distances = [np.linalg.norm(self.agent_states[i, :2] - target) \n                        for i in range(self.n_agents)]\n            closest_agent = np.argmin(distances)\n            for i in range(self.n_agents):\n                if i == closest_agent:\n                    rewards[i] = 1.0\n                else:\n                    rewards[i] = -0.1\n        elif self.env_type == 'mixed':\n            team_size = self.n_agents // 2\n            for i in range(self.n_agents):\n                team_id = i // team_size\n                team_reward = 0\n                for j in range(self.n_agents):\n                    if j // team_size == team_id and i != j:\n                        dist = np.linalg.norm(self.agent_states[i, :2] - self.agent_states[j, :2])\n                        team_reward += -0.1 * dist\n                comp_reward = 0\n                for j in range(self.n_agents):\n                    if j // team_size != team_id:\n                        dist = np.linalg.norm(self.agent_states[i, :2] - self.agent_states[j, :2])\n                        comp_reward += 0.05 * max(0, 2 - dist)\n                rewards[i] = team_reward + comp_reward\n        return rewards\nprint(\"‚úÖ Multi-Agent RL Implementation Complete!\")\nprint(\"Components implemented:\")\nprint(\"- MultiAgentReplayBuffer: Experience storage for multi-agent systems\")\nprint(\"- Actor/Critic: Individual agent networks with centralized training\")\nprint(\"- AttentionCritic: Attention mechanism for selective agent focus\")\nprint(\"- CommunicationNetwork: Neural communication between agents\")\nprint(\"- MADDPGAgent: Complete MADDPG implementation with extensions\")\nprint(\"- MultiAgentEnvironment: Configurable multi-agent test environment\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f25b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_maddpg(env: MultiAgentEnvironment, agents: List[MADDPGAgent],\n                 buffer: MultiAgentReplayBuffer, episodes: int = 1000,\n                 batch_size: int = 64, update_interval: int = 4):\n    episode_rewards = []\n    losses = {f'agent_{i}': {'actor': [], 'critic': []} for i in range(env.n_agents)}\n    for episode in range(episodes):\n        obs = env.reset()\n        episode_reward = np.zeros(env.n_agents)\n        done = False\n        step = 0\n        while not done:\n            messages = []\n            if agents[0].use_communication:\n                for i, agent in enumerate(agents):\n                    obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n                    _, message = agent.act(obs_tensor, explore=True)\n                    messages.append(message)\n                messages = torch.stack(messages, dim=1)\n            actions = np.zeros((env.n_agents, env.action_dim))\n            for i, agent in enumerate(agents):\n                obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n                agent_messages = None\n                if agent.use_communication:\n                    agent_messages = torch.cat([messages[:, :i], messages[:, i+1:]], dim=1)\n                action_tensor, _ = agent.act(obs_tensor, agent_messages, explore=True)\n                actions[i] = action_tensor.cpu().numpy()[0]\n            next_obs, rewards, dones, _ = env.step(actions)\n            buffer.add(obs, actions, rewards, next_obs, dones)\n            episode_reward += rewards\n            obs = next_obs\n            done = np.all(dones)\n            step += 1\n            if buffer.size >= batch_size and step % update_interval == 0:\n                batch = buffer.sample(batch_size)\n                target_actors = [agent.actor_target for agent in agents]\n                for i, agent in enumerate(agents):\n                    update_info = agent.update(batch, target_actors)\n                    losses[f'agent_{i}']['actor'].append(update_info['actor_loss'])\n                    losses[f'agent_{i}']['critic'].append(update_info['critic_loss'])\n        episode_rewards.append(episode_reward.copy())\n        if episode % 100 == 0:\n            mean_reward = np.mean([np.sum(r) for r in episode_rewards[-100:]])\n            print(f\"Episode {episode}, Mean Reward: {mean_reward:.2f}\")\n            for i in range(env.n_agents):\n                noise_std = agents[i].noise_std\n                print(f\"  Agent {i}: Noise={noise_std:.3f}\")\n    return episode_rewards, losses\ndef evaluate_maddpg(env: MultiAgentEnvironment, agents: List[MADDPGAgent],\n                   episodes: int = 100) -> Dict[str, float]:\n    episode_rewards = []\n    coordination_scores = []\n    for episode in range(episodes):\n        obs = env.reset()\n        episode_reward = np.zeros(env.n_agents)\n        positions_history = []\n        done = False\n        while not done:\n            messages = []\n            if agents[0].use_communication:\n                for i, agent in enumerate(agents):\n                    obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n                    _, message = agent.act(obs_tensor, explore=False)\n                    messages.append(message)\n                messages = torch.stack(messages, dim=1)\n            actions = np.zeros((env.n_agents, env.action_dim))\n            for i, agent in enumerate(agents):\n                obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n                agent_messages = None\n                if agent.use_communication:\n                    agent_messages = torch.cat([messages[:, :i], messages[:, i+1:]], dim=1)\n                action_tensor, _ = agent.act(obs_tensor, agent_messages, explore=False)\n                actions[i] = action_tensor.cpu().numpy()[0]\n            next_obs, rewards, dones, _ = env.step(actions)\n            episode_reward += rewards\n            positions_history.append(env.agent_states[:, :2].copy())\n            obs = next_obs\n            done = np.all(dones)\n        episode_rewards.append(episode_reward.copy())\n        positions = np.array(positions_history)\n        mean_positions = np.mean(positions, axis=1)\n        agent_variances = []\n        for t in range(len(positions)):\n            distances_from_center = [\n                np.linalg.norm(positions[t, i] - mean_positions[t])\n                for i in range(env.n_agents)\n            ]\n            agent_variances.append(np.var(distances_from_center))\n        coordination_scores.append(np.mean(agent_variances))\n    results = {\n        'mean_total_reward': np.mean([np.sum(r) for r in episode_rewards]),\n        'std_total_reward': np.std([np.sum(r) for r in episode_rewards]),\n        'mean_individual_reward': np.mean(episode_rewards),\n        'coordination_score': np.mean(coordination_scores),\n        'success_rate': np.mean([np.sum(r) > 0 for r in episode_rewards])\n    }\n    return results\nprint(\"üöÄ Starting Multi-Agent RL Training...\")\nenv_configs = [\n    {'env_type': 'cooperative', 'name': 'Cooperative'},\n    {'env_type': 'competitive', 'name': 'Competitive'},\n    {'env_type': 'mixed', 'name': 'Mixed'}\n]\nresults_summary = {}\nfor config in env_configs[:1]:\n    print(f\"\\n{'='*50}\")\n    print(f\"Training: {config['name']} Environment\")\n    print(f\"{'='*50}\")\n    env = MultiAgentEnvironment(\n        n_agents=3,\n        obs_dim=6,\n        action_dim=2,\n        env_type=config['env_type']\n    )\n    agents = []\n    for i in range(env.n_agents):\n        agent = MADDPGAgent(\n            agent_idx=i,\n            obs_dim=env.obs_dim,\n            action_dim=env.action_dim,\n            n_agents=env.n_agents,\n            use_attention=True,\n            use_communication=True\n        )\n        agents.append(agent)\n    buffer = MultiAgentReplayBuffer(\n        capacity=50000,\n        n_agents=env.n_agents,\n        obs_dim=env.obs_dim,\n        action_dim=env.action_dim\n    )\n    print(\"Training agents...\")\n    episode_rewards, losses = train_maddpg(\n        env, agents, buffer,\n        episodes=500,\n        batch_size=64\n    )\n    print(\"Evaluating agents...\")\n    eval_results = evaluate_maddpg(env, agents, episodes=50)\n    results_summary[config['name']] = {\n        'training_rewards': episode_rewards,\n        'evaluation': eval_results,\n        'losses': losses\n    }\n    print(f\"\\nResults for {config['name']} Environment:\")\n    print(f\"Mean Total Reward: {eval_results['mean_total_reward']:.3f} ¬± {eval_results['std_total_reward']:.3f}\")\n    print(f\"Mean Individual Reward: {eval_results['mean_individual_reward']:.3f}\")\n    print(f\"Coordination Score: {eval_results['coordination_score']:.3f}\")\n    print(f\"Success Rate: {eval_results['success_rate']:.3f}\")\nprint(\"\\n‚úÖ Multi-Agent Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056b8b2",
   "metadata": {},
   "source": [
    "# Part III: Causal Reinforcement Learning\n",
    "\n",
    "## Theoretical Foundations\n",
    "\n",
    "### Introduction to Causality in RL\n",
    "\n",
    "Causal Reinforcement Learning represents a paradigm shift from traditional correlation-based learning to understanding cause-effect relationships in sequential decision making. This approach addresses fundamental limitations in standard RL:\n",
    "\n",
    "**Key Limitations of Standard RL:**\n",
    "- **Spurious Correlations**: Agents may learn policies based on correlations that don't reflect true causal relationships\n",
    "- **Distribution Shift**: Policies trained on specific environments may fail when deployed in different conditions\n",
    "- **Sample Inefficiency**: Without causal understanding, agents require extensive exploration\n",
    "- **Interpretability**: Standard RL policies are often black boxes without clear causal reasoning\n",
    "\n",
    "### Causal Inference Framework\n",
    "\n",
    "#### 1. Structural Causal Models (SCMs)\n",
    "\n",
    "A Structural Causal Model is defined by a tuple $(U, V, F, P(U))$:\n",
    "\n",
    "- **U**: Set of exogenous (external) variables\n",
    "- **V**: Set of endogenous (internal) variables\n",
    "- **F**: Set of functions $f_i$ where $V_i = f_i(PA_i, U_i)$\n",
    "- **P(U)**: Probability distribution over exogenous variables\n",
    "\n",
    "**Causal Graph Representation:**\n",
    "```\n",
    "Exogenous Variables (U) ‚Üí Endogenous Variables (V)\n",
    "      ‚Üì                           ‚Üì\n",
    "Environmental Factors    ‚Üí    Agent States/Actions\n",
    "```\n",
    "\n",
    "#### 2. Causal Hierarchy (Pearl's Ladder)\n",
    "\n",
    "**Level 1: Association** ($P(y|x)$)\n",
    "- \"What is the probability of Y given that we observe X?\"\n",
    "- Standard statistical/ML approaches operate here\n",
    "- Example: \"What's the probability of success given this policy?\"\n",
    "\n",
    "**Level 2: Intervention** ($P(y|do(x))$)\n",
    "- \"What is the probability of Y if we set X to a specific value?\"\n",
    "- Requires understanding of causal mechanisms\n",
    "- Example: \"What happens if we force the agent to take action A?\"\n",
    "\n",
    "**Level 3: Counterfactuals** ($P(y_x|x', y')$)\n",
    "- \"What would have happened if X had been different?\"\n",
    "- Enables reasoning about alternative scenarios\n",
    "- Example: \"Would the agent have succeeded if it had chosen a different action?\"\n",
    "\n",
    "### Causal RL Mathematical Framework\n",
    "\n",
    "#### 1. Causal Markov Decision Process (Causal-MDP)\n",
    "\n",
    "A Causal-MDP extends traditional MDPs with causal structure:\n",
    "\n",
    "**Causal-MDP Definition:**\n",
    "$$\\mathcal{M}_C = \\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{G}, T_C, R_C, \\gamma \\rangle$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{G}$: Causal graph over state variables\n",
    "- $T_C$: Causal transition function respecting $\\mathcal{G}$\n",
    "- $R_C$: Causal reward function\n",
    "\n",
    "**Causal Factorization:**\n",
    "$$P(s_{t+1}|s_t, a_t) = \\prod_{i=1}^{|\\mathcal{S}|} P(s_{t+1}^i | PA_C(s_{t+1}^i), a_t)$$\n",
    "\n",
    "#### 2. Interventional Policy Learning\n",
    "\n",
    "**Interventional Value Function:**\n",
    "$$V^{\\pi}_{do(X=x)}(s) = \\mathbb{E}\\left[\\sum_{t=0}^{\\infty} \\gamma^t R_t | S_0 = s, do(X=x), \\pi\\right]$$\n",
    "\n",
    "**Causal Policy Gradient:**\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{s \\sim d^\\pi, a \\sim \\pi_\\theta}\\left[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot \\frac{\\partial Q^{\\pi}(s,a)}{\\partial do(\\pi_\\theta)}\\right]$$\n",
    "\n",
    "#### 3. Counterfactual Reasoning in RL\n",
    "\n",
    "**Counterfactual Q-Function:**\n",
    "$$Q_{CF}(s, a, s', a') = \\mathbb{E}[R | S=s, A=a, S'_{do(A=a')} = s']$$\n",
    "\n",
    "This captures: \"What would the Q-value be if we had taken action $a'$ instead of $a$?\"\n",
    "\n",
    "### Causal Discovery in RL\n",
    "\n",
    "#### 1. Structure Learning\n",
    "\n",
    "**Constraint-Based Methods:**\n",
    "- Use conditional independence tests\n",
    "- Build causal graph from statistical dependencies\n",
    "- Example: PC Algorithm adapted for sequential data\n",
    "\n",
    "**Score-Based Methods:**\n",
    "- Optimize causal graph structure score\n",
    "- Balance model fit with complexity\n",
    "- Example: BIC score with causal constraints\n",
    "\n",
    "#### 2. Causal Effect Estimation\n",
    "\n",
    "**Backdoor Criterion:**\n",
    "For estimating causal effect of action $A$ on reward $R$:\n",
    "$$P(R|do(A)) = \\sum_z P(R|A,Z) P(Z)$$\n",
    "\n",
    "Where $Z$ blocks all backdoor paths from $A$ to $R$.\n",
    "\n",
    "**Front-door Criterion:**\n",
    "When backdoor adjustment isn't possible:\n",
    "$$P(R|do(A)) = \\sum_m P(M|A) \\sum_{a'} P(R|A',M) P(A')$$\n",
    "\n",
    "### Advanced Causal RL Techniques\n",
    "\n",
    "#### 1. Causal World Models\n",
    "\n",
    "**Causal Representation Learning:**\n",
    "Learn latent representations that respect causal structure:\n",
    "$$z_{t+1} = f_c(z_t, a_t, u_t)$$\n",
    "\n",
    "Where $f_c$ respects the causal graph structure.\n",
    "\n",
    "**Interventional Consistency:**\n",
    "$$\\mathbb{E}[z_{t+1} | do(z_t^i = v)] = \\mathbb{E}[f_c(z_t^{-i}, v, a_t, u_t)]$$\n",
    "\n",
    "#### 2. Causal Meta-Learning\n",
    "\n",
    "**Task-Invariant Causal Features:**\n",
    "Learn features that are causally relevant across tasks:\n",
    "$$\\phi^*(s) = \\arg\\min_\\phi \\sum_{T} L_T(\\phi(s)) + \\lambda \\cdot \\text{Causal-Reg}(\\phi)$$\n",
    "\n",
    "**Causal Transfer:**\n",
    "Transfer causal knowledge between domains:\n",
    "$$\\pi_{new}(a|s) = \\pi_{old}(a|\\phi_{causal}(s))$$\n",
    "\n",
    "#### 3. Confounded RL\n",
    "\n",
    "**Hidden Confounders:**\n",
    "When unobserved variables affect both states and rewards:\n",
    "$$H_t \\rightarrow S_t, H_t \\rightarrow R_t$$\n",
    "\n",
    "**Instrumental Variables:**\n",
    "Use variables correlated with actions but not directly with outcomes:\n",
    "$$IV \\rightarrow A_t \\not\\rightarrow R_t$$\n",
    "\n",
    "### Applications and Benefits\n",
    "\n",
    "#### 1. Robust Policy Learning\n",
    "- Policies that generalize across environments\n",
    "- Reduced sensitivity to spurious correlations\n",
    "- Better performance under distribution shift\n",
    "\n",
    "#### 2. Sample Efficient Exploration\n",
    "- Focus exploration on causally relevant factors\n",
    "- Avoid learning from misleading correlations\n",
    "- Faster convergence to optimal policies\n",
    "\n",
    "#### 3. Interpretable Decision Making\n",
    "- Understand why certain actions are taken\n",
    "- Provide causal explanations for policy decisions\n",
    "- Enable human oversight and validation\n",
    "\n",
    "#### 4. Safe RL Applications\n",
    "- Predict consequences of interventions\n",
    "- Avoid actions with negative causal effects\n",
    "- Enable counterfactual safety analysis\n",
    "\n",
    "### Research Challenges\n",
    "\n",
    "#### 1. Causal Discovery\n",
    "- Identifying causal structure from observational RL data\n",
    "- Handling non-stationarity and temporal dependencies\n",
    "- Scalability to high-dimensional state spaces\n",
    "\n",
    "#### 2. Identifiability\n",
    "- When can causal effects be estimated from data?\n",
    "- Addressing unmeasured confounders\n",
    "- Validation of causal assumptions\n",
    "\n",
    "#### 3. Computational Complexity\n",
    "- Efficient inference in causal graphical models\n",
    "- Scalable algorithms for large state spaces\n",
    "- Real-time causal reasoning during policy execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalGraph:\n    def __init__(self, variables: List[str]):\n        self.variables = variables\n        self.n_vars = len(variables)\n        self.var_to_idx = {var: i for i, var in enumerate(variables)}\n        self.adj_matrix = np.zeros((self.n_vars, self.n_vars), dtype=int)\n    def add_edge(self, from_var: str, to_var: str):\n        from_idx = self.var_to_idx[from_var]\n        to_idx = self.var_to_idx[to_var]\n        self.adj_matrix[from_idx][to_idx] = 1\n    def get_parents(self, var: str) -> List[str]:\n        var_idx = self.var_to_idx[var]\n        parent_indices = np.where(self.adj_matrix[:, var_idx] == 1)[0]\n        return [self.variables[i] for i in parent_indices]\n    def get_children(self, var: str) -> List[str]:\n        var_idx = self.var_to_idx[var]\n        child_indices = np.where(self.adj_matrix[var_idx, :] == 1)[0]\n        return [self.variables[i] for i in child_indices]\n    def is_d_separated(self, x: str, y: str, z: List[str]) -> bool:\n        x_idx = self.var_to_idx[x]\n        y_idx = self.var_to_idx[y]\n        z_indices = [self.var_to_idx[var] for var in z]\n        return not self._has_unblocked_path(x_idx, y_idx, z_indices)\n    def _has_unblocked_path(self, start: int, end: int, blocking: List[int]) -> bool:\n        if start == end:\n            return True\n        visited = set()\n        stack = [start]\n        while stack:\n            current = stack.pop()\n            if current in visited or current in blocking:\n                continue\n            visited.add(current)\n            for next_node in range(self.n_vars):\n                if (self.adj_matrix[current][next_node] == 1 or \n                    self.adj_matrix[next_node][current] == 1):\n                    if next_node == end:\n                        return True\n                    stack.append(next_node)\n        return False\n    def visualize(self):\n        print(\"Causal Graph Structure:\")\n        for i, var in enumerate(self.variables):\n            children = self.get_children(var)\n            if children:\n                print(f\"{var} -> {', '.join(children)}\")\nclass CausalDiscovery:\n    def __init__(self, alpha: float = 0.05):\n        self.alpha = alpha\n    def pc_algorithm(self, data: np.ndarray, var_names: List[str]) -> CausalGraph:\n        n_vars = len(var_names)\n        skeleton = np.ones((n_vars, n_vars)) - np.eye(n_vars)\n        for order in range(n_vars - 2):\n            for i in range(n_vars):\n                for j in range(i + 1, n_vars):\n                    if skeleton[i][j] == 0:\n                        continue\n                    neighbors = [k for k in range(n_vars) \n                                if k != i and k != j and skeleton[i][k] == 1]\n                    if len(neighbors) >= order:\n                        from itertools import combinations\n                        for cond_set in combinations(neighbors, order):\n                            if self._test_independence(data, i, j, list(cond_set)):\n                                skeleton[i][j] = skeleton[j][i] = 0\n                                break\n        graph = CausalGraph(var_names)\n        oriented = self._orient_edges(skeleton, data)\n        for i in range(n_vars):\n            for j in range(n_vars):\n                if oriented[i][j] == 1:\n                    graph.add_edge(var_names[i], var_names[j])\n        return graph\n    def _test_independence(self, data: np.ndarray, i: int, j: int, \n                          cond_set: List[int]) -> bool:\n        if len(cond_set) == 0:\n            corr = np.corrcoef(data[:, i], data[:, j])[0, 1]\n            return abs(corr) < 0.1\n        from scipy.stats import pearsonr\n        X = data[:, [i] + cond_set]\n        Y = data[:, j]\n        if len(cond_set) == 1:\n            r_ij = np.corrcoef(data[:, i], data[:, j])[0, 1]\n            r_ik = np.corrcoef(data[:, i], data[:, cond_set[0]])[0, 1]\n            r_jk = np.corrcoef(data[:, j], data[:, cond_set[0]])[0, 1]\n            partial_corr = (r_ij - r_ik * r_jk) / np.sqrt((1 - r_ik**2) * (1 - r_jk**2))\n            return abs(partial_corr) < 0.1\n        return False\n    def _orient_edges(self, skeleton: np.ndarray, data: np.ndarray) -> np.ndarray:\n        n_vars = skeleton.shape[0]\n        oriented = np.zeros_like(skeleton)\n        for i in range(n_vars):\n            for j in range(n_vars):\n                if skeleton[i][j] == 1:\n                    var_i = np.var(data[:, i])\n                    var_j = np.var(data[:, j])\n                    if var_i > var_j:\n                        oriented[i][j] = 1\n                    else:\n                        oriented[j][i] = 1\n        return oriented\nclass InterventionalDataset:\n    def __init__(self):\n        self.observational_data = []\n        self.interventional_data = {}\n    def add_observational(self, data: Dict[str, np.ndarray]):\n        self.observational_data.append(data)\n    def add_interventional(self, intervention: str, data: Dict[str, np.ndarray]):\n        if intervention not in self.interventional_data:\n            self.interventional_data[intervention] = []\n        self.interventional_data[intervention].append(data)\nclass CausalWorldModel(nn.Module):\n    def __init__(self, causal_graph: CausalGraph, state_dims: Dict[str, int],\n                 action_dim: int, hidden_dim: int = 128):\n        super().__init__()\n        self.causal_graph = causal_graph\n        self.state_dims = state_dims\n        self.action_dim = action_dim\n        self.variables = causal_graph.variables\n        self.predictors = nn.ModuleDict()\n        for var in self.variables:\n            parents = causal_graph.get_parents(var)\n            parent_dim = sum(state_dims[p] for p in parents)\n            input_dim = parent_dim + action_dim\n            output_dim = state_dims[var]\n            self.predictors[var] = nn.Sequential(\n                nn.Linear(input_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, output_dim)\n            )\n    def forward(self, states: Dict[str, torch.Tensor], \n                actions: torch.Tensor) -> Dict[str, torch.Tensor]:\n        predictions = {}\n        for var in self.variables:\n            parents = self.causal_graph.get_parents(var)\n            parent_values = []\n            for parent in parents:\n                parent_values.append(states[parent])\n            if parent_values:\n                parent_input = torch.cat(parent_values, dim=-1)\n                model_input = torch.cat([parent_input, actions], dim=-1)\n            else:\n                model_input = actions\n            predictions[var] = self.predictors[var](model_input)\n        return predictions\n    def intervene(self, states: Dict[str, torch.Tensor], actions: torch.Tensor,\n                  interventions: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n        modified_states = {k: v.clone() for k, v in states.items()}\n        for var, value in interventions.items():\n            modified_states[var] = value\n        predictions = {}\n        for var in self.variables:\n            if var in interventions:\n                predictions[var] = interventions[var]\n            else:\n                parents = self.causal_graph.get_parents(var)\n                parent_values = [modified_states[p] for p in parents]\n                if parent_values:\n                    parent_input = torch.cat(parent_values, dim=-1)\n                    model_input = torch.cat([parent_input, actions], dim=-1)\n                else:\n                    model_input = actions\n                predictions[var] = self.predictors[var](model_input)\n        return predictions\nclass CausalPolicyGradient:\n    def __init__(self, policy: nn.Module, causal_graph: CausalGraph,\n                 lr: float = 3e-4, causal_weight: float = 0.1):\n        self.policy = policy\n        self.causal_graph = causal_graph\n        self.causal_weight = causal_weight\n        self.optimizer = torch.optim.Adam(policy.parameters(), lr=lr)\n    def update(self, states: Dict[str, torch.Tensor], actions: torch.Tensor,\n               rewards: torch.Tensor, causal_world_model: CausalWorldModel):\n        log_probs = self.policy.get_log_prob(states, actions)\n        policy_loss = -(log_probs * rewards).mean()\n        causal_loss = self._compute_causal_regularization(\n            states, actions, causal_world_model\n        )\n        total_loss = policy_loss + self.causal_weight * causal_loss\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        self.optimizer.step()\n        return {\n            'policy_loss': policy_loss.item(),\n            'causal_loss': causal_loss.item(),\n            'total_loss': total_loss.item()\n        }\n    def _compute_causal_regularization(self, states: Dict[str, torch.Tensor],\n                                     actions: torch.Tensor,\n                                     causal_world_model: CausalWorldModel) -> torch.Tensor:\n        consistency_loss = 0\n        n_interventions = 0\n        for var in self.causal_graph.variables:\n            intervention_value = torch.randn_like(states[var])\n            interventions = {var: intervention_value}\n            pred_intervened = causal_world_model.intervene(states, actions, interventions)\n            modified_states = {k: v.clone() for k, v in states.items()}\n            modified_states[var] = intervention_value\n            pred_modified = causal_world_model(modified_states, actions)\n            for other_var in self.causal_graph.variables:\n                if other_var != var and not self._is_descendant(var, other_var):\n                    consistency_loss += F.mse_loss(\n                        pred_intervened[other_var],\n                        pred_modified[other_var]\n                    )\n                    n_interventions += 1\n        return consistency_loss / max(n_interventions, 1)\n    def _is_descendant(self, ancestor: str, var: str) -> bool:\n        visited = set()\n        stack = self.causal_graph.get_children(ancestor)\n        while stack:\n            current = stack.pop()\n            if current == var:\n                return True\n            if current in visited:\n                continue\n            visited.add(current)\n            stack.extend(self.causal_graph.get_children(current))\n        return False\ndef create_synthetic_causal_data(n_samples: int = 1000):\n    data = {}\n    e1 = np.random.normal(0, 0.5, n_samples)\n    e2 = np.random.normal(0, 0.3, n_samples)\n    e3 = np.random.normal(0, 0.4, n_samples)\n    actions = np.random.uniform(-1, 1, (n_samples, 2))\n    X1 = e1\n    X2 = 0.7 * X1 + 0.5 * actions[:, 0] + e2\n    X3 = 0.8 * X2 + 0.3 * X1 + e3\n    data['X1'] = X1.reshape(-1, 1)\n    data['X2'] = X2.reshape(-1, 1)\n    data['X3'] = X3.reshape(-1, 1)\n    return data, actions\nprint(\"üîç Testing Causal Discovery...\")\nstates_data, actions_data = create_synthetic_causal_data(1000)\ndata_matrix = np.hstack([states_data['X1'], states_data['X2'], states_data['X3']])\nvar_names = ['X1', 'X2', 'X3']\ndiscovery = CausalDiscovery(alpha=0.05)\ndiscovered_graph = discovery.pc_algorithm(data_matrix, var_names)\nprint(\"Discovered Causal Structure:\")\ndiscovered_graph.visualize()\ntrue_graph = CausalGraph(var_names)\ntrue_graph.add_edge('X1', 'X2')\ntrue_graph.add_edge('X1', 'X3')\ntrue_graph.add_edge('X2', 'X3')\nprint(\"\\nTrue Causal Structure:\")\ntrue_graph.visualize()\nprint(\"\\n‚úÖ Causal Discovery Complete!\")\nprint(\"Note: Discovery accuracy depends on data size and statistical tests.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe02fc",
   "metadata": {},
   "source": [
    "# Part IV: Quantum Reinforcement Learning\n",
    "\n",
    "## Theoretical Foundations\n",
    "\n",
    "### Introduction to Quantum Computing for RL\n",
    "\n",
    "Quantum Reinforcement Learning (QRL) leverages quantum mechanical phenomena to enhance reinforcement learning algorithms. This emerging field promises exponential speedups for certain RL problems and enables exploration of vast state spaces that are intractable for classical computers.\n",
    "\n",
    "**Key Quantum Phenomena:**\n",
    "- **Superposition**: Quantum states can exist in multiple states simultaneously\n",
    "- **Entanglement**: Quantum systems can be correlated in non-classical ways\n",
    "- **Interference**: Quantum amplitudes can interfere constructively or destructively\n",
    "- **Quantum Parallelism**: Process multiple inputs simultaneously\n",
    "\n",
    "### Quantum Computing Fundamentals\n",
    "\n",
    "#### 1. Quantum State Representation\n",
    "\n",
    "**Qubit State:**\n",
    "$$|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle$$\n",
    "\n",
    "Where $|\\alpha|^2 + |\\beta|^2 = 1$ and $\\alpha, \\beta \\in \\mathbb{C}$.\n",
    "\n",
    "**Multi-qubit System:**\n",
    "$$|\\psi\\rangle = \\sum_{i=0}^{2^n-1} \\alpha_i |i\\rangle$$\n",
    "\n",
    "For $n$ qubits with $\\sum_{i=0}^{2^n-1} |\\alpha_i|^2 = 1$.\n",
    "\n",
    "#### 2. Quantum Operations\n",
    "\n",
    "**Quantum Gates:**\n",
    "- **Pauli-X**: $X = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$ (Bit flip)\n",
    "- **Pauli-Y**: $Y = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}$\n",
    "- **Pauli-Z**: $Z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$ (Phase flip)\n",
    "- **Hadamard**: $H = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$ (Superposition)\n",
    "\n",
    "**Rotation Gates:**\n",
    "$$R_x(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -i\\sin(\\theta/2) \\\\ -i\\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix}$$\n",
    "\n",
    "$$R_y(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -\\sin(\\theta/2) \\\\ \\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix}$$\n",
    "\n",
    "#### 3. Quantum Measurement\n",
    "\n",
    "**Born Rule:**\n",
    "$$P(|i\\rangle) = |\\langle i | \\psi \\rangle|^2$$\n",
    "\n",
    "The probability of measuring state $|i\\rangle$ from state $|\\psi\\rangle$.\n",
    "\n",
    "### Quantum Reinforcement Learning Framework\n",
    "\n",
    "#### 1. Quantum MDP (QMDP)\n",
    "\n",
    "**Quantum State Space:**\n",
    "States are represented as quantum states in Hilbert space $\\mathcal{H}$:\n",
    "$$|\\psi_s\\rangle \\in \\mathcal{H}, \\quad \\langle\\psi_s|\\psi_s\\rangle = 1$$\n",
    "\n",
    "**Quantum Action Space:**\n",
    "Actions correspond to unitary operations:\n",
    "$$\\mathcal{A} = \\{U_a : U_a^\\dagger U_a = I\\}$$\n",
    "\n",
    "**Quantum Transition Dynamics:**\n",
    "$$|\\psi_{t+1}\\rangle = U_{a_t} |\\psi_t\\rangle \\otimes |\\text{env}_t\\rangle$$\n",
    "\n",
    "#### 2. Quantum Value Functions\n",
    "\n",
    "**Quantum Q-Function:**\n",
    "$$Q(|\\psi\\rangle, U_a) = \\langle\\psi| U_a^\\dagger \\hat{R} U_a |\\psi\\rangle + \\gamma \\mathbb{E}[V(|\\psi'\\rangle)]$$\n",
    "\n",
    "Where $\\hat{R}$ is the reward operator.\n",
    "\n",
    "**Quantum Bellman Equation:**\n",
    "$$\\hat{V}|\\psi\\rangle = \\max_{U_a} \\left(\\hat{R}U_a|\\psi\\rangle + \\gamma \\sum_{|\\psi'\\rangle} P(|\\psi'\\rangle||\\psi\\rangle, U_a) \\hat{V}|\\psi'\\rangle\\right)$$\n",
    "\n",
    "#### 3. Quantum Policy Representation\n",
    "\n",
    "**Parameterized Quantum Circuit (PQC):**\n",
    "$$|\\psi(\\theta)\\rangle = U_L(\\theta_L) \\cdots U_2(\\theta_2) U_1(\\theta_1) |\\psi_0\\rangle$$\n",
    "\n",
    "Where each $U_i(\\theta_i)$ is a parameterized unitary gate.\n",
    "\n",
    "**Quantum Policy:**\n",
    "$$\\pi_\\theta(a|s) = |\\langle a | U(\\theta) |s \\rangle|^2$$\n",
    "\n",
    "### Variational Quantum Algorithms for RL\n",
    "\n",
    "#### 1. Variational Quantum Eigensolver (VQE) for Value Functions\n",
    "\n",
    "**Objective:**\n",
    "$$\\theta^* = \\arg\\min_\\theta \\langle\\psi(\\theta)| \\hat{H} |\\psi(\\theta)\\rangle$$\n",
    "\n",
    "Where $\\hat{H}$ encodes the RL problem structure.\n",
    "\n",
    "**Gradient Calculation:**\n",
    "$$\\nabla_\\theta f(\\theta) = \\frac{1}{2}[f(\\theta + \\pi/2) - f(\\theta - \\pi/2)]$$\n",
    "\n",
    "#### 2. Quantum Approximate Optimization Algorithm (QAOA)\n",
    "\n",
    "**QAOA Ansatz:**\n",
    "$$|\\psi(\\gamma, \\beta)\\rangle = \\prod_{p=1}^P U_B(\\beta_p) U_C(\\gamma_p) |\\psi_0\\rangle$$\n",
    "\n",
    "Where:\n",
    "- $U_C(\\gamma) = \\exp(-i\\gamma \\hat{H}_C)$ (Cost Hamiltonian)\n",
    "- $U_B(\\beta) = \\exp(-i\\beta \\hat{H}_B)$ (Mixer Hamiltonian)\n",
    "\n",
    "### Quantum Advantage in RL\n",
    "\n",
    "#### 1. Exponential State Space\n",
    "\n",
    "**Classical Scaling:**\n",
    "Memory: $O(2^n)$ for $n$-qubit states\n",
    "Operations: $O(2^{2n})$ for general operations\n",
    "\n",
    "**Quantum Scaling:**\n",
    "Memory: $O(n)$ qubits\n",
    "Operations: $O(poly(n))$ for many quantum algorithms\n",
    "\n",
    "#### 2. Quantum Speedups\n",
    "\n",
    "**Grover's Algorithm for RL:**\n",
    "- Search optimal actions in $O(\\sqrt{N})$ instead of $O(N)$\n",
    "- Applicable to unstructured action spaces\n",
    "\n",
    "**Quantum Walk for Exploration:**\n",
    "- Quadratic speedup over classical random walk\n",
    "- Enhanced exploration capabilities\n",
    "\n",
    "**Shor's Algorithm Applications:**\n",
    "- Factoring in cryptographic environments\n",
    "- Period finding in periodic MDPs\n",
    "\n",
    "### Quantum Machine Learning Integration\n",
    "\n",
    "#### 1. Quantum Neural Networks (QNNs)\n",
    "\n",
    "**Quantum Perceptron:**\n",
    "$$f(x) = \\langle 0^{\\otimes n} | U^\\dagger(\\theta) M U(\\theta) |x\\rangle$$\n",
    "\n",
    "Where $U(\\theta)$ is a parameterized quantum circuit and $M$ is a measurement operator.\n",
    "\n",
    "**Quantum Convolutional Neural Networks:**\n",
    "- Quantum convolution using local unitaries\n",
    "- Translation equivariance in quantum feature maps\n",
    "\n",
    "#### 2. Quantum Kernel Methods\n",
    "\n",
    "**Quantum Feature Map:**\n",
    "$$\\Phi(x) = |\\phi(x)\\rangle = U_\\phi(x)|0\\rangle^{\\otimes n}$$\n",
    "\n",
    "**Quantum Kernel:**\n",
    "$$K(x_i, x_j) = |\\langle\\phi(x_i)|\\phi(x_j)\\rangle|^2$$\n",
    "\n",
    "Potentially exponential advantage in feature space dimension.\n",
    "\n",
    "### Advanced QRL Techniques\n",
    "\n",
    "#### 1. Quantum Actor-Critic\n",
    "\n",
    "**Quantum Actor:**\n",
    "$$\\pi_\\theta(a|s) = \\text{Tr}[\\Pi_a U_\\theta(s) \\rho_s U_\\theta(s)^\\dagger]$$\n",
    "\n",
    "Where $\\Pi_a$ is the projector onto action $a$.\n",
    "\n",
    "**Quantum Critic:**\n",
    "$$V_\\phi(s) = \\text{Tr}[\\hat{V}_\\phi \\rho_s]$$\n",
    "\n",
    "**Quantum Policy Gradient:**\n",
    "$$\\nabla_\\theta J(\\theta) = \\sum_{s,a} \\rho^\\pi(s) \\nabla_\\theta \\pi_\\theta(a|s) Q^\\pi(s,a)$$\n",
    "\n",
    "#### 2. Quantum Experience Replay\n",
    "\n",
    "**Quantum Superposition of Experiences:**\n",
    "$$|\\text{memory}\\rangle = \\frac{1}{\\sqrt{N}} \\sum_{i=1}^N |s_i, a_i, r_i, s_i'\\rangle$$\n",
    "\n",
    "**Quantum Sampling:**\n",
    "Use quantum interference to bias sampling towards important experiences.\n",
    "\n",
    "#### 3. Quantum Multi-Agent RL\n",
    "\n",
    "**Entangled Agent States:**\n",
    "$$|\\psi_{\\text{agents}}\\rangle = \\frac{1}{\\sqrt{2}}(|\\psi_1\\rangle \\otimes |\\psi_2\\rangle + |\\psi_1'\\rangle \\otimes |\\psi_2'\\rangle)$$\n",
    "\n",
    "**Quantum Communication:**\n",
    "Agents share quantum information through entanglement.\n",
    "\n",
    "### Quantum Error Correction in QRL\n",
    "\n",
    "#### 1. Noisy Intermediate-Scale Quantum (NISQ) Era\n",
    "\n",
    "**Noise Models:**\n",
    "- Decoherence: $\\rho(t) = e^{-\\Gamma t} \\rho(0)$\n",
    "- Gate errors: Imperfect unitary operations\n",
    "- Measurement errors: Probabilistic bit flips\n",
    "\n",
    "**Error Mitigation:**\n",
    "- Zero noise extrapolation\n",
    "- Error amplification and cancellation\n",
    "- Probabilistic error cancellation\n",
    "\n",
    "#### 2. Fault-Tolerant QRL\n",
    "\n",
    "**Quantum Error Correction Codes:**\n",
    "- Surface codes for topological protection\n",
    "- Stabilizer codes for syndrome detection\n",
    "- Logical qubit operations\n",
    "\n",
    "### Applications and Use Cases\n",
    "\n",
    "#### 1. Quantum Chemistry RL\n",
    "- Molecular dynamics simulation\n",
    "- Drug discovery optimization\n",
    "- Catalyst design\n",
    "\n",
    "#### 2. Quantum Finance\n",
    "- Portfolio optimization with quantum speedup\n",
    "- Risk analysis using quantum simulation\n",
    "- Quantum Monte Carlo for derivatives pricing\n",
    "\n",
    "#### 3. Quantum Cryptography RL\n",
    "- Quantum key distribution protocols\n",
    "- Post-quantum cryptography\n",
    "- Quantum-safe communications\n",
    "\n",
    "#### 4. Quantum Optimization\n",
    "- Traffic flow optimization\n",
    "- Supply chain management\n",
    "- Resource allocation problems\n",
    "\n",
    "### Current Limitations and Challenges\n",
    "\n",
    "#### 1. Hardware Limitations\n",
    "- Limited qubit count and coherence time\n",
    "- High error rates in current quantum devices\n",
    "- Connectivity constraints in quantum architectures\n",
    "\n",
    "#### 2. Algorithmic Challenges\n",
    "- Barren plateaus in quantum optimization\n",
    "- Classical simulation for algorithm development\n",
    "- Quantum advantage verification\n",
    "\n",
    "#### 3. Practical Implementation\n",
    "- Quantum software development complexity\n",
    "- Integration with classical systems\n",
    "- Scalability to real-world problems\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "#### 1. Near-term Applications\n",
    "- Hybrid classical-quantum algorithms\n",
    "- NISQ-era quantum advantage demonstrations\n",
    "- Quantum-enhanced machine learning\n",
    "\n",
    "#### 2. Long-term Vision\n",
    "- Fault-tolerant quantum RL systems\n",
    "- Universal quantum learning machines\n",
    "- Quantum artificial general intelligence\n",
    "\n",
    "#### 3. Theoretical Advances\n",
    "- Quantum learning theory foundations\n",
    "- Quantum-classical complexity separations\n",
    "- Novel quantum algorithms for RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df59bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuantumState:\n    def __init__(self, amplitudes: np.ndarray):\n        self.amplitudes = amplitudes / np.linalg.norm(amplitudes)\n        self.n_qubits = int(np.log2(len(amplitudes)))\n    @classmethod\n    def zero_state(cls, n_qubits: int):\n        amplitudes = np.zeros(2**n_qubits)\n        amplitudes[0] = 1.0\n        return cls(amplitudes)\n    @classmethod\n    def uniform_superposition(cls, n_qubits: int):\n        amplitudes = np.ones(2**n_qubits) / np.sqrt(2**n_qubits)\n        return cls(amplitudes)\n    def probability(self, basis_state: int) -> float:\n        return abs(self.amplitudes[basis_state])**2\n    def measure(self) -> int:\n        probabilities = [abs(amp)**2 for amp in self.amplitudes]\n        return np.random.choice(len(probabilities), p=probabilities)\n    def __repr__(self):\n        return f\"QuantumState({self.amplitudes})\"\nclass QuantumGate:\n    def __init__(self, matrix: np.ndarray):\n        self.matrix = matrix.astype(complex)\n    @classmethod\n    def pauli_x(cls):\n        return cls(np.array([[0, 1], [1, 0]]))\n    @classmethod\n    def pauli_y(cls):\n        return cls(np.array([[0, -1j], [1j, 0]]))\n    @classmethod\n    def pauli_z(cls):\n        return cls(np.array([[1, 0], [0, -1]]))\n    @classmethod\n    def hadamard(cls):\n        return cls(np.array([[1, 1], [1, -1]]) / np.sqrt(2))\n    @classmethod\n    def rotation_x(cls, theta: float):\n        c = np.cos(theta/2)\n        s = np.sin(theta/2)\n        return cls(np.array([[c, -1j*s], [-1j*s, c]]))\n    @classmethod\n    def rotation_y(cls, theta: float):\n        c = np.cos(theta/2)\n        s = np.sin(theta/2)\n        return cls(np.array([[c, -s], [s, c]]))\n    @classmethod\n    def rotation_z(cls, theta: float):\n        return cls(np.array([[np.exp(-1j*theta/2), 0], \n                           [0, np.exp(1j*theta/2)]]))\n    @classmethod\n    def cnot(cls):\n        return cls(np.array([[1, 0, 0, 0],\n                           [0, 1, 0, 0],\n                           [0, 0, 0, 1],\n                           [0, 0, 1, 0]]))\n    def apply(self, state: QuantumState) -> QuantumState:\n        new_amplitudes = self.matrix @ state.amplitudes\n        return QuantumState(new_amplitudes)\n    def tensor(self, other: 'QuantumGate') -> 'QuantumGate':\n        return QuantumGate(np.kron(self.matrix, other.matrix))\nclass QuantumCircuit:\n    def __init__(self, n_qubits: int):\n        self.n_qubits = n_qubits\n        self.gates = []\n        self.parameters = []\n    def add_gate(self, gate: QuantumGate, qubits: List[int]):\n        self.gates.append((gate, qubits))\n    def add_parameterized_gate(self, gate_type: str, qubit: int, param_idx: int):\n        self.gates.append((gate_type, qubit, param_idx))\n    def execute(self, initial_state: QuantumState, \n                parameters: np.ndarray = None) -> QuantumState:\n        current_state = initial_state\n        for gate_info in self.gates:\n            if isinstance(gate_info[0], QuantumGate):\n                gate, qubits = gate_info\n                if len(qubits) == 1:\n                    full_gate = self._expand_gate(gate, qubits[0])\n                else:\n                    full_gate = gate\n                current_state = full_gate.apply(current_state)\n            else:\n                gate_type, qubit, param_idx = gate_info\n                param_value = parameters[param_idx] if parameters is not None else 0\n                if gate_type == 'rx':\n                    gate = QuantumGate.rotation_x(param_value)\n                elif gate_type == 'ry':\n                    gate = QuantumGate.rotation_y(param_value)\n                elif gate_type == 'rz':\n                    gate = QuantumGate.rotation_z(param_value)\n                full_gate = self._expand_gate(gate, qubit)\n                current_state = full_gate.apply(current_state)\n        return current_state\n    def _expand_gate(self, gate: QuantumGate, target_qubit: int) -> QuantumGate:\n        identity = QuantumGate(np.eye(2))\n        if self.n_qubits == 1:\n            return gate\n        gates = []\n        for i in range(self.n_qubits):\n            if i == target_qubit:\n                gates.append(gate)\n            else:\n                gates.append(identity)\n        result = gates[0]\n        for i in range(1, len(gates)):\n            result = result.tensor(gates[i])\n        return result\nclass VariationalQuantumCircuit:\n    def __init__(self, n_qubits: int, n_layers: int):\n        self.n_qubits = n_qubits\n        self.n_layers = n_layers\n        self.n_parameters = 3 * n_qubits * n_layers\n        self.parameters = np.random.uniform(0, 2*np.pi, self.n_parameters)\n    def create_circuit(self) -> QuantumCircuit:\n        circuit = QuantumCircuit(self.n_qubits)\n        param_idx = 0\n        for layer in range(self.n_layers):\n            for qubit in range(self.n_qubits):\n                circuit.add_parameterized_gate('rx', qubit, param_idx)\n                param_idx += 1\n                circuit.add_parameterized_gate('ry', qubit, param_idx)\n                param_idx += 1\n                circuit.add_parameterized_gate('rz', qubit, param_idx)\n                param_idx += 1\n            if layer < self.n_layers - 1:\n                for qubit in range(self.n_qubits - 1):\n                    circuit.add_gate(QuantumGate.cnot(), [qubit, qubit + 1])\n                if self.n_qubits > 2:\n                    circuit.add_gate(QuantumGate.cnot(), [self.n_qubits - 1, 0])\n        return circuit\n    def forward(self, input_state: QuantumState) -> QuantumState:\n        circuit = self.create_circuit()\n        return circuit.execute(input_state, self.parameters)\n    def measure_expectation(self, observable: QuantumGate, \n                          input_state: QuantumState) -> float:\n        output_state = self.forward(input_state)\n        expectation = np.real(\n            np.conj(output_state.amplitudes) @ observable.matrix @ output_state.amplitudes\n        )\n        return expectation\n    def gradient(self, observable: QuantumGate, input_state: QuantumState,\n                param_idx: int) -> float:\n        original_param = self.parameters[param_idx]\n        self.parameters[param_idx] = original_param + np.pi/2\n        expectation_plus = self.measure_expectation(observable, input_state)\n        self.parameters[param_idx] = original_param - np.pi/2\n        expectation_minus = self.measure_expectation(observable, input_state)\n        self.parameters[param_idx] = original_param\n        return 0.5 * (expectation_plus - expectation_minus)\nclass QuantumQLearning:\n    def __init__(self, n_qubits: int, n_actions: int, n_layers: int = 3,\n                 learning_rate: float = 0.1, gamma: float = 0.95):\n        self.n_qubits = n_qubits\n        self.n_actions = n_actions\n        self.learning_rate = learning_rate\n        self.gamma = gamma\n        self.q_circuits = {}\n        for action in range(n_actions):\n            self.q_circuits[action] = VariationalQuantumCircuit(n_qubits, n_layers)\n        self.q_observable = QuantumGate.pauli_z()\n    def state_to_quantum(self, state: np.ndarray) -> QuantumState:\n        if len(state) <= 2**self.n_qubits:\n            amplitudes = np.zeros(2**self.n_qubits)\n            amplitudes[:len(state)] = state\n            amplitudes = amplitudes / np.linalg.norm(amplitudes)\n            return QuantumState(amplitudes)\n        else:\n            state_index = int(np.sum(state * [2**i for i in range(len(state))]))\n            state_index = state_index % (2**self.n_qubits)\n            amplitudes = np.zeros(2**self.n_qubits)\n            amplitudes[state_index] = 1.0\n            return QuantumState(amplitudes)\n    def get_q_values(self, state: np.ndarray) -> np.ndarray:\n        quantum_state = self.state_to_quantum(state)\n        q_values = np.zeros(self.n_actions)\n        for action in range(self.n_actions):\n            q_values[action] = self.q_circuits[action].measure_expectation(\n                self.q_observable, quantum_state\n            )\n        return q_values\n    def select_action(self, state: np.ndarray, epsilon: float = 0.1) -> int:\n        if np.random.random() < epsilon:\n            return np.random.randint(self.n_actions)\n        else:\n            q_values = self.get_q_values(state)\n            return np.argmax(q_values)\n    def update(self, state: np.ndarray, action: int, reward: float,\n               next_state: np.ndarray, done: bool):\n        quantum_state = self.state_to_quantum(state)\n        current_q = self.q_circuits[action].measure_expectation(\n            self.q_observable, quantum_state\n        )\n        if done:\n            target_q = reward\n        else:\n            next_q_values = self.get_q_values(next_state)\n            target_q = reward + self.gamma * np.max(next_q_values)\n        td_error = target_q - current_q\n        for param_idx in range(self.q_circuits[action].n_parameters):\n            gradient = self.q_circuits[action].gradient(\n                self.q_observable, quantum_state, param_idx\n            )\n            self.q_circuits[action].parameters[param_idx] += (\n                self.learning_rate * td_error * gradient\n            )\nclass QuantumActorCritic:\n    def __init__(self, n_qubits: int, n_actions: int, n_layers: int = 3):\n        self.n_qubits = n_qubits\n        self.n_actions = n_actions\n        self.actor_circuit = VariationalQuantumCircuit(n_qubits, n_layers)\n        self.critic_circuit = VariationalQuantumCircuit(n_qubits, n_layers)\n        self.policy_observables = [\n            QuantumGate.pauli_z() for _ in range(n_actions)\n        ]\n        self.value_observable = QuantumGate.pauli_z()\n        self.learning_rate = 0.01\n        self.gamma = 0.95\n    def state_to_quantum(self, state: np.ndarray) -> QuantumState:\n        amplitudes = np.zeros(2**self.n_qubits)\n        state_norm = np.linalg.norm(state)\n        if state_norm > 0:\n            state = state / state_norm\n        for i, val in enumerate(state[:2**self.n_qubits]):\n            amplitudes[i] = val\n        amplitudes = amplitudes / np.linalg.norm(amplitudes)\n        return QuantumState(amplitudes)\n    def get_action_probabilities(self, state: np.ndarray) -> np.ndarray:\n        quantum_state = self.state_to_quantum(state)\n        expectations = np.zeros(self.n_actions)\n        for action in range(self.n_actions):\n            expectations[action] = self.actor_circuit.measure_expectation(\n                self.policy_observables[action], quantum_state\n            )\n        exp_vals = np.exp(expectations)\n        probabilities = exp_vals / np.sum(exp_vals)\n        return probabilities\n    def get_value(self, state: np.ndarray) -> float:\n        quantum_state = self.state_to_quantum(state)\n        return self.critic_circuit.measure_expectation(\n            self.value_observable, quantum_state\n        )\n    def select_action(self, state: np.ndarray) -> int:\n        probabilities = self.get_action_probabilities(state)\n        return np.random.choice(self.n_actions, p=probabilities)\n    def update(self, state: np.ndarray, action: int, reward: float,\n               next_state: np.ndarray, done: bool):\n        quantum_state = self.state_to_quantum(state)\n        current_value = self.get_value(state)\n        if done:\n            target_value = reward\n        else:\n            next_value = self.get_value(next_state)\n            target_value = reward + self.gamma * next_value\n        td_error = target_value - current_value\n        for param_idx in range(self.critic_circuit.n_parameters):\n            gradient = self.critic_circuit.gradient(\n                self.value_observable, quantum_state, param_idx\n            )\n            self.critic_circuit.parameters[param_idx] += (\n                self.learning_rate * td_error * gradient\n            )\n        for param_idx in range(self.actor_circuit.n_parameters):\n            gradient = self.actor_circuit.gradient(\n                self.policy_observables[action], quantum_state, param_idx\n            )\n            self.actor_circuit.parameters[param_idx] += (\n                self.learning_rate * td_error * gradient\n            )\nclass QuantumEnvironment:\n    def __init__(self, n_qubits: int = 2):\n        self.n_qubits = n_qubits\n        self.state_dim = 2**n_qubits\n        self.n_actions = 4\n        self.target_state = QuantumState.uniform_superposition(n_qubits)\n        self.reset()\n    def reset(self) -> np.ndarray:\n        self.current_state = QuantumState.zero_state(self.n_qubits)\n        self.steps = 0\n        return self.current_state.amplitudes.real\n    def step(self, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n        if action == 0:\n            gate = QuantumGate.hadamard()\n        elif action == 1:\n            gate = QuantumGate.pauli_x()\n        elif action == 2:\n            gate = QuantumGate.rotation_y(np.pi/4)\n        else:\n            gate = QuantumGate.rotation_z(np.pi/4)\n        if self.n_qubits == 1:\n            self.current_state = gate.apply(self.current_state)\n        else:\n            full_gate = self._expand_gate_to_system(gate, 0)\n            self.current_state = full_gate.apply(self.current_state)\n        fidelity = abs(np.vdot(\n            self.current_state.amplitudes,\n            self.target_state.amplitudes\n        ))**2\n        reward = fidelity\n        self.steps += 1\n        done = self.steps >= 10 or fidelity > 0.95\n        return self.current_state.amplitudes.real, reward, done, {}\n    def _expand_gate_to_system(self, gate: QuantumGate, target_qubit: int) -> QuantumGate:\n        identity = QuantumGate(np.eye(2))\n        gates = []\n        for i in range(self.n_qubits):\n            if i == target_qubit:\n                gates.append(gate)\n            else:\n                gates.append(identity)\n        result = gates[0]\n        for i in range(1, len(gates)):\n            result = result.tensor(gates[i])\n        return result\nprint(\"üöÄ Testing Quantum Reinforcement Learning...\")\nenv = QuantumEnvironment(n_qubits=2)\nstate_dim = env.state_dim\nn_actions = env.n_actions\nprint(f\"State dimension: {state_dim}\")\nprint(f\"Number of actions: {n_actions}\")\nprint(\"\\nüìä Testing Quantum Q-Learning...\")\nqql_agent = QuantumQLearning(\n    n_qubits=2,\n    n_actions=n_actions,\n    n_layers=2,\n    learning_rate=0.1\n)\nepisode_rewards = []\nfor episode in range(50):\n    state = env.reset()\n    total_reward = 0\n    done = False\n    while not done:\n        action = qql_agent.select_action(state, epsilon=0.1)\n        next_state, reward, done, _ = env.step(action)\n        qql_agent.update(state, action, reward, next_state, done)\n        state = next_state\n        total_reward += reward\n    episode_rewards.append(total_reward)\n    if episode % 10 == 0:\n        print(f\"Episode {episode}, Reward: {total_reward:.3f}\")\nprint(f\"\\nQuantum Q-Learning Results:\")\nprint(f\"Average reward (last 10 episodes): {np.mean(episode_rewards[-10:]):.3f}\")\nprint(\"\\nüé≠ Testing Quantum Actor-Critic...\")\nqac_agent = QuantumActorCritic(\n    n_qubits=2,\n    n_actions=n_actions,\n    n_layers=2\n)\nepisode_rewards_ac = []\nfor episode in range(30):\n    state = env.reset()\n    total_reward = 0\n    done = False\n    while not done:\n        action = qac_agent.select_action(state)\n        next_state, reward, done, _ = env.step(action)\n        qac_agent.update(state, action, reward, next_state, done)\n        state = next_state\n        total_reward += reward\n    episode_rewards_ac.append(total_reward)\n    if episode % 10 == 0:\n        print(f\"Episode {episode}, Reward: {total_reward:.3f}\")\nprint(f\"\\nQuantum Actor-Critic Results:\")\nprint(f\"Average reward (last 10 episodes): {np.mean(episode_rewards_ac[-10:]):.3f}\")\nprint(\"\\n‚úÖ Quantum RL Implementation Complete!\")\nprint(\"Note: This is a simplified implementation for educational purposes.\")\nprint(\"Production quantum RL would use specialized quantum computing frameworks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed7f62",
   "metadata": {},
   "source": [
    "# Part V: Integration and Advanced Applications\n",
    "\n",
    "## Synthesis of Advanced RL Paradigms\n",
    "\n",
    "The four paradigms we've explored‚ÄîWorld Models, Multi-Agent RL, Causal RL, and Quantum RL‚Äîrepresent the cutting edge of reinforcement learning research. Each addresses fundamental limitations of traditional RL approaches:\n",
    "\n",
    "### Paradigm Integration Matrix\n",
    "\n",
    "| Aspect | World Models | Multi-Agent RL | Causal RL | Quantum RL |\n",
    "|--------|-------------|----------------|-----------|------------|\n",
    "| **Sample Efficiency** | ‚úì Via planning | ‚úì Via sharing | ‚úì Via causal structure | ‚úì Via superposition |\n",
    "| **Interpretability** | ‚úì Via explicit models | ‚úì Via agent interaction | ‚úì Via causal graphs | ‚óê Via quantum states |\n",
    "| **Scalability** | ‚óê Model complexity | ‚úì Distributed learning | ‚óê Structure discovery | ‚óê Quantum advantage |\n",
    "| **Robustness** | ‚óê Model uncertainty | ‚úì Via diversity | ‚úì Via interventions | ‚óê Quantum decoherence |\n",
    "\n",
    "### Hybrid Approaches\n",
    "\n",
    "#### 1. Causal World Models\n",
    "Combining causal structure discovery with world model learning:\n",
    "```python\n",
    "class CausalWorldModel:\n",
    "    def __init__(self, causal_graph, dynamics_model):\n",
    "        self.causal_graph = causal_graph\n",
    "        self.dynamics_model = dynamics_model\n",
    "    \n",
    "    def predict_intervention(self, state, action, intervention):\n",
    "        # Use causal graph to modify dynamics\n",
    "        return self.dynamics_model.predict_with_intervention(\n",
    "            state, action, intervention, self.causal_graph\n",
    "        )\n",
    "```\n",
    "\n",
    "#### 2. Multi-Agent Causal RL\n",
    "Agents learning shared causal structures:\n",
    "```python\n",
    "class MultiAgentCausalRL:\n",
    "    def __init__(self, agents, shared_causal_graph):\n",
    "        self.agents = agents\n",
    "        self.shared_graph = shared_causal_graph\n",
    "    \n",
    "    def collective_structure_learning(self, experiences):\n",
    "        # Pool experiences for better causal discovery\n",
    "        return update_shared_causal_structure(experiences)\n",
    "```\n",
    "\n",
    "#### 3. Quantum Multi-Agent Systems\n",
    "Leveraging quantum entanglement for coordination:\n",
    "```python\n",
    "class QuantumMultiAgentSystem:\n",
    "    def __init__(self, n_agents, n_qubits):\n",
    "        self.entangled_state = create_entangled_state(n_agents, n_qubits)\n",
    "    \n",
    "    def quantum_coordination(self, local_observations):\n",
    "        return quantum_communication_protocol(\n",
    "            local_observations, self.entangled_state\n",
    "        )\n",
    "```\n",
    "\n",
    "## Real-World Applications\n",
    "\n",
    "### 1. Autonomous Vehicle Networks\n",
    "- **World Models**: Environmental prediction and planning\n",
    "- **Multi-Agent**: Vehicle coordination and traffic optimization\n",
    "- **Causal RL**: Understanding cause-effect in traffic patterns\n",
    "- **Quantum RL**: Optimization of large-scale traffic systems\n",
    "\n",
    "### 2. Financial Trading Systems\n",
    "- **World Models**: Market dynamics modeling\n",
    "- **Multi-Agent**: Multi-market trading strategies\n",
    "- **Causal RL**: Understanding causal relationships in market movements\n",
    "- **Quantum RL**: Portfolio optimization with quantum advantage\n",
    "\n",
    "### 3. Healthcare and Drug Discovery\n",
    "- **World Models**: Patient trajectory modeling\n",
    "- **Multi-Agent**: Multi-specialist treatment planning\n",
    "- **Causal RL**: Understanding treatment causality\n",
    "- **Quantum RL**: Molecular interaction simulation\n",
    "\n",
    "### 4. Climate and Environmental Management\n",
    "- **World Models**: Climate system modeling\n",
    "- **Multi-Agent**: Multi-region policy coordination\n",
    "- **Causal RL**: Climate intervention analysis\n",
    "- **Quantum RL**: Large-scale environmental optimization\n",
    "\n",
    "## Research Frontiers\n",
    "\n",
    "### 1. Theoretical Foundations\n",
    "- **Sample Complexity**: Unified bounds across paradigms\n",
    "- **Convergence Guarantees**: Multi-paradigm learning stability\n",
    "- **Transfer Learning**: Cross-paradigm knowledge transfer\n",
    "- **Meta-Learning**: Learning to choose appropriate paradigms\n",
    "\n",
    "### 2. Algorithmic Advances\n",
    "- **Hybrid Architectures**: Seamless paradigm integration\n",
    "- **Adaptive Switching**: Dynamic paradigm selection\n",
    "- **Federated Learning**: Distributed multi-paradigm training\n",
    "- **Continual Learning**: Lifelong multi-paradigm adaptation\n",
    "\n",
    "### 3. Implementation Challenges\n",
    "- **Computational Efficiency**: Scalable implementations\n",
    "- **Hardware Acceleration**: Specialized computing architectures\n",
    "- **Software Frameworks**: Unified development platforms\n",
    "- **Validation Methods**: Multi-paradigm evaluation metrics\n",
    "\n",
    "## Future Directions\n",
    "\n",
    "### Near-Term (2-5 years)\n",
    "1. **Practical Hybrid Systems**: Working implementations combining 2-3 paradigms\n",
    "2. **Industry Applications**: Deployment in specific domains\n",
    "3. **Standardization**: Common interfaces and evaluation protocols\n",
    "4. **Education**: Curriculum integration and training programs\n",
    "\n",
    "### Medium-Term (5-10 years)\n",
    "1. **Theoretical Unification**: Mathematical frameworks spanning all paradigms\n",
    "2. **Quantum Advantage**: Demonstrated speedups in real applications\n",
    "3. **Autonomous Systems**: Self-improving multi-paradigm agents\n",
    "4. **Societal Integration**: Widespread adoption across industries\n",
    "\n",
    "### Long-Term (10+ years)\n",
    "1. **Artificial General Intelligence**: Multi-paradigm foundations for AGI\n",
    "2. **Quantum-Classical Convergence**: Seamless quantum-classical computing\n",
    "3. **Causal Discovery Automation**: Fully automated causal structure learning\n",
    "4. **Multi-Agent Societies**: Complex artificial societies with emergent behavior\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This comprehensive exploration of advanced Deep Reinforcement Learning paradigms demonstrates the rich landscape of modern RL research. Each paradigm offers unique advantages:\n",
    "\n",
    "- **World Models** provide sample efficiency through learned dynamics\n",
    "- **Multi-Agent RL** enables coordination and emergence in complex systems\n",
    "- **Causal RL** offers interpretability and robustness through causal understanding\n",
    "- **Quantum RL** promises exponential advantages through quantum computation\n",
    "\n",
    "The future of reinforcement learning lies not in choosing a single paradigm, but in their thoughtful integration. By combining the strengths of each approach while mitigating their individual limitations, we can build AI systems that are:\n",
    "\n",
    "- **More Sample Efficient**: Learning faster with less data\n",
    "- **More Interpretable**: Providing clear reasoning for decisions\n",
    "- **More Robust**: Handling distribution shifts and uncertainties\n",
    "- **More Scalable**: Operating in complex, real-world environments\n",
    "\n",
    "The implementations provided in this notebook serve as stepping stones toward more sophisticated systems. While simplified for educational purposes, they demonstrate the core concepts that will drive the next generation of AI systems.\n",
    "\n",
    "As we advance toward artificial general intelligence, these paradigms will play crucial roles in creating AI systems that can understand, reason about, and operate effectively in our complex world. The journey from today's specialized RL agents to tomorrow's general AI systems will be paved with innovations across all these dimensions.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Paradigm Diversity**: Multiple approaches are needed for different aspects of intelligence\n",
    "2. **Integration Benefits**: Hybrid systems outperform single-paradigm approaches\n",
    "3. **Practical Applications**: Real-world deployment requires careful paradigm selection\n",
    "4. **Ongoing Research**: Many open questions remain in each paradigm\n",
    "5. **Future Potential**: The combination of these paradigms may enable breakthrough capabilities\n",
    "\n",
    "The field of reinforcement learning continues to evolve rapidly, and staying at the forefront requires understanding both the fundamental principles and the cutting-edge advances represented by these paradigms. This notebook provides a foundation for further exploration and implementation of these exciting directions in AI research."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}