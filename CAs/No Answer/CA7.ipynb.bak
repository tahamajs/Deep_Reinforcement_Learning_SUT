{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15851f8a",
   "metadata": {},
   "source": [
    "# CA7: Deep Q-Networks (DQN) and Value-Based Methods\n",
    "## Deep Reinforcement Learning - Session 7\n",
    "\n",
    "### Course Information\n",
    "- **Course**: Deep Reinforcement Learning\n",
    "- **Session**: 7\n",
    "- **Topic**: Deep Q-Networks (DQN) and Advanced Value-Based Methods\n",
    "- **Focus**: Complete theoretical foundations and practical implementations\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Theoretical Foundations**:\n",
    "   - Q-learning and its limitations in complex environments\n",
    "   - Deep Q-Networks (DQN) architecture and training process\n",
    "   - Experience replay and target networks\n",
    "   - Double DQN and addressing overestimation bias\n",
    "   - Dueling DQN and advantage decomposition\n",
    "   - Prioritized experience replay\n",
    "\n",
    "2. **Implementation Skills**:\n",
    "   - Complete DQN implementation from scratch\n",
    "   - Experience replay buffer design and management\n",
    "   - Target network updates and stability techniques\n",
    "   - Advanced variants: Double DQN, Dueling DQN, Rainbow DQN\n",
    "   - Performance analysis and debugging techniques\n",
    "\n",
    "3. **Practical Applications**:\n",
    "   - Training DQN on classic control and Atari environments\n",
    "   - Hyperparameter tuning and optimization strategies\n",
    "   - Comparison with policy gradient methods\n",
    "   - Real-world applications and limitations\n",
    "\n",
    "### Contents Overview\n",
    "\n",
    "1. **Section 1**: Theoretical Foundations of Deep Q-Learning\n",
    "2. **Section 2**: Basic DQN Implementation and Core Concepts\n",
    "3. **Section 3**: Experience Replay and Target Networks\n",
    "4. **Section 4**: Double DQN and Overestimation Bias\n",
    "5. **Section 5**: Dueling DQN and Value Decomposition\n",
    "6. **Section 6**: Prioritized Experience Replay\n",
    "7. **Section 7**: Rainbow DQN - Combining All Improvements\n",
    "8. **Section 8**: Performance Analysis and Comparisons\n",
    "9. **Section 9**: Advanced Topics and Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398268ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports and Setup\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import collections\n",
    "from collections import deque, namedtuple\n",
    "import warnings\n",
    "import math\n",
    "from typing import List, Tuple, Optional, Dict, Any\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# PyTorch settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Experience tuple for replay buffer\n",
    "Experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CA7: Deep Q-Networks (DQN) and Value-Based Methods\")\n",
    "print(\"=\"*60)\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c28ac4",
   "metadata": {},
   "source": [
    "# Section 1: Theoretical Foundations of Deep Q-Learning\n",
    "\n",
    "## 1.1 From Tabular Q-Learning to Deep Q-Networks\n",
    "\n",
    "Traditional Q-learning works well for discrete, small state spaces where we can maintain a Q-table. However, in complex environments like Atari games or continuous control tasks, the state space becomes enormous, making tabular methods impractical.\n",
    "\n",
    "### The Q-Learning Foundation\n",
    "\n",
    "The Q-learning update rule is:\n",
    "```\n",
    "Q(s, a) ← Q(s, a) + α[r + γ max Q(s', a') - Q(s, a)]\n",
    "                                a'\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Q(s, a)`: Action-value function\n",
    "- `α`: Learning rate\n",
    "- `r`: Reward\n",
    "- `γ`: Discount factor\n",
    "- `s'`: Next state\n",
    "\n",
    "### The Deep Q-Network Approach\n",
    "\n",
    "DQN replaces the Q-table with a deep neural network `Q(s, a; θ)` that approximates the Q-values for all actions given a state. The network parameters `θ` are updated to minimize the temporal difference (TD) error.\n",
    "\n",
    "## 1.2 Core Challenges in Deep Q-Learning\n",
    "\n",
    "### 1. Instability and Divergence\n",
    "- Neural networks can be unstable when used with bootstrapping\n",
    "- Updates can cause the target to change rapidly\n",
    "- Non-stationary target problem\n",
    "\n",
    "### 2. Correlation in Sequential Data\n",
    "- RL data is highly correlated (sequential states)\n",
    "- Violates the i.i.d. assumption of supervised learning\n",
    "- Can lead to poor generalization\n",
    "\n",
    "### 3. Overestimation Bias\n",
    "- Max operator in Q-learning can lead to overestimation\n",
    "- Amplified in function approximation settings\n",
    "- Can cause instability and poor performance\n",
    "\n",
    "## 1.3 DQN Innovations\n",
    "\n",
    "### Experience Replay\n",
    "- Store experiences in a replay buffer\n",
    "- Sample random batches for training\n",
    "- Breaks correlation and improves sample efficiency\n",
    "\n",
    "### Target Network\n",
    "- Use a separate target network for computing targets\n",
    "- Update target network periodically\n",
    "- Provides stability during training\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The DQN loss function is:\n",
    "```\n",
    "L(θ) = E[(r + γ max Q(s', a'; θ⁻) - Q(s, a; θ))²]\n",
    "                   a'\n",
    "```\n",
    "\n",
    "Where `θ⁻` represents the parameters of the target network.\n",
    "\n",
    "## 1.4 Algorithmic Overview\n",
    "\n",
    "1. **Initialize** main network Q(s,a;θ) and target network Q(s,a;θ⁻)\n",
    "2. **Initialize** experience replay buffer D\n",
    "3. **For each episode**:\n",
    "   - **For each step**:\n",
    "     - Select action using ε-greedy policy\n",
    "     - Execute action and observe reward and next state\n",
    "     - Store experience in replay buffer\n",
    "     - Sample random batch from replay buffer\n",
    "     - Compute target values using target network\n",
    "     - Update main network parameters\n",
    "     - Periodically update target network\n",
    "\n",
    "This foundation enables us to tackle complex, high-dimensional problems that were previously intractable with traditional Q-learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical Foundations Visualization and Basic Implementation\n",
    "\n",
    "class QNetworkVisualization:\n",
    "    \"\"\"\n",
    "    Visualization tools for understanding Q-learning concepts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fig_count = 0\n",
    "    \n",
    "    def visualize_q_learning_concepts(self):\n",
    "        \"\"\"Visualize core Q-learning concepts\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Q-learning Update Mechanism\n",
    "        ax = axes[0, 0]\n",
    "        \n",
    "        # Simulate Q-values for a simple grid world\n",
    "        states = ['S1', 'S2', 'S3', 'S4']\n",
    "        actions = ['Up', 'Down', 'Left', 'Right']\n",
    "        \n",
    "        # Sample Q-values before and after update\n",
    "        q_before = np.random.rand(4, 4) * 10\n",
    "        q_after = q_before.copy()\n",
    "        q_after[1, 2] += 2  # Simulate an update\n",
    "        \n",
    "        # Create heatmap comparison\n",
    "        im1 = ax.imshow(q_before, cmap='viridis', aspect='auto')\n",
    "        ax.set_title('Q-Values Before Update')\n",
    "        ax.set_xticks(range(4))\n",
    "        ax.set_xticklabels(actions)\n",
    "        ax.set_yticks(range(4))\n",
    "        ax.set_yticklabels(states)\n",
    "        \n",
    "        # Add values as text\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                ax.text(j, i, f'{q_before[i, j]:.1f}', ha='center', va='center', color='white')\n",
    "        \n",
    "        plt.colorbar(im1, ax=ax)\n",
    "        \n",
    "        # 2. Experience Replay Concept\n",
    "        ax = axes[0, 1]\n",
    "        \n",
    "        # Simulate sequential vs random sampling\n",
    "        episodes = np.arange(1, 101)\n",
    "        sequential_loss = 10 * np.exp(-episodes/30) + np.random.normal(0, 0.5, 100)\n",
    "        replay_loss = 8 * np.exp(-episodes/20) + np.random.normal(0, 0.3, 100)\n",
    "        \n",
    "        ax.plot(episodes, sequential_loss, label='Sequential Training', alpha=0.7, linewidth=2)\n",
    "        ax.plot(episodes, replay_loss, label='Experience Replay', alpha=0.7, linewidth=2)\n",
    "        ax.fill_between(episodes, sequential_loss, alpha=0.3)\n",
    "        ax.fill_between(episodes, replay_loss, alpha=0.3)\n",
    "        \n",
    "        ax.set_title('Learning Curves: Sequential vs Replay')\n",
    "        ax.set_xlabel('Training Episodes')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Target Network Stability\n",
    "        ax = axes[1, 0]\n",
    "        \n",
    "        steps = np.arange(0, 1000)\n",
    "        # Main network updates frequently\n",
    "        main_q = 10 + np.cumsum(np.random.normal(0, 0.1, 1000))\n",
    "        # Target network updates every 100 steps\n",
    "        target_q = []\n",
    "        current_target = 10\n",
    "        \n",
    "        for i, step in enumerate(steps):\n",
    "            if step % 100 == 0 and step > 0:\n",
    "                current_target = main_q[i]\n",
    "            target_q.append(current_target)\n",
    "        \n",
    "        ax.plot(steps, main_q, label='Main Network Q(s,a)', alpha=0.8, linewidth=1)\n",
    "        ax.plot(steps, target_q, label='Target Network Q(s,a)', alpha=0.8, linewidth=2, drawstyle='steps-post')\n",
    "        \n",
    "        ax.set_title('Target Network Update Schedule')\n",
    "        ax.set_xlabel('Training Steps')\n",
    "        ax.set_ylabel('Q-Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Epsilon-Greedy Exploration\n",
    "        ax = axes[1, 1]\n",
    "        \n",
    "        episodes = np.arange(0, 1000)\n",
    "        epsilon_decay = 0.995\n",
    "        epsilon_min = 0.01\n",
    "        epsilon_values = []\n",
    "        \n",
    "        epsilon = 1.0\n",
    "        for episode in episodes:\n",
    "            epsilon_values.append(epsilon)\n",
    "            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "        \n",
    "        ax.plot(episodes, epsilon_values, linewidth=3, color='red')\n",
    "        ax.fill_between(episodes, epsilon_values, alpha=0.3, color='red')\n",
    "        \n",
    "        ax.set_title('ε-Greedy Exploration Schedule')\n",
    "        ax.set_xlabel('Training Episodes')\n",
    "        ax.set_ylabel('Epsilon Value')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_ylim(0, 1.1)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def demonstrate_overestimation_bias(self):\n",
    "        \"\"\"Demonstrate the overestimation bias problem\"\"\"\n",
    "        \n",
    "        # Simulate true Q-values and noisy estimates\n",
    "        np.random.seed(42)\n",
    "        true_q_values = np.array([1.0, 2.0, 1.5, 0.8, 2.2])\n",
    "        noise_std = 0.5\n",
    "        num_estimates = 1000\n",
    "        \n",
    "        # Generate noisy estimates\n",
    "        estimates = []\n",
    "        max_estimates = []\n",
    "        \n",
    "        for _ in range(num_estimates):\n",
    "            noisy_q = true_q_values + np.random.normal(0, noise_std, len(true_q_values))\n",
    "            estimates.append(noisy_q)\n",
    "            max_estimates.append(np.max(noisy_q))\n",
    "        \n",
    "        estimates = np.array(estimates)\n",
    "        \n",
    "        # Plot results\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Distribution of max Q-values\n",
    "        ax = axes[0]\n",
    "        ax.hist(max_estimates, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax.axvline(np.max(true_q_values), color='red', linestyle='--', linewidth=2, \n",
    "                  label=f'True Max: {np.max(true_q_values):.2f}')\n",
    "        ax.axvline(np.mean(max_estimates), color='green', linestyle='--', linewidth=2,\n",
    "                  label=f'Estimated Max: {np.mean(max_estimates):.2f}')\n",
    "        \n",
    "        ax.set_title('Overestimation Bias in Max Q-Values')\n",
    "        ax.set_xlabel('Max Q-Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-value distributions for each action\n",
    "        ax = axes[1]\n",
    "        positions = np.arange(len(true_q_values))\n",
    "        \n",
    "        violin_parts = ax.violinplot([estimates[:, i] for i in range(len(true_q_values))], \n",
    "                                    positions=positions, showmeans=True, showmedians=True)\n",
    "        \n",
    "        # Plot true values\n",
    "        ax.scatter(positions, true_q_values, color='red', s=100, zorder=10, \n",
    "                  label='True Q-Values', marker='D')\n",
    "        \n",
    "        ax.set_title('Q-Value Distributions with Noise')\n",
    "        ax.set_xlabel('Actions')\n",
    "        ax.set_ylabel('Q-Values')\n",
    "        ax.set_xticks(positions)\n",
    "        ax.set_xticklabels([f'A{i}' for i in range(len(true_q_values))])\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate and print bias\n",
    "        bias = np.mean(max_estimates) - np.max(true_q_values)\n",
    "        print(f\"Overestimation Bias: {bias:.3f}\")\n",
    "        print(f\"True Maximum Q-Value: {np.max(true_q_values):.3f}\")\n",
    "        print(f\"Average Estimated Maximum: {np.mean(max_estimates):.3f}\")\n",
    "\n",
    "# Create visualization instance and demonstrate concepts\n",
    "visualizer = QNetworkVisualization()\n",
    "\n",
    "print(\"1. Visualizing Core Q-Learning Concepts...\")\n",
    "visualizer.visualize_q_learning_concepts()\n",
    "\n",
    "print(\"\\n2. Demonstrating Overestimation Bias...\")\n",
    "visualizer.demonstrate_overestimation_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f500ed",
   "metadata": {},
   "source": [
    "# Section 2: Basic DQN Implementation and Core Concepts\n",
    "\n",
    "## 2.1 Deep Q-Network Architecture\n",
    "\n",
    "The DQN architecture typically consists of:\n",
    "\n",
    "1. **Input Layer**: Processes the state representation (e.g., raw pixels or feature vectors)\n",
    "2. **Hidden Layers**: Fully connected or convolutional layers for feature extraction\n",
    "3. **Output Layer**: Outputs Q-values for all possible actions\n",
    "\n",
    "### Key Design Principles:\n",
    "\n",
    "- **State Preprocessing**: Normalize inputs for stable training\n",
    "- **Network Depth**: Balance between expressiveness and training stability\n",
    "- **Activation Functions**: ReLU is commonly used for hidden layers\n",
    "- **Output Layer**: Linear activation for Q-value regression\n",
    "\n",
    "## 2.2 Experience Replay Buffer\n",
    "\n",
    "The replay buffer serves several critical functions:\n",
    "\n",
    "1. **Decorrelation**: Breaks temporal correlations in sequential data\n",
    "2. **Sample Efficiency**: Allows multiple updates from the same experience\n",
    "3. **Stability**: Provides more stable gradients through diverse batches\n",
    "\n",
    "### Buffer Operations:\n",
    "- **Store**: Add new experiences\n",
    "- **Sample**: Randomly sample batches for training\n",
    "- **Update**: Maintain buffer size limits\n",
    "\n",
    "## 2.3 Training Loop and Key Components\n",
    "\n",
    "The DQN training process involves:\n",
    "\n",
    "1. **Action Selection**: ε-greedy exploration strategy\n",
    "2. **Environment Interaction**: Execute actions and collect experiences\n",
    "3. **Experience Storage**: Add experiences to replay buffer\n",
    "4. **Network Updates**: Sample batches and perform gradient descent\n",
    "5. **Target Network Updates**: Periodic synchronization for stability\n",
    "\n",
    "Let's implement these core components step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Basic DQN Implementation\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Deep Q-Network implementation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256]):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Build network layers\n",
    "        layers = []\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        layers.append(nn.Linear(prev_dim, action_dim))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using Xavier uniform initialization\n",
    "        for layer in self.network:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_uniform_(layer.weight)\n",
    "                nn.init.constant_(layer.bias, 0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass to compute Q-values\n",
    "        \n",
    "        Args:\n",
    "            state: Batch of states [batch_size, state_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Q-values for all actions [batch_size, action_dim]\n",
    "        \"\"\"\n",
    "        return self.network(state)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        \"\"\"\n",
    "        Select action using epsilon-greedy policy\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            epsilon: Exploration probability\n",
    "            \n",
    "        Returns:\n",
    "            Selected action (int)\n",
    "        \"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                q_values = self.forward(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Experience Replay Buffer for storing and sampling experiences\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity=100000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store an experience\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor([e.state for e in batch]).to(device)\n",
    "        actions = torch.LongTensor([e.action for e in batch]).to(device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in batch]).to(device)\n",
    "        next_states = torch.FloatTensor([e.next_state for e in batch]).to(device)\n",
    "        dones = torch.BoolTensor([e.done for e in batch]).to(device)\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    Deep Q-Network Agent\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, \n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n",
    "                 buffer_size=100000, batch_size=64, target_update_freq=1000):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = DQN(state_dim, action_dim).to(device)\n",
    "        self.target_network = DQN(state_dim, action_dim).to(device)\n",
    "        \n",
    "        # Initialize target network with same weights\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "        # Training metrics\n",
    "        self.training_step = 0\n",
    "        self.episode_rewards = []\n",
    "        self.losses = []\n",
    "        self.q_values_history = []\n",
    "        self.epsilon_history = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        return self.q_network.get_action(state, self.epsilon)\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in replay buffer\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Update target network with main network weights\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Perform one training step\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Next Q-values from target network\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.training_step += 1\n",
    "        if self.training_step % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        # Store metrics\n",
    "        self.losses.append(loss.item())\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "        \n",
    "        # Store average Q-values for monitoring\n",
    "        with torch.no_grad():\n",
    "            avg_q_value = current_q_values.mean().item()\n",
    "            self.q_values_history.append(avg_q_value)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def train_episode(self, env, max_steps=1000):\n",
    "        \"\"\"Train for one episode\"\"\"\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select and execute action\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Store experience\n",
    "            self.store_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train\n",
    "            loss = self.train_step()\n",
    "            \n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        return episode_reward, step_count\n",
    "    \n",
    "    def evaluate(self, env, num_episodes=10, render=False):\n",
    "        \"\"\"Evaluate the agent\"\"\"\n",
    "        eval_rewards = []\n",
    "        \n",
    "        # Temporarily disable exploration\n",
    "        original_epsilon = self.epsilon\n",
    "        self.epsilon = 0.0\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            while True:\n",
    "                action = self.select_action(state)\n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            \n",
    "            eval_rewards.append(episode_reward)\n",
    "        \n",
    "        # Restore original epsilon\n",
    "        self.epsilon = original_epsilon\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': np.mean(eval_rewards),\n",
    "            'std_reward': np.std(eval_rewards),\n",
    "            'min_reward': np.min(eval_rewards),\n",
    "            'max_reward': np.max(eval_rewards)\n",
    "        }\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"Get Q-values for a given state\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            return self.q_network(state_tensor).cpu().numpy().flatten()\n",
    "\n",
    "def test_basic_dqn():\n",
    "    \"\"\"Test basic DQN implementation\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Testing Basic DQN Implementation\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    print(f\"Environment: CartPole-v1\")\n",
    "    print(f\"State dimension: {state_dim}\")\n",
    "    print(f\"Action dimension: {action_dim}\")\n",
    "    \n",
    "    # Create DQN agent\n",
    "    agent = DQNAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        lr=1e-3,\n",
    "        gamma=0.99,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.01,\n",
    "        epsilon_decay=0.995,\n",
    "        buffer_size=10000,\n",
    "        batch_size=64,\n",
    "        target_update_freq=100\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    num_episodes = 200\n",
    "    print(f\"\\nTraining for {num_episodes} episodes...\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        reward, steps = agent.train_episode(env)\n",
    "        episode_rewards.append(reward)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            eval_results = agent.evaluate(env, num_episodes=5)\n",
    "            print(f\"Episode {episode+1:3d} | \"\n",
    "                  f\"Train Reward: {reward:6.1f} | \"\n",
    "                  f\"Eval Reward: {eval_results['mean_reward']:6.1f} ± {eval_results['std_reward']:4.1f} | \"\n",
    "                  f\"Epsilon: {agent.epsilon:.3f} | \"\n",
    "                  f\"Buffer Size: {len(agent.replay_buffer)}\")\n",
    "    \n",
    "    # Final evaluation\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"Final Evaluation\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    final_eval = agent.evaluate(env, num_episodes=20)\n",
    "    print(f\"Mean Reward: {final_eval['mean_reward']:.2f} ± {final_eval['std_reward']:.2f}\")\n",
    "    print(f\"Min Reward: {final_eval['min_reward']:.2f}\")\n",
    "    print(f\"Max Reward: {final_eval['max_reward']:.2f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Learning curve\n",
    "    window = 10\n",
    "    smoothed_rewards = pd.Series(episode_rewards).rolling(window).mean()\n",
    "    axes[0, 0].plot(episode_rewards, alpha=0.3, color='blue', label='Episode Rewards')\n",
    "    axes[0, 0].plot(smoothed_rewards, color='red', linewidth=2, label=f'Moving Average ({window})')\n",
    "    axes[0, 0].set_title('Learning Curve')\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Episode Reward')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss curve\n",
    "    if agent.losses:\n",
    "        loss_window = 50\n",
    "        smoothed_losses = pd.Series(agent.losses).rolling(loss_window).mean()\n",
    "        axes[0, 1].plot(agent.losses, alpha=0.3, color='orange', label='Training Loss')\n",
    "        axes[0, 1].plot(smoothed_losses, color='red', linewidth=2, label=f'Moving Average ({loss_window})')\n",
    "        axes[0, 1].set_title('Training Loss')\n",
    "        axes[0, 1].set_xlabel('Training Step')\n",
    "        axes[0, 1].set_ylabel('MSE Loss')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Epsilon decay\n",
    "    axes[1, 0].plot(agent.epsilon_history)\n",
    "    axes[1, 0].set_title('Epsilon Decay')\n",
    "    axes[1, 0].set_xlabel('Training Step')\n",
    "    axes[1, 0].set_ylabel('Epsilon')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Q-values evolution\n",
    "    if agent.q_values_history:\n",
    "        q_window = 50\n",
    "        smoothed_q = pd.Series(agent.q_values_history).rolling(q_window).mean()\n",
    "        axes[1, 1].plot(agent.q_values_history, alpha=0.3, color='green', label='Q-Values')\n",
    "        axes[1, 1].plot(smoothed_q, color='red', linewidth=2, label=f'Moving Average ({q_window})')\n",
    "        axes[1, 1].set_title('Q-Values Evolution')\n",
    "        axes[1, 1].set_xlabel('Training Step')\n",
    "        axes[1, 1].set_ylabel('Average Q-Value')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    env.close()\n",
    "    return agent, episode_rewards\n",
    "\n",
    "# Test the basic DQN implementation\n",
    "basic_agent, basic_rewards = test_basic_dqn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054e6ec",
   "metadata": {},
   "source": [
    "# Section 3: Experience Replay and Target Networks - Deep Dive\n",
    "\n",
    "## 3.1 Experience Replay: Breaking the Correlation Chain\n",
    "\n",
    "Experience replay is one of the most crucial innovations in DQN. It addresses several fundamental challenges in deep reinforcement learning:\n",
    "\n",
    "### Problems with Sequential Training\n",
    "1. **Temporal Correlation**: Consecutive states are highly correlated\n",
    "2. **Non-stationarity**: The data distribution changes as the policy evolves\n",
    "3. **Sample Inefficiency**: Each experience is used only once\n",
    "4. **Catastrophic Forgetting**: New experiences can overwrite important past learning\n",
    "\n",
    "### Benefits of Experience Replay\n",
    "1. **Decorrelation**: Random sampling breaks temporal dependencies\n",
    "2. **Sample Efficiency**: Multiple learning updates from each experience\n",
    "3. **Stability**: More stable gradients from diverse batches\n",
    "4. **Better Generalization**: Exposure to wider range of state-action pairs\n",
    "\n",
    "## 3.2 Target Networks: Stabilizing the Moving Target\n",
    "\n",
    "The target network addresses the moving target problem in Q-learning:\n",
    "\n",
    "### The Problem\n",
    "In standard Q-learning, both the predicted Q-value and the target Q-value are computed using the same network, creating instability:\n",
    "- As we update Q(s,a), the target for the next state Q(s',a') also changes\n",
    "- This can lead to oscillations and divergence\n",
    "\n",
    "### The Solution\n",
    "- Maintain two networks: main (online) and target\n",
    "- Use target network to compute stable targets\n",
    "- Update target network less frequently than main network\n",
    "\n",
    "### Update Strategies\n",
    "1. **Hard Updates**: Periodic copying of main network weights\n",
    "2. **Soft Updates**: Gradual blending with momentum (used in DDPG)\n",
    "\n",
    "## 3.3 Mathematical Analysis\n",
    "\n",
    "Let's analyze the impact of these components on learning stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cac2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay and Target Network Analysis\n",
    "\n",
    "class ExperienceReplayAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of experience replay and target networks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "    \n",
    "    def compare_replay_strategies(self):\n",
    "        \"\"\"Compare different experience replay strategies\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Experience Replay Strategy Comparison\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Environment setup\n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Test different strategies\n",
    "        strategies = {\n",
    "            'No Replay': {'buffer_size': 1, 'batch_size': 1},\n",
    "            'Small Buffer': {'buffer_size': 1000, 'batch_size': 32},\n",
    "            'Large Buffer': {'buffer_size': 50000, 'batch_size': 64},\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        num_episodes = 100\n",
    "        \n",
    "        for strategy_name, config in strategies.items():\n",
    "            print(f\"\\nTesting {strategy_name}...\")\n",
    "            \n",
    "            # Create agent with specific configuration\n",
    "            agent = DQNAgent(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                buffer_size=config['buffer_size'],\n",
    "                batch_size=config['batch_size'],\n",
    "                lr=1e-3,\n",
    "                epsilon_decay=0.99,\n",
    "                target_update_freq=100\n",
    "            )\n",
    "            \n",
    "            episode_rewards = []\n",
    "            losses = []\n",
    "            \n",
    "            for episode in range(num_episodes):\n",
    "                reward, _ = agent.train_episode(env, max_steps=500)\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                # Collect recent losses\n",
    "                if len(agent.losses) > len(losses):\n",
    "                    losses.extend(agent.losses[len(losses):])\n",
    "                \n",
    "                if (episode + 1) % 25 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-25:])\n",
    "                    print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n",
    "            \n",
    "            results[strategy_name] = {\n",
    "                'rewards': episode_rewards,\n",
    "                'losses': losses,\n",
    "                'final_performance': np.mean(episode_rewards[-20:])\n",
    "            }\n",
    "        \n",
    "        self.results['replay_comparison'] = results\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "        \n",
    "        # Learning curves\n",
    "        ax = axes[0]\n",
    "        colors = ['red', 'blue', 'green']\n",
    "        \n",
    "        for i, (strategy, data) in enumerate(results.items()):\n",
    "            rewards = data['rewards']\n",
    "            smoothed = pd.Series(rewards).rolling(10).mean()\n",
    "            ax.plot(smoothed, label=strategy, color=colors[i], linewidth=2)\n",
    "            ax.fill_between(range(len(smoothed)), smoothed, alpha=0.3, color=colors[i])\n",
    "        \n",
    "        ax.set_title('Learning Curves by Replay Strategy')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Episode Reward (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Final performance comparison\n",
    "        ax = axes[1]\n",
    "        strategies_list = list(results.keys())\n",
    "        final_perfs = [results[s]['final_performance'] for s in strategies_list]\n",
    "        \n",
    "        bars = ax.bar(strategies_list, final_perfs, alpha=0.7, color=colors)\n",
    "        ax.set_title('Final Performance Comparison')\n",
    "        ax.set_ylabel('Average Reward (Last 20 Episodes)')\n",
    "        ax.set_xticklabels(strategies_list, rotation=45)\n",
    "        \n",
    "        for bar, perf in zip(bars, final_perfs):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                   f'{perf:.1f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Loss comparison (if available)\n",
    "        ax = axes[2]\n",
    "        for i, (strategy, data) in enumerate(results.items()):\n",
    "            if len(data['losses']) > 10:\n",
    "                losses = data['losses']\n",
    "                smoothed_losses = pd.Series(losses).rolling(50).mean()\n",
    "                ax.plot(smoothed_losses, label=strategy, color=colors[i], linewidth=2, alpha=0.7)\n",
    "        \n",
    "        ax.set_title('Training Loss Comparison')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('MSE Loss (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        env.close()\n",
    "        return results\n",
    "    \n",
    "    def analyze_target_network_frequency(self):\n",
    "        \"\"\"Analyze the impact of target network update frequency\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Target Network Update Frequency Analysis\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Test different update frequencies\n",
    "        frequencies = {\n",
    "            'Very Frequent (10)': 10,\n",
    "            'Frequent (100)': 100,\n",
    "            'Moderate (500)': 500,\n",
    "            'Infrequent (1000)': 1000,\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        num_episodes = 150\n",
    "        \n",
    "        for freq_name, freq_value in frequencies.items():\n",
    "            print(f\"\\nTesting {freq_name}...\")\n",
    "            \n",
    "            agent = DQNAgent(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                target_update_freq=freq_value,\n",
    "                lr=1e-3,\n",
    "                epsilon_decay=0.99\n",
    "            )\n",
    "            \n",
    "            episode_rewards = []\n",
    "            q_value_stds = []  # Track Q-value stability\n",
    "            \n",
    "            for episode in range(num_episodes):\n",
    "                reward, _ = agent.train_episode(env, max_steps=500)\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                # Periodically check Q-value stability\n",
    "                if episode % 10 == 0 and len(agent.replay_buffer) > 1000:\n",
    "                    # Sample some states and compute Q-value standard deviation\n",
    "                    sample_states, _, _, _, _ = agent.replay_buffer.sample(100)\n",
    "                    with torch.no_grad():\n",
    "                        q_vals = agent.q_network(sample_states)\n",
    "                        q_std = q_vals.std().item()\n",
    "                        q_value_stds.append(q_std)\n",
    "                \n",
    "                if (episode + 1) % 50 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-25:])\n",
    "                    print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n",
    "            \n",
    "            results[freq_name] = {\n",
    "                'rewards': episode_rewards,\n",
    "                'q_stds': q_value_stds,\n",
    "                'final_performance': np.mean(episode_rewards[-20:])\n",
    "            }\n",
    "        \n",
    "        self.results['target_frequency'] = results\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Learning curves\n",
    "        ax = axes[0]\n",
    "        colors = ['purple', 'blue', 'green', 'orange']\n",
    "        \n",
    "        for i, (freq_name, data) in enumerate(results.items()):\n",
    "            rewards = data['rewards']\n",
    "            smoothed = pd.Series(rewards).rolling(10).mean()\n",
    "            ax.plot(smoothed, label=freq_name, color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('Learning Curves by Target Update Frequency')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Episode Reward (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Q-value stability\n",
    "        ax = axes[1]\n",
    "        for i, (freq_name, data) in enumerate(results.items()):\n",
    "            if data['q_stds']:\n",
    "                episodes_for_q = np.arange(0, len(data['q_stds'])) * 10\n",
    "                ax.plot(episodes_for_q, data['q_stds'], \n",
    "                       label=freq_name, color=colors[i], linewidth=2, marker='o')\n",
    "        \n",
    "        ax.set_title('Q-Value Stability Over Training')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Q-Value Standard Deviation')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        env.close()\n",
    "        return results\n",
    "    \n",
    "    def demonstrate_replay_buffer_analysis(self):\n",
    "        \"\"\"Analyze the content and dynamics of the replay buffer\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Replay Buffer Content Analysis\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Create agent and train for some episodes\n",
    "        agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, buffer_size=5000)\n",
    "        \n",
    "        # Train for a few episodes to fill buffer\n",
    "        for episode in range(50):\n",
    "            agent.train_episode(env, max_steps=500)\n",
    "        \n",
    "        # Analyze buffer content\n",
    "        buffer_size = len(agent.replay_buffer)\n",
    "        print(f\"Buffer size after training: {buffer_size}\")\n",
    "        \n",
    "        # Sample all experiences for analysis\n",
    "        if buffer_size > 100:\n",
    "            # Get all experiences\n",
    "            all_states = []\n",
    "            all_rewards = []\n",
    "            all_actions = []\n",
    "            all_dones = []\n",
    "            \n",
    "            for experience in agent.replay_buffer.buffer:\n",
    "                all_states.append(experience.state)\n",
    "                all_rewards.append(experience.reward)\n",
    "                all_actions.append(experience.action)\n",
    "                all_dones.append(experience.done)\n",
    "            \n",
    "            all_states = np.array(all_states)\n",
    "            all_rewards = np.array(all_rewards)\n",
    "            all_actions = np.array(all_actions)\n",
    "            all_dones = np.array(all_dones)\n",
    "            \n",
    "            # Analysis and visualization\n",
    "            fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "            \n",
    "            # State distribution analysis\n",
    "            for i in range(min(4, state_dim)):\n",
    "                ax = axes[0, i] if i < 3 else axes[1, 0]\n",
    "                ax.hist(all_states[:, i], bins=30, alpha=0.7, edgecolor='black')\n",
    "                ax.set_title(f'State Dimension {i} Distribution')\n",
    "                ax.set_xlabel(f'State[{i}]')\n",
    "                ax.set_ylabel('Frequency')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Reward distribution\n",
    "            if len(axes[0]) > 2:\n",
    "                ax = axes[0, 2]\n",
    "            else:\n",
    "                ax = axes[1, 1]\n",
    "            ax.hist(all_rewards, bins=np.unique(all_rewards), alpha=0.7, edgecolor='black')\n",
    "            ax.set_title('Reward Distribution')\n",
    "            ax.set_xlabel('Reward')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Action distribution\n",
    "            ax = axes[1, 1] if len(axes[0]) > 2 else axes[1, 2]\n",
    "            action_counts = np.bincount(all_actions)\n",
    "            ax.bar(range(len(action_counts)), action_counts, alpha=0.7)\n",
    "            ax.set_title('Action Distribution')\n",
    "            ax.set_xlabel('Action')\n",
    "            ax.set_ylabel('Frequency')\n",
    "            ax.set_xticks(range(action_dim))\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Episode termination analysis\n",
    "            ax = axes[1, 2]\n",
    "            done_ratio = np.mean(all_dones)\n",
    "            ax.pie([done_ratio, 1-done_ratio], \n",
    "                  labels=[f'Terminal ({done_ratio:.1%})', f'Non-terminal ({1-done_ratio:.1%})'],\n",
    "                  autopct='%1.1f%%', startangle=90)\n",
    "            ax.set_title('Terminal vs Non-terminal States')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print statistics\n",
    "            print(f\"\\nBuffer Statistics:\")\n",
    "            print(f\"  Total experiences: {buffer_size}\")\n",
    "            print(f\"  Reward range: [{np.min(all_rewards):.2f}, {np.max(all_rewards):.2f}]\")\n",
    "            print(f\"  Average reward: {np.mean(all_rewards):.2f}\")\n",
    "            print(f\"  Action distribution: {dict(zip(range(action_dim), action_counts))}\")\n",
    "            print(f\"  Terminal state ratio: {done_ratio:.1%}\")\n",
    "            \n",
    "            # State correlation analysis\n",
    "            if state_dim >= 2:\n",
    "                print(f\"\\nState Correlation Analysis:\")\n",
    "                state_corr = np.corrcoef(all_states.T)\n",
    "                \n",
    "                plt.figure(figsize=(8, 6))\n",
    "                sns.heatmap(state_corr, annot=True, cmap='coolwarm', center=0,\n",
    "                           xticklabels=[f'State[{i}]' for i in range(state_dim)],\n",
    "                           yticklabels=[f'State[{i}]' for i in range(state_dim)])\n",
    "                plt.title('State Dimension Correlations in Replay Buffer')\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        \n",
    "        env.close()\n",
    "        return agent\n",
    "\n",
    "# Run comprehensive experience replay analysis\n",
    "analyzer = ExperienceReplayAnalyzer()\n",
    "\n",
    "print(\"1. Comparing Replay Strategies...\")\n",
    "replay_results = analyzer.compare_replay_strategies()\n",
    "\n",
    "print(\"\\n2. Analyzing Target Network Update Frequency...\")\n",
    "target_results = analyzer.analyze_target_network_frequency()\n",
    "\n",
    "print(\"\\n3. Analyzing Replay Buffer Content...\")\n",
    "buffer_agent = analyzer.demonstrate_replay_buffer_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf888c25",
   "metadata": {},
   "source": [
    "# Section 4: Double DQN and Overestimation Bias\n",
    "\n",
    "## 4.1 The Overestimation Problem in Q-Learning\n",
    "\n",
    "Standard Q-learning suffers from a systematic overestimation bias due to the max operator in the Bellman equation. This problem is amplified in function approximation settings.\n",
    "\n",
    "### Mathematical Analysis of Overestimation Bias\n",
    "\n",
    "In standard DQN, the target is computed as:\n",
    "```\n",
    "y = r + γ max Q(s', a'; θ⁻)\n",
    "          a'\n",
    "```\n",
    "\n",
    "The issue arises because:\n",
    "1. We use the same network to both **select** the action and **evaluate** it\n",
    "2. Noise in Q-value estimates leads to overestimation when taking the maximum\n",
    "3. This bias propagates through the Bellman updates\n",
    "\n",
    "### Impact on Learning\n",
    "- **Suboptimal Policies**: Overestimated Q-values can lead to poor action selection\n",
    "- **Instability**: Inconsistent value estimates cause training instability  \n",
    "- **Slow Convergence**: Biased estimates slow down learning\n",
    "\n",
    "## 4.2 Double DQN Solution\n",
    "\n",
    "Double DQN addresses this by **decoupling action selection from action evaluation**:\n",
    "\n",
    "### Key Insight\n",
    "Use the main network to select actions, but the target network to evaluate them:\n",
    "\n",
    "```\n",
    "y = r + γ Q(s', argmax Q(s', a'; θ), θ⁻)\n",
    "              a'\n",
    "```\n",
    "\n",
    "### Algorithm Steps\n",
    "1. **Action Selection**: Use main network to find the best action in next state\n",
    "2. **Action Evaluation**: Use target network to evaluate that action\n",
    "3. **Update**: Compute loss and update main network\n",
    "\n",
    "### Benefits\n",
    "- **Reduced Bias**: Eliminates the correlation between selection and evaluation\n",
    "- **Better Stability**: More consistent Q-value estimates\n",
    "- **Improved Performance**: Often leads to better final policies\n",
    "\n",
    "## 4.3 Implementation Details\n",
    "\n",
    "The modification to standard DQN is minimal but effective:\n",
    "- Only changes the target computation\n",
    "- No additional computational overhead\n",
    "- Compatible with other DQN improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef939ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Double DQN Implementation\n",
    "\n",
    "class DoubleDQNAgent(DQNAgent):\n",
    "    \"\"\"\n",
    "    Double DQN Agent - Extends basic DQN to address overestimation bias\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, **kwargs):\n",
    "        super().__init__(state_dim, action_dim, **kwargs)\n",
    "        \n",
    "        # Additional tracking for bias analysis\n",
    "        self.q_value_estimates = {'main': [], 'target': [], 'double': []}\n",
    "        self.overestimation_metrics = []\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Double DQN training step with bias tracking\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Double DQN target computation\n",
    "        with torch.no_grad():\n",
    "            # Use main network to select actions\n",
    "            next_actions = self.q_network(next_states).argmax(1)\n",
    "            \n",
    "            # Use target network to evaluate selected actions\n",
    "            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Compute targets\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "            \n",
    "            # For comparison: also compute standard DQN targets\n",
    "            standard_next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            standard_targets = rewards + (self.gamma * standard_next_q_values * (~dones))\n",
    "            \n",
    "            # Track bias metrics\n",
    "            self.track_bias_metrics(current_q_values, target_q_values, standard_targets)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.training_step += 1\n",
    "        if self.training_step % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        # Store metrics\n",
    "        self.losses.append(loss.item())\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            avg_q_value = current_q_values.mean().item()\n",
    "            self.q_values_history.append(avg_q_value)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def track_bias_metrics(self, current_q, double_targets, standard_targets):\n",
    "        \"\"\"Track overestimation bias metrics\"\"\"\n",
    "        \n",
    "        # Store average values for analysis\n",
    "        self.q_value_estimates['main'].append(current_q.mean().item())\n",
    "        self.q_value_estimates['double'].append(double_targets.mean().item())\n",
    "        \n",
    "        # Compute overestimation (standard DQN targets - Double DQN targets)\n",
    "        overestimation = (standard_targets - double_targets).mean().item()\n",
    "        self.overestimation_metrics.append(overestimation)\n",
    "\n",
    "class OverestimationAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of overestimation bias in DQN variants\n",
    "    \"\"\"\n",
    "    \n",
    "    def compare_dqn_variants(self):\n",
    "        \"\"\"Compare standard DQN vs Double DQN\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"DQN vs Double DQN Comparison\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Test both variants\n",
    "        variants = {\n",
    "            'Standard DQN': DQNAgent,\n",
    "            'Double DQN': DoubleDQNAgent\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        num_episodes = 150\n",
    "        \n",
    "        for variant_name, agent_class in variants.items():\n",
    "            print(f\"\\nTraining {variant_name}...\")\n",
    "            \n",
    "            # Create agent\n",
    "            agent = agent_class(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                lr=1e-3,\n",
    "                epsilon_decay=0.995,\n",
    "                target_update_freq=100,\n",
    "                buffer_size=20000\n",
    "            )\n",
    "            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            for episode in range(num_episodes):\n",
    "                reward, _ = agent.train_episode(env, max_steps=500)\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if (episode + 1) % 50 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-25:])\n",
    "                    print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n",
    "            \n",
    "            # Final evaluation\n",
    "            eval_results = agent.evaluate(env, num_episodes=20)\n",
    "            \n",
    "            results[variant_name] = {\n",
    "                'agent': agent,\n",
    "                'rewards': episode_rewards,\n",
    "                'eval_performance': eval_results,\n",
    "                'final_performance': np.mean(episode_rewards[-20:])\n",
    "            }\n",
    "        \n",
    "        # Visualization and Analysis\n",
    "        self.visualize_comparison(results)\n",
    "        \n",
    "        env.close()\n",
    "        return results\n",
    "    \n",
    "    def visualize_comparison(self, results):\n",
    "        \"\"\"Visualize comparison between DQN variants\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        colors = ['blue', 'red']\n",
    "        \n",
    "        # 1. Learning curves\n",
    "        ax = axes[0, 0]\n",
    "        for i, (variant, data) in enumerate(results.items()):\n",
    "            rewards = data['rewards']\n",
    "            smoothed = pd.Series(rewards).rolling(10).mean()\n",
    "            ax.plot(smoothed, label=variant, color=colors[i], linewidth=2)\n",
    "            ax.fill_between(range(len(smoothed)), smoothed, alpha=0.3, color=colors[i])\n",
    "        \n",
    "        ax.set_title('Learning Curves Comparison')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Episode Reward (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Final performance comparison\n",
    "        ax = axes[0, 1]\n",
    "        variant_names = list(results.keys())\n",
    "        final_perfs = [results[v]['final_performance'] for v in variant_names]\n",
    "        eval_means = [results[v]['eval_performance']['mean_reward'] for v in variant_names]\n",
    "        eval_stds = [results[v]['eval_performance']['std_reward'] for v in variant_names]\n",
    "        \n",
    "        x = np.arange(len(variant_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, final_perfs, width, label='Training Performance', \n",
    "               alpha=0.7, color=colors)\n",
    "        ax.bar(x + width/2, eval_means, width, yerr=eval_stds, \n",
    "               label='Evaluation Performance', alpha=0.7, color=[c for c in colors])\n",
    "        \n",
    "        ax.set_title('Performance Comparison')\n",
    "        ax.set_ylabel('Average Reward')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(variant_names)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Q-value evolution (if available)\n",
    "        ax = axes[0, 2]\n",
    "        for i, (variant, data) in enumerate(results.items()):\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'q_values_history') and agent.q_values_history:\n",
    "                smoothed_q = pd.Series(agent.q_values_history).rolling(50).mean()\n",
    "                ax.plot(smoothed_q, label=f'{variant} Q-values', \n",
    "                       color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('Q-Value Evolution')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Average Q-Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Overestimation bias analysis (if available)\n",
    "        ax = axes[1, 0]\n",
    "        double_dqn_agent = None\n",
    "        \n",
    "        for variant, data in results.items():\n",
    "            if 'Double' in variant:\n",
    "                double_dqn_agent = data['agent']\n",
    "                break\n",
    "        \n",
    "        if double_dqn_agent and hasattr(double_dqn_agent, 'overestimation_metrics'):\n",
    "            if double_dqn_agent.overestimation_metrics:\n",
    "                overest_smooth = pd.Series(double_dqn_agent.overestimation_metrics).rolling(50).mean()\n",
    "                ax.plot(overest_smooth, color='purple', linewidth=2)\n",
    "                ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "                ax.fill_between(range(len(overest_smooth)), overest_smooth, 0, alpha=0.3, color='purple')\n",
    "        \n",
    "        ax.set_title('Overestimation Bias Over Training')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Overestimation (Standard - Double)')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Loss comparison\n",
    "        ax = axes[1, 1]\n",
    "        for i, (variant, data) in enumerate(results.items()):\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'losses') and agent.losses:\n",
    "                losses = agent.losses\n",
    "                if len(losses) > 50:\n",
    "                    smoothed_loss = pd.Series(losses).rolling(50).mean()\n",
    "                    ax.plot(smoothed_loss, label=f'{variant} Loss', \n",
    "                           color=colors[i], linewidth=2, alpha=0.7)\n",
    "        \n",
    "        ax.set_title('Training Loss Comparison')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('MSE Loss (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Action-value analysis\n",
    "        ax = axes[1, 2]\n",
    "        \n",
    "        # Sample some states and analyze Q-values\n",
    "        sample_state = [0.1, 0.1, 0.1, 0.1]  # Example CartPole state\n",
    "        \n",
    "        q_values_comparison = {}\n",
    "        for variant, data in results.items():\n",
    "            agent = data['agent']\n",
    "            q_vals = agent.get_q_values(sample_state)\n",
    "            q_values_comparison[variant] = q_vals\n",
    "        \n",
    "        if q_values_comparison:\n",
    "            x = np.arange(len(q_vals))\n",
    "            width = 0.35\n",
    "            \n",
    "            for i, (variant, q_vals) in enumerate(q_values_comparison.items()):\n",
    "                ax.bar(x + i * width, q_vals, width, label=variant, \n",
    "                      alpha=0.7, color=colors[i])\n",
    "            \n",
    "            ax.set_title('Q-Values for Sample State')\n",
    "            ax.set_xlabel('Actions')\n",
    "            ax.set_ylabel('Q-Value')\n",
    "            ax.set_xticks(x + width/2)\n",
    "            ax.set_xticklabels([f'Action {i}' for i in range(len(q_vals))])\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_overestimation_in_depth(self):\n",
    "        \"\"\"Deep dive into overestimation bias\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Deep Analysis of Overestimation Bias\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Create a controlled environment to study overestimation\n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Train Double DQN to track bias\n",
    "        agent = DoubleDQNAgent(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            lr=1e-3,\n",
    "            epsilon_decay=0.995,\n",
    "            buffer_size=10000\n",
    "        )\n",
    "        \n",
    "        print(\"Training Double DQN to analyze overestimation...\")\n",
    "        \n",
    "        for episode in range(100):\n",
    "            agent.train_episode(env, max_steps=500)\n",
    "            \n",
    "            if (episode + 1) % 25 == 0:\n",
    "                print(f\"Episode {episode+1} completed\")\n",
    "        \n",
    "        # Analyze the results\n",
    "        if agent.overestimation_metrics and agent.q_value_estimates['main']:\n",
    "            \n",
    "            plt.figure(figsize=(15, 10))\n",
    "            \n",
    "            # 1. Overestimation bias over time\n",
    "            plt.subplot(2, 2, 1)\n",
    "            overest_smooth = pd.Series(agent.overestimation_metrics).rolling(20).mean()\n",
    "            plt.plot(overest_smooth, color='red', linewidth=2)\n",
    "            plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            plt.fill_between(range(len(overest_smooth)), overest_smooth, 0, \n",
    "                           alpha=0.3, color='red')\n",
    "            plt.title('Overestimation Bias Evolution')\n",
    "            plt.xlabel('Training Step')\n",
    "            plt.ylabel('Bias (Standard DQN - Double DQN)')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 2. Q-value estimates comparison\n",
    "            plt.subplot(2, 2, 2)\n",
    "            main_q_smooth = pd.Series(agent.q_value_estimates['main']).rolling(20).mean()\n",
    "            double_q_smooth = pd.Series(agent.q_value_estimates['double']).rolling(20).mean()\n",
    "            \n",
    "            plt.plot(main_q_smooth, label='Current Q-values', color='blue', linewidth=2)\n",
    "            plt.plot(double_q_smooth, label='Double DQN Targets', color='green', linewidth=2)\n",
    "            plt.title('Q-Value Estimates Comparison')\n",
    "            plt.xlabel('Training Step')\n",
    "            plt.ylabel('Average Q-Value')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 3. Bias distribution\n",
    "            plt.subplot(2, 2, 3)\n",
    "            plt.hist(agent.overestimation_metrics, bins=30, alpha=0.7, \n",
    "                    edgecolor='black', color='orange')\n",
    "            plt.axvline(np.mean(agent.overestimation_metrics), color='red', \n",
    "                       linestyle='--', linewidth=2, label=f'Mean: {np.mean(agent.overestimation_metrics):.3f}')\n",
    "            plt.title('Overestimation Bias Distribution')\n",
    "            plt.xlabel('Bias Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # 4. Correlation analysis\n",
    "            plt.subplot(2, 2, 4)\n",
    "            if len(agent.overestimation_metrics) == len(agent.q_values_history):\n",
    "                plt.scatter(agent.q_values_history, agent.overestimation_metrics, \n",
    "                          alpha=0.6, color='purple')\n",
    "                plt.xlabel('Average Q-Value')\n",
    "                plt.ylabel('Overestimation Bias')\n",
    "                plt.title('Q-Value vs Overestimation Bias')\n",
    "                \n",
    "                # Add trend line\n",
    "                if len(agent.q_values_history) > 10:\n",
    "                    z = np.polyfit(agent.q_values_history, agent.overestimation_metrics, 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    plt.plot(sorted(agent.q_values_history), \n",
    "                            p(sorted(agent.q_values_history)), \n",
    "                            \"r--\", alpha=0.8, linewidth=2)\n",
    "                \n",
    "                plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Print statistics\n",
    "            print(f\"\\nOverestimation Bias Statistics:\")\n",
    "            print(f\"  Mean bias: {np.mean(agent.overestimation_metrics):.4f}\")\n",
    "            print(f\"  Std bias: {np.std(agent.overestimation_metrics):.4f}\")\n",
    "            print(f\"  Max bias: {np.max(agent.overestimation_metrics):.4f}\")\n",
    "            print(f\"  Min bias: {np.min(agent.overestimation_metrics):.4f}\")\n",
    "            \n",
    "            # Correlation between Q-values and bias\n",
    "            if len(agent.overestimation_metrics) == len(agent.q_values_history):\n",
    "                correlation = np.corrcoef(agent.q_values_history, agent.overestimation_metrics)[0, 1]\n",
    "                print(f\"  Correlation (Q-values vs Bias): {correlation:.4f}\")\n",
    "        \n",
    "        env.close()\n",
    "        return agent\n",
    "\n",
    "# Run overestimation bias analysis\n",
    "analyzer = OverestimationAnalyzer()\n",
    "\n",
    "print(\"1. Comparing Standard DQN vs Double DQN...\")\n",
    "comparison_results = analyzer.compare_dqn_variants()\n",
    "\n",
    "print(\"\\n2. Deep Analysis of Overestimation Bias...\")\n",
    "bias_agent = analyzer.analyze_overestimation_in_depth()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7cf2b",
   "metadata": {},
   "source": [
    "# Section 5: Dueling DQN and Value Decomposition\n",
    "\n",
    "## 5.1 The Motivation Behind Dueling Architecture\n",
    "\n",
    "Standard DQN learns Q-values directly, but these can be decomposed into two meaningful components:\n",
    "\n",
    "### Value Decomposition Theory\n",
    "The Q-function can be decomposed as:\n",
    "```\n",
    "Q(s,a) = V(s) + A(s,a)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **V(s)**: State value function - \"How good is this state?\"\n",
    "- **A(s,a)**: Advantage function - \"How much better is action a compared to average?\"\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "1. **State Value Learning**: Many states have similar values regardless of action\n",
    "2. **Action Ranking**: What matters most is the relative advantage of actions\n",
    "3. **Sample Efficiency**: Decoupling allows better generalization\n",
    "4. **Faster Learning**: State values can be learned from all experiences\n",
    "\n",
    "## 5.2 Dueling Network Architecture\n",
    "\n",
    "### Network Structure\n",
    "```\n",
    "Input State\n",
    "     |\n",
    "Feature Extraction\n",
    "     |\n",
    "   Split into two streams\n",
    "     /              \\\n",
    "Value Stream    Advantage Stream  \n",
    "   V(s)           A(s,a)\n",
    "     \\              /\n",
    "      Combining Module\n",
    "           |\n",
    "        Q(s,a)\n",
    "```\n",
    "\n",
    "### Combining the Streams\n",
    "\n",
    "The naive combination `Q(s,a) = V(s) + A(s,a)` has an identifiability problem. \n",
    "\n",
    "**Solution**: Subtract the mean advantage:\n",
    "```\n",
    "Q(s,a) = V(s) + A(s,a) - (1/|A|) Σ A(s,a')\n",
    "                                    a'\n",
    "```\n",
    "\n",
    "### Alternative Formulation\n",
    "Use max instead of mean for better performance:\n",
    "```\n",
    "Q(s,a) = V(s) + A(s,a) - max A(s,a')\n",
    "                           a'\n",
    "```\n",
    "\n",
    "## 5.3 Benefits of Dueling Architecture\n",
    "\n",
    "1. **Better Value Estimation**: State values learned more efficiently\n",
    "2. **Improved Policy**: Better action selection through advantage learning\n",
    "3. **Robustness**: More stable learning across different environments\n",
    "4. **Generalization**: Better performance on states with similar values\n",
    "\n",
    "Let's implement and analyze the Dueling DQN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Dueling DQN Implementation\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    \"\"\"\n",
    "    Dueling Deep Q-Network with separate value and advantage streams\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256], dueling_type='mean'):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.dueling_type = dueling_type\n",
    "        \n",
    "        # Shared feature extraction layers\n",
    "        self.feature_layers = nn.Sequential()\n",
    "        prev_dim = state_dim\n",
    "        \n",
    "        for i, hidden_dim in enumerate(hidden_dims[:-1]):\n",
    "            self.feature_layers.add_module(f'fc{i}', nn.Linear(prev_dim, hidden_dim))\n",
    "            self.feature_layers.add_module(f'relu{i}', nn.ReLU())\n",
    "            prev_dim = hidden_dim\n",
    "        \n",
    "        # Split into value and advantage streams\n",
    "        feature_dim = hidden_dims[-1] if hidden_dims else hidden_dims[0]\n",
    "        \n",
    "        # Value stream - outputs single value V(s)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(prev_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream - outputs advantage for each action A(s,a)\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(prev_dim, feature_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(feature_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"\n",
    "        Forward pass through dueling architecture\n",
    "        \n",
    "        Args:\n",
    "            state: Batch of states [batch_size, state_dim]\n",
    "            \n",
    "        Returns:\n",
    "            Q-values [batch_size, action_dim]\n",
    "            value: State values [batch_size, 1] (for analysis)\n",
    "            advantage: Action advantages [batch_size, action_dim] (for analysis)\n",
    "        \"\"\"\n",
    "        # Extract features\n",
    "        features = self.feature_layers(state)\n",
    "        \n",
    "        # Compute value and advantage\n",
    "        value = self.value_stream(features)  # [batch_size, 1]\n",
    "        advantage = self.advantage_stream(features)  # [batch_size, action_dim]\n",
    "        \n",
    "        # Combine to get Q-values\n",
    "        if self.dueling_type == 'mean':\n",
    "            # Subtract mean advantage\n",
    "            q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "        elif self.dueling_type == 'max':\n",
    "            # Subtract max advantage\n",
    "            q_values = value + advantage - advantage.max(dim=1, keepdim=True)[0]\n",
    "        else:\n",
    "            # Naive combination (not recommended)\n",
    "            q_values = value + advantage\n",
    "        \n",
    "        return q_values, value, advantage\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "                q_values, _, _ = self.forward(state_tensor)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "class DuelingDQNAgent(DoubleDQNAgent):\n",
    "    \"\"\"\n",
    "    Dueling DQN Agent combining Double DQN with Dueling Architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, dueling_type='mean', **kwargs):\n",
    "        # Initialize parent class first\n",
    "        self.dueling_type = dueling_type\n",
    "        super().__init__(state_dim, action_dim, **kwargs)\n",
    "        \n",
    "        # Replace networks with dueling architecture\n",
    "        self.q_network = DuelingDQN(state_dim, action_dim, dueling_type=dueling_type).to(device)\n",
    "        self.target_network = DuelingDQN(state_dim, action_dim, dueling_type=dueling_type).to(device)\n",
    "        \n",
    "        # Initialize target network\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        # Update optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=kwargs.get('lr', 1e-4))\n",
    "        \n",
    "        # Additional tracking for value/advantage analysis\n",
    "        self.value_history = []\n",
    "        self.advantage_history = []\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Dueling Double DQN training step\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch\n",
    "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q-values and decomposition\n",
    "        current_q_values, current_values, current_advantages = self.q_network(states)\n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Double DQN target computation with dueling\n",
    "        with torch.no_grad():\n",
    "            # Use main network to select actions\n",
    "            next_q_main, _, _ = self.q_network(next_states)\n",
    "            next_actions = next_q_main.argmax(1)\n",
    "            \n",
    "            # Use target network to evaluate selected actions\n",
    "            next_q_target, _, _ = self.target_network(next_states)\n",
    "            next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Compute targets\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.training_step += 1\n",
    "        if self.training_step % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        # Store metrics\n",
    "        self.losses.append(loss.item())\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "        \n",
    "        # Track Q-values, values, and advantages\n",
    "        with torch.no_grad():\n",
    "            avg_q_value = current_q_values.mean().item()\n",
    "            avg_value = current_values.mean().item()\n",
    "            avg_advantage = current_advantages.mean().item()\n",
    "            \n",
    "            self.q_values_history.append(avg_q_value)\n",
    "            self.value_history.append(avg_value)\n",
    "            self.advantage_history.append(avg_advantage)\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def get_value_advantage_decomposition(self, state):\n",
    "        \"\"\"Get value and advantage decomposition for a state\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values, value, advantage = self.q_network(state_tensor)\n",
    "            \n",
    "            return {\n",
    "                'q_values': q_values.cpu().numpy().flatten(),\n",
    "                'value': value.item(),\n",
    "                'advantage': advantage.cpu().numpy().flatten()\n",
    "            }\n",
    "\n",
    "class DuelingAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of Dueling DQN architecture\n",
    "    \"\"\"\n",
    "    \n",
    "    def compare_dueling_variants(self):\n",
    "        \"\"\"Compare different dueling combination methods\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Dueling DQN Variants Comparison\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Test different dueling types\n",
    "        variants = {\n",
    "            'Standard DQN': {'agent_class': DoubleDQNAgent, 'dueling_type': None},\n",
    "            'Dueling (Mean)': {'agent_class': DuelingDQNAgent, 'dueling_type': 'mean'},\n",
    "            'Dueling (Max)': {'agent_class': DuelingDQNAgent, 'dueling_type': 'max'},\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        num_episodes = 120\n",
    "        \n",
    "        for variant_name, config in variants.items():\n",
    "            print(f\"\\nTraining {variant_name}...\")\n",
    "            \n",
    "            # Create agent\n",
    "            agent_kwargs = {\n",
    "                'state_dim': state_dim,\n",
    "                'action_dim': action_dim,\n",
    "                'lr': 1e-3,\n",
    "                'epsilon_decay': 0.995,\n",
    "                'target_update_freq': 100,\n",
    "                'buffer_size': 15000\n",
    "            }\n",
    "            \n",
    "            if config['dueling_type']:\n",
    "                agent_kwargs['dueling_type'] = config['dueling_type']\n",
    "            \n",
    "            agent = config['agent_class'](**agent_kwargs)\n",
    "            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            for episode in range(num_episodes):\n",
    "                reward, _ = agent.train_episode(env, max_steps=500)\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if (episode + 1) % 40 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-20:])\n",
    "                    print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n",
    "            \n",
    "            # Final evaluation\n",
    "            eval_results = agent.evaluate(env, num_episodes=15)\n",
    "            \n",
    "            results[variant_name] = {\n",
    "                'agent': agent,\n",
    "                'rewards': episode_rewards,\n",
    "                'eval_performance': eval_results,\n",
    "                'final_performance': np.mean(episode_rewards[-15:])\n",
    "            }\n",
    "        \n",
    "        # Visualization\n",
    "        self.visualize_dueling_comparison(results)\n",
    "        \n",
    "        env.close()\n",
    "        return results\n",
    "    \n",
    "    def visualize_dueling_comparison(self, results):\n",
    "        \"\"\"Visualize dueling architecture comparison\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        colors = ['blue', 'red', 'green']\n",
    "        \n",
    "        # 1. Learning curves\n",
    "        ax = axes[0, 0]\n",
    "        for i, (variant, data) in enumerate(results.items()):\n",
    "            rewards = data['rewards']\n",
    "            smoothed = pd.Series(rewards).rolling(8).mean()\n",
    "            ax.plot(smoothed, label=variant, color=colors[i], linewidth=2)\n",
    "            ax.fill_between(range(len(smoothed)), smoothed, alpha=0.3, color=colors[i])\n",
    "        \n",
    "        ax.set_title('Learning Curves: Dueling Variants')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Episode Reward (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Performance comparison\n",
    "        ax = axes[0, 1]\n",
    "        variant_names = list(results.keys())\n",
    "        final_perfs = [results[v]['final_performance'] for v in variant_names]\n",
    "        eval_means = [results[v]['eval_performance']['mean_reward'] for v in variant_names]\n",
    "        eval_stds = [results[v]['eval_performance']['std_reward'] for v in variant_names]\n",
    "        \n",
    "        x = np.arange(len(variant_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, final_perfs, width, label='Training', \n",
    "               alpha=0.7, color=colors)\n",
    "        ax.bar(x + width/2, eval_means, width, yerr=eval_stds, \n",
    "               label='Evaluation', alpha=0.7, color=['darkblue', 'darkred', 'darkgreen'])\n",
    "        \n",
    "        ax.set_title('Performance Comparison')\n",
    "        ax.set_ylabel('Average Reward')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(variant_names, rotation=45)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Value/Advantage evolution (for dueling agents)\n",
    "        ax = axes[0, 2]\n",
    "        for i, (variant, data) in enumerate(results.items()):\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'value_history') and agent.value_history:\n",
    "                value_smooth = pd.Series(agent.value_history).rolling(30).mean()\n",
    "                ax.plot(value_smooth, label=f'{variant} Values', \n",
    "                       color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('State Value Evolution')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Average State Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Q-value decomposition analysis\n",
    "        ax = axes[1, 0]\n",
    "        sample_state = [0.1, 0.1, 0.1, 0.1]  # Example state\n",
    "        \n",
    "        decompositions = {}\n",
    "        for variant, data in results.items():\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'get_value_advantage_decomposition'):\n",
    "                decomp = agent.get_value_advantage_decomposition(sample_state)\n",
    "                decompositions[variant] = decomp\n",
    "        \n",
    "        if decompositions:\n",
    "            # Plot Q-values comparison\n",
    "            for i, (variant, decomp) in enumerate(decompositions.items()):\n",
    "                q_vals = decomp['q_values']\n",
    "                x_pos = np.arange(len(q_vals)) + i * 0.25\n",
    "                ax.bar(x_pos, q_vals, 0.25, label=f'{variant} Q-values', \n",
    "                      alpha=0.7, color=colors[i])\n",
    "            \n",
    "            ax.set_title('Q-Values for Sample State')\n",
    "            ax.set_xlabel('Actions')\n",
    "            ax.set_ylabel('Q-Value')\n",
    "            ax.set_xticks(np.arange(len(q_vals)) + 0.25)\n",
    "            ax.set_xticklabels([f'A{i}' for i in range(len(q_vals))])\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Value vs Advantage contributions\n",
    "        ax = axes[1, 1]\n",
    "        if decompositions:\n",
    "            dueling_variants = {k: v for k, v in decompositions.items() if 'Dueling' in k}\n",
    "            \n",
    "            if dueling_variants:\n",
    "                variant_names_dueling = list(dueling_variants.keys())\n",
    "                values = [decomp['value'] for decomp in dueling_variants.values()]\n",
    "                max_advantages = [np.max(decomp['advantage']) for decomp in dueling_variants.values()]\n",
    "                min_advantages = [np.min(decomp['advantage']) for decomp in dueling_variants.values()]\n",
    "                \n",
    "                x = np.arange(len(variant_names_dueling))\n",
    "                width = 0.25\n",
    "                \n",
    "                ax.bar(x - width, values, width, label='State Value', alpha=0.7, color='blue')\n",
    "                ax.bar(x, max_advantages, width, label='Max Advantage', alpha=0.7, color='green')\n",
    "                ax.bar(x + width, min_advantages, width, label='Min Advantage', alpha=0.7, color='red')\n",
    "                \n",
    "                ax.set_title('Value vs Advantage Decomposition')\n",
    "                ax.set_ylabel('Value')\n",
    "                ax.set_xticks(x)\n",
    "                ax.set_xticklabels([name.replace('Dueling ', '') for name in variant_names_dueling])\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Training stability comparison\n",
    "        ax = axes[1, 2]\n",
    "        for i, (variant, data) in enumerate(results.items()):\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'losses') and len(agent.losses) > 30:\n",
    "                losses = agent.losses\n",
    "                loss_smooth = pd.Series(losses).rolling(30).mean()\n",
    "                ax.plot(loss_smooth, label=f'{variant}', \n",
    "                       color=colors[i], linewidth=2, alpha=0.7)\n",
    "        \n",
    "        ax.set_title('Training Loss Comparison')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('MSE Loss (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_value_advantage_dynamics(self):\n",
    "        \"\"\"Deep analysis of value and advantage learning dynamics\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Value-Advantage Dynamics Analysis\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Train dueling agent\n",
    "        agent = DuelingDQNAgent(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            dueling_type='mean',\n",
    "            lr=1e-3,\n",
    "            epsilon_decay=0.995,\n",
    "            buffer_size=10000\n",
    "        )\n",
    "        \n",
    "        print(\"Training Dueling DQN for dynamics analysis...\")\n",
    "        \n",
    "        # Store decompositions at different states throughout training\n",
    "        sample_states = [\n",
    "            [0.0, 0.0, 0.0, 0.0],    # Centered\n",
    "            [1.0, 0.0, 0.1, 0.0],    # Cart right, pole tilted\n",
    "            [-1.0, 0.0, -0.1, 0.0],  # Cart left, pole tilted opposite\n",
    "            [0.0, 1.0, 0.0, 1.0],    # Moving right\n",
    "            [0.0, -1.0, 0.0, -1.0],  # Moving left\n",
    "        ]\n",
    "        \n",
    "        decomposition_history = {i: {'values': [], 'advantages': [], 'q_values': []} \n",
    "                               for i in range(len(sample_states))}\n",
    "        \n",
    "        for episode in range(80):\n",
    "            agent.train_episode(env, max_steps=500)\n",
    "            \n",
    "            # Record decompositions every few episodes\n",
    "            if episode % 10 == 0:\n",
    "                for i, state in enumerate(sample_states):\n",
    "                    decomp = agent.get_value_advantage_decomposition(state)\n",
    "                    decomposition_history[i]['values'].append(decomp['value'])\n",
    "                    decomposition_history[i]['advantages'].append(decomp['advantage'].copy())\n",
    "                    decomposition_history[i]['q_values'].append(decomp['q_values'].copy())\n",
    "            \n",
    "            if (episode + 1) % 20 == 0:\n",
    "                print(f\"Episode {episode+1} completed\")\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # 1. Value evolution for different states\n",
    "        ax = axes[0, 0]\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(sample_states)))\n",
    "        \n",
    "        for i, color in enumerate(colors):\n",
    "            values = decomposition_history[i]['values']\n",
    "            episodes = np.arange(0, len(values)) * 10\n",
    "            ax.plot(episodes, values, label=f'State {i}', color=color, linewidth=2)\n",
    "        \n",
    "        ax.set_title('State Value Evolution')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('State Value V(s)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Advantage evolution for first state\n",
    "        ax = axes[0, 1]\n",
    "        if decomposition_history[0]['advantages']:\n",
    "            advantages_array = np.array(decomposition_history[0]['advantages'])  # [time, actions]\n",
    "            episodes = np.arange(0, len(advantages_array)) * 10\n",
    "            \n",
    "            for action in range(action_dim):\n",
    "                ax.plot(episodes, advantages_array[:, action], \n",
    "                       label=f'Action {action}', linewidth=2)\n",
    "        \n",
    "        ax.set_title('Advantage Evolution (State 0)')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Advantage A(s,a)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Q-value vs Value+Advantage consistency check\n",
    "        ax = axes[0, 2]\n",
    "        if decomposition_history[0]['q_values'] and decomposition_history[0]['values']:\n",
    "            q_vals = np.array(decomposition_history[0]['q_values'])\n",
    "            values = np.array(decomposition_history[0]['values'])\n",
    "            advantages = np.array(decomposition_history[0]['advantages'])\n",
    "            \n",
    "            # Compute reconstructed Q-values\n",
    "            reconstructed_q = values[:, None] + advantages - advantages.mean(axis=1, keepdims=True)\n",
    "            \n",
    "            # Plot comparison for one action\n",
    "            episodes = np.arange(0, len(q_vals)) * 10\n",
    "            ax.plot(episodes, q_vals[:, 0], label='Direct Q(s,a=0)', linewidth=2)\n",
    "            ax.plot(episodes, reconstructed_q[:, 0], label='V(s) + A(s,a=0) - mean(A)', \n",
    "                   linewidth=2, linestyle='--')\n",
    "            \n",
    "            ax.set_title('Q-Value Decomposition Consistency')\n",
    "            ax.set_xlabel('Episode')\n",
    "            ax.set_ylabel('Q-Value')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Final value vs advantage magnitudes\n",
    "        ax = axes[1, 0]\n",
    "        if all(decomposition_history[i]['values'] for i in range(len(sample_states))):\n",
    "            final_values = [decomposition_history[i]['values'][-1] for i in range(len(sample_states))]\n",
    "            final_advantage_ranges = [\n",
    "                np.max(decomposition_history[i]['advantages'][-1]) - \n",
    "                np.min(decomposition_history[i]['advantages'][-1]) \n",
    "                for i in range(len(sample_states))\n",
    "            ]\n",
    "            \n",
    "            ax.scatter(final_values, final_advantage_ranges, s=100, alpha=0.7, c=colors)\n",
    "            \n",
    "            for i, (val, adv_range) in enumerate(zip(final_values, final_advantage_ranges)):\n",
    "                ax.annotate(f'State {i}', (val, adv_range), xytext=(5, 5), \n",
    "                           textcoords='offset points')\n",
    "            \n",
    "            ax.set_xlabel('Final State Value')\n",
    "            ax.set_ylabel('Advantage Range (Max - Min)')\n",
    "            ax.set_title('State Value vs Advantage Spread')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Action preference evolution\n",
    "        ax = axes[1, 1]\n",
    "        if decomposition_history[0]['q_values']:\n",
    "            q_vals = np.array(decomposition_history[0]['q_values'])\n",
    "            episodes = np.arange(0, len(q_vals)) * 10\n",
    "            \n",
    "            # Compute action preferences (softmax probabilities)\n",
    "            action_probs = F.softmax(torch.tensor(q_vals), dim=1).numpy()\n",
    "            \n",
    "            for action in range(action_dim):\n",
    "                ax.plot(episodes, action_probs[:, action], \n",
    "                       label=f'Action {action} Preference', linewidth=2)\n",
    "            \n",
    "            ax.set_title('Action Preferences Over Time (State 0)')\n",
    "            ax.set_xlabel('Episode')\n",
    "            ax.set_ylabel('Action Probability')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Value prediction accuracy\n",
    "        ax = axes[1, 2]\n",
    "        if hasattr(agent, 'value_history') and hasattr(agent, 'q_values_history'):\n",
    "            # Compare average value predictions with average Q-values\n",
    "            episodes_v = range(len(agent.value_history))\n",
    "            episodes_q = range(len(agent.q_values_history))\n",
    "            \n",
    "            if len(episodes_v) > 10 and len(episodes_q) > 10:\n",
    "                # Smooth the curves\n",
    "                value_smooth = pd.Series(agent.value_history).rolling(20).mean()\n",
    "                q_smooth = pd.Series(agent.q_values_history).rolling(20).mean()\n",
    "                \n",
    "                ax.plot(episodes_v, value_smooth, label='Average State Values', linewidth=2)\n",
    "                ax.plot(episodes_q, q_smooth, label='Average Q-Values', linewidth=2)\n",
    "                \n",
    "                ax.set_title('Value vs Q-Value Evolution')\n",
    "                ax.set_xlabel('Training Step')\n",
    "                ax.set_ylabel('Average Value')\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        env.close()\n",
    "        return agent, decomposition_history\n",
    "\n",
    "# Run dueling architecture analysis\n",
    "dueling_analyzer = DuelingAnalyzer()\n",
    "\n",
    "print(\"1. Comparing Dueling DQN Variants...\")\n",
    "dueling_results = dueling_analyzer.compare_dueling_variants()\n",
    "\n",
    "print(\"\\n2. Analyzing Value-Advantage Dynamics...\")\n",
    "dynamics_agent, dynamics_history = dueling_analyzer.analyze_value_advantage_dynamics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ae08e",
   "metadata": {},
   "source": [
    "# Section 6: Prioritized Experience Replay\n",
    "\n",
    "## 6.1 Motivation for Prioritized Sampling\n",
    "\n",
    "Standard experience replay samples uniformly from the buffer, but not all experiences are equally valuable for learning:\n",
    "\n",
    "### Key Insights\n",
    "1. **Learning Opportunity**: Experiences with high TD error provide more learning signal\n",
    "2. **Sample Efficiency**: Focus on experiences where we can learn the most\n",
    "3. **Rare Events**: Important but infrequent experiences might be undersampled\n",
    "\n",
    "### Problems with Uniform Sampling\n",
    "- Wastes computation on experiences with low TD error\n",
    "- May miss important experiences that occur rarely\n",
    "- Doesn't focus learning where it's most needed\n",
    "\n",
    "## 6.2 Prioritized Experience Replay (PER) Algorithm\n",
    "\n",
    "### Priority Assignment\n",
    "Assign priority based on TD error magnitude:\n",
    "```\n",
    "p_i = |δ_i| + ε\n",
    "```\n",
    "Where:\n",
    "- `δ_i`: TD error for experience i\n",
    "- `ε`: Small constant to ensure all experiences have non-zero probability\n",
    "\n",
    "### Sampling Probability\n",
    "```\n",
    "P(i) = p_i^α / Σ_k p_k^α\n",
    "```\n",
    "Where `α` controls how much prioritization is applied:\n",
    "- `α = 0`: Uniform sampling (standard replay)\n",
    "- `α = 1`: Full prioritization\n",
    "\n",
    "### Importance Sampling Weights\n",
    "To correct for the bias introduced by prioritized sampling:\n",
    "```\n",
    "w_i = (1/N * 1/P(i))^β\n",
    "```\n",
    "Where:\n",
    "- `N`: Buffer size  \n",
    "- `β`: Importance sampling exponent (annealed from initial value to 1)\n",
    "\n",
    "## 6.3 Implementation Strategies\n",
    "\n",
    "### Efficient Data Structures\n",
    "1. **Sum Tree**: For efficient sampling and updating\n",
    "2. **Min Tree**: For tracking minimum priority\n",
    "3. **Segment Tree**: Alternative efficient implementation\n",
    "\n",
    "### Practical Considerations\n",
    "- **Priority Updates**: Update priorities after each training step\n",
    "- **Stale Priorities**: Handle experiences that haven't been updated recently\n",
    "- **Memory Efficiency**: Balance between accuracy and memory usage\n",
    "\n",
    "Let's implement a complete Prioritized Experience Replay system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf11e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Prioritized Experience Replay Implementation\n",
    "\n",
    "class SumTree:\n",
    "    \"\"\"\n",
    "    Sum Tree data structure for efficient prioritized sampling\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1)  # Internal nodes + leaves\n",
    "        self.data = np.zeros(capacity, dtype=object)  # Store experiences\n",
    "        self.data_pointer = 0\n",
    "        self.n_entries = 0\n",
    "    \n",
    "    def add(self, priority, data):\n",
    "        \"\"\"Add experience with given priority\"\"\"\n",
    "        tree_index = self.data_pointer + self.capacity - 1\n",
    "        \n",
    "        self.data[self.data_pointer] = data\n",
    "        self.update(tree_index, priority)\n",
    "        \n",
    "        self.data_pointer = (self.data_pointer + 1) % self.capacity\n",
    "        if self.n_entries < self.capacity:\n",
    "            self.n_entries += 1\n",
    "    \n",
    "    def update(self, tree_index, priority):\n",
    "        \"\"\"Update priority of a leaf node\"\"\"\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        \n",
    "        # Propagate change up the tree\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "    \n",
    "    def get_leaf(self, value):\n",
    "        \"\"\"Get leaf index, priority value, and data for given cumulative value\"\"\"\n",
    "        parent_index = 0\n",
    "        \n",
    "        while True:\n",
    "            left_child_index = 2 * parent_index + 1\n",
    "            right_child_index = left_child_index + 1\n",
    "            \n",
    "            # If we reach a leaf\n",
    "            if left_child_index >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            \n",
    "            if value <= self.tree[left_child_index]:\n",
    "                parent_index = left_child_index\n",
    "            else:\n",
    "                value -= self.tree[left_child_index]\n",
    "                parent_index = right_child_index\n",
    "        \n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "        return leaf_index, self.tree[leaf_index], self.data[data_index]\n",
    "    \n",
    "    @property\n",
    "    def total_priority(self):\n",
    "        \"\"\"Get total priority (sum of all priorities)\"\"\"\n",
    "        return self.tree[0]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.n_entries\n",
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"\n",
    "    Prioritized Experience Replay Buffer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1\n",
    "        \n",
    "        # Initialize sum tree\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "        # Priority constants\n",
    "        self.epsilon = 1e-6  # Small value to prevent zero priorities\n",
    "        self.max_priority = 1.0  # Initial max priority\n",
    "        \n",
    "        # Stats tracking\n",
    "        self.priority_history = []\n",
    "        self.sampling_weights_history = []\n",
    "    \n",
    "    def beta(self):\n",
    "        \"\"\"Annealed importance sampling weight\"\"\"\n",
    "        return min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience with max priority\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Use max priority for new experiences\n",
    "        priority = self.max_priority\n",
    "        self.tree.add(priority, experience)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch with priorities\"\"\"\n",
    "        batch_indices = []\n",
    "        batch_experiences = []\n",
    "        priorities = []\n",
    "        \n",
    "        # Calculate segment size for stratified sampling\n",
    "        segment_size = self.tree.total_priority / batch_size\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # Sample from segment\n",
    "            a = segment_size * i\n",
    "            b = segment_size * (i + 1)\n",
    "            value = random.uniform(a, b)\n",
    "            \n",
    "            # Get experience\n",
    "            index, priority, experience = self.tree.get_leaf(value)\n",
    "            \n",
    "            batch_indices.append(index)\n",
    "            batch_experiences.append(experience)\n",
    "            priorities.append(priority)\n",
    "        \n",
    "        # Calculate importance sampling weights\n",
    "        sampling_probabilities = np.array(priorities) / self.tree.total_priority\n",
    "        weights = (len(self.tree) * sampling_probabilities) ** -self.beta()\n",
    "        weights = weights / weights.max()  # Normalize\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor([e.state for e in batch_experiences]).to(device)\n",
    "        actions = torch.LongTensor([e.action for e in batch_experiences]).to(device)\n",
    "        rewards = torch.FloatTensor([e.reward for e in batch_experiences]).to(device)\n",
    "        next_states = torch.FloatTensor([e.next_state for e in batch_experiences]).to(device)\n",
    "        dones = torch.BoolTensor([e.done for e in batch_experiences]).to(device)\n",
    "        weights = torch.FloatTensor(weights).to(device)\n",
    "        \n",
    "        # Store stats\n",
    "        self.priority_history.append(np.mean(priorities))\n",
    "        self.sampling_weights_history.append(weights.mean().item())\n",
    "        \n",
    "        self.frame += 1\n",
    "        \n",
    "        return states, actions, rewards, next_states, dones, weights, batch_indices\n",
    "    \n",
    "    def update_priorities(self, indices, priorities):\n",
    "        \"\"\"Update priorities for given indices\"\"\"\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            priority = abs(priority) + self.epsilon\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "            priority = priority ** self.alpha\n",
    "            self.tree.update(idx, priority)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tree)\n",
    "\n",
    "class PrioritizedDQNAgent(DuelingDQNAgent):\n",
    "    \"\"\"\n",
    "    DQN Agent with Prioritized Experience Replay\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, alpha=0.6, beta_start=0.4, **kwargs):\n",
    "        # Initialize parent without replay buffer\n",
    "        buffer_size = kwargs.pop('buffer_size', 100000)\n",
    "        super().__init__(state_dim, action_dim, **kwargs)\n",
    "        \n",
    "        # Replace with prioritized replay buffer\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(\n",
    "            capacity=buffer_size,\n",
    "            alpha=alpha,\n",
    "            beta_start=beta_start\n",
    "        )\n",
    "        \n",
    "        # Additional tracking\n",
    "        self.priority_stats = []\n",
    "        self.td_errors_history = []\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in prioritized buffer\"\"\"\n",
    "        self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Prioritized training step\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch with priorities and weights\n",
    "        states, actions, rewards, next_states, dones, weights, indices = \\\n",
    "            self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Current Q-values\n",
    "        if hasattr(self.q_network, 'forward') and len(self.q_network.forward(states)) == 3:\n",
    "            # Dueling network\n",
    "            current_q_values, _, _ = self.q_network(states)\n",
    "        else:\n",
    "            current_q_values = self.q_network(states)\n",
    "        \n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute targets (Double DQN)\n",
    "        with torch.no_grad():\n",
    "            if hasattr(self.q_network, 'forward') and len(self.q_network.forward(next_states)) == 3:\n",
    "                # Dueling network\n",
    "                next_q_main, _, _ = self.q_network(next_states)\n",
    "                next_q_target, _, _ = self.target_network(next_states)\n",
    "            else:\n",
    "                next_q_main = self.q_network(next_states)\n",
    "                next_q_target = self.target_network(next_states)\n",
    "            \n",
    "            next_actions = next_q_main.argmax(1)\n",
    "            next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "        \n",
    "        # Compute TD errors\n",
    "        td_errors = target_q_values - current_q_values\n",
    "        \n",
    "        # Weighted loss (importance sampling correction)\n",
    "        loss = (weights * td_errors.pow(2)).mean()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update priorities with TD errors\n",
    "        priorities = abs(td_errors.detach().cpu().numpy())\n",
    "        self.replay_buffer.update_priorities(indices, priorities)\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.training_step += 1\n",
    "        if self.training_step % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Update epsilon\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        # Store metrics\n",
    "        self.losses.append(loss.item())\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "        self.td_errors_history.append(td_errors.abs().mean().item())\n",
    "        \n",
    "        # Track Q-values and additional metrics\n",
    "        with torch.no_grad():\n",
    "            avg_q_value = current_q_values.mean().item()\n",
    "            self.q_values_history.append(avg_q_value)\n",
    "            \n",
    "            if hasattr(self, 'value_history'):\n",
    "                # For dueling networks\n",
    "                _, values, advantages = self.q_network(states)\n",
    "                self.value_history.append(values.mean().item())\n",
    "                self.advantage_history.append(advantages.mean().item())\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "class PrioritizedReplayAnalyzer:\n",
    "    \"\"\"\n",
    "    Analysis of Prioritized Experience Replay\n",
    "    \"\"\"\n",
    "    \n",
    "    def compare_replay_methods(self):\n",
    "        \"\"\"Compare standard vs prioritized replay\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Standard vs Prioritized Experience Replay Comparison\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Test different replay methods\n",
    "        methods = {\n",
    "            'Standard Replay': DuelingDQNAgent,\n",
    "            'Prioritized Replay': PrioritizedDQNAgent,\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        num_episodes = 100\n",
    "        \n",
    "        for method_name, agent_class in methods.items():\n",
    "            print(f\"\\nTraining {method_name}...\")\n",
    "            \n",
    "            agent_kwargs = {\n",
    "                'state_dim': state_dim,\n",
    "                'action_dim': action_dim,\n",
    "                'lr': 1e-3,\n",
    "                'epsilon_decay': 0.995,\n",
    "                'target_update_freq': 100,\n",
    "                'buffer_size': 20000,\n",
    "                'dueling_type': 'mean'\n",
    "            }\n",
    "            \n",
    "            if method_name == 'Prioritized Replay':\n",
    "                agent_kwargs.update({'alpha': 0.6, 'beta_start': 0.4})\n",
    "            \n",
    "            agent = agent_class(**agent_kwargs)\n",
    "            \n",
    "            episode_rewards = []\n",
    "            sample_efficiency = []\n",
    "            \n",
    "            for episode in range(num_episodes):\n",
    "                reward, steps = agent.train_episode(env, max_steps=500)\n",
    "                episode_rewards.append(reward)\n",
    "                sample_efficiency.append(steps)\n",
    "                \n",
    "                if (episode + 1) % 25 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-10:])\n",
    "                    avg_steps = np.mean(sample_efficiency[-10:])\n",
    "                    print(f\"  Episode {episode+1}: Reward = {avg_reward:.1f}, Steps = {avg_steps:.1f}\")\n",
    "            \n",
    "            # Final evaluation\n",
    "            eval_results = agent.evaluate(env, num_episodes=20)\n",
    "            \n",
    "            results[method_name] = {\n",
    "                'agent': agent,\n",
    "                'rewards': episode_rewards,\n",
    "                'steps': sample_efficiency,\n",
    "                'eval_performance': eval_results,\n",
    "                'final_performance': np.mean(episode_rewards[-15:])\n",
    "            }\n",
    "        \n",
    "        # Visualization\n",
    "        self.visualize_replay_comparison(results)\n",
    "        \n",
    "        env.close()\n",
    "        return results\n",
    "    \n",
    "    def visualize_replay_comparison(self, results):\n",
    "        \"\"\"Visualize replay method comparison\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        colors = ['blue', 'red']\n",
    "        \n",
    "        # 1. Learning curves\n",
    "        ax = axes[0, 0]\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            rewards = data['rewards']\n",
    "            smoothed = pd.Series(rewards).rolling(8).mean()\n",
    "            ax.plot(smoothed, label=method, color=colors[i], linewidth=2)\n",
    "            ax.fill_between(range(len(smoothed)), smoothed, alpha=0.3, color=colors[i])\n",
    "        \n",
    "        ax.set_title('Learning Curves: Replay Methods')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Episode Reward (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Sample efficiency\n",
    "        ax = axes[0, 1]\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            steps = data['steps']\n",
    "            smoothed_steps = pd.Series(steps).rolling(8).mean()\n",
    "            ax.plot(smoothed_steps, label=method, color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('Sample Efficiency (Steps per Episode)')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Steps (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Final performance\n",
    "        ax = axes[0, 2]\n",
    "        method_names = list(results.keys())\n",
    "        final_perfs = [results[m]['final_performance'] for m in method_names]\n",
    "        eval_means = [results[m]['eval_performance']['mean_reward'] for m in method_names]\n",
    "        eval_stds = [results[m]['eval_performance']['std_reward'] for m in method_names]\n",
    "        \n",
    "        x = np.arange(len(method_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        ax.bar(x - width/2, final_perfs, width, label='Training', alpha=0.7, color=colors)\n",
    "        ax.bar(x + width/2, eval_means, width, yerr=eval_stds, \n",
    "               label='Evaluation', alpha=0.7, color=['darkblue', 'darkred'])\n",
    "        \n",
    "        ax.set_title('Performance Comparison')\n",
    "        ax.set_ylabel('Average Reward')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(method_names, rotation=15)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Priority analysis (for prioritized agent)\n",
    "        ax = axes[1, 0]\n",
    "        prioritized_agent = None\n",
    "        for method, data in results.items():\n",
    "            if 'Prioritized' in method:\n",
    "                prioritized_agent = data['agent']\n",
    "                break\n",
    "        \n",
    "        if prioritized_agent and hasattr(prioritized_agent.replay_buffer, 'priority_history'):\n",
    "            priorities = prioritized_agent.replay_buffer.priority_history\n",
    "            if priorities:\n",
    "                ax.plot(priorities, color='purple', linewidth=2)\n",
    "                ax.set_title('Priority Evolution')\n",
    "                ax.set_xlabel('Training Step')\n",
    "                ax.set_ylabel('Average Priority')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. TD Error evolution\n",
    "        ax = axes[1, 1]\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'td_errors_history') and agent.td_errors_history:\n",
    "                td_errors = agent.td_errors_history\n",
    "                smoothed_td = pd.Series(td_errors).rolling(20).mean()\n",
    "                ax.plot(smoothed_td, label=method, color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('TD Error Evolution')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Average TD Error')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Importance sampling weights (for prioritized)\n",
    "        ax = axes[1, 2]\n",
    "        if prioritized_agent and hasattr(prioritized_agent.replay_buffer, 'sampling_weights_history'):\n",
    "            weights = prioritized_agent.replay_buffer.sampling_weights_history\n",
    "            if weights:\n",
    "                ax.plot(weights, color='orange', linewidth=2)\n",
    "                ax.set_title('Importance Sampling Weights')\n",
    "                ax.set_xlabel('Training Step')\n",
    "                ax.set_ylabel('Average Weight')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add beta schedule\n",
    "                beta_values = [prioritized_agent.replay_buffer.beta_start + \n",
    "                              i * (1.0 - prioritized_agent.replay_buffer.beta_start) / \n",
    "                              prioritized_agent.replay_buffer.beta_frames \n",
    "                              for i in range(len(weights))]\n",
    "                \n",
    "                ax2 = ax.twinx()\n",
    "                ax2.plot(beta_values, color='green', linewidth=2, alpha=0.7, linestyle='--')\n",
    "                ax2.set_ylabel('Beta (Annealing)', color='green')\n",
    "                ax2.tick_params(axis='y', labelcolor='green')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def analyze_priority_distribution(self):\n",
    "        \"\"\"Analyze priority distribution and sampling behavior\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Priority Distribution Analysis\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Create prioritized agent\n",
    "        agent = PrioritizedDQNAgent(\n",
    "            state_dim=state_dim,\n",
    "            action_dim=action_dim,\n",
    "            alpha=0.6,\n",
    "            beta_start=0.4,\n",
    "            lr=1e-3,\n",
    "            buffer_size=5000\n",
    "        )\n",
    "        \n",
    "        print(\"Training agent to analyze priorities...\")\n",
    "        \n",
    "        # Train for some episodes\n",
    "        for episode in range(40):\n",
    "            agent.train_episode(env, max_steps=500)\n",
    "            \n",
    "            if (episode + 1) % 10 == 0:\n",
    "                print(f\"Episode {episode+1} completed\")\n",
    "        \n",
    "        # Analyze the priority distribution\n",
    "        if len(agent.replay_buffer) > 100:\n",
    "            print(\"\\nAnalyzing priority distribution...\")\n",
    "            \n",
    "            # Sample a large batch to analyze priorities\n",
    "            try:\n",
    "                states, actions, rewards, next_states, dones, weights, indices = \\\n",
    "                    agent.replay_buffer.sample(min(1000, len(agent.replay_buffer)))\n",
    "                \n",
    "                # Compute TD errors for all sampled experiences\n",
    "                with torch.no_grad():\n",
    "                    if hasattr(agent.q_network, 'forward') and len(agent.q_network.forward(states)) == 3:\n",
    "                        current_q_values, _, _ = agent.q_network(states)\n",
    "                        next_q_main, _, _ = agent.q_network(next_states)\n",
    "                        next_q_target, _, _ = agent.target_network(next_states)\n",
    "                    else:\n",
    "                        current_q_values = agent.q_network(states)\n",
    "                        next_q_main = agent.q_network(next_states)\n",
    "                        next_q_target = agent.target_network(next_states)\n",
    "                    \n",
    "                    current_q_selected = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "                    next_actions = next_q_main.argmax(1)\n",
    "                    next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "                    target_q_values = rewards + (agent.gamma * next_q_values * (~dones))\n",
    "                    td_errors = abs(target_q_values - current_q_selected).cpu().numpy()\n",
    "                \n",
    "                # Visualization\n",
    "                fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "                \n",
    "                # 1. Priority distribution\n",
    "                ax = axes[0, 0]\n",
    "                # Extract priorities from tree (approximation)\n",
    "                all_priorities = []\n",
    "                for i in range(len(agent.replay_buffer.tree.data)):\n",
    "                    if agent.replay_buffer.tree.data[i] is not None:\n",
    "                        tree_idx = i + agent.replay_buffer.capacity - 1\n",
    "                        priority = agent.replay_buffer.tree.tree[tree_idx]\n",
    "                        all_priorities.append(priority)\n",
    "                \n",
    "                if all_priorities:\n",
    "                    ax.hist(all_priorities, bins=30, alpha=0.7, edgecolor='black')\n",
    "                    ax.set_title('Priority Distribution in Buffer')\n",
    "                    ax.set_xlabel('Priority')\n",
    "                    ax.set_ylabel('Frequency')\n",
    "                    ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # 2. TD Error vs Priority correlation\n",
    "                ax = axes[0, 1]\n",
    "                sampled_priorities = []\n",
    "                for idx in indices:\n",
    "                    priority = agent.replay_buffer.tree.tree[idx]\n",
    "                    sampled_priorities.append(priority)\n",
    "                \n",
    "                ax.scatter(td_errors, sampled_priorities, alpha=0.6)\n",
    "                ax.set_xlabel('TD Error')\n",
    "                ax.set_ylabel('Priority')\n",
    "                ax.set_title('TD Error vs Priority Correlation')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add correlation coefficient\n",
    "                if len(td_errors) > 10:\n",
    "                    correlation = np.corrcoef(td_errors, sampled_priorities)[0, 1]\n",
    "                    ax.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n",
    "                           transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "                \n",
    "                # 3. Importance weights distribution\n",
    "                ax = axes[1, 0]\n",
    "                weights_np = weights.cpu().numpy()\n",
    "                ax.hist(weights_np, bins=30, alpha=0.7, edgecolor='black', color='orange')\n",
    "                ax.set_title('Importance Sampling Weights')\n",
    "                ax.set_xlabel('Weight')\n",
    "                ax.set_ylabel('Frequency')\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add statistics\n",
    "                ax.text(0.05, 0.95, f'Mean: {weights_np.mean():.3f}\\nStd: {weights_np.std():.3f}', \n",
    "                       transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n",
    "                \n",
    "                # 4. Beta annealing schedule\n",
    "                ax = axes[1, 1]\n",
    "                frame_range = np.arange(0, agent.replay_buffer.beta_frames, 1000)\n",
    "                beta_schedule = [\n",
    "                    min(1.0, agent.replay_buffer.beta_start + \n",
    "                        frame * (1.0 - agent.replay_buffer.beta_start) / agent.replay_buffer.beta_frames)\n",
    "                    for frame in frame_range\n",
    "                ]\n",
    "                \n",
    "                ax.plot(frame_range, beta_schedule, linewidth=2, color='green')\n",
    "                ax.axvline(agent.replay_buffer.frame, color='red', linestyle='--', \n",
    "                          label=f'Current Frame: {agent.replay_buffer.frame}')\n",
    "                ax.set_title('Beta Annealing Schedule')\n",
    "                ax.set_xlabel('Training Frame')\n",
    "                ax.set_ylabel('Beta Value')\n",
    "                ax.legend()\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "                \n",
    "                # Print statistics\n",
    "                print(f\"\\nPriority Statistics:\")\n",
    "                print(f\"  Buffer size: {len(agent.replay_buffer)}\")\n",
    "                print(f\"  Priority range: [{min(all_priorities):.4f}, {max(all_priorities):.4f}]\")\n",
    "                print(f\"  Average priority: {np.mean(all_priorities):.4f}\")\n",
    "                print(f\"  Priority std: {np.std(all_priorities):.4f}\")\n",
    "                print(f\"\\nSampling Statistics:\")\n",
    "                print(f\"  Weight range: [{weights_np.min():.4f}, {weights_np.max():.4f}]\")\n",
    "                print(f\"  Average weight: {weights_np.mean():.4f}\")\n",
    "                print(f\"  Current beta: {agent.replay_buffer.beta():.4f}\")\n",
    "                print(f\"  Current frame: {agent.replay_buffer.frame}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in priority analysis: {e}\")\n",
    "        \n",
    "        env.close()\n",
    "        return agent\n",
    "\n",
    "# Run prioritized experience replay analysis\n",
    "per_analyzer = PrioritizedReplayAnalyzer()\n",
    "\n",
    "print(\"1. Comparing Standard vs Prioritized Replay...\")\n",
    "per_comparison = per_analyzer.compare_replay_methods()\n",
    "\n",
    "print(\"\\n2. Analyzing Priority Distribution...\")\n",
    "per_analysis_agent = per_analyzer.analyze_priority_distribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7dbf6f",
   "metadata": {},
   "source": [
    "# Section 7: Rainbow DQN - Combining All Improvements\n",
    "\n",
    "## Comprehensive Deep Q-Network\n",
    "\n",
    "Rainbow DQN represents the state-of-the-art combination of multiple DQN improvements:\n",
    "\n",
    "1. **Double DQN**: Reduces overestimation bias\n",
    "2. **Dueling DQN**: Separates value and advantage learning\n",
    "3. **Prioritized Experience Replay**: Improves sample efficiency\n",
    "4. **Multi-step Learning**: Reduces bias in temporal difference learning\n",
    "5. **Distributional RL**: Models the full return distribution\n",
    "6. **Noisy Networks**: Exploration through parametric noise\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Multi-step Learning\n",
    "Instead of 1-step TD targets, we use n-step returns:\n",
    "$$R_t^{(n)} = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k} + \\gamma^n \\max_{a'} Q(s_{t+n}, a')$$\n",
    "\n",
    "### Distributional Learning\n",
    "Model the return distribution instead of expected return:\n",
    "$$Z(s,a) = \\mathbb{E}[R|s,a]$$\n",
    "\n",
    "Where $Z(s,a)$ represents the distribution of returns starting from state $s$ and action $a$.\n",
    "\n",
    "### Noisy Networks\n",
    "Replace standard linear layers with noisy versions:\n",
    "$$y = (W + \\sigma_W \\odot \\epsilon_W) x + (b + \\sigma_b \\odot \\epsilon_b)$$\n",
    "\n",
    "Where $\\epsilon$ represents noise samples and $\\sigma$ are learnable noise parameters.\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "We'll implement a simplified Rainbow DQN focusing on the core improvements that provide the most benefit while maintaining clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8afbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Rainbow DQN Implementation\n",
    "\n",
    "class NoisyLinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Noisy Linear Layer for exploration without epsilon-greedy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "        \n",
    "        # Learnable parameters\n",
    "        self.weight_mu = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.zeros(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.zeros(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.zeros(out_features))\n",
    "        \n",
    "        # Noise buffers (not parameters)\n",
    "        self.register_buffer('weight_epsilon', torch.zeros(out_features, in_features))\n",
    "        self.register_buffer('bias_epsilon', torch.zeros(out_features))\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Initialize parameters\"\"\"\n",
    "        mu_range = 1 / math.sqrt(self.in_features)\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n",
    "        \n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with noisy weights\"\"\"\n",
    "        if self.training:\n",
    "            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "        \n",
    "        return F.linear(x, weight, bias)\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset noise for both weight and bias\"\"\"\n",
    "        epsilon_i = self._scale_noise(self.in_features)\n",
    "        epsilon_j = self._scale_noise(self.out_features)\n",
    "        \n",
    "        self.weight_epsilon.copy_(epsilon_j.ger(epsilon_i))\n",
    "        self.bias_epsilon.copy_(epsilon_j)\n",
    "    \n",
    "    def _scale_noise(self, size):\n",
    "        \"\"\"Scale noise using factorized gaussian noise\"\"\"\n",
    "        x = torch.randn(size, device=self.weight_mu.device)\n",
    "        return x.sign().mul(x.abs().sqrt())\n",
    "\n",
    "class RainbowNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Rainbow DQN Network with Dueling Architecture and Noisy Layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=128, n_step=3, use_noisy=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_step = n_step\n",
    "        self.use_noisy = use_noisy\n",
    "        \n",
    "        # Feature extraction\n",
    "        self.feature_layer = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Dueling streams\n",
    "        if use_noisy:\n",
    "            # Value stream with noisy layers\n",
    "            self.value_stream = nn.Sequential(\n",
    "                NoisyLinear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                NoisyLinear(hidden_dim // 2, 1)\n",
    "            )\n",
    "            \n",
    "            # Advantage stream with noisy layers\n",
    "            self.advantage_stream = nn.Sequential(\n",
    "                NoisyLinear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                NoisyLinear(hidden_dim // 2, output_dim)\n",
    "            )\n",
    "        else:\n",
    "            # Standard dueling streams\n",
    "            self.value_stream = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim // 2, 1)\n",
    "            )\n",
    "            \n",
    "            self.advantage_stream = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim // 2, output_dim)\n",
    "            )\n",
    "        \n",
    "        self.reset_noise()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass with dueling architecture\"\"\"\n",
    "        features = self.feature_layer(x)\n",
    "        \n",
    "        values = self.value_stream(features)\n",
    "        advantages = self.advantage_stream(features)\n",
    "        \n",
    "        # Dueling aggregation (mean subtraction)\n",
    "        q_values = values + (advantages - advantages.mean(dim=1, keepdim=True))\n",
    "        \n",
    "        return q_values, values, advantages\n",
    "    \n",
    "    def reset_noise(self):\n",
    "        \"\"\"Reset noise in noisy layers\"\"\"\n",
    "        if self.use_noisy:\n",
    "            for module in self.modules():\n",
    "                if isinstance(module, NoisyLinear):\n",
    "                    module.reset_noise()\n",
    "\n",
    "class MultiStepBuffer:\n",
    "    \"\"\"\n",
    "    Multi-step experience buffer for n-step learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_step, gamma):\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "        self.buffer = []\n",
    "    \n",
    "    def append(self, experience):\n",
    "        \"\"\"Add experience to buffer\"\"\"\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "        if len(self.buffer) < self.n_step:\n",
    "            return None\n",
    "        \n",
    "        # Calculate n-step return\n",
    "        n_step_reward = 0\n",
    "        for i, exp in enumerate(self.buffer):\n",
    "            n_step_reward += (self.gamma ** i) * exp.reward\n",
    "            if exp.done:\n",
    "                break\n",
    "        \n",
    "        # Return n-step experience\n",
    "        first_exp = self.buffer[0]\n",
    "        last_exp = self.buffer[-1]\n",
    "        \n",
    "        n_step_exp = Experience(\n",
    "            state=first_exp.state,\n",
    "            action=first_exp.action,\n",
    "            reward=n_step_reward,\n",
    "            next_state=last_exp.next_state,\n",
    "            done=last_exp.done\n",
    "        )\n",
    "        \n",
    "        self.buffer.pop(0)\n",
    "        return n_step_exp\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear buffer\"\"\"\n",
    "        self.buffer.clear()\n",
    "\n",
    "class RainbowDQNAgent:\n",
    "    \"\"\"\n",
    "    Rainbow DQN Agent combining multiple improvements\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, \n",
    "                 buffer_size=100000, batch_size=64, target_update_freq=500,\n",
    "                 n_step=3, alpha=0.6, beta_start=0.4, use_noisy=True,\n",
    "                 device=None):\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.n_step = n_step\n",
    "        self.use_noisy = use_noisy\n",
    "        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Networks\n",
    "        self.q_network = RainbowNetwork(state_dim, action_dim, use_noisy=use_noisy).to(self.device)\n",
    "        self.target_network = RainbowNetwork(state_dim, action_dim, use_noisy=use_noisy).to(self.device)\n",
    "        \n",
    "        # Copy weights to target network\n",
    "        self.update_target_network()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience replay with priorities\n",
    "        self.replay_buffer = PrioritizedReplayBuffer(\n",
    "            capacity=buffer_size,\n",
    "            alpha=alpha,\n",
    "            beta_start=beta_start\n",
    "        )\n",
    "        \n",
    "        # Multi-step buffer\n",
    "        self.multi_step_buffer = MultiStepBuffer(n_step, gamma)\n",
    "        \n",
    "        # Exploration (only used if not using noisy networks)\n",
    "        self.epsilon = 1.0 if not use_noisy else 0.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_end = 0.01\n",
    "        \n",
    "        # Training tracking\n",
    "        self.training_step = 0\n",
    "        self.losses = []\n",
    "        self.q_values_history = []\n",
    "        self.value_history = []\n",
    "        self.advantage_history = []\n",
    "        self.epsilon_history = []\n",
    "        self.td_errors_history = []\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using noisy networks or epsilon-greedy\"\"\"\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "        \n",
    "        if self.use_noisy:\n",
    "            # Reset noise for exploration\n",
    "            self.q_network.reset_noise()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_values, _, _ = self.q_network(state)\n",
    "                action = q_values.argmax().item()\n",
    "        else:\n",
    "            # Epsilon-greedy exploration\n",
    "            if random.random() < self.epsilon:\n",
    "                action = random.randrange(self.action_dim)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    q_values, _, _ = self.q_network(state)\n",
    "                    action = q_values.argmax().item()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience with multi-step processing\"\"\"\n",
    "        experience = Experience(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Add to multi-step buffer\n",
    "        n_step_exp = self.multi_step_buffer.append(experience)\n",
    "        \n",
    "        # Store in replay buffer if we have n-step experience\n",
    "        if n_step_exp is not None:\n",
    "            self.replay_buffer.push(\n",
    "                n_step_exp.state, n_step_exp.action, n_step_exp.reward,\n",
    "                n_step_exp.next_state, n_step_exp.done\n",
    "            )\n",
    "        \n",
    "        # Clear multi-step buffer on episode end\n",
    "        if done:\n",
    "            self.multi_step_buffer.clear()\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Rainbow training step\"\"\"\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return None\n",
    "        \n",
    "        # Sample batch with priorities\n",
    "        states, actions, rewards, next_states, dones, weights, indices = \\\n",
    "            self.replay_buffer.sample(self.batch_size)\n",
    "        \n",
    "        # Reset noise in networks\n",
    "        if self.use_noisy:\n",
    "            self.q_network.reset_noise()\n",
    "            self.target_network.reset_noise()\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q_values, current_values, current_advantages = self.q_network(states)\n",
    "        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # Compute targets with Double DQN and multi-step\n",
    "        with torch.no_grad():\n",
    "            next_q_main, _, _ = self.q_network(next_states)\n",
    "            next_q_target, _, _ = self.target_network(next_states)\n",
    "            \n",
    "            next_actions = next_q_main.argmax(1)\n",
    "            next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
    "            \n",
    "            # Multi-step targets (discount already applied in multi-step buffer)\n",
    "            target_q_values = rewards + ((self.gamma ** self.n_step) * next_q_values * (~dones))\n",
    "        \n",
    "        # Compute TD errors\n",
    "        td_errors = target_q_values - current_q_values\n",
    "        \n",
    "        # Weighted loss (importance sampling correction)\n",
    "        loss = (weights * td_errors.pow(2)).mean()\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update priorities\n",
    "        priorities = abs(td_errors.detach().cpu().numpy())\n",
    "        self.replay_buffer.update_priorities(indices, priorities)\n",
    "        \n",
    "        # Update target network\n",
    "        self.training_step += 1\n",
    "        if self.training_step % self.target_update_freq == 0:\n",
    "            self.update_target_network()\n",
    "        \n",
    "        # Update epsilon (if not using noisy networks)\n",
    "        if not self.use_noisy:\n",
    "            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        # Store metrics\n",
    "        self.losses.append(loss.item())\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "        self.td_errors_history.append(td_errors.abs().mean().item())\n",
    "        \n",
    "        # Track additional metrics\n",
    "        with torch.no_grad():\n",
    "            self.q_values_history.append(current_q_values.mean().item())\n",
    "            self.value_history.append(current_values.mean().item())\n",
    "            self.advantage_history.append(current_advantages.mean().item())\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def update_target_network(self):\n",
    "        \"\"\"Copy weights from main network to target network\"\"\"\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "    \n",
    "    def train_episode(self, env, max_steps=1000):\n",
    "        \"\"\"Train for one episode\"\"\"\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            self.store_experience(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Train\n",
    "            loss = self.train_step()\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        return total_reward, steps\n",
    "    \n",
    "    def evaluate(self, env, num_episodes=10):\n",
    "        \"\"\"Evaluate agent performance\"\"\"\n",
    "        rewards = []\n",
    "        \n",
    "        # Disable noise for evaluation\n",
    "        old_training = self.q_network.training\n",
    "        self.q_network.eval()\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            \n",
    "            for _ in range(1000):\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                total_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            rewards.append(total_reward)\n",
    "        \n",
    "        # Restore training mode\n",
    "        self.q_network.train(old_training)\n",
    "        \n",
    "        return {\n",
    "            'mean_reward': np.mean(rewards),\n",
    "            'std_reward': np.std(rewards),\n",
    "            'min_reward': np.min(rewards),\n",
    "            'max_reward': np.max(rewards),\n",
    "            'rewards': rewards\n",
    "        }\n",
    "\n",
    "class RainbowAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of Rainbow DQN\n",
    "    \"\"\"\n",
    "    \n",
    "    def compare_all_methods(self):\n",
    "        \"\"\"Compare all DQN variants we've implemented\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Comprehensive DQN Methods Comparison\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Test all methods\n",
    "        methods = {\n",
    "            'Basic DQN': DQNAgent,\n",
    "            'Double DQN': DoubleDQNAgent,\n",
    "            'Dueling DQN': DuelingDQNAgent,\n",
    "            'Prioritized DQN': PrioritizedDQNAgent,\n",
    "            'Rainbow DQN': RainbowDQNAgent,\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        num_episodes = 80\n",
    "        \n",
    "        for method_name, agent_class in methods.items():\n",
    "            print(f\"\\nTraining {method_name}...\")\n",
    "            \n",
    "            # Common parameters\n",
    "            base_kwargs = {\n",
    "                'state_dim': state_dim,\n",
    "                'action_dim': action_dim,\n",
    "                'lr': 1e-3,\n",
    "                'target_update_freq': 100,\n",
    "                'buffer_size': 15000,\n",
    "                'batch_size': 64\n",
    "            }\n",
    "            \n",
    "            # Method-specific parameters\n",
    "            if method_name in ['Dueling DQN', 'Prioritized DQN']:\n",
    "                base_kwargs['dueling_type'] = 'mean'\n",
    "                base_kwargs['epsilon_decay'] = 0.995\n",
    "            \n",
    "            if method_name == 'Prioritized DQN':\n",
    "                base_kwargs.update({'alpha': 0.6, 'beta_start': 0.4})\n",
    "            \n",
    "            if method_name == 'Rainbow DQN':\n",
    "                base_kwargs.update({\n",
    "                    'n_step': 3,\n",
    "                    'alpha': 0.6,\n",
    "                    'beta_start': 0.4,\n",
    "                    'use_noisy': True\n",
    "                })\n",
    "            \n",
    "            agent = agent_class(**base_kwargs)\n",
    "            \n",
    "            episode_rewards = []\n",
    "            sample_efficiency = []\n",
    "            \n",
    "            for episode in range(num_episodes):\n",
    "                reward, steps = agent.train_episode(env, max_steps=500)\n",
    "                episode_rewards.append(reward)\n",
    "                sample_efficiency.append(steps)\n",
    "                \n",
    "                if (episode + 1) % 20 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-10:])\n",
    "                    print(f\"  Episode {episode+1}: Reward = {avg_reward:.1f}\")\n",
    "            \n",
    "            # Final evaluation\n",
    "            eval_results = agent.evaluate(env, num_episodes=15)\n",
    "            \n",
    "            results[method_name] = {\n",
    "                'agent': agent,\n",
    "                'rewards': episode_rewards,\n",
    "                'steps': sample_efficiency,\n",
    "                'eval_performance': eval_results,\n",
    "                'final_performance': np.mean(episode_rewards[-10:])\n",
    "            }\n",
    "        \n",
    "        # Comprehensive visualization\n",
    "        self.visualize_comprehensive_comparison(results)\n",
    "        \n",
    "        env.close()\n",
    "        return results\n",
    "    \n",
    "    def visualize_comprehensive_comparison(self, results):\n",
    "        \"\"\"Comprehensive visualization of all methods\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "        colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "        \n",
    "        # 1. Learning curves\n",
    "        ax = axes[0, 0]\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            rewards = data['rewards']\n",
    "            smoothed = pd.Series(rewards).rolling(5).mean()\n",
    "            ax.plot(smoothed, label=method, color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('Learning Curves Comparison')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Episode Reward (Smoothed)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Sample efficiency\n",
    "        ax = axes[0, 1]\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            steps = data['steps']\n",
    "            smoothed_steps = pd.Series(steps).rolling(5).mean()\n",
    "            ax.plot(smoothed_steps, label=method, color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('Sample Efficiency')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Steps per Episode')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Final performance comparison\n",
    "        ax = axes[0, 2]\n",
    "        method_names = list(results.keys())\n",
    "        final_perfs = [results[m]['final_performance'] for m in method_names]\n",
    "        eval_means = [results[m]['eval_performance']['mean_reward'] for m in method_names]\n",
    "        eval_stds = [results[m]['eval_performance']['std_reward'] for m in method_names]\n",
    "        \n",
    "        x = np.arange(len(method_names))\n",
    "        width = 0.35\n",
    "        \n",
    "        bars1 = ax.bar(x - width/2, final_perfs, width, label='Training', alpha=0.7, color=colors)\n",
    "        bars2 = ax.bar(x + width/2, eval_means, width, yerr=eval_stds, \n",
    "                      label='Evaluation', alpha=0.7, color=[c + '80' for c in ['darkblue', 'darkred', 'darkgreen', 'darkorange', 'indigo']])\n",
    "        \n",
    "        ax.set_title('Performance Comparison')\n",
    "        ax.set_ylabel('Average Reward')\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels(method_names, rotation=15)\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Loss evolution\n",
    "        ax = axes[1, 0]\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'losses') and agent.losses:\n",
    "                losses = agent.losses\n",
    "                if len(losses) > 10:\n",
    "                    smoothed_losses = pd.Series(losses).rolling(20).mean()\n",
    "                    ax.plot(smoothed_losses, label=method, color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('Training Loss Evolution')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Loss')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        # 5. Q-values evolution\n",
    "        ax = axes[1, 1]\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'q_values_history') and agent.q_values_history:\n",
    "                q_values = agent.q_values_history\n",
    "                if len(q_values) > 10:\n",
    "                    smoothed_q = pd.Series(q_values).rolling(20).mean()\n",
    "                    ax.plot(smoothed_q, label=method, color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('Q-Values Evolution')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Average Q-Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Exploration analysis\n",
    "        ax = axes[1, 2]\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'epsilon_history') and agent.epsilon_history:\n",
    "                epsilons = agent.epsilon_history\n",
    "                if len(epsilons) > 10:\n",
    "                    ax.plot(epsilons, label=method, color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('Exploration Schedule (Epsilon)')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Epsilon')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 7. TD Error comparison\n",
    "        ax = axes[2, 0]\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'td_errors_history') and agent.td_errors_history:\n",
    "                td_errors = agent.td_errors_history\n",
    "                if len(td_errors) > 10:\n",
    "                    smoothed_td = pd.Series(td_errors).rolling(20).mean()\n",
    "                    ax.plot(smoothed_td, label=method, color=colors[i], linewidth=2)\n",
    "        \n",
    "        ax.set_title('TD Error Evolution')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Average TD Error')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 8. Value/Advantage analysis (for dueling methods)\n",
    "        ax = axes[2, 1]\n",
    "        dueling_methods = []\n",
    "        for method, data in results.items():\n",
    "            agent = data['agent']\n",
    "            if hasattr(agent, 'value_history') and agent.value_history:\n",
    "                dueling_methods.append((method, agent))\n",
    "        \n",
    "        if dueling_methods:\n",
    "            for i, (method, agent) in enumerate(dueling_methods):\n",
    "                values = agent.value_history\n",
    "                if len(values) > 10:\n",
    "                    smoothed_values = pd.Series(values).rolling(20).mean()\n",
    "                    ax.plot(smoothed_values, label=f'{method} Values', color=colors[i], linewidth=2)\n",
    "                \n",
    "                if hasattr(agent, 'advantage_history') and agent.advantage_history:\n",
    "                    advantages = agent.advantage_history\n",
    "                    if len(advantages) > 10:\n",
    "                        smoothed_adv = pd.Series(advantages).rolling(20).mean()\n",
    "                        ax.plot(smoothed_adv, label=f'{method} Advantages', \n",
    "                               color=colors[i], linewidth=2, linestyle='--')\n",
    "        \n",
    "        ax.set_title('Value/Advantage Decomposition')\n",
    "        ax.set_xlabel('Training Step')\n",
    "        ax.set_ylabel('Value')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 9. Performance summary table\n",
    "        ax = axes[2, 2]\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # Create performance table\n",
    "        table_data = []\n",
    "        headers = ['Method', 'Final Reward', 'Eval Mean', 'Eval Std', 'Sample Efficiency']\n",
    "        \n",
    "        for method, data in results.items():\n",
    "            final_perf = data['final_performance']\n",
    "            eval_mean = data['eval_performance']['mean_reward']\n",
    "            eval_std = data['eval_performance']['std_reward']\n",
    "            avg_steps = np.mean(data['steps'][-10:])\n",
    "            \n",
    "            table_data.append([\n",
    "                method,\n",
    "                f'{final_perf:.1f}',\n",
    "                f'{eval_mean:.1f}',\n",
    "                f'{eval_std:.1f}',\n",
    "                f'{avg_steps:.1f}'\n",
    "            ])\n",
    "        \n",
    "        table = ax.table(cellText=table_data, colLabels=headers, \n",
    "                        cellLoc='center', loc='center')\n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(9)\n",
    "        table.scale(1.2, 1.5)\n",
    "        \n",
    "        # Style the table\n",
    "        for i in range(len(headers)):\n",
    "            table[(0, i)].set_facecolor('#40466e')\n",
    "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        # Color code performance\n",
    "        best_eval = max([results[m]['eval_performance']['mean_reward'] for m in results.keys()])\n",
    "        for i, (method, data) in enumerate(results.items()):\n",
    "            eval_perf = data['eval_performance']['mean_reward']\n",
    "            if eval_perf == best_eval:\n",
    "                for j in range(len(headers)):\n",
    "                    table[(i+1, j)].set_facecolor('#90EE90')  # Light green\n",
    "        \n",
    "        ax.set_title('Performance Summary')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary statistics\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"PERFORMANCE SUMMARY\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        best_method = max(results.keys(), \n",
    "                         key=lambda k: results[k]['eval_performance']['mean_reward'])\n",
    "        best_score = results[best_method]['eval_performance']['mean_reward']\n",
    "        \n",
    "        print(f\"🏆 Best Method: {best_method}\")\n",
    "        print(f\"🏆 Best Score: {best_score:.1f} ± {results[best_method]['eval_performance']['std_reward']:.1f}\")\n",
    "        \n",
    "        print(f\"\\nMethod Rankings (by evaluation performance):\")\n",
    "        sorted_methods = sorted(results.items(), \n",
    "                              key=lambda x: x[1]['eval_performance']['mean_reward'], \n",
    "                              reverse=True)\n",
    "        \n",
    "        for i, (method, data) in enumerate(sorted_methods):\n",
    "            eval_perf = data['eval_performance']['mean_reward']\n",
    "            eval_std = data['eval_performance']['std_reward']\n",
    "            improvement = ((eval_perf / sorted_methods[-1][1]['eval_performance']['mean_reward'] - 1) * 100)\n",
    "            print(f\"  {i+1}. {method}: {eval_perf:.1f} ± {eval_std:.1f} (+{improvement:.1f}% vs baseline)\")\n",
    "\n",
    "# Run comprehensive Rainbow analysis\n",
    "rainbow_analyzer = RainbowAnalyzer()\n",
    "\n",
    "print(\"Running Comprehensive DQN Methods Comparison...\")\n",
    "print(\"This will compare Basic DQN, Double DQN, Dueling DQN, Prioritized DQN, and Rainbow DQN\")\n",
    "print(\"Training each method for comparison...\")\n",
    "\n",
    "comprehensive_results = rainbow_analyzer.compare_all_methods()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a548869",
   "metadata": {},
   "source": [
    "# Section 8: Performance Analysis and Ablation Studies\n",
    "\n",
    "## Understanding Component Contributions\n",
    "\n",
    "This section focuses on understanding which components of advanced DQN methods contribute most to performance improvements. We'll conduct systematic ablation studies to isolate the impact of individual components.\n",
    "\n",
    "## Key Analysis Areas\n",
    "\n",
    "1. **Individual Component Impact**: How much does each improvement contribute?\n",
    "2. **Component Interactions**: Do improvements combine synergistically?\n",
    "3. **Environment Sensitivity**: Which improvements work best in different environments?\n",
    "4. **Computational Overhead**: What's the cost-benefit trade-off?\n",
    "5. **Hyperparameter Sensitivity**: How robust are the improvements?\n",
    "\n",
    "## Theoretical Analysis\n",
    "\n",
    "### Sample Complexity\n",
    "Different improvements affect sample complexity in different ways:\n",
    "- **Experience Replay**: Reduces correlation, improves sample efficiency\n",
    "- **Double DQN**: Reduces bias, may require more samples initially but converges better\n",
    "- **Dueling**: Better value estimation, especially beneficial with many actions\n",
    "- **Prioritized Replay**: Focuses on important transitions, improves sample efficiency\n",
    "- **Multi-step**: Propagates rewards faster, reduces variance\n",
    "\n",
    "### Convergence Properties\n",
    "Each component affects convergence:\n",
    "- Target networks provide stability\n",
    "- Double DQN reduces overestimation bias\n",
    "- Prioritized replay can introduce bias but improves practical performance\n",
    "- Noisy networks provide consistent exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd217b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Performance Analysis and Ablation Studies\n",
    "\n",
    "class AblationAnalyzer:\n",
    "    \"\"\"\n",
    "    Systematic ablation study of DQN components\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results_cache = {}\n",
    "    \n",
    "    def rainbow_ablation_study(self):\n",
    "        \"\"\"\n",
    "        Systematic ablation study of Rainbow DQN components\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Rainbow DQN Ablation Study\")\n",
    "        print(\"=\"*70)\n",
    "        print(\"Testing individual component contributions...\")\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Define component combinations\n",
    "        configurations = {\n",
    "            'Baseline (Basic DQN)': {\n",
    "                'class': DQNAgent,\n",
    "                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3}\n",
    "            },\n",
    "            '+ Double': {\n",
    "                'class': DoubleDQNAgent,\n",
    "                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3}\n",
    "            },\n",
    "            '+ Double + Dueling': {\n",
    "                'class': DuelingDQNAgent,\n",
    "                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3, 'dueling_type': 'mean'}\n",
    "            },\n",
    "            '+ Double + Dueling + PER': {\n",
    "                'class': PrioritizedDQNAgent,\n",
    "                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3, \n",
    "                          'dueling_type': 'mean', 'alpha': 0.6, 'beta_start': 0.4}\n",
    "            },\n",
    "            'Rainbow (All Components)': {\n",
    "                'class': RainbowDQNAgent,\n",
    "                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3,\n",
    "                          'n_step': 3, 'alpha': 0.6, 'beta_start': 0.4, 'use_noisy': True}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Individual component tests\n",
    "        component_tests = {\n",
    "            'Only Double': {\n",
    "                'class': DoubleDQNAgent,\n",
    "                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3}\n",
    "            },\n",
    "            'Only Dueling': {\n",
    "                'class': DuelingDQNAgent,\n",
    "                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3, \n",
    "                          'dueling_type': 'mean'}\n",
    "            },\n",
    "            'Only PER': {\n",
    "                'class': PrioritizedDQNAgent,\n",
    "                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3,\n",
    "                          'alpha': 0.6, 'beta_start': 0.4}\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Run experiments\n",
    "        results = self._run_ablation_experiments(env, configurations, episodes=60)\n",
    "        individual_results = self._run_ablation_experiments(env, component_tests, episodes=60)\n",
    "        \n",
    "        # Combine results\n",
    "        all_results = {**results, **individual_results}\n",
    "        \n",
    "        # Analysis\n",
    "        self._analyze_component_contributions(all_results)\n",
    "        \n",
    "        env.close()\n",
    "        return all_results\n",
    "    \n",
    "    def _run_ablation_experiments(self, env, configurations, episodes=50):\n",
    "        \"\"\"Run experiments for given configurations\"\"\"\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for config_name, config in configurations.items():\n",
    "            print(f\"\\nTesting: {config_name}\")\n",
    "            \n",
    "            agent = config['class'](**config['params'])\n",
    "            \n",
    "            episode_rewards = []\n",
    "            training_times = []\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            for episode in range(episodes):\n",
    "                episode_start = time.time()\n",
    "                reward, steps = agent.train_episode(env, max_steps=500)\n",
    "                episode_time = time.time() - episode_start\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                training_times.append(episode_time)\n",
    "                \n",
    "                if (episode + 1) % 15 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-5:])\n",
    "                    print(f\"  Episode {episode+1}: Reward = {avg_reward:.1f}\")\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            \n",
    "            # Evaluation\n",
    "            eval_results = agent.evaluate(env, num_episodes=10)\n",
    "            \n",
    "            results[config_name] = {\n",
    "                'agent': agent,\n",
    "                'rewards': episode_rewards,\n",
    "                'training_times': training_times,\n",
    "                'total_time': total_time,\n",
    "                'eval_performance': eval_results,\n",
    "                'final_performance': np.mean(episode_rewards[-10:]),\n",
    "                'convergence_episode': self._find_convergence_episode(episode_rewards),\n",
    "                'sample_efficiency': np.mean(episode_rewards[-5:]) / episodes\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _find_convergence_episode(self, rewards, threshold=0.95, window=10):\n",
    "        \"\"\"Find when agent converges to threshold of final performance\"\"\"\n",
    "        if len(rewards) < window:\n",
    "            return len(rewards)\n",
    "        \n",
    "        final_performance = np.mean(rewards[-window:])\n",
    "        target = threshold * final_performance\n",
    "        \n",
    "        for i in range(window, len(rewards)):\n",
    "            if np.mean(rewards[i-window:i]) >= target:\n",
    "                return i\n",
    "        \n",
    "        return len(rewards)\n",
    "    \n",
    "    def _analyze_component_contributions(self, results):\n",
    "        \"\"\"Analyze individual component contributions\"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"COMPONENT CONTRIBUTION ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        baseline_performance = results['Baseline (Basic DQN)']['eval_performance']['mean_reward']\n",
    "        \n",
    "        # Calculate improvements\n",
    "        improvements = {}\n",
    "        for name, data in results.items():\n",
    "            if name != 'Baseline (Basic DQN)':\n",
    "                current_perf = data['eval_performance']['mean_reward']\n",
    "                improvement = ((current_perf / baseline_performance - 1) * 100)\n",
    "                improvements[name] = {\n",
    "                    'absolute': current_perf,\n",
    "                    'improvement': improvement,\n",
    "                    'convergence': data['convergence_episode'],\n",
    "                    'efficiency': data['sample_efficiency']\n",
    "                }\n",
    "        \n",
    "        # Print analysis\n",
    "        print(f\"Baseline Performance: {baseline_performance:.1f}\")\n",
    "        print(f\"\\nComponent Contributions:\")\n",
    "        \n",
    "        for name, stats in improvements.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            print(f\"  Performance: {stats['absolute']:.1f} (+{stats['improvement']:.1f}%)\")\n",
    "            print(f\"  Convergence: Episode {stats['convergence']}\")\n",
    "            print(f\"  Sample Efficiency: {stats['efficiency']:.3f}\")\n",
    "        \n",
    "        # Visualize contributions\n",
    "        self._visualize_ablation_results(results, improvements)\n",
    "    \n",
    "    def _visualize_ablation_results(self, results, improvements):\n",
    "        \"\"\"Visualize ablation study results\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "        \n",
    "        # 1. Performance progression\n",
    "        ax = axes[0, 0]\n",
    "        cumulative_configs = [\n",
    "            'Baseline (Basic DQN)',\n",
    "            '+ Double',\n",
    "            '+ Double + Dueling', \n",
    "            '+ Double + Dueling + PER',\n",
    "            'Rainbow (All Components)'\n",
    "        ]\n",
    "        \n",
    "        performances = [results[config]['eval_performance']['mean_reward'] \n",
    "                       for config in cumulative_configs if config in results]\n",
    "        config_names = [config for config in cumulative_configs if config in results]\n",
    "        \n",
    "        bars = ax.bar(range(len(performances)), performances, \n",
    "                     color=['red', 'orange', 'yellow', 'lightgreen', 'green'][:len(performances)])\n",
    "        ax.set_title('Cumulative Component Addition')\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Evaluation Performance')\n",
    "        ax.set_xticks(range(len(config_names)))\n",
    "        ax.set_xticklabels(config_names, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, perf) in enumerate(zip(bars, performances)):\n",
    "            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                   f'{perf:.1f}', ha='center', va='bottom')\n",
    "        \n",
    "        # 2. Individual component impact\n",
    "        ax = axes[0, 1]\n",
    "        individual_components = ['Only Double', 'Only Dueling', 'Only PER']\n",
    "        individual_perfs = []\n",
    "        baseline_perf = results['Baseline (Basic DQN)']['eval_performance']['mean_reward']\n",
    "        \n",
    "        for comp in individual_components:\n",
    "            if comp in results:\n",
    "                individual_perfs.append(results[comp]['eval_performance']['mean_reward'])\n",
    "        \n",
    "        if individual_perfs:\n",
    "            improvements_pct = [(perf - baseline_perf) / baseline_perf * 100 \n",
    "                               for perf in individual_perfs]\n",
    "            \n",
    "            bars = ax.bar(range(len(improvements_pct)), improvements_pct, \n",
    "                         color=['blue', 'purple', 'orange'][:len(improvements_pct)])\n",
    "            ax.set_title('Individual Component Impact')\n",
    "            ax.set_xlabel('Component')\n",
    "            ax.set_ylabel('Performance Improvement (%)')\n",
    "            ax.set_xticks(range(len(individual_components)))\n",
    "            ax.set_xticklabels([comp.replace('Only ', '') for comp in individual_components])\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add value labels\n",
    "            for bar, imp in zip(bars, improvements_pct):\n",
    "                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                       f'{imp:.1f}%', ha='center', va='bottom')\n",
    "        \n",
    "        # 3. Learning curves comparison\n",
    "        ax = axes[0, 2]\n",
    "        colors = ['red', 'orange', 'yellow', 'lightgreen', 'green', 'blue', 'purple', 'brown']\n",
    "        \n",
    "        for i, (name, data) in enumerate(results.items()):\n",
    "            rewards = data['rewards']\n",
    "            smoothed = pd.Series(rewards).rolling(5).mean()\n",
    "            ax.plot(smoothed, label=name, color=colors[i % len(colors)], linewidth=2)\n",
    "        \n",
    "        ax.set_title('Learning Curves Comparison')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Episode Reward (Smoothed)')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Convergence analysis\n",
    "        ax = axes[1, 0]\n",
    "        convergence_episodes = []\n",
    "        config_names_conv = []\n",
    "        \n",
    "        for name, data in results.items():\n",
    "            convergence_episodes.append(data['convergence_episode'])\n",
    "            config_names_conv.append(name.replace(' (All Components)', '').replace('+ Double + Dueling + PER', 'Full'))\n",
    "        \n",
    "        bars = ax.bar(range(len(convergence_episodes)), convergence_episodes, \n",
    "                     color=colors[:len(convergence_episodes)])\n",
    "        ax.set_title('Convergence Speed')\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Episodes to Convergence')\n",
    "        ax.set_xticks(range(len(config_names_conv)))\n",
    "        ax.set_xticklabels(config_names_conv, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 5. Training time analysis\n",
    "        ax = axes[1, 1]\n",
    "        training_times = [data['total_time'] for data in results.values()]\n",
    "        config_names_time = [name.split('(')[0].strip() for name in results.keys()]\n",
    "        \n",
    "        bars = ax.bar(range(len(training_times)), training_times, \n",
    "                     color=colors[:len(training_times)])\n",
    "        ax.set_title('Training Time Comparison')\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Total Training Time (s)')\n",
    "        ax.set_xticks(range(len(config_names_time)))\n",
    "        ax.set_xticklabels(config_names_time, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Efficiency scatter plot\n",
    "        ax = axes[1, 2]\n",
    "        final_performances = [data['eval_performance']['mean_reward'] for data in results.values()]\n",
    "        training_times_norm = [data['total_time'] for data in results.values()]\n",
    "        \n",
    "        scatter = ax.scatter(training_times_norm, final_performances, \n",
    "                           c=range(len(results)), cmap='viridis', s=100)\n",
    "        \n",
    "        # Add labels\n",
    "        for i, name in enumerate(results.keys()):\n",
    "            short_name = name.replace('+ Double + Dueling + PER', 'Full').split('(')[0].strip()\n",
    "            ax.annotate(short_name, (training_times_norm[i], final_performances[i]),\n",
    "                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        ax.set_title('Performance vs Training Time')\n",
    "        ax.set_xlabel('Training Time (s)')\n",
    "        ax.set_ylabel('Final Performance')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def hyperparameter_sensitivity_analysis(self):\n",
    "        \"\"\"Analyze sensitivity to hyperparameters\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Hyperparameter Sensitivity Analysis\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Test different hyperparameters for Rainbow DQN\n",
    "        hyperparameter_configs = {\n",
    "            'Default': {\n",
    "                'lr': 1e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n",
    "                'target_update_freq': 100, 'buffer_size': 10000\n",
    "            },\n",
    "            'High Learning Rate': {\n",
    "                'lr': 5e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n",
    "                'target_update_freq': 100, 'buffer_size': 10000\n",
    "            },\n",
    "            'Low Learning Rate': {\n",
    "                'lr': 1e-4, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n",
    "                'target_update_freq': 100, 'buffer_size': 10000\n",
    "            },\n",
    "            'High Priority Alpha': {\n",
    "                'lr': 1e-3, 'alpha': 0.8, 'beta_start': 0.4, 'n_step': 3,\n",
    "                'target_update_freq': 100, 'buffer_size': 10000\n",
    "            },\n",
    "            'Low Priority Alpha': {\n",
    "                'lr': 1e-3, 'alpha': 0.4, 'beta_start': 0.4, 'n_step': 3,\n",
    "                'target_update_freq': 100, 'buffer_size': 10000\n",
    "            },\n",
    "            'Long N-Step': {\n",
    "                'lr': 1e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 5,\n",
    "                'target_update_freq': 100, 'buffer_size': 10000\n",
    "            },\n",
    "            'Frequent Target Update': {\n",
    "                'lr': 1e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n",
    "                'target_update_freq': 50, 'buffer_size': 10000\n",
    "            },\n",
    "            'Infrequent Target Update': {\n",
    "                'lr': 1e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n",
    "                'target_update_freq': 200, 'buffer_size': 10000\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for config_name, params in hyperparameter_configs.items():\n",
    "            print(f\"\\nTesting: {config_name}\")\n",
    "            \n",
    "            agent = RainbowDQNAgent(\n",
    "                state_dim=state_dim,\n",
    "                action_dim=action_dim,\n",
    "                use_noisy=True,\n",
    "                **params\n",
    "            )\n",
    "            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            for episode in range(40):\n",
    "                reward, _ = agent.train_episode(env, max_steps=500)\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if (episode + 1) % 10 == 0:\n",
    "                    avg_reward = np.mean(episode_rewards[-5:])\n",
    "                    print(f\"  Episode {episode+1}: Reward = {avg_reward:.1f}\")\n",
    "            \n",
    "            eval_results = agent.evaluate(env, num_episodes=10)\n",
    "            \n",
    "            results[config_name] = {\n",
    "                'rewards': episode_rewards,\n",
    "                'eval_performance': eval_results,\n",
    "                'params': params\n",
    "            }\n",
    "        \n",
    "        # Visualize sensitivity\n",
    "        self._visualize_sensitivity_analysis(results)\n",
    "        \n",
    "        env.close()\n",
    "        return results\n",
    "    \n",
    "    def _visualize_sensitivity_analysis(self, results):\n",
    "        \"\"\"Visualize hyperparameter sensitivity\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Performance comparison\n",
    "        ax = axes[0, 0]\n",
    "        config_names = list(results.keys())\n",
    "        performances = [results[name]['eval_performance']['mean_reward'] for name in config_names]\n",
    "        \n",
    "        bars = ax.bar(range(len(performances)), performances)\n",
    "        ax.set_title('Hyperparameter Sensitivity')\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Evaluation Performance')\n",
    "        ax.set_xticks(range(len(config_names)))\n",
    "        ax.set_xticklabels(config_names, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight default\n",
    "        for i, name in enumerate(config_names):\n",
    "            if name == 'Default':\n",
    "                bars[i].set_color('green')\n",
    "                bars[i].set_alpha(0.8)\n",
    "        \n",
    "        # 2. Learning curves\n",
    "        ax = axes[0, 1]\n",
    "        colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "        \n",
    "        for i, (name, data) in enumerate(results.items()):\n",
    "            rewards = data['rewards']\n",
    "            smoothed = pd.Series(rewards).rolling(3).mean()\n",
    "            \n",
    "            if name == 'Default':\n",
    "                ax.plot(smoothed, label=name, color='green', linewidth=3)\n",
    "            else:\n",
    "                ax.plot(smoothed, label=name, color=colors[i], linewidth=2, alpha=0.7)\n",
    "        \n",
    "        ax.set_title('Learning Curves')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Episode Reward (Smoothed)')\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Parameter impact analysis\n",
    "        ax = axes[1, 0]\n",
    "        default_performance = results['Default']['eval_performance']['mean_reward']\n",
    "        \n",
    "        param_impacts = {}\n",
    "        for name, data in results.items():\n",
    "            if name != 'Default':\n",
    "                performance = data['eval_performance']['mean_reward']\n",
    "                impact = (performance - default_performance) / default_performance * 100\n",
    "                param_impacts[name] = impact\n",
    "        \n",
    "        names = list(param_impacts.keys())\n",
    "        impacts = list(param_impacts.values())\n",
    "        colors = ['red' if imp < 0 else 'green' for imp in impacts]\n",
    "        \n",
    "        bars = ax.bar(range(len(impacts)), impacts, color=colors, alpha=0.7)\n",
    "        ax.set_title('Impact vs Default Configuration')\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Performance Change (%)')\n",
    "        ax.set_xticks(range(len(names)))\n",
    "        ax.set_xticklabels(names, rotation=45, ha='right')\n",
    "        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Stability analysis (coefficient of variation)\n",
    "        ax = axes[1, 1]\n",
    "        stabilities = []\n",
    "        config_names_stab = []\n",
    "        \n",
    "        for name, data in results.items():\n",
    "            rewards = data['rewards']\n",
    "            cv = np.std(rewards[-10:]) / np.mean(rewards[-10:])  # Coefficient of variation\n",
    "            stabilities.append(cv)\n",
    "            config_names_stab.append(name)\n",
    "        \n",
    "        bars = ax.bar(range(len(stabilities)), stabilities)\n",
    "        ax.set_title('Training Stability (Lower = More Stable)')\n",
    "        ax.set_xlabel('Configuration')\n",
    "        ax.set_ylabel('Coefficient of Variation')\n",
    "        ax.set_xticks(range(len(config_names_stab)))\n",
    "        ax.set_xticklabels(config_names_stab, rotation=45, ha='right')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight most stable\n",
    "        min_cv_idx = np.argmin(stabilities)\n",
    "        bars[min_cv_idx].set_color('green')\n",
    "        bars[min_cv_idx].set_alpha(0.8)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"SENSITIVITY ANALYSIS SUMMARY\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        default_perf = results['Default']['eval_performance']['mean_reward']\n",
    "        print(f\"Default Performance: {default_perf:.1f}\")\n",
    "        \n",
    "        print(\"\\nBest Alternative Configurations:\")\n",
    "        sorted_configs = sorted([(name, data['eval_performance']['mean_reward']) \n",
    "                               for name, data in results.items() if name != 'Default'],\n",
    "                              key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for i, (name, perf) in enumerate(sorted_configs[:3]):\n",
    "            improvement = (perf - default_perf) / default_perf * 100\n",
    "            print(f\"  {i+1}. {name}: {perf:.1f} ({improvement:+.1f}%)\")\n",
    "\n",
    "# Run comprehensive performance analysis\n",
    "performance_analyzer = AblationAnalyzer()\n",
    "\n",
    "print(\"Starting Comprehensive Performance Analysis...\")\n",
    "print(\"\\n1. Running Rainbow DQN Ablation Study...\")\n",
    "ablation_results = performance_analyzer.rainbow_ablation_study()\n",
    "\n",
    "print(\"\\n2. Running Hyperparameter Sensitivity Analysis...\")\n",
    "sensitivity_results = performance_analyzer.hyperparameter_sensitivity_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c629c",
   "metadata": {},
   "source": [
    "# Section 9: Advanced Topics and Future Directions\n",
    "\n",
    "## Beyond Rainbow DQN\n",
    "\n",
    "While Rainbow DQN represents a significant advancement in value-based reinforcement learning, the field continues to evolve. This section explores cutting-edge developments and future research directions.\n",
    "\n",
    "## Recent Advances\n",
    "\n",
    "### 1. Implicit Quantile Networks (IQN)\n",
    "**Key Idea**: Learn the full return distribution implicitly through quantile regression\n",
    "**Advantages**: \n",
    "- No need to pre-specify support for return distribution\n",
    "- Better risk-sensitive policies\n",
    "- Improved performance on complex environments\n",
    "\n",
    "### 2. Agent57\n",
    "**Key Idea**: Combine value-based and policy-based methods with meta-learning\n",
    "**Components**:\n",
    "- Never Give Up (NGU) exploration\n",
    "- Population-based training\n",
    "- Universal value functions\n",
    "\n",
    "### 3. MuZero\n",
    "**Key Idea**: Model-based planning with learned environment models\n",
    "**Innovation**: Plans in learned latent space rather than raw observations\n",
    "\n",
    "## Theoretical Understanding\n",
    "\n",
    "### Overestimation Bias Analysis\n",
    "Recent theoretical work has provided deeper insights into:\n",
    "- Sources of overestimation bias in Q-learning\n",
    "- Conditions under which Double DQN provably reduces bias\n",
    "- Trade-offs between bias and variance\n",
    "\n",
    "### Sample Complexity Bounds\n",
    "New theoretical results establish:\n",
    "- PAC bounds for deep Q-learning\n",
    "- Role of function approximation in sample complexity\n",
    "- Conditions for polynomial sample complexity\n",
    "\n",
    "## Implementation Considerations\n",
    "\n",
    "### Distributed Training\n",
    "Modern implementations leverage:\n",
    "- Distributed experience collection (Ape-X)\n",
    "- Asynchronous training (A3C-style)\n",
    "- GPU acceleration for neural network training\n",
    "\n",
    "### Architecture Innovations\n",
    "Recent architectural advances include:\n",
    "- Attention mechanisms in value networks\n",
    "- Graph neural networks for structured observations\n",
    "- Meta-learning architectures\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "### Real-World Deployment\n",
    "Challenges and solutions for deploying DQN in practice:\n",
    "- Safety constraints and safe exploration\n",
    "- Transfer learning and domain adaptation\n",
    "- Robustness to distribution shift\n",
    "\n",
    "### Multi-Task Learning\n",
    "Extensions for learning multiple tasks:\n",
    "- Universal value functions\n",
    "- Task-conditional networks\n",
    "- Meta-learning approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd151911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Topics and Future Research Implementations\n",
    "\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "class AdvancedDQNAnalyzer:\n",
    "    \"\"\"\n",
    "    Analysis of advanced DQN concepts and future directions\n",
    "    \"\"\"\n",
    "    \n",
    "    def demonstrate_overestimation_sources(self):\n",
    "        \"\"\"Demonstrate different sources of overestimation bias\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Sources of Overestimation Bias in Q-Learning\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Create synthetic environment to demonstrate bias sources\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Simulate Q-values for different scenarios\n",
    "        n_actions = 4\n",
    "        n_states = 100\n",
    "        true_q_values = np.random.randn(n_states, n_actions)\n",
    "        \n",
    "        # Source 1: Function approximation error\n",
    "        approximation_noise = 0.2\n",
    "        approx_q_values = true_q_values + np.random.normal(0, approximation_noise, true_q_values.shape)\n",
    "        \n",
    "        # Source 2: Bootstrap error (temporal difference)\n",
    "        bootstrap_noise = 0.3\n",
    "        bootstrap_errors = np.random.normal(0, bootstrap_noise, n_states)\n",
    "        \n",
    "        # Source 3: Maximization bias\n",
    "        max_true = np.max(true_q_values, axis=1)\n",
    "        max_approx = np.max(approx_q_values, axis=1)\n",
    "        max_bias = max_approx - max_true\n",
    "        \n",
    "        # Visualization\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Function approximation bias\n",
    "        ax = axes[0, 0]\n",
    "        ax.scatter(true_q_values.flatten(), approx_q_values.flatten(), alpha=0.5)\n",
    "        ax.plot([true_q_values.min(), true_q_values.max()], \n",
    "                [true_q_values.min(), true_q_values.max()], 'r--', label='Perfect Approximation')\n",
    "        ax.set_xlabel('True Q-Values')\n",
    "        ax.set_ylabel('Approximate Q-Values')\n",
    "        ax.set_title('Function Approximation Bias')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate and show bias\n",
    "        approx_bias = np.mean(approx_q_values.flatten() - true_q_values.flatten())\n",
    "        ax.text(0.05, 0.95, f'Mean Bias: {approx_bias:.3f}', \n",
    "                transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n",
    "        \n",
    "        # 2. Maximization bias\n",
    "        ax = axes[0, 1]\n",
    "        ax.hist(max_bias, bins=20, alpha=0.7, edgecolor='black')\n",
    "        ax.axvline(np.mean(max_bias), color='red', linestyle='--', \n",
    "                  label=f'Mean Bias: {np.mean(max_bias):.3f}')\n",
    "        ax.set_xlabel('Maximization Bias (Approx Max - True Max)')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.set_title('Maximization Bias Distribution')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Bias over time (simulated)\n",
    "        ax = axes[1, 0]\n",
    "        time_steps = np.arange(1000)\n",
    "        \n",
    "        # Simulate accumulating bias\n",
    "        bias_accumulation = np.cumsum(np.random.normal(0.01, 0.05, 1000))  # Slight positive bias\n",
    "        double_dqn_bias = np.cumsum(np.random.normal(0.002, 0.04, 1000))   # Reduced bias\n",
    "        \n",
    "        ax.plot(time_steps, bias_accumulation, label='Standard DQN', linewidth=2)\n",
    "        ax.plot(time_steps, double_dqn_bias, label='Double DQN', linewidth=2)\n",
    "        ax.set_xlabel('Training Steps')\n",
    "        ax.set_ylabel('Cumulative Bias')\n",
    "        ax.set_title('Bias Accumulation Over Time')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Bias sources summary\n",
    "        ax = axes[1, 1]\n",
    "        ax.axis('off')\n",
    "        \n",
    "        bias_sources = [\n",
    "            \"1. Function Approximation Error\",\n",
    "            \"   • Neural network approximation\",\n",
    "            \"   • Limited capacity\",\n",
    "            \"   • Training dynamics\",\n",
    "            \"\",\n",
    "            \"2. Bootstrapping Error\", \n",
    "            \"   • TD target estimation\",\n",
    "            \"   • Recursive value updates\",\n",
    "            \"   • Error propagation\",\n",
    "            \"\",\n",
    "            \"3. Maximization Bias\",\n",
    "            \"   • Max operator over noisy estimates\",\n",
    "            \"   • Upward bias in action selection\",\n",
    "            \"   • Compounding over time\",\n",
    "            \"\",\n",
    "            \"Solutions:\",\n",
    "            \"• Double DQN: Separate action selection/evaluation\",\n",
    "            \"• Target networks: Stable bootstrap targets\",\n",
    "            \"• Better function approximation\"\n",
    "        ]\n",
    "        \n",
    "        y_pos = 0.95\n",
    "        for line in bias_sources:\n",
    "            if line.startswith(\"   •\"):\n",
    "                ax.text(0.1, y_pos, line, transform=ax.transAxes, fontsize=10, color='blue')\n",
    "            elif line.startswith(\"Solutions:\"):\n",
    "                ax.text(0.05, y_pos, line, transform=ax.transAxes, fontsize=12, \n",
    "                       weight='bold', color='green')\n",
    "            elif line.startswith(\"•\"):\n",
    "                ax.text(0.1, y_pos, line, transform=ax.transAxes, fontsize=10, color='green')\n",
    "            elif line and not line.startswith(\" \"):\n",
    "                ax.text(0.05, y_pos, line, transform=ax.transAxes, fontsize=12, weight='bold')\n",
    "            else:\n",
    "                ax.text(0.05, y_pos, line, transform=ax.transAxes, fontsize=10)\n",
    "            y_pos -= 0.05\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nBias Analysis Results:\")\n",
    "        print(f\"  Function Approximation Bias: {approx_bias:.4f}\")\n",
    "        print(f\"  Maximization Bias (mean): {np.mean(max_bias):.4f}\")\n",
    "        print(f\"  Maximization Bias (std): {np.std(max_bias):.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'approximation_bias': approx_bias,\n",
    "            'maximization_bias_mean': np.mean(max_bias),\n",
    "            'maximization_bias_std': np.std(max_bias)\n",
    "        }\n",
    "    \n",
    "    def visualize_dqn_evolution_timeline(self):\n",
    "        \"\"\"Visualize the evolution of DQN methods over time\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"DQN Evolution Timeline\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 10))\n",
    "        \n",
    "        # Timeline data\n",
    "        methods = [\n",
    "            {\"name\": \"Q-Learning\", \"year\": 1989, \"description\": \"Tabular value iteration\", \"color\": \"lightblue\"},\n",
    "            {\"name\": \"DQN\", \"year\": 2013, \"description\": \"Neural network function approximation\\n+ Experience Replay\", \"color\": \"blue\"},\n",
    "            {\"name\": \"Double DQN\", \"year\": 2015, \"description\": \"Reduced overestimation bias\", \"color\": \"green\"},\n",
    "            {\"name\": \"Dueling DQN\", \"year\": 2016, \"description\": \"Value/advantage decomposition\", \"color\": \"orange\"},\n",
    "            {\"name\": \"Prioritized ER\", \"year\": 2016, \"description\": \"Priority-based experience sampling\", \"color\": \"red\"},\n",
    "            {\"name\": \"Rainbow DQN\", \"year\": 2017, \"description\": \"Combination of improvements\\n+ Distributional RL\", \"color\": \"purple\"},\n",
    "            {\"name\": \"IQN\", \"year\": 2018, \"description\": \"Implicit Quantile Networks\", \"color\": \"brown\"},\n",
    "            {\"name\": \"Agent57\", \"year\": 2020, \"description\": \"Meta-learning + exploration\", \"color\": \"pink\"},\n",
    "            {\"name\": \"MuZero\", \"year\": 2020, \"description\": \"Model-based planning in latent space\", \"color\": \"gold\"}\n",
    "        ]\n",
    "        \n",
    "        # Plot timeline\n",
    "        years = [method[\"year\"] for method in methods]\n",
    "        y_positions = np.arange(len(methods))\n",
    "        \n",
    "        # Create the timeline\n",
    "        for i, method in enumerate(methods):\n",
    "            # Draw the point\n",
    "            ax.scatter(method[\"year\"], i, s=200, c=method[\"color\"], zorder=3, alpha=0.8)\n",
    "            \n",
    "            # Draw connecting line\n",
    "            if i > 0:\n",
    "                ax.plot([methods[i-1][\"year\"], method[\"year\"]], [i-1, i], \n",
    "                       'k--', alpha=0.3, zorder=1)\n",
    "            \n",
    "            # Add method name and description\n",
    "            ax.text(method[\"year\"] + 0.5, i, f\"{method['name']}\\n{method['description']}\", \n",
    "                   fontsize=10, verticalalignment='center', \n",
    "                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=method[\"color\"], alpha=0.3))\n",
    "        \n",
    "        # Formatting\n",
    "        ax.set_yticks(y_positions)\n",
    "        ax.set_yticklabels([method[\"name\"] for method in methods])\n",
    "        ax.set_xlabel(\"Year\", fontsize=12)\n",
    "        ax.set_title(\"Evolution of Deep Q-Learning Methods\", fontsize=16, weight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.set_xlim(1985, 2025)\n",
    "        \n",
    "        # Add key innovations annotations\n",
    "        innovations = [\n",
    "            {\"year\": 2013, \"innovation\": \"Deep Learning\\nRevolution\", \"y_offset\": 0.3},\n",
    "            {\"year\": 2016, \"innovation\": \"Multiple\\nImprovements\", \"y_offset\": -0.3},\n",
    "            {\"year\": 2017, \"innovation\": \"Integration\\nEra\", \"y_offset\": 0.3},\n",
    "            {\"year\": 2020, \"innovation\": \"Beyond\\nValue-Based\", \"y_offset\": -0.3}\n",
    "        ]\n",
    "        \n",
    "        for inn in innovations:\n",
    "            ax.annotate(inn[\"innovation\"], xy=(inn[\"year\"], len(methods)/2), \n",
    "                       xytext=(inn[\"year\"], len(methods)/2 + inn[\"y_offset\"] * len(methods)),\n",
    "                       arrowprops=dict(arrowstyle='->', alpha=0.5),\n",
    "                       fontsize=9, ha='center', weight='bold', color='darkred')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def future_research_directions(self):\n",
    "        \"\"\"Discuss and visualize future research directions\"\"\"\n",
    "        \n",
    "        print(\"=\"*70)\n",
    "        print(\"Future Research Directions in Value-Based RL\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Research Areas Network\n",
    "        ax = axes[0, 0]\n",
    "        ax.set_xlim(0, 10)\n",
    "        ax.set_ylim(0, 10)\n",
    "        \n",
    "        # Define research areas and connections\n",
    "        areas = {\n",
    "            \"Safety & Robustness\": {\"pos\": (2, 8), \"color\": \"red\"},\n",
    "            \"Sample Efficiency\": {\"pos\": (8, 8), \"color\": \"blue\"},\n",
    "            \"Multi-Task Learning\": {\"pos\": (2, 5), \"color\": \"green\"},\n",
    "            \"Theoretical Understanding\": {\"pos\": (8, 5), \"color\": \"orange\"},\n",
    "            \"Distributed Learning\": {\"pos\": (2, 2), \"color\": \"purple\"},\n",
    "            \"Real-World Applications\": {\"pos\": (8, 2), \"color\": \"brown\"},\n",
    "            \"Model-Based Integration\": {\"pos\": (5, 6.5), \"color\": \"pink\"},\n",
    "            \"Meta-Learning\": {\"pos\": (5, 3.5), \"color\": \"gray\"}\n",
    "        }\n",
    "        \n",
    "        # Draw connections\n",
    "        connections = [\n",
    "            (\"Safety & Robustness\", \"Real-World Applications\"),\n",
    "            (\"Sample Efficiency\", \"Theoretical Understanding\"),\n",
    "            (\"Multi-Task Learning\", \"Meta-Learning\"),\n",
    "            (\"Distributed Learning\", \"Sample Efficiency\"),\n",
    "            (\"Model-Based Integration\", \"Sample Efficiency\"),\n",
    "            (\"Meta-Learning\", \"Real-World Applications\")\n",
    "        ]\n",
    "        \n",
    "        for area1, area2 in connections:\n",
    "            pos1 = areas[area1][\"pos\"]\n",
    "            pos2 = areas[area2][\"pos\"]\n",
    "            ax.plot([pos1[0], pos2[0]], [pos1[1], pos2[1]], 'k--', alpha=0.3)\n",
    "        \n",
    "        # Draw areas\n",
    "        for area, props in areas.items():\n",
    "            x, y = props[\"pos\"]\n",
    "            circle = plt.Circle((x, y), 0.8, color=props[\"color\"], alpha=0.6)\n",
    "            ax.add_patch(circle)\n",
    "            ax.text(x, y, area, ha='center', va='center', fontsize=9, \n",
    "                   weight='bold', wrap=True)\n",
    "        \n",
    "        ax.set_title(\"Future Research Network\", fontsize=14, weight='bold')\n",
    "        ax.set_aspect('equal')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        # 2. Timeline of Expected Developments\n",
    "        ax = axes[0, 1]\n",
    "        \n",
    "        future_timeline = [\n",
    "            {\"year\": 2024, \"development\": \"Improved Safety Guarantees\", \"probability\": 0.8},\n",
    "            {\"year\": 2025, \"development\": \"Better Theoretical Bounds\", \"probability\": 0.7},\n",
    "            {\"year\": 2026, \"development\": \"Large-Scale Multi-Task RL\", \"probability\": 0.6},\n",
    "            {\"year\": 2027, \"development\": \"Real-World Deployment\", \"probability\": 0.5},\n",
    "            {\"year\": 2028, \"development\": \"Human-Level Sample Efficiency\", \"probability\": 0.4},\n",
    "            {\"year\": 2030, \"development\": \"General Value Learning\", \"probability\": 0.3}\n",
    "        ]\n",
    "        \n",
    "        years = [item[\"year\"] for item in future_timeline]\n",
    "        probabilities = [item[\"probability\"] for item in future_timeline]\n",
    "        developments = [item[\"development\"] for item in future_timeline]\n",
    "        \n",
    "        bars = ax.barh(developments, probabilities, color=plt.cm.viridis(probabilities))\n",
    "        ax.set_xlabel(\"Estimated Probability\")\n",
    "        ax.set_title(\"Future Developments Timeline\", fontsize=14, weight='bold')\n",
    "        ax.set_xlim(0, 1)\n",
    "        \n",
    "        # Add year labels\n",
    "        for i, (dev, prob, year) in enumerate(zip(developments, probabilities, years)):\n",
    "            ax.text(prob + 0.02, i, f\"{year}\", va='center', fontsize=9)\n",
    "        \n",
    "        # 3. Challenges and Solutions Matrix\n",
    "        ax = axes[1, 0]\n",
    "        \n",
    "        challenges = [\"Sample Efficiency\", \"Safety\", \"Generalization\", \"Scalability\", \"Interpretability\"]\n",
    "        solutions = [\"Meta-Learning\", \"Constrained RL\", \"Transfer Learning\", \"Distributed Training\", \"Attention Mechanisms\"]\n",
    "        \n",
    "        # Create impact matrix (higher values = stronger relationship)\n",
    "        impact_matrix = np.array([\n",
    "            [0.9, 0.3, 0.7, 0.6, 0.4],  # Sample Efficiency\n",
    "            [0.2, 0.9, 0.4, 0.3, 0.5],  # Safety\n",
    "            [0.8, 0.3, 0.9, 0.4, 0.3],  # Generalization\n",
    "            [0.4, 0.2, 0.5, 0.9, 0.3],  # Scalability\n",
    "            [0.3, 0.6, 0.4, 0.3, 0.8]   # Interpretability\n",
    "        ])\n",
    "        \n",
    "        im = ax.imshow(impact_matrix, cmap='RdYlBu_r', aspect='auto')\n",
    "        \n",
    "        # Add labels\n",
    "        ax.set_xticks(range(len(solutions)))\n",
    "        ax.set_yticks(range(len(challenges)))\n",
    "        ax.set_xticklabels(solutions, rotation=45, ha='right')\n",
    "        ax.set_yticklabels(challenges)\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(challenges)):\n",
    "            for j in range(len(solutions)):\n",
    "                ax.text(j, i, f'{impact_matrix[i, j]:.1f}', \n",
    "                       ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        ax.set_title(\"Challenges vs Solutions Impact Matrix\", fontsize=14, weight='bold')\n",
    "        plt.colorbar(im, ax=ax, label='Impact Score')\n",
    "        \n",
    "        # 4. Key Research Questions\n",
    "        ax = axes[1, 1]\n",
    "        ax.axis('off')\n",
    "        \n",
    "        research_questions = [\n",
    "            \"🔬 How can we provide theoretical guarantees for deep RL?\",\n",
    "            \"\",\n",
    "            \"🛡️ How do we ensure safety in continuous learning systems?\",\n",
    "            \"\",\n",
    "            \"🧠 Can we achieve human-level sample efficiency?\",\n",
    "            \"\",\n",
    "            \"🌐 How do we scale RL to real-world complexity?\",\n",
    "            \"\",\n",
    "            \"🔄 How can we enable lifelong learning without forgetting?\",\n",
    "            \"\",\n",
    "            \"🤖 How do we make RL systems interpretable and trustworthy?\",\n",
    "            \"\",\n",
    "            \"🌟 What are the fundamental limits of value-based methods?\"\n",
    "        ]\n",
    "        \n",
    "        y_pos = 0.95\n",
    "        for question in research_questions:\n",
    "            if question:\n",
    "                ax.text(0.05, y_pos, question, transform=ax.transAxes, fontsize=11, \n",
    "                       weight='bold' if question.startswith('🔬') or question.startswith('🛡️') \n",
    "                              or question.startswith('🧠') or question.startswith('🌐') \n",
    "                              or question.startswith('🔄') or question.startswith('🤖') \n",
    "                              or question.startswith('🌟') else 'normal',\n",
    "                       wrap=True)\n",
    "            y_pos -= 0.12\n",
    "        \n",
    "        ax.set_title(\"Key Open Research Questions\", fontsize=14, weight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nFuture Research Priorities:\")\n",
    "        print(\"1. Safety and Robustness: Developing provably safe RL systems\")\n",
    "        print(\"2. Sample Efficiency: Approaching human-level learning efficiency\") \n",
    "        print(\"3. Theoretical Foundations: Better understanding of deep RL\")\n",
    "        print(\"4. Real-World Deployment: Bridging sim-to-real gap\")\n",
    "        print(\"5. Multi-Task Learning: General value learning systems\")\n",
    "\n",
    "# Final Summary and Conclusions\n",
    "def comprehensive_dqn_summary():\n",
    "    \"\"\"Provide comprehensive summary of DQN developments\"\"\"\n",
    "    \n",
    "    print(\"=\"*70)\n",
    "    print(\"COMPREHENSIVE DQN SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    summary_text = f\"\"\"\n",
    "    Deep Q-Networks (DQN) and Value-Based Methods: Complete Analysis\n",
    "    \n",
    "    📚 THEORETICAL FOUNDATIONS\n",
    "    • Q-Learning: Fundamental value iteration algorithm\n",
    "    • Function Approximation: Neural networks for continuous state spaces\n",
    "    • Experience Replay: Breaking temporal correlations in training data\n",
    "    • Target Networks: Providing stable bootstrap targets\n",
    "    \n",
    "    🔧 CORE IMPROVEMENTS\n",
    "    • Double DQN: Addressing overestimation bias through action decoupling\n",
    "    • Dueling DQN: Separating state values and action advantages\n",
    "    • Prioritized Experience Replay: Learning from important transitions\n",
    "    • Multi-step Learning: Balancing bias and variance in temporal difference\n",
    "    \n",
    "    🌈 INTEGRATION (Rainbow DQN)\n",
    "    • Combines multiple improvements for state-of-the-art performance\n",
    "    • Noisy Networks: Parameter space exploration\n",
    "    • Distributional RL: Modeling return distributions\n",
    "    • Careful hyperparameter tuning and implementation details\n",
    "    \n",
    "    📊 KEY INSIGHTS FROM ANALYSIS\n",
    "    • Each component provides meaningful improvements\n",
    "    • Components often have synergistic effects when combined\n",
    "    • Prioritized replay provides largest single improvement\n",
    "    • Double DQN is crucial for stability\n",
    "    • Dueling architecture helps with value estimation\n",
    "    \n",
    "    🎯 PRACTICAL CONSIDERATIONS\n",
    "    • Implementation complexity increases with sophistication\n",
    "    • Computational overhead varies significantly between methods\n",
    "    • Hyperparameter sensitivity requires careful tuning\n",
    "    • Environment characteristics affect relative performance\n",
    "    \n",
    "    🔮 FUTURE DIRECTIONS\n",
    "    • Better theoretical understanding of deep RL\n",
    "    • Improved sample efficiency for real-world applications  \n",
    "    • Safety and robustness guarantees\n",
    "    • Integration with model-based methods\n",
    "    • Multi-task and meta-learning capabilities\n",
    "    \n",
    "    💡 MAIN TAKEAWAYS\n",
    "    • DQN revolutionized RL by enabling function approximation\n",
    "    • Systematic improvements address specific algorithmic weaknesses\n",
    "    • Rainbow represents effective integration of multiple advances\n",
    "    • Future work focuses on theory, safety, and real-world deployment\n",
    "    \"\"\"\n",
    "    \n",
    "    print(summary_text)\n",
    "    \n",
    "    # Create final visualization\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Performance progression (conceptual)\n",
    "    methods = ['Basic DQN', 'Double DQN', 'Dueling DQN', 'Prioritized DQN', 'Rainbow DQN']\n",
    "    performance_gains = [1.0, 1.3, 1.5, 1.8, 2.2]  # Relative performance\n",
    "    complexity_scores = [1.0, 1.1, 1.3, 1.6, 2.0]  # Implementation complexity\n",
    "    \n",
    "    # Scatter plot of performance vs complexity\n",
    "    colors = ['red', 'orange', 'yellow', 'lightgreen', 'green']\n",
    "    sizes = [100, 120, 140, 160, 200]\n",
    "    \n",
    "    for i, (method, perf, comp) in enumerate(zip(methods, performance_gains, complexity_scores)):\n",
    "        ax.scatter(comp, perf, s=sizes[i], c=colors[i], alpha=0.7, \n",
    "                  edgecolors='black', linewidths=2)\n",
    "        ax.annotate(method, (comp, perf), xytext=(5, 5), \n",
    "                   textcoords='offset points', fontsize=10, weight='bold')\n",
    "    \n",
    "    ax.set_xlabel('Implementation Complexity', fontsize=12)\n",
    "    ax.set_ylabel('Relative Performance', fontsize=12)\n",
    "    ax.set_title('DQN Methods: Performance vs Complexity Trade-off', fontsize=14, weight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add efficiency frontier\n",
    "    ax.plot(complexity_scores, performance_gains, 'k--', alpha=0.5, linewidth=2)\n",
    "    \n",
    "    # Add annotations\n",
    "    ax.text(0.05, 0.95, 'Sweet Spot:\\nGood performance\\nManageable complexity', \n",
    "            transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightblue'),\n",
    "            fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    ax.text(0.7, 0.3, 'Cutting Edge:\\nBest performance\\nHigh complexity', \n",
    "            transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightgreen'),\n",
    "            fontsize=10, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run advanced analysis\n",
    "advanced_analyzer = AdvancedDQNAnalyzer()\n",
    "\n",
    "print(\"Running Advanced DQN Analysis...\")\n",
    "\n",
    "print(\"\\n1. Analyzing Sources of Overestimation Bias...\")\n",
    "bias_analysis = advanced_analyzer.demonstrate_overestimation_sources()\n",
    "\n",
    "print(\"\\n2. Visualizing DQN Evolution Timeline...\")\n",
    "advanced_analyzer.visualize_dqn_evolution_timeline()\n",
    "\n",
    "print(\"\\n3. Exploring Future Research Directions...\")\n",
    "advanced_analyzer.future_research_directions()\n",
    "\n",
    "print(\"\\n4. Final Comprehensive Summary...\")\n",
    "comprehensive_dqn_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"🎉 CA7: Deep Q-Networks and Value-Based Methods - COMPLETED!\")\n",
    "print(\"=\"*70)\n",
    "print(\"This comprehensive notebook covers:\")\n",
    "print(\"• Theoretical foundations and mathematical formulations\")\n",
    "print(\"• Complete implementations of all major DQN variants\")\n",
    "print(\"• Systematic analysis and comparison of methods\")\n",
    "print(\"• Ablation studies and performance analysis\")\n",
    "print(\"• Advanced topics and future research directions\")\n",
    "print(\"• Practical insights for real-world applications\")\n",
    "print(\"\\nYou now have a complete understanding of modern value-based RL! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
