{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec826b0",
   "metadata": {},
   "source": [
    "# CA10: Model-Based Reinforcement Learning and Planning Methods\n",
    "\n",
    "## Deep Reinforcement Learning - Session 10\n",
    "\n",
    "**Comprehensive Coverage of Model-Based Reinforcement Learning**\n",
    "\n",
    "This notebook provides a complete exploration of model-based reinforcement learning, covering theoretical foundations, planning algorithms, and practical implementations of various model-based approaches including Dyna-Q, Monte Carlo Tree Search (MCTS), Model Predictive Control (MPC), and modern neural approaches.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand the theoretical foundations of model-based reinforcement learning\n",
    "2. Implement classical planning algorithms: Dynamic Programming, Value Iteration\n",
    "3. Explore integrated planning and learning: Dyna-Q algorithm\n",
    "4. Master Monte Carlo Tree Search (MCTS) and its applications\n",
    "5. Implement Model Predictive Control (MPC) for continuous control\n",
    "6. Understand modern neural model-based approaches\n",
    "7. Compare model-based vs model-free methods\n",
    "8. Apply model-based methods to complex environments\n",
    "\n",
    "### Notebook Structure:\n",
    "1. **Theoretical Foundations** - Model-based RL theory and framework\n",
    "2. **Environment Models** - Learning and representing environment dynamics\n",
    "3. **Classical Planning** - Dynamic Programming and Value Iteration with learned models\n",
    "4. **Dyna-Q Algorithm** - Integrating planning and learning\n",
    "5. **Monte Carlo Tree Search** - MCTS algorithm and applications\n",
    "6. **Model Predictive Control** - MPC for continuous control problems\n",
    "7. **Modern Neural Methods** - World models and neural planning\n",
    "8. **Comparative Analysis** - Model-based vs model-free comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports and Environment Setup\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Normal\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import pickle\n",
    "from typing import Tuple, List, Dict, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d996f71",
   "metadata": {},
   "source": [
    "# Section 1: Theoretical Foundations of Model-Based Reinforcement Learning\n",
    "\n",
    "## 1.1 From Model-Free to Model-Based Learning\n",
    "\n",
    "In our journey through reinforcement learning, we have primarily focused on **model-free methods** such as Q-learning, SARSA, and policy gradient methods. These methods learn directly from experience without explicitly modeling the environment. However, there are fundamental advantages to learning and using environment models:\n",
    "\n",
    "### Model-Free vs Model-Based Comparison\n",
    "\n",
    "| Aspect | Model-Free Methods | Model-Based Methods |\n",
    "|--------|-------------------|--------------------|\n",
    "| **Learning** | Learn value functions or policies directly | Learn environment model first |\n",
    "| **Sample Efficiency** | Generally less sample efficient | Generally more sample efficient |\n",
    "| **Computational Cost** | Lower per-step computation | Higher per-step computation |\n",
    "| **Planning** | No explicit planning | Can plan with learned model |\n",
    "| **Robustness** | More robust to model errors | Sensitive to model inaccuracies |\n",
    "| **Interpretability** | Less interpretable | More interpretable (explicit model) |\n",
    "\n",
    "## 1.2 The Model-Based RL Framework\n",
    "\n",
    "The general model-based RL framework consists of three main components:\n",
    "\n",
    "1. **Model Learning**: Learn a model of the environment from experience\n",
    "   $$\\hat{P}(s'|s,a) \\approx P(s'|s,a)$$\n",
    "   $$\\hat{R}(s,a) \\approx E[R|s,a]$$\n",
    "\n",
    "2. **Planning**: Use the learned model to compute optimal policies\n",
    "   - Value iteration with learned model\n",
    "   - Policy iteration with learned model  \n",
    "   - Monte Carlo Tree Search\n",
    "   - Model Predictive Control\n",
    "\n",
    "3. **Acting**: Execute actions in the real environment\n",
    "   - Collect new experience\n",
    "   - Update the model\n",
    "   - Replan with improved model\n",
    "\n",
    "## 1.3 Advantages of Model-Based Methods\n",
    "\n",
    "**Sample Efficiency**: \n",
    "- Can generate synthetic experience using the learned model\n",
    "- Each real experience can be used multiple times for planning\n",
    "- Particularly important in expensive real-world applications\n",
    "\n",
    "**Transfer Learning**:\n",
    "- Models can transfer across different tasks in the same environment\n",
    "- Learned dynamics are often more general than policies\n",
    "\n",
    "**Interpretability and Safety**:\n",
    "- Explicit models provide insight into system behavior\n",
    "- Can simulate outcomes before taking actions\n",
    "- Enable safety verification and constraint checking\n",
    "\n",
    "**Planning Capabilities**:\n",
    "- Can look ahead and plan optimal sequences of actions\n",
    "- Adapt quickly to changes in rewards or goals\n",
    "- Enable hierarchical and long-term planning\n",
    "\n",
    "## 1.4 Challenges in Model-Based RL\n",
    "\n",
    "**Model Bias and Compounding Errors**:\n",
    "- Errors in the learned model can compound over time\n",
    "- Model bias can lead to suboptimal policies\n",
    "- Challenge: Learning accurate models in complex environments\n",
    "\n",
    "**Computational Complexity**:\n",
    "- Planning with models can be computationally expensive\n",
    "- Trade-off between planning depth and computational cost\n",
    "\n",
    "**Partial Observability**:\n",
    "- Real environments often have hidden state\n",
    "- Challenge: Learning models from partial observations\n",
    "\n",
    "**Stochastic Environments**:\n",
    "- Need to model uncertainty in transitions and rewards\n",
    "- Balance between model complexity and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical Foundation Visualization\n",
    "\n",
    "class ModelBasedFrameworkVisualizer:\n",
    "    \"\"\"Visualize the model-based RL framework and concepts\"\"\"\n",
    "    \n",
    "    def visualize_framework_comparison(self):\n",
    "        \"\"\"Compare model-free vs model-based frameworks\"\"\"\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "        \n",
    "        # Model-Free Framework\n",
    "        ax = axes[0]\n",
    "        ax.set_xlim(0, 10)\n",
    "        ax.set_ylim(0, 10)\n",
    "        ax.set_title('Model-Free RL Framework', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Components\n",
    "        env_box = plt.Rectangle((1, 7), 8, 2, fill=True, facecolor='lightblue', edgecolor='black')\n",
    "        ax.add_patch(env_box)\n",
    "        ax.text(5, 8, 'Environment\\n(Unknown Dynamics)', ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        agent_box = plt.Rectangle((3, 4), 4, 2, fill=True, facecolor='lightgreen', edgecolor='black')\n",
    "        ax.add_patch(agent_box)\n",
    "        ax.text(5, 5, 'Agent\\n(Policy/Value Function)', ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        experience_box = plt.Rectangle((1, 1), 8, 1.5, fill=True, facecolor='lightyellow', edgecolor='black')\n",
    "        ax.add_patch(experience_box)\n",
    "        ax.text(5, 1.75, 'Direct Learning from Experience\\n(s, a, r, s\\')', ha='center', va='center')\n",
    "        \n",
    "        # Arrows\n",
    "        ax.annotate('', xy=(5, 6.8), xytext=(5, 6.2), arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "        ax.text(5.5, 6.5, 'Actions', ha='left', va='center', color='red')\n",
    "        \n",
    "        ax.annotate('', xy=(5, 3.8), xytext=(5, 4.2), arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
    "        ax.text(5.5, 3.5, 'Experience', ha='left', va='center', color='blue')\n",
    "        \n",
    "        ax.annotate('', xy=(5, 2.8), xytext=(5, 3.5), arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n",
    "        ax.text(2, 3, 'Learning', ha='center', va='center', color='green')\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        \n",
    "        # Model-Based Framework\n",
    "        ax = axes[1]\n",
    "        ax.set_xlim(0, 10)\n",
    "        ax.set_ylim(0, 10)\n",
    "        ax.set_title('Model-Based RL Framework', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # Components\n",
    "        env_box = plt.Rectangle((1, 8), 8, 1.5, fill=True, facecolor='lightblue', edgecolor='black')\n",
    "        ax.add_patch(env_box)\n",
    "        ax.text(5, 8.75, 'Real Environment', ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        model_box = plt.Rectangle((1, 6), 3.5, 1.5, fill=True, facecolor='lightcoral', edgecolor='black')\n",
    "        ax.add_patch(model_box)\n",
    "        ax.text(2.75, 6.75, 'Learned\\nModel', ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        planner_box = plt.Rectangle((5.5, 6), 3.5, 1.5, fill=True, facecolor='lightsalmon', edgecolor='black')\n",
    "        ax.add_patch(planner_box)\n",
    "        ax.text(7.25, 6.75, 'Planner', ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        policy_box = plt.Rectangle((3, 3.5), 4, 1.5, fill=True, facecolor='lightgreen', edgecolor='black')\n",
    "        ax.add_patch(policy_box)\n",
    "        ax.text(5, 4.25, 'Policy', ha='center', va='center', fontweight='bold')\n",
    "        \n",
    "        experience_box = plt.Rectangle((1, 1), 8, 1.5, fill=True, facecolor='lightyellow', edgecolor='black')\n",
    "        ax.add_patch(experience_box)\n",
    "        ax.text(5, 1.75, 'Experience Buffer\\n(s, a, r, s\\')', ha='center', va='center')\n",
    "        \n",
    "        # Arrows with labels\n",
    "        ax.annotate('', xy=(3, 7.8), xytext=(3, 7.2), arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n",
    "        ax.text(2, 7.5, 'Experience', ha='center', va='center', color='blue', rotation=90)\n",
    "        \n",
    "        ax.annotate('', xy=(6, 6.8), xytext=(4, 6.8), arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n",
    "        ax.text(5, 7.2, 'Model', ha='center', va='center', color='red')\n",
    "        \n",
    "        ax.annotate('', xy=(6, 5.8), xytext=(6, 5.2), arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n",
    "        ax.text(6.5, 5.5, 'Policy', ha='left', va='center', color='green', rotation=-90)\n",
    "        \n",
    "        ax.annotate('', xy=(5, 7.8), xytext=(5, 5.2), arrowprops=dict(arrowstyle='->', lw=2, color='purple'))\n",
    "        ax.text(5.5, 6.5, 'Actions', ha='left', va='center', color='purple')\n",
    "        \n",
    "        ax.annotate('', xy=(5, 2.8), xytext=(5, 3.2), arrowprops=dict(arrowstyle='->', lw=2, color='orange'))\n",
    "        ax.text(6, 3, 'Store', ha='center', va='center', color='orange')\n",
    "        \n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.spines['right'].set_visible(False)\n",
    "        ax.spines['bottom'].set_visible(False)\n",
    "        ax.spines['left'].set_visible(False)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_sample_efficiency(self):\n",
    "        \"\"\"Visualize sample efficiency comparison\"\"\"\n",
    "        \n",
    "        episodes = np.arange(1, 201)\n",
    "        \n",
    "        # Simulate learning curves\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Model-free: slower initial learning but eventually converges\n",
    "        model_free = 100 * (1 - np.exp(-episodes/50)) + np.random.normal(0, 5, len(episodes))\n",
    "        model_free = np.maximum(0, model_free)\n",
    "        \n",
    "        # Model-based: faster initial learning but may plateau due to model bias\n",
    "        model_based = 120 * (1 - np.exp(-episodes/20)) + np.random.normal(0, 3, len(episodes))\n",
    "        model_based = np.maximum(0, model_based)\n",
    "        # Add some bias that causes plateau\n",
    "        model_based[100:] *= 0.95\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        plt.plot(episodes, model_free, label='Model-Free RL', linewidth=2, color='blue', alpha=0.8)\n",
    "        plt.plot(episodes, model_based, label='Model-Based RL', linewidth=2, color='red', alpha=0.8)\n",
    "        \n",
    "        # Add smoothed versions\n",
    "        window = 10\n",
    "        model_free_smooth = pd.Series(model_free).rolling(window).mean()\n",
    "        model_based_smooth = pd.Series(model_based).rolling(window).mean()\n",
    "        \n",
    "        plt.plot(episodes, model_free_smooth, linewidth=3, color='darkblue', alpha=0.9)\n",
    "        plt.plot(episodes, model_based_smooth, linewidth=3, color='darkred', alpha=0.9)\n",
    "        \n",
    "        plt.axvline(x=50, color='gray', linestyle='--', alpha=0.7)\n",
    "        plt.text(52, 80, 'Model-Based\\nAdvantage\\nRegion', fontsize=12, color='darkred')\n",
    "        \n",
    "        plt.axvline(x=150, color='gray', linestyle='--', alpha=0.7)\n",
    "        plt.text(152, 80, 'Model-Free\\nCatches Up', fontsize=12, color='darkblue')\n",
    "        \n",
    "        plt.xlabel('Training Episodes', fontsize=14)\n",
    "        plt.ylabel('Average Return', fontsize=14)\n",
    "        plt.title('Sample Efficiency: Model-Based vs Model-Free RL', fontsize=16, fontweight='bold')\n",
    "        plt.legend(fontsize=12)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add annotations\n",
    "        plt.annotate('Faster initial learning\\n(better sample efficiency)', \n",
    "                    xy=(30, model_based_smooth.iloc[30]), xytext=(70, 40),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "                    fontsize=10, color='darkred')\n",
    "        \n",
    "        plt.annotate('May plateau due to model bias', \n",
    "                    xy=(120, model_based_smooth.iloc[120]), xytext=(140, 60),\n",
    "                    arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n",
    "                    fontsize=10, color='darkred')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def create_summary_table(self):\n",
    "        \"\"\"Create comprehensive comparison table\"\"\"\n",
    "        \n",
    "        comparison_data = {\n",
    "            'Characteristic': [\n",
    "                'Sample Efficiency',\n",
    "                'Computational Cost',\n",
    "                'Robustness to Model Errors',\n",
    "                'Planning Capability',\n",
    "                'Transfer Learning',\n",
    "                'Interpretability',\n",
    "                'Real-time Performance',\n",
    "                'Asymptotic Performance'\n",
    "            ],\n",
    "            'Model-Free': [\n",
    "                'Lower (more samples needed)',\n",
    "                'Lower (per interaction)',\n",
    "                'High (no model dependence)',\n",
    "                'None (reactive only)',\n",
    "                'Limited (policy/value specific)',\n",
    "                'Low (black box)',\n",
    "                'Fast (direct action)',\n",
    "                'High (no model bias)'\n",
    "            ],\n",
    "            'Model-Based': [\n",
    "                'Higher (reuse experience)',\n",
    "                'Higher (planning overhead)',\n",
    "                'Low (sensitive to model errors)',\n",
    "                'Excellent (explicit planning)',\n",
    "                'Excellent (model transfers)',\n",
    "                'High (interpretable model)',\n",
    "                'Variable (depends on planner)',\n",
    "                'Variable (depends on model accuracy)'\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(16, 10))\n",
    "        ax.axis('tight')\n",
    "        ax.axis('off')\n",
    "        \n",
    "        table = ax.table(cellText=df.values,\n",
    "                        colLabels=df.columns,\n",
    "                        cellLoc='left',\n",
    "                        loc='center',\n",
    "                        colWidths=[0.25, 0.375, 0.375])\n",
    "        \n",
    "        table.auto_set_font_size(False)\n",
    "        table.set_fontsize(11)\n",
    "        table.scale(1, 2.5)\n",
    "        \n",
    "        # Style the header\n",
    "        for i in range(len(df.columns)):\n",
    "            table[(0, i)].set_facecolor('#4CAF50')\n",
    "            table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "        \n",
    "        # Style the rows\n",
    "        for i in range(1, len(df) + 1):\n",
    "            table[(i, 0)].set_facecolor('#E8F5E8')\n",
    "            table[(i, 0)].set_text_props(weight='bold')\n",
    "            \n",
    "            if i % 2 == 0:\n",
    "                table[(i, 1)].set_facecolor('#F0F8FF')\n",
    "                table[(i, 2)].set_facecolor('#FFF8DC')\n",
    "            else:\n",
    "                table[(i, 1)].set_facecolor('#F8F8FF')\n",
    "                table[(i, 2)].set_facecolor('#FFFACD')\n",
    "        \n",
    "        plt.title('Comprehensive Comparison: Model-Free vs Model-Based RL', \n",
    "                 fontsize=16, fontweight='bold', pad=20)\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Create and run visualizations\n",
    "visualizer = ModelBasedFrameworkVisualizer()\n",
    "\n",
    "print(\"Model-Based RL Framework Visualizations\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n1. Framework Comparison:\")\n",
    "visualizer.visualize_framework_comparison()\n",
    "\n",
    "print(\"\\n2. Sample Efficiency Analysis:\")\n",
    "visualizer.visualize_sample_efficiency()\n",
    "\n",
    "print(\"\\n3. Comprehensive Comparison Table:\")\n",
    "comparison_df = visualizer.create_summary_table()\n",
    "\n",
    "print(\"\\nâœ… Theoretical foundations established!\")\n",
    "print(\"ðŸ“Š Next: Environment model learning and representation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632a623",
   "metadata": {},
   "source": [
    "# Section 2: Environment Models and Model Learning\n",
    "\n",
    "## 2.1 Types of Environment Models\n",
    "\n",
    "Environment models can be categorized along several dimensions:\n",
    "\n",
    "### By Representation Type:\n",
    "\n",
    "**Tabular Models**:\n",
    "- Store explicit transition probabilities: $P(s'|s,a)$\n",
    "- Store explicit rewards: $R(s,a)$\n",
    "- Suitable for small, discrete state-action spaces\n",
    "- Example: Storing counts and computing maximum likelihood estimates\n",
    "\n",
    "**Function Approximation Models**:\n",
    "- Use neural networks to approximate dynamics\n",
    "- $s' = f_\\theta(s,a) + \\epsilon$ (deterministic + noise)\n",
    "- $P(s'|s,a) = \\pi_\\theta(s'|s,a)$ (stochastic)\n",
    "- Suitable for large, continuous state-action spaces\n",
    "\n",
    "### By Uncertainty Representation:\n",
    "\n",
    "**Deterministic Models**:\n",
    "- Predict single next state: $s' = f(s,a)$\n",
    "- Simple but ignores environment stochasticity\n",
    "- Can add noise independently\n",
    "\n",
    "**Stochastic Models**:\n",
    "- Predict distribution over next states: $P(s'|s,a)$\n",
    "- More accurate for stochastic environments\n",
    "- Can be parametric (Gaussian) or non-parametric\n",
    "\n",
    "**Ensemble Models**:\n",
    "- Multiple models trained on different data subsets\n",
    "- Uncertainty estimated from ensemble disagreement\n",
    "- More robust and better uncertainty quantification\n",
    "\n",
    "## 2.2 Model Learning Approaches\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "For tabular environments, we can use simple counting:\n",
    "$$\\hat{P}(s'|s,a) = \\frac{N(s,a,s')}{N(s,a)}$$\n",
    "$$\\hat{R}(s,a) = \\frac{1}{N(s,a)} \\sum_{i} R_i(s,a)$$\n",
    "\n",
    "### Neural Network Models\n",
    "\n",
    "For complex environments, use neural networks:\n",
    "- **Forward Model**: $(s,a) \\rightarrow (s', r)$\n",
    "- **Inverse Model**: $(s,s') \\rightarrow a$\n",
    "- **Combined**: Learn both forward and inverse models jointly\n",
    "\n",
    "### Training Objectives\n",
    "\n",
    "**Deterministic Dynamics**:\n",
    "$$L = \\mathbb{E}_{(s,a,s',r) \\sim D}[||s' - f_\\theta(s,a)||^2 + ||r - g_\\theta(s,a)||^2]$$\n",
    "\n",
    "**Stochastic Dynamics**:\n",
    "$$L = -\\mathbb{E}_{(s,a,s',r) \\sim D}[\\log P_\\theta(s'|s,a) + \\log P_\\theta(r|s,a)]$$\n",
    "\n",
    "## 2.3 Model Validation and Selection\n",
    "\n",
    "### Validation Strategies\n",
    "\n",
    "**Hold-out Validation**:\n",
    "- Split data into training and validation sets\n",
    "- Evaluate model accuracy on unseen transitions\n",
    "- Risk: May not reflect planning performance\n",
    "\n",
    "**Cross-Validation**:\n",
    "- Multiple train/validation splits\n",
    "- More robust estimate of model quality\n",
    "- Higher computational cost\n",
    "\n",
    "**Policy-Aware Validation**:\n",
    "- Evaluate model on states visited by current policy\n",
    "- More relevant for planning performance\n",
    "- Adapts as policy changes\n",
    "\n",
    "### Model Selection Criteria\n",
    "\n",
    "**Prediction Accuracy**:\n",
    "- Mean squared error for continuous states\n",
    "- Cross-entropy for discrete states\n",
    "- May not correlate with planning performance\n",
    "\n",
    "**Planning Performance**:\n",
    "- Evaluate policies learned with the model\n",
    "- More relevant but computationally expensive\n",
    "- Gold standard when feasible\n",
    "\n",
    "**Uncertainty Calibration**:\n",
    "- Ensure predicted uncertainty matches actual errors\n",
    "- Important for robust planning\n",
    "- Use reliability diagrams and calibration error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Models Implementation\n",
    "\n",
    "class TabularModel:\n",
    "    \"\"\"Tabular environment model using counting\"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, num_actions):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        \n",
    "        # Transition counts: N(s,a,s')\n",
    "        self.transition_counts = np.zeros((num_states, num_actions, num_states))\n",
    "        \n",
    "        # State-action counts: N(s,a)\n",
    "        self.sa_counts = np.zeros((num_states, num_actions))\n",
    "        \n",
    "        # Reward sums and counts\n",
    "        self.reward_sums = np.zeros((num_states, num_actions))\n",
    "        self.reward_counts = np.zeros((num_states, num_actions))\n",
    "        \n",
    "    def update(self, state, action, next_state, reward):\n",
    "        \"\"\"Update model with new transition\"\"\"\n",
    "        self.transition_counts[state, action, next_state] += 1\n",
    "        self.sa_counts[state, action] += 1\n",
    "        \n",
    "        self.reward_sums[state, action] += reward\n",
    "        self.reward_counts[state, action] += 1\n",
    "    \n",
    "    def get_transition_prob(self, state, action, next_state):\n",
    "        \"\"\"Get transition probability P(s'|s,a)\"\"\"\n",
    "        if self.sa_counts[state, action] == 0:\n",
    "            return 1.0 / self.num_states  # Uniform prior\n",
    "        return self.transition_counts[state, action, next_state] / self.sa_counts[state, action]\n",
    "    \n",
    "    def get_expected_reward(self, state, action):\n",
    "        \"\"\"Get expected reward R(s,a)\"\"\"\n",
    "        if self.reward_counts[state, action] == 0:\n",
    "            return 0.0  # Neutral prior\n",
    "        return self.reward_sums[state, action] / self.reward_counts[state, action]\n",
    "    \n",
    "    def sample_transition(self, state, action):\n",
    "        \"\"\"Sample next state and reward from model\"\"\"\n",
    "        # Sample next state\n",
    "        if self.sa_counts[state, action] == 0:\n",
    "            next_state = np.random.randint(self.num_states)\n",
    "        else:\n",
    "            probs = self.transition_counts[state, action] / self.sa_counts[state, action]\n",
    "            next_state = np.random.choice(self.num_states, p=probs)\n",
    "        \n",
    "        # Get expected reward\n",
    "        reward = self.get_expected_reward(state, action)\n",
    "        \n",
    "        return next_state, reward\n",
    "    \n",
    "    def get_transition_matrix(self, action):\n",
    "        \"\"\"Get full transition matrix P(s'|s,a) for given action\"\"\"\n",
    "        P = np.zeros((self.num_states, self.num_states))\n",
    "        \n",
    "        for s in range(self.num_states):\n",
    "            if self.sa_counts[s, action] == 0:\n",
    "                P[s, :] = 1.0 / self.num_states  # Uniform prior\n",
    "            else:\n",
    "                P[s, :] = self.transition_counts[s, action, :] / self.sa_counts[s, action]\n",
    "        \n",
    "        return P\n",
    "    \n",
    "    def get_reward_vector(self, action):\n",
    "        \"\"\"Get reward vector R(s,a) for given action\"\"\"\n",
    "        R = np.zeros(self.num_states)\n",
    "        \n",
    "        for s in range(self.num_states):\n",
    "            R[s] = self.get_expected_reward(s, action)\n",
    "        \n",
    "        return R\n",
    "\n",
    "class NeuralModel(nn.Module):\n",
    "    \"\"\"Neural network environment model\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, ensemble_size=1):\n",
    "        super(NeuralModel, self).__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.ensemble_size = ensemble_size\n",
    "        \n",
    "        # Create ensemble of models\n",
    "        self.models = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(ensemble_size):\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, state_dim + 1)  # next_state + reward\n",
    "            )\n",
    "            self.models.append(model)\n",
    "    \n",
    "    def forward(self, state, action, model_idx=None):\n",
    "        \"\"\"Forward pass through model(s)\"\"\"\n",
    "        # Concatenate state and action\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        if len(action.shape) == 1:\n",
    "            action = action.unsqueeze(0)\n",
    "        \n",
    "        # Handle discrete actions\n",
    "        if action.dtype == torch.long:\n",
    "            action_one_hot = torch.zeros(action.size(0), self.action_dim).to(action.device)\n",
    "            action_one_hot.scatter_(1, action.unsqueeze(1), 1)\n",
    "            action = action_one_hot\n",
    "        \n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        \n",
    "        if model_idx is not None:\n",
    "            # Use specific model\n",
    "            output = self.models[model_idx](x)\n",
    "        else:\n",
    "            # Use ensemble average\n",
    "            outputs = torch.stack([model(x) for model in self.models])\n",
    "            output = outputs.mean(dim=0)\n",
    "        \n",
    "        # Split into next state and reward\n",
    "        next_state = output[:, :self.state_dim]\n",
    "        reward = output[:, self.state_dim]\n",
    "        \n",
    "        return next_state, reward\n",
    "    \n",
    "    def predict_with_uncertainty(self, state, action):\n",
    "        \"\"\"Predict with uncertainty from ensemble\"\"\"\n",
    "        outputs = []\n",
    "        \n",
    "        for i in range(self.ensemble_size):\n",
    "            next_state, reward = self.forward(state, action, model_idx=i)\n",
    "            outputs.append(torch.cat([next_state, reward.unsqueeze(1)], dim=1))\n",
    "        \n",
    "        outputs = torch.stack(outputs)  # (ensemble_size, batch_size, state_dim + 1)\n",
    "        \n",
    "        # Compute mean and uncertainty\n",
    "        mean = outputs.mean(dim=0)\n",
    "        uncertainty = outputs.std(dim=0)\n",
    "        \n",
    "        next_state_mean = mean[:, :self.state_dim]\n",
    "        reward_mean = mean[:, self.state_dim]\n",
    "        next_state_std = uncertainty[:, :self.state_dim]\n",
    "        reward_std = uncertainty[:, self.state_dim]\n",
    "        \n",
    "        return next_state_mean, reward_mean, next_state_std, reward_std\n",
    "    \n",
    "    def sample_from_model(self, state, action):\n",
    "        \"\"\"Sample transition from one random model in ensemble\"\"\"\n",
    "        model_idx = np.random.randint(self.ensemble_size)\n",
    "        return self.forward(state, action, model_idx=model_idx)\n",
    "\n",
    "class ModelTrainer:\n",
    "    \"\"\"Trainer for neural environment models\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr=1e-3):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def train_step(self, states, actions, next_states, rewards):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device) if len(actions.shape) == 1 else torch.FloatTensor(actions).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        # Train each model in ensemble\n",
    "        for i in range(self.model.ensemble_size):\n",
    "            pred_next_states, pred_rewards = self.model.forward(states, actions, model_idx=i)\n",
    "            \n",
    "            # Compute loss\n",
    "            state_loss = F.mse_loss(pred_next_states, next_states)\n",
    "            reward_loss = F.mse_loss(pred_rewards, rewards)\n",
    "            \n",
    "            loss = state_loss + reward_loss\n",
    "            total_loss += loss\n",
    "        \n",
    "        # Backward pass\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "    \n",
    "    def train_batch(self, data, epochs=10, batch_size=32):\n",
    "        \"\"\"Train on batch of data\"\"\"\n",
    "        states, actions, next_states, rewards = data\n",
    "        n_samples = len(states)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "            \n",
    "            # Shuffle data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_indices = indices[i:i+batch_size]\n",
    "                \n",
    "                batch_states = states[batch_indices]\n",
    "                batch_actions = actions[batch_indices]\n",
    "                batch_next_states = next_states[batch_indices]\n",
    "                batch_rewards = rewards[batch_indices]\n",
    "                \n",
    "                loss = self.train_step(batch_states, batch_actions, batch_next_states, batch_rewards)\n",
    "                epoch_loss += loss\n",
    "                n_batches += 1\n",
    "            \n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            self.loss_history.append(avg_loss)\n",
    "            \n",
    "            if (epoch + 1) % 5 == 0:\n",
    "                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Demonstration with simple gridworld\n",
    "class SimpleGridWorld:\n",
    "    \"\"\"Simple gridworld for model learning demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.num_states = size * size\n",
    "        self.num_actions = 4  # up, down, left, right\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0  # Start at top-left\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        x, y = self.state % self.size, self.state // self.size\n",
    "        \n",
    "        # Actions: 0=up, 1=down, 2=left, 3=right\n",
    "        if action == 0 and y > 0:  # up\n",
    "            y -= 1\n",
    "        elif action == 1 and y < self.size - 1:  # down\n",
    "            y += 1\n",
    "        elif action == 2 and x > 0:  # left\n",
    "            x -= 1\n",
    "        elif action == 3 and x < self.size - 1:  # right\n",
    "            x += 1\n",
    "        \n",
    "        self.state = y * self.size + x\n",
    "        \n",
    "        # Reward: +1 for reaching bottom-right corner, -0.01 for each step\n",
    "        if self.state == self.num_states - 1:\n",
    "            reward = 1.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.01\n",
    "            done = False\n",
    "        \n",
    "        return self.state, reward, done\n",
    "\n",
    "# Demonstrate model learning\n",
    "print(\"Environment Model Learning Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create environment and collect data\n",
    "env = SimpleGridWorld(size=4)\n",
    "tabular_model = TabularModel(env.num_states, env.num_actions)\n",
    "\n",
    "# Collect experience\n",
    "n_episodes = 1000\n",
    "experience_data = []\n",
    "\n",
    "print(\"\\n1. Collecting experience...\")\n",
    "for episode in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = np.random.randint(env.num_actions)  # Random policy\n",
    "        next_state, reward, done = env.step(action)\n",
    "        \n",
    "        # Update tabular model\n",
    "        tabular_model.update(state, action, next_state, reward)\n",
    "        \n",
    "        # Store for neural model\n",
    "        experience_data.append((state, action, next_state, reward))\n",
    "        \n",
    "        state = next_state\n",
    "\n",
    "print(f\"Collected {len(experience_data)} transitions\")\n",
    "\n",
    "# Prepare data for neural model\n",
    "states = np.array([exp[0] for exp in experience_data])\n",
    "actions = np.array([exp[1] for exp in experience_data])\n",
    "next_states = np.array([exp[2] for exp in experience_data])\n",
    "rewards = np.array([exp[3] for exp in experience_data])\n",
    "\n",
    "# Convert states to one-hot for neural model\n",
    "states_onehot = np.eye(env.num_states)[states]\n",
    "next_states_onehot = np.eye(env.num_states)[next_states]\n",
    "\n",
    "# Train neural model\n",
    "print(\"\\n2. Training neural model...\")\n",
    "neural_model = NeuralModel(env.num_states, env.num_actions, hidden_dim=64, ensemble_size=3).to(device)\n",
    "trainer = ModelTrainer(neural_model, lr=1e-3)\n",
    "\n",
    "trainer.train_batch((states_onehot, actions, next_states_onehot, rewards), epochs=50, batch_size=64)\n",
    "\n",
    "print(\"\\n3. Model accuracy comparison:\")\n",
    "\n",
    "# Test model accuracy\n",
    "test_states = np.random.randint(0, env.num_states, 100)\n",
    "test_actions = np.random.randint(0, env.num_actions, 100)\n",
    "\n",
    "tabular_errors = []\n",
    "neural_errors = []\n",
    "\n",
    "for s, a in zip(test_states, test_actions):\n",
    "    # Get true transition (using many samples)\n",
    "    true_next_states = []\n",
    "    true_rewards = []\n",
    "    \n",
    "    for _ in range(50):\n",
    "        env.state = s\n",
    "        next_state, reward, _ = env.step(a)\n",
    "        true_next_states.append(next_state)\n",
    "        true_rewards.append(reward)\n",
    "    \n",
    "    true_reward = np.mean(true_rewards)\n",
    "    \n",
    "    # Tabular model prediction\n",
    "    pred_reward_tabular = tabular_model.get_expected_reward(s, a)\n",
    "    tabular_errors.append(abs(true_reward - pred_reward_tabular))\n",
    "    \n",
    "    # Neural model prediction\n",
    "    state_tensor = torch.FloatTensor(np.eye(env.num_states)[s]).to(device)\n",
    "    action_tensor = torch.LongTensor([a]).to(device)\n",
    "    _, pred_reward_neural = neural_model(state_tensor, action_tensor)\n",
    "    pred_reward_neural = pred_reward_neural.cpu().item()\n",
    "    neural_errors.append(abs(true_reward - pred_reward_neural))\n",
    "\n",
    "print(f\"Tabular model MAE: {np.mean(tabular_errors):.4f}\")\n",
    "print(f\"Neural model MAE: {np.mean(neural_errors):.4f}\")\n",
    "\n",
    "# Visualize training progress\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(trainer.loss_history)\n",
    "plt.title('Neural Model Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(['Tabular Model', 'Neural Model'], [np.mean(tabular_errors), np.mean(neural_errors)], \n",
    "        color=['blue', 'red'], alpha=0.7)\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Environment model learning complete!\")\n",
    "print(\"ðŸ“Š Next: Classical planning with learned models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc141ae",
   "metadata": {},
   "source": [
    "# Section 3: Classical Planning with Learned Models\n",
    "\n",
    "## 3.1 Dynamic Programming with Learned Models\n",
    "\n",
    "Once we have learned an environment model, we can use classical dynamic programming algorithms to compute optimal policies. This is one of the most straightforward applications of model-based RL.\n",
    "\n",
    "### Value Iteration with Learned Models\n",
    "\n",
    "The Value Iteration algorithm can be applied directly using our learned transition probabilities and rewards:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} \\hat{P}(s'|s,a)[\\hat{R}(s,a,s') + \\gamma V_k(s')]$$\n",
    "\n",
    "**Key Advantages:**\n",
    "- Guaranteed convergence to optimal policy (if model is accurate)\n",
    "- Can compute policy for all states simultaneously\n",
    "- No need for exploration during planning phase\n",
    "\n",
    "**Potential Issues:**\n",
    "- Model errors compound over planning horizon\n",
    "- Assumes learned model is accurate\n",
    "- May overfit to limited experience\n",
    "\n",
    "### Policy Iteration with Learned Models\n",
    "\n",
    "Policy Iteration alternates between policy evaluation and policy improvement using the learned model:\n",
    "\n",
    "**Policy Evaluation:**\n",
    "$$V^\\pi(s) = \\sum_{s'} \\hat{P}(s'|s,\\pi(s))[\\hat{R}(s,\\pi(s),s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "**Policy Improvement:**\n",
    "$$\\pi'(s) = \\arg\\max_a \\sum_{s'} \\hat{P}(s'|s,a)[\\hat{R}(s,a,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "## 3.2 Handling Model Uncertainty\n",
    "\n",
    "Real learned models have uncertainty. Several approaches address this:\n",
    "\n",
    "### Pessimistic Planning\n",
    "- Use lower confidence bounds for model predictions\n",
    "- $\\hat{P}_{pessimistic}(s'|s,a) = \\hat{P}(s'|s,a) - \\beta \\sigma(s'|s,a)$\n",
    "- Leads to more robust but potentially conservative policies\n",
    "\n",
    "### Optimistic Planning  \n",
    "- Use upper confidence bounds for model predictions\n",
    "- Encourages exploration of uncertain regions\n",
    "- Can lead to more aggressive exploration policies\n",
    "\n",
    "### Robust Planning\n",
    "- Optimize for worst-case model within confidence region\n",
    "- $\\max_\\pi \\min_{M \\in \\mathcal{U}} V^\\pi_M$\n",
    "- Very conservative but safe approach\n",
    "\n",
    "## 3.3 Model-Based Policy Search\n",
    "\n",
    "Instead of computing value functions, we can directly search in policy space using the learned model:\n",
    "\n",
    "### Gradient-Based Policy Search\n",
    "- Use model to compute policy gradients\n",
    "- $\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta, M}[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) R(\\tau)]$\n",
    "- Generate synthetic rollouts with learned model\n",
    "\n",
    "### Evolutionary Policy Search\n",
    "- Maintain population of policy parameters\n",
    "- Evaluate policies using learned model\n",
    "- Select and mutate best policies\n",
    "\n",
    "### Random Shooting\n",
    "- Sample random action sequences\n",
    "- Evaluate using learned model\n",
    "- Select best sequence and execute first action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285abb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classical Planning Implementation\n",
    "\n",
    "class ModelBasedPlanner:\n",
    "    \"\"\"Classical planning algorithms using learned models\"\"\"\n",
    "    \n",
    "    def __init__(self, model, num_states, num_actions, gamma=0.99):\n",
    "        self.model = model\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Initialize value function and policy\n",
    "        self.V = np.zeros(num_states)\n",
    "        self.policy = np.zeros(num_states, dtype=int)\n",
    "        \n",
    "        # Planning history for analysis\n",
    "        self.value_history = []\n",
    "        self.policy_history = []\n",
    "    \n",
    "    def value_iteration(self, max_iterations=100, tolerance=1e-6):\n",
    "        \"\"\"Value Iteration using learned model\"\"\"\n",
    "        \n",
    "        print(f\"Running Value Iteration (max_iter={max_iterations}, tol={tolerance})\")\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            old_V = self.V.copy()\n",
    "            \n",
    "            for state in range(self.num_states):\n",
    "                # Compute Q-values for all actions\n",
    "                q_values = np.zeros(self.num_actions)\n",
    "                \n",
    "                for action in range(self.num_actions):\n",
    "                    # Compute expected value using model\n",
    "                    expected_value = 0\n",
    "                    \n",
    "                    for next_state in range(self.num_states):\n",
    "                        transition_prob = self.model.get_transition_prob(state, action, next_state)\n",
    "                        reward = self.model.get_expected_reward(state, action)\n",
    "                        expected_value += transition_prob * (reward + self.gamma * old_V[next_state])\n",
    "                    \n",
    "                    q_values[action] = expected_value\n",
    "                \n",
    "                # Update value and policy\n",
    "                self.V[state] = np.max(q_values)\n",
    "                self.policy[state] = np.argmax(q_values)\n",
    "            \n",
    "            # Store history\n",
    "            self.value_history.append(self.V.copy())\n",
    "            self.policy_history.append(self.policy.copy())\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.max(np.abs(self.V - old_V)) < tolerance:\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        return self.V, self.policy\n",
    "    \n",
    "    def policy_iteration(self, max_iterations=50, eval_max_iterations=100):\n",
    "        \"\"\"Policy Iteration using learned model\"\"\"\n",
    "        \n",
    "        print(f\"Running Policy Iteration (max_iter={max_iterations})\")\n",
    "        \n",
    "        # Initialize random policy\n",
    "        self.policy = np.random.randint(0, self.num_actions, self.num_states)\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            old_policy = self.policy.copy()\n",
    "            \n",
    "            # Policy Evaluation\n",
    "            self.V = self.policy_evaluation(self.policy, max_iterations=eval_max_iterations)\n",
    "            \n",
    "            # Policy Improvement\n",
    "            for state in range(self.num_states):\n",
    "                q_values = np.zeros(self.num_actions)\n",
    "                \n",
    "                for action in range(self.num_actions):\n",
    "                    expected_value = 0\n",
    "                    \n",
    "                    for next_state in range(self.num_states):\n",
    "                        transition_prob = self.model.get_transition_prob(state, action, next_state)\n",
    "                        reward = self.model.get_expected_reward(state, action)\n",
    "                        expected_value += transition_prob * (reward + self.gamma * self.V[next_state])\n",
    "                    \n",
    "                    q_values[action] = expected_value\n",
    "                \n",
    "                self.policy[state] = np.argmax(q_values)\n",
    "            \n",
    "            # Store history\n",
    "            self.value_history.append(self.V.copy())\n",
    "            self.policy_history.append(self.policy.copy())\n",
    "            \n",
    "            # Check convergence\n",
    "            if np.array_equal(self.policy, old_policy):\n",
    "                print(f\"Converged after {iteration + 1} iterations\")\n",
    "                break\n",
    "        \n",
    "        return self.V, self.policy\n",
    "    \n",
    "    def policy_evaluation(self, policy, max_iterations=100, tolerance=1e-6):\n",
    "        \"\"\"Evaluate a given policy using learned model\"\"\"\n",
    "        \n",
    "        V = np.zeros(self.num_states)\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            old_V = V.copy()\n",
    "            \n",
    "            for state in range(self.num_states):\n",
    "                action = policy[state]\n",
    "                expected_value = 0\n",
    "                \n",
    "                for next_state in range(self.num_states):\n",
    "                    transition_prob = self.model.get_transition_prob(state, action, next_state)\n",
    "                    reward = self.model.get_expected_reward(state, action)\n",
    "                    expected_value += transition_prob * (reward + self.gamma * old_V[next_state])\n",
    "                \n",
    "                V[state] = expected_value\n",
    "            \n",
    "            if np.max(np.abs(V - old_V)) < tolerance:\n",
    "                break\n",
    "        \n",
    "        return V\n",
    "    \n",
    "    def compute_q_function(self):\n",
    "        \"\"\"Compute Q-function from current value function\"\"\"\n",
    "        \n",
    "        Q = np.zeros((self.num_states, self.num_actions))\n",
    "        \n",
    "        for state in range(self.num_states):\n",
    "            for action in range(self.num_actions):\n",
    "                expected_value = 0\n",
    "                \n",
    "                for next_state in range(self.num_states):\n",
    "                    transition_prob = self.model.get_transition_prob(state, action, next_state)\n",
    "                    reward = self.model.get_expected_reward(state, action)\n",
    "                    expected_value += transition_prob * (reward + self.gamma * self.V[next_state])\n",
    "                \n",
    "                Q[state, action] = expected_value\n",
    "        \n",
    "        return Q\n",
    "\n",
    "class UncertaintyAwarePlanner:\n",
    "    \"\"\"Planning with model uncertainty\"\"\"\n",
    "    \n",
    "    def __init__(self, ensemble_model, num_states, num_actions, gamma=0.99):\n",
    "        self.ensemble_model = ensemble_model\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def pessimistic_value_iteration(self, beta=1.0, max_iterations=100):\n",
    "        \"\"\"Value iteration with pessimistic model estimates\"\"\"\n",
    "        \n",
    "        V = np.zeros(self.num_states)\n",
    "        policy = np.zeros(self.num_states, dtype=int)\n",
    "        \n",
    "        print(f\"Running Pessimistic Value Iteration (beta={beta})\")\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            old_V = V.copy()\n",
    "            \n",
    "            for state in range(self.num_states):\n",
    "                q_values = np.zeros(self.num_actions)\n",
    "                \n",
    "                for action in range(self.num_actions):\n",
    "                    # Get predictions from ensemble\n",
    "                    state_onehot = np.eye(self.num_states)[state:state+1]\n",
    "                    action_tensor = np.array([action])\n",
    "                    \n",
    "                    # Convert to tensors\n",
    "                    state_tensor = torch.FloatTensor(state_onehot).to(device)\n",
    "                    action_tensor = torch.LongTensor(action_tensor).to(device)\n",
    "                    \n",
    "                    # Get ensemble predictions\n",
    "                    next_state_mean, reward_mean, next_state_std, reward_std = \\\n",
    "                        self.ensemble_model.predict_with_uncertainty(state_tensor, action_tensor)\\n\",\n",
    "                    \\n\",\n",
    "                    # Use pessimistic estimates\\n\",\n",
    "                    pessimistic_reward = reward_mean.cpu().item() - beta * reward_std.cpu().item()\\n\",\n",
    "                    \\n\",\n",
    "                    # For simplicity, assume deterministic next state (can be extended)\\n\",\n",
    "                    next_state_pred = next_state_mean.cpu().numpy()[0]\\n\",\n",
    "                    next_state_idx = np.argmax(next_state_pred)  # Most likely next state\\n\",\n",
    "                    \\n\",\n",
    "                    q_values[action] = pessimistic_reward + self.gamma * old_V[next_state_idx]\\n\",\n",
    "                \\n\",\n",
    "                V[state] = np.max(q_values)\\n\",\n",
    "                policy[state] = np.argmax(q_values)\\n\",\n",
    "            \\n\",\n",
    "            if np.max(np.abs(V - old_V)) < 1e-6:\\n\",\n",
    "                print(f\\\"Converged after {iteration + 1} iterations\\\")\\n\",\n",
    "                break\\n\",\n",
    "        \\n\",\n",
    "        return V, policy\\n\",\n",
    "    \\n\",\n",
    "    def optimistic_value_iteration(self, beta=1.0, max_iterations=100):\\n\",\n",
    "        \\\"\\\"\\\"Value iteration with optimistic model estimates\\\"\\\"\\\"\\n\",\n",
    "        \\n\",\n",
    "        V = np.zeros(self.num_states)\\n\",\n",
    "        policy = np.zeros(self.num_states, dtype=int)\\n\",\n",
    "        \\n\",\n",
    "        print(f\\\"Running Optimistic Value Iteration (beta={beta})\\\")\\n\",\n",
    "        \\n\",\n",
    "        for iteration in range(max_iterations):\\n\",\n",
    "            old_V = V.copy()\\n\",\n",
    "            \\n\",\n",
    "            for state in range(self.num_states):\\n\",\n",
    "                q_values = np.zeros(self.num_actions)\\n\",\n",
    "                \\n\",\n",
    "                for action in range(self.num_actions):\\n\",\n",
    "                    # Get predictions from ensemble\\n\",\n",
    "                    state_onehot = np.eye(self.num_states)[state:state+1]\\n\",\n",
    "                    action_tensor = np.array([action])\\n\",\n",
    "                    \\n\",\n",
    "                    # Convert to tensors\\n\",\n",
    "                    state_tensor = torch.FloatTensor(state_onehot).to(device)\\n\",\n",
    "                    action_tensor = torch.LongTensor(action_tensor).to(device)\\n\",\n",
    "                    \\n\",\n",
    "                    # Get ensemble predictions\\n\",\n",
    "                    next_state_mean, reward_mean, next_state_std, reward_std = \\\\\\n\",\n",
    "                        self.ensemble_model.predict_with_uncertainty(state_tensor, action_tensor)\\n\",\n",
    "                    \\n\",\n",
    "                    # Use optimistic estimates\\n\",\n",
    "                    optimistic_reward = reward_mean.cpu().item() + beta * reward_std.cpu().item()\\n\",\n",
    "                    \\n\",\n",
    "                    # For simplicity, assume deterministic next state\\n\",\n",
    "                    next_state_pred = next_state_mean.cpu().numpy()[0]\\n\",\n",
    "                    next_state_idx = np.argmax(next_state_pred)\\n\",\n",
    "                    \\n\",\n",
    "                    q_values[action] = optimistic_reward + self.gamma * old_V[next_state_idx]\\n\",\n",
    "                \\n\",\n",
    "                V[state] = np.max(q_values)\\n\",\n",
    "                policy[state] = np.argmax(q_values)\\n\",\n",
    "            \\n\",\n",
    "            if np.max(np.abs(V - old_V)) < 1e-6:\\n\",\n",
    "                print(f\\\"Converged after {iteration + 1} iterations\\\")\\n\",\n",
    "                break\\n\",\n",
    "        \\n\",\n",
    "        return V, policy\\n\",\n",
    "\"\\n\",\n",
    "\"class ModelBasedPolicySearch:\\n\",\n",
    "\"    \\\"\\\"\\\"Policy search using learned models\\\"\\\"\\\"\\n\",\n",
    "\"    \\n\",\n",
    "\"    def __init__(self, model, state_dim, action_dim, gamma=0.99):\\n\",\n",
    "\"        self.model = model\\n\",\n",
    "\"        self.state_dim = state_dim\\n\",\n",
    "\"        self.action_dim = action_dim\\n\",\n",
    "\"        self.gamma = gamma\\n\",\n",
    "\"    \\n\",\n",
    "\"    def random_shooting(self, initial_state, horizon=10, num_sequences=1000):\\n\",\n",
    "\"        \\\"\\\"\\\"Random shooting with learned model\\\"\\\"\\\"\\n\",\n",
    "\"        \\n\",\n",
    "\"        best_sequence = None\\n\",\n",
    "\"        best_value = -np.inf\\n\",\n",
    "\"        \\n\",\n",
    "\"        for _ in range(num_sequences):\\n\",\n",
    "\"            # Sample random action sequence\\n\",\n",
    "\"            action_sequence = np.random.randint(0, self.action_dim, horizon)\\n\",\n",
    "\"            \\n\",\n",
    "\"            # Evaluate sequence using model\\n\",\n",
    "\"            total_reward = 0\\n\",\n",
    "\"            current_state = initial_state\\n\",\n",
    "\"            discount = 1.0\\n\",\n",
    "\"            \\n\",\n",
    "\"            for action in action_sequence:\\n\",\n",
    "\"                next_state, reward = self.model.sample_transition(current_state, action)\\n\",\n",
    "\"                total_reward += discount * reward\\n\",\n",
    "\"                discount *= self.gamma\\n\",\n",
    "\"                current_state = next_state\\n\",\n",
    "\"            \\n\",\n",
    "\"            if total_reward > best_value:\\n\",\n",
    "\"                best_value = total_reward\\n\",\n",
    "\"                best_sequence = action_sequence\\n\",\n",
    "\"        \\n\",\n",
    "\"        return best_sequence, best_value\\n\",\n",
    "\"    \\n\",\n",
    "\"    def cross_entropy_method(self, initial_state, horizon=10, num_sequences=1000, \\n\",\n",
    "\"                           num_elite=100, num_iterations=10):\\n\",\n",
    "\"        \\\"\\\"\\\"Cross-entropy method for policy search\\\"\\\"\\\"\\n\",\n",
    "\"        \\n\",\n",
    "\"        # Initialize action probabilities (uniform)\\n\",\n",
    "\"        action_probs = np.ones((horizon, self.action_dim)) / self.action_dim\\n\",\n",
    "\"        \\n\",\n",
    "\"        for iteration in range(num_iterations):\\n\",\n",
    "\"            # Sample action sequences\\n\",\n",
    "\"            sequences = []\\n\",\n",
    "\"            values = []\\n\",\n",
    "\"            \\n\",\n",
    "\"            for _ in range(num_sequences):\\n\",\n",
    "\"                sequence = []\\n\",\n",
    "\"                for t in range(horizon):\\n\",\n",
    "\"                    action = np.random.choice(self.action_dim, p=action_probs[t])\\n\",\n",
    "\"                    sequence.append(action)\\n\",\n",
    "\"                \\n\",\n",
    "\"                # Evaluate sequence\\n\",\n",
    "\"                total_reward = 0\\n\",\n",
    "\"                current_state = initial_state\\n\",\n",
    "\"                discount = 1.0\\n\",\n",
    "\"                \\n\",\n",
    "\"                for action in sequence:\\n\",\n",
    "\"                    next_state, reward = self.model.sample_transition(current_state, action)\\n\",\n",
    "\"                    total_reward += discount * reward\\n\",\n",
    "\"                    discount *= self.gamma\\n\",\n",
    "\"                    current_state = next_state\\n\",\n",
    "\"                \\n\",\n",
    "\"                sequences.append(sequence)\\n\",\n",
    "\"                values.append(total_reward)\\n\",\n",
    "\"            \\n\",\n",
    "\"            # Select elite sequences\\n\",\n",
    "\"            elite_indices = np.argsort(values)[-num_elite:]\\n\",\n",
    "\"            elite_sequences = [sequences[i] for i in elite_indices]\\n\",\n",
    "\"            \\n\",\n",
    "\"            # Update action probabilities\\n\",\n",
    "\"            action_counts = np.zeros((horizon, self.action_dim))\\n\",\n",
    "\"            \\n\",\n",
    "\"            for sequence in elite_sequences:\\n\",\n",
    "\"                for t, action in enumerate(sequence):\\n\",\n",
    "\"                    action_counts[t, action] += 1\\n\",\n",
    "\"            \\n\",\n",
    "\"            # Smooth update\\n\",\n",
    "\"            alpha = 0.7\\n\",\n",
    "\"            new_probs = action_counts / num_elite\\n\",\n",
    "\"            action_probs = alpha * new_probs + (1 - alpha) * action_probs\\n\",\n",
    "\"            \\n\",\n",
    "\"            # Add small amount of noise for exploration\\n\",\n",
    "\"            action_probs += 0.01 / self.action_dim\\n\",\n",
    "\"            action_probs /= np.sum(action_probs, axis=1, keepdims=True)\\n\",\n",
    "\"        \\n\",\n",
    "\"        # Return best sequence\\n\",\n",
    "\"        best_sequence = elite_sequences[np.argmax([values[i] for i in elite_indices])]\\n\",\n",
    "\"        best_value = max([values[i] for i in elite_indices])\\n\",\n",
    "\"        \\n\",\n",
    "\"        return best_sequence, best_value\\n\",\n",
    "\"\\n\",\n",
    "\"# Demonstration of classical planning\\n\",\n",
    "\"print(\\\"Classical Planning with Learned Models\\\")\\n\",\n",
    "\"print(\\\"=\\\" * 50)\\n\",\n",
    "\"\\n\",\n",
    "\"# Use the tabular model we learned earlier\\n\",\n",
    "\"planner = ModelBasedPlanner(tabular_model, env.num_states, env.num_actions, gamma=0.95)\\n\",\n",
    "\"\\n\",\n",
    "\"print(\\\"\\\\n1. Value Iteration with Learned Model:\\\")\\n\",\n",
    "\"vi_values, vi_policy = planner.value_iteration(max_iterations=50)\\n\",\n",
    "\"\\n\",\n",
    "\"print(\\\"\\\\n2. Policy Iteration with Learned Model:\\\")\\n\",\n",
    "\"planner_pi = ModelBasedPlanner(tabular_model, env.num_states, env.num_actions, gamma=0.95)\\n\",\n",
    "\"pi_values, pi_policy = planner_pi.policy_iteration(max_iterations=20)\\n\",\n",
    "\"\\n\",\n",
    "\"# Compare with uncertainty-aware planning\\n\",\n",
    "\"print(\\\"\\\\n3. Uncertainty-Aware Planning:\\\")\\n\",\n",
    "\"uncertainty_planner = UncertaintyAwarePlanner(neural_model, env.num_states, env.num_actions)\\n\",\n",
    "\"pessimistic_V, pessimistic_policy = uncertainty_planner.pessimistic_value_iteration(beta=0.5)\\n\",\n",
    "\"optimistic_V, optimistic_policy = uncertainty_planner.optimistic_value_iteration(beta=0.5)\\n\",\n",
    "\"\\n\",\n",
    "\"# Visualize results\\n\",\n",
    "\"fig, axes = plt.subplots(2, 3, figsize=(18, 12))\\n\",\n",
    "\"\\n\",\n",
    "\"# Reshape values and policies for visualization\\n\",\n",
    "\"grid_size = int(np.sqrt(env.num_states))\\n\",\n",
    "\"\\n\",\n",
    "\"def plot_value_function(ax, values, title):\\n\",\n",
    "\"    value_grid = values.reshape(grid_size, grid_size)\\n\",\n",
    "\"    im = ax.imshow(value_grid, cmap='viridis')\\n\",\n",
    "\"    ax.set_title(title)\\n\",\n",
    "\"    plt.colorbar(im, ax=ax)\\n\",\n",
    "\"    \\n\",\n",
    "\"def plot_policy(ax, policy, title):\\n\",\n",
    "\"    policy_grid = policy.reshape(grid_size, grid_size)\\n\",\n",
    "\"    # Map actions to arrows: 0=â†‘, 1=â†“, 2=â†, 3=â†’\\n\",\n",
    "\"    arrow_map = {0: 'â†‘', 1: 'â†“', 2: 'â†', 3: 'â†’'}\\n\",\n",
    "\"    \\n\",\n",
    "\"    ax.imshow(np.zeros((grid_size, grid_size)), cmap='gray', alpha=0.3)\\n\",\n",
    "\"    \\n\",\n",
    "\"    for i in range(grid_size):\\n\",\n",
    "\"        for j in range(grid_size):\\n\",\n",
    "\"            action = policy_grid[i, j]\\n\",\n",
    "\"            ax.text(j, i, arrow_map[action], ha='center', va='center', \\n\",\n",
    "\"                   fontsize=20, fontweight='bold', color='blue')\\n\",\n",
    "\"    \\n\",\n",
    "\"    ax.set_title(title)\\n\",\n",
    "\"    ax.set_xticks([])\\n\",\n",
    "\"    ax.set_yticks([])\\n\",\n",
    "\"\\n\",\n",
    "\"# Plot value functions\\n\",\n",
    "\"plot_value_function(axes[0, 0], vi_values, 'Value Iteration - Values')\\n\",\n",
    "\"plot_value_function(axes[0, 1], pi_values, 'Policy Iteration - Values')\\n\",\n",
    "\"plot_value_function(axes[0, 2], pessimistic_V, 'Pessimistic Planning - Values')\\n\",\n",
    "\"\\n\",\n",
    "\"# Plot policies\\n\",\n",
    "\"plot_policy(axes[1, 0], vi_policy, 'Value Iteration - Policy')\\n\",\n",
    "\"plot_policy(axes[1, 1], pi_policy, 'Policy Iteration - Policy')\\n\",\n",
    "\"plot_policy(axes[1, 2], pessimistic_policy, 'Pessimistic Planning - Policy')\\n\",\n",
    "\"\\n\",\n",
    "\"plt.tight_layout()\\n\",\n",
    "\"plt.show()\\n\",\n",
    "\"\\n\",\n",
    "\"# Compare planning methods\\n\",\n",
    "\"print(\\\"\\\\n4. Planning Method Comparison:\\\")\\n\",\n",
    "\"print(f\\\"Value Iteration - Max Value: {np.max(vi_values):.3f}, Policy Changes: {len(planner.value_history)}\\\")\\n\",\n",
    "\"print(f\\\"Policy Iteration - Max Value: {np.max(pi_values):.3f}, Policy Changes: {len(planner_pi.value_history)}\\\")\\n\",\n",
    "\"print(f\\\"Pessimistic Planning - Max Value: {np.max(pessimistic_V):.3f}\\\")\\n\",\n",
    "\"print(f\\\"Optimistic Planning - Max Value: {np.max(optimistic_V):.3f}\\\")\\n\",\n",
    "\"\\n\",\n",
    "\"# Test policy search methods\\n\",\n",
    "\"print(\\\"\\\\n5. Model-Based Policy Search:\\\")\\n\",\n",
    "\"policy_searcher = ModelBasedPolicySearch(tabular_model, env.num_states, env.num_actions)\\n\",\n",
    "\"\\n\",\n",
    "\"# Random shooting\\n\",\n",
    "\"initial_state = 0\\n\",\n",
    "\"best_sequence_rs, best_value_rs = policy_searcher.random_shooting(initial_state, horizon=5, num_sequences=500)\\n\",\n",
    "\"print(f\\\"Random Shooting - Best Value: {best_value_rs:.3f}, Best Sequence: {best_sequence_rs}\\\")\\n\",\n",
    "\"\\n\",\n",
    "\"# Cross-entropy method\\n\",\n",
    "\"best_sequence_cem, best_value_cem = policy_searcher.cross_entropy_method(initial_state, horizon=5, \\n\",\n",
    "\"                                                                        num_sequences=200, num_elite=20)\\n\",\n",
    "\"print(f\\\"Cross-Entropy Method - Best Value: {best_value_cem:.3f}, Best Sequence: {best_sequence_cem}\\\")\\n\",\n",
    "\"\\n\",\n",
    "\"print(\\\"\\\\nâœ… Classical planning with learned models complete!\\\")\\n\",\n",
    "\"print(\\\"ðŸ“Š Next: Dyna-Q algorithm - integrating planning and learning\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c05caa9",
   "metadata": {},
   "source": [
    "# Section 4: Dyna-Q Algorithm - Integrating Planning and Learning\n",
    "\n",
    "## 4.1 The Dyna Architecture\n",
    "\n",
    "Dyna-Q is a pioneering algorithm that integrates **direct reinforcement learning** with **planning** using a learned model. It represents one of the first successful attempts to combine model-free and model-based approaches.\n",
    "\n",
    "### The Dyna Framework Components:\n",
    "\n",
    "1. **Direct RL**: Learn from real experience using Q-learning\n",
    "2. **Model Learning**: Learn environment model from real experience  \n",
    "3. **Planning**: Use learned model to generate simulated experience\n",
    "4. **Policy**: Act using Îµ-greedy policy based on Q-values\n",
    "\n",
    "### Dyna-Q Algorithm Structure:\n",
    "\n",
    "```\n",
    "For each episode:\n",
    "    For each step:\n",
    "        1. Take action using Îµ-greedy policy\n",
    "        2. Observe reward and next state\n",
    "        3. Update Q-function with real experience\n",
    "        4. Update model with real experience\n",
    "        5. Planning: Do n planning updates using model\n",
    "            - Sample random state-action pair\n",
    "            - Use model to predict next state and reward\n",
    "            - Update Q-function with simulated experience\n",
    "```\n",
    "\n",
    "## 4.2 Mathematical Formulation\n",
    "\n",
    "### Direct Learning (Q-Learning):\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "### Model Learning:\n",
    "- **Transition Model**: $\\hat{P}(s'|s,a)$ learned from observed transitions\n",
    "- **Reward Model**: $\\hat{R}(s,a)$ learned from observed rewards\n",
    "\n",
    "### Planning Updates:\n",
    "For randomly sampled $(s,a)$ pairs:\n",
    "1. Generate simulated experience: $(s',r) \\sim \\hat{P}(\\cdot|s,a), \\hat{R}(s,a)$\n",
    "2. Update Q-function: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$\n",
    "\n",
    "## 4.3 Key Advantages of Dyna-Q\n",
    "\n",
    "**Sample Efficiency**:\n",
    "- Each real experience is used multiple times (once for direct learning, multiple times for planning)\n",
    "- Can learn much faster than pure model-free methods\n",
    "\n",
    "**Adaptability**:\n",
    "- Continues to work even if model is inaccurate\n",
    "- Direct learning corrects for model errors\n",
    "\n",
    "**Simplicity**:\n",
    "- Easy to implement and understand\n",
    "- Natural extension of Q-learning\n",
    "\n",
    "**Flexibility**:\n",
    "- Can adjust planning steps based on computational budget\n",
    "- Works with any model learning approach\n",
    "\n",
    "## 4.4 Dyna-Q+ Extensions\n",
    "\n",
    "**Exploration Bonus**:\n",
    "- Add bonus for actions not taken recently\n",
    "- Encourages exploration of potentially changed parts of environment\n",
    "\n",
    "**Prioritized Updates**:\n",
    "- Focus planning on states with largest Q-value changes\n",
    "- More efficient use of planning computation\n",
    "\n",
    "**Dyna-H**:\n",
    "- Use learned models for hierarchical planning\n",
    "- Plan at multiple temporal scales\n",
    "\n",
    "## 4.5 When Model Changes: The Blocking Maze\n",
    "\n",
    "A classic example where Dyna-Q demonstrates its adaptability is the \"blocking maze\" scenario:\n",
    "\n",
    "1. **Initial Phase**: Agent learns optimal path through maze\n",
    "2. **Environment Change**: Optimal path becomes blocked, new path opens\n",
    "3. **Adaptation**: Dyna-Q must discover the environment change and adapt\n",
    "\n",
    "This scenario highlights the importance of continued exploration even with a seemingly good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dyna-Q Algorithm Implementation\n",
    "\n",
    "class DynaQAgent:\n",
    "    \"\"\"Dyna-Q Agent implementing integrated planning and learning\"\"\"\n",
    "    \n",
    "    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.95, epsilon=0.1, planning_steps=5):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.planning_steps = planning_steps\n",
    "        \n",
    "        # Q-function\n",
    "        self.Q = np.zeros((num_states, num_actions))\n",
    "        \n",
    "        # Model components\n",
    "        self.model = {}  # Dictionary to store (s,a) -> (r, s') mappings\n",
    "        self.visited_state_actions = set()  # Track visited (s,a) pairs\n",
    "        \n",
    "        # Learning statistics\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        self.planning_updates = 0\n",
    "        self.direct_updates = 0\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Îµ-greedy action selection\"\"\"\n",
    "        if np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.num_actions)\n",
    "        else:\n",
    "            return np.argmax(self.Q[state])\n",
    "    \n",
    "    def update_q_function(self, state, action, reward, next_state):\n",
    "        \"\"\"Q-learning update\"\"\"\n",
    "        td_target = reward + self.gamma * np.max(self.Q[next_state])\n",
    "        td_error = td_target - self.Q[state, action]\n",
    "        self.Q[state, action] += self.alpha * td_error\n",
    "        return td_error\n",
    "    \n",
    "    def update_model(self, state, action, reward, next_state):\n",
    "        \"\"\"Update internal model with new experience\"\"\"\n",
    "        self.model[(state, action)] = (reward, next_state)\n",
    "        self.visited_state_actions.add((state, action))\n",
    "    \n",
    "    def planning_update(self):\n",
    "        \"\"\"Perform planning updates using learned model\"\"\"\n",
    "        if len(self.visited_state_actions) == 0:\n",
    "            return\n",
    "        \n",
    "        for _ in range(self.planning_steps):\n",
    "            # Randomly sample a previously visited state-action pair\n",
    "            state, action = random.choice(list(self.visited_state_actions))\n",
    "            \n",
    "            # Get model prediction\n",
    "            if (state, action) in self.model:\n",
    "                reward, next_state = self.model[(state, action)]\n",
    "                \n",
    "                # Update Q-function with simulated experience\n",
    "                self.update_q_function(state, action, reward, next_state)\n",
    "                self.planning_updates += 1\n",
    "    \n",
    "    def train_episode(self, env, max_steps=200):\n",
    "        \"\"\"Train for one episode\"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Select and take action\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            # Direct learning: update Q-function with real experience\n",
    "            self.update_q_function(state, action, reward, next_state)\n",
    "            self.direct_updates += 1\n",
    "            \n",
    "            # Model learning: update model with real experience\n",
    "            self.update_model(state, action, reward, next_state)\n",
    "            \n",
    "            # Planning: perform planning updates\n",
    "            self.planning_update()\\n\",\n",
    "            \\n\",\n",
    "            total_reward += reward\\n\",\n",
    "            steps += 1\\n\",\n",
    "            \\n\",\n",
    "            if done:\\n\",\n",
    "                break\\n\",\n",
    "                \\n\",\n",
    "            state = next_state\\n\",\n",
    "        \\n\",\n",
    "        self.episode_rewards.append(total_reward)\\n\",\n",
    "        self.episode_lengths.append(steps)\\n\",\n",
    "        \\n\",\n",
    "        return total_reward, steps\\n\",\n",
    "    \\n\",\n",
    "    def get_statistics(self):\\n\",\n",
    "        \\\"\\\"\\\"Get training statistics\\\"\\\"\\\"\\n\",\n",
    "        return {\\n\",\n",
    "            'direct_updates': self.direct_updates,\\n\",\n",
    "            'planning_updates': self.planning_updates,\\n\",\n",
    "            'model_size': len(self.model),\\n\",\n",
    "            'avg_episode_reward': np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0\\n\",\n",
    "        }\\n\",\n",
    "\\n\",\n",
    "class DynaQPlusAgent(DynaQAgent):\\n\",\n",
    "    \\\"\\\"\\\"Dyna-Q+ with exploration bonus for changed environments\\\"\\\"\\\"\\n\",\n",
    "    \\n\",\n",
    "    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.95, epsilon=0.1, \\n\",\n",
    "                 planning_steps=5, kappa=0.001):\\n\",\n",
    "        super().__init__(num_states, num_actions, alpha, gamma, epsilon, planning_steps)\\n\",\n",
    "        \\n\",\n",
    "        self.kappa = kappa  # Exploration bonus weight\\n\",\n",
    "        self.last_visit_time = {}  # Track when each (s,a) was last tried\\n\",\n",
    "        self.current_time = 0\\n\",\n",
    "        \\n\",\n",
    "    def update_q_function(self, state, action, reward, next_state, is_real_experience=True):\\n\",\n",
    "        \\\"\\\"\\\"Enhanced Q-learning update with exploration bonus\\\"\\\"\\\"\\n\",\n",
    "        \\n\",\n",
    "        if is_real_experience:\\n\",\n",
    "            # Update visit time for real experiences\\n\",\n",
    "            self.last_visit_time[(state, action)] = self.current_time\\n\",\n",
    "            self.current_time += 1\\n\",\n",
    "        \\n\",\n",
    "        # Add exploration bonus for planning updates\\n\",\n",
    "        exploration_bonus = 0\\n\",\n",
    "        if not is_real_experience and (state, action) in self.last_visit_time:\\n\",\n",
    "            time_since_visit = self.current_time - self.last_visit_time[(state, action)]\\n\",\n",
    "            exploration_bonus = self.kappa * np.sqrt(time_since_visit)\\n\",\n",
    "        \\n\",\n",
    "        td_target = reward + exploration_bonus + self.gamma * np.max(self.Q[next_state])\\n\",\n",
    "        td_error = td_target - self.Q[state, action]\\n\",\n",
    "        self.Q[state, action] += self.alpha * td_error\\n\",\n",
    "        \\n\",\n",
    "        return td_error\\n\",\n",
    "    \\n\",\n",
    "    def planning_update(self):\\n\",\n",
    "        \\\"\\\"\\\"Planning update with exploration bonus\\\"\\\"\\\"\\n\",\n",
    "        if len(self.visited_state_actions) == 0:\\n\",\n",
    "            return\\n\",\n",
    "        \\n\",\n",
    "        for _ in range(self.planning_steps):\\n\",\n",
    "            state, action = random.choice(list(self.visited_state_actions))\\n\",\n",
    "            \\n\",\n",
    "            if (state, action) in self.model:\\n\",\n",
    "                reward, next_state = self.model[(state, action)]\\n\",\n",
    "                \\n\",\n",
    "                # Planning update with exploration bonus\\n\",\n",
    "                self.update_q_function(state, action, reward, next_state, is_real_experience=False)\\n\",\n",
    "                self.planning_updates += 1\\n\",\n",
    "    \\n\",\n",
    "    def train_episode(self, env, max_steps=200):\\n\",\n",
    "        \\\"\\\"\\\"Training episode with proper experience tracking\\\"\\\"\\\"\\n\",\n",
    "        state = env.reset()\\n\",\n",
    "        total_reward = 0\\n\",\n",
    "        steps = 0\\n\",\n",
    "        \\n\",\n",
    "        for step in range(max_steps):\\n\",\n",
    "            action = self.select_action(state)\\n\",\n",
    "            next_state, reward, done = env.step(action)\\n\",\n",
    "            \\n\",\n",
    "            # Direct learning with real experience flag\\n\",\n",
    "            self.update_q_function(state, action, reward, next_state, is_real_experience=True)\\n\",\n",
    "            self.direct_updates += 1\\n\",\n",
    "            \\n\",\n",
    "            self.update_model(state, action, reward, next_state)\\n\",\n",
    "            self.planning_update()\\n\",\n",
    "            \\n\",\n",
    "            total_reward += reward\\n\",\n",
    "            steps += 1\\n\",\n",
    "            \\n\",\n",
    "            if done:\\n\",\n",
    "                break\\n\",\n",
    "                \\n\",\n",
    "            state = next_state\\n\",\n",
    "        \\n\",\n",
    "        self.episode_rewards.append(total_reward)\\n\",\n",
    "        self.episode_lengths.append(steps)\\n\",\n",
    "        \\n\",\n",
    "        return total_reward, steps\\n\",\n",
    "\\n\",\n",
    "class BlockingMaze:\\n\",\n",
    "    \\\"\\\"\\\"Environment that changes to test Dyna-Q adaptability\\\"\\\"\\\"\\n\",\n",
    "    \\n\",\n",
    "    def __init__(self, width=9, height=6, change_episode=1000):\\n\",\n",
    "        self.width = width\\n\",\n",
    "        self.height = height\\n\",\n",
    "        self.num_states = width * height\\n\",\n",
    "        self.num_actions = 4  # up, down, left, right\\n\",\n",
    "        self.change_episode = change_episode\\n\",\n",
    "        self.episode_count = 0\\n\",\n",
    "        \\n\",\n",
    "        # Define maze layout\\n\",\n",
    "        self.start_pos = (0, 3)  # Start position\\n\",\n",
    "        self.goal_pos = (8, 0)   # Goal position\\n\",\n",
    "        \\n\",\n",
    "        # Initial blocking configuration\\n\",\n",
    "        self.blocked_cells = set()\\n\",\n",
    "        self.setup_initial_maze()\\n\",\n",
    "        \\n\",\n",
    "        self.state = self.pos_to_state(self.start_pos)\\n\",\n",
    "    \\n\",\n",
    "    def pos_to_state(self, pos):\\n\",\n",
    "        \\\"\\\"\\\"Convert (x, y) position to state index\\\"\\\"\\\"\\n\",\n",
    "        return pos[1] * self.width + pos[0]\\n\",\n",
    "    \\n\",\n",
    "    def state_to_pos(self, state):\\n\",\n",
    "        \\\"\\\"\\\"Convert state index to (x, y) position\\\"\\\"\\\"\\n\",\n",
    "        return (state % self.width, state // self.width)\\n\",\n",
    "    \\n\",\n",
    "    def setup_initial_maze(self):\\n\",\n",
    "        \\\"\\\"\\\"Setup initial maze with one path blocked\\\"\\\"\\\"\\n\",\n",
    "        # Block initial shortcut path\\n\",\n",
    "        for y in range(1, 4):\\n\",\n",
    "            self.blocked_cells.add((3, y))\\n\",\n",
    "        \\n\",\n",
    "        # Later, we'll open this and block the long way\\n\",\n",
    "        self.initial_blocks = self.blocked_cells.copy()\\n\",\n",
    "        \\n\",\n",
    "        # Alternative blocks for changed environment\\n\",\n",
    "        self.changed_blocks = set()\\n\",\n",
    "        for x in range(1, 8):\\n\",\n",
    "            self.changed_blocks.add((x, 2))\\n\",\n",
    "    \\n\",\n",
    "    def reset(self):\\n\",\n",
    "        \\\"\\\"\\\"Reset environment\\\"\\\"\\\"\\n\",\n",
    "        self.episode_count += 1\\n\",\n",
    "        \\n\",\n",
    "        # Change environment after specified episodes\\n\",\n",
    "        if self.episode_count == self.change_episode:\\n\",\n",
    "            self.blocked_cells = self.changed_blocks.copy()\\n\",\n",
    "            print(f\\\"\\\\n*** Environment changed at episode {self.episode_count} ***\\\")\\n\",\n",
    "        \\n\",\n",
    "        self.state = self.pos_to_state(self.start_pos)\\n\",\n",
    "        return self.state\\n\",\n",
    "    \\n\",\n",
    "    def step(self, action):\\n\",\n",
    "        \\\"\\\"\\\"Take action in environment\\\"\\\"\\\"\\n\",\n",
    "        current_pos = self.state_to_pos(self.state)\\n\",\n",
    "        x, y = current_pos\\n\",\n",
    "        \\n\",\n",
    "        # Actions: 0=up, 1=down, 2=left, 3=right\\n\",\n",
    "        if action == 0 and y > 0:  # up\\n\",\n",
    "            new_pos = (x, y - 1)\\n\",\n",
    "        elif action == 1 and y < self.height - 1:  # down\\n\",\n",
    "            new_pos = (x, y + 1)\\n\",\n",
    "        elif action == 2 and x > 0:  # left\\n\",\n",
    "            new_pos = (x - 1, y)\\n\",\n",
    "        elif action == 3 and x < self.width - 1:  # right\\n\",\n",
    "            new_pos = (x + 1, y)\\n\",\n",
    "        else:\\n\",\n",
    "            new_pos = current_pos  # Invalid move, stay in place\\n\",\n",
    "        \\n\",\n",
    "        # Check if new position is blocked\\n\",\n",
    "        if new_pos in self.blocked_cells:\\n\",\n",
    "            new_pos = current_pos  # Can't move into blocked cell\\n\",\n",
    "        \\n\",\n",
    "        self.state = self.pos_to_state(new_pos)\\n\",\n",
    "        \\n\",\n",
    "        # Reward structure\\n\",\n",
    "        if new_pos == self.goal_pos:\\n\",\n",
    "            reward = 1.0\\n\",\n",
    "            done = True\\n\",\n",
    "        else:\\n\",\n",
    "            reward = 0.0\\n\",\n",
    "            done = False\\n\",\n",
    "        \\n\",\n",
    "        return self.state, reward, done\\n\",\n",
    "    \\n\",\n",
    "    def render_maze(self):\\n\",\n",
    "        \\\"\\\"\\\"Render current maze state\\\"\\\"\\\"\\n\",\n",
    "        maze = np.zeros((self.height, self.width))\\n\",\n",
    "        \\n\",\n",
    "        # Mark blocked cells\\n\",\n",
    "        for x, y in self.blocked_cells:\\n\",\n",
    "            maze[y, x] = -1\\n\",\n",
    "        \\n\",\n",
    "        # Mark start and goal\\n\",\n",
    "        maze[self.start_pos[1], self.start_pos[0]] = 2\\n\",\n",
    "        maze[self.goal_pos[1], self.goal_pos[0]] = 3\\n\",\n",
    "        \\n\",\n",
    "        # Mark current position\\n\",\n",
    "        current_pos = self.state_to_pos(self.state)\\n\",\n",
    "        if current_pos != self.start_pos and current_pos != self.goal_pos:\\n\",\n",
    "            maze[current_pos[1], current_pos[0]] = 1\\n\",\n",
    "        \\n\",\n",
    "        return maze\\n\",\n",
    "\\n\",\n",
    "\"# Comprehensive Dyna-Q Demonstration\\n\",\n",
    "\"print(\\\"Dyna-Q Algorithm Demonstration\\\")\\n\",\n",
    "\"print(\\\"=\\\" * 50)\\n\",\n",
    "\"\\n\",\n",
    "\"# Compare different agents\\n\",\n",
    "\"agents = {\\n\",\n",
    "\"    'Q-Learning': DynaQAgent(25, 4, planning_steps=0),  # No planning\\n\",\n",
    "\"    'Dyna-Q (n=5)': DynaQAgent(25, 4, planning_steps=5),\\n\",\n",
    "\"    'Dyna-Q (n=50)': DynaQAgent(25, 4, planning_steps=50),\\n\",\n",
    "\"    'Dyna-Q+ (n=5)': DynaQPlusAgent(25, 4, planning_steps=5, kappa=0.001)\\n\",\n",
    "\"}\\n\",\n",
    "\"\\n\",\n",
    "\"# Training on simple gridworld first\\n\",\n",
    "\"print(\\\"\\\\n1. Training on Simple GridWorld:\\\")\\n\",\n",
    "\"simple_env = SimpleGridWorld(size=5)\\n\",\n",
    "\"\\n\",\n",
    "\"results = {}\\n\",\n",
    "\"n_episodes = 200\\n\",\n",
    "\"\\n\",\n",
    "\"for name, agent in agents.items():\\n\",\n",
    "\"    print(f\\\"\\\\nTraining {name}...\\\")\\n\",\n",
    "\"    episode_rewards = []\\n\",\n",
    "\"    \\n\",\n",
    "\"    for episode in range(n_episodes):\\n\",\n",
    "\"        reward, _ = agent.train_episode(simple_env, max_steps=100)\\n\",\n",
    "\"        episode_rewards.append(reward)\\n\",\n",
    "\"        \\n\",\n",
    "\"        if (episode + 1) % 50 == 0:\\n\",\n",
    "\"            avg_reward = np.mean(episode_rewards[-10:])\\n\",\n",
    "\"            stats = agent.get_statistics()\\n\",\n",
    "\"            print(f\\\"  Episode {episode+1}: Avg Reward = {avg_reward:.3f}, \\\"\\n\",\n",
    "\"                  f\\\"Direct Updates = {stats['direct_updates']}, \\\"\\n\",\n",
    "\"                  f\\\"Planning Updates = {stats['planning_updates']}\\\")\\n\",\n",
    "\"    \\n\",\n",
    "\"    results[name] = {\\n\",\n",
    "\"        'episode_rewards': agent.episode_rewards.copy(),\\n\",\n",
    "\"        'statistics': agent.get_statistics()\\n\",\n",
    "\"    }\\n\",\n",
    "\"\\n\",\n",
    "\"# Visualize learning curves\\n\",\n",
    "\"plt.figure(figsize=(15, 10))\\n\",\n",
    "\"\\n\",\n",
    "\"plt.subplot(2, 2, 1)\\n\",\n",
    "\"colors = ['blue', 'red', 'green', 'orange']\\n\",\n",
    "\"for i, (name, data) in enumerate(results.items()):\\n\",\n",
    "\"    rewards = data['episode_rewards']\\n\",\n",
    "\"    # Smooth the rewards\\n\",\n",
    "\"    smoothed = pd.Series(rewards).rolling(window=10).mean()\\n\",\n",
    "\"    plt.plot(smoothed, label=name, color=colors[i], linewidth=2)\\n\",\n",
    "\"\\n\",\n",
    "\"plt.title('Learning Performance Comparison')\\n\",\n",
    "\"plt.xlabel('Episode')\\n\",\n",
    "\"plt.ylabel('Episode Reward (Smoothed)')\\n\",\n",
    "\"plt.legend()\\n\",\n",
    "\"plt.grid(True, alpha=0.3)\\n\",\n",
    "\"\\n\",\n",
    "\"# Show update statistics\\n\",\n",
    "\"plt.subplot(2, 2, 2)\\n\",\n",
    "\"agent_names = list(results.keys())\\n\",\n",
    "\"direct_updates = [results[name]['statistics']['direct_updates'] for name in agent_names]\\n\",\n",
    "\"planning_updates = [results[name]['statistics']['planning_updates'] for name in agent_names]\\n\",\n",
    "\"\\n\",\n",
    "\"x = np.arange(len(agent_names))\\n\",\n",
    "\"width = 0.35\\n\",\n",
    "\"\\n\",\n",
    "\"plt.bar(x - width/2, direct_updates, width, label='Direct Updates', alpha=0.7)\\n\",\n",
    "\"plt.bar(x + width/2, planning_updates, width, label='Planning Updates', alpha=0.7)\\n\",\n",
    "\"\\n\",\n",
    "\"plt.title('Update Statistics')\\n\",\n",
    "\"plt.xlabel('Agent')\\n\",\n",
    "\"plt.ylabel('Number of Updates')\\n\",\n",
    "\"plt.xticks(x, agent_names, rotation=45)\\n\",\n",
    "\"plt.legend()\\n\",\n",
    "\"plt.grid(True, alpha=0.3)\\n\",\n",
    "\"\\n\",\n",
    "\"# Test on blocking maze\\n\",\n",
    "\"print(\\\"\\\\n2. Testing on Blocking Maze (Environment Change):\\\")\\n\",\n",
    "\"maze_env = BlockingMaze(change_episode=100)\\n\",\n",
    "\"\\n\",\n",
    "\"# Create fresh agents for maze test\\n\",\n",
    "\"maze_agents = {\\n\",\n",
    "\"    'Dyna-Q': DynaQAgent(maze_env.num_states, maze_env.num_actions, planning_steps=50),\\n\",\n",
    "\"    'Dyna-Q+': DynaQPlusAgent(maze_env.num_states, maze_env.num_actions, planning_steps=50, kappa=0.01)\\n\",\n",
    "\"}\\n\",\n",
    "\"\\n\",\n",
    "\"maze_results = {}\\n\",\n",
    "\"n_episodes = 300\\n\",\n",
    "\"\\n\",\n",
    "\"for name, agent in maze_agents.items():\\n\",\n",
    "\"    print(f\\\"\\\\nTraining {name} on Blocking Maze...\\\")\\n\",\n",
    "\"    # Reset environment episode counter\\n\",\n",
    "\"    maze_env.episode_count = 0\\n\",\n",
    "\"    \\n\",\n",
    "\"    for episode in range(n_episodes):\\n\",\n",
    "\"        reward, steps = agent.train_episode(maze_env, max_steps=3000)\\n\",\n",
    "\"        \\n\",\n",
    "\"        if episode in [50, 99, 150, 200, 250]:\\n\",\n",
    "\"            print(f\\\"  Episode {episode+1}: Reward = {reward:.1f}, Steps = {steps}\\\")\\n\",\n",
    "\"    \\n\",\n",
    "\"    maze_results[name] = {\\n\",\n",
    "\"        'episode_rewards': agent.episode_rewards.copy(),\\n\",\n",
    "\"        'episode_lengths': agent.episode_lengths.copy()\\n\",\n",
    "\"    }\\n\",\n",
    "\"\\n\",\n",
    "\"# Plot maze results\\n\",\n",
    "\"plt.subplot(2, 2, 3)\\n\",\n",
    "\"for name, data in maze_results.items():\\n\",\n",
    "\"    rewards = data['episode_rewards']\\n\",\n",
    "\"    smoothed = pd.Series(rewards).rolling(window=20).mean()\\n\",\n",
    "\"    plt.plot(smoothed, label=name, linewidth=2)\\n\",\n",
    "\"\\n\",\n",
    "\"plt.axvline(x=100, color='red', linestyle='--', alpha=0.7, label='Environment Change')\\n\",\n",
    "\"plt.title('Blocking Maze Performance')\\n\",\n",
    "\"plt.xlabel('Episode')\\n\",\n",
    "\"plt.ylabel('Episode Reward (Smoothed)')\\n\",\n",
    "\"plt.legend()\\n\",\n",
    "\"plt.grid(True, alpha=0.3)\\n\",\n",
    "\"\\n\",\n",
    "\"plt.subplot(2, 2, 4)\\n\",\n",
    "\"for name, data in maze_results.items():\\n\",\n",
    "\"    lengths = data['episode_lengths']\\n\",\n",
    "\"    smoothed = pd.Series(lengths).rolling(window=20).mean()\\n\",\n",
    "\"    plt.plot(smoothed, label=name, linewidth=2)\\n\",\n",
    "\"\\n\",\n",
    "\"plt.axvline(x=100, color='red', linestyle='--', alpha=0.7, label='Environment Change')\\n\",\n",
    "\"plt.title('Episode Length (Steps to Goal)')\\n\",\n",
    "\"plt.xlabel('Episode')\\n\",\n",
    "\"plt.ylabel('Episode Length (Smoothed)')\\n\",\n",
    "\"plt.legend()\\n\",\n",
    "\"plt.grid(True, alpha=0.3)\\n\",\n",
    "\"\\n\",\n",
    "\"plt.tight_layout()\\n\",\n",
    "\"plt.show()\\n\",\n",
    "\"\\n\",\n",
    "\"# Analysis and insights\\n\",\n",
    "\"print(\\\"\\\\n3. Key Insights from Dyna-Q Experiments:\\\")\\n\",\n",
    "\"print(\\\"\\\\nSimple GridWorld Results:\\\")\\n\",\n",
    "\"for name, data in results.items():\\n\",\n",
    "\"    final_performance = np.mean(data['episode_rewards'][-20:])\\n\",\n",
    "\"    stats = data['statistics']\\n\",\n",
    "\"    efficiency = stats['planning_updates'] / max(stats['direct_updates'], 1)\\n\",\n",
    "\"    print(f\\\"  {name}: Final Performance = {final_performance:.3f}, \\\"\\n\",\n",
    "\"          f\\\"Planning Efficiency = {efficiency:.1f}x\\\")\\n\",\n",
    "\"\\n\",\n",
    "\"print(\\\"\\\\nBlocking Maze Results (Adaptability):\\\")\\n\",\n",
    "\"for name, data in maze_results.items():\\n\",\n",
    "\"    # Performance before and after change\\n\",\n",
    "\"    before_change = np.mean(data['episode_rewards'][80:100])\\n\",\n",
    "\"    after_change = np.mean(data['episode_rewards'][120:140])\\n\",\n",
    "\"    adaptation_speed = after_change - min(data['episode_rewards'][100:120])\\n\",\n",
    "\"    \\n\",\n",
    "\"    print(f\\\"  {name}: Performance before change = {before_change:.3f}, \\\"\\n\",\n",
    "\"          f\\\"after change = {after_change:.3f}, adaptation = {adaptation_speed:.3f}\\\")\\n\",\n",
    "\"\\n\",\n",
    "\"print(\\\"\\\\nðŸ“Š Key Takeaways:\\\")\\n\",\n",
    "\"print(\\\"â€¢ Dyna-Q achieves better sample efficiency through planning\\\")\\n\",\n",
    "\"print(\\\"â€¢ More planning steps generally improve performance\\\")\\n\",\n",
    "\"print(\\\"â€¢ Dyna-Q+ adapts better to environment changes\\\")\\n\",\n",
    "\"print(\\\"â€¢ Model-based methods excel when environment is stable\\\")\\n\",\n",
    "\"\\n\",\n",
    "\"print(\\\"\\\\nâœ… Dyna-Q algorithm demonstration complete!\\\")\\n\",\n",
    "\"print(\\\"ðŸ“Š Next: Monte Carlo Tree Search (MCTS)\\\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e0a69",
   "metadata": {},
   "source": [
    "# Section 5: Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "Monte Carlo Tree Search is a powerful planning algorithm that has achieved remarkable success in games like Go and has been extended to general reinforcement learning problems.\n",
    "\n",
    "## 5.1 Theoretical Foundation\n",
    "\n",
    "MCTS combines:\n",
    "- **Tree Search**: Systematic exploration of possible future states\n",
    "- **Monte Carlo Simulation**: Random rollouts to estimate value\n",
    "- **Multi-Armed Bandit**: UCB for action selection in tree nodes\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Selection**: Navigate from root to leaf using bandit strategy\n",
    "2. **Expansion**: Add one or more child nodes\n",
    "3. **Simulation**: Random rollout from new node\n",
    "4. **Backpropagation**: Update all nodes on path with result\n",
    "\n",
    "### UCB1 Formula for Node Selection:\n",
    "\n",
    "$$UCB1(i) = \\overline{X_i} + C\\sqrt{\\frac{\\ln n}{n_i}}$$\n",
    "\n",
    "Where:\n",
    "- $\\overline{X_i}$ = average reward of action i\n",
    "- $n_i$ = number of times action i was selected\n",
    "- $n$ = total number of selections\n",
    "- $C$ = exploration parameter\n",
    "\n",
    "### MCTS in Model-Based RL:\n",
    "\n",
    "MCTS can be used with learned models to perform sophisticated planning by building search trees that explore promising action sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397012bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Tree Search Implementation\n",
    "\n",
    "class MCTSNode:\n",
    "    \"\"\"Node in MCTS tree\"\"\"\n",
    "    \n",
    "    def __init__(self, state, parent=None, action=None):\n",
    "        self.state = state\n",
    "        self.parent = parent\n",
    "        self.action = action  # Action that led to this state\n",
    "        self.children = {}  # Action -> child node mapping\n",
    "        self.visits = 0\n",
    "        self.total_reward = 0.0\n",
    "        self.untried_actions = None  # Will be set when expanded\n",
    "        \n",
    "    def is_fully_expanded(self):\n",
    "        \"\"\"Check if all actions have been tried\"\"\"\n",
    "        return len(self.untried_actions) == 0\n",
    "    \n",
    "    def is_terminal(self):\n",
    "        \"\"\"Check if this is a terminal state\"\"\"\n",
    "        return len(self.children) == 0 and self.visits > 0 and self.untried_actions is not None and len(self.untried_actions) == 0\n",
    "    \n",
    "    def get_ucb_value(self, exploration_weight=1.0):\n",
    "        \"\"\"Calculate UCB value for node selection\"\"\"\n",
    "        if self.visits == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        exploitation = self.total_reward / self.visits\n",
    "        exploration = exploration_weight * np.sqrt(np.log(self.parent.visits) / self.visits)\n",
    "        return exploitation + exploration\n",
    "    \n",
    "    def select_child(self, exploration_weight=1.0):\n",
    "        \"\"\"Select child with highest UCB value\"\"\"\n",
    "        return max(self.children.values(), \n",
    "                  key=lambda child: child.get_ucb_value(exploration_weight))\n",
    "    \n",
    "    def expand(self, action, new_state):\n",
    "        \"\"\"Expand node by adding a child\"\"\"\n",
    "        if action in self.untried_actions:\n",
    "            self.untried_actions.remove(action)\n",
    "            \n",
    "        child = MCTSNode(state=new_state, parent=self, action=action)\n",
    "        self.children[action] = child\n",
    "        return child\n",
    "    \n",
    "    def update(self, reward):\n",
    "        \"\"\"Update node statistics\"\"\"\n",
    "        self.visits += 1\n",
    "        self.total_reward += reward\n",
    "    \n",
    "    def get_best_action(self):\n",
    "        \"\"\"Get action leading to most visited child\"\"\"\n",
    "        if not self.children:\n",
    "            return None\n",
    "        return max(self.children.items(), \n",
    "                  key=lambda item: item[1].visits)[0]\n",
    "\n",
    "class MCTS:\n",
    "    \"\"\"Monte Carlo Tree Search implementation\"\"\"\n",
    "    \n",
    "    def __init__(self, model, num_actions, exploration_weight=1.0, max_depth=50):\n",
    "        self.model = model\n",
    "        self.num_actions = num_actions\n",
    "        self.exploration_weight = exploration_weight\n",
    "        self.max_depth = max_depth\n",
    "        self.gamma = 0.95  # Discount factor\n",
    "        \n",
    "    def search(self, root_state, num_simulations=1000):\n",
    "        \"\"\"Perform MCTS to find best action\"\"\"\n",
    "        root = MCTSNode(root_state)\n",
    "        root.untried_actions = list(range(self.num_actions))\n",
    "        \n",
    "        for _ in range(num_simulations):\n",
    "            # 1. Selection: traverse tree to leaf\n",
    "            leaf = self._select_leaf(root)\n",
    "            \n",
    "            # 2. Expansion: add child if not terminal\n",
    "            if leaf.untried_actions and len(leaf.untried_actions) > 0:\n",
    "                action = np.random.choice(leaf.untried_actions)\n",
    "                next_state, reward, done = self._simulate_step(leaf.state, action)\n",
    "                child = leaf.expand(action, next_state)\n",
    "                child.untried_actions = list(range(self.num_actions)) if not done else []\n",
    "                leaf = child\n",
    "            \n",
    "            # 3. Simulation: random rollout\n",
    "            simulation_reward = self._simulate_rollout(leaf.state)\n",
    "            \n",
    "            # 4. Backpropagation: update path to root\n",
    "            self._backpropagate(leaf, simulation_reward)\n",
    "        \n",
    "        return root.get_best_action(), root\n",
    "    \n",
    "    def _select_leaf(self, node):\n",
    "        \"\"\"Select leaf node using UCB\"\"\"\n",
    "        while node.is_fully_expanded() and node.children:\n",
    "            node = node.select_child(self.exploration_weight)\n",
    "        return node\n",
    "    \n",
    "    def _simulate_step(self, state, action):\n",
    "        \"\"\"Simulate one step using the model\"\"\"\n",
    "        if hasattr(self.model, 'predict'):\n",
    "            # Neural network model\n",
    "            next_state, reward = self.model.predict(state, action)\n",
    "            # Simple done condition (can be made more sophisticated)\n",
    "            done = False\n",
    "        else:\n",
    "            # Tabular model\n",
    "            if (state, action) in self.model.transitions:\n",
    "                next_state = self.model.transitions[(state, action)]\n",
    "                reward = self.model.rewards[(state, action)]\n",
    "                done = False\n",
    "            else:\n",
    "                # Unknown transition\n",
    "                next_state = state\n",
    "                reward = 0.0\n",
    "                done = True\n",
    "        \n",
    "        return next_state, reward, done\n",
    "    \n",
    "    def _simulate_rollout(self, state, max_depth=None):\n",
    "        \"\"\"Perform random rollout from state\"\"\"\n",
    "        if max_depth is None:\n",
    "            max_depth = self.max_depth\n",
    "        \n",
    "        total_reward = 0.0\n",
    "        current_state = state\n",
    "        discount = 1.0\n",
    "        \n",
    "        for depth in range(max_depth):\n",
    "            action = np.random.randint(self.num_actions)\n",
    "            next_state, reward, done = self._simulate_step(current_state, action)\n",
    "            \n",
    "            total_reward += discount * reward\n",
    "            discount *= self.gamma\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            current_state = next_state\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def _backpropagate(self, node, reward):\n",
    "        \"\"\"Backpropagate reward up the tree\"\"\"\n",
    "        while node is not None:\n",
    "            node.update(reward)\n",
    "            node = node.parent\n",
    "            reward *= self.gamma  # Discount for parent nodes\n",
    "\n",
    "class MCTSAgent:\n",
    "    \"\"\"Agent using MCTS for planning\"\"\"\n",
    "    \n",
    "    def __init__(self, model, num_states, num_actions, num_simulations=1000, \n",
    "                 exploration_weight=1.0):\n",
    "        self.model = model\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.mcts = MCTS(model, num_actions, exploration_weight)\n",
    "        self.num_simulations = num_simulations\n",
    "        \n",
    "        # Statistics\n",
    "        self.search_times = []\n",
    "        self.tree_sizes = []\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "    def select_action(self, state, deterministic=False):\n",
    "        \"\"\"Select action using MCTS\"\"\"\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        best_action, root = self.mcts.search(state, self.num_simulations)\n",
    "        \n",
    "        search_time = time.time() - start_time\n",
    "        tree_size = self._count_nodes(root)\n",
    "        \n",
    "        self.search_times.append(search_time)\n",
    "        self.tree_sizes.append(tree_size)\n",
    "        \n",
    "        return best_action if best_action is not None else np.random.randint(self.num_actions)\n",
    "    \n",
    "    def _count_nodes(self, node):\n",
    "        \"\"\"Count total nodes in tree\"\"\"\n",
    "        if not node.children:\n",
    "            return 1\n",
    "        return 1 + sum(self._count_nodes(child) for child in node.children.values())\n",
    "    \n",
    "    def train_episode(self, env, max_steps=200):\n",
    "        \"\"\"Run episode with MCTS planning\"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        return total_reward, steps\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get performance statistics\"\"\"\n",
    "        return {\n",
    "            'avg_search_time': np.mean(self.search_times) if self.search_times else 0,\n",
    "            'avg_tree_size': np.mean(self.tree_sizes) if self.tree_sizes else 0,\n",
    "            'total_searches': len(self.search_times),\n",
    "            'avg_episode_reward': np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0\n",
    "        }\n",
    "\n",
    "# MCTS Demonstration\n",
    "print(\"Monte Carlo Tree Search (MCTS) Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create environment and model\n",
    "print(\"\\n1. Setting up environment and learned model...\")\n",
    "env = SimpleGridWorld(size=6)\n",
    "tabular_model = TabularModel(env.num_states, env.num_actions)\n",
    "\n",
    "# Train a simple model first\n",
    "print(\"Training tabular model...\")\n",
    "for episode in range(100):\n",
    "    state = env.reset()\n",
    "    for step in range(50):\n",
    "        action = np.random.randint(env.num_actions)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        tabular_model.update(state, action, reward, next_state)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "\n",
    "# Create MCTS agent\n",
    "mcts_agent = MCTSAgent(\n",
    "    model=tabular_model,\n",
    "    num_states=env.num_states,\n",
    "    num_actions=env.num_actions,\n",
    "    num_simulations=200,\n",
    "    exploration_weight=1.4\n",
    ")\n",
    "\n",
    "print(f\"Model trained with {len(tabular_model.transitions)} transitions\")\n",
    "\n",
    "# Test MCTS performance\n",
    "print(\"\\n2. Testing MCTS performance...\")\n",
    "n_test_episodes = 20\n",
    "episode_rewards = []\n",
    "episode_lengths = []\n",
    "\n",
    "for episode in range(n_test_episodes):\n",
    "    reward, length = mcts_agent.train_episode(env, max_steps=100)\n",
    "    episode_rewards.append(reward)\n",
    "    episode_lengths.append(length)\n",
    "    \n",
    "    if (episode + 1) % 5 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-5:])\n",
    "        avg_length = np.mean(episode_lengths[-5:])\n",
    "        stats = mcts_agent.get_statistics()\n",
    "        print(f\"Episodes {episode-4}-{episode+1}: Avg Reward = {avg_reward:.2f}, \"\n",
    "              f\"Avg Length = {avg_length:.1f}, Avg Search Time = {stats['avg_search_time']:.4f}s\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Performance over episodes\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(episode_rewards, 'b-', linewidth=2, label='Episode Reward')\n",
    "plt.axhline(y=np.mean(episode_rewards), color='r', linestyle='--', alpha=0.7, label='Average')\n",
    "plt.title('MCTS Episode Performance')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(episode_lengths, 'g-', linewidth=2, label='Episode Length')\n",
    "plt.axhline(y=np.mean(episode_lengths), color='r', linestyle='--', alpha=0.7, label='Average')\n",
    "plt.title('Episode Lengths')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps to Goal')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Search statistics\n",
    "plt.subplot(2, 3, 3)\n",
    "search_times = mcts_agent.search_times\n",
    "plt.plot(search_times, 'purple', linewidth=2, label='Search Time')\n",
    "plt.axhline(y=np.mean(search_times), color='r', linestyle='--', alpha=0.7, label='Average')\n",
    "plt.title('MCTS Search Times')\n",
    "plt.xlabel('Search Number')\n",
    "plt.ylabel('Time (seconds)')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Tree sizes\n",
    "plt.subplot(2, 3, 4)\n",
    "tree_sizes = mcts_agent.tree_sizes\n",
    "plt.plot(tree_sizes, 'orange', linewidth=2, label='Tree Size')\n",
    "plt.axhline(y=np.mean(tree_sizes), color='r', linestyle='--', alpha=0.7, label='Average')\n",
    "plt.title('MCTS Tree Sizes')\n",
    "plt.xlabel('Search Number')\n",
    "plt.ylabel('Number of Nodes')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Search time vs tree size correlation\n",
    "plt.subplot(2, 3, 5)\n",
    "if len(search_times) > 0 and len(tree_sizes) > 0:\n",
    "    plt.scatter(tree_sizes, search_times, alpha=0.6, c='red', s=30)\n",
    "    # Add trend line\n",
    "    if len(tree_sizes) > 1:\n",
    "        z = np.polyfit(tree_sizes, search_times, 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(sorted(tree_sizes), p(sorted(tree_sizes)), \"r--\", alpha=0.8, linewidth=2)\n",
    "plt.title('Search Time vs Tree Size')\n",
    "plt.xlabel('Tree Size (nodes)')\n",
    "plt.ylabel('Search Time (seconds)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison\n",
    "plt.subplot(2, 3, 6)\n",
    "# Compare with random policy\n",
    "random_rewards = []\n",
    "for _ in range(n_test_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    for step in range(100):\n",
    "        action = np.random.randint(env.num_actions)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    random_rewards.append(total_reward)\n",
    "\n",
    "comparison_data = [episode_rewards, random_rewards]\n",
    "labels = ['MCTS', 'Random']\n",
    "plt.boxplot(comparison_data, labels=labels)\n",
    "plt.title('Performance Comparison')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Detailed analysis\n",
    "print(f\"\\n3. MCTS Performance Analysis:\")\n",
    "final_stats = mcts_agent.get_statistics()\n",
    "print(f\"Average Episode Reward: {np.mean(episode_rewards):.3f} Â± {np.std(episode_rewards):.3f}\")\n",
    "print(f\"Average Episode Length: {np.mean(episode_lengths):.1f} Â± {np.std(episode_lengths):.1f}\")\n",
    "print(f\"Average Search Time: {final_stats['avg_search_time']:.4f} seconds\")\n",
    "print(f\"Average Tree Size: {final_stats['avg_tree_size']:.1f} nodes\")\n",
    "print(f\"Total MCTS Searches: {final_stats['total_searches']}\")\n",
    "\n",
    "print(f\"\\nRandom Policy Baseline:\")\n",
    "print(f\"Average Episode Reward: {np.mean(random_rewards):.3f} Â± {np.std(random_rewards):.3f}\")\n",
    "\n",
    "improvement = (np.mean(episode_rewards) - np.mean(random_rewards)) / np.mean(random_rewards) * 100\n",
    "print(f\"\\nMCTS Improvement over Random: {improvement:.1f}%\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Key MCTS Insights:\")\n",
    "print(\"â€¢ MCTS provides sophisticated planning through tree search\")\n",
    "print(\"â€¢ UCB balances exploration and exploitation in tree nodes\")\n",
    "print(\"â€¢ Performance scales with number of simulations\")\n",
    "print(\"â€¢ Computational cost grows with search depth and simulations\")\n",
    "print(\"â€¢ Effective for discrete action spaces with learned models\")\n",
    "\n",
    "print(f\"\\nâœ… MCTS demonstration complete!\")\n",
    "print(\"ðŸ“Š Next: Model Predictive Control (MPC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8db3d2",
   "metadata": {},
   "source": [
    "# Section 6: Model Predictive Control (MPC)\n",
    "\n",
    "Model Predictive Control is a control strategy that uses a model to predict future behavior and optimizes a sequence of control actions over a finite horizon.\n",
    "\n",
    "## 6.1 Theoretical Foundation\n",
    "\n",
    "MPC operates on the principle of **receding horizon control**:\n",
    "\n",
    "1. **Prediction**: Use model to predict future states over horizon H\n",
    "2. **Optimization**: Solve optimal control problem over this horizon\n",
    "3. **Execution**: Apply only the first control action\n",
    "4. **Recede**: Shift horizon forward and repeat\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **Prediction Model**: $\\hat{s}_{t+1} = f(s_t, a_t)$\n",
    "- **Cost Function**: $J = \\sum_{k=0}^{H-1} c(s_{t+k}, a_{t+k}) + V_f(s_{t+H})$\n",
    "- **Constraints**: State and action constraints\n",
    "- **Terminal Cost**: $V_f(s_{t+H})$ (optional)\n",
    "\n",
    "### Advantages:\n",
    "- Handles constraints naturally\n",
    "- Provides explicit planning horizon\n",
    "- Can incorporate uncertainty\n",
    "- Works with nonlinear models\n",
    "\n",
    "### MPC in RL Context:\n",
    "- Use learned dynamics models\n",
    "- Optimize with gradient-based or sampling methods\n",
    "- Can incorporate learned value functions as terminal costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80395007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Predictive Control Implementation\n",
    "\n",
    "class MPCController:\n",
    "    \"\"\"Model Predictive Control for RL\"\"\"\n",
    "    \n",
    "    def __init__(self, model, num_actions, horizon=10, num_samples=1000, \n",
    "                 temperature=1.0, elite_ratio=0.1):\n",
    "        self.model = model\n",
    "        self.num_actions = num_actions\n",
    "        self.horizon = horizon\n",
    "        self.num_samples = num_samples\n",
    "        self.temperature = temperature\n",
    "        self.elite_ratio = elite_ratio\n",
    "        self.elite_size = max(1, int(num_samples * elite_ratio))\n",
    "        \n",
    "        # Statistics\n",
    "        self.optimization_costs = []\n",
    "        self.episode_rewards = []\n",
    "        \n",
    "    def cross_entropy_optimization(self, initial_state):\n",
    "        \"\"\"Cross-Entropy Method for action sequence optimization\"\"\"\n",
    "        # Initialize action distribution parameters\n",
    "        action_means = np.zeros((self.horizon, self.num_actions))\n",
    "        action_stds = np.ones((self.horizon, self.num_actions))\n",
    "        \n",
    "        best_cost = float('inf')\n",
    "        best_actions = None\n",
    "        \n",
    "        # CEM iterations\n",
    "        for iteration in range(10):  # Number of CEM iterations\n",
    "            # Sample action sequences\n",
    "            action_sequences = []\n",
    "            costs = []\n",
    "            \n",
    "            for _ in range(self.num_samples):\n",
    "                # Sample action sequence\n",
    "                actions = []\n",
    "                for h in range(self.horizon):\n",
    "                    # Sample from categorical distribution (softmax of means)\n",
    "                    probs = np.exp(action_means[h] / self.temperature)\n",
    "                    probs = probs / np.sum(probs)\n",
    "                    action = np.random.choice(self.num_actions, p=probs)\n",
    "                    actions.append(action)\n",
    "                \n",
    "                action_sequences.append(actions)\n",
    "                # Evaluate cost of this sequence\n",
    "                cost = self.evaluate_sequence(initial_state, actions)\n",
    "                costs.append(cost)\n",
    "            \n",
    "            # Select elite samples\n",
    "            elite_indices = np.argsort(costs)[:self.elite_size]\n",
    "            elite_actions = [action_sequences[i] for i in elite_indices]\n",
    "            \n",
    "            # Update distribution parameters\n",
    "            for h in range(self.horizon):\n",
    "                elite_actions_h = [seq[h] for seq in elite_actions]\n",
    "                # Update means based on elite actions\n",
    "                for a in range(self.num_actions):\n",
    "                    count = elite_actions_h.count(a)\n",
    "                    action_means[h, a] = count / len(elite_actions_h)\n",
    "                \n",
    "                # Convert to log probabilities for numerical stability\n",
    "                action_means[h] = np.log(action_means[h] + 1e-8)\n",
    "            \n",
    "            # Track best solution\n",
    "            min_cost = min(costs)\n",
    "            if min_cost < best_cost:\n",
    "                best_cost = min_cost\n",
    "                best_actions = action_sequences[costs.index(min_cost)]\n",
    "        \n",
    "        self.optimization_costs.append(best_cost)\n",
    "        return best_actions\n",
    "    \n",
    "    def random_shooting(self, initial_state):\n",
    "        \"\"\"Random shooting optimization\"\"\"\n",
    "        best_cost = float('inf')\n",
    "        best_actions = None\n",
    "        \n",
    "        for _ in range(self.num_samples):\n",
    "            # Generate random action sequence\n",
    "            actions = [np.random.randint(self.num_actions) for _ in range(self.horizon)]\n",
    "            \n",
    "            # Evaluate sequence\n",
    "            cost = self.evaluate_sequence(initial_state, actions)\n",
    "            \n",
    "            if cost < best_cost:\n",
    "                best_cost = cost\n",
    "                best_actions = actions\n",
    "        \n",
    "        self.optimization_costs.append(best_cost)\n",
    "        return best_actions\n",
    "    \n",
    "    def evaluate_sequence(self, initial_state, actions):\n",
    "        \"\"\"Evaluate cost of action sequence using model\"\"\"\n",
    "        state = initial_state\n",
    "        total_cost = 0.0\n",
    "        discount = 1.0\n",
    "        \n",
    "        for action in actions:\n",
    "            # Predict next state and reward\n",
    "            if hasattr(self.model, 'predict'):\n",
    "                next_state, reward = self.model.predict(state, action)\n",
    "            else:\n",
    "                # Tabular model\n",
    "                if (state, action) in self.model.transitions:\n",
    "                    next_state = self.model.transitions[(state, action)]\n",
    "                    reward = self.model.rewards[(state, action)]\n",
    "                else:\n",
    "                    next_state = state\n",
    "                    reward = 0.0\n",
    "            \n",
    "            # Convert reward to cost (negative reward)\n",
    "            cost = -reward\n",
    "            total_cost += discount * cost\n",
    "            discount *= 0.95  # Discount factor\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        return total_cost\n",
    "    \n",
    "    def select_action(self, state, method='cross_entropy'):\n",
    "        \"\"\"Select action using MPC\"\"\"\n",
    "        if method == 'cross_entropy':\n",
    "            action_sequence = self.cross_entropy_optimization(state)\n",
    "        else:\n",
    "            action_sequence = self.random_shooting(state)\n",
    "        \n",
    "        # Return first action in optimal sequence\n",
    "        return action_sequence[0] if action_sequence else np.random.randint(self.num_actions)\n",
    "\n",
    "class MPCAgent:\n",
    "    \"\"\"RL Agent using MPC for control\"\"\"\n",
    "    \n",
    "    def __init__(self, model, num_states, num_actions, horizon=10, method='cross_entropy'):\n",
    "        self.model = model\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.controller = MPCController(model, num_actions, horizon=horizon)\n",
    "        self.method = method\n",
    "        \n",
    "        # Statistics\n",
    "        self.episode_rewards = []\n",
    "        self.planning_costs = []\n",
    "    \n",
    "    def train_episode(self, env, max_steps=200):\n",
    "        \"\"\"Run episode with MPC planning\"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            action = self.controller.select_action(state, self.method)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        self.episode_rewards.append(total_reward)\n",
    "        if self.controller.optimization_costs:\n",
    "            self.planning_costs.extend(self.controller.optimization_costs)\n",
    "        \n",
    "        return total_reward, steps\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Get performance statistics\"\"\"\n",
    "        return {\n",
    "            'avg_episode_reward': np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0,\n",
    "            'avg_planning_cost': np.mean(self.planning_costs[-10:]) if self.planning_costs else 0,\n",
    "            'total_episodes': len(self.episode_rewards)\n",
    "        }\n",
    "\n",
    "# MPC Demonstration\n",
    "print(\"Model Predictive Control (MPC) Demonstration\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Setup environment and model\n",
    "print(\"\\n1. Setting up MPC with learned model...\")\n",
    "env = SimpleGridWorld(size=5)\n",
    "\n",
    "# Use previously trained neural model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "neural_model = NeuralModel(env.num_states, env.num_actions, hidden_size=64).to(device)\n",
    "\n",
    "# Quick training for demonstration\n",
    "trainer = ModelTrainer(neural_model, env, device=device)\n",
    "print(\"Training neural model for MPC...\")\n",
    "trainer.train(num_episodes=200, verbose=False)\n",
    "\n",
    "# Create MPC agents with different methods\n",
    "agents = {\n",
    "    'MPC-CEM': MPCAgent(neural_model, env.num_states, env.num_actions, horizon=8, method='cross_entropy'),\n",
    "    'MPC-RS': MPCAgent(neural_model, env.num_states, env.num_actions, horizon=8, method='random_shooting')\n",
    "}\n",
    "\n",
    "# Test MPC performance\n",
    "print(\"\\n2. Testing MPC performance...\")\n",
    "n_episodes = 15\n",
    "results = {}\n",
    "\n",
    "for name, agent in agents.items():\n",
    "    print(f\"\\nTesting {name}...\")\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        reward, length = agent.train_episode(env, max_steps=100)\n",
    "        episode_rewards.append(reward)\n",
    "        episode_lengths.append(length)\n",
    "        \n",
    "        if (episode + 1) % 5 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-5:])\n",
    "            print(f\"  Episodes {episode-4}-{episode+1}: Avg Reward = {avg_reward:.2f}\")\n",
    "    \n",
    "    results[name] = {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'statistics': agent.get_statistics()\n",
    "    }\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Performance comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "for name, data in results.items():\n",
    "    rewards = data['episode_rewards']\n",
    "    plt.plot(rewards, linewidth=2, label=name, marker='o', markersize=4)\n",
    "\n",
    "plt.title('MPC Performance Comparison')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Episode lengths\n",
    "plt.subplot(2, 3, 2)\n",
    "for name, data in results.items():\n",
    "    lengths = data['episode_lengths']\n",
    "    plt.plot(lengths, linewidth=2, label=name, marker='s', markersize=4)\n",
    "\n",
    "plt.title('Episode Lengths')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Steps to Goal')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance distribution\n",
    "plt.subplot(2, 3, 3)\n",
    "reward_data = [results[name]['episode_rewards'] for name in results.keys()]\n",
    "labels = list(results.keys())\n",
    "plt.boxplot(reward_data, labels=labels)\n",
    "plt.title('Reward Distribution')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Planning cost analysis for CEM agent\n",
    "plt.subplot(2, 3, 4)\n",
    "if 'MPC-CEM' in results:\n",
    "    agent = agents['MPC-CEM']\n",
    "    if agent.planning_costs:\n",
    "        plt.plot(agent.planning_costs, 'purple', linewidth=2, alpha=0.7)\n",
    "        plt.axhline(y=np.mean(agent.planning_costs), color='red', linestyle='--', \n",
    "                   alpha=0.7, label=f'Mean: {np.mean(agent.planning_costs):.2f}')\n",
    "        plt.title('MPC-CEM Planning Costs')\n",
    "        plt.xlabel('Planning Step')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Horizon analysis\n",
    "plt.subplot(2, 3, 5)\n",
    "print(\"\\n3. Analyzing effect of planning horizon...\")\n",
    "horizon_results = {}\n",
    "horizons = [3, 5, 8, 12]\n",
    "\n",
    "for h in horizons:\n",
    "    agent = MPCAgent(neural_model, env.num_states, env.num_actions, horizon=h, method='cross_entropy')\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(5):  # Quick test\n",
    "        reward, _ = agent.train_episode(env, max_steps=100)\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    horizon_results[h] = np.mean(rewards)\n",
    "\n",
    "horizons_list = list(horizon_results.keys())\n",
    "performance_list = list(horizon_results.values())\n",
    "plt.bar(horizons_list, performance_list, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Performance vs Planning Horizon')\n",
    "plt.xlabel('Planning Horizon')\n",
    "plt.ylabel('Average Episode Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Method comparison summary\n",
    "plt.subplot(2, 3, 6)\n",
    "method_names = list(results.keys())\n",
    "avg_rewards = [np.mean(results[name]['episode_rewards']) for name in method_names]\n",
    "avg_lengths = [np.mean(results[name]['episode_lengths']) for name in method_names]\n",
    "\n",
    "x = np.arange(len(method_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, avg_rewards, width, label='Avg Reward', alpha=0.7)\n",
    "plt.bar(x + width/2, [l/10 for l in avg_lengths], width, label='Avg Length/10', alpha=0.7)\n",
    "\n",
    "plt.title('MPC Method Comparison')\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Performance')\n",
    "plt.xticks(x, method_names, rotation=45)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analysis summary\n",
    "print(f\"\\n4. MPC Analysis Summary:\")\n",
    "for name, data in results.items():\n",
    "    stats = data['statistics']\n",
    "    print(f\"\\n{name}:\")\n",
    "    print(f\"  Average Episode Reward: {np.mean(data['episode_rewards']):.3f} Â± {np.std(data['episode_rewards']):.3f}\")\n",
    "    print(f\"  Average Episode Length: {np.mean(data['episode_lengths']):.1f} Â± {np.std(data['episode_lengths']):.1f}\")\n",
    "    if stats['avg_planning_cost'] > 0:\n",
    "        print(f\"  Average Planning Cost: {stats['avg_planning_cost']:.3f}\")\n",
    "\n",
    "print(f\"\\nHorizon Analysis:\")\n",
    "for h, perf in horizon_results.items():\n",
    "    print(f\"  Horizon {h}: {perf:.3f} average reward\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Key MPC Insights:\")\n",
    "print(\"â€¢ MPC provides principled planning with explicit horizons\")\n",
    "print(\"â€¢ Cross-Entropy Method often outperforms random shooting\")\n",
    "print(\"â€¢ Longer horizons generally improve performance but increase computation\")\n",
    "print(\"â€¢ MPC naturally handles constraints and can incorporate uncertainty\")\n",
    "print(\"â€¢ Effective for continuous control and discrete planning problems\")\n",
    "\n",
    "print(f\"\\nâœ… MPC demonstration complete!\")\n",
    "print(\"ðŸŽ¯ Final section: Comprehensive comparison and conclusions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec553db",
   "metadata": {},
   "source": [
    "# Section 7: Advanced Model-Based Methods and Modern Approaches\n",
    "\n",
    "## 7.1 Modern Neural Model-Based Methods\n",
    "\n",
    "### Model-Based Meta-Learning\n",
    "- **MAML for Model Learning**: Learning models that can quickly adapt to new environments\n",
    "- **Gradient-Based Meta-Learning**: Using gradients to update model parameters efficiently\n",
    "\n",
    "### Uncertainty-Aware Models\n",
    "- **Bayesian Neural Networks**: Capturing epistemic uncertainty in dynamics\n",
    "- **Ensemble Methods**: Multiple models for uncertainty quantification\n",
    "- **Dropout-Based Uncertainty**: Using Monte Carlo dropout for uncertainty estimation\n",
    "\n",
    "### Advanced Planning Methods\n",
    "- **Differentiable Planning**: End-to-end training of planning modules\n",
    "- **Learned Optimizers**: Using neural networks as optimizers for planning\n",
    "- **Hierarchical Planning**: Multi-level planning for complex tasks\n",
    "\n",
    "## 7.2 State-of-the-Art Methods\n",
    "\n",
    "### Model-Based Policy Optimization (MBPO)\n",
    "- Combines model-based and model-free learning\n",
    "- Uses learned models to generate synthetic data\n",
    "- Applies model-free algorithms to mixed real and synthetic data\n",
    "\n",
    "### Dreamer and DreamerV2\n",
    "- World models with latent state representations\n",
    "- Planning in latent space\n",
    "- Actor-critic learning within the world model\n",
    "\n",
    "### MuZero\n",
    "- Combines MCTS with learned models\n",
    "- No explicit environment model\n",
    "- Learns value, policy, and reward predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 8: Comprehensive Analysis and Conclusions\n",
    "\n",
    "# Comprehensive Model-Based RL Comparison\n",
    "print(\"Comprehensive Model-Based Reinforcement Learning Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create a comprehensive comparison framework\n",
    "class ModelBasedComparisonFramework:\n",
    "    \"\"\"Framework for comparing different model-based RL approaches\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        self.environments = {}\n",
    "        self.methods = {}\n",
    "    \n",
    "    def add_environment(self, name, env):\n",
    "        \"\"\"Add environment for testing\"\"\"\n",
    "        self.environments[name] = env\n",
    "    \n",
    "    def add_method(self, name, method_class, **kwargs):\n",
    "        \"\"\"Add method to compare\"\"\"\n",
    "        self.methods[name] = {'class': method_class, 'kwargs': kwargs}\n",
    "    \n",
    "    def run_comparison(self, n_episodes=50, max_steps=200, n_runs=3):\n",
    "        \"\"\"Run comprehensive comparison\"\"\"\n",
    "        print(f\"\\nRunning comprehensive comparison...\")\n",
    "        print(f\"Episodes per run: {n_episodes}, Runs per method: {n_runs}\")\n",
    "        \n",
    "        for env_name, env in self.environments.items():\n",
    "            print(f\"\\nðŸŒ Environment: {env_name}\")\n",
    "            self.results[env_name] = {}\n",
    "            \n",
    "            # First train a model for model-based methods\n",
    "            if hasattr(env, 'num_states'):\n",
    "                # Create and train models\n",
    "                tabular_model = TabularModel(env.num_states, env.num_actions)\n",
    "                neural_model = NeuralModel(env.num_states, env.num_actions, hidden_size=32)\n",
    "                \n",
    "                # Quick model training\n",
    "                self._train_models(env, tabular_model, neural_model)\n",
    "            \n",
    "            for method_name, method_info in self.methods.items():\n",
    "                print(f\"  ðŸ“Š Testing {method_name}...\")\n",
    "                \n",
    "                method_results = []\n",
    "                \n",
    "                for run in range(n_runs):\n",
    "                    # Create agent instance\n",
    "                    kwargs = method_info['kwargs'].copy()\n",
    "                    \n",
    "                    # Inject models if needed\n",
    "                    if 'model' in kwargs:\n",
    "                        if kwargs['model'] == 'tabular':\n",
    "                            kwargs['model'] = tabular_model\n",
    "                        elif kwargs['model'] == 'neural':\n",
    "                            kwargs['model'] = neural_model\n",
    "                    \n",
    "                    try:\n",
    "                        agent = method_info['class'](**kwargs)\n",
    "                        \n",
    "                        # Run episodes\n",
    "                        episode_rewards = []\n",
    "                        episode_lengths = []\n",
    "                        \n",
    "                        for episode in range(n_episodes):\n",
    "                            reward, length = agent.train_episode(env, max_steps=max_steps)\n",
    "                            episode_rewards.append(reward)\n",
    "                            episode_lengths.append(length)\n",
    "                        \n",
    "                        method_results.append({\n",
    "                            'episode_rewards': episode_rewards,\n",
    "                            'episode_lengths': episode_lengths,\n",
    "                            'final_performance': np.mean(episode_rewards[-10:]),\n",
    "                            'learning_efficiency': self._calculate_learning_efficiency(episode_rewards),\n",
    "                            'statistics': agent.get_statistics() if hasattr(agent, 'get_statistics') else {}\n",
    "                        })\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"    âš ï¸ Error with {method_name}: {str(e)}\")\n",
    "                        continue\n",
    "                \n",
    "                if method_results:\n",
    "                    # Aggregate results across runs\n",
    "                    self.results[env_name][method_name] = self._aggregate_results(method_results)\n",
    "                    \n",
    "                    avg_performance = self.results[env_name][method_name]['avg_final_performance']\n",
    "                    std_performance = self.results[env_name][method_name]['std_final_performance']\n",
    "                    print(f\"    âœ… Final Performance: {avg_performance:.3f} Â± {std_performance:.3f}\")\n",
    "    \n",
    "    def _train_models(self, env, tabular_model, neural_model, episodes=100):\n",
    "        \"\"\"Quick model training\"\"\"\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        neural_model = neural_model.to(device)\n",
    "        trainer = ModelTrainer(neural_model, env, device=device)\n",
    "        \n",
    "        # Train neural model\n",
    "        trainer.train(num_episodes=episodes, verbose=False)\n",
    "        \n",
    "        # Train tabular model\n",
    "        for episode in range(episodes):\n",
    "            state = env.reset()\n",
    "            for step in range(50):\n",
    "                action = np.random.randint(env.num_actions)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                tabular_model.update(state, action, reward, next_state)\n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "    \n",
    "    def _calculate_learning_efficiency(self, rewards):\n",
    "        \"\"\"Calculate learning efficiency (area under learning curve)\"\"\"\n",
    "        return np.sum(rewards) / len(rewards)\n",
    "    \n",
    "    def _aggregate_results(self, method_results):\n",
    "        \"\"\"Aggregate results across multiple runs\"\"\"\n",
    "        final_performances = [r['final_performance'] for r in method_results]\n",
    "        learning_efficiencies = [r['learning_efficiency'] for r in method_results]\n",
    "        \n",
    "        return {\n",
    "            'avg_final_performance': np.mean(final_performances),\n",
    "            'std_final_performance': np.std(final_performances),\n",
    "            'avg_learning_efficiency': np.mean(learning_efficiencies),\n",
    "            'std_learning_efficiency': np.std(learning_efficiencies),\n",
    "            'all_results': method_results\n",
    "        }\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Create comprehensive visualization\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        fig.suptitle('Model-Based RL Comprehensive Comparison', fontsize=16)\n",
    "        \n",
    "        # Performance comparison\n",
    "        ax1 = axes[0, 0]\n",
    "        for env_name, env_results in self.results.items():\n",
    "            methods = list(env_results.keys())\n",
    "            performances = [env_results[m]['avg_final_performance'] for m in methods]\n",
    "            errors = [env_results[m]['std_final_performance'] for m in methods]\n",
    "            \n",
    "            x = np.arange(len(methods))\n",
    "            ax1.bar(x, performances, yerr=errors, alpha=0.7, \n",
    "                   label=env_name, capsize=5)\n",
    "            ax1.set_xticks(x)\n",
    "            ax1.set_xticklabels(methods, rotation=45, ha='right')\n",
    "        \n",
    "        ax1.set_title('Final Performance Comparison')\n",
    "        ax1.set_ylabel('Average Episode Reward')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning efficiency\n",
    "        ax2 = axes[0, 1]\n",
    "        for env_name, env_results in self.results.items():\n",
    "            methods = list(env_results.keys())\n",
    "            efficiencies = [env_results[m]['avg_learning_efficiency'] for m in methods]\n",
    "            errors = [env_results[m]['std_learning_efficiency'] for m in methods]\n",
    "            \n",
    "            x = np.arange(len(methods))\n",
    "            ax2.bar(x, efficiencies, yerr=errors, alpha=0.7,\n",
    "                   label=env_name, capsize=5)\n",
    "            ax2.set_xticks(x)\n",
    "            ax2.set_xticklabels(methods, rotation=45, ha='right')\n",
    "        \n",
    "        ax2.set_title('Learning Efficiency')\n",
    "        ax2.set_ylabel('Average Reward over Episodes')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Learning curves for one environment\n",
    "        ax3 = axes[1, 0]\n",
    "        if self.results:\n",
    "            env_name = list(self.results.keys())[0]\n",
    "            env_results = self.results[env_name]\n",
    "            \n",
    "            for method_name, method_data in env_results.items():\n",
    "                if method_data['all_results']:\n",
    "                    # Average learning curve\n",
    "                    all_rewards = [r['episode_rewards'] for r in method_data['all_results']]\n",
    "                    if all_rewards:\n",
    "                        avg_rewards = np.mean(all_rewards, axis=0)\n",
    "                        smoothed = pd.Series(avg_rewards).rolling(window=5).mean()\n",
    "                        ax3.plot(smoothed, label=method_name, linewidth=2)\n",
    "            \n",
    "            ax3.set_title(f'Learning Curves - {env_name}')\n",
    "            ax3.set_xlabel('Episode')\n",
    "            ax3.set_ylabel('Episode Reward (Smoothed)')\n",
    "            ax3.legend()\n",
    "            ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Method characteristics radar chart\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.text(0.5, 0.5, 'Method Characteristics:\\\\n\\\\n'\n",
    "                          'â€¢ Sample Efficiency\\\\n'\n",
    "                          'â€¢ Computational Cost\\\\n'\n",
    "                          'â€¢ Adaptability\\\\n'\n",
    "                          'â€¢ Theoretical Guarantees\\\\n'\n",
    "                          'â€¢ Implementation Complexity',\n",
    "                ha='center', va='center', transform=ax4.transAxes,\n",
    "                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n",
    "        ax4.set_title('Key Method Properties')\n",
    "        ax4.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print comprehensive summary\"\"\"\n",
    "        print(f\"\\nðŸ“‹ COMPREHENSIVE ANALYSIS SUMMARY\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        for env_name, env_results in self.results.items():\n",
    "            print(f\"\\nðŸŒ Environment: {env_name}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Sort methods by performance\n",
    "            sorted_methods = sorted(env_results.items(), \n",
    "                                  key=lambda x: x[1]['avg_final_performance'], \n",
    "                                  reverse=True)\n",
    "            \n",
    "            print(\"Performance Ranking:\")\n",
    "            for i, (method_name, data) in enumerate(sorted_methods, 1):\n",
    "                perf = data['avg_final_performance']\n",
    "                std = data['std_final_performance']\n",
    "                eff = data['avg_learning_efficiency']\n",
    "                print(f\"  {i}. {method_name}: {perf:.3f} Â± {std:.3f} \"\n",
    "                      f\"(efficiency: {eff:.3f})\")\n",
    "\n",
    "# Initialize comparison framework\n",
    "framework = ModelBasedComparisonFramework()\n",
    "\n",
    "# Add environments\n",
    "framework.add_environment(\"GridWorld-5x5\", SimpleGridWorld(size=5))\n",
    "\n",
    "# Add methods for comparison\n",
    "framework.add_method(\"Q-Learning\", \n",
    "                    lambda **kwargs: DynaQAgent(25, 4, planning_steps=0),\n",
    "                    num_states=25, num_actions=4)\n",
    "\n",
    "framework.add_method(\"Dyna-Q(5)\", \n",
    "                    lambda **kwargs: DynaQAgent(25, 4, planning_steps=5),\n",
    "                    num_states=25, num_actions=4)\n",
    "\n",
    "framework.add_method(\"Dyna-Q(20)\", \n",
    "                    lambda **kwargs: DynaQAgent(25, 4, planning_steps=20),\n",
    "                    num_states=25, num_actions=4)\n",
    "\n",
    "framework.add_method(\"MCTS\", \n",
    "                    lambda **kwargs: MCTSAgent(kwargs['model'], 25, 4, num_simulations=100),\n",
    "                    model='tabular')\n",
    "\n",
    "framework.add_method(\"MPC-CEM\", \n",
    "                    lambda **kwargs: MPCAgent(kwargs['model'], 25, 4, horizon=5, method='cross_entropy'),\n",
    "                    model='neural')\n",
    "\n",
    "# Run comprehensive comparison\n",
    "framework.run_comparison(n_episodes=30, n_runs=2)\n",
    "\n",
    "# Visualize and analyze results\n",
    "framework.visualize_results()\n",
    "framework.print_summary()\n",
    "\n",
    "print(f\"\\nðŸŽ¯ FINAL CONCLUSIONS: Model-Based Reinforcement Learning\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Key Findings:\")\n",
    "print(\"1. Sample Efficiency: Model-based methods generally require fewer environment interactions\")\n",
    "print(\"2. Planning Benefits: More planning steps typically improve performance\")  \n",
    "print(\"3. Model Quality: Better models lead to better planning performance\")\n",
    "print(\"4. Computational Trade-offs: Planning methods trade computation for sample efficiency\")\n",
    "print(\"5. Adaptability: Some methods (Dyna-Q+) handle environment changes better\")\n",
    "\n",
    "print(f\"\\nðŸ”¬ Method Characteristics:\")\n",
    "print(\"â€¢ Tabular Models: Simple, exact, limited to discrete spaces\")\n",
    "print(\"â€¢ Neural Models: Flexible, scalable, but require careful training\")\n",
    "print(\"â€¢ Dyna-Q: Simple integration of learning and planning\")\n",
    "print(\"â€¢ MCTS: Sophisticated tree search, good for discrete actions\")\n",
    "print(\"â€¢ MPC: Principled control theory approach, handles constraints\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Practical Recommendations:\")\n",
    "print(\"1. Use model-based methods when sample efficiency is critical\")\n",
    "print(\"2. Choose tabular models for small discrete environments\")\n",
    "print(\"3. Use neural models for high-dimensional or continuous spaces\")\n",
    "print(\"4. Apply Dyna-Q for balanced learning and planning\")\n",
    "print(\"5. Use MCTS for complex decision trees\")\n",
    "print(\"6. Apply MPC when constraints are important\")\n",
    "\n",
    "print(f\"\\nðŸš€ Future Directions:\")\n",
    "print(\"â€¢ Uncertainty-aware planning\")\n",
    "print(\"â€¢ Hierarchical model-based RL\")\n",
    "print(\"â€¢ Meta-learning for quick model adaptation\")\n",
    "print(\"â€¢ Differentiable planning modules\")\n",
    "print(\"â€¢ Hybrid model-free and model-based methods\")\n",
    "\n",
    "print(f\"\\nâœ… MODEL-BASED REINFORCEMENT LEARNING COMPLETE!\")\n",
    "print(\"ðŸŽ“ You now have a comprehensive understanding of:\")\n",
    "print(\"   â€¢ Theoretical foundations and mathematical formulations\")\n",
    "print(\"   â€¢ Environment model learning (tabular and neural)\")\n",
    "print(\"   â€¢ Classical planning with learned models\")\n",
    "print(\"   â€¢ Dyna-Q algorithm for integrated learning and planning\")\n",
    "print(\"   â€¢ Monte Carlo Tree Search (MCTS) for sophisticated planning\")\n",
    "print(\"   â€¢ Model Predictive Control (MPC) for constrained optimization\")\n",
    "print(\"   â€¢ Modern approaches and state-of-the-art methods\")\n",
    "print(\"   â€¢ Comparative analysis and practical guidelines\")\n",
    "\n",
    "print(f\"\\nðŸŒŸ Congratulations on completing this comprehensive study!\")\n",
    "print(\"ðŸ“š Continue exploring advanced topics in model-based RL!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
