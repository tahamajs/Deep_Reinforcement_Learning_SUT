{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa8ba02",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning - Session 4\n",
    "## Policy Gradient Methods and Neural Networks in RL\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will understand:\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Policy Gradient Methods**: Direct optimization of parameterized policies\n",
    "- **REINFORCE Algorithm**: Monte Carlo policy gradient method\n",
    "- **Actor-Critic Methods**: Combining value functions with policy gradients\n",
    "- **Function Approximation**: Using neural networks for large state spaces\n",
    "- **Advantage Function**: Reducing variance in policy gradient estimation\n",
    "\n",
    "**Practical Skills:**\n",
    "- Implement REINFORCE algorithm from scratch\n",
    "- Build Actor-Critic agents with neural networks\n",
    "- Design neural network architectures for RL\n",
    "- Train policies using policy gradient methods\n",
    "- Compare value-based vs policy-based methods\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Continuous control (robotics, autonomous vehicles)\n",
    "- Game playing with large action spaces\n",
    "- Natural language processing and generation\n",
    "- Portfolio optimization and trading\n",
    "- Recommendation systems\n",
    "\n",
    "---\n",
    "\n",
    "## Session Overview\n",
    "\n",
    "1. **Part 1**: From Value-Based to Policy-Based Methods\n",
    "2. **Part 2**: Policy Gradient Theory and Mathematics\n",
    "3. **Part 3**: REINFORCE Algorithm Implementation\n",
    "4. **Part 4**: Actor-Critic Methods\n",
    "5. **Part 5**: Neural Network Function Approximation\n",
    "6. **Part 6**: Advanced Topics and Applications\n",
    "\n",
    "---\n",
    "\n",
    "## Transition from Previous Sessions\n",
    "\n",
    "**Session 1-2**: MDPs, Dynamic Programming (model-based)\n",
    "**Session 3**: Q-Learning, SARSA (value-based, model-free)\n",
    "**Session 4**: Policy Gradients (policy-based, model-free)\n",
    "\n",
    "**Key Evolution:**\n",
    "- **Model-based** → **Model-free** → **Policy-based**\n",
    "- **Discrete actions** → **Continuous actions**\n",
    "- **Tabular methods** → **Function approximation**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb327959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential imports for Policy Gradient Methods\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Normal\n",
    "import gym\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✓ All libraries imported successfully\")\n",
    "print(\"✓ Random seeds set for reproducibility\")\n",
    "print(\"✓ PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b33b266",
   "metadata": {},
   "source": [
    "# Part 1: From Value-Based to Policy-Based Methods\n",
    "\n",
    "## 1.1 Limitations of Value-Based Methods\n",
    "\n",
    "**Challenges with Q-Learning and SARSA:**\n",
    "- **Discrete Action Spaces**: Difficult to handle continuous actions\n",
    "- **Deterministic Policies**: Always select highest Q-value action\n",
    "- **Exploration Issues**: ε-greedy exploration can be inefficient\n",
    "- **Large Action Spaces**: Memory and computation become intractable\n",
    "\n",
    "**Example Problem**: Consider a robotic arm with 7 joints, each with continuous angles [0, 2π]. The action space is infinite!\n",
    "\n",
    "## 1.2 Introduction to Policy-Based Methods\n",
    "\n",
    "**Key Idea**: Instead of learning value functions, directly learn a parameterized policy π(a|s,θ).\n",
    "\n",
    "**Policy Parameterization:**\n",
    "- **θ**: Parameters of the policy (e.g., neural network weights)\n",
    "- **π(a|s,θ)**: Probability of taking action a in state s given parameters θ\n",
    "- **Goal**: Find optimal parameters θ* that maximize expected return\n",
    "\n",
    "**Advantages:**\n",
    "- **Continuous Actions**: Natural handling of continuous action spaces\n",
    "- **Stochastic Policies**: Can learn probabilistic behaviors\n",
    "- **Better Convergence**: Guaranteed convergence properties\n",
    "- **No Need for Value Function**: Direct policy optimization\n",
    "\n",
    "## 1.3 Types of Policy Representations\n",
    "\n",
    "### Discrete Actions (Softmax Policy)\n",
    "For discrete actions, use softmax over action preferences:\n",
    "\n",
    "```\n",
    "π(a|s,θ) = exp(h(s,a,θ)) / Σ_b exp(h(s,b,θ))\n",
    "```\n",
    "\n",
    "Where h(s,a,θ) is the preference for action a in state s.\n",
    "\n",
    "### Continuous Actions (Gaussian Policy)\n",
    "For continuous actions, use Gaussian distribution:\n",
    "\n",
    "```\n",
    "π(a|s,θ) = N(μ(s,θ), σ(s,θ)²)\n",
    "```\n",
    "\n",
    "Where μ(s,θ) is the mean and σ(s,θ) is the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a93773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstration: Policy Representations\n",
    "class PolicyDemo:\n",
    "    \"\"\"Demonstrate different policy representations\"\"\"\n",
    "    \n",
    "    def __init__(self, n_states=4, n_actions=2):\n",
    "        self.n_states = n_states\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "    def softmax_policy(self, preferences):\n",
    "        \"\"\"Softmax policy for discrete actions\"\"\"\n",
    "        exp_prefs = np.exp(preferences - np.max(preferences))  # Numerical stability\n",
    "        return exp_prefs / np.sum(exp_prefs)\n",
    "    \n",
    "    def gaussian_policy(self, mu, sigma, action):\n",
    "        \"\"\"Gaussian policy for continuous actions\"\"\"\n",
    "        return (1.0 / (sigma * np.sqrt(2 * np.pi))) * \\\n",
    "               np.exp(-0.5 * ((action - mu) / sigma) ** 2)\n",
    "    \n",
    "    def visualize_policies(self):\n",
    "        \"\"\"Compare different policy types\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Deterministic vs Stochastic (Discrete)\n",
    "        states = range(self.n_states)\n",
    "        deterministic_probs = np.array([[1.0, 0.0], [0.0, 1.0], [1.0, 0.0], [0.0, 1.0]])\n",
    "        stochastic_probs = np.array([[0.7, 0.3], [0.4, 0.6], [0.8, 0.2], [0.3, 0.7]])\n",
    "        \n",
    "        x = np.arange(len(states))\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[0,0].bar(x - width/2, deterministic_probs[:, 0], width, \n",
    "                     label='Action 0', alpha=0.8, color='skyblue')\n",
    "        axes[0,0].bar(x + width/2, deterministic_probs[:, 1], width, \n",
    "                     label='Action 1', alpha=0.8, color='lightcoral')\n",
    "        axes[0,0].set_title('Deterministic Policy')\n",
    "        axes[0,0].set_xlabel('State')\n",
    "        axes[0,0].set_ylabel('Action Probability')\n",
    "        axes[0,0].legend()\n",
    "        \n",
    "        axes[0,1].bar(x - width/2, stochastic_probs[:, 0], width, \n",
    "                     label='Action 0', alpha=0.8, color='skyblue')\n",
    "        axes[0,1].bar(x + width/2, stochastic_probs[:, 1], width, \n",
    "                     label='Action 1', alpha=0.8, color='lightcoral')\n",
    "        axes[0,1].set_title('Stochastic Policy')\n",
    "        axes[0,1].set_xlabel('State')\n",
    "        axes[0,1].set_ylabel('Action Probability')\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # 2. Softmax temperature effects\n",
    "        preferences = np.array([2.0, 1.0, 0.5])\n",
    "        temperatures = [0.1, 1.0, 10.0]\n",
    "        \n",
    "        for i, temp in enumerate(temperatures):\n",
    "            probs = self.softmax_policy(preferences / temp)\n",
    "            axes[1,0].plot(preferences, probs, 'o-', \n",
    "                          label=f'Temperature = {temp}', linewidth=2, markersize=8)\n",
    "        \n",
    "        axes[1,0].set_title('Softmax Policy with Different Temperatures')\n",
    "        axes[1,0].set_xlabel('Action Preferences')\n",
    "        axes[1,0].set_ylabel('Action Probability')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Gaussian policy for continuous actions\n",
    "        actions = np.linspace(-3, 3, 100)\n",
    "        mu_values = [0.0, 1.0, -0.5]\n",
    "        sigma_values = [0.5, 1.0, 1.5]\n",
    "        \n",
    "        for mu, sigma in zip(mu_values, sigma_values):\n",
    "            probs = [self.gaussian_policy(mu, sigma, a) for a in actions]\n",
    "            axes[1,1].plot(actions, probs, linewidth=2, \n",
    "                          label=f'μ={mu}, σ={sigma}')\n",
    "        \n",
    "        axes[1,1].set_title('Gaussian Policy for Continuous Actions')\n",
    "        axes[1,1].set_xlabel('Action Value')\n",
    "        axes[1,1].set_ylabel('Probability Density')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and visualize policy demonstrations\n",
    "policy_demo = PolicyDemo()\n",
    "policy_demo.visualize_policies()\n",
    "\n",
    "print(\"Policy Representation Analysis:\")\n",
    "print(\"✓ Deterministic policies: Single action per state\")\n",
    "print(\"✓ Stochastic policies: Probability distribution over actions\")\n",
    "print(\"✓ Softmax temperature controls exploration vs exploitation\")\n",
    "print(\"✓ Gaussian policies handle continuous action spaces naturally\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0a8e3",
   "metadata": {},
   "source": [
    "# Part 2: Policy Gradient Theory and Mathematics\n",
    "\n",
    "## 2.1 The Policy Gradient Objective\n",
    "\n",
    "**Goal**: Find policy parameters θ that maximize expected return J(θ).\n",
    "\n",
    "**Performance Measure:**\n",
    "```\n",
    "J(θ) = E[G₀ | π_θ] = E[Σ(t=0 to T) γᵗrₜ₊₁ | π_θ]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **G₀**: Return from initial state\n",
    "- **π_θ**: Policy parameterized by θ\n",
    "- **γ**: Discount factor\n",
    "- **rₜ₊₁**: Reward at time t+1\n",
    "\n",
    "## 2.2 Policy Gradient Theorem\n",
    "\n",
    "**The Fundamental Result**: For any differentiable policy π(a|s,θ), the gradient of J(θ) is:\n",
    "\n",
    "```\n",
    "∇_θ J(θ) = E[∇_θ log π(a|s,θ) * G_t | π_θ]\n",
    "```\n",
    "\n",
    "**Key Components:**\n",
    "- **∇_θ log π(a|s,θ)**: Score function (eligibility traces)\n",
    "- **G_t**: Return from time t\n",
    "- **Expectation**: Over trajectories generated by π_θ\n",
    "\n",
    "## 2.3 Derivation of Policy Gradient Theorem\n",
    "\n",
    "**Step 1**: Express J(θ) using state visitation distribution\n",
    "```\n",
    "J(θ) = Σ_s ρ^π(s) Σ_a π(a|s,θ) R_s^a\n",
    "```\n",
    "\n",
    "**Step 2**: Take gradient with respect to θ\n",
    "```\n",
    "∇_θ J(θ) = Σ_s [∇_θ ρ^π(s) Σ_a π(a|s,θ) R_s^a + ρ^π(s) Σ_a ∇_θ π(a|s,θ) R_s^a]\n",
    "```\n",
    "\n",
    "**Step 3**: Use the log-derivative trick\n",
    "```\n",
    "∇_θ π(a|s,θ) = π(a|s,θ) ∇_θ log π(a|s,θ)\n",
    "```\n",
    "\n",
    "**Step 4**: After mathematical manipulation (proof omitted for brevity):\n",
    "```\n",
    "∇_θ J(θ) = E[∇_θ log π(A_t|S_t,θ) * G_t]\n",
    "```\n",
    "\n",
    "## 2.4 REINFORCE Algorithm\n",
    "\n",
    "**Monte Carlo Policy Gradient:**\n",
    "\n",
    "```\n",
    "θ_{t+1} = θ_t + α ∇_θ log π(A_t|S_t,θ_t) G_t\n",
    "```\n",
    "\n",
    "**Algorithm Steps:**\n",
    "1. **Generate Episode**: Run policy π_θ to collect trajectory τ = (s₀,a₀,r₁,s₁,a₁,r₂,...)\n",
    "2. **Compute Returns**: Calculate G_t = Σ(k=0 to T-t) γᵏr_{t+k+1} for each step t\n",
    "3. **Update Parameters**: θ ← θ + α ∇_θ log π(a_t|s_t,θ) G_t\n",
    "4. **Repeat**: Until convergence\n",
    "\n",
    "## 2.5 Variance Reduction Techniques\n",
    "\n",
    "**Problem**: High variance in Monte Carlo estimates\n",
    "\n",
    "**Solution 1: Baseline Subtraction**\n",
    "```\n",
    "∇_θ J(θ) ≈ ∇_θ log π(A_t|S_t,θ) * (G_t - b(S_t))\n",
    "```\n",
    "\n",
    "Where b(S_t) is a baseline that doesn't depend on A_t.\n",
    "\n",
    "**Solution 2: Advantage Function**\n",
    "```\n",
    "A^π(s,a) = Q^π(s,a) - V^π(s)\n",
    "```\n",
    "\n",
    "The advantage function measures how much better action a is compared to the average.\n",
    "\n",
    "**Solution 3: Actor-Critic Methods**\n",
    "Use a learned value function as baseline and advantage estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1edd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mathematical Demonstration: Policy Gradient Components\n",
    "class PolicyGradientMath:\n",
    "    \"\"\"Demonstrate policy gradient mathematical concepts\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.n_states = 3\n",
    "        self.n_actions = 2\n",
    "        \n",
    "    def softmax_policy_gradient(self, preferences, action):\n",
    "        \"\"\"Compute gradient of log softmax policy\"\"\"\n",
    "        # Softmax probabilities\n",
    "        exp_prefs = np.exp(preferences - np.max(preferences))\n",
    "        probs = exp_prefs / np.sum(exp_prefs)\n",
    "        \n",
    "        # Gradient of log π(a|s,θ)\n",
    "        grad_log_policy = np.zeros_like(preferences)\n",
    "        grad_log_policy[action] = 1.0\n",
    "        grad_log_policy -= probs\n",
    "        \n",
    "        return probs, grad_log_policy\n",
    "    \n",
    "    def demonstrate_score_function(self):\n",
    "        \"\"\"Visualize score function properties\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # 1. Score function for different actions\n",
    "        preferences = np.array([1.0, 2.0])\n",
    "        actions = [0, 1]\n",
    "        \n",
    "        pref_range = np.linspace(-2, 4, 100)\n",
    "        \n",
    "        for action in actions:\n",
    "            scores = []\n",
    "            for pref in pref_range:\n",
    "                current_prefs = preferences.copy()\n",
    "                current_prefs[action] = pref\n",
    "                _, grad = self.softmax_policy_gradient(current_prefs, action)\n",
    "                scores.append(grad[action])\n",
    "            \n",
    "            axes[0,0].plot(pref_range, scores, linewidth=2, \n",
    "                          label=f'Action {action}')\n",
    "        \n",
    "        axes[0,0].set_title('Score Function: ∇_θ log π(a|s,θ)')\n",
    "        axes[0,0].set_xlabel('Action Preference θ_a')\n",
    "        axes[0,0].set_ylabel('Score')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        axes[0,0].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        \n",
    "        # 2. Policy probabilities vs preferences\n",
    "        for action in actions:\n",
    "            probs = []\n",
    "            for pref in pref_range:\n",
    "                current_prefs = preferences.copy()\n",
    "                current_prefs[action] = pref\n",
    "                prob, _ = self.softmax_policy_gradient(current_prefs, action)\n",
    "                probs.append(prob[action])\n",
    "            \n",
    "            axes[0,1].plot(pref_range, probs, linewidth=2, \n",
    "                          label=f'π(a={action}|s,θ)')\n",
    "        \n",
    "        axes[0,1].set_title('Policy Probabilities')\n",
    "        axes[0,1].set_xlabel('Action Preference θ_a')\n",
    "        axes[0,1].set_ylabel('Probability')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Variance reduction with baseline\n",
    "        returns = np.random.normal(10, 5, 1000)  # Sample returns\n",
    "        baseline_values = np.linspace(5, 15, 50)\n",
    "        variances = []\n",
    "        \n",
    "        for baseline in baseline_values:\n",
    "            adjusted_returns = returns - baseline\n",
    "            variances.append(np.var(adjusted_returns))\n",
    "        \n",
    "        axes[1,0].plot(baseline_values, variances, linewidth=2, color='red')\n",
    "        optimal_baseline = np.mean(returns)\n",
    "        axes[1,0].axvline(x=optimal_baseline, color='blue', linestyle='--', \n",
    "                         label=f'Optimal baseline = {optimal_baseline:.2f}')\n",
    "        axes[1,0].set_title('Variance Reduction with Baseline')\n",
    "        axes[1,0].set_xlabel('Baseline Value')\n",
    "        axes[1,0].set_ylabel('Variance')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Learning curves with and without baseline\n",
    "        n_episodes = 500\n",
    "        true_return = 10.0\n",
    "        noise_std = 3.0\n",
    "        \n",
    "        # Without baseline\n",
    "        gradients_no_baseline = []\n",
    "        returns_sample = np.random.normal(true_return, noise_std, n_episodes)\n",
    "        \n",
    "        # With optimal baseline\n",
    "        gradients_with_baseline = []\n",
    "        baseline = np.mean(returns_sample)\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            # Simulate gradient estimates\n",
    "            grad_no_baseline = returns_sample[episode]  # G_t\n",
    "            grad_with_baseline = returns_sample[episode] - baseline  # G_t - b\n",
    "            \n",
    "            gradients_no_baseline.append(grad_no_baseline)\n",
    "            gradients_with_baseline.append(grad_with_baseline)\n",
    "        \n",
    "        # Running variance\n",
    "        window = 50\n",
    "        var_no_baseline = []\n",
    "        var_with_baseline = []\n",
    "        \n",
    "        for i in range(window, n_episodes):\n",
    "            var_no_baseline.append(np.var(gradients_no_baseline[i-window:i]))\n",
    "            var_with_baseline.append(np.var(gradients_with_baseline[i-window:i]))\n",
    "        \n",
    "        episodes = range(window, n_episodes)\n",
    "        axes[1,1].plot(episodes, var_no_baseline, label='Without Baseline', \n",
    "                      linewidth=2, alpha=0.8)\n",
    "        axes[1,1].plot(episodes, var_with_baseline, label='With Baseline', \n",
    "                      linewidth=2, alpha=0.8)\n",
    "        axes[1,1].set_title('Gradient Variance Over Training')\n",
    "        axes[1,1].set_xlabel('Episode')\n",
    "        axes[1,1].set_ylabel('Gradient Variance')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstrate policy gradient mathematics\n",
    "math_demo = PolicyGradientMath()\n",
    "math_demo.demonstrate_score_function()\n",
    "\n",
    "print(\"Policy Gradient Mathematics Analysis:\")\n",
    "print(\"✓ Score function guides parameter updates\")\n",
    "print(\"✓ Higher preference → higher probability → lower score\")\n",
    "print(\"✓ Baseline subtraction reduces variance without bias\")\n",
    "print(\"✓ Optimal baseline is the expected return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f9b143",
   "metadata": {},
   "source": [
    "# Part 3: REINFORCE Algorithm Implementation\n",
    "\n",
    "## 3.1 REINFORCE Algorithm Overview\n",
    "\n",
    "**REINFORCE** (REward Increment = Nonnegative Factor × Offset Reinforcement × Characteristic Eligibility) is the canonical policy gradient algorithm.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Monte Carlo**: Uses full episode returns\n",
    "- **On-Policy**: Updates policy being followed\n",
    "- **Model-Free**: No knowledge of transition probabilities\n",
    "- **Unbiased**: Gradient estimates are unbiased\n",
    "\n",
    "## 3.2 REINFORCE Pseudocode\n",
    "\n",
    "```\n",
    "Algorithm: REINFORCE\n",
    "Input: differentiable policy π(a|s,θ)\n",
    "Input: step size α > 0\n",
    "Initialize: policy parameters θ arbitrarily\n",
    "\n",
    "repeat (for each episode):\n",
    "    Generate episode S₀,A₀,R₁,S₁,A₁,R₂,...,S_{T-1},A_{T-1},R_T following π(·|·,θ)\n",
    "    \n",
    "    for t = 0 to T-1:\n",
    "        G ← return from step t\n",
    "        θ ← θ + α * γᵗ * G * ∇_θ ln π(A_t|S_t,θ)\n",
    "        \n",
    "until θ converges\n",
    "```\n",
    "\n",
    "## 3.3 Implementation Considerations\n",
    "\n",
    "**Neural Network Policy:**\n",
    "- **Input**: State representation\n",
    "- **Hidden Layers**: Feature extraction\n",
    "- **Output**: Action probabilities (softmax for discrete) or parameters (for continuous)\n",
    "\n",
    "**Training Process:**\n",
    "1. **Forward Pass**: Compute action probabilities\n",
    "2. **Action Selection**: Sample from policy distribution  \n",
    "3. **Episode Collection**: Run until terminal state\n",
    "4. **Return Calculation**: Compute discounted returns\n",
    "5. **Backward Pass**: Compute gradients and update parameters\n",
    "\n",
    "**Challenges:**\n",
    "- **High Variance**: Monte Carlo estimates are noisy\n",
    "- **Sample Efficiency**: Requires many episodes\n",
    "- **Credit Assignment**: Long episodes make learning difficult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10d0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete REINFORCE Implementation\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"Neural network policy for discrete action spaces\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, action_size)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        action_probs = F.softmax(self.fc3(x), dim=-1)\n",
    "        return action_probs\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"REINFORCE agent with baseline\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr=0.001, gamma=0.99):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy_net = PolicyNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        \n",
    "        # Episode storage\n",
    "        self.reset_episode()\n",
    "        \n",
    "        # Training history\n",
    "        self.episode_rewards = []\n",
    "        self.episode_lengths = []\n",
    "        \n",
    "    def reset_episode(self):\n",
    "        \"\"\"Reset episode-specific storage\"\"\"\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Select action using current policy\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action_probs = self.policy_net(state_tensor)\n",
    "        \n",
    "        # Create categorical distribution and sample\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        # Store log probability for gradient computation\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob\n",
    "    \n",
    "    def store_transition(self, state, action, reward, log_prob):\n",
    "        \"\"\"Store transition for episode\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.log_probs.append(log_prob)\n",
    "    \n",
    "    def compute_returns(self, rewards):\n",
    "        \"\"\"Compute discounted returns\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for reward in reversed(rewards):\n",
    "            G = reward + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        return returns\n",
    "    \n",
    "    def update_policy(self):\n",
    "        \"\"\"Update policy using REINFORCE algorithm\"\"\"\n",
    "        if len(self.rewards) == 0:\n",
    "            return\n",
    "            \n",
    "        # Compute returns\n",
    "        returns = self.compute_returns(self.rewards)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        \n",
    "        # Normalize returns for stability\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Compute policy loss\n",
    "        policy_loss = []\n",
    "        for log_prob, G in zip(self.log_probs, returns):\n",
    "            policy_loss.append(-log_prob * G)  # Negative for gradient ascent\n",
    "        \n",
    "        # Update parameters\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss = torch.stack(policy_loss).sum()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Store episode statistics\n",
    "        self.episode_rewards.append(sum(self.rewards))\n",
    "        self.episode_lengths.append(len(self.rewards))\n",
    "        \n",
    "        return policy_loss.item()\n",
    "    \n",
    "    def train(self, env, num_episodes=1000, print_every=100):\n",
    "        \"\"\"Train REINFORCE agent\"\"\"\n",
    "        scores = []\n",
    "        losses = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]  # Handle new gym API\n",
    "            \n",
    "            self.reset_episode()\n",
    "            total_reward = 0\n",
    "            \n",
    "            # Generate episode\n",
    "            while True:\n",
    "                action, log_prob = self.get_action(state)\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                self.store_transition(state, action, reward, log_prob)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Update policy\n",
    "            loss = self.update_policy()\n",
    "            scores.append(total_reward)\n",
    "            losses.append(loss if loss is not None else 0)\n",
    "            \n",
    "            # Print progress\n",
    "            if (episode + 1) % print_every == 0:\n",
    "                avg_score = np.mean(scores[-print_every:])\n",
    "                avg_loss = np.mean(losses[-print_every:]) if losses[-1] != 0 else 0\n",
    "                print(f\"Episode {episode + 1:4d} | \"\n",
    "                      f\"Avg Score: {avg_score:7.2f} | \"\n",
    "                      f\"Avg Loss: {avg_loss:8.4f}\")\n",
    "        \n",
    "        return scores, losses\n",
    "\n",
    "# Test environment setup\n",
    "def create_simple_env():\n",
    "    \"\"\"Create a simple test environment\"\"\"\n",
    "    try:\n",
    "        env = gym.make('CartPole-v1')\n",
    "        return env, env.observation_space.shape[0], env.action_space.n\n",
    "    except:\n",
    "        print(\"CartPole environment not available, creating mock environment\")\n",
    "        return None, 4, 2\n",
    "\n",
    "# Initialize and demonstrate REINFORCE\n",
    "env, state_size, action_size = create_simple_env()\n",
    "\n",
    "if env is not None:\n",
    "    print(f\"Environment: CartPole-v1\")\n",
    "    print(f\"State Space: {state_size}\")\n",
    "    print(f\"Action Space: {action_size}\")\n",
    "    print(\"REINFORCE agent initialized successfully\")\n",
    "    \n",
    "    # Create agent\n",
    "    agent = REINFORCEAgent(state_size=state_size, \n",
    "                          action_size=action_size,\n",
    "                          lr=0.001,\n",
    "                          gamma=0.99)\n",
    "    \n",
    "    print(\"✓ REINFORCE agent ready for training\")\n",
    "else:\n",
    "    print(\"✓ REINFORCE implementation complete (environment not available)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec3fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training REINFORCE Agent\n",
    "if env is not None:\n",
    "    print(\"Training REINFORCE Agent on CartPole...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Train the agent\n",
    "    scores, losses = agent.train(env, num_episodes=500, print_every=50)\n",
    "    \n",
    "    # Close environment\n",
    "    env.close()\n",
    "    \n",
    "    # Visualize training progress\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Episode rewards over time\n",
    "    axes[0,0].plot(scores, alpha=0.6, color='blue')\n",
    "    \n",
    "    # Moving average\n",
    "    window = 20\n",
    "    if len(scores) >= window:\n",
    "        moving_avg = [np.mean(scores[i-window:i]) for i in range(window, len(scores))]\n",
    "        axes[0,0].plot(range(window, len(scores)), moving_avg, \n",
    "                      color='red', linewidth=2, label=f'{window}-Episode Average')\n",
    "        axes[0,0].legend()\n",
    "    \n",
    "    axes[0,0].set_title('REINFORCE Training Progress')\n",
    "    axes[0,0].set_xlabel('Episode')\n",
    "    axes[0,0].set_ylabel('Total Reward')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Policy loss over time\n",
    "    valid_losses = [loss for loss in losses if loss != 0]\n",
    "    if valid_losses:\n",
    "        axes[0,1].plot(valid_losses, color='orange', alpha=0.7)\n",
    "        axes[0,1].set_title('Policy Loss')\n",
    "        axes[0,1].set_xlabel('Update Step')\n",
    "        axes[0,1].set_ylabel('Loss')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Episode length distribution\n",
    "    axes[1,0].hist(agent.episode_lengths, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "    axes[1,0].set_title('Episode Length Distribution')\n",
    "    axes[1,0].set_xlabel('Episode Length')\n",
    "    axes[1,0].set_ylabel('Frequency')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Learning curve analysis\n",
    "    if len(scores) >= 100:\n",
    "        # Divide training into phases\n",
    "        phase_size = len(scores) // 4\n",
    "        phases = ['Early', 'Mid-Early', 'Mid-Late', 'Late']\n",
    "        phase_scores = []\n",
    "        \n",
    "        for i in range(4):\n",
    "            start_idx = i * phase_size\n",
    "            end_idx = (i + 1) * phase_size if i < 3 else len(scores)\n",
    "            phase_scores.append(scores[start_idx:end_idx])\n",
    "        \n",
    "        axes[1,1].boxplot(phase_scores, labels=phases)\n",
    "        axes[1,1].set_title('Learning Progress by Training Phase')\n",
    "        axes[1,1].set_ylabel('Episode Reward')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance analysis\n",
    "    final_performance = np.mean(scores[-50:]) if len(scores) >= 50 else np.mean(scores)\n",
    "    initial_performance = np.mean(scores[:50]) if len(scores) >= 50 else np.mean(scores)\n",
    "    improvement = final_performance - initial_performance\n",
    "    \n",
    "    print(\"\\\\nTraining Results:\")\n",
    "    print(\"=\" * 30)\n",
    "    print(f\"Initial Performance (first 50 episodes): {initial_performance:.2f}\")\n",
    "    print(f\"Final Performance (last 50 episodes): {final_performance:.2f}\")\n",
    "    print(f\"Improvement: {improvement:.2f}\")\n",
    "    print(f\"Best Episode: {max(scores):.2f}\")\n",
    "    print(f\"Average Episode Length: {np.mean(agent.episode_lengths):.2f}\")\n",
    "    \n",
    "    # Success rate analysis for CartPole\n",
    "    success_threshold = 195  # CartPole is \"solved\" at 195+ for 100 consecutive episodes\n",
    "    success_episodes = [score for score in scores if score >= success_threshold]\n",
    "    success_rate = len(success_episodes) / len(scores) * 100\n",
    "    \n",
    "    print(f\"Episodes with score ≥ {success_threshold}: {len(success_episodes)}\")\n",
    "    print(f\"Success Rate: {success_rate:.1f}%\")\n",
    "    \n",
    "else:\n",
    "    # Create synthetic training data for demonstration\n",
    "    print(\"Generating synthetic training results for demonstration...\")\n",
    "    \n",
    "    # Simulate REINFORCE learning curve\n",
    "    np.random.seed(42)\n",
    "    num_episodes = 500\n",
    "    \n",
    "    # Realistic CartPole learning curve\n",
    "    base_performance = np.linspace(20, 180, num_episodes)\n",
    "    noise = np.random.normal(0, 20, num_episodes)\n",
    "    learning_boost = np.exp(np.linspace(0, 2, num_episodes)) - 1\n",
    "    scores = base_performance + noise + learning_boost * 5\n",
    "    scores = np.clip(scores, 0, 500)  # Reasonable bounds\n",
    "    \n",
    "    # Visualize synthetic results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(scores, alpha=0.6, color='blue', label='Episode Rewards')\n",
    "    \n",
    "    # Moving average\n",
    "    window = 20\n",
    "    moving_avg = [np.mean(scores[i-window:i]) for i in range(window, len(scores))]\n",
    "    plt.plot(range(window, len(scores)), moving_avg, \n",
    "             color='red', linewidth=2, label=f'{window}-Episode Average')\n",
    "    \n",
    "    plt.title('REINFORCE Training Progress (Synthetic)')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(scores, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "    plt.title('Reward Distribution')\n",
    "    plt.xlabel('Episode Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Synthetic training completed: {len(scores)} episodes\")\n",
    "    print(f\"Average final performance: {np.mean(scores[-50:]):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55485558",
   "metadata": {},
   "source": [
    "# Part 4: Actor-Critic Methods\n",
    "\n",
    "## 4.1 Motivation for Actor-Critic\n",
    "\n",
    "**Problems with REINFORCE:**\n",
    "- **High Variance**: Monte Carlo returns are very noisy\n",
    "- **Slow Learning**: Requires many episodes to converge\n",
    "- **Sample Inefficiency**: Cannot learn from partial episodes\n",
    "\n",
    "**Solution: Actor-Critic Architecture**\n",
    "- **Actor**: Learns the policy π(a|s,θ)\n",
    "- **Critic**: Learns the value function V(s,w) or Q(s,a,w)\n",
    "- **Synergy**: Critic provides low-variance baseline for Actor\n",
    "\n",
    "## 4.2 Actor-Critic Framework\n",
    "\n",
    "**Key Idea**: Replace Monte Carlo returns in REINFORCE with bootstrapped estimates from the critic.\n",
    "\n",
    "**REINFORCE Update:**\n",
    "```\n",
    "θ ← θ + α ∇_θ log π(a|s,θ) G_t\n",
    "```\n",
    "\n",
    "**Actor-Critic Update:**\n",
    "```\n",
    "θ ← θ + α ∇_θ log π(a|s,θ) δ_t\n",
    "```\n",
    "\n",
    "Where δ_t is the **TD error**: δ_t = r_{t+1} + γV(s_{t+1},w) - V(s_t,w)\n",
    "\n",
    "## 4.3 Types of Actor-Critic Methods\n",
    "\n",
    "### 4.3.1 One-Step Actor-Critic\n",
    "- Uses TD(0) for critic updates\n",
    "- Actor uses immediate TD error\n",
    "- Fast updates but potential bias\n",
    "\n",
    "### 4.3.2 Multi-Step Actor-Critic  \n",
    "- Uses n-step returns for less bias\n",
    "- Trades off bias vs variance\n",
    "- A3C uses this approach\n",
    "\n",
    "### 4.3.3 Advantage Actor-Critic (A2C)\n",
    "- Uses advantage function A(s,a) = Q(s,a) - V(s)\n",
    "- Reduces variance while maintaining zero bias\n",
    "- State-of-the-art method\n",
    "\n",
    "## 4.4 Advantage Function Estimation\n",
    "\n",
    "**True Advantage:**\n",
    "```\n",
    "A^π(s,a) = Q^π(s,a) - V^π(s)\n",
    "```\n",
    "\n",
    "**TD Error Advantage:**\n",
    "```\n",
    "A(s,a) ≈ δ_t = r + γV(s') - V(s)\n",
    "```\n",
    "\n",
    "**Generalized Advantage Estimation (GAE):**\n",
    "```\n",
    "A_t^{GAE(λ)} = Σ_{l=0}^∞ (γλ)^l δ_{t+l}\n",
    "```\n",
    "\n",
    "## 4.5 Algorithm: One-Step Actor-Critic\n",
    "\n",
    "```\n",
    "Initialize: actor parameters θ, critic parameters w\n",
    "Initialize: step sizes α_θ > 0, α_w > 0\n",
    "\n",
    "repeat (for each episode):\n",
    "    Initialize state s\n",
    "    \n",
    "    repeat (for each step):\n",
    "        a ~ π(·|s,θ)           # Sample action from actor\n",
    "        Take action a, observe r, s'\n",
    "        \n",
    "        δ ← r + γV(s',w) - V(s,w)    # TD error\n",
    "        \n",
    "        w ← w + α_w δ ∇_w V(s,w)     # Update critic\n",
    "        θ ← θ + α_θ δ ∇_θ log π(a|s,θ) # Update actor\n",
    "        \n",
    "        s ← s'\n",
    "    until s is terminal\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a274efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Actor-Critic Implementation\n",
    "class ValueNetwork(nn.Module):\n",
    "    \"\"\"Critic network for state value estimation\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, hidden_size=128):\n",
    "        super(ValueNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        value = self.fc3(x)\n",
    "        return value\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    \"\"\"Actor-Critic agent with separate networks\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr_actor=0.001, lr_critic=0.005, gamma=0.99):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr_actor = lr_actor\n",
    "        self.lr_critic = lr_critic\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = PolicyNetwork(state_size, action_size)\n",
    "        self.critic = ValueNetwork(state_size)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        # Training history\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.episode_rewards = []\n",
    "        self.td_errors = []\n",
    "        \n",
    "    def get_action_and_value(self, state):\n",
    "        \"\"\"Get action from actor and value from critic\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        # Get action probabilities and value\n",
    "        action_probs = self.actor(state_tensor)\n",
    "        value = self.critic(state_tensor)\n",
    "        \n",
    "        # Sample action\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob, value.squeeze()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done, log_prob, value):\n",
    "        \"\"\"Update actor and critic networks\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "        \n",
    "        # Compute TD target and error\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            next_value = self.critic(next_state_tensor).squeeze()\n",
    "            td_target = reward + self.gamma * next_value.detach()\n",
    "        \n",
    "        td_error = td_target - value\n",
    "        \n",
    "        # Update critic (minimize TD error)\n",
    "        critic_loss = F.mse_loss(value, td_target.detach())\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor (policy gradient with advantage)\n",
    "        actor_loss = -log_prob * td_error.detach()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Store metrics\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        self.critic_losses.append(critic_loss.item())\n",
    "        self.td_errors.append(abs(td_error.item()))\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item(), td_error.item()\n",
    "    \n",
    "    def train(self, env, num_episodes=1000, print_every=100):\n",
    "        \"\"\"Train Actor-Critic agent\"\"\"\n",
    "        scores = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            if isinstance(state, tuple):\n",
    "                state = state[0]\n",
    "                \n",
    "            total_reward = 0\n",
    "            episode_actor_losses = []\n",
    "            episode_critic_losses = []\n",
    "            \n",
    "            while True:\n",
    "                # Get action and value\n",
    "                action, log_prob, value = self.get_action_and_value(state)\n",
    "                \n",
    "                # Take action in environment\n",
    "                next_state, reward, done, truncated, _ = env.step(action)\n",
    "                \n",
    "                # Update networks\n",
    "                actor_loss, critic_loss, td_error = self.update(\n",
    "                    state, action, reward, next_state, done or truncated, log_prob, value\n",
    "                )\n",
    "                \n",
    "                episode_actor_losses.append(actor_loss)\n",
    "                episode_critic_losses.append(critic_loss)\n",
    "                \n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                \n",
    "                if done or truncated:\n",
    "                    break\n",
    "            \n",
    "            scores.append(total_reward)\n",
    "            \n",
    "            # Print progress\n",
    "            if (episode + 1) % print_every == 0:\n",
    "                avg_score = np.mean(scores[-print_every:])\n",
    "                avg_actor_loss = np.mean(episode_actor_losses)\n",
    "                avg_critic_loss = np.mean(episode_critic_losses)\n",
    "                avg_td_error = np.mean(self.td_errors[-len(episode_actor_losses):])\n",
    "                \n",
    "                print(f\"Episode {episode + 1:4d} | \"\n",
    "                      f\"Avg Score: {avg_score:7.2f} | \"\n",
    "                      f\"Actor Loss: {avg_actor_loss:8.4f} | \"\n",
    "                      f\"Critic Loss: {avg_critic_loss:8.4f} | \"\n",
    "                      f\"TD Error: {avg_td_error:6.3f}\")\n",
    "        \n",
    "        self.episode_rewards = scores\n",
    "        return scores\n",
    "\n",
    "# Comparison experiment: REINFORCE vs Actor-Critic\n",
    "class ComparisonExperiment:\n",
    "    \"\"\"Compare REINFORCE and Actor-Critic performance\"\"\"\n",
    "    \n",
    "    def __init__(self, env, state_size, action_size):\n",
    "        self.env = env\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "    def run_comparison(self, num_episodes=300):\n",
    "        \"\"\"Run comparison between methods\"\"\"\n",
    "        print(\"Starting REINFORCE vs Actor-Critic Comparison\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Initialize agents\n",
    "        reinforce_agent = REINFORCEAgent(self.state_size, self.action_size, lr=0.001)\n",
    "        ac_agent = ActorCriticAgent(self.state_size, self.action_size, \n",
    "                                   lr_actor=0.001, lr_critic=0.005)\n",
    "        \n",
    "        # Train REINFORCE\n",
    "        print(\"Training REINFORCE...\")\n",
    "        reinforce_scores = reinforce_agent.train(self.env, num_episodes, print_every=50)\n",
    "        \n",
    "        # Train Actor-Critic  \n",
    "        print(\"\\\\nTraining Actor-Critic...\")\n",
    "        ac_scores = ac_agent.train(self.env, num_episodes, print_every=50)\n",
    "        \n",
    "        return reinforce_scores, ac_scores, reinforce_agent, ac_agent\n",
    "    \n",
    "    def visualize_comparison(self, reinforce_scores, ac_scores):\n",
    "        \"\"\"Visualize comparison results\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        episodes = range(len(reinforce_scores))\n",
    "        \n",
    "        # 1. Learning curves\n",
    "        axes[0,0].plot(episodes, reinforce_scores, alpha=0.6, \n",
    "                      label='REINFORCE', color='blue')\n",
    "        axes[0,0].plot(episodes, ac_scores, alpha=0.6, \n",
    "                      label='Actor-Critic', color='red')\n",
    "        \n",
    "        # Moving averages\n",
    "        window = 20\n",
    "        if len(reinforce_scores) >= window:\n",
    "            rf_avg = [np.mean(reinforce_scores[i-window:i]) \n",
    "                     for i in range(window, len(reinforce_scores))]\n",
    "            ac_avg = [np.mean(ac_scores[i-window:i]) \n",
    "                     for i in range(window, len(ac_scores))]\n",
    "            \n",
    "            axes[0,0].plot(range(window, len(reinforce_scores)), rf_avg, \n",
    "                          color='blue', linewidth=2, alpha=0.8)\n",
    "            axes[0,0].plot(range(window, len(ac_scores)), ac_avg, \n",
    "                          color='red', linewidth=2, alpha=0.8)\n",
    "        \n",
    "        axes[0,0].set_title('Learning Curves Comparison')\n",
    "        axes[0,0].set_xlabel('Episode')\n",
    "        axes[0,0].set_ylabel('Total Reward')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Performance distribution\n",
    "        axes[0,1].boxplot([reinforce_scores, ac_scores], \n",
    "                         labels=['REINFORCE', 'Actor-Critic'])\n",
    "        axes[0,1].set_title('Performance Distribution')\n",
    "        axes[0,1].set_ylabel('Episode Reward')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Convergence analysis\n",
    "        window_size = 50\n",
    "        reinforce_convergence = []\n",
    "        ac_convergence = []\n",
    "        \n",
    "        for i in range(window_size, len(reinforce_scores)):\n",
    "            rf_var = np.var(reinforce_scores[i-window_size:i])\n",
    "            ac_var = np.var(ac_scores[i-window_size:i])\n",
    "            reinforce_convergence.append(rf_var)\n",
    "            ac_convergence.append(ac_var)\n",
    "        \n",
    "        conv_episodes = range(window_size, len(reinforce_scores))\n",
    "        axes[1,0].plot(conv_episodes, reinforce_convergence, \n",
    "                      label='REINFORCE', color='blue', alpha=0.7)\n",
    "        axes[1,0].plot(conv_episodes, ac_convergence, \n",
    "                      label='Actor-Critic', color='red', alpha=0.7)\n",
    "        axes[1,0].set_title('Learning Stability (Variance)')\n",
    "        axes[1,0].set_xlabel('Episode')\n",
    "        axes[1,0].set_ylabel(f'{window_size}-Episode Variance')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Cumulative performance\n",
    "        reinforce_cumsum = np.cumsum(reinforce_scores)\n",
    "        ac_cumsum = np.cumsum(ac_scores)\n",
    "        \n",
    "        axes[1,1].plot(episodes, reinforce_cumsum, \n",
    "                      label='REINFORCE', color='blue', linewidth=2)\n",
    "        axes[1,1].plot(episodes, ac_cumsum, \n",
    "                      label='Actor-Critic', color='red', linewidth=2)\n",
    "        axes[1,1].set_title('Cumulative Reward')\n",
    "        axes[1,1].set_xlabel('Episode')\n",
    "        axes[1,1].set_ylabel('Cumulative Reward')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Performance statistics\n",
    "        print(\"\\\\nComparison Results:\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"REINFORCE - Final 50 episodes avg: {np.mean(reinforce_scores[-50:]):.2f}\")\n",
    "        print(f\"Actor-Critic - Final 50 episodes avg: {np.mean(ac_scores[-50:]):.2f}\")\n",
    "        print(f\"REINFORCE - Best episode: {max(reinforce_scores):.2f}\")\n",
    "        print(f\"Actor-Critic - Best episode: {max(ac_scores):.2f}\")\n",
    "        print(f\"REINFORCE - Total reward: {sum(reinforce_scores):.0f}\")\n",
    "        print(f\"Actor-Critic - Total reward: {sum(ac_scores):.0f}\")\n",
    "\n",
    "# Initialize for comparison\n",
    "if env is not None:\n",
    "    print(\"Setting up Actor-Critic vs REINFORCE comparison...\")\n",
    "    comparison = ComparisonExperiment(env, state_size, action_size)\n",
    "    print(\"✓ Comparison experiment ready\")\n",
    "else:\n",
    "    print(\"✓ Actor-Critic implementation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ce6f5",
   "metadata": {},
   "source": [
    "# Part 5: Neural Network Function Approximation\n",
    "\n",
    "## 5.1 The Need for Function Approximation\n",
    "\n",
    "**Limitation of Tabular Methods:**\n",
    "- **Memory**: Exponential growth with state dimensions\n",
    "- **Generalization**: No learning transfer between states\n",
    "- **Continuous Spaces**: Infinite state/action spaces impossible\n",
    "\n",
    "**Solution: Function Approximation**\n",
    "- **Compact Representation**: Parameters θ instead of lookup tables\n",
    "- **Generalization**: Similar states share similar values/policies\n",
    "- **Scalability**: Handle high-dimensional problems\n",
    "\n",
    "## 5.2 Neural Networks in RL\n",
    "\n",
    "### Universal Function Approximators\n",
    "Neural networks can approximate any continuous function to arbitrary accuracy (Universal Approximation Theorem).\n",
    "\n",
    "**Architecture Choices:**\n",
    "- **Feedforward Networks**: Most common, good for most RL tasks\n",
    "- **Convolutional Networks**: Image-based observations (Atari games)\n",
    "- **Recurrent Networks**: Partially observable environments\n",
    "- **Attention Mechanisms**: Long sequences, complex dependencies\n",
    "\n",
    "### Key Considerations\n",
    "\n",
    "**1. Non-Stationarity**\n",
    "- Target values change as policy improves\n",
    "- Can cause instability in learning\n",
    "- **Solutions**: Experience replay, target networks\n",
    "\n",
    "**2. Temporal Correlations**\n",
    "- Sequential data violates i.i.d. assumption\n",
    "- Can lead to catastrophic forgetting\n",
    "- **Solutions**: Experience replay, batch updates\n",
    "\n",
    "**3. Exploration vs Exploitation**\n",
    "- Need to balance learning and performance\n",
    "- Neural networks can be overconfident\n",
    "- **Solutions**: Proper exploration strategies, entropy regularization\n",
    "\n",
    "## 5.3 Deep Policy Gradients\n",
    "\n",
    "### Network Architecture Design\n",
    "\n",
    "**Policy Network (Actor):**\n",
    "```\n",
    "State → FC → ReLU → FC → ReLU → FC → Softmax → Action Probabilities\n",
    "```\n",
    "\n",
    "**Value Network (Critic):**\n",
    "```\n",
    "State → FC → ReLU → FC → ReLU → FC → Linear → State Value\n",
    "```\n",
    "\n",
    "**Shared Features:**\n",
    "```\n",
    "State → Shared FC → ReLU → Shared FC → ReLU → Split\n",
    "                                            ├── Policy Head\n",
    "                                            └── Value Head\n",
    "```\n",
    "\n",
    "### Training Stability Techniques\n",
    "\n",
    "**1. Gradient Clipping**\n",
    "```python\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "```\n",
    "\n",
    "**2. Learning Rate Scheduling**\n",
    "- Decay learning rate over time\n",
    "- Different rates for actor and critic\n",
    "\n",
    "**3. Batch Normalization**\n",
    "- Normalize inputs to each layer\n",
    "- Reduces internal covariate shift\n",
    "\n",
    "**4. Dropout**\n",
    "- Prevent overfitting\n",
    "- Improve generalization\n",
    "\n",
    "## 5.4 Advanced Policy Gradient Methods\n",
    "\n",
    "### Proximal Policy Optimization (PPO)\n",
    "- Constrains policy updates to prevent large changes\n",
    "- Uses clipped objective function\n",
    "- State-of-the-art for many tasks\n",
    "\n",
    "### Trust Region Policy Optimization (TRPO)\n",
    "- Guarantees monotonic improvement\n",
    "- Uses natural policy gradients\n",
    "- More complex but theoretically sound\n",
    "\n",
    "### Advantage Actor-Critic (A2C/A3C)\n",
    "- Asynchronous training (A3C)\n",
    "- Synchronous training (A2C)\n",
    "- Uses entropy regularization\n",
    "\n",
    "## 5.5 Continuous Action Spaces\n",
    "\n",
    "### Gaussian Policies\n",
    "For continuous control tasks:\n",
    "\n",
    "```python\n",
    "mu, sigma = policy_network(state)\n",
    "action = torch.normal(mu, sigma)\n",
    "log_prob = -0.5 * ((action - mu) / sigma) ** 2 - torch.log(sigma) - 0.5 * log(2π)\n",
    "```\n",
    "\n",
    "### Beta and Other Distributions\n",
    "- **Beta Distribution**: Actions bounded in [0,1]\n",
    "- **Mixture Models**: Multi-modal action distributions\n",
    "- **Normalizing Flows**: Complex action distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455327ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Neural Network Architectures for RL\n",
    "class SharedFeatureNetwork(nn.Module):\n",
    "    \"\"\"Shared feature extraction for Actor-Critic\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, hidden_size=128, feature_size=64):\n",
    "        super(SharedFeatureNetwork, self).__init__()\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, feature_size),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        return self.shared_layers(state)\n",
    "\n",
    "class AdvancedActorCritic(nn.Module):\n",
    "    \"\"\"Advanced Actor-Critic with shared features\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128, feature_size=64):\n",
    "        super(AdvancedActorCritic, self).__init__()\n",
    "        \n",
    "        # Shared feature extractor\n",
    "        self.shared_features = SharedFeatureNetwork(state_size, hidden_size, feature_size)\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor_head = nn.Sequential(\n",
    "            nn.Linear(feature_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, action_size),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic_head = nn.Sequential(\n",
    "            nn.Linear(feature_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, state):\n",
    "        features = self.shared_features(state)\n",
    "        action_probs = self.actor_head(features)\n",
    "        value = self.critic_head(features)\n",
    "        return action_probs, value.squeeze()\n",
    "\n",
    "class ContinuousPolicyNetwork(nn.Module):\n",
    "    \"\"\"Policy network for continuous action spaces\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, hidden_size=128):\n",
    "        super(ContinuousPolicyNetwork, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(state_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "        # Mean and log std for Gaussian policy\n",
    "        self.mu_head = nn.Linear(hidden_size, action_size)\n",
    "        self.log_std_head = nn.Linear(hidden_size, action_size)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "        mu = torch.tanh(self.mu_head(x))  # Bounded actions [-1, 1]\n",
    "        log_std = torch.clamp(self.log_std_head(x), -20, 2)  # Prevent extreme values\n",
    "        \n",
    "        return mu, log_std\n",
    "\n",
    "class ContinuousActorCriticAgent:\n",
    "    \"\"\"Actor-Critic for continuous action spaces\"\"\"\n",
    "    \n",
    "    def __init__(self, state_size, action_size, lr=0.001, gamma=0.99):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = ContinuousPolicyNetwork(state_size, action_size)\n",
    "        self.value_net = ValueNetwork(state_size)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Sample action from continuous policy\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu, log_std = self.policy_net(state_tensor)\n",
    "            std = torch.exp(log_std)\n",
    "            \n",
    "            # Sample action from normal distribution\n",
    "            dist = Normal(mu, std)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "            \n",
    "        return action.squeeze().numpy(), log_prob.item()\n",
    "    \n",
    "    def evaluate_action(self, state, action):\n",
    "        \"\"\"Evaluate action under current policy\"\"\"\n",
    "        state_tensor = torch.FloatTensor(state)\n",
    "        action_tensor = torch.FloatTensor(action)\n",
    "        \n",
    "        mu, log_std = self.policy_net(state_tensor)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        dist = Normal(mu, std)\n",
    "        log_prob = dist.log_prob(action_tensor).sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        \n",
    "        value = self.value_net(state_tensor)\n",
    "        \n",
    "        return log_prob, entropy, value.squeeze()\n",
    "\n",
    "# Network Architecture Visualization\n",
    "class NetworkVisualizer:\n",
    "    \"\"\"Visualize different network architectures\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.architectures = {\n",
    "            'Separate Networks': self._create_separate_diagram(),\n",
    "            'Shared Features': self._create_shared_diagram(),\n",
    "            'Continuous Policy': self._create_continuous_diagram()\n",
    "        }\n",
    "    \n",
    "    def _create_separate_diagram(self):\n",
    "        return {\n",
    "            'layers': [\n",
    "                'State Input',\n",
    "                'Actor: FC(128) → ReLU → FC(64) → ReLU → FC(actions) → Softmax',\n",
    "                'Critic: FC(128) → ReLU → FC(64) → ReLU → FC(1) → Linear'\n",
    "            ],\n",
    "            'params': 'High (separate parameters)',\n",
    "            'learning': 'Independent updates'\n",
    "        }\n",
    "    \n",
    "    def _create_shared_diagram(self):\n",
    "        return {\n",
    "            'layers': [\n",
    "                'State Input',\n",
    "                'Shared: FC(128) → ReLU → FC(64) → ReLU',\n",
    "                'Actor Head: FC(32) → FC(actions) → Softmax',  \n",
    "                'Critic Head: FC(32) → FC(1) → Linear'\n",
    "            ],\n",
    "            'params': 'Medium (shared features)',\n",
    "            'learning': 'Joint feature learning'\n",
    "        }\n",
    "    \n",
    "    def _create_continuous_diagram(self):\n",
    "        return {\n",
    "            'layers': [\n",
    "                'State Input',\n",
    "                'Shared: FC(128) → ReLU → FC(64) → ReLU',\n",
    "                'Mean Head: FC(actions) → Tanh',\n",
    "                'Log Std Head: FC(actions) → Clamp'\n",
    "            ],\n",
    "            'params': 'Medium (Gaussian policy)',\n",
    "            'learning': 'Continuous actions'\n",
    "        }\n",
    "    \n",
    "    def visualize_architectures(self):\n",
    "        \"\"\"Create visual comparison of architectures\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Parameter comparison\n",
    "        models = ['Tabular', 'Separate NN', 'Shared NN', 'Continuous NN']\n",
    "        state_sizes = [10, 100, 1000, 10000]\n",
    "        \n",
    "        tabular_params = [s * 2 for s in state_sizes]  # Q-table size\n",
    "        separate_params = [(s * 128 + 128 * 64 + 64 * 2) * 2 for s in state_sizes]\n",
    "        shared_params = [s * 128 + 128 * 64 + 64 * 2 + 64 * 32 + 32 * 2 for s in state_sizes]\n",
    "        continuous_params = [s * 128 + 128 * 64 + 64 * 4 for s in state_sizes]  # 2 actions\n",
    "        \n",
    "        x = np.arange(len(state_sizes))\n",
    "        width = 0.2\n",
    "        \n",
    "        axes[0,0].bar(x - width*1.5, tabular_params, width, label='Tabular', alpha=0.8)\n",
    "        axes[0,0].bar(x - width*0.5, separate_params, width, label='Separate NN', alpha=0.8)\n",
    "        axes[0,0].bar(x + width*0.5, shared_params, width, label='Shared NN', alpha=0.8)\n",
    "        axes[0,0].bar(x + width*1.5, continuous_params, width, label='Continuous NN', alpha=0.8)\n",
    "        \n",
    "        axes[0,0].set_title('Parameter Count vs State Size')\n",
    "        axes[0,0].set_xlabel('State Size')\n",
    "        axes[0,0].set_ylabel('Number of Parameters')\n",
    "        axes[0,0].set_yscale('log')\n",
    "        axes[0,0].set_xticks(x)\n",
    "        axes[0,0].set_xticklabels([str(s) for s in state_sizes])\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Learning curve comparison (synthetic)\n",
    "        episodes = np.arange(1000)\n",
    "        \n",
    "        # Simulate different convergence rates\n",
    "        tabular_curve = 100 * (1 - np.exp(-episodes / 200)) + np.random.normal(0, 5, 1000)\n",
    "        separate_curve = 150 * (1 - np.exp(-episodes / 300)) + np.random.normal(0, 8, 1000)\n",
    "        shared_curve = 180 * (1 - np.exp(-episodes / 250)) + np.random.normal(0, 6, 1000)\n",
    "        \n",
    "        axes[0,1].plot(episodes, tabular_curve, alpha=0.7, label='Tabular (small state)')\n",
    "        axes[0,1].plot(episodes, separate_curve, alpha=0.7, label='Separate Networks')\n",
    "        axes[0,1].plot(episodes, shared_curve, alpha=0.7, label='Shared Networks')\n",
    "        \n",
    "        axes[0,1].set_title('Learning Curves Comparison')\n",
    "        axes[0,1].set_xlabel('Episode')\n",
    "        axes[0,1].set_ylabel('Average Return')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Sample efficiency\n",
    "        sample_sizes = [1000, 5000, 10000, 50000]\n",
    "        tabular_performance = [0.3, 0.8, 0.95, 0.98]\n",
    "        nn_performance = [0.1, 0.4, 0.7, 0.9]\n",
    "        shared_performance = [0.15, 0.5, 0.8, 0.95]\n",
    "        \n",
    "        axes[1,0].plot(sample_sizes, tabular_performance, 'o-', \n",
    "                      label='Tabular', linewidth=2, markersize=8)\n",
    "        axes[1,0].plot(sample_sizes, nn_performance, 's-', \n",
    "                      label='Separate NN', linewidth=2, markersize=8)\n",
    "        axes[1,0].plot(sample_sizes, shared_performance, '^-', \n",
    "                      label='Shared NN', linewidth=2, markersize=8)\n",
    "        \n",
    "        axes[1,0].set_title('Sample Efficiency')\n",
    "        axes[1,0].set_xlabel('Training Samples')\n",
    "        axes[1,0].set_ylabel('Normalized Performance')\n",
    "        axes[1,0].set_xscale('log')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. Action space comparison\n",
    "        action_types = ['Discrete\\\\n(4 actions)', 'Discrete\\\\n(100 actions)', \n",
    "                       'Continuous\\\\n(1D)', 'Continuous\\\\n(10D)']\n",
    "        memory_requirements = [16, 400, 1, 10]  # Relative memory for action representation\n",
    "        \n",
    "        colors = ['skyblue', 'lightcoral', 'lightgreen', 'orange']\n",
    "        bars = axes[1,1].bar(action_types, memory_requirements, color=colors, alpha=0.8)\n",
    "        \n",
    "        axes[1,1].set_title('Action Space Memory Requirements')\n",
    "        axes[1,1].set_ylabel('Relative Memory (log scale)')\n",
    "        axes[1,1].set_yscale('log')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for bar, value in zip(bars, memory_requirements):\n",
    "            height = bar.get_height()\n",
    "            axes[1,1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                          f'{value}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create and demonstrate visualizations\n",
    "print(\"Creating Neural Network Architecture Analysis...\")\n",
    "visualizer = NetworkVisualizer()\n",
    "visualizer.visualize_architectures()\n",
    "\n",
    "print(\"\\\\nNetwork Architecture Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "for name, arch in visualizer.architectures.items():\n",
    "    print(f\"\\\\n{name}:\")\n",
    "    print(f\"  Parameters: {arch['params']}\")\n",
    "    print(f\"  Learning: {arch['learning']}\")\n",
    "    for i, layer in enumerate(arch['layers']):\n",
    "        print(f\"  Layer {i+1}: {layer}\")\n",
    "\n",
    "print(\"\\\\n✓ Advanced architectures implemented\")\n",
    "print(\"✓ Continuous control capabilities added\")\n",
    "print(\"✓ Network comparison analysis complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f29e0af",
   "metadata": {},
   "source": [
    "# Part 6: Advanced Topics and Real-World Applications\n",
    "\n",
    "## 6.1 State-of-the-Art Policy Gradient Methods\n",
    "\n",
    "### Proximal Policy Optimization (PPO)\n",
    "**Key Innovation**: Prevents destructively large policy updates\n",
    "\n",
    "**Clipped Objective:**\n",
    "```\n",
    "L^CLIP(θ) = min(r_t(θ)Â_t, clip(r_t(θ), 1-ε, 1+ε)Â_t)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- r_t(θ) = π_θ(a_t|s_t) / π_θ_old(a_t|s_t)\n",
    "- Â_t is the advantage estimate\n",
    "- ε is the clipping parameter (typically 0.2)\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement and tune\n",
    "- Stable training\n",
    "- Good sample efficiency\n",
    "- Works well across many domains\n",
    "\n",
    "### Trust Region Policy Optimization (TRPO)\n",
    "**Constraint-based approach**: Ensures policy improvement\n",
    "\n",
    "**Objective:**\n",
    "```\n",
    "maximize E[π_θ(a|s)/π_θ_old(a|s) * A(s,a)]\n",
    "subject to E[KL(π_θ_old(·|s), π_θ(·|s))] ≤ δ\n",
    "```\n",
    "\n",
    "**Theoretical Guarantees:**\n",
    "- Monotonic policy improvement\n",
    "- Convergence guarantees\n",
    "- Natural policy gradients\n",
    "\n",
    "### Soft Actor-Critic (SAC)\n",
    "**Maximum Entropy RL**: Balances reward and policy entropy\n",
    "\n",
    "**Objective:**\n",
    "```\n",
    "J(θ) = E[R(s,a) + α H(π(·|s))]\n",
    "```\n",
    "\n",
    "**Benefits:**\n",
    "- Robust exploration\n",
    "- Stable off-policy learning\n",
    "- Works well in continuous control\n",
    "\n",
    "## 6.2 Multi-Agent Policy Gradients\n",
    "\n",
    "### Independent Learning\n",
    "- Each agent learns independently\n",
    "- Simple but can be unstable\n",
    "- Non-stationary environment from each agent's perspective\n",
    "\n",
    "### Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "- Centralized training, decentralized execution\n",
    "- Each agent has access to other agents' policies during training\n",
    "- Addresses non-stationarity issues\n",
    "\n",
    "### Policy Gradient with Opponent Modeling\n",
    "- Learn models of other agents\n",
    "- Predict opponent actions\n",
    "- Plan optimal responses\n",
    "\n",
    "## 6.3 Hierarchical Policy Gradients\n",
    "\n",
    "### Option-Critic Architecture\n",
    "- Learn both options (sub-policies) and option selection\n",
    "- Hierarchical decision making\n",
    "- Better exploration and transfer learning\n",
    "\n",
    "### Goal-Conditioned RL\n",
    "- Policies conditioned on goals\n",
    "- Universal value functions\n",
    "- Hindsight Experience Replay (HER)\n",
    "\n",
    "## 6.4 Real-World Applications\n",
    "\n",
    "### Robotics and Control\n",
    "**Applications:**\n",
    "- Robotic manipulation\n",
    "- Autonomous vehicles\n",
    "- Drone control\n",
    "- Walking robots\n",
    "\n",
    "**Challenges:**\n",
    "- Safety constraints\n",
    "- Sample efficiency\n",
    "- Sim-to-real transfer\n",
    "- Partial observability\n",
    "\n",
    "**Solutions:**\n",
    "- Safe policy optimization\n",
    "- Domain randomization\n",
    "- Residual policy learning\n",
    "- Model-based acceleration\n",
    "\n",
    "### Game Playing\n",
    "**Successes:**\n",
    "- AlphaGo/AlphaZero (Go, Chess, Shogi)\n",
    "- OpenAI Five (Dota 2)\n",
    "- AlphaStar (StarCraft II)\n",
    "\n",
    "**Techniques:**\n",
    "- Self-play training\n",
    "- Population-based training\n",
    "- Curriculum learning\n",
    "- Multi-task learning\n",
    "\n",
    "### Natural Language Processing\n",
    "**Applications:**\n",
    "- Text generation\n",
    "- Dialogue systems\n",
    "- Machine translation\n",
    "- Summarization\n",
    "\n",
    "**Methods:**\n",
    "- REINFORCE for sequence generation\n",
    "- Actor-Critic for dialogue\n",
    "- Policy gradients for style transfer\n",
    "\n",
    "### Finance and Trading\n",
    "**Applications:**\n",
    "- Portfolio optimization\n",
    "- Algorithmic trading\n",
    "- Risk management\n",
    "- Market making\n",
    "\n",
    "**Considerations:**\n",
    "- Non-stationarity of markets\n",
    "- Risk constraints\n",
    "- Interpretability requirements\n",
    "- Regulatory compliance\n",
    "\n",
    "## 6.5 Current Challenges and Future Directions\n",
    "\n",
    "### Sample Efficiency\n",
    "**Problem**: Deep RL requires many interactions\n",
    "**Solutions**:\n",
    "- Model-based methods\n",
    "- Transfer learning\n",
    "- Meta-learning\n",
    "- Few-shot learning\n",
    "\n",
    "### Exploration\n",
    "**Problem**: Effective exploration in complex environments\n",
    "**Solutions**:\n",
    "- Curiosity-driven exploration\n",
    "- Count-based exploration\n",
    "- Information-theoretic approaches\n",
    "- Go-Explore algorithm\n",
    "\n",
    "### Safety and Robustness\n",
    "**Problem**: Safe deployment in real-world systems\n",
    "**Solutions**:\n",
    "- Constrained policy optimization\n",
    "- Robust RL methods\n",
    "- Verification techniques\n",
    "- Safe exploration\n",
    "\n",
    "### Interpretability\n",
    "**Problem**: Understanding agent decisions\n",
    "**Solutions**:\n",
    "- Attention mechanisms\n",
    "- Causal analysis\n",
    "- Prototype-based explanations\n",
    "- Policy distillation\n",
    "\n",
    "### Scalability\n",
    "**Problem**: Scaling to complex multi-agent systems\n",
    "**Solutions**:\n",
    "- Distributed training\n",
    "- Communication-efficient methods\n",
    "- Federated learning\n",
    "- Emergent coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc631f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Exercises and Real-World Applications Demo\n",
    "class PolicyGradientWorkshop:\n",
    "    \"\"\"Comprehensive workshop with practical exercises\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.exercises = {\n",
    "            'basic': self._create_basic_exercises(),\n",
    "            'intermediate': self._create_intermediate_exercises(),\n",
    "            'advanced': self._create_advanced_exercises()\n",
    "        }\n",
    "    \n",
    "    def _create_basic_exercises(self):\n",
    "        return [\n",
    "            {\n",
    "                'title': 'Implement Basic REINFORCE',\n",
    "                'description': 'Create a simple REINFORCE agent for CartPole',\n",
    "                'difficulty': 'Beginner',\n",
    "                'estimated_time': '2-3 hours',\n",
    "                'key_concepts': ['Policy gradients', 'Monte Carlo returns', 'Softmax policy'],\n",
    "                'deliverables': [\n",
    "                    'Working REINFORCE implementation',\n",
    "                    'Training curves visualization',\n",
    "                    'Performance analysis report'\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                'title': 'Policy vs Value Methods Comparison',\n",
    "                'description': 'Compare REINFORCE with Q-Learning on the same environment',\n",
    "                'difficulty': 'Beginner',\n",
    "                'estimated_time': '1-2 hours',\n",
    "                'key_concepts': ['Policy vs value methods', 'Sample efficiency', 'Convergence'],\n",
    "                'deliverables': [\n",
    "                    'Side-by-side comparison',\n",
    "                    'Learning curves analysis',\n",
    "                    'Discussion of trade-offs'\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def _create_intermediate_exercises(self):\n",
    "        return [\n",
    "            {\n",
    "                'title': 'Actor-Critic Implementation',\n",
    "                'description': 'Build and train an Actor-Critic agent with baseline',\n",
    "                'difficulty': 'Intermediate',\n",
    "                'estimated_time': '3-4 hours',\n",
    "                'key_concepts': ['Actor-Critic', 'Baseline', 'TD error', 'Variance reduction'],\n",
    "                'deliverables': [\n",
    "                    'Actor-Critic agent',\n",
    "                    'Comparison with REINFORCE',\n",
    "                    'Variance analysis'\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                'title': 'Continuous Control Challenge',\n",
    "                'description': 'Implement Gaussian policy for continuous action spaces',\n",
    "                'difficulty': 'Intermediate',\n",
    "                'estimated_time': '4-5 hours',\n",
    "                'key_concepts': ['Continuous actions', 'Gaussian policy', 'Exploration'],\n",
    "                'deliverables': [\n",
    "                    'Continuous policy network',\n",
    "                    'Training on control task',\n",
    "                    'Action distribution analysis'\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def _create_advanced_exercises(self):\n",
    "        return [\n",
    "            {\n",
    "                'title': 'PPO Implementation',\n",
    "                'description': 'Implement Proximal Policy Optimization with clipped objective',\n",
    "                'difficulty': 'Advanced',\n",
    "                'estimated_time': '6-8 hours',\n",
    "                'key_concepts': ['PPO', 'Clipped objective', 'Trust regions', 'KL divergence'],\n",
    "                'deliverables': [\n",
    "                    'Full PPO implementation',\n",
    "                    'Clipping analysis',\n",
    "                    'Performance comparison with vanilla policy gradients'\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                'title': 'Multi-Agent Policy Gradients',\n",
    "                'description': 'Implement multi-agent policy gradients for competitive/cooperative tasks',\n",
    "                'difficulty': 'Advanced',\n",
    "                'estimated_time': '8-10 hours',\n",
    "                'key_concepts': ['Multi-agent RL', 'Non-stationarity', 'Coordination'],\n",
    "                'deliverables': [\n",
    "                    'Multi-agent environment',\n",
    "                    'Independent learning agents',\n",
    "                    'Centralized training analysis'\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    \n",
    "    def display_workshop_overview(self):\n",
    "        \"\"\"Display comprehensive workshop overview\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # 1. Exercise difficulty distribution\n",
    "        all_exercises = (self.exercises['basic'] + \n",
    "                        self.exercises['intermediate'] + \n",
    "                        self.exercises['advanced'])\n",
    "        \n",
    "        difficulties = [ex['difficulty'] for ex in all_exercises]\n",
    "        difficulty_counts = {d: difficulties.count(d) for d in set(difficulties)}\n",
    "        \n",
    "        colors = ['lightblue', 'orange', 'lightcoral']\n",
    "        axes[0,0].pie(difficulty_counts.values(), labels=difficulty_counts.keys(), \n",
    "                     autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "        axes[0,0].set_title('Exercise Difficulty Distribution')\n",
    "        \n",
    "        # 2. Time commitment breakdown\n",
    "        times = []\n",
    "        labels = []\n",
    "        for category, exercises in self.exercises.items():\n",
    "            for ex in exercises:\n",
    "                time_range = ex['estimated_time']\n",
    "                # Extract average time (simplified)\n",
    "                if '-' in time_range:\n",
    "                    time_parts = time_range.split('-')\n",
    "                    avg_time = (float(time_parts[0]) + float(time_parts[1].split()[0])) / 2\n",
    "                else:\n",
    "                    avg_time = float(time_range.split()[0])\n",
    "                times.append(avg_time)\n",
    "                labels.append(f\"{ex['title'][:15]}...\")\n",
    "        \n",
    "        bars = axes[0,1].barh(labels, times, color=['lightblue']*2 + ['orange']*2 + ['lightcoral']*2)\n",
    "        axes[0,1].set_title('Estimated Time Commitment (hours)')\n",
    "        axes[0,1].set_xlabel('Hours')\n",
    "        \n",
    "        # 3. Key concepts coverage\n",
    "        all_concepts = []\n",
    "        for exercises in self.exercises.values():\n",
    "            for ex in exercises:\n",
    "                all_concepts.extend(ex['key_concepts'])\n",
    "        \n",
    "        concept_counts = {c: all_concepts.count(c) for c in set(all_concepts)}\n",
    "        top_concepts = sorted(concept_counts.items(), key=lambda x: x[1], reverse=True)[:8]\n",
    "        \n",
    "        concepts, counts = zip(*top_concepts)\n",
    "        axes[1,0].bar(range(len(concepts)), counts, color='lightgreen', alpha=0.7)\n",
    "        axes[1,0].set_title('Most Covered Concepts')\n",
    "        axes[1,0].set_ylabel('Frequency')\n",
    "        axes[1,0].set_xticks(range(len(concepts)))\n",
    "        axes[1,0].set_xticklabels(concepts, rotation=45, ha='right')\n",
    "        \n",
    "        # 4. Learning progression\n",
    "        progression_stages = [\n",
    "            'Basic Policy Gradients',\n",
    "            'Variance Reduction',\n",
    "            'Actor-Critic Methods', \n",
    "            'Continuous Control',\n",
    "            'Advanced Algorithms',\n",
    "            'Real-World Applications'\n",
    "        ]\n",
    "        \n",
    "        stage_difficulty = [1, 2, 3, 4, 5, 6]\n",
    "        stage_importance = [5, 4, 5, 4, 3, 2]\n",
    "        \n",
    "        axes[1,1].scatter(stage_difficulty, stage_importance, s=[100*i for i in range(1,7)], \n",
    "                         alpha=0.6, c=range(len(progression_stages)), cmap='viridis')\n",
    "        \n",
    "        for i, stage in enumerate(progression_stages):\n",
    "            axes[1,1].annotate(stage, (stage_difficulty[i], stage_importance[i]),\n",
    "                              xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
    "        \n",
    "        axes[1,1].set_title('Learning Progression Map')\n",
    "        axes[1,1].set_xlabel('Difficulty Level')\n",
    "        axes[1,1].set_ylabel('Foundation Importance')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return all_exercises\n",
    "    \n",
    "    def generate_exercise_assignments(self):\n",
    "        \"\"\"Generate detailed exercise assignments\"\"\"\n",
    "        print(\"DEEP REINFORCEMENT LEARNING - SESSION 4 EXERCISES\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Policy Gradient Methods and Neural Networks in RL\")\n",
    "        print()\n",
    "        \n",
    "        for level, exercises in self.exercises.items():\n",
    "            print(f\"\\\\n{level.upper()} LEVEL EXERCISES:\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            for i, exercise in enumerate(exercises, 1):\n",
    "                print(f\"\\\\n{i}. {exercise['title']}\")\n",
    "                print(f\"   Difficulty: {exercise['difficulty']}\")\n",
    "                print(f\"   Estimated Time: {exercise['estimated_time']}\")\n",
    "                print(f\"   Description: {exercise['description']}\")\n",
    "                print(\"   Key Concepts:\")\n",
    "                for concept in exercise['key_concepts']:\n",
    "                    print(f\"     • {concept}\")\n",
    "                print(\"   Deliverables:\")\n",
    "                for deliverable in exercise['deliverables']:\n",
    "                    print(f\"     ✓ {deliverable}\")\n",
    "        \n",
    "        print(\"\\\\n\\\\nADDITIONAL RESOURCES:\")\n",
    "        print(\"-\" * 25)\n",
    "        print(\"• Original Papers:\")\n",
    "        print(\"  - Williams (1992): REINFORCE Algorithm\")\n",
    "        print(\"  - Sutton et al. (2000): Policy Gradient Methods\")\n",
    "        print(\"  - Mnih et al. (2016): A3C Algorithm\")\n",
    "        print(\"  - Schulman et al. (2017): PPO Algorithm\")\n",
    "        print(\"• Implementation References:\")\n",
    "        print(\"  - OpenAI Spinning Up documentation\")\n",
    "        print(\"  - PyTorch RL examples\")\n",
    "        print(\"  - Stable Baselines3 implementations\")\n",
    "\n",
    "# Real-world application showcase\n",
    "class ApplicationShowcase:\n",
    "    \"\"\"Demonstrate real-world applications of policy gradients\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.applications = {\n",
    "            'Robotics': {\n",
    "                'examples': ['Robot Manipulation', 'Autonomous Driving', 'Drone Control'],\n",
    "                'challenges': ['Safety', 'Sample Efficiency', 'Sim-to-real Transfer'],\n",
    "                'techniques': ['Safe RL', 'Domain Randomization', 'Model-based RL'],\n",
    "                'success_rate': 0.7\n",
    "            },\n",
    "            'Game Playing': {\n",
    "                'examples': ['AlphaGo/Zero', 'OpenAI Five', 'AlphaStar'],\n",
    "                'challenges': ['Large Action Spaces', 'Partial Observability', 'Multi-agent'],\n",
    "                'techniques': ['Self-play', 'Population Training', 'Curriculum Learning'],\n",
    "                'success_rate': 0.9\n",
    "            },\n",
    "            'Finance': {\n",
    "                'examples': ['Portfolio Optimization', 'Algorithmic Trading', 'Risk Management'],\n",
    "                'challenges': ['Non-stationarity', 'Risk Constraints', 'Interpretability'],\n",
    "                'techniques': ['Robust RL', 'Constrained Optimization', 'Risk-aware RL'],\n",
    "                'success_rate': 0.6\n",
    "            },\n",
    "            'NLP': {\n",
    "                'examples': ['Text Generation', 'Dialogue Systems', 'Machine Translation'],\n",
    "                'challenges': ['Discrete Actions', 'Long Sequences', 'Evaluation'],\n",
    "                'techniques': ['Actor-Critic', 'Sequence-to-sequence', 'BLEU optimization'],\n",
    "                'success_rate': 0.8\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def visualize_applications(self):\n",
    "        \"\"\"Create comprehensive application overview\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "        \n",
    "        # 1. Success rates by domain\n",
    "        domains = list(self.applications.keys())\n",
    "        success_rates = [self.applications[domain]['success_rate'] for domain in domains]\n",
    "        \n",
    "        colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']\n",
    "        bars = axes[0,0].bar(domains, success_rates, color=colors, alpha=0.8)\n",
    "        axes[0,0].set_title('Policy Gradient Success Rate by Domain')\n",
    "        axes[0,0].set_ylabel('Success Rate')\n",
    "        axes[0,0].set_ylim(0, 1)\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, rate in zip(bars, success_rates):\n",
    "            height = bar.get_height()\n",
    "            axes[0,0].text(bar.get_x() + bar.get_width()/2., height + 0.02,\n",
    "                          f'{rate:.1%}', ha='center', va='bottom')\n",
    "        \n",
    "        # 2. Challenge frequency analysis\n",
    "        all_challenges = []\n",
    "        for domain_info in self.applications.values():\n",
    "            all_challenges.extend(domain_info['challenges'])\n",
    "        \n",
    "        challenge_counts = {c: all_challenges.count(c) for c in set(all_challenges)}\n",
    "        top_challenges = sorted(challenge_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        if top_challenges:\n",
    "            challenges, counts = zip(*top_challenges)\n",
    "            axes[0,1].barh(challenges, counts, color='lightcoral', alpha=0.7)\n",
    "            axes[0,1].set_title('Most Common Challenges')\n",
    "            axes[0,1].set_xlabel('Frequency Across Domains')\n",
    "        \n",
    "        # 3. Technique adoption\n",
    "        all_techniques = []\n",
    "        for domain_info in self.applications.values():\n",
    "            all_techniques.extend(domain_info['techniques'])\n",
    "        \n",
    "        technique_counts = {t: all_techniques.count(t) for t in set(all_techniques)}\n",
    "        top_techniques = sorted(technique_counts.items(), key=lambda x: x[1], reverse=True)[:6]\n",
    "        \n",
    "        if top_techniques:\n",
    "            techniques, counts = zip(*top_techniques)\n",
    "            axes[1,0].pie(counts, labels=techniques, autopct='%1.1f%%', startangle=90)\n",
    "            axes[1,0].set_title('Popular Techniques Distribution')\n",
    "        \n",
    "        # 4. Domain complexity vs maturity\n",
    "        complexity_scores = {'Robotics': 5, 'Game Playing': 4, 'Finance': 3, 'NLP': 4}\n",
    "        maturity_scores = {'Robotics': 3, 'Game Playing': 5, 'Finance': 2, 'NLP': 4}\n",
    "        \n",
    "        for domain in domains:\n",
    "            axes[1,1].scatter(complexity_scores[domain], maturity_scores[domain], \n",
    "                             s=success_rates[domains.index(domain)] * 500,\n",
    "                             alpha=0.6, label=domain)\n",
    "        \n",
    "        axes[1,1].set_xlabel('Technical Complexity')\n",
    "        axes[1,1].set_ylabel('Field Maturity')\n",
    "        axes[1,1].set_title('Domain Analysis (size = success rate)')\n",
    "        axes[1,1].legend()\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Execute workshop and showcase\n",
    "print(\"Creating Policy Gradient Workshop and Application Showcase...\")\n",
    "print()\n",
    "\n",
    "workshop = PolicyGradientWorkshop()\n",
    "exercises = workshop.display_workshop_overview()\n",
    "\n",
    "print(\"\\\\n\" + \"=\"*80)\n",
    "workshop.generate_exercise_assignments()\n",
    "\n",
    "print(\"\\\\n\\\\n\" + \"=\"*80)\n",
    "print(\"REAL-WORLD APPLICATIONS SHOWCASE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "showcase = ApplicationShowcase()\n",
    "showcase.visualize_applications()\n",
    "\n",
    "print(\"\\\\nApplication Domain Summary:\")\n",
    "for domain, info in showcase.applications.items():\n",
    "    print(f\"\\\\n{domain}:\")\n",
    "    print(f\"  Success Rate: {info['success_rate']:.1%}\")\n",
    "    print(f\"  Key Examples: {', '.join(info['examples'])}\")\n",
    "    print(f\"  Main Techniques: {', '.join(info['techniques'])}\")\n",
    "\n",
    "print(\"\\\\n✓ Comprehensive workshop materials generated\")\n",
    "print(\"✓ Real-world applications analyzed\")\n",
    "print(\"✓ Exercise assignments created\")\n",
    "print(\"\\\\n🎯 Ready for hands-on policy gradient implementation!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0db6e",
   "metadata": {},
   "source": [
    "# Session 4 Summary and Conclusions\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Evolution from Value-Based to Policy-Based Methods\n",
    "- **Value-based methods (Q-learning, SARSA)**: Learn action values, derive policies\n",
    "- **Policy-based methods**: Directly optimize parameterized policies\n",
    "- **Actor-Critic methods**: Combine both approaches for reduced variance\n",
    "\n",
    "### 2. Policy Gradient Fundamentals\n",
    "- **Policy Gradient Theorem**: Foundation for all policy gradient methods\n",
    "- **REINFORCE Algorithm**: Monte Carlo policy gradient method\n",
    "- **Score Function**: ∇_θ log π(a|s,θ) guides parameter updates\n",
    "- **Baseline Subtraction**: Reduces variance without introducing bias\n",
    "\n",
    "### 3. Neural Network Function Approximation\n",
    "- **Universal Function Approximation**: Handle large/continuous state-action spaces\n",
    "- **Shared Feature Learning**: Efficient parameter sharing between actor and critic\n",
    "- **Continuous Action Spaces**: Gaussian policies for continuous control\n",
    "- **Training Stability**: Gradient clipping, learning rate scheduling, normalization\n",
    "\n",
    "### 4. Advanced Algorithms\n",
    "- **PPO (Proximal Policy Optimization)**: Stable policy updates with clipping\n",
    "- **TRPO (Trust Region Policy Optimization)**: Theoretical guarantees\n",
    "- **A3C/A2C (Advantage Actor-Critic)**: Asynchronous/synchronous training\n",
    "\n",
    "### 5. Real-World Impact\n",
    "- **Robotics**: Manipulation, autonomous vehicles, drone control\n",
    "- **Games**: AlphaGo/Zero, OpenAI Five, AlphaStar\n",
    "- **NLP**: Text generation, dialogue systems, machine translation\n",
    "- **Finance**: Portfolio optimization, algorithmic trading\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison: Session 3 vs Session 4\n",
    "\n",
    "| Aspect | Session 3 (TD Learning) | Session 4 (Policy Gradients) |\n",
    "|--------|------------------------|-------------------------------|\n",
    "| **Learning Target** | Action-value function Q(s,a) | Policy π(a\\|s,θ) |\n",
    "| **Action Selection** | ε-greedy, Boltzmann | Stochastic sampling |\n",
    "| **Update Rule** | TD error: δ = r + γQ(s',a') - Q(s,a) | Policy gradient: ∇J(θ) |\n",
    "| **Convergence** | To optimal Q-function | To optimal policy |\n",
    "| **Action Spaces** | Discrete (easily) | Discrete and continuous |\n",
    "| **Exploration** | External (ε-greedy) | Built-in (stochastic policy) |\n",
    "| **Sample Efficiency** | Generally higher | Lower (but improving) |\n",
    "| **Theoretical Guarantees** | Strong (tabular case) | Strong (policy gradient theorem) |\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Implementation Checklist\n",
    "\n",
    "### ✅ Basic REINFORCE Implementation\n",
    "- [ ] Policy network with softmax output\n",
    "- [ ] Episode trajectory collection\n",
    "- [ ] Monte Carlo return computation\n",
    "- [ ] Policy gradient updates\n",
    "- [ ] Learning curve visualization\n",
    "\n",
    "### ✅ Actor-Critic Implementation\n",
    "- [ ] Separate actor and critic networks\n",
    "- [ ] TD error computation\n",
    "- [ ] Advantage estimation\n",
    "- [ ] Simultaneous network updates\n",
    "- [ ] Variance reduction analysis\n",
    "\n",
    "### ✅ Continuous Control Extension\n",
    "- [ ] Gaussian policy network\n",
    "- [ ] Action sampling and log-probability\n",
    "- [ ] Continuous environment interface\n",
    "- [ ] Policy entropy monitoring\n",
    "\n",
    "### ✅ Advanced Features\n",
    "- [ ] Baseline subtraction\n",
    "- [ ] Gradient clipping\n",
    "- [ ] Learning rate scheduling\n",
    "- [ ] Experience normalization\n",
    "- [ ] Performance benchmarking\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps and Further Learning\n",
    "\n",
    "### Immediate Next Topics (Session 5+)\n",
    "1. **Model-Based Reinforcement Learning**\n",
    "   - Dyna-Q, PETS, MPC\n",
    "   - Sample efficiency improvements\n",
    "   \n",
    "2. **Deep Q-Networks and Variants**\n",
    "   - DQN, Double DQN, Dueling DQN\n",
    "   - Rainbow improvements\n",
    "   \n",
    "3. **Multi-Agent Reinforcement Learning**\n",
    "   - Independent learning\n",
    "   - Centralized training, decentralized execution\n",
    "   - Game theory applications\n",
    "\n",
    "### Advanced Research Directions\n",
    "1. **Meta-Learning in RL**\n",
    "   - Learning to learn quickly\n",
    "   - Few-shot adaptation\n",
    "   \n",
    "2. **Safe Reinforcement Learning**\n",
    "   - Constrained policy optimization\n",
    "   - Risk-aware methods\n",
    "   \n",
    "3. **Explainable RL**\n",
    "   - Interpretable policies\n",
    "   - Causal reasoning\n",
    "\n",
    "### Recommended Resources\n",
    "- **Books**: \"Reinforcement Learning: An Introduction\" by Sutton & Barto\n",
    "- **Papers**: Original policy gradient papers (Williams 1992, Sutton 2000)\n",
    "- **Code**: OpenAI Spinning Up, Stable Baselines3\n",
    "- **Environments**: OpenAI Gym, PyBullet, MuJoCo\n",
    "\n",
    "---\n",
    "\n",
    "## Final Reflection Questions\n",
    "\n",
    "1. **When would you choose policy gradients over Q-learning?**\n",
    "   - Continuous action spaces\n",
    "   - Stochastic optimal policies\n",
    "   - Direct policy optimization needs\n",
    "\n",
    "2. **How do you handle the exploration-exploitation trade-off in policy gradients?**\n",
    "   - Stochastic policies provide natural exploration\n",
    "   - Entropy regularization\n",
    "   - Curiosity-driven methods\n",
    "\n",
    "3. **What are the main challenges in scaling policy gradients to real applications?**\n",
    "   - Sample efficiency\n",
    "   - Safety constraints\n",
    "   - Hyperparameter sensitivity\n",
    "   - Sim-to-real transfer\n",
    "\n",
    "4. **How do neural networks change the RL landscape?**\n",
    "   - Function approximation for large spaces\n",
    "   - End-to-end learning\n",
    "   - Representation learning\n",
    "   - Transfer capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**Session 4 Complete: Policy Gradient Methods and Neural Networks in RL**\n",
    "\n",
    "You now have the theoretical foundation and practical tools to implement and apply policy gradient methods in deep reinforcement learning. The journey from tabular methods (Session 1-2) through temporal difference learning (Session 3) to policy gradients (Session 4) represents the core evolution of modern RL algorithms.\n",
    "\n",
    "**🚀 Ready to tackle real-world RL problems with policy gradient methods!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
