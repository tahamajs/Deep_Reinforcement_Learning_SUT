{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82bb9112",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning - Computer Assignment 1\n",
    "\n",
    "## Introduction to Deep Reinforcement Learning\n",
    "\n",
    "Deep Reinforcement Learning (DRL) combines reinforcement learning with deep neural networks to solve complex decision-making problems. This assignment will cover the fundamental concepts, algorithms, and practical implementations of DRL.\n",
    "\n",
    "### Learning Objectives\n",
    "By the end of this assignment, you will understand:\n",
    "1. **Markov Decision Processes (MDPs)** - The mathematical framework for decision making\n",
    "2. **Value Functions** - State-value and action-value functions\n",
    "3. **Policy Optimization** - Policy gradient methods and actor-critic algorithms\n",
    "4. **Deep Q-Networks (DQN)** - Value-based deep RL methods\n",
    "5. **Policy Gradient Methods** - Direct policy optimization\n",
    "6. **Actor-Critic Methods** - Combining value and policy-based approaches\n",
    "\n",
    "---\n",
    "\n",
    "## Part 1: Theoretical Foundations\n",
    "\n",
    "### 1.1 Markov Decision Process (MDP)\n",
    "\n",
    "**Definition:**\n",
    "An MDP is defined by the tuple $(S, A, P, R, \\gamma)$ where:\n",
    "\n",
    "-   **$S$**: Set of states - represents all possible situations the agent can encounter\n",
    "-   **$A$**: Set of actions - all possible decisions the agent can make\n",
    "-   **$P$**: Transition probability function $P(s'|s,a)$ - probability of moving to state $s'$ given current state $s$ and action $a$\n",
    "-   **$R$**: Reward function $R(s,a,s')$ - immediate reward received for transitioning from state $s$ to $s'$ via action $a$\n",
    "-   **$\\gamma$**: Discount factor $[0,1]$ - determines the importance of future rewards\n",
    "\n",
    "**Objective:**\n",
    "The agent's goal is to find an optimal policy $\\pi^*(a|s)$ that maximizes the expected cumulative reward:\n",
    "\n",
    "$$G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}$$\n",
    "\n",
    "**Intuition:**\n",
    "Think of an MDP as a decision-making framework where:\n",
    "-   You're in a specific situation (state)\n",
    "-   You can take certain actions\n",
    "-   Your action determines what happens next (probabilistically)\n",
    "-   You get feedback (reward) for your choices\n",
    "-   You want to maximize long-term success, not just immediate gain\n",
    "\n",
    "---\n",
    "\n",
    "### 1.2 Value Functions\n",
    "\n",
    "**State Value Function:**\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t = s]$$\n",
    "\n",
    "**Interpretation:** The expected total reward when starting from state $s$ and following policy $\\pi$. It answers: \"How good is it to be in this state?\"\n",
    "\n",
    "**Action Value Function (Q-function):**\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$$\n",
    "\n",
    "**Interpretation:** The expected total reward when taking action $a$ in state $s$ and then following policy $\\pi$. It answers: \"How good is it to take this specific action in this state?\"\n",
    "\n",
    "**Bellman Equations:**\n",
    "\n",
    "For State Value Function:\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s',r} p(s',r|s,a)[r + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "For Action Value Function:\n",
    "$$Q^\\pi(s,a) = \\sum_{s',r} p(s',r|s,a)[r + \\gamma \\sum_{a'} \\pi(a'|s') Q^\\pi(s',a')]$$\n",
    "\n",
    "**Key Insight:** The Bellman equations express a recursive relationship - the value of a state depends on the immediate reward plus the discounted value of future states. This is the foundation of dynamic programming in RL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b200f275",
   "metadata": {},
   "source": [
    "## Part 2: Deep Q-Learning (DQN)\n",
    "\n",
    "### 2.1 Q-Learning Algorithm\n",
    "\n",
    "Q-Learning is a model-free, off-policy algorithm that learns the optimal action-value function:\n",
    "\n",
    "**Q-Learning Update Rule:**\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "### 2.2 Deep Q-Network (DQN) Enhancements\n",
    "\n",
    "**Key Innovations:**\n",
    "1. **Experience Replay**: Store transitions $(s,a,r,s')$ in replay buffer\n",
    "2. **Target Network**: Use separate network for target values to improve stability\n",
    "3. **Double DQN**: Mitigate overestimation bias\n",
    "4. **Dueling DQN**: Separate value and advantage streams\n",
    "\n",
    "**DQN Loss Function:**\n",
    "$$L(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim D} \\left[ \\left( r + \\gamma \\max_{a'} Q(s',a';\\theta^-) - Q(s,a;\\theta) \\right)^2 \\right]$$\n",
    "\n",
    "Where $\\theta^-$ represents the target network parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e427e759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport matplotlib.pyplot as plt\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gym\nimport random\nfrom collections import deque, namedtuple\nimport seaborn as sns\nfrom typing import Tuple, List, Optional, Any\ndef set_seed(seed: int = 42) -> None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    random.seed(seed)\nset_seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\ndef moving_average(x: List[float], window: int = 10) -> np.ndarray:\n    if len(x) < 1:\n        return np.array([])\n    if window <= 1:\n        return np.array(x)\n    return np.convolve(x, np.ones(window) / window, mode='valid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n    def __init__(self, state_size: int, action_size: int, hidden_size: int = 64) -> None:\n        super(DQN, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, action_size)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\nclass DuelingDQN(nn.Module):\n    def __init__(self, state_size: int, action_size: int, hidden_size: int = 64) -> None:\n        super(DuelingDQN, self).__init__()\n        self.feature_layer = nn.Sequential(\n            nn.Linear(state_size, hidden_size),\n            nn.ReLU()\n        )\n        self.value_stream = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, 1)\n        )\n        self.advantage_stream = nn.Sequential(\n            nn.Linear(hidden_size, hidden_size),\n            nn.ReLU(),\n            nn.Linear(hidden_size, action_size)\n        )\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n        features = self.feature_layer(x)\n        value = self.value_stream(features)\n        advantage = self.advantage_stream(features)\n        q_value = value + advantage - advantage.mean(dim=1, keepdim=True)\n        return q_value\nstate_size = 4\naction_size = 2\n_dqn = DQN(state_size, action_size)\n_dueling = DuelingDQN(state_size, action_size)\nprint(\"DQN and DuelingDQN classes defined.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32f8ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n    def __init__(self, capacity: int) -> None:\n        self.buffer = deque(maxlen=capacity)\n        self.experience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\n    def add(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool) -> None:\n        e = self.experience(state, action, reward, next_state, done)\n        self.buffer.append(e)\n    def sample(self, batch_size: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n        experiences = random.sample(self.buffer, k=batch_size)\n        states = np.vstack([e.state for e in experiences if e is not None])\n        actions = np.vstack([e.action for e in experiences if e is not None])\n        rewards = np.vstack([e.reward for e in experiences if e is not None])\n        next_states = np.vstack([e.next_state for e in experiences if e is not None])\n        dones = np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)\n        states = torch.from_numpy(states).float().to(device)\n        actions = torch.from_numpy(actions).long().to(device)\n        rewards = torch.from_numpy(rewards).float().to(device)\n        next_states = torch.from_numpy(next_states).float().to(device)\n        dones = torch.from_numpy(dones).float().to(device)\n        return states, actions, rewards, next_states, dones\n    def __len__(self) -> int:\n        return len(self.buffer)\nbuffer = ReplayBuffer(10000)\nprint(f\"Replay buffer initialized with capacity: {10000}\")\nprint(f\"Current buffer size: {len(buffer)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994b8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n    def __init__(self,\n                 state_size: int,\n                 action_size: int,\n                 lr: float = 1e-3,\n                 gamma: float = 0.99,\n                 epsilon: float = 1.0,\n                 epsilon_decay: float = 0.995,\n                 epsilon_min: float = 0.01,\n                 buffer_size: int = 10000,\n                 batch_size: int = 64,\n                 update_every: int = 4,\n                 tau: float = 1e-3,\n                 use_double_dqn: bool = False,\n                 use_dueling: bool = False) -> None:\n        self.state_size = state_size\n        self.action_size = action_size\n        self.lr = lr\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.epsilon_decay = epsilon_decay\n        self.epsilon_min = epsilon_min\n        self.batch_size = batch_size\n        self.update_every = update_every\n        self.tau = tau\n        self.use_double_dqn = use_double_dqn\n        if use_dueling:\n            self.q_network = DuelingDQN(state_size, action_size).to(device)\n            self.target_network = DuelingDQN(state_size, action_size).to(device)\n        else:\n            self.q_network = DQN(state_size, action_size).to(device)\n            self.target_network = DQN(state_size, action_size).to(device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n        self.memory = ReplayBuffer(buffer_size)\n        self.t_step = 0\n        self.hard_update(self.target_network, self.q_network)\n    def step(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool) -> None:\n        self.memory.add(state, action, reward, next_state, done)\n        self.t_step = (self.t_step + 1) % self.update_every\n        if self.t_step == 0:\n            if len(self.memory) > self.batch_size:\n                experiences = self.memory.sample(self.batch_size)\n                self.learn(experiences)\n    def act(self, state: np.ndarray, eps: Optional[float] = None) -> int:\n        if eps is None:\n            eps = self.epsilon\n        if random.random() > eps:\n            state_t = torch.from_numpy(state).float().unsqueeze(0).to(device)\n            self.q_network.eval()\n            with torch.no_grad():\n                action_values = self.q_network(state_t)\n            self.q_network.train()\n            return int(action_values.argmax(dim=1).item())\n        else:\n            return int(random.choice(np.arange(self.action_size)))\n    def learn(self, experiences: Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]) -> None:\n        states, actions, rewards, next_states, dones = experiences\n        if self.use_double_dqn:\n            next_actions = self.q_network(next_states).detach().argmax(1).unsqueeze(1)\n            Q_targets_next = self.target_network(next_states).detach().gather(1, next_actions)\n        else:\n            Q_targets_next = self.target_network(next_states).detach().max(1)[0].unsqueeze(1)\n        Q_targets = rewards + (self.gamma * Q_targets_next * (1 - dones))\n        Q_expected = self.q_network(states).gather(1, actions)\n        loss = F.mse_loss(Q_expected, Q_targets)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        self.soft_update(self.q_network, self.target_network, self.tau)\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n    def soft_update(self, local_model: nn.Module, target_model: nn.Module, tau: float) -> None:\n        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n    def hard_update(self, target: nn.Module, source: nn.Module) -> None:\n        for target_param, param in zip(target.parameters(), source.parameters()):\n            target_param.data.copy_(param.data)\nagent = DQNAgent(state_size=4, action_size=2, use_dueling=True, use_double_dqn=True)\nprint(\"DQN Agent initialized successfully!\")\nprint(f\"Network type: {'Dueling' if isinstance(agent.q_network, DuelingDQN) else 'Standard'}\")\nprint(f\"Double DQN enabled: {agent.use_double_dqn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d91c144",
   "metadata": {},
   "source": [
    "## Part 3: Policy Gradient Methods\n",
    "\n",
    "### 3.1 Policy Gradient Theorem\n",
    "\n",
    "Instead of learning value functions, policy gradient methods directly optimize the policy parameters $\\theta$.\n",
    "\n",
    "**Policy Gradient Theorem:**\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot Q^{\\pi_\\theta}(s,a) \\right]$$\n",
    "\n",
    "Where $J(\\theta)$ is the expected return under policy $\\pi_\\theta$.\n",
    "\n",
    "### 3.2 REINFORCE Algorithm\n",
    "\n",
    "REINFORCE uses Monte Carlo sampling to estimate the policy gradient:\n",
    "\n",
    "**REINFORCE Update:**\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t$$\n",
    "\n",
    "Where $G_t$ is the return from time step $t$.\n",
    "\n",
    "### 3.3 Actor-Critic Methods\n",
    "\n",
    "Actor-Critic combines policy gradient (actor) with value function approximation (critic):\n",
    "\n",
    "- **Actor**: Updates policy parameters using policy gradient\n",
    "- **Critic**: Updates value function parameters using TD learning\n",
    "\n",
    "**Actor Update:**\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha_\\theta \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\delta_t$$\n",
    "\n",
    "**Critic Update:**\n",
    "$$w_{t+1} = w_t + \\alpha_w \\delta_t \\nabla_w V_w(s_t)$$\n",
    "\n",
    "Where $\\delta_t = r_t + \\gamma V_w(s_{t+1}) - V_w(s_t)$ is the TD error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21f79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(nn.Module):\n    def __init__(self, state_size: int, action_size: int, hidden_size: int = 64) -> None:\n        super(PolicyNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, action_size)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return F.softmax(self.fc3(x), dim=1)\nclass REINFORCEAgent:\n    def __init__(self, state_size: int, action_size: int, lr: float = 1e-3, gamma: float = 0.99) -> None:\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = gamma\n        self.policy = PolicyNetwork(state_size, action_size).to(device)\n        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)\n        self.reset_episode()\n    def reset_episode(self) -> None:\n        self.states: List[np.ndarray] = []\n        self.actions: List[int] = []\n        self.rewards: List[float] = []\n        self.log_probs: List[torch.Tensor] = []\n    def act(self, state: np.ndarray) -> int:\n        state_t = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.policy(state_t)\n        m = torch.distributions.Categorical(probs)\n        action = m.sample()\n        self.log_probs.append(m.log_prob(action))\n        return int(action.item())\n    def step(self, state: np.ndarray, action: int, reward: float) -> None:\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n    def learn(self) -> float:\n        returns: List[float] = []\n        G = 0.0\n        for r in reversed(self.rewards):\n            G = r + self.gamma * G\n            returns.insert(0, G)\n        returns_t = torch.tensor(returns).float().to(device)\n        returns_t = (returns_t - returns_t.mean()) / (returns_t.std() + 1e-8)\n        policy_loss = []\n        for log_prob, Gt in zip(self.log_probs, returns_t):\n            policy_loss.append(-log_prob * Gt)\n        self.optimizer.zero_grad()\n        loss = torch.stack(policy_loss).sum()\n        loss.backward()\n        self.optimizer.step()\n        self.reset_episode()\n        return float(loss.item())\nclass ValueNetwork(nn.Module):\n    def __init__(self, state_size: int, hidden_size: int = 64) -> None:\n        super(ValueNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, 1)\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if x.dim() == 1:\n            x = x.unsqueeze(0)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\nclass ActorCriticAgent:\n    def __init__(self, state_size: int, action_size: int, lr_actor: float = 1e-3, lr_critic: float = 1e-3, gamma: float = 0.99) -> None:\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = gamma\n        self.actor = PolicyNetwork(state_size, action_size).to(device)\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n        self.critic = ValueNetwork(state_size).to(device)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n    def act(self, state: np.ndarray) -> Tuple[int, torch.Tensor]:\n        state_t = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.actor(state_t)\n        m = torch.distributions.Categorical(probs)\n        action = m.sample()\n        return int(action.item()), m.log_prob(action)\n    def learn(self, state: np.ndarray, action: int, reward: float, next_state: np.ndarray, done: bool, log_prob: torch.Tensor) -> Tuple[float, float]:\n        state_t = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        next_state_t = torch.from_numpy(next_state).float().unsqueeze(0).to(device)\n        reward_t = torch.tensor([reward]).float().to(device)\n        done_t = torch.tensor([done]).float().to(device)\n        current_value = self.critic(state_t)\n        next_value = self.critic(next_state_t) if not done else torch.zeros_like(current_value).to(device)\n        td_target = reward_t + self.gamma * next_value * (1 - done_t)\n        td_error = td_target - current_value\n        critic_loss = td_error.pow(2).mean()\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n        actor_loss = (-log_prob * td_error.detach()).mean()\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n        return float(actor_loss.item()), float(critic_loss.item())\nreinforce_agent = REINFORCEAgent(state_size=4, action_size=2, lr=1e-3)\nac_agent = ActorCriticAgent(state_size=4, action_size=2)\nprint(\"REINFORCE and Actor-Critic agents initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faea9787",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueNetwork(nn.Module):\n    def __init__(self, state_size, hidden_size=64):\n        super(ValueNetwork, self).__init__()\n        self.fc1 = nn.Linear(state_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, 1)\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\nclass ActorCriticAgent:\n    def __init__(self, state_size, action_size, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.gamma = gamma\n        self.actor = PolicyNetwork(state_size, action_size).to(device)\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n        self.critic = ValueNetwork(state_size).to(device)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n    def act(self, state):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        probs = self.actor(state)\n        m = torch.distributions.Categorical(probs)\n        action = m.sample()\n        return action.item(), m.log_prob(action)\n    def learn(self, state, action, reward, next_state, done, log_prob):\n        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n        next_state = torch.from_numpy(next_state).float().unsqueeze(0).to(device)\n        reward = torch.tensor([reward]).float().to(device)\n        done = torch.tensor([done]).float().to(device)\n        current_value = self.critic(state)\n        next_value = self.critic(next_state) if not done else torch.zeros(1).to(device)\n        td_target = reward + self.gamma * next_value * (1 - done)\n        td_error = td_target - current_value\n        critic_loss = td_error.pow(2)\n        actor_loss = -log_prob * td_error.detach()\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        self.critic_optimizer.step()\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n        return actor_loss.item(), critic_loss.item()\nac_agent = ActorCriticAgent(state_size=4, action_size=2, lr_actor=1e-3, lr_critic=1e-3)\nprint(\"Actor-Critic Agent initialized successfully!\")\nprint(\"Actor (Policy) network:\")\nprint(ac_agent.actor)\nprint(\"\\nCritic (Value) network:\")\nprint(ac_agent.critic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f7508a",
   "metadata": {},
   "source": [
    "## Part 4: Practical Implementation and Comparison\n",
    "\n",
    "### 4.1 Training Environment Setup\n",
    "\n",
    "We'll use the CartPole environment from OpenAI Gym to demonstrate the algorithms:\n",
    "\n",
    "- **State Space**: 4-dimensional continuous (position, velocity, angle, angular velocity)\n",
    "- **Action Space**: 2 discrete actions (left, right)\n",
    "- **Reward**: +1 for every step the pole stays upright\n",
    "- **Episode Termination**: Pole angle > 15° or cart position > 2.4 units\n",
    "- **Success Criteria**: Average reward > 195 over 100 consecutive episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d54c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gym_reset(env: gym.Env) -> np.ndarray:\n    result = env.reset()\n    if isinstance(result, tuple):\n        state, _ = result\n    else:\n        state = result\n    return np.array(state, dtype=np.float32)\ndef gym_step(env: gym.Env, action: int) -> Tuple[np.ndarray, float, bool, dict]:\n    result = env.step(action)\n    if len(result) == 4:\n        next_state, reward, done, info = result\n    else:\n        next_state, reward, terminated, truncated, info = result\n        done = terminated or truncated\n    return np.array(next_state, dtype=np.float32), float(reward), bool(done), info\ndef train_dqn_agent(agent: DQNAgent, env: gym.Env, n_episodes: int = 1000, max_t: int = 1000) -> List[float]:\n    scores: List[float] = []\n    scores_window = deque(maxlen=100)\n    for i_episode in range(1, n_episodes + 1):\n        state = gym_reset(env)\n        score = 0.0\n        for t in range(max_t):\n            action = agent.act(state)\n            next_state, reward, done, _ = gym_step(env, action)\n            agent.step(state, action, reward, next_state, done)\n            state = next_state\n            score += reward\n            if done:\n                break\n        scores_window.append(score)\n        scores.append(score)\n        if i_episode % 100 == 0:\n            print(f'Episode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tEpsilon: {agent.epsilon:.3f}')\n        if len(scores_window) == scores_window.maxlen and np.mean(scores_window) >= 195.0:\n            print(f'Environment solved in {i_episode} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n            break\n    return scores\ndef train_reinforce_agent(agent: REINFORCEAgent, env: gym.Env, n_episodes: int = 1000, max_t: int = 1000) -> List[float]:\n    scores: List[float] = []\n    scores_window = deque(maxlen=100)\n    for i_episode in range(1, n_episodes + 1):\n        state = gym_reset(env)\n        agent.reset_episode()\n        score = 0.0\n        for t in range(max_t):\n            action = agent.act(state)\n            next_state, reward, done, _ = gym_step(env, action)\n            agent.step(state, action, reward)\n            state = next_state\n            score += reward\n            if done:\n                break\n        loss = agent.learn()\n        scores_window.append(score)\n        scores.append(score)\n        if i_episode % 100 == 0:\n            print(f'Episode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tLoss: {loss:.3f}')\n        if len(scores_window) == scores_window.maxlen and np.mean(scores_window) >= 195.0:\n            print(f'Environment solved in {i_episode} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n            break\n    return scores\ndef train_actor_critic_agent(agent: ActorCriticAgent, env: gym.Env, n_episodes: int = 1000, max_t: int = 1000) -> List[float]:\n    scores: List[float] = []\n    scores_window = deque(maxlen=100)\n    for i_episode in range(1, n_episodes + 1):\n        state = gym_reset(env)\n        score = 0.0\n        for t in range(max_t):\n            action, log_prob = agent.act(state)\n            next_state, reward, done, _ = gym_step(env, action)\n            actor_loss, critic_loss = agent.learn(state, action, reward, next_state, done, log_prob)\n            state = next_state\n            score += reward\n            if done:\n                break\n        scores_window.append(score)\n        scores.append(score)\n        if i_episode % 100 == 0:\n            print(f'Episode {i_episode}\\tAverage Score: {np.mean(scores_window):.2f}\\tActorLoss: {actor_loss:.3f}\\tCriticLoss: {critic_loss:.3f}')\n        if len(scores_window) == scores_window.maxlen and np.mean(scores_window) >= 195.0:\n            print(f'Environment solved in {i_episode} episodes!\\tAverage Score: {np.mean(scores_window):.2f}')\n            break\n    return scores\nprint(\"Training functions defined successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cffda03",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n    env_name = 'CartPole-v1'\n    try:\n        env = gym.make(env_name)\n        print(f\"Environment '{env_name}' created successfully!\")\n        print(f\"State space: {env.observation_space}\")\n        print(f\"Action space: {env.action_space}\")\n        state = gym_reset(env)\n        print(f\"Initial state shape: {np.shape(state)}\")\n    except Exception as e:\n        print(f\"Error creating gym environment: {e}\")\n        print(\"Creating mock environment for demonstration...\")\n        raise ImportError from e\nexcept Exception:\n    class MockEnv:\n        def __init__(self):\n            self.observation_space = type('', (), {'shape': (4,)})()\n            self.action_space = type('', (), {'n': 2})()\n            self.state = np.random.random(4).astype(np.float32)\n        def reset(self):\n            self.state = np.random.random(4).astype(np.float32)\n            return self.state\n        def step(self, action):\n            self.state = np.random.random(4).astype(np.float32)\n            reward = float(np.random.randint(0, 2))\n            done = np.random.random() < 0.05\n            return self.state, reward, done, {}\n    env = MockEnv()\n    print(\"Mock environment created for demonstration purposes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213deaef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_demo_episodes = 50\nprint(\"=\" * 60)\nprint(\"TRAINING DEMONSTRATION (short runs)\")\nprint(\"=\" * 60)\n_demo_hidden = 64\ndqn_agent_demo = DQNAgent(state_size=4, action_size=2, use_dueling=True, use_double_dqn=True)\nreinforce_agent_demo = REINFORCEAgent(state_size=4, action_size=2)\nac_agent_demo = ActorCriticAgent(state_size=4, action_size=2)\nresults = {}\nprint(\"\\n1. Training DQN Agent (demo)...\")\ndqn_scores = train_dqn_agent(dqn_agent_demo, env, n_episodes=n_demo_episodes, max_t=200)\nresults['DQN'] = dqn_scores\nprint(\"\\n2. Training REINFORCE Agent (demo)...\")\nreinforce_scores = train_reinforce_agent(reinforce_agent_demo, env, n_episodes=n_demo_episodes, max_t=200)\nresults['REINFORCE'] = reinforce_scores\nprint(\"\\n3. Training Actor-Critic Agent (demo)...\")\nac_scores = train_actor_critic_agent(ac_agent_demo, env, n_episodes=n_demo_episodes, max_t=200)\nresults['Actor-Critic'] = ac_scores\nprint(\"\\nTraining demonstration completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not results:\n    print(\"No results to plot. Run the demo training cell first.\")\nelse:\n    plt.figure(figsize=(15, 10))\n    plt.subplot(2, 2, 1)\n    for algorithm, scores in results.items():\n        plt.plot(scores, label=algorithm, linewidth=2)\n        if len(scores) >= 3:\n            ma = moving_average(scores, window=min(10, max(1, len(scores))))\n            plt.plot(range(len(scores) - len(ma) + 1), ma, '--', alpha=0.7, linewidth=1)\n    plt.xlabel('Episode')\n    plt.ylabel('Score')\n    plt.title('Learning Curves Comparison')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.subplot(2, 2, 2)\n    final_scores = [np.mean(scores[-10:]) if len(scores) >= 1 else 0.0 for scores in results.values()]\n    algorithms = list(results.keys())\n    colors = ['skyblue', 'lightcoral', 'lightgreen']\n    bars = plt.bar(algorithms, final_scores, color=colors[:len(algorithms)], alpha=0.8)\n    plt.ylabel('Average Score (Last 10 Episodes)')\n    plt.title('Final Performance Comparison')\n    plt.grid(True, alpha=0.3, axis='y')\n    for bar, score in zip(bars, final_scores):\n        plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{score:.1f}', ha='center', va='bottom', fontweight='bold')\n    plt.subplot(2, 2, 3)\n    for i, (algorithm, scores) in enumerate(results.items()):\n        plt.hist(scores, bins=15, alpha=0.6, label=algorithm, color=colors[i % len(colors)], density=True)\n    plt.xlabel('Score')\n    plt.ylabel('Density')\n    plt.title('Score Distribution')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.subplot(2, 2, 4)\n    stats_data = []\n    for algorithm, scores in results.items():\n        stats_data.append({\n            'Algorithm': algorithm,\n            'Mean': float(np.mean(scores)) if len(scores) > 0 else 0.0,\n            'Std': float(np.std(scores)) if len(scores) > 0 else 0.0,\n            'Max': float(np.max(scores)) if len(scores) > 0 else 0.0,\n            'Min': float(np.min(scores)) if len(scores) > 0 else 0.0\n        })\n    import pandas as pd\n    df_stats = pd.DataFrame(stats_data).set_index('Algorithm')\n    df_stats.plot(kind='bar', ax=plt.gca())\n    plt.title('Statistical Summary')\n    plt.ylabel('Value')\n    plt.xticks(rotation=45)\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.grid(True, alpha=0.3, axis='y')\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"DETAILED PERFORMANCE ANALYSIS\")\n    print(\"=\" * 60)\n    for algorithm, scores in results.items():\n        if len(scores) == 0:\n            print(f\"\\n{algorithm}: No data\")\n            continue\n        print(f\"\\n{algorithm}:\")\n        print(f\"  Mean Score: {np.mean(scores):.2f} ± {np.std(scores):.2f}\")\n        print(f\"  Max Score: {np.max(scores):.2f}\")\n        print(f\"  Min Score: {np.min(scores):.2f}\")\n        print(f\"  Final 10 Episodes: {np.mean(scores[-10:]) if len(scores) >= 10 else np.mean(scores):.2f}\")\n        best_avg = max([np.mean(scores[i:i+10]) for i in range(len(scores)-9)] if len(scores) >= 10 else [np.mean(scores)])\n        print(f\"  Best 10-Episode Average: {best_avg:.2f}\")\n        print(f\"  Environment Solved: {'Yes' if best_avg >= 195 else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032ba87c",
   "metadata": {},
   "source": [
    "## Part 5: Exercises and Questions\n",
    "\n",
    "### Exercise 1: Theoretical Understanding\n",
    "\n",
    "**Question 1.1**: Explain the difference between on-policy and off-policy learning. Which algorithms implemented in this notebook are on-policy and which are off-policy?\n",
    "\n",
    "**Answer**: \n",
    "**On-policy vs. Off-policy Learning:**\n",
    "\n",
    "The distinction lies in how data is used to update the policy.\n",
    "\n",
    "-   **On-policy algorithms** update the policy based on actions taken by the *current* version of that same policy. The agent learns from the experience it generates while following its own strategy. It's like learning to cook by trying your own recipes and adjusting them based on how the food tastes. You learn from what you are currently doing.\n",
    "\n",
    "-   **Off-policy algorithms** update the policy using data generated by a *different* policy. The agent can learn from past experiences (e.g., from a replay buffer) or from observing another agent. This separates data collection (exploration) from the learning of the optimal policy (exploitation). It's like learning to cook by watching a master chef's videos; you learn from their experience, not your own.\n",
    "\n",
    "**Algorithms in this Notebook:**\n",
    "\n",
    "-   **DQN (Deep Q-Network)** is **off-policy**. It uses a replay buffer to store past experiences, which may have been generated by older versions of the policy. The learning update samples from this buffer, so the data used for learning is not strictly from the current policy. This improves sample efficiency and stability.\n",
    "\n",
    "-   **REINFORCE** is **on-policy**. It collects a full trajectory of states, actions, and rewards using its current policy. At the end of the episode, it uses this trajectory to update the policy. The data is then discarded, and a new trajectory is collected with the updated policy.\n",
    "\n",
    "-   **Actor-Critic** (as implemented here) is **on-policy**. The actor (policy) generates an action, and the critic evaluates it. The updates are based on this immediate experience. The data is generated and used by the current policy, and then the process repeats.\n",
    "\n",
    "---\n",
    "\n",
    "**Question 1.2**: What is the exploration-exploitation dilemma in reinforcement learning? How do the three algorithms (DQN, REINFORCE, Actor-Critic) handle this dilemma?\n",
    "\n",
    "**Answer**:\n",
    "**The Exploration-Exploitation Dilemma:**\n",
    "\n",
    "This is a fundamental challenge in reinforcement learning. The agent must make a trade-off between:\n",
    "-   **Exploitation**: Taking the action it currently believes is the best to maximize immediate reward. This leverages known information.\n",
    "-   **Exploration**: Taking a different, potentially suboptimal action to gather more information about the environment. This might lead to discovering a better long-term strategy.\n",
    "\n",
    "The dilemma is that excessive exploration can lead to poor performance, while excessive exploitation can cause the agent to get stuck in a suboptimal strategy, never discovering better alternatives.\n",
    "\n",
    "**How the Algorithms Handle It:**\n",
    "\n",
    "-   **DQN**: Uses an **ε-greedy (epsilon-greedy) strategy**. With a probability `ε`, the agent takes a random action (exploration). With probability `1-ε`, it takes the action with the highest estimated Q-value (exploitation). Typically, `ε` starts high (e.g., 1.0) and is gradually decayed to a small value (e.g., 0.01), shifting the agent from exploration to exploitation as it learns more about the environment.\n",
    "\n",
    "-   **REINFORCE**: Handles exploration through its **stochastic policy**. The policy network outputs a probability distribution over all possible actions. Actions are then sampled from this distribution. This means that even actions with lower probabilities have a non-zero chance of being selected, leading to natural exploration. As the policy improves, it will assign higher probabilities to better actions, but the inherent randomness ensures exploration continues.\n",
    "\n",
    "-   **Actor-Critic**: Similar to REINFORCE, the **actor is a stochastic policy**. It outputs probabilities for each action, and actions are sampled accordingly. This inherent stochasticity ensures exploration. The critic's feedback helps refine these probabilities, but the agent will always have a chance to try different actions.\n",
    "\n",
    "---\n",
    "\n",
    "**Question 1.3**: Derive the policy gradient theorem starting from the performance measure $J(\\theta) = \\mathbb{E}_{s \\sim \\rho^\\pi}[V^\\pi(s)]$.\n",
    "\n",
    "**Answer**:\n",
    "The goal is to find the gradient of the performance measure $J(\\theta)$ with respect to the policy parameters $\\theta$. We start with the definition of the state-value function:\n",
    "$V^\\pi(s) = \\mathbb{E}_\\pi[G_t | S_t=s]$\n",
    "\n",
    "The policy gradient theorem states:\n",
    "$$\\nabla_\\theta J(\\theta) \\propto \\sum_s d^\\pi(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) Q^\\pi(s,a)$$\n",
    "\n",
    "Here is a common derivation:\n",
    "1.  Start with the gradient of the state-value function:\n",
    "    $\\nabla_\\theta V^\\pi(s) = \\nabla_\\theta \\sum_a \\pi_\\theta(a|s) Q^\\pi(s,a)$\n",
    "    $= \\sum_a [\\nabla_\\theta \\pi_\\theta(a|s) Q^\\pi(s,a) + \\pi_\\theta(a|s) \\nabla_\\theta Q^\\pi(s,a)]$ (Product Rule)\n",
    "\n",
    "2.  Now expand the gradient of the Q-value function:\n",
    "    $\\nabla_\\theta Q^\\pi(s,a) = \\nabla_\\theta \\sum_{s',r} p(s',r|s,a) [r + \\gamma V^\\pi(s')]$\n",
    "    $= \\gamma \\sum_{s'} p(s'|s) \\nabla_\\theta V^\\pi(s')$\n",
    "\n",
    "3.  Substitute (2) back into (1):\n",
    "    $\\nabla_\\theta V^\\pi(s) = \\sum_a [\\nabla_\\theta \\pi_\\theta(a|s) Q^\\pi(s,a) + \\pi_\\theta(a|s) \\gamma \\sum_{s'} p(s'|s,a) \\nabla_\\theta V^\\pi(s')]$\n",
    "\n",
    "4.  This equation expresses a recursive relationship for the gradient. If we unroll it, we can see how the gradient at a state `s` depends on the gradients of future states. Let's define the discounted state distribution $d^\\pi(s)$.\n",
    "    The performance measure is $J(\\theta) = V^\\pi(s_0)$.\n",
    "    $\\nabla_\\theta J(\\theta) = \\nabla_\\theta V^\\pi(s_0)$\n",
    "\n",
    "5.  Unrolling the recursion from step 3 gives:\n",
    "    $\\nabla_\\theta J(\\theta) = \\sum_{x \\in S} d^\\pi(x) \\sum_a \\nabla_\\theta \\pi_\\theta(a|x) Q^\\pi(x,a)$\n",
    "\n",
    "6.  Now, use the **log-derivative trick**: $\\nabla_\\theta \\pi_\\theta(a|s) = \\pi_\\theta(a|s) \\nabla_\\theta \\log \\pi_\\theta(a|s)$.\n",
    "    Substitute this into the equation:\n",
    "    $\\nabla_\\theta J(\\theta) = \\sum_{s \\in S} d^\\pi(s) \\sum_a \\pi_\\theta(a|s) (\\nabla_\\theta \\log \\pi_\\theta(a|s)) Q^\\pi(s,a)$\n",
    "\n",
    "7.  This can be expressed as an expectation:\n",
    "    $\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta(A_t|S_t) Q^{\\pi_\\theta}(S_t, A_t)]$\n",
    "\n",
    "This final form is the most common expression of the Policy Gradient Theorem. It tells us to increase the probability of actions that lead to higher-than-expected rewards.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 2: Implementation Analysis\n",
    "\n",
    "**Question 2.1**: Compare the memory requirements of DQN vs REINFORCE. Which algorithm requires more memory and why?\n",
    "\n",
    "**Answer**:\n",
    "**DQN requires significantly more memory than REINFORCE.**\n",
    "\n",
    "The primary reason is the **Experience Replay Buffer** in DQN.\n",
    "-   **DQN**: To improve stability and sample efficiency, DQN stores a large number of past transitions (`state`, `action`, `reward`, `next_state`, `done`) in a replay buffer. This buffer can be very large (e.g., holding 10,000 to 1,000,000 experiences). The agent then samples mini-batches from this buffer to perform learning updates. The memory footprint is dominated by this buffer.\n",
    "-   **REINFORCE**: This algorithm is much more memory-efficient. It only needs to store the states, actions, and rewards for the *current episode*. Once the episode is finished, it uses this data to perform a single policy update, and then the data is discarded. The memory required is proportional to the length of one episode, which is typically much smaller than the capacity of a DQN replay buffer.\n",
    "\n",
    "---\n",
    "\n",
    "**Question 2.2**: Explain why we use a target network in DQN. What would happen if we removed it?\n",
    "\n",
    "**Answer**:\n",
    "**Why we use a target network:**\n",
    "\n",
    "The target network is a crucial innovation for stabilizing the learning process in DQN. The Q-learning update involves calculating a target value: $y_t = r_t + \\gamma \\max_{a'} Q(s_{t+1}, a'; \\theta)$.\n",
    "\n",
    "If we use the *same* network for both estimating the current Q-value ($Q(s_t, a_t; \\theta)$) and the target Q-value ($Q(s_{t+1}, a'; \\theta)$), a problem arises. Every time we update the network weights $\\theta$, the target value $y_t$ also changes. This is like trying to hit a moving target. The learning process can become unstable, leading to oscillations or divergence.\n",
    "\n",
    "The **target network** solves this by providing a stable, fixed target for a period of time. It is a separate network whose weights ($\\theta^-$) are a copy of the main Q-network's weights. These weights are held constant for several training steps and are only updated periodically (e.g., by copying the main network's weights every C steps, or through a slow \"soft\" update).\n",
    "\n",
    "**What would happen if we removed it?**\n",
    "\n",
    "Without the target network, the Q-learning target would be constantly shifting. This leads to several problems:\n",
    "1.  **Instability**: The learning process is more likely to be unstable and may diverge. The loss can fluctuate wildly instead of smoothly converging.\n",
    "2.  **Poor Performance**: The agent would have a much harder time learning an effective policy because it is chasing a non-stationary target.\n",
    "3.  **Correlations**: The updates would be highly correlated with the current weights, which can lead to a feedback loop where incorrect Q-value estimates are reinforced.\n",
    "\n",
    "---\n",
    "\n",
    "**Question 2.3**: In the Dueling DQN architecture, why do we subtract the mean of the advantage values? What would happen if we didn't do this?\n",
    "\n",
    "**Answer**:\n",
    "**Why we subtract the mean of the advantage values:**\n",
    "\n",
    "The core idea of Dueling DQN is to separately estimate the state-value function $V(s)$ and the action-advantage function $A(s,a)$. The Q-value is then reconstructed as:\n",
    "$Q(s,a) = V(s) + A(s,a)$\n",
    "\n",
    "However, this formula has an **identifiability problem**. Given a Q-value, we cannot uniquely determine the values of $V(s)$ and $A(s,a)$. For example, we could add a constant `c` to $V(s)$ and subtract it from all $A(s,a)$ values, and the resulting Q-value would be the same. This ambiguity can make training less stable.\n",
    "\n",
    "To solve this, we enforce a constraint on the advantage function. By subtracting the mean of the advantages, we ensure that the average advantage for any state is zero:\n",
    "$$Q(s,a) = V(s) + \\left( A(s,a) - \\frac{1}{|\\mathcal{A}|} \\sum_{a'} A(s,a') \\right)$$\n",
    "This forces $V(s)$ to be a good estimate of the state value, as it becomes the central point around which the advantages fluctuate. It stabilizes learning by ensuring that the advantage of the chosen action is a relative measure compared to the other actions.\n",
    "\n",
    "**What would happen if we didn't do this?**\n",
    "\n",
    "Without subtracting the mean, the network could learn to produce the same Q-values in many different ways. For example, it could:\n",
    "-   Set $V(s)$ to zero and have all the Q-value information in $A(s,a)$.\n",
    "-   Set all $A(s,a)$ to zero and have all the Q-value information in $V(s)$.\n",
    "\n",
    "This ambiguity makes it difficult for the optimizer to know how to attribute the TD-error during backpropagation. The network might learn to change $V(s)$ when it should be changing $A(s,a)$, or vice-versa. This leads to **poorer performance and less stable training**. Subtracting the mean provides a clear separation of concerns, improving learning efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### Exercise 3: Experimental Design\n",
    "\n",
    "Design and implement an experiment to compare the sample efficiency of the three algorithms. Consider:\n",
    "- How would you measure sample efficiency?\n",
    "- What metrics would you use?\n",
    "- How would you ensure a fair comparison?\n",
    "\n",
    "**Answer**:\n",
    "**Experimental Design for Sample Efficiency:**\n",
    "\n",
    "Sample efficiency refers to how much data (i.e., how many interactions with the environment) an agent needs to achieve a certain level of performance. An algorithm is more sample-efficient if it learns faster from fewer interactions.\n",
    "\n",
    "**1. How to Measure Sample Efficiency:**\n",
    "\n",
    "We can measure this by tracking the total number of environment steps (timesteps) taken by the agent. This is a more direct measure of experience than the number of episodes, as episodes can have variable lengths.\n",
    "\n",
    "**2. Metrics to Use:**\n",
    "\n",
    "-   **Timesteps to Threshold**: The primary metric would be the number of total environment interactions (timesteps) required to reach a predefined performance threshold (e.g., an average score of 195 over 100 episodes for CartPole). The algorithm that reaches this threshold in fewer timesteps is more sample-efficient.\n",
    "-   **Area Under the Learning Curve (AUC)**: Plot the average score against the number of timesteps. A higher area under the curve indicates that the agent achieved higher scores earlier, signifying better sample efficiency.\n",
    "-   **Performance after a Fixed Number of Steps**: Compare the average score of each algorithm after a fixed number of timesteps (e.g., after 50,000 steps). The algorithm with the higher score is more sample-efficient up to that point.\n",
    "\n",
    "**3. How to Ensure a Fair Comparison:**\n",
    "\n",
    "To ensure the comparison is fair, we must control for confounding variables:\n",
    "-   **Identical Environments**: All agents must be trained on the exact same environment, initialized with the same random seed for the environment itself.\n",
    "-   **Consistent Hyperparameters**: Use equivalent network architectures (e.g., same number of layers and hidden units) for all agents. Hyperparameters like learning rate and discount factor ($\\gamma$) should be kept consistent or tuned optimally for each algorithm to ensure each is performing at its best.\n",
    "-   **Averaging over Multiple Runs**: RL training can have high variance. To get reliable results, each experiment should be run multiple times (e.g., 5-10 runs) with different random seeds (for agent initialization and action selection). The results (e.g., timesteps to threshold) should then be averaged, and standard deviations should be reported to show the variance.\n",
    "-   **Consistent Evaluation**: Use the same evaluation protocol for all agents, such as measuring the average score over the last 100 episodes.\n",
    "-   **Total Timesteps**: The x-axis of all plots should be the total number of environment steps, not episodes, to account for varying episode lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591fb91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_efficiency_experiment():\n    pass\ndef hyperparameter_sensitivity_analysis():\n    learning_rates = [1e-4, 1e-3, 1e-2]\n    gamma_values = [0.9, 0.95, 0.99]\n    results = {}\n    for lr in learning_rates:\n        for gamma in gamma_values:\n            pass\n    return results\nclass PrioritizedReplayBuffer:\n    def __init__(self, capacity, alpha=0.6):\n        self.capacity = capacity\n        self.alpha = alpha\n        pass\n    def add(self, experience, priority):\n        pass\n    def sample(self, batch_size, beta=0.4):\n        pass\nclass NoisyDQN(nn.Module):\n    def __init__(self, state_size, action_size, hidden_size=64):\n        super(NoisyDQN, self).__init__()\n        pass\n    def forward(self, x):\n        pass\nprint(\"Exercise templates created!\")\nprint(\"TODO: Complete the implementations above\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c24e1e2",
   "metadata": {},
   "source": [
    "## Part 6: Conclusions and Analysis\n",
    "\n",
    "### 6.1 Algorithm Comparison Summary\n",
    "\n",
    "| Algorithm | Type | Memory | Stability | Sample Efficiency | Exploration |\n",
    "|-----------|------|--------|-----------|-------------------|-------------|\n",
    "| **DQN** | Value-based | High (replay buffer) | High (target network) | High | ε-greedy |\n",
    "| **REINFORCE** | Policy-based | Low | Low (high variance) | Low | Stochastic policy |\n",
    "| **Actor-Critic** | Hybrid | Medium | Medium | Medium | Stochastic policy |\n",
    "\n",
    "### 6.2 Key Insights\n",
    "\n",
    "1. **DQN Advantages**:\n",
    "   - Sample efficient due to experience replay\n",
    "   - Stable learning with target networks\n",
    "   - Good for discrete action spaces\n",
    "\n",
    "2. **REINFORCE Advantages**:\n",
    "   - Simple implementation\n",
    "   - Works with continuous actions\n",
    "   - Direct policy optimization\n",
    "\n",
    "3. **Actor-Critic Advantages**:\n",
    "   - Lower variance than REINFORCE\n",
    "   - Online learning capability\n",
    "   - Balances bias-variance tradeoff\n",
    "\n",
    "### 6.3 When to Use Each Algorithm\n",
    "\n",
    "- **Use DQN when**: Discrete actions, sample efficiency is important, you have memory constraints\n",
    "- **Use REINFORCE when**: Simple problems, continuous actions, you need interpretable policies\n",
    "- **Use Actor-Critic when**: You need balance between sample efficiency and stability\n",
    "\n",
    "### 6.4 Advanced Topics for Further Study\n",
    "\n",
    "1. **Advanced DQN Variants**:\n",
    "   - Rainbow DQN (combines multiple improvements)\n",
    "   - Distributional DQN\n",
    "   - Quantile Regression DQN\n",
    "\n",
    "2. **Advanced Policy Methods**:\n",
    "   - Proximal Policy Optimization (PPO)\n",
    "   - Trust Region Policy Optimization (TRPO)\n",
    "   - Soft Actor-Critic (SAC)\n",
    "\n",
    "3. **Model-Based RL**:\n",
    "   - Model-Predictive Control\n",
    "   - Dyna-Q\n",
    "   - Model-based Policy Optimization\n",
    "\n",
    "### 6.5 Further Reading\n",
    "\n",
    "- **Books**:\n",
    "  - \"Reinforcement Learning: An Introduction\" by Sutton & Barto\n",
    "  - \"Deep Reinforcement Learning Hands-On\" by Maxim Lapan\n",
    "\n",
    "- **Papers**:\n",
    "  - DQN: \"Human-level control through deep reinforcement learning\" (Mnih et al., 2015)\n",
    "  - Actor-Critic: \"Actor-Critic Algorithms\" (Konda & Tsitsiklis, 2000)\n",
    "  - Policy Gradients: \"Policy Gradient Methods\" (Sutton et al., 1999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650cff23",
   "metadata": {},
   "source": [
    "## Assignment Submission Requirements\n",
    "\n",
    "### What to Submit:\n",
    "\n",
    "1. **This completed notebook** with:\n",
    "   - All code cells executed\n",
    "   - All theoretical questions answered\n",
    "   - Experimental results and analysis\n",
    "\n",
    "2. **Written Report** (2-3 pages) including:\n",
    "   - Comparison of the three algorithms\n",
    "   - Analysis of experimental results\n",
    "   - Discussion of hyperparameter sensitivity\n",
    "   - Recommendations for different scenarios\n",
    "\n",
    "3. **Code Implementation** of at least one advanced feature:\n",
    "   - Prioritized Experience Replay\n",
    "   - Dueling DQN improvements\n",
    "   - Custom environment implementation\n",
    "   - Hyperparameter optimization\n",
    "\n",
    "### Evaluation Criteria:\n",
    "\n",
    "- **Theoretical Understanding (30%)**: Correct answers to theoretical questions\n",
    "- **Implementation Quality (40%)**: Working code, proper documentation, clean structure\n",
    "- **Experimental Analysis (20%)**: Thorough analysis of results, meaningful comparisons\n",
    "- **Innovation/Extensions (10%)**: Creative improvements or additional implementations\n",
    "\n",
    "### Submission Deadline: [Insert Date]\n",
    "\n",
    "### Additional Notes:\n",
    "\n",
    "- Ensure all code runs without errors\n",
    "- Include clear comments and documentation\n",
    "- Use proper citation for any external sources\n",
    "- Submit both .ipynb and .pdf versions of the notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Good luck with your Deep Reinforcement Learning journey!** 🚀\n",
    "\n",
    "Remember: The key to mastering DRL is understanding the trade-offs between different algorithms and knowing when to apply each one. Practice implementing these algorithms on different environments to build intuition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}