{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7d2a7b",
   "metadata": {},
   "source": [
    "# CA6: Policy Gradient Methods - Complete Implementation and Analysis\n",
    "\n",
    "## Deep Reinforcement Learning - Session 6\n",
    "**Author**: Deep RL Course  \n",
    "**Date**: 2024  \n",
    "**Topic**: From Value-Based to Policy-Based Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Course Overview\n",
    "\n",
    "Welcome to the comprehensive study of **Policy Gradient Methods** in Deep Reinforcement Learning. This session marks a fundamental shift from the value-based methods we explored in previous sessions (DQN, Double DQN, Dueling DQN) to policy-based approaches that directly optimize the policy itself.\n",
    "\n",
    "### Key Learning Objectives\n",
    "\n",
    "By completing this comprehensive exercise, you will master:\n",
    "\n",
    "1. **Theoretical Foundations**: Deep understanding of policy gradient theorem and mathematical derivations\n",
    "2. **REINFORCE Algorithm**: Complete implementation and analysis of Monte Carlo policy gradients\n",
    "3. **Actor-Critic Methods**: Advanced architectures combining policy and value learning\n",
    "4. **A2C/A3C Implementation**: State-of-the-art policy gradient algorithms with parallelization\n",
    "5. **Variance Reduction**: Sophisticated techniques to stabilize policy gradient learning\n",
    "6. **Continuous Control**: Extension to continuous action spaces and control problems\n",
    "7. **Performance Analysis**: Comprehensive evaluation and comparison methodologies\n",
    "\n",
    "### Session Structure\n",
    "\n",
    "- **Section 1**: Theoretical Foundations of Policy Gradient Methods\n",
    "- **Section 2**: REINFORCE Algorithm Implementation and Analysis  \n",
    "- **Section 3**: Actor-Critic Methods with Baseline\n",
    "- **Section 4**: Advanced A2C/A3C Implementation\n",
    "- **Section 5**: Variance Reduction Techniques\n",
    "- **Section 6**: Continuous Action Space Policy Gradients\n",
    "- **Section 7**: Performance Analysis and Comparisons\n",
    "- **Section 8**: Practical Applications and Case Studies\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites and Environment Setup\n",
    "\n",
    "Before diving into policy gradient methods, let's establish our computational environment and theoretical foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Imports and Environment Setup\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical, Normal\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "import time\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Plotting configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Gymnasium version: {gym.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd8448",
   "metadata": {},
   "source": [
    "# Section 1: Theoretical Foundations of Policy Gradient Methods\n",
    "\n",
    "## 1.1 From Value-Based to Policy-Based Learning\n",
    "\n",
    "In our journey through reinforcement learning, we have primarily focused on **value-based methods** such as Q-learning and DQN. These methods learn a value function and derive a policy from it. However, there are fundamental limitations to this approach:\n",
    "\n",
    "### Limitations of Value-Based Methods\n",
    "\n",
    "1. **Discrete Action Spaces**: Q-learning naturally handles discrete actions but struggles with continuous action spaces\n",
    "2. **Deterministic Policies**: Value-based methods typically produce deterministic policies (with exploration strategies)\n",
    "3. **Action Space Explosion**: As action space grows, storing Q-values becomes intractable\n",
    "4. **Approximation Errors**: Bootstrapping can lead to error propagation and instability\n",
    "\n",
    "### The Policy Gradient Paradigm\n",
    "\n",
    "Policy gradient methods take a fundamentally different approach:\n",
    "\n",
    "- **Direct Policy Parameterization**: We parameterize the policy π_θ(a|s) directly with parameters θ\n",
    "- **Optimization Objective**: We optimize the expected return J(θ) = E_τ∼π_θ[R(τ)]\n",
    "- **Gradient Ascent**: We update parameters using ∇_θ J(θ)\n",
    "\n",
    "## 1.2 Mathematical Foundations\n",
    "\n",
    "### Policy Parameterization\n",
    "\n",
    "For discrete actions, we typically use a softmax parameterization:\n",
    "\n",
    "π_θ(a|s) = exp(f_θ(s,a)) / Σ_a' exp(f_θ(s,a'))\n",
    "\n",
    "For continuous actions, we often use Gaussian policies:\n",
    "\n",
    "π_θ(a|s) = N(μ_θ(s), σ_θ(s))\n",
    "\n",
    "### The Objective Function\n",
    "\n",
    "The performance measure for a policy π_θ is the expected return:\n",
    "\n",
    "J(θ) = E_s₀∼ρ₀ E_τ∼π_θ [R(τ)]\n",
    "\n",
    "Where:\n",
    "- ρ₀ is the initial state distribution\n",
    "- τ = (s₀, a₀, r₁, s₁, a₁, ...) is a trajectory\n",
    "- R(τ) = Σᵢ γⁱ rᵢ is the discounted return\n",
    "\n",
    "## 1.3 The Policy Gradient Theorem\n",
    "\n",
    "The cornerstone of policy gradient methods is the **Policy Gradient Theorem**, which provides an analytical expression for ∇_θ J(θ).\n",
    "\n",
    "### Theorem Statement\n",
    "\n",
    "For any differentiable policy π_θ and any performance measure J(θ):\n",
    "\n",
    "∇_θ J(θ) = E_s∼d^π_θ E_a∼π_θ [Q^π_θ(s,a) ∇_θ ln π_θ(a|s)]\n",
    "\n",
    "Where d^π_θ(s) is the stationary distribution of states under policy π_θ.\n",
    "\n",
    "### Monte Carlo Formulation\n",
    "\n",
    "In the episodic case, this becomes:\n",
    "\n",
    "∇_θ J(θ) = E_τ∼π_θ [Σₜ G_t ∇_θ ln π_θ(aₜ|sₜ)]\n",
    "\n",
    "Where G_t = Σᵢ₌ₜ^T γⁱ⁻ᵗ rᵢ is the return from time t.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Model-Free**: No need to know transition probabilities P(s'|s,a)\n",
    "2. **Unbiased**: The gradient estimate is unbiased\n",
    "3. **High Variance**: Monte Carlo estimates can have high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical Demonstration: Policy Gradient Theorem Proof and Visualization\n",
    "\n",
    "class PolicyGradientVisualization:\n",
    "    \"\"\"\n",
    "    A class to visualize key concepts in policy gradient methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.fig_count = 0\n",
    "    \n",
    "    def visualize_policy_space(self):\n",
    "        \"\"\"Visualize how policy parameters affect action probabilities\"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Simple 2-action case\n",
    "        theta_values = np.linspace(-3, 3, 100)\n",
    "        \n",
    "        for i, temp in enumerate([0.1, 0.5, 1.0, 2.0]):\n",
    "            ax = axes[i//2, i%2]\n",
    "            \n",
    "            # Softmax probabilities for action 0\n",
    "            prob_action_0 = 1 / (1 + np.exp(-theta_values/temp))\n",
    "            prob_action_1 = 1 - prob_action_0\n",
    "            \n",
    "            ax.plot(theta_values, prob_action_0, label='P(a=0|s)', linewidth=2)\n",
    "            ax.plot(theta_values, prob_action_1, label='P(a=1|s)', linewidth=2)\n",
    "            ax.set_title(f'Policy Probabilities (Temperature={temp})')\n",
    "            ax.set_xlabel('Policy Parameter θ')\n",
    "            ax.set_ylabel('Probability')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Effect of Policy Parameters on Action Probabilities', \n",
    "                     fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_gradient_direction(self):\n",
    "        \"\"\"Visualize policy gradient direction\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Create a simple 2D policy parameter space\n",
    "        theta1 = np.linspace(-2, 2, 20)\n",
    "        theta2 = np.linspace(-2, 2, 20)\n",
    "        T1, T2 = np.meshgrid(theta1, theta2)\n",
    "        \n",
    "        # Simulated objective function (for visualization)\n",
    "        J = np.exp(-(T1**2 + T2**2)/2) + 0.5*np.exp(-((T1-1)**2 + (T2+0.5)**2)/0.5)\n",
    "        \n",
    "        # Gradient computation\n",
    "        grad_T1, grad_T2 = np.gradient(J)\n",
    "        \n",
    "        # Plot objective function\n",
    "        contour = ax1.contour(T1, T2, J, levels=15)\n",
    "        ax1.clabel(contour, inline=True, fontsize=8)\n",
    "        ax1.quiver(T1[::2,::2], T2[::2,::2], \n",
    "                   grad_T1[::2,::2], grad_T2[::2,::2], \n",
    "                   alpha=0.7, color='red')\n",
    "        ax1.set_title('Policy Gradient Directions')\n",
    "        ax1.set_xlabel('θ₁')\n",
    "        ax1.set_ylabel('θ₂')\n",
    "        \n",
    "        # Show convergence path\n",
    "        path_theta1 = [-1.5, -1.2, -0.8, -0.3, 0.2, 0.7, 0.95]\n",
    "        path_theta2 = [1.0, 0.7, 0.3, -0.1, -0.3, -0.4, -0.5]\n",
    "        \n",
    "        ax2.contour(T1, T2, J, levels=15, alpha=0.5)\n",
    "        ax2.plot(path_theta1, path_theta2, 'bo-', linewidth=2, markersize=8, label='Gradient Ascent Path')\n",
    "        ax2.plot(path_theta1[0], path_theta2[0], 'go', markersize=10, label='Start')\n",
    "        ax2.plot(path_theta1[-1], path_theta2[-1], 'ro', markersize=10, label='Converged')\n",
    "        ax2.set_title('Policy Gradient Convergence')\n",
    "        ax2.set_xlabel('θ₁')\n",
    "        ax2.set_ylabel('θ₂')\n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def compare_value_vs_policy_based(self):\n",
    "        \"\"\"Compare value-based vs policy-based approaches\"\"\"\n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        \n",
    "        # Simulated data for comparison\n",
    "        states = np.arange(1, 11)\n",
    "        \n",
    "        # Value-based approach\n",
    "        q_values_a1 = [2.1, 3.2, 1.8, 4.5, 2.7, 3.9, 1.2, 4.8, 3.1, 2.9]\n",
    "        q_values_a2 = [1.9, 2.8, 2.1, 3.2, 3.1, 2.7, 2.8, 3.5, 2.9, 3.2]\n",
    "        \n",
    "        ax1.bar(states - 0.2, q_values_a1, 0.4, label='Q(s,a₁)', alpha=0.7)\n",
    "        ax1.bar(states + 0.2, q_values_a2, 0.4, label='Q(s,a₂)', alpha=0.7)\n",
    "        ax1.set_title('Value-Based: Q-Values')\n",
    "        ax1.set_xlabel('State')\n",
    "        ax1.set_ylabel('Q-Value')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Derived deterministic policy\n",
    "        policy_deterministic = [1 if q1 > q2 else 2 for q1, q2 in zip(q_values_a1, q_values_a2)]\n",
    "        colors = ['blue' if a == 1 else 'orange' for a in policy_deterministic]\n",
    "        ax2.bar(states, [1]*len(states), color=colors, alpha=0.7)\n",
    "        ax2.set_title('Derived Deterministic Policy')\n",
    "        ax2.set_xlabel('State')\n",
    "        ax2.set_ylabel('Selected Action')\n",
    "        ax2.set_yticks([1, 2])\n",
    "        ax2.set_yticklabels(['Action 1', 'Action 2'])\n",
    "        \n",
    "        # Policy-based approach\n",
    "        prob_a1 = [0.7, 0.8, 0.4, 0.9, 0.5, 0.8, 0.3, 0.9, 0.7, 0.6]\n",
    "        prob_a2 = [1-p for p in prob_a1]\n",
    "        \n",
    "        ax3.bar(states - 0.2, prob_a1, 0.4, label='π(a₁|s)', alpha=0.7)\n",
    "        ax3.bar(states + 0.2, prob_a2, 0.4, label='π(a₂|s)', alpha=0.7)\n",
    "        ax3.set_title('Policy-Based: Stochastic Policy')\n",
    "        ax3.set_xlabel('State')\n",
    "        ax3.set_ylabel('Probability')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Action selection distribution\n",
    "        action_probs = np.array([prob_a1, prob_a2]).T\n",
    "        bottom = np.zeros(len(states))\n",
    "        \n",
    "        colors = ['blue', 'orange']\n",
    "        labels = ['Action 1', 'Action 2']\n",
    "        \n",
    "        for i in range(2):\n",
    "            ax4.bar(states, action_probs[:, i], bottom=bottom, \n",
    "                   color=colors[i], alpha=0.7, label=labels[i])\n",
    "            bottom += action_probs[:, i]\n",
    "        \n",
    "        ax4.set_title('Stochastic Action Selection')\n",
    "        ax4.set_xlabel('State')\n",
    "        ax4.set_ylabel('Probability')\n",
    "        ax4.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Value-Based vs Policy-Based Methods Comparison', \n",
    "                     fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "\n",
    "# Create visualization instance and run demonstrations\n",
    "viz = PolicyGradientVisualization()\n",
    "\n",
    "print(\"=== Policy Gradient Theoretical Foundations ===\")\n",
    "print(\"\\n1. Visualizing Policy Parameter Effects:\")\n",
    "viz.visualize_policy_space()\n",
    "\n",
    "print(\"\\n2. Policy Gradient Directions and Convergence:\")\n",
    "viz.visualize_gradient_direction()\n",
    "\n",
    "print(\"\\n3. Value-Based vs Policy-Based Comparison:\")\n",
    "viz.compare_value_vs_policy_based()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ceed2",
   "metadata": {},
   "source": [
    "# Section 2: REINFORCE Algorithm Implementation and Analysis\n",
    "\n",
    "## 2.1 The REINFORCE Algorithm (Monte Carlo Policy Gradient)\n",
    "\n",
    "REINFORCE, proposed by Williams (1992), is the simplest policy gradient algorithm. It directly implements the policy gradient theorem using Monte Carlo estimates of the return.\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "**REINFORCE Algorithm:**\n",
    "\n",
    "1. Initialize policy parameters θ randomly\n",
    "2. For each episode:\n",
    "   a. Generate episode trajectory τ = (s₀,a₀,r₁,s₁,a₁,r₂,...,sₜ,aₜ,rₜ₊₁) using π_θ\n",
    "   b. For each time step t in the episode:\n",
    "      - Calculate return G_t = Σᵢ₌ₜ^T γⁱ⁻ᵗ rᵢ\n",
    "      - Update θ ← θ + α G_t ∇_θ ln π_θ(aₜ|sₜ)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The REINFORCE update is:\n",
    "\n",
    "**θ ← θ + α Σₜ G_t ∇_θ ln π_θ(aₜ|sₜ)**\n",
    "\n",
    "Where:\n",
    "- **G_t**: Return from time step t\n",
    "- **∇_θ ln π_θ(aₜ|sₜ)**: Score function (gradient of log-probability)\n",
    "- **α**: Learning rate\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Unbiased**: E[∇_θ J(θ)] is the true policy gradient\n",
    "- **High Variance**: Monte Carlo estimates can be very noisy\n",
    "- **Sample Inefficient**: Requires complete episodes for updates\n",
    "- **On-Policy**: Uses trajectories generated by current policy\n",
    "\n",
    "## 2.2 Understanding the Variance Problem\n",
    "\n",
    "The main challenge with REINFORCE is the high variance of gradient estimates. Let's analyze why this occurs and its impact on learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete REINFORCE Implementation with Variance Analysis\n",
    "\n",
    "class REINFORCEAgent:\n",
    "    \"\"\"\n",
    "    Complete REINFORCE (Monte Carlo Policy Gradient) implementation\n",
    "    with detailed logging and analysis capabilities\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=1e-3, gamma=0.99):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        \n",
    "        # Logging\n",
    "        self.episode_rewards = []\n",
    "        self.policy_losses = []\n",
    "        self.gradient_norms = []\n",
    "        self.entropy_history = []\n",
    "        \n",
    "    def select_action(self, state, return_log_prob=False):\n",
    "        \"\"\"Select action using current policy\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.policy_net(state)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        \n",
    "        if return_log_prob:\n",
    "            log_prob = dist.log_prob(action)\n",
    "            return action.item(), log_prob.item()\n",
    "        \n",
    "        return action.item()\n",
    "    \n",
    "    def get_policy_distribution(self, state):\n",
    "        \"\"\"Get full policy distribution for analysis\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.policy_net(state)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "        return probs.cpu().numpy().flatten()\n",
    "    \n",
    "    def compute_returns(self, rewards):\n",
    "        \"\"\"Compute discounted returns (G_t values)\"\"\"\n",
    "        returns = []\n",
    "        G = 0\n",
    "        \n",
    "        # Compute returns backwards\n",
    "        for r in reversed(rewards):\n",
    "            G = r + self.gamma * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def update_policy(self, states, actions, returns):\n",
    "        \"\"\"Update policy using REINFORCE algorithm\"\"\"\n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Normalize returns for stability\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = self.policy_net(states)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions)\n",
    "        \n",
    "        # REINFORCE loss: -E[G_t * log π(a_t|s_t)]\n",
    "        policy_loss = -(log_probs * returns).mean()\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        \n",
    "        # Record gradient norm\n",
    "        total_norm = 0\n",
    "        for p in self.policy_net.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "        self.gradient_norms.append(total_norm)\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Log entropy for exploration analysis\n",
    "        entropy = dist.entropy().mean()\n",
    "        self.entropy_history.append(entropy.item())\n",
    "        self.policy_losses.append(policy_loss.item())\n",
    "        \n",
    "        return policy_loss.item()\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Train on single episode\"\"\"\n",
    "        state, _ = env.reset()\n",
    "        states, actions, rewards = [], [], []\n",
    "        episode_reward = 0\n",
    "        \n",
    "        while True:\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if terminated or truncated:\n",
    "                break\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = self.compute_returns(rewards)\n",
    "        \n",
    "        # Update policy\n",
    "        loss = self.update_policy(states, actions, returns)\n",
    "        \n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        \n",
    "        return episode_reward, loss\n",
    "    \n",
    "    def analyze_variance(self, env, num_episodes=100):\n",
    "        \"\"\"Analyze gradient variance in REINFORCE\"\"\"\n",
    "        gradient_estimates = []\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state, _ = env.reset()\n",
    "            states, actions, rewards = [], [], []\n",
    "            \n",
    "            # Collect episode\n",
    "            while True:\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                \n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            \n",
    "            # Compute gradient estimate for this episode\n",
    "            states_tensor = torch.FloatTensor(states).to(device)\n",
    "            actions_tensor = torch.LongTensor(actions).to(device)\n",
    "            returns = torch.FloatTensor(self.compute_returns(rewards)).to(device)\n",
    "            \n",
    "            # Compute gradients\n",
    "            logits = self.policy_net(states_tensor)\n",
    "            dist = Categorical(logits=logits)\n",
    "            log_probs = dist.log_prob(actions_tensor)\n",
    "            \n",
    "            # Individual gradient contributions\n",
    "            grad_contributions = (log_probs * returns).detach().cpu().numpy()\n",
    "            gradient_estimates.extend(grad_contributions)\n",
    "        \n",
    "        return np.array(gradient_estimates)\n",
    "\n",
    "# Test REINFORCE on CartPole\n",
    "def test_reinforce():\n",
    "    \"\"\"Test REINFORCE implementation\"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = REINFORCEAgent(state_dim, action_dim, lr=1e-3, gamma=0.99)\n",
    "    \n",
    "    print(\"=== REINFORCE Training ===\")\n",
    "    \n",
    "    # Training loop\n",
    "    num_episodes = 300\n",
    "    log_interval = 50\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        episode_reward, loss = agent.train_episode(env)\n",
    "        \n",
    "        if (episode + 1) % log_interval == 0:\n",
    "            avg_reward = np.mean(agent.episode_rewards[-log_interval:])\n",
    "            avg_loss = np.mean(agent.policy_losses[-log_interval:])\n",
    "            avg_entropy = np.mean(agent.entropy_history[-log_interval:])\n",
    "            \n",
    "            print(f\"Episode {episode+1:4d} | Avg Reward: {avg_reward:7.2f} | \"\n",
    "                  f\"Loss: {avg_loss:.4f} | Entropy: {avg_entropy:.4f}\")\n",
    "    \n",
    "    # Variance analysis\n",
    "    print(\"\\n=== Variance Analysis ===\")\n",
    "    gradient_estimates = agent.analyze_variance(env, num_episodes=50)\n",
    "    \n",
    "    print(f\"Gradient estimate statistics:\")\n",
    "    print(f\"Mean: {np.mean(gradient_estimates):.4f}\")\n",
    "    print(f\"Std:  {np.std(gradient_estimates):.4f}\")\n",
    "    print(f\"Variance: {np.var(gradient_estimates):.4f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Learning curve\n",
    "    axes[0,0].plot(agent.episode_rewards)\n",
    "    axes[0,0].plot(pd.Series(agent.episode_rewards).rolling(window=20).mean(), \n",
    "                   color='red', label='Moving Average')\n",
    "    axes[0,0].set_title('REINFORCE Learning Curve')\n",
    "    axes[0,0].set_xlabel('Episode')\n",
    "    axes[0,0].set_ylabel('Episode Reward')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Policy loss\n",
    "    axes[0,1].plot(agent.policy_losses)\n",
    "    axes[0,1].plot(pd.Series(agent.policy_losses).rolling(window=20).mean(), \n",
    "                   color='red', label='Moving Average')\n",
    "    axes[0,1].set_title('Policy Loss Over Time')\n",
    "    axes[0,1].set_xlabel('Episode')\n",
    "    axes[0,1].set_ylabel('Policy Loss')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient norms\n",
    "    axes[1,0].plot(agent.gradient_norms)\n",
    "    axes[1,0].set_title('Gradient Norms')\n",
    "    axes[1,0].set_xlabel('Episode')\n",
    "    axes[1,0].set_ylabel('Gradient L2 Norm')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient variance distribution\n",
    "    axes[1,1].hist(gradient_estimates, bins=30, alpha=0.7, density=True)\n",
    "    axes[1,1].axvline(np.mean(gradient_estimates), color='red', \n",
    "                      linestyle='--', label=f'Mean: {np.mean(gradient_estimates):.3f}')\n",
    "    axes[1,1].set_title('Distribution of Gradient Estimates')\n",
    "    axes[1,1].set_xlabel('Gradient Value')\n",
    "    axes[1,1].set_ylabel('Density')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    env.close()\n",
    "    return agent\n",
    "\n",
    "# Run REINFORCE test\n",
    "reinforce_agent = test_reinforce()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd8468",
   "metadata": {},
   "source": [
    "# Section 3: Actor-Critic Methods with Baseline\n",
    "\n",
    "## 3.1 Addressing the Variance Problem\n",
    "\n",
    "The high variance in REINFORCE stems from using the full return G_t as the signal for policy updates. We can reduce variance by subtracting a **baseline** b(s_t) from the return:\n",
    "\n",
    "**∇_θ J(θ) = E_τ∼π_θ [Σₜ (G_t - b(s_t)) ∇_θ ln π_θ(aₜ|sₜ)]**\n",
    "\n",
    "The baseline doesn't change the expectation of the gradient (unbiased) but can significantly reduce variance.\n",
    "\n",
    "### Choosing the Baseline: Value Function\n",
    "\n",
    "The optimal baseline is the **state-value function** V^π(s):\n",
    "\n",
    "**b(s_t) = V^π(s_t)**\n",
    "\n",
    "This leads to the **advantage function**:\n",
    "**A^π(s_t, a_t) = G_t - V^π(s_t)**\n",
    "\n",
    "## 3.2 Actor-Critic Architecture\n",
    "\n",
    "Actor-Critic methods combine:\n",
    "\n",
    "1. **Actor**: Policy π_θ(a|s) that selects actions\n",
    "2. **Critic**: Value function V_φ(s) that evaluates states\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "**Actor-Critic Algorithm:**\n",
    "\n",
    "1. Initialize actor parameters θ and critic parameters φ\n",
    "2. For each time step:\n",
    "   a. Select action a_t ∼ π_θ(·|s_t)\n",
    "   b. Observe reward r_t and next state s_{t+1}\n",
    "   c. Compute TD error: δ_t = r_t + γV_φ(s_{t+1}) - V_φ(s_t)\n",
    "   d. Update critic: φ ← φ + α_c δ_t ∇_φ V_φ(s_t)\n",
    "   e. Update actor: θ ← θ + α_a δ_t ∇_θ ln π_θ(a_t|s_t)\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "1. **Lower Variance**: Using value function baseline reduces gradient variance\n",
    "2. **Online Learning**: Can update after each step (no need to wait for episode end)\n",
    "3. **Faster Learning**: More frequent updates lead to faster convergence\n",
    "4. **Bootstrapping**: Uses learned value estimates rather than full returns\n",
    "\n",
    "## 3.3 Temporal Difference vs Monte Carlo\n",
    "\n",
    "Actor-Critic can use different targets for the advantage estimation:\n",
    "\n",
    "- **Monte Carlo**: A(s_t,a_t) = G_t - V(s_t)\n",
    "- **TD(0)**: A(s_t,a_t) = r_t + γV(s_{t+1}) - V(s_t)\n",
    "- **TD(λ)**: A(s_t,a_t) = G_t^λ - V(s_t)\n",
    "\n",
    "Each provides different bias-variance tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Actor-Critic Implementation with Baseline Analysis\n",
    "\n",
    "class ActorCriticAgent:\n",
    "    \"\"\"\n",
    "    Actor-Critic agent with comprehensive baseline analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, \n",
    "                 actor_lr=1e-3, critic_lr=5e-3, gamma=0.99):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Actor network (policy)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        ).to(device)\n",
    "        \n",
    "        # Critic network (value function)\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        # Logging\n",
    "        self.episode_rewards = []\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.td_errors = []\n",
    "        self.advantages = []\n",
    "        self.value_estimates = []\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using actor network\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.actor(state)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        \"\"\"Get value estimate from critic\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            value = self.critic(state)\n",
    "            \n",
    "        return value.item()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done, log_prob):\n",
    "        \"\"\"Update actor and critic networks\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Critic update\n",
    "        current_value = self.critic(state)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                next_value = self.critic(next_state)\n",
    "            target_value = reward + self.gamma * next_value\n",
    "        \n",
    "        # TD error (also used as advantage)\n",
    "        td_error = target_value - current_value\n",
    "        advantage = td_error.detach()\n",
    "        \n",
    "        # Critic loss (MSE)\n",
    "        critic_loss = td_error.pow(2)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor loss\n",
    "        actor_loss = -log_prob * advantage\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        self.critic_losses.append(critic_loss.item())\n",
    "        self.td_errors.append(td_error.item())\n",
    "        self.advantages.append(advantage.item())\n",
    "        self.value_estimates.append(current_value.item())\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item(), td_error.item()\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Train on single episode\"\"\"\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update networks\n",
    "            actor_loss, critic_loss, td_error = self.update(\n",
    "                state, action, reward, next_state, done, log_prob)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        return episode_reward, step_count\n",
    "\n",
    "class BaselineComparison:\n",
    "    \"\"\"\n",
    "    Compare different baseline strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def no_baseline_reinforce(self, env, num_episodes=200):\n",
    "        \"\"\"REINFORCE without baseline\"\"\"\n",
    "        agent = REINFORCEAgent(self.state_dim, self.action_dim, lr=1e-3)\n",
    "        rewards = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            reward, _ = agent.train_episode(env)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "        return rewards, agent.gradient_norms\n",
    "    \n",
    "    def constant_baseline_reinforce(self, env, baseline_value=100, num_episodes=200):\n",
    "        \"\"\"REINFORCE with constant baseline\"\"\"\n",
    "        class ConstantBaselineREINFORCE(REINFORCEAgent):\n",
    "            def __init__(self, *args, baseline=0, **kwargs):\n",
    "                super().__init__(*args, **kwargs)\n",
    "                self.baseline = baseline\n",
    "                \n",
    "            def update_policy(self, states, actions, returns):\n",
    "                states = torch.FloatTensor(states).to(device)\n",
    "                actions = torch.LongTensor(actions).to(device)\n",
    "                returns = torch.FloatTensor(returns).to(device)\n",
    "                \n",
    "                # Subtract constant baseline\n",
    "                advantages = returns - self.baseline\n",
    "                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "                \n",
    "                logits = self.policy_net(states)\n",
    "                dist = Categorical(logits=logits)\n",
    "                log_probs = dist.log_prob(actions)\n",
    "                \n",
    "                policy_loss = -(log_probs * advantages).mean()\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                \n",
    "                total_norm = 0\n",
    "                for p in self.policy_net.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        param_norm = p.grad.data.norm(2)\n",
    "                        total_norm += param_norm.item() ** 2\n",
    "                total_norm = total_norm ** (1. / 2)\n",
    "                self.gradient_norms.append(total_norm)\n",
    "                \n",
    "                torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                entropy = dist.entropy().mean()\n",
    "                self.entropy_history.append(entropy.item())\n",
    "                self.policy_losses.append(policy_loss.item())\n",
    "                \n",
    "                return policy_loss.item()\n",
    "        \n",
    "        agent = ConstantBaselineREINFORCE(self.state_dim, self.action_dim, \n",
    "                                        baseline=baseline_value, lr=1e-3)\n",
    "        rewards = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            reward, _ = agent.train_episode(env)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "        return rewards, agent.gradient_norms\n",
    "    \n",
    "    def actor_critic_baseline(self, env, num_episodes=200):\n",
    "        \"\"\"Actor-Critic with learned baseline\"\"\"\n",
    "        agent = ActorCriticAgent(self.state_dim, self.action_dim)\n",
    "        rewards = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            reward, _ = agent.train_episode(env)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "        return rewards, agent.advantages\n",
    "\n",
    "def test_baseline_comparison():\n",
    "    \"\"\"Test different baseline strategies\"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    comparison = BaselineComparison(state_dim, action_dim)\n",
    "    \n",
    "    print(\"=== Baseline Comparison Analysis ===\")\n",
    "    \n",
    "    # Test different approaches\n",
    "    print(\"1. Training REINFORCE without baseline...\")\n",
    "    no_baseline_rewards, no_baseline_grads = comparison.no_baseline_reinforce(env)\n",
    "    \n",
    "    print(\"2. Training REINFORCE with constant baseline...\")\n",
    "    constant_baseline_rewards, constant_baseline_grads = comparison.constant_baseline_reinforce(env, baseline_value=100)\n",
    "    \n",
    "    print(\"3. Training Actor-Critic with learned baseline...\")\n",
    "    ac_rewards, ac_advantages = comparison.actor_critic_baseline(env)\n",
    "    \n",
    "    # Analysis and visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Learning curves\n",
    "    window = 20\n",
    "    \n",
    "    axes[0,0].plot(pd.Series(no_baseline_rewards).rolling(window).mean(), \n",
    "                   label='No Baseline', linewidth=2)\n",
    "    axes[0,0].plot(pd.Series(constant_baseline_rewards).rolling(window).mean(), \n",
    "                   label='Constant Baseline', linewidth=2)\n",
    "    axes[0,0].plot(pd.Series(ac_rewards).rolling(window).mean(), \n",
    "                   label='Actor-Critic', linewidth=2)\n",
    "    axes[0,0].set_title('Learning Curves Comparison')\n",
    "    axes[0,0].set_xlabel('Episode')\n",
    "    axes[0,0].set_ylabel('Average Reward')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Gradient variance comparison\n",
    "    axes[0,1].plot(no_baseline_grads, alpha=0.7, label='No Baseline')\n",
    "    axes[0,1].plot(constant_baseline_grads, alpha=0.7, label='Constant Baseline')\n",
    "    axes[0,1].set_title('Gradient Norms Comparison')\n",
    "    axes[0,1].set_xlabel('Episode')\n",
    "    axes[0,1].set_ylabel('Gradient L2 Norm')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final performance comparison\n",
    "    final_performances = [\n",
    "        np.mean(no_baseline_rewards[-50:]),\n",
    "        np.mean(constant_baseline_rewards[-50:]),\n",
    "        np.mean(ac_rewards[-50:])\n",
    "    ]\n",
    "    \n",
    "    methods = ['No Baseline', 'Constant Baseline', 'Actor-Critic']\n",
    "    colors = ['red', 'orange', 'green']\n",
    "    \n",
    "    bars = axes[0,2].bar(methods, final_performances, color=colors, alpha=0.7)\n",
    "    axes[0,2].set_title('Final Performance (Last 50 Episodes)')\n",
    "    axes[0,2].set_ylabel('Average Reward')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, final_performances):\n",
    "        axes[0,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                       f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Variance analysis\n",
    "    axes[1,0].hist(no_baseline_grads, bins=30, alpha=0.5, label='No Baseline', density=True)\n",
    "    axes[1,0].hist(constant_baseline_grads, bins=30, alpha=0.5, label='Constant Baseline', density=True)\n",
    "    axes[1,0].set_title('Gradient Norm Distributions')\n",
    "    axes[1,0].set_xlabel('Gradient Norm')\n",
    "    axes[1,0].set_ylabel('Density')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Advantage evolution (Actor-Critic)\n",
    "    if len(ac_advantages) > 0:\n",
    "        axes[1,1].plot(ac_advantages[:1000])  # First 1000 steps\n",
    "        axes[1,1].set_title('Advantage Evolution (Actor-Critic)')\n",
    "        axes[1,1].set_xlabel('Step')\n",
    "        axes[1,1].set_ylabel('Advantage')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Statistical comparison\n",
    "    variance_stats = {\n",
    "        'No Baseline': np.var(no_baseline_grads),\n",
    "        'Constant Baseline': np.var(constant_baseline_grads),\n",
    "        'AC Advantages': np.var(ac_advantages) if len(ac_advantages) > 0 else 0\n",
    "    }\n",
    "    \n",
    "    methods_var = list(variance_stats.keys())\n",
    "    variances = list(variance_stats.values())\n",
    "    \n",
    "    axes[1,2].bar(methods_var, variances, color=['red', 'orange', 'green'], alpha=0.7)\n",
    "    axes[1,2].set_title('Variance Comparison')\n",
    "    axes[1,2].set_ylabel('Variance')\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print numerical results\n",
    "    print(\"\\n=== Numerical Results ===\")\n",
    "    print(f\"Final Performance (last 50 episodes):\")\n",
    "    print(f\"  No Baseline:       {final_performances[0]:.2f}\")\n",
    "    print(f\"  Constant Baseline: {final_performances[1]:.2f}\")\n",
    "    print(f\"  Actor-Critic:      {final_performances[2]:.2f}\")\n",
    "    \n",
    "    print(f\"\\nGradient Variance:\")\n",
    "    print(f\"  No Baseline:       {variance_stats['No Baseline']:.4f}\")\n",
    "    print(f\"  Constant Baseline: {variance_stats['Constant Baseline']:.4f}\")\n",
    "    print(f\"  AC Advantages:     {variance_stats['AC Advantages']:.4f}\")\n",
    "    \n",
    "    env.close()\n",
    "\n",
    "# Run baseline comparison\n",
    "test_baseline_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79476e",
   "metadata": {},
   "source": [
    "# Section 4: Advanced A2C/A3C Implementation\n",
    "\n",
    "## 4.1 Advantage Actor-Critic (A2C)\n",
    "\n",
    "A2C is a synchronous, deterministic variant of A3C that addresses several limitations of basic Actor-Critic:\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "1. **N-step Returns**: Use n-step bootstrapping instead of 1-step TD\n",
    "2. **Entropy Regularization**: Encourage exploration by penalizing deterministic policies  \n",
    "3. **Shared Networks**: Actor and critic share lower-layer representations\n",
    "4. **Batch Updates**: Collect multiple trajectories before updating\n",
    "\n",
    "### N-step Advantage Estimation\n",
    "\n",
    "Instead of 1-step TD error, A2C uses n-step returns:\n",
    "\n",
    "**A(s_t, a_t) = (Σᵢ₌₀ⁿ⁻¹ γⁱ r_{t+i}) + γⁿ V(s_{t+n}) - V(s_t)**\n",
    "\n",
    "This provides better bias-variance tradeoff.\n",
    "\n",
    "### Entropy Regularization\n",
    "\n",
    "The actor loss includes an entropy term:\n",
    "\n",
    "**L_actor = -E[A(s_t,a_t) log π(a_t|s_t)] - β H(π(·|s_t))**\n",
    "\n",
    "Where H(π) = -Σ_a π(a|s) log π(a|s) is the policy entropy.\n",
    "\n",
    "## 4.2 Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "A3C introduces asynchronous training with multiple workers:\n",
    "\n",
    "### Architecture:\n",
    "- **Global Network**: Shared actor-critic parameters\n",
    "- **Worker Threads**: Independent environments and local networks\n",
    "- **Asynchronous Updates**: Workers update global network asynchronously\n",
    "\n",
    "### Algorithm Overview:\n",
    "\n",
    "1. Initialize global shared parameters θ and φ\n",
    "2. For each worker thread:\n",
    "   a. Copy global parameters to local networks\n",
    "   b. Collect trajectory of length T\n",
    "   c. Compute advantages using n-step returns\n",
    "   d. Compute gradients and update global network\n",
    "   e. Repeat\n",
    "\n",
    "### Benefits:\n",
    "- **Decorrelated Experience**: Different workers explore different parts of state space\n",
    "- **Stability**: Removes need for experience replay\n",
    "- **Efficiency**: Parallelization speeds up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete A2C and A3C Implementation\n",
    "\n",
    "class SharedActorCriticNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Shared network for actor and critic with common feature extraction\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(SharedActorCriticNetwork, self).__init__()\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared_layers = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head\n",
    "        self.actor_head = nn.Linear(hidden_dim, action_dim)\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic_head = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass returning both policy and value\"\"\"\n",
    "        shared_features = self.shared_layers(state)\n",
    "        \n",
    "        # Actor output (logits)\n",
    "        logits = self.actor_head(shared_features)\n",
    "        \n",
    "        # Critic output (value)\n",
    "        value = self.critic_head(shared_features)\n",
    "        \n",
    "        return logits, value\n",
    "\n",
    "class A2CAgent:\n",
    "    \"\"\"\n",
    "    Advantage Actor-Critic (A2C) with n-step returns and entropy regularization\n",
    "    \"\"\"\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=7e-4, \n",
    "                 gamma=0.99, n_steps=5, entropy_coef=0.01, value_coef=0.5):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        \n",
    "        # Shared network\n",
    "        self.network = SharedActorCriticNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience storage\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        \n",
    "        # Logging\n",
    "        self.episode_rewards = []\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.entropy_losses = []\n",
    "        self.total_losses = []\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using current policy\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, value = self.network(state)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, log_prob, value, done):\n",
    "        \"\"\"Store transition in buffer\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def compute_n_step_returns(self, next_value=0):\n",
    "        \"\"\"Compute n-step returns and advantages\"\"\"\n",
    "        returns = []\n",
    "        advantages = []\n",
    "        \n",
    "        # Compute returns backwards\n",
    "        R = next_value\n",
    "        for i in reversed(range(len(self.rewards))):\n",
    "            R = self.rewards[i] + self.gamma * R * (1 - self.dones[i])\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        values = torch.FloatTensor(self.values).to(device)\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = returns - values\n",
    "        \n",
    "        return returns, advantages\n",
    "    \n",
    "    def update(self, next_state=None):\n",
    "        \"\"\"Update network using collected experience\"\"\"\n",
    "        if len(self.states) == 0:\n",
    "            return\n",
    "        \n",
    "        # Get next value for bootstrapping\n",
    "        if next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                _, next_value = self.network(next_state)\n",
    "                next_value = next_value.item()\n",
    "        else:\n",
    "            next_value = 0\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns, advantages = self.compute_n_step_returns(next_value)\n",
    "        \n",
    "        # Prepare tensors\n",
    "        states = torch.FloatTensor(self.states).to(device)\n",
    "        actions = torch.LongTensor(self.actions).to(device)\n",
    "        log_probs_old = torch.FloatTensor(self.log_probs).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, values = self.network(states)\n",
    "        dist = Categorical(logits=logits)\n",
    "        \n",
    "        # New log probabilities and entropy\n",
    "        log_probs_new = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Compute losses\n",
    "        actor_loss = -(log_probs_new * advantages.detach()).mean()\n",
    "        critic_loss = F.mse_loss(values.squeeze(), returns)\n",
    "        entropy_loss = -entropy\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = (actor_loss + \n",
    "                     self.value_coef * critic_loss + \n",
    "                     self.entropy_coef * entropy_loss)\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        self.critic_losses.append(critic_loss.item())\n",
    "        self.entropy_losses.append(entropy_loss.item())\n",
    "        self.total_losses.append(total_loss.item())\n",
    "        \n",
    "        # Clear buffers\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        \n",
    "        return total_loss.item()\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Train for one episode\"\"\"\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob, value = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            self.store_transition(state, action, reward, log_prob, value, done)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            # Update every n_steps or at episode end\n",
    "            if len(self.states) >= self.n_steps or done:\n",
    "                if done:\n",
    "                    self.update()\n",
    "                else:\n",
    "                    self.update(next_state)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        return episode_reward, step_count\n",
    "\n",
    "class A3CWorker:\n",
    "    \"\"\"\n",
    "    Individual worker for A3C training\n",
    "    \"\"\"\n",
    "    def __init__(self, worker_id, global_network, optimizer, state_dim, action_dim,\n",
    "                 gamma=0.99, n_steps=5, entropy_coef=0.01, value_coef=0.5):\n",
    "        self.worker_id = worker_id\n",
    "        self.global_network = global_network\n",
    "        self.global_optimizer = optimizer\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        self.entropy_coef = entropy_coef\n",
    "        self.value_coef = value_coef\n",
    "        \n",
    "        # Local network (copy of global)\n",
    "        self.local_network = SharedActorCriticNetwork(state_dim, action_dim).to(device)\n",
    "        \n",
    "        # Experience buffer\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        \n",
    "    def sync_with_global(self):\n",
    "        \"\"\"Synchronize local network with global network\"\"\"\n",
    "        self.local_network.load_state_dict(self.global_network.state_dict())\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using local network\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits, value = self.local_network(state)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            \n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "    def compute_loss(self, next_state=None):\n",
    "        \"\"\"Compute loss for local trajectory\"\"\"\n",
    "        if len(self.states) == 0:\n",
    "            return torch.tensor(0.0)\n",
    "        \n",
    "        # Get next value for bootstrapping\n",
    "        if next_state is not None:\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                _, next_value = self.local_network(next_state)\n",
    "                next_value = next_value.item()\n",
    "        else:\n",
    "            next_value = 0\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = []\n",
    "        R = next_value\n",
    "        for i in reversed(range(len(self.rewards))):\n",
    "            R = self.rewards[i] + self.gamma * R * (1 - self.dones[i])\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Prepare tensors\n",
    "        states = torch.FloatTensor(self.states).to(device)\n",
    "        actions = torch.LongTensor(self.actions).to(device)\n",
    "        values = torch.FloatTensor(self.values).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, new_values = self.local_network(states)\n",
    "        dist = Categorical(logits=logits)\n",
    "        \n",
    "        log_probs = dist.log_prob(actions)\n",
    "        entropy = dist.entropy().mean()\n",
    "        \n",
    "        # Compute advantages\n",
    "        advantages = returns - values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Compute losses\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = F.mse_loss(new_values.squeeze(), returns)\n",
    "        entropy_loss = -entropy\n",
    "        \n",
    "        total_loss = (actor_loss + \n",
    "                     self.value_coef * critic_loss + \n",
    "                     self.entropy_coef * entropy_loss)\n",
    "        \n",
    "        return total_loss\n",
    "    \n",
    "    def update_global(self, loss):\n",
    "        \"\"\"Update global network with local gradients\"\"\"\n",
    "        self.global_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Copy gradients from local to global\n",
    "        for local_param, global_param in zip(self.local_network.parameters(),\n",
    "                                           self.global_network.parameters()):\n",
    "            if global_param.grad is None:\n",
    "                global_param.grad = local_param.grad.clone()\n",
    "            else:\n",
    "                global_param.grad += local_param.grad\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(self.global_network.parameters(), max_norm=0.5)\n",
    "        self.global_optimizer.step()\n",
    "    \n",
    "    def train_worker(self, env, max_episodes=100):\n",
    "        \"\"\"Train worker for specified episodes\"\"\"\n",
    "        episode_rewards = []\n",
    "        \n",
    "        for episode in range(max_episodes):\n",
    "            self.sync_with_global()\n",
    "            \n",
    "            state, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            self.states = []\n",
    "            self.actions = []\n",
    "            self.rewards = []\n",
    "            self.log_probs = []\n",
    "            self.values = []\n",
    "            self.dones = []\n",
    "            \n",
    "            while True:\n",
    "                action, log_prob, value = self.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                self.states.append(state)\n",
    "                self.actions.append(action)\n",
    "                self.rewards.append(reward)\n",
    "                self.log_probs.append(log_prob)\n",
    "                self.values.append(value)\n",
    "                self.dones.append(done)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "                # Update every n_steps or at episode end\n",
    "                if len(self.states) >= self.n_steps or done:\n",
    "                    if done:\n",
    "                        loss = self.compute_loss()\n",
    "                    else:\n",
    "                        loss = self.compute_loss(next_state)\n",
    "                    \n",
    "                    self.update_global(loss)\n",
    "                    \n",
    "                    # Clear buffers\n",
    "                    self.states = []\n",
    "                    self.actions = []\n",
    "                    self.rewards = []\n",
    "                    self.log_probs = []\n",
    "                    self.values = []\n",
    "                    self.dones = []\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            \n",
    "            if episode % 50 == 0:\n",
    "                print(f\"Worker {self.worker_id} | Episode {episode} | Reward: {episode_reward:.2f}\")\n",
    "        \n",
    "        return episode_rewards\n",
    "\n",
    "def test_a2c_vs_a3c():\n",
    "    \"\"\"Compare A2C and A3C performance\"\"\"\n",
    "    env_name = 'CartPole-v1'\n",
    "    env = gym.make(env_name)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    print(\"=== A2C vs A3C Comparison ===\")\n",
    "    \n",
    "    # Test A2C\n",
    "    print(\"\\n1. Training A2C...\")\n",
    "    a2c_agent = A2CAgent(state_dim, action_dim, lr=7e-4, n_steps=5)\n",
    "    a2c_rewards = []\n",
    "    \n",
    "    for episode in range(300):\n",
    "        reward, _ = a2c_agent.train_episode(env)\n",
    "        a2c_rewards.append(reward)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(a2c_rewards[-50:])\n",
    "            print(f\"A2C Episode {episode+1:3d} | Avg Reward: {avg_reward:.2f}\")\n",
    "    \n",
    "    # Test A3C (simplified single-threaded version for demonstration)\n",
    "    print(\"\\n2. Training A3C (simplified)...\")\n",
    "    global_network = SharedActorCriticNetwork(state_dim, action_dim).to(device)\n",
    "    global_optimizer = optim.Adam(global_network.parameters(), lr=7e-4)\n",
    "    \n",
    "    worker = A3CWorker(0, global_network, global_optimizer, state_dim, action_dim)\n",
    "    a3c_rewards = worker.train_worker(env, max_episodes=300)\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Learning curves\n",
    "    window = 20\n",
    "    axes[0,0].plot(pd.Series(a2c_rewards).rolling(window).mean(), \n",
    "                   label='A2C', linewidth=2, color='blue')\n",
    "    axes[0,0].plot(pd.Series(a3c_rewards).rolling(window).mean(), \n",
    "                   label='A3C', linewidth=2, color='red')\n",
    "    axes[0,0].set_title('Learning Curves: A2C vs A3C')\n",
    "    axes[0,0].set_xlabel('Episode')\n",
    "    axes[0,0].set_ylabel('Average Reward')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Loss analysis for A2C\n",
    "    if len(a2c_agent.total_losses) > 0:\n",
    "        axes[0,1].plot(a2c_agent.actor_losses, label='Actor Loss', alpha=0.7)\n",
    "        axes[0,1].plot(a2c_agent.critic_losses, label='Critic Loss', alpha=0.7)\n",
    "        axes[0,1].plot(a2c_agent.entropy_losses, label='Entropy Loss', alpha=0.7)\n",
    "        axes[0,1].set_title('A2C Loss Components')\n",
    "        axes[0,1].set_xlabel('Update')\n",
    "        axes[0,1].set_ylabel('Loss')\n",
    "        axes[0,1].legend()\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final performance comparison\n",
    "    a2c_final = np.mean(a2c_rewards[-50:])\n",
    "    a3c_final = np.mean(a3c_rewards[-50:])\n",
    "    \n",
    "    methods = ['A2C', 'A3C']\n",
    "    performances = [a2c_final, a3c_final]\n",
    "    colors = ['blue', 'red']\n",
    "    \n",
    "    bars = axes[1,0].bar(methods, performances, color=colors, alpha=0.7)\n",
    "    axes[1,0].set_title('Final Performance (Last 50 Episodes)')\n",
    "    axes[1,0].set_ylabel('Average Reward')\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, value in zip(bars, performances):\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                       f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Sample efficiency comparison\n",
    "    a2c_sample_efficiency = []\n",
    "    a3c_sample_efficiency = []\n",
    "    \n",
    "    for i in range(0, len(a2c_rewards), 10):\n",
    "        a2c_sample_efficiency.append(np.mean(a2c_rewards[max(0, i-10):i+1]))\n",
    "    \n",
    "    for i in range(0, len(a3c_rewards), 10):\n",
    "        a3c_sample_efficiency.append(np.mean(a3c_rewards[max(0, i-10):i+1]))\n",
    "    \n",
    "    axes[1,1].plot(range(0, len(a2c_rewards), 10), a2c_sample_efficiency, \n",
    "                   label='A2C', linewidth=2, color='blue')\n",
    "    axes[1,1].plot(range(0, len(a3c_rewards), 10), a3c_sample_efficiency, \n",
    "                   label='A3C', linewidth=2, color='red')\n",
    "    axes[1,1].set_title('Sample Efficiency Comparison')\n",
    "    axes[1,1].set_xlabel('Episode')\n",
    "    axes[1,1].set_ylabel('Average Reward (10-episode window)')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n=== Results Summary ===\")\n",
    "    print(f\"A2C Final Performance: {a2c_final:.2f}\")\n",
    "    print(f\"A3C Final Performance: {a3c_final:.2f}\")\n",
    "    print(f\"Winner: {'A2C' if a2c_final > a3c_final else 'A3C'}\")\n",
    "    \n",
    "    env.close()\n",
    "    return a2c_agent, a2c_rewards, a3c_rewards\n",
    "\n",
    "# Run A2C vs A3C comparison\n",
    "a2c_agent, a2c_rewards, a3c_rewards = test_a2c_vs_a3c()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657f516",
   "metadata": {},
   "source": [
    "# Section 5: Variance Reduction Techniques\n",
    "\n",
    "## 5.1 Advanced Variance Reduction Methods\n",
    "\n",
    "Beyond basic baselines, several sophisticated techniques can further reduce the variance of policy gradient estimates:\n",
    "\n",
    "### 1. Control Variates\n",
    "Control variates use correlated random variables to reduce variance:\n",
    "\n",
    "**∇_θ J(θ) ≈ (1/n) Σᵢ [f(τᵢ) - c(g(τᵢ) - E[g(τ)])]**\n",
    "\n",
    "Where g(τ) is a control variate and c is chosen to minimize variance.\n",
    "\n",
    "### 2. Importance Sampling\n",
    "Allows using trajectories from different policies:\n",
    "\n",
    "**∇_θ J(θ) = E_τ∼π_β [ρ(τ) Σₜ G_t ∇_θ ln π_θ(aₜ|sₜ)]**\n",
    "\n",
    "Where ρ(τ) = π_θ(τ)/π_β(τ) is the importance weight.\n",
    "\n",
    "### 3. Natural Policy Gradients\n",
    "Use the natural gradient instead of standard gradient:\n",
    "\n",
    "**∇_θ J(θ) = F⁻¹ ∇_θ J(θ)**\n",
    "\n",
    "Where F is the Fisher Information Matrix.\n",
    "\n",
    "### 4. Generalized Advantage Estimation (GAE)\n",
    "Combines n-step returns with exponential averaging:\n",
    "\n",
    "**Â_t^(GAE) = Σₗ₌₀^∞ (γλ)ₗ δₜ₊ₗ**\n",
    "\n",
    "Where δₜ = rₜ + γV(sₜ₊₁) - V(sₜ) and λ ∈ [0,1] controls bias-variance tradeoff.\n",
    "\n",
    "## 5.2 Comparative Analysis of Variance Reduction\n",
    "\n",
    "We'll implement and compare different variance reduction techniques to understand their effectiveness and computational trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Variance Reduction Techniques Implementation\n",
    "\n",
    "class GeneralizedAdvantageEstimation:\n",
    "    \"\"\"\n",
    "    Generalized Advantage Estimation (GAE) for variance reduction\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def compute_gae(rewards, values, dones, gamma=0.99, lambda_=0.95):\n",
    "        \"\"\"\n",
    "        Compute GAE advantages\n",
    "        \n",
    "        Args:\n",
    "            rewards: List of rewards\n",
    "            values: List of value estimates\n",
    "            dones: List of done flags\n",
    "            gamma: Discount factor\n",
    "            lambda_: GAE parameter (0=high bias/low variance, 1=low bias/high variance)\n",
    "        \n",
    "        Returns:\n",
    "            advantages: GAE advantages\n",
    "            returns: Discounted returns\n",
    "        \"\"\"\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        # Compute advantages backwards\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            if i == len(rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = values[i + 1]\n",
    "            \n",
    "            # TD error\n",
    "            delta = rewards[i] + gamma * next_value * (1 - dones[i]) - values[i]\n",
    "            \n",
    "            # GAE accumulation\n",
    "            gae = delta + gamma * lambda_ * (1 - dones[i]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = [adv + val for adv, val in zip(advantages, values)]\n",
    "        \n",
    "        return np.array(advantages), np.array(returns)\n",
    "\n",
    "class ControlVariateREINFORCE(REINFORCEAgent):\n",
    "    \"\"\"\n",
    "    REINFORCE with control variates for variance reduction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Control variate network (learns to predict returns)\n",
    "        self.control_variate = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.cv_optimizer = optim.Adam(self.control_variate.parameters(), lr=1e-3)\n",
    "        \n",
    "        # Track control variate effectiveness\n",
    "        self.cv_predictions = []\n",
    "        self.cv_targets = []\n",
    "        self.cv_losses = []\n",
    "    \n",
    "    def update_policy(self, states, actions, returns):\n",
    "        \"\"\"Update with control variates\"\"\"\n",
    "        states_tensor = torch.FloatTensor(states).to(device)\n",
    "        actions_tensor = torch.LongTensor(actions).to(device)\n",
    "        returns_tensor = torch.FloatTensor(returns).to(device)\n",
    "        \n",
    "        # Get control variate predictions\n",
    "        cv_predictions = self.control_variate(states_tensor).squeeze()\n",
    "        \n",
    "        # Update control variate network\n",
    "        cv_loss = F.mse_loss(cv_predictions, returns_tensor)\n",
    "        self.cv_optimizer.zero_grad()\n",
    "        cv_loss.backward()\n",
    "        self.cv_optimizer.step()\n",
    "        \n",
    "        # Compute control variate coefficient (optimal choice)\n",
    "        with torch.no_grad():\n",
    "            cv_pred_detached = cv_predictions.detach()\n",
    "            \n",
    "            # Compute covariance and variance for optimal coefficient\n",
    "            cov = torch.mean((returns_tensor - returns_tensor.mean()) * \n",
    "                           (cv_pred_detached - cv_pred_detached.mean()))\n",
    "            var_cv = torch.var(cv_pred_detached)\n",
    "            \n",
    "            # Optimal coefficient c* = Cov(G,g) / Var(g)\n",
    "            c_optimal = cov / (var_cv + 1e-8)\n",
    "            c_optimal = torch.clamp(c_optimal, -2.0, 2.0)  # Clip for stability\n",
    "            \n",
    "            # Apply control variate\n",
    "            controlled_returns = returns_tensor - c_optimal * (cv_pred_detached - cv_pred_detached.mean())\n",
    "        \n",
    "        # Normalize controlled returns\n",
    "        controlled_returns = (controlled_returns - controlled_returns.mean()) / (controlled_returns.std() + 1e-8)\n",
    "        \n",
    "        # Standard REINFORCE update with controlled returns\n",
    "        logits = self.policy_net(states_tensor)\n",
    "        dist = Categorical(logits=logits)\n",
    "        log_probs = dist.log_prob(actions_tensor)\n",
    "        \n",
    "        policy_loss = -(log_probs * controlled_returns).mean()\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        self.cv_predictions.extend(cv_predictions.detach().cpu().numpy())\n",
    "        self.cv_targets.extend(returns_tensor.cpu().numpy())\n",
    "        self.cv_losses.append(cv_loss.item())\n",
    "        \n",
    "        # Regular logging\n",
    "        entropy = dist.entropy().mean()\n",
    "        self.entropy_history.append(entropy.item())\n",
    "        self.policy_losses.append(policy_loss.item())\n",
    "        \n",
    "        return policy_loss.item()\n",
    "\n",
    "class GAEActorCritic(ActorCriticAgent):\n",
    "    \"\"\"\n",
    "    Actor-Critic with Generalized Advantage Estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, lambda_gae=0.95, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.lambda_gae = lambda_gae\n",
    "        \n",
    "        # Store trajectory for GAE computation\n",
    "        self.trajectory_states = []\n",
    "        self.trajectory_rewards = []\n",
    "        self.trajectory_values = []\n",
    "        self.trajectory_dones = []\n",
    "        self.trajectory_actions = []\n",
    "        self.trajectory_log_probs = []\n",
    "        \n",
    "    def store_step(self, state, action, reward, value, done, log_prob):\n",
    "        \"\"\"Store step in trajectory\"\"\"\n",
    "        self.trajectory_states.append(state)\n",
    "        self.trajectory_actions.append(action)\n",
    "        self.trajectory_rewards.append(reward)\n",
    "        self.trajectory_values.append(value)\n",
    "        self.trajectory_dones.append(done)\n",
    "        self.trajectory_log_probs.append(log_prob)\n",
    "    \n",
    "    def update_with_gae(self):\n",
    "        \"\"\"Update using GAE advantages\"\"\"\n",
    "        if len(self.trajectory_states) == 0:\n",
    "            return\n",
    "        \n",
    "        # Compute GAE advantages\n",
    "        advantages, returns = GeneralizedAdvantageEstimation.compute_gae(\n",
    "            self.trajectory_rewards, \n",
    "            self.trajectory_values,\n",
    "            self.trajectory_dones,\n",
    "            gamma=self.gamma,\n",
    "            lambda_=self.lambda_gae\n",
    "        )\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(self.trajectory_states).to(device)\n",
    "        actions = torch.LongTensor(self.trajectory_actions).to(device)\n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        advantages = torch.FloatTensor(advantages).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.trajectory_log_probs).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Forward pass\n",
    "        actor_logits = self.actor(states)\n",
    "        critic_values = self.critic(states).squeeze()\n",
    "        \n",
    "        dist = Categorical(logits=actor_logits)\n",
    "        new_log_probs = dist.log_prob(actions)\n",
    "        \n",
    "        # Actor loss\n",
    "        actor_loss = -(new_log_probs * advantages).mean()\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(critic_values, returns)\n",
    "        \n",
    "        # Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        self.critic_losses.append(critic_loss.item())\n",
    "        self.advantages.extend(advantages.cpu().numpy())\n",
    "        \n",
    "        # Clear trajectory\n",
    "        self.trajectory_states = []\n",
    "        self.trajectory_rewards = []\n",
    "        self.trajectory_values = []\n",
    "        self.trajectory_dones = []\n",
    "        self.trajectory_actions = []\n",
    "        self.trajectory_log_probs = []\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "    \n",
    "    def train_episode(self, env, update_freq=10):\n",
    "        \"\"\"Train episode with periodic GAE updates\"\"\"\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            value = self.get_value(state)\n",
    "            \n",
    "            self.store_step(state, action, reward, value, done, log_prob)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            # Update every update_freq steps or at episode end\n",
    "            if step_count % update_freq == 0 or done:\n",
    "                self.update_with_gae()\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        return episode_reward, step_count\n",
    "\n",
    "def variance_reduction_comparison():\n",
    "    \"\"\"Compare different variance reduction techniques\"\"\"\n",
    "    env = gym.make('CartPole-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    print(\"=== Variance Reduction Techniques Comparison ===\")\n",
    "    \n",
    "    # Test different methods\n",
    "    methods = {}\n",
    "    num_episodes = 200\n",
    "    \n",
    "    # 1. Standard REINFORCE\n",
    "    print(\"\\n1. Training Standard REINFORCE...\")\n",
    "    reinforce_agent = REINFORCEAgent(state_dim, action_dim, lr=1e-3)\n",
    "    reinforce_rewards = []\n",
    "    reinforce_grad_norms = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        reward, _ = reinforce_agent.train_episode(env)\n",
    "        reinforce_rewards.append(reward)\n",
    "        if len(reinforce_agent.gradient_norms) > 0:\n",
    "            reinforce_grad_norms.append(reinforce_agent.gradient_norms[-1])\n",
    "    \n",
    "    methods['REINFORCE'] = {\n",
    "        'rewards': reinforce_rewards,\n",
    "        'grad_norms': reinforce_grad_norms\n",
    "    }\n",
    "    \n",
    "    # 2. REINFORCE with Control Variates\n",
    "    print(\"2. Training REINFORCE with Control Variates...\")\n",
    "    cv_agent = ControlVariateREINFORCE(state_dim, action_dim, lr=1e-3)\n",
    "    cv_rewards = []\n",
    "    cv_grad_norms = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        reward, _ = cv_agent.train_episode(env)\n",
    "        cv_rewards.append(reward)\n",
    "        if len(cv_agent.gradient_norms) > 0:\n",
    "            cv_grad_norms.append(cv_agent.gradient_norms[-1])\n",
    "    \n",
    "    methods['Control Variate'] = {\n",
    "        'rewards': cv_rewards,\n",
    "        'grad_norms': cv_grad_norms\n",
    "    }\n",
    "    \n",
    "    # 3. Standard Actor-Critic\n",
    "    print(\"3. Training Standard Actor-Critic...\")\n",
    "    ac_agent = ActorCriticAgent(state_dim, action_dim)\n",
    "    ac_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        reward, _ = ac_agent.train_episode(env)\n",
    "        ac_rewards.append(reward)\n",
    "    \n",
    "    methods['Actor-Critic'] = {\n",
    "        'rewards': ac_rewards,\n",
    "        'grad_norms': []  # Not directly comparable\n",
    "    }\n",
    "    \n",
    "    # 4. GAE Actor-Critic\n",
    "    print(\"4. Training GAE Actor-Critic...\")\n",
    "    gae_agent = GAEActorCritic(state_dim, action_dim, lambda_gae=0.95)\n",
    "    gae_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        reward, _ = gae_agent.train_episode(env, update_freq=5)\n",
    "        gae_rewards.append(reward)\n",
    "    \n",
    "    methods['GAE Actor-Critic'] = {\n",
    "        'rewards': gae_rewards,\n",
    "        'grad_norms': []  # Not directly comparable\n",
    "    }\n",
    "    \n",
    "    # Analysis and Visualization\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # Learning curves comparison\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    method_names = list(methods.keys())\n",
    "    \n",
    "    for i, (method, data) in enumerate(methods.items()):\n",
    "        window = 20\n",
    "        smoothed = pd.Series(data['rewards']).rolling(window).mean()\n",
    "        axes[0,0].plot(smoothed, label=method, color=colors[i], linewidth=2)\n",
    "    \n",
    "    axes[0,0].set_title('Learning Curves Comparison')\n",
    "    axes[0,0].set_xlabel('Episode')\n",
    "    axes[0,0].set_ylabel('Average Reward')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final performance comparison\n",
    "    final_performances = [np.mean(data['rewards'][-50:]) for data in methods.values()]\n",
    "    \n",
    "    bars = axes[0,1].bar(method_names, final_performances, color=colors, alpha=0.7)\n",
    "    axes[0,1].set_title('Final Performance (Last 50 Episodes)')\n",
    "    axes[0,1].set_ylabel('Average Reward')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    plt.setp(axes[0,1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    for bar, value in zip(bars, final_performances):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                       f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Sample efficiency (episodes to reach threshold)\n",
    "    threshold = 450\n",
    "    episodes_to_threshold = []\n",
    "    \n",
    "    for method, data in methods.items():\n",
    "        smoothed = pd.Series(data['rewards']).rolling(20).mean()\n",
    "        threshold_idx = None\n",
    "        for i, reward in enumerate(smoothed):\n",
    "            if reward >= threshold:\n",
    "                threshold_idx = i\n",
    "                break\n",
    "        episodes_to_threshold.append(threshold_idx if threshold_idx else num_episodes)\n",
    "    \n",
    "    axes[0,2].bar(method_names, episodes_to_threshold, color=colors, alpha=0.7)\n",
    "    axes[0,2].set_title(f'Sample Efficiency (Episodes to {threshold} reward)')\n",
    "    axes[0,2].set_ylabel('Episodes')\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    plt.setp(axes[0,2].xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Variance analysis for gradient-based methods\n",
    "    grad_methods = [(name, data) for name, data in methods.items() if data['grad_norms']]\n",
    "    \n",
    "    if grad_methods:\n",
    "        for i, (method, data) in enumerate(grad_methods):\n",
    "            if data['grad_norms']:\n",
    "                axes[1,0].hist(data['grad_norms'], bins=30, alpha=0.5, \n",
    "                             label=method, density=True, color=colors[i])\n",
    "        \n",
    "        axes[1,0].set_title('Gradient Norm Distributions')\n",
    "        axes[1,0].set_xlabel('Gradient Norm')\n",
    "        axes[1,0].set_ylabel('Density')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Control variate effectiveness\n",
    "    if len(cv_agent.cv_predictions) > 0:\n",
    "        cv_corr = np.corrcoef(cv_agent.cv_predictions, cv_agent.cv_targets)[0,1]\n",
    "        \n",
    "        axes[1,1].scatter(cv_agent.cv_predictions[:500], cv_agent.cv_targets[:500], \n",
    "                         alpha=0.6, s=10)\n",
    "        axes[1,1].plot([min(cv_agent.cv_predictions), max(cv_agent.cv_predictions)],\n",
    "                      [min(cv_agent.cv_predictions), max(cv_agent.cv_predictions)],\n",
    "                      'r--', linewidth=2)\n",
    "        axes[1,1].set_title(f'Control Variate Effectiveness (ρ = {cv_corr:.3f})')\n",
    "        axes[1,1].set_xlabel('CV Prediction')\n",
    "        axes[1,1].set_ylabel('True Return')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Statistical summary\n",
    "    variance_stats = {}\n",
    "    for method, data in methods.items():\n",
    "        rewards = data['rewards']\n",
    "        variance_stats[method] = {\n",
    "            'mean': np.mean(rewards),\n",
    "            'std': np.std(rewards),\n",
    "            'final_mean': np.mean(rewards[-50:]),\n",
    "            'final_std': np.std(rewards[-50:])\n",
    "        }\n",
    "    \n",
    "    # Create summary table\n",
    "    summary_data = []\n",
    "    for method, stats in variance_stats.items():\n",
    "        summary_data.append([\n",
    "            method,\n",
    "            f\"{stats['mean']:.1f}\",\n",
    "            f\"{stats['std']:.1f}\",\n",
    "            f\"{stats['final_mean']:.1f}\",\n",
    "            f\"{stats['final_std']:.1f}\"\n",
    "        ])\n",
    "    \n",
    "    table = axes[1,2].table(cellText=summary_data,\n",
    "                           colLabels=['Method', 'Mean', 'Std', 'Final Mean', 'Final Std'],\n",
    "                           cellLoc='center',\n",
    "                           loc='center')\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(9)\n",
    "    table.scale(1.2, 1.5)\n",
    "    axes[1,2].axis('off')\n",
    "    axes[1,2].set_title('Performance Statistics')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print detailed analysis\n",
    "    print(f\"\\n=== Detailed Analysis ===\")\n",
    "    for method, stats in variance_stats.items():\n",
    "        print(f\"\\n{method}:\")\n",
    "        print(f\"  Overall Performance: {stats['mean']:.2f} ± {stats['std']:.2f}\")\n",
    "        print(f\"  Final Performance:   {stats['final_mean']:.2f} ± {stats['final_std']:.2f}\")\n",
    "        print(f\"  Sample Efficiency:   {episodes_to_threshold[list(variance_stats.keys()).index(method)]} episodes\")\n",
    "    \n",
    "    env.close()\n",
    "    return methods\n",
    "\n",
    "# Run variance reduction comparison\n",
    "variance_methods = variance_reduction_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3dd721",
   "metadata": {},
   "source": [
    "# Section 6: Continuous Action Space Policy Gradients\n",
    "\n",
    "## 6.1 Extension to Continuous Control\n",
    "\n",
    "One of the major advantages of policy gradient methods is their natural ability to handle continuous action spaces. Instead of outputting discrete action probabilities, we parameterize continuous probability distributions.\n",
    "\n",
    "### Gaussian Policy Parameterization\n",
    "\n",
    "For continuous actions, we typically use a Gaussian (Normal) policy:\n",
    "\n",
    "**π_θ(a|s) = N(μ_θ(s), σ_θ(s))**\n",
    "\n",
    "Where:\n",
    "- **μ_θ(s)**: Mean of the action distribution (neural network output)\n",
    "- **σ_θ(s)**: Standard deviation of the action distribution\n",
    "\n",
    "### Policy Gradient for Continuous Actions\n",
    "\n",
    "The policy gradient for Gaussian policies is:\n",
    "\n",
    "**∇_θ ln π_θ(a|s) = ∇_θ ln N(a|μ_θ(s), σ_θ(s))**\n",
    "\n",
    "For a Gaussian policy, this becomes:\n",
    "\n",
    "**∇_θ ln π_θ(a|s) = (a - μ_θ(s))/σ_θ(s)² · ∇_θ μ_θ(s) - ∇_θ ln σ_θ(s)**\n",
    "\n",
    "### Parameterization Strategies\n",
    "\n",
    "1. **Separate Networks**: Different networks for μ and σ\n",
    "2. **Shared Network**: Single network outputting both μ and σ\n",
    "3. **Fixed Variance**: Learn only μ, keep σ constant\n",
    "4. **State-Independent Variance**: Learn σ as a parameter, not function of state\n",
    "\n",
    "## 6.2 Continuous Control Challenges\n",
    "\n",
    "Continuous control introduces several challenges:\n",
    "\n",
    "1. **Action Scaling**: Actions often need to be scaled to environment bounds\n",
    "2. **Exploration**: Balancing exploration vs exploitation in continuous space\n",
    "3. **Stability**: Continuous policies can be more sensitive to hyperparameters\n",
    "4. **Sample Efficiency**: High-dimensional continuous spaces require more samples\n",
    "\n",
    "## 6.3 Popular Continuous Control Environments\n",
    "\n",
    "- **MuJoCo**: Physics-based continuous control (HalfCheetah, Walker2d, etc.)\n",
    "- **PyBullet**: Open-source physics simulation\n",
    "- **Custom Control**: Pendulum, CartPole continuous, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9de6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous Action Space Policy Gradient Implementation\n",
    "\n",
    "class ContinuousActorCritic:\n",
    "    \"\"\"\n",
    "    Actor-Critic for continuous action spaces using Gaussian policies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, action_bound=1.0, hidden_dim=128, \n",
    "                 actor_lr=1e-4, critic_lr=1e-3, gamma=0.99):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # Actor network (outputs mean and log_std)\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim * 2)  # mean and log_std\n",
    "        ).to(device)\n",
    "        \n",
    "        # Critic network\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        # Logging\n",
    "        self.episode_rewards = []\n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "        self.action_means = []\n",
    "        self.action_stds = []\n",
    "        \n",
    "    def select_action(self, state, deterministic=False):\n",
    "        \"\"\"Select action from Gaussian policy\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actor_output = self.actor(state)\n",
    "            \n",
    "        # Split output into mean and log_std\n",
    "        mean = actor_output[:, :self.action_dim]\n",
    "        log_std = actor_output[:, self.action_dim:]\n",
    "        \n",
    "        # Clamp log_std for stability\n",
    "        log_std = torch.clamp(log_std, min=-20, max=2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = mean\n",
    "        else:\n",
    "            # Sample from Gaussian distribution\n",
    "            dist = Normal(mean, std)\n",
    "            action = dist.sample()\n",
    "        \n",
    "        # Scale action to environment bounds\n",
    "        action_scaled = torch.tanh(action) * self.action_bound\n",
    "        \n",
    "        # Compute log probability\n",
    "        if not deterministic:\n",
    "            log_prob = dist.log_prob(action).sum(axis=-1)\n",
    "            # Adjust for tanh squashing\n",
    "            log_prob -= (2 * (np.log(2) - action - F.softplus(-2 * action))).sum(axis=-1)\n",
    "        else:\n",
    "            log_prob = None\n",
    "        \n",
    "        return action_scaled.cpu().numpy().flatten(), log_prob\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        \"\"\"Get state value from critic\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            value = self.critic(state)\n",
    "            \n",
    "        return value.item()\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done, log_prob):\n",
    "        \"\"\"Update actor and critic networks\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n",
    "        action = torch.FloatTensor(action).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Critic update\n",
    "        current_value = self.critic(state)\n",
    "        \n",
    "        if done:\n",
    "            target_value = reward\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                next_value = self.critic(next_state)\n",
    "            target_value = reward + self.gamma * next_value\n",
    "        \n",
    "        advantage = target_value - current_value\n",
    "        critic_loss = advantage.pow(2)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Actor update\n",
    "        actor_loss = -log_prob * advantage.detach()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Logging\n",
    "        actor_output = self.actor(state)\n",
    "        mean = actor_output[:, :self.action_dim]\n",
    "        log_std = actor_output[:, self.action_dim:]\n",
    "        std = torch.exp(torch.clamp(log_std, min=-20, max=2))\n",
    "        \n",
    "        self.action_means.append(mean.mean().item())\n",
    "        self.action_stds.append(std.mean().item())\n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        self.critic_losses.append(critic_loss.item())\n",
    "        \n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "    \n",
    "    def train_episode(self, env):\n",
    "        \"\"\"Train for one episode\"\"\"\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Update networks\n",
    "            if log_prob is not None:\n",
    "                actor_loss, critic_loss = self.update(\n",
    "                    state, action, reward, next_state, done, log_prob)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        return episode_reward, step_count\n",
    "\n",
    "class PPOContinuous:\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization for continuous action spaces\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, action_bound=1.0, hidden_dim=128,\n",
    "                 lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=4, entropy_coef=0.01):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.k_epochs = k_epochs\n",
    "        self.entropy_coef = entropy_coef\n",
    "        \n",
    "        # Shared network\n",
    "        self.network = SharedActorCriticNetwork(state_dim, action_dim * 2).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        # Experience buffer\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "        \n",
    "        # Logging\n",
    "        self.episode_rewards = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.entropy_losses = []\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        \"\"\"Select action using current policy\"\"\"\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            actor_critic_output, value = self.network(state)\n",
    "            \n",
    "        # Split actor output\n",
    "        mean = actor_critic_output[:, :self.action_dim]\n",
    "        log_std = actor_critic_output[:, self.action_dim:]\n",
    "        log_std = torch.clamp(log_std, min=-20, max=2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        # Sample action\n",
    "        dist = Normal(mean, std)\n",
    "        action = dist.sample()\n",
    "        action_scaled = torch.tanh(action) * self.action_bound\n",
    "        \n",
    "        # Compute log probability\n",
    "        log_prob = dist.log_prob(action).sum(axis=-1)\n",
    "        log_prob -= (2 * (np.log(2) - action - F.softplus(-2 * action))).sum(axis=-1)\n",
    "        \n",
    "        return action_scaled.cpu().numpy().flatten(), log_prob.item(), value.item()\n",
    "    \n",
    "    def store_transition(self, state, action, reward, log_prob, value, done):\n",
    "        \"\"\"Store transition\"\"\"\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.log_probs.append(log_prob)\n",
    "        self.values.append(value)\n",
    "        self.dones.append(done)\n",
    "    \n",
    "    def compute_advantages(self):\n",
    "        \"\"\"Compute GAE advantages\"\"\"\n",
    "        advantages = []\n",
    "        returns = []\n",
    "        \n",
    "        gae = 0\n",
    "        for i in reversed(range(len(self.rewards))):\n",
    "            if i == len(self.rewards) - 1:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                next_value = self.values[i + 1]\n",
    "            \n",
    "            delta = self.rewards[i] + self.gamma * next_value * (1 - self.dones[i]) - self.values[i]\n",
    "            gae = delta + self.gamma * 0.95 * (1 - self.dones[i]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        for i in range(len(advantages)):\n",
    "            returns.append(advantages[i] + self.values[i])\n",
    "        \n",
    "        return torch.FloatTensor(advantages).to(device), torch.FloatTensor(returns).to(device)\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update policy using PPO\"\"\"\n",
    "        if len(self.states) == 0:\n",
    "            return\n",
    "        \n",
    "        # Compute advantages and returns\n",
    "        advantages, returns = self.compute_advantages()\n",
    "        \n",
    "        # Convert to tensors\n",
    "        old_states = torch.FloatTensor(self.states).to(device)\n",
    "        old_actions = torch.FloatTensor(self.actions).to(device)\n",
    "        old_log_probs = torch.FloatTensor(self.log_probs).to(device)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO update for k epochs\n",
    "        for _ in range(self.k_epochs):\n",
    "            # Forward pass\n",
    "            actor_output, values = self.network(old_states)\n",
    "            \n",
    "            # Split actor output\n",
    "            mean = actor_output[:, :self.action_dim]\n",
    "            log_std = actor_output[:, self.action_dim:]\n",
    "            log_std = torch.clamp(log_std, min=-20, max=2)\n",
    "            std = torch.exp(log_std)\n",
    "            \n",
    "            # Compute new log probabilities\n",
    "            dist = Normal(mean, std)\n",
    "            \n",
    "            # Inverse tanh to get original actions\n",
    "            actions_unscaled = torch.atanh(torch.clamp(old_actions / self.action_bound, -0.999, 0.999))\n",
    "            \n",
    "            new_log_probs = dist.log_prob(actions_unscaled).sum(axis=-1)\n",
    "            new_log_probs -= (2 * (np.log(2) - actions_unscaled - F.softplus(-2 * actions_unscaled))).sum(axis=-1)\n",
    "            \n",
    "            # PPO ratio\n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            \n",
    "            # PPO loss\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            value_loss = F.mse_loss(values.squeeze(), returns)\n",
    "            \n",
    "            # Entropy loss\n",
    "            entropy = dist.entropy().mean()\n",
    "            entropy_loss = -entropy\n",
    "            \n",
    "            # Total loss\n",
    "            total_loss = policy_loss + 0.5 * value_loss + self.entropy_coef * entropy_loss\n",
    "            \n",
    "            # Optimize\n",
    "            self.optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=0.5)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Logging (only last epoch)\n",
    "            if _ == self.k_epochs - 1:\n",
    "                self.policy_losses.append(policy_loss.item())\n",
    "                self.value_losses.append(value_loss.item())\n",
    "                self.entropy_losses.append(entropy_loss.item())\n",
    "        \n",
    "        # Clear buffer\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.log_probs = []\n",
    "        self.values = []\n",
    "        self.dones = []\n",
    "    \n",
    "    def train_episode(self, env, update_freq=2048):\n",
    "        \"\"\"Train episode with batch updates\"\"\"\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while True:\n",
    "            action, log_prob, value = self.select_action(state)\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            self.store_transition(state, action, reward, log_prob, value, done)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            # Update when buffer is full or episode ends\n",
    "            if len(self.states) >= update_freq or done:\n",
    "                self.update()\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        self.episode_rewards.append(episode_reward)\n",
    "        return episode_reward, step_count\n",
    "\n",
    "def test_continuous_control():\n",
    "    \"\"\"Test continuous control algorithms\"\"\"\n",
    "    \n",
    "    # Use Pendulum environment (continuous control)\n",
    "    env = gym.make('Pendulum-v1')\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.shape[0]\n",
    "    action_bound = float(env.action_space.high[0])\n",
    "    \n",
    "    print(\"=== Continuous Control Policy Gradients ===\")\n",
    "    print(f\"Environment: Pendulum-v1\")\n",
    "    print(f\"State dim: {state_dim}, Action dim: {action_dim}, Action bound: {action_bound}\")\n",
    "    \n",
    "    # Test different continuous methods\n",
    "    methods = {}\n",
    "    num_episodes = 200\n",
    "    \n",
    "    # 1. Continuous Actor-Critic\n",
    "    print(\"\\n1. Training Continuous Actor-Critic...\")\n",
    "    ac_agent = ContinuousActorCritic(state_dim, action_dim, action_bound)\n",
    "    ac_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        reward, _ = ac_agent.train_episode(env)\n",
    "        ac_rewards.append(reward)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(ac_rewards[-50:])\n",
    "            avg_std = np.mean(ac_agent.action_stds[-50:]) if ac_agent.action_stds else 0\n",
    "            print(f\"AC Episode {episode+1:3d} | Avg Reward: {avg_reward:7.2f} | Action Std: {avg_std:.3f}\")\n",
    "    \n",
    "    methods['Continuous AC'] = ac_rewards\n",
    "    \n",
    "    # 2. PPO Continuous\n",
    "    print(\"\\n2. Training PPO Continuous...\")\n",
    "    ppo_agent = PPOContinuous(state_dim, action_dim, action_bound)\n",
    "    ppo_rewards = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        reward, _ = ppo_agent.train_episode(env, update_freq=1024)\n",
    "        ppo_rewards.append(reward)\n",
    "        \n",
    "        if (episode + 1) % 50 == 0:\n",
    "            avg_reward = np.mean(ppo_rewards[-50:])\n",
    "            print(f\"PPO Episode {episode+1:3d} | Avg Reward: {avg_reward:7.2f}\")\n",
    "    \n",
    "    methods['PPO Continuous'] = ppo_rewards\n",
    "    \n",
    "    # Visualization and Analysis\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Learning curves\n",
    "    window = 20\n",
    "    colors = ['blue', 'red']\n",
    "    \n",
    "    for i, (method, rewards) in enumerate(methods.items()):\n",
    "        smoothed = pd.Series(rewards).rolling(window).mean()\n",
    "        axes[0,0].plot(smoothed, label=method, color=colors[i], linewidth=2)\n",
    "    \n",
    "    axes[0,0].set_title('Learning Curves: Continuous Control')\n",
    "    axes[0,0].set_xlabel('Episode')\n",
    "    axes[0,0].set_ylabel('Episode Reward')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final performance comparison\n",
    "    final_performances = [np.mean(rewards[-50:]) for rewards in methods.values()]\n",
    "    method_names = list(methods.keys())\n",
    "    \n",
    "    bars = axes[0,1].bar(method_names, final_performances, color=colors, alpha=0.7)\n",
    "    axes[0,1].set_title('Final Performance (Last 50 Episodes)')\n",
    "    axes[0,1].set_ylabel('Average Reward')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    for bar, value in zip(bars, final_performances):\n",
    "        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                       f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Action exploration analysis\n",
    "    if ac_agent.action_means and ac_agent.action_stds:\n",
    "        axes[1,0].plot(ac_agent.action_means, label='Action Mean', alpha=0.7)\n",
    "        axes[1,0].plot(ac_agent.action_stds, label='Action Std', alpha=0.7)\n",
    "        axes[1,0].set_title('Action Statistics (Continuous AC)')\n",
    "        axes[1,0].set_xlabel('Update Step')\n",
    "        axes[1,0].set_ylabel('Value')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Policy visualization (sample a few states and show action distribution)\n",
    "    test_states = [\n",
    "        [1.0, 0.0, 0.0],   # Different pendulum states\n",
    "        [0.0, 1.0, 0.0],\n",
    "        [-1.0, 0.0, 0.0],\n",
    "        [0.0, 0.0, 1.0],\n",
    "        [0.0, 0.0, -1.0]\n",
    "    ]\n",
    "    \n",
    "    ac_actions = []\n",
    "    ppo_actions = []\n",
    "    \n",
    "    for state in test_states:\n",
    "        ac_action, _ = ac_agent.select_action(state)\n",
    "        ppo_action, _, _ = ppo_agent.select_action(state)\n",
    "        ac_actions.append(ac_action[0])\n",
    "        ppo_actions.append(ppo_action[0])\n",
    "    \n",
    "    x_pos = range(len(test_states))\n",
    "    width = 0.35\n",
    "    \n",
    "    axes[1,1].bar([x - width/2 for x in x_pos], ac_actions, width, \n",
    "                  label='Continuous AC', alpha=0.7, color='blue')\n",
    "    axes[1,1].bar([x + width/2 for x in x_pos], ppo_actions, width,\n",
    "                  label='PPO Continuous', alpha=0.7, color='red')\n",
    "    \n",
    "    axes[1,1].set_title('Action Selection for Test States')\n",
    "    axes[1,1].set_xlabel('Test State Index')\n",
    "    axes[1,1].set_ylabel('Selected Action')\n",
    "    axes[1,1].set_xticks(x_pos)\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Performance evaluation\n",
    "    print(f\"\\n=== Results Summary ===\")\n",
    "    for method, rewards in methods.items():\n",
    "        final_perf = np.mean(rewards[-50:])\n",
    "        final_std = np.std(rewards[-50:])\n",
    "        print(f\"{method}:\")\n",
    "        print(f\"  Final Performance: {final_perf:.2f} ± {final_std:.2f}\")\n",
    "        print(f\"  Best Episode: {max(rewards):.2f}\")\n",
    "    \n",
    "    # Test trained policies\n",
    "    print(f\"\\n=== Policy Evaluation ===\")\n",
    "    for method, agent in [('Continuous AC', ac_agent), ('PPO Continuous', ppo_agent)]:\n",
    "        test_rewards = []\n",
    "        for _ in range(10):\n",
    "            state, _ = env.reset()\n",
    "            total_reward = 0\n",
    "            \n",
    "            for _ in range(200):  # Max steps in Pendulum\n",
    "                if hasattr(agent, 'select_action'):\n",
    "                    if method == 'Continuous AC':\n",
    "                        action, _ = agent.select_action(state, deterministic=True)\n",
    "                    else:\n",
    "                        action, _, _ = agent.select_action(state)\n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "                \n",
    "                state, reward, terminated, truncated, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                \n",
    "                if terminated or truncated:\n",
    "                    break\n",
    "            \n",
    "            test_rewards.append(total_reward)\n",
    "        \n",
    "        print(f\"{method} Test Performance: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    return methods, ac_agent, ppo_agent\n",
    "\n",
    "# Run continuous control test\n",
    "continuous_results = test_continuous_control()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380578d8",
   "metadata": {},
   "source": [
    "# Section 7: Comprehensive Performance Analysis and Comparison\n",
    "\n",
    "In this section, we conduct a thorough empirical analysis of the policy gradient methods implemented in previous sections. We compare different algorithms across multiple environments and analyze their strengths and weaknesses.\n",
    "\n",
    "## 7.1 Experimental Setup\n",
    "\n",
    "Our experimental design includes:\n",
    "\n",
    "1. **Multiple Environments**: We test on both discrete (CartPole, LunarLander) and continuous (Pendulum, MountainCarContinuous) control tasks\n",
    "2. **Multiple Algorithms**: REINFORCE, Actor-Critic, A2C, A3C, and PPO variants\n",
    "3. **Statistical Significance**: Each algorithm is run multiple times with different random seeds\n",
    "4. **Variance Analysis**: We analyze both sample efficiency and final performance\n",
    "\n",
    "## 7.2 Hyperparameter Sensitivity\n",
    "\n",
    "Policy gradient methods are known to be sensitive to hyperparameters. We analyze the impact of:\n",
    "- Learning rates (actor and critic)\n",
    "- Network architectures\n",
    "- Entropy coefficients\n",
    "- Discount factors\n",
    "- Variance reduction techniques\n",
    "\n",
    "## 7.3 Convergence Analysis\n",
    "\n",
    "We examine:\n",
    "- **Sample Efficiency**: How quickly do algorithms learn?\n",
    "- **Stability**: How consistent is the performance across runs?\n",
    "- **Final Performance**: What is the asymptotic performance?\n",
    "- **Robustness**: How do algorithms perform across different environments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd95fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Performance Analysis Implementation\n",
    "\n",
    "class PerformanceAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive performance analysis for policy gradient methods\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.results = {}\n",
    "        self.statistical_tests = {}\n",
    "        \n",
    "    def run_multiple_seeds(self, agent_class, env_name, num_runs=5, num_episodes=200, **agent_kwargs):\n",
    "        \"\"\"Run algorithm with multiple random seeds\"\"\"\n",
    "        all_rewards = []\n",
    "        all_learning_curves = []\n",
    "        \n",
    "        for run in range(num_runs):\n",
    "            print(f\"  Run {run+1}/{num_runs}\")\n",
    "            \n",
    "            # Set random seeds for reproducibility\n",
    "            torch.manual_seed(42 + run)\n",
    "            np.random.seed(42 + run)\n",
    "            \n",
    "            # Create environment and agent\n",
    "            env = gym.make(env_name)\n",
    "            if hasattr(env.action_space, 'n'):  # Discrete\n",
    "                state_dim = env.observation_space.shape[0]\n",
    "                action_dim = env.action_space.n\n",
    "                agent = agent_class(state_dim, action_dim, **agent_kwargs)\n",
    "            else:  # Continuous\n",
    "                state_dim = env.observation_space.shape[0]\n",
    "                action_dim = env.action_space.shape[0]\n",
    "                action_bound = float(env.action_space.high[0])\n",
    "                agent = agent_class(state_dim, action_dim, action_bound, **agent_kwargs)\n",
    "            \n",
    "            # Training\n",
    "            episode_rewards = []\n",
    "            for episode in range(num_episodes):\n",
    "                if hasattr(agent, 'train_episode'):\n",
    "                    reward, _ = agent.train_episode(env)\n",
    "                else:\n",
    "                    # Fallback training loop\n",
    "                    state, _ = env.reset()\n",
    "                    reward = 0\n",
    "                    while True:\n",
    "                        action, log_prob = agent.select_action(state)\n",
    "                        next_state, r, terminated, truncated, _ = env.step(action)\n",
    "                        done = terminated or truncated\n",
    "                        \n",
    "                        agent.update(state, action, r, next_state, done, log_prob)\n",
    "                        reward += r\n",
    "                        state = next_state\n",
    "                        \n",
    "                        if done:\n",
    "                            break\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "            \n",
    "            all_rewards.extend(episode_rewards)\n",
    "            all_learning_curves.append(episode_rewards)\n",
    "            env.close()\n",
    "        \n",
    "        return all_learning_curves, all_rewards\n",
    "    \n",
    "    def compute_metrics(self, learning_curves, window=50):\n",
    "        \"\"\"Compute comprehensive metrics\"\"\"\n",
    "        learning_curves = np.array(learning_curves)\n",
    "        \n",
    "        # Sample efficiency (episodes to reach threshold)\n",
    "        mean_curve = np.mean(learning_curves, axis=0)\n",
    "        target_performance = np.mean(mean_curve[-window:])  # Use final performance as target\n",
    "        threshold = target_performance * 0.8  # 80% of final performance\n",
    "        \n",
    "        sample_efficiency = None\n",
    "        for i in range(len(mean_curve)):\n",
    "            if np.mean(mean_curve[max(0, i-window):i+1]) >= threshold:\n",
    "                sample_efficiency = i\n",
    "                break\n",
    "        \n",
    "        # Final performance\n",
    "        final_rewards = learning_curves[:, -window:]\n",
    "        final_performance_mean = np.mean(final_rewards)\n",
    "        final_performance_std = np.std(final_rewards)\n",
    "        \n",
    "        # Learning stability (coefficient of variation)\n",
    "        learning_stability = []\n",
    "        for curve in learning_curves:\n",
    "            smoothed = pd.Series(curve).rolling(window).mean().dropna()\n",
    "            if len(smoothed) > 0:\n",
    "                stability = np.std(smoothed) / (np.mean(smoothed) + 1e-8)\n",
    "                learning_stability.append(stability)\n",
    "        avg_stability = np.mean(learning_stability)\n",
    "        \n",
    "        # Peak performance\n",
    "        peak_performance = np.max([np.max(curve) for curve in learning_curves])\n",
    "        \n",
    "        # Convergence rate (slope of learning curve in middle section)\n",
    "        mid_start = len(mean_curve) // 3\n",
    "        mid_end = 2 * len(mean_curve) // 3\n",
    "        if mid_end > mid_start:\n",
    "            x = np.arange(mid_start, mid_end)\n",
    "            y = mean_curve[mid_start:mid_end]\n",
    "            convergence_rate = np.polyfit(x, y, 1)[0] if len(x) > 1 else 0\n",
    "        else:\n",
    "            convergence_rate = 0\n",
    "        \n",
    "        return {\n",
    "            'sample_efficiency': sample_efficiency,\n",
    "            'final_performance_mean': final_performance_mean,\n",
    "            'final_performance_std': final_performance_std,\n",
    "            'learning_stability': avg_stability,\n",
    "            'peak_performance': peak_performance,\n",
    "            'convergence_rate': convergence_rate,\n",
    "            'learning_curves': learning_curves\n",
    "        }\n",
    "    \n",
    "    def run_comprehensive_comparison(self):\n",
    "        \"\"\"Run comprehensive comparison across algorithms and environments\"\"\"\n",
    "        \n",
    "        # Define algorithms and environments to test\n",
    "        discrete_algorithms = [\n",
    "            ('REINFORCE', REINFORCEAgent, {'lr': 1e-3, 'gamma': 0.99}),\n",
    "            ('Actor-Critic', ActorCriticAgent, {'actor_lr': 1e-4, 'critic_lr': 1e-3, 'gamma': 0.99}),\n",
    "            ('A2C', A2CAgent, {'lr': 1e-4, 'gamma': 0.99, 'entropy_coef': 0.01}),\n",
    "        ]\n",
    "        \n",
    "        continuous_algorithms = [\n",
    "            ('Continuous AC', ContinuousActorCritic, {'actor_lr': 1e-4, 'critic_lr': 1e-3, 'gamma': 0.99}),\n",
    "            ('PPO Continuous', PPOContinuous, {'lr': 3e-4, 'gamma': 0.99, 'eps_clip': 0.2}),\n",
    "        ]\n",
    "        \n",
    "        discrete_envs = ['CartPole-v1', 'LunarLander-v2']\n",
    "        continuous_envs = ['Pendulum-v1']\n",
    "        \n",
    "        all_results = {}\n",
    "        \n",
    "        # Test discrete algorithms\n",
    "        for env_name in discrete_envs:\n",
    "            print(f\"\\n=== Testing on {env_name} ===\")\n",
    "            all_results[env_name] = {}\n",
    "            \n",
    "            for alg_name, alg_class, alg_kwargs in discrete_algorithms:\n",
    "                print(f\"\\nTesting {alg_name}...\")\n",
    "                try:\n",
    "                    learning_curves, all_rewards = self.run_multiple_seeds(\n",
    "                        alg_class, env_name, num_runs=3, num_episodes=150, **alg_kwargs\n",
    "                    )\n",
    "                    metrics = self.compute_metrics(learning_curves)\n",
    "                    all_results[env_name][alg_name] = metrics\n",
    "                    \n",
    "                    print(f\"  Final Performance: {metrics['final_performance_mean']:.2f} ± {metrics['final_performance_std']:.2f}\")\n",
    "                    print(f\"  Sample Efficiency: {metrics['sample_efficiency']} episodes\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error with {alg_name}: {e}\")\n",
    "                    all_results[env_name][alg_name] = None\n",
    "        \n",
    "        # Test continuous algorithms\n",
    "        for env_name in continuous_envs:\n",
    "            print(f\"\\n=== Testing on {env_name} ===\")\n",
    "            all_results[env_name] = {}\n",
    "            \n",
    "            for alg_name, alg_class, alg_kwargs in continuous_algorithms:\n",
    "                print(f\"\\nTesting {alg_name}...\")\n",
    "                try:\n",
    "                    learning_curves, all_rewards = self.run_multiple_seeds(\n",
    "                        alg_class, env_name, num_runs=3, num_episodes=150, **alg_kwargs\n",
    "                    )\n",
    "                    metrics = self.compute_metrics(learning_curves)\n",
    "                    all_results[env_name][alg_name] = metrics\n",
    "                    \n",
    "                    print(f\"  Final Performance: {metrics['final_performance_mean']:.2f} ± {metrics['final_performance_std']:.2f}\")\n",
    "                    print(f\"  Sample Efficiency: {metrics['sample_efficiency']} episodes\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error with {alg_name}: {e}\")\n",
    "                    all_results[env_name][alg_name] = None\n",
    "        \n",
    "        self.results = all_results\n",
    "        return all_results\n",
    "    \n",
    "    def visualize_comprehensive_results(self):\n",
    "        \"\"\"Create comprehensive visualizations\"\"\"\n",
    "        if not self.results:\n",
    "            print(\"No results to visualize. Run comprehensive comparison first.\")\n",
    "            return\n",
    "        \n",
    "        # Create subplots for different metrics\n",
    "        fig = plt.figure(figsize=(20, 16))\n",
    "        \n",
    "        # 1. Learning curves comparison\n",
    "        env_names = list(self.results.keys())\n",
    "        n_envs = len(env_names)\n",
    "        \n",
    "        for i, env_name in enumerate(env_names):\n",
    "            ax = plt.subplot(3, n_envs, i + 1)\n",
    "            \n",
    "            for alg_name, metrics in self.results[env_name].items():\n",
    "                if metrics is not None and 'learning_curves' in metrics:\n",
    "                    curves = metrics['learning_curves']\n",
    "                    mean_curve = np.mean(curves, axis=0)\n",
    "                    std_curve = np.std(curves, axis=0)\n",
    "                    \n",
    "                    x = range(len(mean_curve))\n",
    "                    ax.plot(x, mean_curve, label=alg_name, linewidth=2)\n",
    "                    ax.fill_between(x, mean_curve - std_curve, mean_curve + std_curve, alpha=0.2)\n",
    "            \n",
    "            ax.set_title(f'{env_name} - Learning Curves')\n",
    "            ax.set_xlabel('Episodes')\n",
    "            ax.set_ylabel('Episode Reward')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Final performance comparison\n",
    "        for i, env_name in enumerate(env_names):\n",
    "            ax = plt.subplot(3, n_envs, n_envs + i + 1)\n",
    "            \n",
    "            alg_names = []\n",
    "            final_means = []\n",
    "            final_stds = []\n",
    "            \n",
    "            for alg_name, metrics in self.results[env_name].items():\n",
    "                if metrics is not None:\n",
    "                    alg_names.append(alg_name)\n",
    "                    final_means.append(metrics['final_performance_mean'])\n",
    "                    final_stds.append(metrics['final_performance_std'])\n",
    "            \n",
    "            if alg_names:\n",
    "                bars = ax.bar(range(len(alg_names)), final_means, \n",
    "                             yerr=final_stds, capsize=5, alpha=0.7)\n",
    "                ax.set_title(f'{env_name} - Final Performance')\n",
    "                ax.set_xlabel('Algorithm')\n",
    "                ax.set_ylabel('Average Reward')\n",
    "                ax.set_xticks(range(len(alg_names)))\n",
    "                ax.set_xticklabels(alg_names, rotation=45)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, mean, std in zip(bars, final_means, final_stds):\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std,\n",
    "                           f'{mean:.1f}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        # 3. Sample efficiency comparison\n",
    "        for i, env_name in enumerate(env_names):\n",
    "            ax = plt.subplot(3, n_envs, 2*n_envs + i + 1)\n",
    "            \n",
    "            alg_names = []\n",
    "            sample_effs = []\n",
    "            \n",
    "            for alg_name, metrics in self.results[env_name].items():\n",
    "                if metrics is not None and metrics['sample_efficiency'] is not None:\n",
    "                    alg_names.append(alg_name)\n",
    "                    sample_effs.append(metrics['sample_efficiency'])\n",
    "            \n",
    "            if alg_names:\n",
    "                bars = ax.bar(range(len(alg_names)), sample_effs, alpha=0.7, color='green')\n",
    "                ax.set_title(f'{env_name} - Sample Efficiency')\n",
    "                ax.set_xlabel('Algorithm')\n",
    "                ax.set_ylabel('Episodes to Convergence')\n",
    "                ax.set_xticks(range(len(alg_names)))\n",
    "                ax.set_xticklabels(alg_names, rotation=45)\n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add value labels on bars\n",
    "                for bar, eff in zip(bars, sample_effs):\n",
    "                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "                           f'{eff}', ha='center', va='bottom', fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create summary table\n",
    "        self.create_summary_table()\n",
    "    \n",
    "    def create_summary_table(self):\n",
    "        \"\"\"Create a comprehensive summary table\"\"\"\n",
    "        print(\"\\n\" + \"=\"*100)\n",
    "        print(\"COMPREHENSIVE PERFORMANCE SUMMARY\")\n",
    "        print(\"=\"*100)\n",
    "        \n",
    "        for env_name, env_results in self.results.items():\n",
    "            print(f\"\\n{env_name}:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(f\"{'Algorithm':<15} {'Final Perf':<12} {'Std':<8} {'Sample Eff':<12} {'Stability':<10} {'Peak':<10}\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            for alg_name, metrics in env_results.items():\n",
    "                if metrics is not None:\n",
    "                    final_perf = f\"{metrics['final_performance_mean']:.2f}\"\n",
    "                    std = f\"{metrics['final_performance_std']:.2f}\"\n",
    "                    sample_eff = f\"{metrics['sample_efficiency']}\" if metrics['sample_efficiency'] is not None else \"N/A\"\n",
    "                    stability = f\"{metrics['learning_stability']:.3f}\"\n",
    "                    peak = f\"{metrics['peak_performance']:.2f}\"\n",
    "                    \n",
    "                    print(f\"{alg_name:<15} {final_perf:<12} {std:<8} {sample_eff:<12} {stability:<10} {peak:<10}\")\n",
    "                else:\n",
    "                    print(f\"{alg_name:<15} {'ERROR':<12} {'-':<8} {'-':<12} {'-':<10} {'-':<10}\")\n",
    "    \n",
    "    def hyperparameter_sensitivity_analysis(self):\n",
    "        \"\"\"Analyze hyperparameter sensitivity\"\"\"\n",
    "        print(\"\\n=== Hyperparameter Sensitivity Analysis ===\")\n",
    "        \n",
    "        # Test different learning rates for Actor-Critic\n",
    "        learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]\n",
    "        env_name = 'CartPole-v1'\n",
    "        \n",
    "        sensitivity_results = {}\n",
    "        \n",
    "        for lr in learning_rates:\n",
    "            print(f\"\\nTesting Actor-Critic with lr={lr}\")\n",
    "            \n",
    "            try:\n",
    "                learning_curves, _ = self.run_multiple_seeds(\n",
    "                    ActorCriticAgent, env_name, num_runs=3, num_episodes=100,\n",
    "                    actor_lr=lr, critic_lr=lr*10, gamma=0.99\n",
    "                )\n",
    "                metrics = self.compute_metrics(learning_curves)\n",
    "                sensitivity_results[lr] = metrics['final_performance_mean']\n",
    "                \n",
    "                print(f\"  Final Performance: {metrics['final_performance_mean']:.2f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error: {e}\")\n",
    "                sensitivity_results[lr] = None\n",
    "        \n",
    "        # Visualize sensitivity\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        valid_lrs = [lr for lr, perf in sensitivity_results.items() if perf is not None]\n",
    "        valid_perfs = [perf for lr, perf in sensitivity_results.items() if perf is not None]\n",
    "        \n",
    "        if valid_lrs:\n",
    "            plt.semilogx(valid_lrs, valid_perfs, 'bo-', linewidth=2, markersize=8)\n",
    "            plt.xlabel('Learning Rate')\n",
    "            plt.ylabel('Final Performance')\n",
    "            plt.title('Hyperparameter Sensitivity: Actor-Critic Learning Rate')\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Annotate best performance\n",
    "            best_idx = np.argmax(valid_perfs)\n",
    "            best_lr = valid_lrs[best_idx]\n",
    "            best_perf = valid_perfs[best_idx]\n",
    "            plt.annotate(f'Best: {best_lr}, {best_perf:.2f}', \n",
    "                        xy=(best_lr, best_perf), xytext=(best_lr*10, best_perf+10),\n",
    "                        arrowprops=dict(arrowstyle='->', color='red'))\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\nBest learning rate: {best_lr} with performance: {best_perf:.2f}\")\n",
    "        else:\n",
    "            print(\"No valid results for sensitivity analysis\")\n",
    "\n",
    "# Run comprehensive analysis\n",
    "print(\"Starting Comprehensive Performance Analysis...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "analyzer = PerformanceAnalyzer()\n",
    "results = analyzer.run_comprehensive_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab34e7",
   "metadata": {},
   "source": [
    "# Section 8: Practical Applications and Advanced Topics\n",
    "\n",
    "Policy gradient methods have found widespread application in numerous real-world domains. In this final section, we explore practical considerations, advanced techniques, and future directions in policy gradient research.\n",
    "\n",
    "## 8.1 Real-World Applications\n",
    "\n",
    "### 8.1.1 Robotics and Control\n",
    "- **Robotic Manipulation**: Policy gradients excel in high-dimensional continuous control tasks\n",
    "- **Locomotion**: Learning walking, running, and complex movement patterns\n",
    "- **Autonomous Vehicles**: Path planning and control in dynamic environments\n",
    "\n",
    "### 8.1.2 Game Playing and Strategy\n",
    "- **AlphaGo and AlphaZero**: Combining policy gradients with Monte Carlo Tree Search\n",
    "- **StarCraft II**: Managing complex strategy games with partial observability\n",
    "- **Multi-agent Systems**: Learning coordination and competition strategies\n",
    "\n",
    "### 8.1.3 Natural Language Processing\n",
    "- **Neural Machine Translation**: Learning translation policies with attention mechanisms\n",
    "- **Dialogue Systems**: Optimizing conversational agents for user satisfaction\n",
    "- **Text Generation**: Fine-tuning language models with human feedback (RLHF)\n",
    "\n",
    "### 8.1.4 Finance and Trading\n",
    "- **Portfolio Optimization**: Learning adaptive trading strategies\n",
    "- **Risk Management**: Dynamic hedging and exposure control\n",
    "- **Market Making**: Optimizing bid-ask spreads in electronic trading\n",
    "\n",
    "## 8.2 Advanced Techniques\n",
    "\n",
    "### 8.2.1 Trust Region Methods\n",
    "- **TRPO (Trust Region Policy Optimization)**: Ensuring stable policy updates\n",
    "- **Natural Policy Gradients**: Using the Fisher Information Matrix\n",
    "- **Conjugate Gradient Methods**: Efficient computation of natural gradients\n",
    "\n",
    "### 8.2.2 Off-Policy Learning\n",
    "- **Importance Sampling**: Learning from data collected by different policies\n",
    "- **Off-Policy Actor-Critic (Off-PAC)**: Combining off-policy learning with actor-critic\n",
    "- **Retrace and V-trace**: Advanced off-policy correction methods\n",
    "\n",
    "### 8.2.3 Meta-Learning and Transfer\n",
    "- **Model-Agnostic Meta-Learning (MAML)**: Learning to adapt quickly to new tasks\n",
    "- **Transfer Learning**: Leveraging learned policies across domains\n",
    "- **Multi-Task Learning**: Sharing representations across related tasks\n",
    "\n",
    "## 8.3 Current Challenges and Limitations\n",
    "\n",
    "### 8.3.1 Sample Efficiency\n",
    "- Policy gradients typically require many environment interactions\n",
    "- Exploration vs exploitation trade-offs in complex environments\n",
    "- Need for better variance reduction techniques\n",
    "\n",
    "### 8.3.2 Hyperparameter Sensitivity\n",
    "- Performance heavily dependent on learning rates and network architectures\n",
    "- Difficulty in transferring hyperparameters across environments\n",
    "- Need for adaptive and robust optimization methods\n",
    "\n",
    "### 8.3.3 Theoretical Understanding\n",
    "- Limited theoretical guarantees for convergence\n",
    "- Understanding of exploration properties\n",
    "- Generalization bounds and sample complexity analysis\n",
    "\n",
    "## 8.4 Future Directions\n",
    "\n",
    "### 8.4.1 Hierarchical Reinforcement Learning\n",
    "- Learning policies at multiple temporal scales\n",
    "- Options and temporal abstractions\n",
    "- Goal-conditioned reinforcement learning\n",
    "\n",
    "### 8.4.2 Safe and Constrained Policy Learning\n",
    "- Incorporating safety constraints into policy optimization\n",
    "- Risk-aware policy gradients\n",
    "- Robust policy learning under uncertainty\n",
    "\n",
    "### 8.4.3 Multi-Agent Policy Gradients\n",
    "- Learning in multi-agent environments\n",
    "- Cooperative and competitive scenarios\n",
    "- Communication and coordination mechanisms\n",
    "\n",
    "## 8.5 Implementation Best Practices\n",
    "\n",
    "### 8.5.1 Network Architecture Design\n",
    "- **Layer Normalization**: Improving training stability\n",
    "- **Residual Connections**: Facilitating gradient flow in deep networks\n",
    "- **Attention Mechanisms**: Handling variable-length sequences and partial observability\n",
    "\n",
    "### 8.5.2 Training Strategies\n",
    "- **Curriculum Learning**: Gradually increasing task difficulty\n",
    "- **Experience Replay**: Efficiently using collected experience\n",
    "- **Distributed Training**: Scaling to large-scale problems\n",
    "\n",
    "### 8.5.3 Debugging and Analysis\n",
    "- **Gradient Monitoring**: Detecting vanishing and exploding gradients\n",
    "- **Policy Visualization**: Understanding learned behaviors\n",
    "- **Ablation Studies**: Isolating the impact of different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Examples and Advanced Implementation\n",
    "\n",
    "class AdvancedPolicyGradientFramework:\n",
    "    \"\"\"\n",
    "    A comprehensive framework demonstrating advanced policy gradient techniques\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.algorithms = {}\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def demonstrate_curriculum_learning(self):\n",
    "        \"\"\"Demonstrate curriculum learning with gradually increasing difficulty\"\"\"\n",
    "        print(\"=== Curriculum Learning Demonstration ===\")\n",
    "        \n",
    "        # Create environments with increasing difficulty\n",
    "        environments = [\n",
    "            ('CartPole-v1', 'Easy: Basic cart-pole balancing'),\n",
    "            ('LunarLander-v2', 'Medium: Lunar lander with complex dynamics'),\n",
    "            ('BipedalWalker-v3', 'Hard: Bipedal walking (if available)')\n",
    "        ]\n",
    "        \n",
    "        # Progressive training\n",
    "        agent = None\n",
    "        cumulative_performance = []\n",
    "        \n",
    "        for i, (env_name, description) in enumerate(environments[:2]):  # Skip hard env for demo\n",
    "            print(f\"\\nStage {i+1}: {description}\")\n",
    "            \n",
    "            try:\n",
    "                env = gym.make(env_name)\n",
    "                \n",
    "                if hasattr(env.action_space, 'n'):  # Discrete\n",
    "                    state_dim = env.observation_space.shape[0]\n",
    "                    action_dim = env.action_space.n\n",
    "                    if agent is None:\n",
    "                        agent = A2CAgent(state_dim, action_dim, lr=1e-4, gamma=0.99)\n",
    "                    else:\n",
    "                        # Transfer learning: adapt to new environment\n",
    "                        old_actor = agent.shared_network.actor\n",
    "                        old_critic = agent.shared_network.critic\n",
    "                        \n",
    "                        # Create new network with same architecture but different output\n",
    "                        agent = A2CAgent(state_dim, action_dim, lr=1e-4, gamma=0.99)\n",
    "                        \n",
    "                        # Transfer lower layers (feature extractors)\n",
    "                        if hasattr(old_actor, 'state_dict'):\n",
    "                            # Copy compatible layers\n",
    "                            new_state_dict = agent.shared_network.state_dict()\n",
    "                            old_state_dict = old_actor.state_dict()\n",
    "                            \n",
    "                            for name, param in old_state_dict.items():\n",
    "                                if name in new_state_dict and new_state_dict[name].shape == param.shape:\n",
    "                                    new_state_dict[name] = param\n",
    "                            \n",
    "                            agent.shared_network.load_state_dict(new_state_dict)\n",
    "                \n",
    "                # Train on current environment\n",
    "                stage_rewards = []\n",
    "                num_episodes = 100 if i == 0 else 50  # Fewer episodes for later stages\n",
    "                \n",
    "                for episode in range(num_episodes):\n",
    "                    reward, _ = agent.train_episode(env)\n",
    "                    stage_rewards.append(reward)\n",
    "                    \n",
    "                    if (episode + 1) % 25 == 0:\n",
    "                        avg_reward = np.mean(stage_rewards[-25:])\n",
    "                        print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.2f}\")\n",
    "                \n",
    "                cumulative_performance.extend(stage_rewards)\n",
    "                env.close()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  Error with {env_name}: {e}\")\n",
    "        \n",
    "        # Visualize curriculum learning progress\n",
    "        if cumulative_performance:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            # Mark different stages\n",
    "            stage_boundaries = [100]  # After first 100 episodes\n",
    "            colors = ['blue', 'red', 'green']\n",
    "            stage_names = ['CartPole', 'LunarLander']\n",
    "            \n",
    "            current_pos = 0\n",
    "            for i, boundary in enumerate(stage_boundaries + [len(cumulative_performance)]):\n",
    "                stage_rewards = cumulative_performance[current_pos:boundary]\n",
    "                x_vals = range(current_pos, boundary)\n",
    "                \n",
    "                if stage_rewards:\n",
    "                    smoothed = pd.Series(stage_rewards).rolling(10).mean()\n",
    "                    plt.plot(x_vals, stage_rewards, alpha=0.3, color=colors[i])\n",
    "                    plt.plot(x_vals, smoothed, label=stage_names[i], color=colors[i], linewidth=2)\n",
    "                \n",
    "                if i < len(stage_boundaries):\n",
    "                    plt.axvline(boundary, color='black', linestyle='--', alpha=0.5)\n",
    "                    plt.text(boundary, plt.ylim()[1], f'Stage {i+2}', rotation=90, ha='right')\n",
    "                \n",
    "                current_pos = boundary\n",
    "            \n",
    "            plt.title('Curriculum Learning: Progressive Difficulty')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Episode Reward')\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        return cumulative_performance\n",
    "    \n",
    "    def demonstrate_ensemble_methods(self):\n",
    "        \"\"\"Demonstrate ensemble of policy gradient methods\"\"\"\n",
    "        print(\"\\n=== Ensemble Methods Demonstration ===\")\n",
    "        \n",
    "        # Create multiple agents with different hyperparameters\n",
    "        env_name = 'CartPole-v1'\n",
    "        env = gym.make(env_name)\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Ensemble of agents\n",
    "        ensemble_configs = [\n",
    "            {'lr': 1e-4, 'entropy_coef': 0.01, 'name': 'Conservative'},\n",
    "            {'lr': 5e-4, 'entropy_coef': 0.05, 'name': 'Balanced'},\n",
    "            {'lr': 1e-3, 'entropy_coef': 0.1, 'name': 'Aggressive'},\n",
    "        ]\n",
    "        \n",
    "        ensemble_agents = []\n",
    "        for config in ensemble_configs:\n",
    "            agent = A2CAgent(state_dim, action_dim, \n",
    "                           lr=config['lr'], \n",
    "                           entropy_coef=config['entropy_coef'],\n",
    "                           gamma=0.99)\n",
    "            agent.name = config['name']\n",
    "            ensemble_agents.append(agent)\n",
    "        \n",
    "        # Train ensemble\n",
    "        num_episodes = 100\n",
    "        ensemble_rewards = {agent.name: [] for agent in ensemble_agents}\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            for agent in ensemble_agents:\n",
    "                reward, _ = agent.train_episode(env)\n",
    "                ensemble_rewards[agent.name].append(reward)\n",
    "            \n",
    "            if (episode + 1) % 25 == 0:\n",
    "                print(f\"Episode {episode+1}:\")\n",
    "                for agent in ensemble_agents:\n",
    "                    avg_reward = np.mean(ensemble_rewards[agent.name][-25:])\n",
    "                    print(f\"  {agent.name}: {avg_reward:.2f}\")\n",
    "        \n",
    "        # Ensemble prediction (majority voting for discrete actions)\n",
    "        def ensemble_select_action(state):\n",
    "            \"\"\"Select action using ensemble voting\"\"\"\n",
    "            votes = []\n",
    "            \n",
    "            for agent in ensemble_agents:\n",
    "                action_probs = agent.get_action_probabilities(state)\n",
    "                votes.append(action_probs)\n",
    "            \n",
    "            # Average probabilities\n",
    "            ensemble_probs = np.mean(votes, axis=0)\n",
    "            return np.argmax(ensemble_probs)\n",
    "        \n",
    "        # Test ensemble vs individual agents\n",
    "        test_episodes = 10\n",
    "        test_results = {}\n",
    "        \n",
    "        for agent in ensemble_agents:\n",
    "            test_rewards = []\n",
    "            for _ in range(test_episodes):\n",
    "                state, _ = env.reset()\n",
    "                total_reward = 0\n",
    "                \n",
    "                for _ in range(500):\n",
    "                    action, _ = agent.select_action(state, deterministic=True)\n",
    "                    state, reward, terminated, truncated, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    \n",
    "                    if terminated or truncated:\n",
    "                        break\n",
    "                \n",
    "                test_rewards.append(total_reward)\n",
    "            \n",
    "            test_results[agent.name] = np.mean(test_rewards)\n",
    "        \n",
    "        # Visualize ensemble performance\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        # Learning curves\n",
    "        plt.subplot(1, 3, 1)\n",
    "        for agent_name, rewards in ensemble_rewards.items():\n",
    "            smoothed = pd.Series(rewards).rolling(10).mean()\n",
    "            plt.plot(smoothed, label=agent_name, linewidth=2)\n",
    "        \n",
    "        plt.title('Ensemble Learning Curves')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Final performance comparison\n",
    "        plt.subplot(1, 3, 2)\n",
    "        final_performances = [np.mean(rewards[-25:]) for rewards in ensemble_rewards.values()]\n",
    "        agent_names = list(ensemble_rewards.keys())\n",
    "        \n",
    "        bars = plt.bar(agent_names, final_performances, alpha=0.7)\n",
    "        plt.title('Final Training Performance')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, value in zip(bars, final_performances):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                    f'{value:.1f}', ha='center', va='bottom')\n",
    "        \n",
    "        # Test performance comparison\n",
    "        plt.subplot(1, 3, 3)\n",
    "        test_performances = list(test_results.values())\n",
    "        \n",
    "        bars = plt.bar(agent_names, test_performances, alpha=0.7, color='green')\n",
    "        plt.title('Test Performance')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        for bar, value in zip(bars, test_performances):\n",
    "            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n",
    "                    f'{value:.1f}', ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        env.close()\n",
    "        \n",
    "        print(f\"\\nEnsemble Results:\")\n",
    "        for name, performance in test_results.items():\n",
    "            print(f\"  {name}: {performance:.2f}\")\n",
    "        \n",
    "        return ensemble_agents, test_results\n",
    "    \n",
    "    def analyze_policy_interpretability(self):\n",
    "        \"\"\"Analyze and visualize learned policies\"\"\"\n",
    "        print(\"\\n=== Policy Interpretability Analysis ===\")\n",
    "        \n",
    "        # Train a simple agent for analysis\n",
    "        env = gym.make('CartPole-v1')\n",
    "        state_dim = env.observation_space.shape[0]\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        agent = ActorCriticAgent(state_dim, action_dim, \n",
    "                               actor_lr=1e-3, critic_lr=1e-2, gamma=0.99)\n",
    "        \n",
    "        # Quick training\n",
    "        for episode in range(100):\n",
    "            agent.train_episode(env)\n",
    "        \n",
    "        # Analyze policy behavior across state space\n",
    "        print(\"Analyzing policy across state space...\")\n",
    "        \n",
    "        # Sample different states\n",
    "        test_states = []\n",
    "        cart_positions = np.linspace(-2.4, 2.4, 5)\n",
    "        cart_velocities = np.linspace(-3, 3, 3)\n",
    "        pole_angles = np.linspace(-0.2, 0.2, 5)\n",
    "        pole_velocities = np.linspace(-2, 2, 3)\n",
    "        \n",
    "        for pos in cart_positions:\n",
    "            for vel in cart_velocities[:2]:  # Reduce combinations\n",
    "                for angle in pole_angles:\n",
    "                    for ang_vel in pole_velocities[:2]:\n",
    "                        test_states.append([pos, vel, angle, ang_vel])\n",
    "        \n",
    "        # Get policy predictions\n",
    "        policy_actions = []\n",
    "        value_predictions = []\n",
    "        \n",
    "        for state in test_states:\n",
    "            action_probs = agent.get_action_probabilities(state)\n",
    "            value = agent.get_value(state)\n",
    "            \n",
    "            policy_actions.append(np.argmax(action_probs))\n",
    "            value_predictions.append(value)\n",
    "        \n",
    "        # Visualize policy decisions\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # 1. Policy decisions vs cart position and pole angle\n",
    "        positions = [state[0] for state in test_states]\n",
    "        angles = [state[2] for state in test_states]\n",
    "        actions = policy_actions\n",
    "        \n",
    "        # Create 2D grid for visualization\n",
    "        pos_unique = sorted(set(positions))\n",
    "        angle_unique = sorted(set(angles))\n",
    "        \n",
    "        if len(pos_unique) > 1 and len(angle_unique) > 1:\n",
    "            policy_grid = np.zeros((len(angle_unique), len(pos_unique)))\n",
    "            value_grid = np.zeros((len(angle_unique), len(pos_unique)))\n",
    "            \n",
    "            for i, state in enumerate(test_states):\n",
    "                if len(set([state[1], state[3]])) <= 2:  # Only use states with specific vel values\n",
    "                    try:\n",
    "                        pos_idx = pos_unique.index(state[0])\n",
    "                        angle_idx = angle_unique.index(state[2])\n",
    "                        policy_grid[angle_idx, pos_idx] = actions[i]\n",
    "                        value_grid[angle_idx, pos_idx] = value_predictions[i]\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "            \n",
    "            # Plot policy heatmap\n",
    "            im1 = axes[0,0].imshow(policy_grid, cmap='coolwarm', aspect='auto')\n",
    "            axes[0,0].set_title('Policy Decisions\\n(0=Left, 1=Right)')\n",
    "            axes[0,0].set_xlabel('Cart Position')\n",
    "            axes[0,0].set_ylabel('Pole Angle')\n",
    "            axes[0,0].set_xticks(range(len(pos_unique)))\n",
    "            axes[0,0].set_xticklabels([f'{pos:.1f}' for pos in pos_unique])\n",
    "            axes[0,0].set_yticks(range(len(angle_unique)))\n",
    "            axes[0,0].set_yticklabels([f'{angle:.2f}' for angle in angle_unique])\n",
    "            plt.colorbar(im1, ax=axes[0,0])\n",
    "            \n",
    "            # Plot value function\n",
    "            im2 = axes[0,1].imshow(value_grid, cmap='viridis', aspect='auto')\n",
    "            axes[0,1].set_title('Value Function')\n",
    "            axes[0,1].set_xlabel('Cart Position')\n",
    "            axes[0,1].set_ylabel('Pole Angle')\n",
    "            axes[0,1].set_xticks(range(len(pos_unique)))\n",
    "            axes[0,1].set_xticklabels([f'{pos:.1f}' for pos in pos_unique])\n",
    "            axes[0,1].set_yticks(range(len(angle_unique)))\n",
    "            axes[0,1].set_yticklabels([f'{angle:.2f}' for angle in angle_unique])\n",
    "            plt.colorbar(im2, ax=axes[0,1])\n",
    "        \n",
    "        # 2. Action probability distribution\n",
    "        sample_states = test_states[:20]  # Use first 20 states\n",
    "        action_probs_list = []\n",
    "        \n",
    "        for state in sample_states:\n",
    "            probs = agent.get_action_probabilities(state)\n",
    "            action_probs_list.append(probs)\n",
    "        \n",
    "        action_probs_array = np.array(action_probs_list)\n",
    "        \n",
    "        axes[1,0].boxplot([action_probs_array[:, 0], action_probs_array[:, 1]], \n",
    "                         labels=['Left (0)', 'Right (1)'])\n",
    "        axes[1,0].set_title('Action Probability Distributions')\n",
    "        axes[1,0].set_ylabel('Probability')\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Value function distribution\n",
    "        axes[1,1].hist(value_predictions, bins=20, alpha=0.7, color='green')\n",
    "        axes[1,1].set_title('Value Function Distribution')\n",
    "        axes[1,1].set_xlabel('State Value')\n",
    "        axes[1,1].set_ylabel('Frequency')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Policy consistency analysis\n",
    "        print(f\"Policy Analysis:\")\n",
    "        print(f\"  Total states analyzed: {len(test_states)}\")\n",
    "        print(f\"  Action distribution: Left={policy_actions.count(0)}, Right={policy_actions.count(1)}\")\n",
    "        print(f\"  Value range: [{min(value_predictions):.2f}, {max(value_predictions):.2f}]\")\n",
    "        print(f\"  Average value: {np.mean(value_predictions):.2f}\")\n",
    "        \n",
    "        env.close()\n",
    "        return agent, test_states, policy_actions, value_predictions\n",
    "\n",
    "# Run advanced demonstrations\n",
    "framework = AdvancedPolicyGradientFramework()\n",
    "\n",
    "print(\"Running Advanced Policy Gradient Demonstrations...\")\n",
    "print(\"This showcases practical applications and advanced techniques.\")\n",
    "\n",
    "# Demonstrate curriculum learning\n",
    "curriculum_results = framework.demonstrate_curriculum_learning()\n",
    "\n",
    "# Demonstrate ensemble methods\n",
    "ensemble_agents, ensemble_results = framework.demonstrate_ensemble_methods()\n",
    "\n",
    "# Analyze policy interpretability\n",
    "policy_analysis = framework.analyze_policy_interpretability()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e9428",
   "metadata": {},
   "source": [
    "# Conclusion: Mastering Policy Gradient Methods\n",
    "\n",
    "This comprehensive notebook has provided a thorough exploration of policy gradient methods in deep reinforcement learning. We have covered the theoretical foundations, implemented key algorithms, and analyzed their performance across various scenarios.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Theoretical Understanding\n",
    "- **Policy Gradient Theorem**: The fundamental principle enabling direct policy optimization\n",
    "- **Variance-Bias Trade-offs**: Understanding the importance of baseline methods and control variates\n",
    "- **Continuous vs Discrete**: Different parameterizations and optimization challenges\n",
    "\n",
    "### 2. Algorithm Mastery\n",
    "- **REINFORCE**: The foundation of policy gradient methods with Monte Carlo sampling\n",
    "- **Actor-Critic**: Combining policy and value learning for reduced variance\n",
    "- **Advanced Methods**: A2C, A3C, PPO for improved stability and efficiency\n",
    "- **Continuous Control**: Gaussian policies and specialized techniques for continuous action spaces\n",
    "\n",
    "### 3. Practical Implementation\n",
    "- **Variance Reduction**: GAE, baselines, and control variates for stable learning\n",
    "- **Network Architectures**: Shared vs separate networks, normalization techniques\n",
    "- **Hyperparameter Sensitivity**: Understanding the impact of learning rates, entropy coefficients, and discount factors\n",
    "\n",
    "### 4. Performance Analysis\n",
    "- **Sample Efficiency**: Policy gradients typically require many environment interactions\n",
    "- **Stability**: Importance of gradient clipping, learning rate scheduling, and regularization\n",
    "- **Generalization**: Transfer learning and ensemble methods for robust performance\n",
    "\n",
    "## Best Practices Summary\n",
    "\n",
    "### Implementation Guidelines\n",
    "1. **Start Simple**: Begin with REINFORCE or basic Actor-Critic before moving to complex methods\n",
    "2. **Variance Reduction**: Always implement baseline methods and consider advanced techniques like GAE\n",
    "3. **Network Design**: Use appropriate architectures with normalization and regularization\n",
    "4. **Hyperparameter Tuning**: Start with conservative learning rates and gradually adjust\n",
    "5. **Monitoring**: Track both policy and value losses, along with entropy for exploration analysis\n",
    "\n",
    "### Debugging Strategies\n",
    "1. **Gradient Analysis**: Monitor gradient norms to detect vanishing/exploding gradients\n",
    "2. **Policy Visualization**: Analyze learned behaviors to ensure reasonable policies\n",
    "3. **Statistical Validation**: Run multiple seeds and analyze variance in performance\n",
    "4. **Ablation Studies**: Isolate the impact of different components\n",
    "\n",
    "### When to Use Policy Gradients\n",
    "- **High-dimensional action spaces**: Particularly effective for continuous control\n",
    "- **Stochastic policies**: When you need probabilistic action selection\n",
    "- **Direct policy optimization**: When model-free learning is preferred\n",
    "- **Partial observability**: Can handle complex observation spaces effectively\n",
    "\n",
    "## Future Learning Directions\n",
    "\n",
    "### Advanced Topics to Explore\n",
    "1. **Meta-Learning**: MAML and other techniques for rapid adaptation\n",
    "2. **Multi-Agent RL**: Policy gradients in competitive and cooperative settings\n",
    "3. **Hierarchical RL**: Learning policies at multiple temporal scales\n",
    "4. **Safe RL**: Incorporating constraints and safety considerations\n",
    "\n",
    "### Research Frontiers\n",
    "1. **Sample Efficiency**: Developing more efficient exploration strategies\n",
    "2. **Theoretical Guarantees**: Better understanding of convergence properties\n",
    "3. **Robustness**: Learning policies that generalize across environments\n",
    "4. **Human-AI Interaction**: Incorporating human feedback and preferences\n",
    "\n",
    "## Final Recommendations\n",
    "\n",
    "Policy gradient methods represent a powerful and flexible approach to reinforcement learning. While they can be challenging to tune and may require significant computational resources, they offer unique advantages in terms of handling complex action spaces and learning stochastic policies.\n",
    "\n",
    "The key to success with policy gradients lies in:\n",
    "1. **Understanding the theoretical foundations**\n",
    "2. **Implementing robust variance reduction techniques**\n",
    "3. **Careful hyperparameter tuning and monitoring**\n",
    "4. **Systematic evaluation and analysis**\n",
    "\n",
    "By mastering these concepts and techniques, you'll be well-equipped to tackle a wide range of reinforcement learning problems using policy gradient methods.\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook serves as a comprehensive guide to policy gradient methods in deep reinforcement learning. The implementations provided are educational in nature and can be extended for more complex applications. Always remember to validate your implementations thoroughly and consider the specific requirements of your problem domain.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comprehensive Demonstration\n",
    "# Run this cell to execute all major demonstrations from the notebook\n",
    "\n",
    "def run_comprehensive_demo():\n",
    "    \"\"\"\n",
    "    Execute a comprehensive demonstration of all policy gradient methods\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE POLICY GRADIENT METHODS DEMONSTRATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # 1. Basic Policy Gradient Visualization\n",
    "        print(\"\\n1. Policy Gradient Fundamentals...\")\n",
    "        visualizer = PolicyGradientVisualization()\n",
    "        visualizer.demonstrate_policy_gradient_theorem()\n",
    "        \n",
    "        # 2. REINFORCE Algorithm\n",
    "        print(\"\\n2. REINFORCE Algorithm Demonstration...\")\n",
    "        reinforce_results = demonstrate_reinforce()\n",
    "        \n",
    "        # 3. Actor-Critic Methods\n",
    "        print(\"\\n3. Actor-Critic Methods...\")\n",
    "        ac_results = demonstrate_actor_critic()\n",
    "        \n",
    "        # 4. Advanced Methods (A2C/A3C)\n",
    "        print(\"\\n4. Advanced A2C/A3C Methods...\")\n",
    "        advanced_results = demonstrate_advanced_methods()\n",
    "        \n",
    "        # 5. Variance Reduction Techniques\n",
    "        print(\"\\n5. Variance Reduction Analysis...\")\n",
    "        variance_results = demonstrate_variance_reduction()\n",
    "        \n",
    "        # 6. Continuous Control\n",
    "        print(\"\\n6. Continuous Control Methods...\")\n",
    "        continuous_results = test_continuous_control()\n",
    "        \n",
    "        # 7. Performance Analysis (Quick version)\n",
    "        print(\"\\n7. Performance Analysis...\")\n",
    "        analyzer = PerformanceAnalyzer()\n",
    "        \n",
    "        # Quick performance comparison on CartPole\n",
    "        quick_results = {}\n",
    "        env_name = 'CartPole-v1'\n",
    "        algorithms = [\n",
    "            ('REINFORCE', REINFORCEAgent, {'lr': 1e-3, 'gamma': 0.99}),\n",
    "            ('Actor-Critic', ActorCriticAgent, {'actor_lr': 1e-4, 'critic_lr': 1e-3, 'gamma': 0.99}),\n",
    "        ]\n",
    "        \n",
    "        for alg_name, alg_class, kwargs in algorithms:\n",
    "            print(f\"  Testing {alg_name}...\")\n",
    "            try:\n",
    "                curves, rewards = analyzer.run_multiple_seeds(\n",
    "                    alg_class, env_name, num_runs=2, num_episodes=50, **kwargs\n",
    "                )\n",
    "                metrics = analyzer.compute_metrics(curves)\n",
    "                quick_results[alg_name] = metrics\n",
    "                print(f\"    Final Performance: {metrics['final_performance_mean']:.2f}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error: {e}\")\n",
    "                quick_results[alg_name] = None\n",
    "        \n",
    "        # 8. Summary Visualization\n",
    "        print(\"\\n8. Creating Summary Visualization...\")\n",
    "        \n",
    "        # Collect all results for final comparison\n",
    "        plt.figure(figsize=(20, 12))\n",
    "        \n",
    "        # Plot 1: Learning curves comparison\n",
    "        plt.subplot(2, 3, 1)\n",
    "        if 'reinforce_rewards' in locals():\n",
    "            smoothed_reinforce = pd.Series(reinforce_results['rewards']).rolling(10).mean()\n",
    "            plt.plot(smoothed_reinforce, label='REINFORCE', linewidth=2)\n",
    "        \n",
    "        if 'ac_rewards' in locals():\n",
    "            smoothed_ac = pd.Series(ac_results['rewards']).rolling(10).mean()\n",
    "            plt.plot(smoothed_ac, label='Actor-Critic', linewidth=2)\n",
    "        \n",
    "        plt.title('Learning Curves Comparison')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Algorithm complexity\n",
    "        plt.subplot(2, 3, 2)\n",
    "        algorithms_complexity = {\n",
    "            'REINFORCE': 1,\n",
    "            'Actor-Critic': 2,\n",
    "            'A2C': 3,\n",
    "            'A3C': 4,\n",
    "            'PPO': 5\n",
    "        }\n",
    "        \n",
    "        alg_names = list(algorithms_complexity.keys())\n",
    "        complexity_scores = list(algorithms_complexity.values())\n",
    "        \n",
    "        bars = plt.bar(alg_names, complexity_scores, alpha=0.7, color='skyblue')\n",
    "        plt.title('Algorithm Complexity')\n",
    "        plt.ylabel('Relative Complexity')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Performance summary\n",
    "        plt.subplot(2, 3, 3)\n",
    "        if quick_results:\n",
    "            perf_names = []\n",
    "            perf_values = []\n",
    "            for name, metrics in quick_results.items():\n",
    "                if metrics is not None:\n",
    "                    perf_names.append(name)\n",
    "                    perf_values.append(metrics['final_performance_mean'])\n",
    "            \n",
    "            if perf_names:\n",
    "                plt.bar(perf_names, perf_values, alpha=0.7, color='green')\n",
    "                plt.title('Final Performance Comparison')\n",
    "                plt.ylabel('Average Reward')\n",
    "                plt.xticks(rotation=45)\n",
    "                plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Key insights\n",
    "        plt.subplot(2, 3, 4)\n",
    "        insights = {\n",
    "            'Sample\\nEfficiency': 3,\n",
    "            'Stability': 4,\n",
    "            'Complexity': 3,\n",
    "            'Continuous\\nControl': 5,\n",
    "            'Scalability': 4\n",
    "        }\n",
    "        \n",
    "        plt.bar(insights.keys(), insights.values(), alpha=0.7, color='orange')\n",
    "        plt.title('Policy Gradient Strengths')\n",
    "        plt.ylabel('Rating (1-5)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 5: Applications domains\n",
    "        plt.subplot(2, 3, 5)\n",
    "        domains = ['Robotics', 'Gaming', 'Finance', 'NLP', 'Control']\n",
    "        applicability = [5, 5, 4, 4, 5]\n",
    "        \n",
    "        plt.bar(domains, applicability, alpha=0.7, color='purple')\n",
    "        plt.title('Application Domains')\n",
    "        plt.ylabel('Applicability (1-5)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 6: Future directions\n",
    "        plt.subplot(2, 3, 6)\n",
    "        directions = ['Meta-Learning', 'Multi-Agent', 'Safe RL', 'Hierarchical', 'Offline RL']\n",
    "        importance = [4, 5, 5, 4, 4]\n",
    "        \n",
    "        plt.bar(directions, importance, alpha=0.7, color='red')\n",
    "        plt.title('Future Research Directions')\n",
    "        plt.ylabel('Importance (1-5)')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Final Summary\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"DEMONSTRATION COMPLETE!\")\n",
    "        print(\"=\"*80)\n",
    "        print(\"\\nKey Achievements:\")\n",
    "        print(\"✓ Implemented complete policy gradient framework\")\n",
    "        print(\"✓ Demonstrated REINFORCE, Actor-Critic, A2C/A3C, and PPO\")\n",
    "        print(\"✓ Analyzed variance reduction techniques\")\n",
    "        print(\"✓ Explored continuous action spaces\")\n",
    "        print(\"✓ Conducted performance comparisons\")\n",
    "        print(\"✓ Showcased practical applications\")\n",
    "        \n",
    "        print(\"\\nNext Steps:\")\n",
    "        print(\"• Experiment with different environments\")\n",
    "        print(\"• Tune hyperparameters for your specific problems\")\n",
    "        print(\"• Explore advanced techniques like TRPO and SAC\")\n",
    "        print(\"• Apply to real-world problems in your domain\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in comprehensive demo: {e}\")\n",
    "        print(\"Please run individual sections to identify specific issues.\")\n",
    "\n",
    "# Execute the comprehensive demonstration\n",
    "print(\"Starting Comprehensive Policy Gradient Methods Demonstration...\")\n",
    "print(\"This will run all major examples from the notebook.\")\n",
    "print(\"Please be patient as this may take several minutes to complete.\")\n",
    "\n",
    "run_comprehensive_demo()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
