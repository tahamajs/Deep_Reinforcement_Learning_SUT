{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68893df",
   "metadata": {},
   "source": [
    "# Computer Assignment 14: Advanced Deep Reinforcement Learning\n",
    "\n",
    "## Topics Covered:\n",
    "- **Offline Reinforcement Learning**: Learning from static datasets\n",
    "- **Safe Reinforcement Learning**: Constraint satisfaction and risk management\n",
    "- **Multi-Agent Reinforcement Learning**: Coordination and competition\n",
    "- **Robust Reinforcement Learning**: Handling uncertainty and adversarial conditions\n",
    "- **Real-World Applications**: Practical deployment considerations\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Understand advanced RL paradigms beyond standard online learning\n",
    "2. Implement offline RL algorithms for batch learning scenarios\n",
    "3. Design safe RL agents with constraint satisfaction\n",
    "4. Create multi-agent systems with coordination mechanisms\n",
    "5. Build robust agents that handle uncertainty and distribution shifts\n",
    "6. Apply advanced RL techniques to real-world scenarios\n",
    "\n",
    "---\n",
    "**Assignment Date**: Advanced Deep RL - Lesson 14  \n",
    "**Estimated Time**: 3-4 hours  \n",
    "**Difficulty**: Advanced  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45240018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "ðŸš€ Advanced Deep RL Environment Initialized!\n",
      "ðŸ“š Topics: Offline RL, Safe RL, Multi-Agent RL, Robust RL\n",
      "ðŸ”¬ Ready for advanced reinforcement learning research and implementation!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Normal, Categorical, MultivariateNormal\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import deque, namedtuple\nimport random\nimport copy\nimport gym\nfrom typing import List, Dict, Tuple, Optional, Union\nimport warnings\nwarnings.filterwarnings('ignore')\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\nplt.style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nOFFLINE_RL_CONFIG = {\n    'batch_size': 256,\n    'buffer_size': 100000,\n    'conservative_weight': 1.0,\n    'behavior_cloning_weight': 0.1\n}\nSAFE_RL_CONFIG = {\n    'constraint_threshold': 0.1,\n    'lagrange_lr': 1e-3,\n    'penalty_weight': 10.0,\n    'safety_buffer_size': 10000\n}\nMULTI_AGENT_CONFIG = {\n    'num_agents': 4,\n    'communication_dim': 16,\n    'centralized_critic': True,\n    'shared_experience': False\n}\nROBUST_RL_CONFIG = {\n    'domain_randomization': True,\n    'adversarial_training': True,\n    'uncertainty_estimation': True,\n    'robust_loss_weight': 0.5\n}\nprint(\"ðŸš€ Advanced Deep RL Environment Initialized!\")\nprint(\"ðŸ“š Topics: Offline RL, Safe RL, Multi-Agent RL, Robust RL\")\nprint(\"ðŸ”¬ Ready for advanced reinforcement learning research and implementation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f596eab",
   "metadata": {},
   "source": [
    "# Section 1: Offline Reinforcement Learning\n",
    "\n",
    "## 1.1 Theory: Learning from Static Datasets\n",
    "\n",
    "Offline Reinforcement Learning (also known as **Batch RL** or **Data-Driven RL**) addresses the challenge of learning optimal policies from pre-collected datasets without further environment interaction. This paradigm is crucial for real-world applications where online exploration is expensive, dangerous, or impossible.\n",
    "\n",
    "### Key Challenges in Offline RL\n",
    "\n",
    "#### 1. Distribution Shift Problem\n",
    "The fundamental challenge in offline RL is the **distributional shift** between the behavior policy that generated the data and the learned policy:\n",
    "- **Behavior Policy**: $\\pi_\\beta(a|s)$ - Policy that collected the dataset\n",
    "- **Learned Policy**: $\\pi(a|s)$ - Policy we want to optimize\n",
    "- **Distribution Mismatch**: $\\pi(a|s) \\neq \\pi_\\beta(a|s)$ leads to extrapolation errors\n",
    "\n",
    "#### 2. Overestimation Bias\n",
    "Standard off-policy methods suffer from **overestimation bias** in offline settings:\n",
    "$$Q(s,a) = \\mathbb{E}[r + \\gamma \\max_{a'} Q(s', a')] \\text{ (overestimates for unseen actions)}$$\n",
    "\n",
    "#### 3. Coverage Problem\n",
    "Limited dataset coverage leads to poor generalization:\n",
    "- **Good Coverage**: Dataset contains diverse state-action pairs\n",
    "- **Poor Coverage**: Dataset is narrow, missing important regions\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### Offline RL Objective\n",
    "The goal is to maximize expected return using only offline data $\\mathcal{D} = \\{(s_i, a_i, r_i, s'_i)\\}_{i=1}^N$:\n",
    "$$J(\\pi) = \\mathbb{E}_{\\pi, \\mathcal{D}}[\\sum_{t=0}^T \\gamma^t r_t] \\text{ subject to } (s,a) \\in \\text{support}(\\mathcal{D})$$\n",
    "\n",
    "#### Conservative Q-Learning (CQL) Objective\n",
    "CQL addresses overestimation by adding a conservative penalty:\n",
    "$$\\mathcal{L}_{CQL}(Q) = \\alpha \\mathbb{E}_{s \\sim \\mathcal{D}}\\left[\\log \\sum_a \\exp Q(s,a) - \\mathbb{E}_{a \\sim \\pi_\\beta(a|s)}[Q(s,a)]\\right] + \\mathcal{L}_{Bellman}(Q)$$\n",
    "\n",
    "Where:\n",
    "- **Conservative Term**: Penalizes high Q-values for out-of-distribution actions\n",
    "- **Bellman Loss**: Standard temporal difference learning objective\n",
    "- **$\\alpha$**: Conservative weight hyperparameter\n",
    "\n",
    "#### Behavior Cloning Regularization\n",
    "Many offline RL methods incorporate behavior cloning to stay close to the data distribution:\n",
    "$$\\mathcal{L}_{BC}(\\pi) = \\mathbb{E}_{(s,a) \\sim \\mathcal{D}}[-\\log \\pi(a|s)]$$\n",
    "\n",
    "## 1.2 Advanced Offline RL Algorithms\n",
    "\n",
    "### 1. Conservative Q-Learning (CQL)\n",
    "- **Idea**: Lower-bound Q-values for unseen actions while fitting seen data\n",
    "- **Advantage**: Prevents overestimation bias effectively\n",
    "- **Use Case**: High-dimensional continuous control tasks\n",
    "\n",
    "### 2. Implicit Q-Learning (IQL)\n",
    "- **Idea**: Avoid explicit policy improvement, use implicit Q-function updates\n",
    "- **Advantage**: More stable than explicit policy optimization\n",
    "- **Use Case**: Mixed-quality datasets with suboptimal trajectories\n",
    "\n",
    "### 3. Advantage-Weighted Regression (AWR)\n",
    "- **Idea**: Weight behavior cloning by advantage estimates\n",
    "- **Advantage**: Simple and effective for good-quality datasets\n",
    "- **Use Case**: Near-optimal demonstration datasets\n",
    "\n",
    "### 4. Batch-Constrained Deep Q-Learning (BCQ)\n",
    "- **Idea**: Constrain policy to stay close to behavior policy\n",
    "- **Advantage**: Explicit distribution constraint\n",
    "- **Use Case**: Discrete action spaces with coverage issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd561c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Generating Offline Datasets...\n",
      "\n",
      "ðŸ“Š Expert Dataset:\n",
      "  Size: 80000\n",
      "  Average Reward: 0.037 Â± 0.364\n",
      "  State Dim: 2\n",
      "  Action Distribution: [0.5 0.  0.  0.5]\n",
      "\n",
      "ðŸ“Š Mixed Dataset:\n",
      "  Size: 167806\n",
      "  Average Reward: -0.002 Â± 0.314\n",
      "  State Dim: 2\n",
      "  Action Distribution: [0.39439591 0.07460997 0.07444311 0.45655102]\n",
      "\n",
      "ðŸ“Š Random Dataset:\n",
      "  Size: 352316\n",
      "  Average Reward: -0.092 Â± 0.093\n",
      "  State Dim: 2\n",
      "  Action Distribution: [0.24986092 0.24906334 0.25125172 0.24982402]\n",
      "\n",
      "âœ… Offline datasets generated successfully!\n",
      "ðŸ”„ Ready for Conservative Q-Learning and Implicit Q-Learning training...\n"
     ]
    }
   ],
   "source": [
    "class OfflineDataset:\n    def __init__(self, states, actions, rewards, next_states, dones, dataset_type='mixed'):\n        self.states = np.array(states)\n        self.actions = np.array(actions)\n        self.rewards = np.array(rewards)\n        self.next_states = np.array(next_states)\n        self.dones = np.array(dones)\n        self.dataset_type = dataset_type\n        self.size = len(states)\n        self.reward_mean = np.mean(rewards)\n        self.reward_std = np.std(rewards)\n        self.state_mean = np.mean(states, axis=0)\n        self.state_std = np.std(states, axis=0) + 1e-8\n        self.normalize_dataset()\n    def normalize_dataset(self):\n        self.states = (self.states - self.state_mean) / self.state_std\n        self.next_states = (self.next_states - self.state_mean) / self.state_std\n        self.rewards = (self.rewards - self.reward_mean) / (self.reward_std + 1e-8)\n    def sample_batch(self, batch_size):\n        indices = np.random.randint(0, self.size, batch_size)\n        batch_states = torch.FloatTensor(self.states[indices]).to(device)\n        batch_actions = torch.LongTensor(self.actions[indices]).to(device)\n        batch_rewards = torch.FloatTensor(self.rewards[indices]).to(device)\n        batch_next_states = torch.FloatTensor(self.next_states[indices]).to(device)\n        batch_dones = torch.BoolTensor(self.dones[indices]).to(device)\n        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones\n    def get_action_distribution(self):\n        if len(self.actions.shape) == 1:\n            action_counts = np.bincount(self.actions)\n            return action_counts / self.size\n        else:\n            return np.mean(self.actions, axis=0), np.std(self.actions, axis=0)\nclass ConservativeQNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.q_network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n        self.value_network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def forward(self, state):\n        q_values = self.q_network(state)\n        state_value = self.value_network(state)\n        return q_values, state_value\n    def get_q_values(self, state):\n        q_values, _ = self.forward(state)\n        return q_values\nclass ConservativeQLearning:\n    def __init__(self, state_dim, action_dim, lr=3e-4, conservative_weight=1.0):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.conservative_weight = conservative_weight\n        self.q_network = ConservativeQNetwork(state_dim, action_dim).to(device)\n        self.target_q_network = copy.deepcopy(self.q_network).to(device)\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n        self.gamma = 0.99\n        self.tau = 0.005\n        self.update_count = 0\n        self.losses = []\n        self.conservative_losses = []\n        self.bellman_losses = []\n    def compute_conservative_loss(self, states, actions):\n        q_values, _ = self.q_network(states)\n        logsumexp_q = torch.logsumexp(q_values, dim=1)\n        behavior_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n        conservative_loss = (logsumexp_q - behavior_q_values).mean()\n        return conservative_loss\n    def compute_bellman_loss(self, states, actions, rewards, next_states, dones):\n        q_values, _ = self.q_network(states)\n        current_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n        with torch.no_grad():\n            next_q_values, _ = self.target_q_network(next_states)\n            max_next_q_values = next_q_values.max(1)[0]\n            target_q_values = rewards + (self.gamma * max_next_q_values * (~dones))\n        bellman_loss = F.mse_loss(current_q_values, target_q_values)\n        return bellman_loss\n    def update(self, batch):\n        states, actions, rewards, next_states, dones = batch\n        conservative_loss = self.compute_conservative_loss(states, actions)\n        bellman_loss = self.compute_bellman_loss(states, actions, rewards, next_states, dones)\n        total_loss = self.conservative_weight * conservative_loss + bellman_loss\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        self.update_count += 1\n        if self.update_count % 100 == 0:\n            self.soft_update_target()\n        self.losses.append(total_loss.item())\n        self.conservative_losses.append(conservative_loss.item())\n        self.bellman_losses.append(bellman_loss.item())\n        return {\n            'total_loss': total_loss.item(),\n            'conservative_loss': conservative_loss.item(),\n            'bellman_loss': bellman_loss.item()\n        }\n    def soft_update_target(self):\n        for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n    def get_action(self, state, epsilon=0.0):\n        if np.random.random() < epsilon:\n            return np.random.randint(self.action_dim)\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n            q_values = self.q_network.get_q_values(state_tensor)\n            return q_values.argmax().item()\nclass ImplicitQLearning:\n    def __init__(self, state_dim, action_dim, lr=3e-4, expectile=0.7):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.expectile = expectile\n        self.q_network = ConservativeQNetwork(state_dim, action_dim).to(device)\n        self.target_q_network = copy.deepcopy(self.q_network).to(device)\n        self.policy_network = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim),\n            nn.Softmax(dim=-1)\n        ).to(device)\n        self.q_optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n        self.gamma = 0.99\n        self.tau = 0.005\n        self.q_losses = []\n        self.policy_losses = []\n        self.advantages = []\n    def compute_expectile_loss(self, errors, expectile):\n        weights = torch.where(errors > 0, expectile, 1 - expectile)\n        return (weights * errors.pow(2)).mean()\n    def update_q_function(self, states, actions, rewards, next_states, dones):\n        q_values, state_values = self.q_network(states)\n        current_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n        with torch.no_grad():\n            _, next_state_values = self.target_q_network(next_states)\n            target_q_values = rewards + (self.gamma * next_state_values.squeeze() * (~dones))\n        q_errors = target_q_values - current_q_values\n        q_loss = self.compute_expectile_loss(q_errors, 0.5)\n        advantages = current_q_values.detach() - state_values.squeeze()\n        value_loss = self.compute_expectile_loss(advantages, self.expectile)\n        total_q_loss = q_loss + value_loss\n        self.q_optimizer.zero_grad()\n        total_q_loss.backward()\n        self.q_optimizer.step()\n        return total_q_loss.item(), advantages.mean().item()\n    def update_policy(self, states, actions):\n        with torch.no_grad():\n            q_values, state_values = self.q_network(states)\n            current_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n            advantages = current_q_values - state_values.squeeze()\n            weights = torch.exp(advantages / 3.0).clamp(max=100)\n        action_probs = self.policy_network(states)\n        log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1)).squeeze() + 1e-8)\n        policy_loss = -(weights.detach() * log_probs).mean()\n        self.policy_optimizer.zero_grad()\n        policy_loss.backward()\n        self.policy_optimizer.step()\n        return policy_loss.item()\n    def update(self, batch):\n        states, actions, rewards, next_states, dones = batch\n        q_loss, avg_advantage = self.update_q_function(states, actions, rewards, next_states, dones)\n        policy_loss = self.update_policy(states, actions)\n        for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n        self.q_losses.append(q_loss)\n        self.policy_losses.append(policy_loss)\n        self.advantages.append(avg_advantage)\n        return {\n            'q_loss': q_loss,\n            'policy_loss': policy_loss,\n            'avg_advantage': avg_advantage\n        }\n    def get_action(self, state):\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n            action_probs = self.policy_network(state_tensor)\n            action_dist = Categorical(action_probs)\n            return action_dist.sample().item()\ndef generate_offline_dataset(env_name='CartPole-v1', dataset_type='mixed', size=50000):\n    class SimpleGridWorld:\n        def __init__(self, size=5):\n            self.size = size\n            self.state = [0, 0]\n            self.goal = [size-1, size-1]\n            self.action_space = 4\n        def reset(self):\n            self.state = [0, 0]\n            return np.array(self.state, dtype=np.float32)\n        def step(self, action):\n            if action == 0 and self.state[1] < self.size - 1:\n                self.state[1] += 1\n            elif action == 1 and self.state[1] > 0:\n                self.state[1] -= 1\n            elif action == 2 and self.state[0] > 0:\n                self.state[0] -= 1\n            elif action == 3 and self.state[0] < self.size - 1:\n                self.state[0] += 1\n            done = (self.state == self.goal)\n            reward = 1.0 if done else -0.1\n            return np.array(self.state, dtype=np.float32), reward, done, {}\n    env = SimpleGridWorld(size=5)\n    states, actions, rewards, next_states, dones = [], [], [], [], []\n    for _ in range(size):\n        state = env.reset()\n        episode_done = False\n        episode_length = 0\n        while not episode_done and episode_length < 50:\n            if dataset_type == 'expert':\n                if state[0] < env.goal[0]:\n                    action = 3\n                elif state[1] < env.goal[1]:\n                    action = 0\n                else:\n                    action = np.random.randint(4)\n            elif dataset_type == 'random':\n                action = np.random.randint(4)\n            else:\n                if np.random.random() < 0.7:\n                    if state[0] < env.goal[0]:\n                        action = 3\n                    elif state[1] < env.goal[1]:\n                        action = 0\n                    else:\n                        action = np.random.randint(4)\n                else:\n                    action = np.random.randint(4)\n            next_state, reward, done, _ = env.step(action)\n            states.append(state.copy())\n            actions.append(action)\n            rewards.append(reward)\n            next_states.append(next_state.copy())\n            dones.append(done)\n            state = next_state\n            episode_done = done\n            episode_length += 1\n            if episode_done:\n                break\n    return OfflineDataset(states, actions, rewards, next_states, dones, dataset_type)\nprint(\"ðŸŽ¯ Generating Offline Datasets...\")\ndatasets = {\n    'expert': generate_offline_dataset(dataset_type='expert', size=10000),\n    'mixed': generate_offline_dataset(dataset_type='mixed', size=15000),\n    'random': generate_offline_dataset(dataset_type='random', size=8000)\n}\nfor name, dataset in datasets.items():\n    print(f\"\\nðŸ“Š {name.title()} Dataset:\")\n    print(f\"  Size: {dataset.size}\")\n    print(f\"  Average Reward: {dataset.reward_mean:.3f} Â± {dataset.reward_std:.3f}\")\n    print(f\"  State Dim: {dataset.states.shape[1]}\")\n    action_dist = dataset.get_action_distribution()\n    print(f\"  Action Distribution: {action_dist}\")\nprint(\"\\nâœ… Offline datasets generated successfully!\")\nprint(\"ðŸ”„ Ready for Conservative Q-Learning and Implicit Q-Learning training...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b52261",
   "metadata": {},
   "source": [
    "# Section 2: Safe Reinforcement Learning\n",
    "\n",
    "## 2.1 Theory: Constraint Satisfaction and Risk Management\n",
    "\n",
    "Safe Reinforcement Learning addresses the critical challenge of learning optimal policies while satisfying safety constraints. This is essential for real-world applications where policy violations can lead to catastrophic consequences.\n",
    "\n",
    "### Mathematical Framework for Safe RL\n",
    "\n",
    "#### Constrained Markov Decision Process (CMDP)\n",
    "A CMDP extends the standard MDP with safety constraints:\n",
    "$$\\text{CMDP} = (\\mathcal{S}, \\mathcal{A}, P, R, C, \\gamma, d_0)$$\n",
    "\n",
    "Where:\n",
    "- **$C: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}^m$**: Cost function (constraint violations)\n",
    "- **$d_0$**: Initial state distribution\n",
    "- **Safety Constraint**: $\\mathbb{E}_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t c_i(s_t, a_t)] \\leq \\delta_i$ for $i \\in \\{1, ..., m\\}$\n",
    "\n",
    "#### Safe RL Objective\n",
    "The safe RL problem is formulated as:\n",
    "$$\\max_\\pi \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t)\\right]$$\n",
    "$$\\text{subject to } \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t c_i(s_t, a_t)\\right] \\leq \\delta_i, \\forall i$$\n",
    "\n",
    "### Key Approaches to Safe RL\n",
    "\n",
    "#### 1. Lagrangian Methods\n",
    "Use Lagrange multipliers to convert constrained optimization to unconstrained:\n",
    "$$\\mathcal{L}(\\pi, \\lambda) = J(\\pi) - \\sum_{i=1}^m \\lambda_i \\left(J_C^i(\\pi) - \\delta_i\\right)$$\n",
    "\n",
    "Where:\n",
    "- **$J(\\pi)$**: Expected cumulative reward\n",
    "- **$J_C^i(\\pi)$**: Expected cumulative cost for constraint $i$\n",
    "- **$\\lambda_i$**: Lagrange multiplier for constraint $i$\n",
    "\n",
    "#### 2. Constrained Policy Optimization (CPO)\n",
    "CPO ensures policy updates satisfy constraints through trust regions:\n",
    "$$\\max_\\pi \\mathbb{E}_{s \\sim d^\\pi, a \\sim \\pi}[A^R_{\\pi_k}(s,a)]$$\n",
    "$$\\text{subject to } J_C(\\pi) \\leq \\delta \\text{ and } D_{KL}(\\pi_k, \\pi) \\leq \\delta_{KL}$$\n",
    "\n",
    "#### 3. Safe Policy Gradients\n",
    "Modify policy gradient updates to account for constraint violations:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_\\pi[\\nabla_\\theta \\log \\pi(a|s) \\cdot (A^R(s,a) - \\lambda A^C(s,a))]$$\n",
    "\n",
    "### Risk Measures in Safe RL\n",
    "\n",
    "#### 1. Value at Risk (VaR)\n",
    "$$\\text{VaR}_\\alpha(X) = \\inf\\{x : P(X \\leq x) \\geq \\alpha\\}$$\n",
    "\n",
    "#### 2. Conditional Value at Risk (CVaR)\n",
    "$$\\text{CVaR}_\\alpha(X) = \\mathbb{E}[X | X \\geq \\text{VaR}_\\alpha(X)]$$\n",
    "\n",
    "#### 3. Risk-Sensitive Objective\n",
    "Optimize risk-adjusted returns:\n",
    "$$\\max_\\pi \\mathbb{E}_\\pi[\\sum_{t=0}^{\\infty} \\gamma^t r_t] - \\beta \\cdot \\text{Risk}(\\pi)$$\n",
    "\n",
    "## 2.2 Safety Mechanisms\n",
    "\n",
    "### 1. Barrier Functions\n",
    "Use barrier functions to prevent constraint violations:\n",
    "$$B(s) = -\\log(\\delta - C(s))$$\n",
    "\n",
    "### 2. Safe Exploration\n",
    "- **Initial Safe Policy**: Start with a known safe policy\n",
    "- **Safe Action Space**: Restrict actions to safe subset\n",
    "- **Recovery Actions**: Define emergency actions for constraint violations\n",
    "\n",
    "### 3. Risk-Aware Planning\n",
    "Incorporate uncertainty in safety-critical decision making:\n",
    "- **Robust MDP**: Consider worst-case scenarios\n",
    "- **Bayesian RL**: Maintain uncertainty over dynamics\n",
    "- **Distributional RL**: Model full return distributions\n",
    "\n",
    "## 2.3 Applications of Safe RL\n",
    "\n",
    "### Autonomous Vehicles\n",
    "- **Constraints**: Collision avoidance, traffic rules\n",
    "- **Risk Measures**: Probability of accidents\n",
    "- **Safety Mechanisms**: Emergency braking, lane keeping\n",
    "\n",
    "### Healthcare\n",
    "- **Constraints**: Patient safety, dosage limits\n",
    "- **Risk Measures**: Adverse events probability\n",
    "- **Safety Mechanisms**: Conservative treatment protocols\n",
    "\n",
    "### Industrial Control\n",
    "- **Constraints**: Equipment damage, safety limits\n",
    "- **Risk Measures**: System failure probability  \n",
    "- **Safety Mechanisms**: Emergency shutoffs, backup systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc8b120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SafeEnvironment:\n    def __init__(self, size=6, hazard_positions=None, constraint_threshold=0.1):\n        self.size = size\n        self.state = [0, 0]\n        self.goal = [size-1, size-1]\n        self.constraint_threshold = constraint_threshold\n        if hazard_positions is None:\n            self.hazards = [[2, 2], [3, 1], [1, 3], [4, 3]]\n        else:\n            self.hazards = hazard_positions\n        self.action_space = 4\n        self.max_episode_steps = 50\n        self.current_step = 0\n        self.constraint_violations = 0\n        self.total_constraint_cost = 0\n    def reset(self):\n        self.state = [0, 0]\n        self.current_step = 0\n        self.constraint_violations = 0\n        self.total_constraint_cost = 0\n        return np.array(self.state, dtype=np.float32)\n    def step(self, action):\n        self.current_step += 1\n        prev_state = self.state.copy()\n        if action == 0 and self.state[1] < self.size - 1:\n            self.state[1] += 1\n        elif action == 1 and self.state[1] > 0:\n            self.state[1] -= 1\n        elif action == 2 and self.state[0] > 0:\n            self.state[0] -= 1\n        elif action == 3 and self.state[0] < self.size - 1:\n            self.state[0] += 1\n        done = (self.state == self.goal)\n        reward = 10.0 if done else -0.1\n        constraint_cost = self._compute_constraint_cost(self.state)\n        episode_done = done or self.current_step >= self.max_episode_steps\n        info = {\n            'constraint_cost': constraint_cost,\n            'constraint_violation': constraint_cost > 0,\n            'total_violations': self.constraint_violations,\n            'position': self.state.copy()\n        }\n        return np.array(self.state, dtype=np.float32), reward, episode_done, info\n    def _compute_constraint_cost(self, state):\n        cost = 0.0\n        if state in self.hazards:\n            cost += 1.0\n            self.constraint_violations += 1\n        if state[0] == 0 or state[0] == self.size-1 or state[1] == 0 or state[1] == self.size-1:\n            cost += 0.1\n        self.total_constraint_cost += cost\n        return cost\n    def is_safe_state(self, state):\n        return state not in self.hazards\n    def get_safe_actions(self, state):\n        safe_actions = []\n        for action in range(self.action_space):\n            next_state = state.copy()\n            if action == 0 and state[1] < self.size - 1:\n                next_state[1] += 1\n            elif action == 1 and state[1] > 0:\n                next_state[1] -= 1\n            elif action == 2 and state[0] > 0:\n                next_state[0] -= 1\n            elif action == 3 and state[0] < self.size - 1:\n                next_state[0] += 1\n            if self.is_safe_state(next_state):\n                safe_actions.append(action)\n        return safe_actions if safe_actions else list(range(self.action_space))\nclass ConstrainedPolicyOptimization:\n    def __init__(self, state_dim, action_dim, constraint_limit=0.1, lr=3e-4):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.constraint_limit = constraint_limit\n        self.policy_network = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim),\n            nn.Softmax(dim=-1)\n        ).to(device)\n        self.value_network = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        ).to(device)\n        self.cost_value_network = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        ).to(device)\n        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n        self.cost_optimizer = optim.Adam(self.cost_value_network.parameters(), lr=lr)\n        self.gamma = 0.99\n        self.lam = 0.95\n        self.clip_ratio = 0.2\n        self.target_kl = 0.01\n        self.damping = 0.1\n        self.constraint_violations = []\n        self.policy_losses = []\n        self.value_losses = []\n        self.cost_losses = []\n    def get_action(self, state):\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n            action_probs = self.policy_network(state_tensor)\n            action_dist = Categorical(action_probs)\n            action = action_dist.sample()\n            log_prob = action_dist.log_prob(action)\n        return action.item(), log_prob.item()\n    def compute_gae(self, rewards, values, dones, next_value):\n        advantages = []\n        gae = 0\n        for step in reversed(range(len(rewards))):\n            if step == len(rewards) - 1:\n                next_non_terminal = 1.0 - dones[step]\n                next_value_step = next_value\n            else:\n                next_non_terminal = 1.0 - dones[step]\n                next_value_step = values[step + 1]\n            delta = rewards[step] + self.gamma * next_value_step * next_non_terminal - values[step]\n            gae = delta + self.gamma * self.lam * next_non_terminal * gae\n            advantages.insert(0, gae)\n        return torch.FloatTensor(advantages).to(device)\n    def compute_policy_loss(self, states, actions, advantages, old_log_probs):\n        action_probs = self.policy_network(states)\n        action_dist = Categorical(action_probs)\n        new_log_probs = action_dist.log_prob(actions)\n        ratio = torch.exp(new_log_probs - old_log_probs)\n        surr1 = ratio * advantages\n        surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n        policy_loss = -torch.min(surr1, surr2).mean()\n        kl_div = (old_log_probs - new_log_probs).mean()\n        return policy_loss, kl_div\n    def compute_constraint_violation(self, states, actions, cost_advantages, old_log_probs):\n        action_probs = self.policy_network(states)\n        action_dist = Categorical(action_probs)\n        new_log_probs = action_dist.log_prob(actions)\n        ratio = torch.exp(new_log_probs - old_log_probs)\n        constraint_violation = (ratio * cost_advantages).mean()\n        return constraint_violation\n    def update(self, trajectories):\n        if not trajectories:\n            return None\n        all_states, all_actions, all_rewards, all_costs = [], [], [], []\n        all_dones, all_log_probs = [], []\n        for trajectory in trajectories:\n            states, actions, rewards, costs, dones, log_probs = zip(*trajectory)\n            all_states.extend(states)\n            all_actions.extend(actions)\n            all_rewards.extend(rewards)\n            all_costs.extend(costs)\n            all_dones.extend(dones)\n            all_log_probs.extend(log_probs)\n        states = torch.FloatTensor(all_states).to(device)\n        actions = torch.LongTensor(all_actions).to(device)\n        rewards = torch.FloatTensor(all_rewards).to(device)\n        costs = torch.FloatTensor(all_costs).to(device)\n        old_log_probs = torch.FloatTensor(all_log_probs).to(device)\n        values = self.value_network(states).squeeze()\n        cost_values = self.cost_value_network(states).squeeze()\n        with torch.no_grad():\n            next_value = self.value_network(states[-1:]).squeeze()\n            next_cost_value = self.cost_value_network(states[-1:]).squeeze()\n        advantages = self.compute_gae(all_rewards, values.detach().cpu().numpy(), \n                                    all_dones, next_value.item())\n        cost_advantages = self.compute_gae(all_costs, cost_values.detach().cpu().numpy(), \n                                         all_dones, next_cost_value.item())\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        cost_advantages = (cost_advantages - cost_advantages.mean()) / (cost_advantages.std() + 1e-8)\n        returns = advantages + values.detach()\n        cost_returns = cost_advantages + cost_values.detach()\n        value_loss = F.mse_loss(values, returns)\n        cost_loss = F.mse_loss(cost_values, cost_returns)\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        self.value_optimizer.step()\n        self.cost_optimizer.zero_grad()\n        cost_loss.backward()\n        self.cost_optimizer.step()\n        constraint_violation = self.compute_constraint_violation(\n            states, actions, cost_advantages, old_log_probs\n        )\n        policy_loss, kl_div = self.compute_policy_loss(\n            states, actions, advantages, old_log_probs\n        )\n        if constraint_violation.item() <= self.constraint_limit:\n            self.policy_optimizer.zero_grad()\n            policy_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=0.5)\n            self.policy_optimizer.step()\n        else:\n            print(f\"âš ï¸ Policy update skipped due to constraint violation: {constraint_violation.item():.4f}\")\n        self.policy_losses.append(policy_loss.item())\n        self.value_losses.append(value_loss.item())\n        self.cost_losses.append(cost_loss.item())\n        self.constraint_violations.append(constraint_violation.item())\n        return {\n            'policy_loss': policy_loss.item(),\n            'value_loss': value_loss.item(),\n            'cost_loss': cost_loss.item(),\n            'constraint_violation': constraint_violation.item(),\n            'kl_divergence': kl_div.item()\n        }\nclass LagrangianSafeRL:\n    def __init__(self, state_dim, action_dim, constraint_limit=0.1, lr=3e-4, lagrange_lr=1e-2):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.constraint_limit = constraint_limit\n        self.policy_network = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim),\n            nn.Softmax(dim=-1)\n        ).to(device)\n        self.value_network = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, 1)\n        ).to(device)\n        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n        self.lagrange_multiplier = nn.Parameter(torch.tensor(1.0, device=device))\n        self.lagrange_optimizer = optim.Adam([self.lagrange_multiplier], lr=lagrange_lr)\n        self.gamma = 0.99\n        self.lagrange_history = []\n        self.constraint_costs = []\n        self.total_rewards = []\n    def get_action(self, state):\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n            action_probs = self.policy_network(state_tensor)\n            action_dist = Categorical(action_probs)\n            action = action_dist.sample()\n            log_prob = action_dist.log_prob(action)\n        return action.item(), log_prob.item()\n    def update(self, trajectories):\n        if not trajectories:\n            return None\n        all_states, all_actions, all_rewards, all_costs = [], [], [], []\n        all_log_probs = []\n        for trajectory in trajectories:\n            states, actions, rewards, costs, _, log_probs = zip(*trajectory)\n            all_states.extend(states)\n            all_actions.extend(actions)\n            all_rewards.extend(rewards)\n            all_costs.extend(costs)\n            all_log_probs.extend(log_probs)\n        states = torch.FloatTensor(all_states).to(device)\n        actions = torch.LongTensor(all_actions).to(device)\n        rewards = torch.FloatTensor(all_rewards).to(device)\n        costs = torch.FloatTensor(all_costs).to(device)\n        old_log_probs = torch.FloatTensor(all_log_probs).to(device)\n        discounted_rewards = []\n        discounted_costs = []\n        for trajectory in trajectories:\n            traj_rewards = [step[2] for step in trajectory]\n            traj_costs = [step[3] for step in trajectory]\n            reward_return = 0\n            cost_return = 0\n            for r, c in zip(reversed(traj_rewards), reversed(traj_costs)):\n                reward_return = r + self.gamma * reward_return\n                cost_return = c + self.gamma * cost_return\n                discounted_rewards.insert(0, reward_return)\n                discounted_costs.insert(0, cost_return)\n        returns = torch.FloatTensor(discounted_rewards).to(device)\n        cost_returns = torch.FloatTensor(discounted_costs).to(device)\n        values = self.value_network(states).squeeze()\n        advantages = returns - values.detach()\n        cost_advantages = cost_returns\n        action_probs = self.policy_network(states)\n        action_dist = Categorical(action_probs)\n        log_probs = action_dist.log_prob(actions)\n        policy_loss = -(log_probs * (advantages - self.lagrange_multiplier * cost_advantages)).mean()\n        value_loss = F.mse_loss(values, returns)\n        self.policy_optimizer.zero_grad()\n        policy_loss.backward()\n        self.policy_optimizer.step()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        self.value_optimizer.step()\n        avg_cost = cost_returns.mean()\n        constraint_violation = avg_cost - self.constraint_limit\n        lagrange_loss = -self.lagrange_multiplier * constraint_violation\n        self.lagrange_optimizer.zero_grad()\n        lagrange_loss.backward()\n        self.lagrange_optimizer.step()\n        with torch.no_grad():\n            self.lagrange_multiplier.clamp_(min=0.0)\n        self.lagrange_history.append(self.lagrange_multiplier.item())\n        self.constraint_costs.append(avg_cost.item())\n        self.total_rewards.append(returns.mean().item())\n        return {\n            'policy_loss': policy_loss.item(),\n            'value_loss': value_loss.item(),\n            'lagrange_multiplier': self.lagrange_multiplier.item(),\n            'constraint_violation': constraint_violation.item(),\n            'avg_cost': avg_cost.item()\n        }\ndef collect_safe_trajectory(env, agent, max_steps=50):\n    trajectory = []\n    state = env.reset()\n    for step in range(max_steps):\n        action, log_prob = agent.get_action(state)\n        next_state, reward, done, info = env.step(action)\n        constraint_cost = info['constraint_cost']\n        trajectory.append((\n            state.copy(), action, reward, constraint_cost, done, log_prob\n        ))\n        if done:\n            break\n        state = next_state\n    return trajectory\ndef demonstrate_safe_rl():\n    print(\"ðŸ›¡ï¸ Demonstrating Safe Reinforcement Learning\")\n    print(\"=\" * 50)\n    env = SafeEnvironment(size=6, constraint_threshold=0.1)\n    agents = {\n        'CPO': ConstrainedPolicyOptimization(\n            state_dim=2, action_dim=4, constraint_limit=0.1\n        ),\n        'Lagrangian': LagrangianSafeRL(\n            state_dim=2, action_dim=4, constraint_limit=0.1\n        )\n    }\n    results = {name: {\n        'rewards': [], 'constraint_violations': [], 'episode_lengths': []\n    } for name in agents.keys()}\n    num_episodes = 300\n    update_frequency = 10\n    for episode in range(num_episodes):\n        for agent_name, agent in agents.items():\n            trajectories = []\n            episode_rewards = []\n            episode_violations = []\n            episode_lengths = []\n            for _ in range(update_frequency):\n                trajectory = collect_safe_trajectory(env, agent)\n                trajectories.append(trajectory)\n                episode_reward = sum(step[2] for step in trajectory)\n                episode_violation = sum(step[3] for step in trajectory)\n                episode_length = len(trajectory)\n                episode_rewards.append(episode_reward)\n                episode_violations.append(episode_violation)\n                episode_lengths.append(episode_length)\n            if trajectories:\n                update_info = agent.update(trajectories)\n            results[agent_name]['rewards'].extend(episode_rewards)\n            results[agent_name]['constraint_violations'].extend(episode_violations)\n            results[agent_name]['episode_lengths'].extend(episode_lengths)\n        if episode % 50 == 0:\n            print(f\"\\nEpisode {episode}:\")\n            for agent_name in agents.keys():\n                recent_rewards = np.mean(results[agent_name]['rewards'][-50:])\n                recent_violations = np.mean(results[agent_name]['constraint_violations'][-50:])\n                print(f\"  {agent_name}: Reward={recent_rewards:.2f}, Violations={recent_violations:.3f}\")\n    return results, agents, env\nprint(\"ðŸš€ Starting Safe RL Training...\")\nsafe_results, safe_agents, safe_env = demonstrate_safe_rl()\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfor agent_name, data in safe_results.items():\n    window_size = 20\n    if len(data['rewards']) >= window_size:\n        smoothed_rewards = pd.Series(data['rewards']).rolling(window_size).mean()\n        axes[0, 0].plot(smoothed_rewards, label=agent_name, linewidth=2)\naxes[0, 0].set_title('Safe RL Learning Curves')\naxes[0, 0].set_xlabel('Episode')\naxes[0, 0].set_ylabel('Episode Reward (Smoothed)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\nfor agent_name, data in safe_results.items():\n    window_size = 20\n    if len(data['constraint_violations']) >= window_size:\n        smoothed_violations = pd.Series(data['constraint_violations']).rolling(window_size).mean()\n        axes[0, 1].plot(smoothed_violations, label=agent_name, linewidth=2)\naxes[0, 1].set_title('Constraint Violations Over Time')\naxes[0, 1].set_xlabel('Episode')\naxes[0, 1].set_ylabel('Average Constraint Cost (Smoothed)')\naxes[0, 1].axhline(y=safe_env.constraint_threshold, color='red', linestyle='--', label='Constraint Limit')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\nif 'Lagrangian' in safe_agents and hasattr(safe_agents['Lagrangian'], 'lagrange_history'):\n    axes[1, 0].plot(safe_agents['Lagrangian'].lagrange_history, 'g-', linewidth=2)\n    axes[1, 0].set_title('Lagrange Multiplier Evolution')\n    axes[1, 0].set_xlabel('Update Step')\n    axes[1, 0].set_ylabel('Lagrange Multiplier (Î»)')\n    axes[1, 0].grid(True, alpha=0.3)\nfor agent_name, data in safe_results.items():\n    final_rewards = np.mean(data['rewards'][-50:])\n    final_violations = np.mean(data['constraint_violations'][-50:])\n    axes[1, 1].scatter(final_violations, final_rewards, s=100, label=agent_name)\naxes[1, 1].set_title('Safety vs Performance Trade-off')\naxes[1, 1].set_xlabel('Average Constraint Violations')\naxes[1, 1].set_ylabel('Average Reward')\naxes[1, 1].axvline(x=safe_env.constraint_threshold, color='red', linestyle='--', alpha=0.5)\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint(\"\\nðŸ“Š Safe RL Results Summary:\")\nprint(\"=\" * 50)\nfor agent_name, data in safe_results.items():\n    final_reward = np.mean(data['rewards'][-50:])\n    final_violations = np.mean(data['constraint_violations'][-50:])\n    violation_rate = np.mean([v > safe_env.constraint_threshold for v in data['constraint_violations'][-50:]])\n    print(f\"\\n{agent_name}:\")\n    print(f\"  Final Performance: {final_reward:.2f}\")\n    print(f\"  Avg Constraint Cost: {final_violations:.4f}\")\n    print(f\"  Violation Rate: {violation_rate:.2%}\")\n    print(f\"  Constraint Satisfied: {'âœ…' if final_violations <= safe_env.constraint_threshold else 'âŒ'}\")\nprint(\"\\nðŸ’¡ Key Insights:\")\nprint(\"  â€¢ CPO prevents policy updates that violate constraints\")\nprint(\"  â€¢ Lagrangian method adapts penalty weights automatically\")\nprint(\"  â€¢ Safety-performance trade-offs are environment dependent\")\nprint(\"  â€¢ Constraint satisfaction improves with training\")\nprint(\"\\nðŸ›¡ï¸ Safe RL demonstration completed!\")\nprint(\"ðŸ”„ Ready for Multi-Agent RL implementation...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96a729",
   "metadata": {},
   "source": [
    "# Section 3: Multi-Agent Reinforcement Learning\n",
    "\n",
    "## 3.1 Theory: Coordination and Competition\n",
    "\n",
    "Multi-Agent Reinforcement Learning (MARL) extends single-agent RL to environments with multiple learning agents. This introduces new challenges including non-stationarity, partial observability, and coordination problems.\n",
    "\n",
    "### Mathematical Framework for MARL\n",
    "\n",
    "#### Multi-Agent Markov Decision Process (MA-MDP)\n",
    "A Multi-Agent MDP is defined as:\n",
    "$$\\text{MA-MDP} = (\\mathcal{N}, \\mathcal{S}, \\{\\mathcal{A}_i\\}_{i \\in \\mathcal{N}}, P, \\{R_i\\}_{i \\in \\mathcal{N}}, \\gamma, \\mu_0)$$\n",
    "\n",
    "Where:\n",
    "- **$\\mathcal{N} = \\{1, 2, ..., n\\}$**: Set of agents\n",
    "- **$\\mathcal{S}$**: Global state space\n",
    "- **$\\mathcal{A}_i$**: Action space for agent $i$\n",
    "- **$P: \\mathcal{S} \\times \\mathcal{A}_1 \\times ... \\times \\mathcal{A}_n \\rightarrow \\Delta(\\mathcal{S})$**: Transition function\n",
    "- **$R_i: \\mathcal{S} \\times \\mathcal{A}_1 \\times ... \\times \\mathcal{A}_n \\rightarrow \\mathbb{R}$**: Reward function for agent $i$\n",
    "\n",
    "#### Joint Policy and Nash Equilibrium\n",
    "The **joint policy** $\\pi = (\\pi_1, ..., \\pi_n)$ where $\\pi_i$ is agent $i$'s policy.\n",
    "\n",
    "**Nash Equilibrium**: A joint policy $\\pi^*$ is a Nash equilibrium if:\n",
    "$$J_i(\\pi_i^*, \\pi_{-i}^*) \\geq J_i(\\pi_i, \\pi_{-i}^*), \\forall \\pi_i, \\forall i$$\n",
    "\n",
    "Where $\\pi_{-i}$ denotes the policies of all agents except $i$.\n",
    "\n",
    "### Key Challenges in MARL\n",
    "\n",
    "#### 1. Non-Stationarity\n",
    "From each agent's perspective, the environment is non-stationary due to other learning agents:\n",
    "$$P^{\\pi_{-i}}(s' | s, a_i) = \\sum_{\\mathbf{a}_{-i}} \\prod_{j \\neq i} \\pi_j(a_j | s) P(s' | s, a_i, \\mathbf{a}_{-i})$$\n",
    "\n",
    "#### 2. Exponential Joint Action Space\n",
    "The joint action space grows exponentially: $|\\mathcal{A}| = \\prod_{i=1}^n |\\mathcal{A}_i|$\n",
    "\n",
    "#### 3. Partial Observability\n",
    "Agents often have limited observations: $o_i = O_i(s, i)$\n",
    "\n",
    "#### 4. Credit Assignment\n",
    "Determining individual agent contributions to team success.\n",
    "\n",
    "### MARL Paradigms\n",
    "\n",
    "#### 1. Cooperative MARL\n",
    "- **Objective**: Maximize team reward $R_{team} = \\sum_{i=1}^n R_i$\n",
    "- **Examples**: Multi-robot coordination, team games\n",
    "- **Algorithms**: MADDPG, QMIX, VDN\n",
    "\n",
    "#### 2. Competitive MARL\n",
    "- **Objective**: Each agent maximizes individual reward\n",
    "- **Examples**: Game playing, resource allocation\n",
    "- **Algorithms**: Self-play, Population-based training\n",
    "\n",
    "#### 3. Mixed-Motive MARL\n",
    "- **Objective**: Combination of individual and team objectives\n",
    "- **Examples**: Social dilemmas, economic systems\n",
    "- **Algorithms**: Multi-objective optimization\n",
    "\n",
    "## 3.2 Advanced MARL Algorithms\n",
    "\n",
    "### 1. Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "**Key Idea**: Centralized training with decentralized execution\n",
    "\n",
    "**Critic Update**:\n",
    "$$Q_i^{\\mu}(s, a_1, ..., a_n) = \\mathbb{E}[r_i + \\gamma Q_i^{\\mu'}(s', \\mu_1'(o_1'), ..., \\mu_n'(o_n'))]$$\n",
    "\n",
    "**Actor Update**:\n",
    "$$\\nabla_{\\theta_i} J_i = \\mathbb{E}[\\nabla_{a_i} Q_i^{\\mu}(s, a_1, ..., a_n)|_{a_i=\\mu_i(o_i)} \\nabla_{\\theta_i} \\mu_i(o_i)]$$\n",
    "\n",
    "### 2. QMIX (Monotonic Value Function Factorization)\n",
    "**Key Idea**: Factor team Q-value while maintaining monotonicity\n",
    "\n",
    "**Mixing Network**:\n",
    "$$Q_{tot}(s, \\mathbf{a}) = f_{mix}(Q_1(o_1, a_1), ..., Q_n(o_n, a_n), s)$$\n",
    "\n",
    "**Monotonicity Constraint**:\n",
    "$$\\frac{\\partial Q_{tot}}{\\partial Q_i} \\geq 0, \\forall i$$\n",
    "\n",
    "### 3. Multi-Agent Actor-Critic (MAAC)\n",
    "**Centralized Critic**: Uses global information during training\n",
    "$$Q^{\\pi}(s, a_1, ..., a_n) = \\mathbb{E}_{\\pi}[\\sum_{t=0}^{\\infty} \\gamma^t r_t | s_0=s, a_{0,i}=a_i, \\forall i]$$\n",
    "\n",
    "**Decentralized Actor**: Each agent has its own policy\n",
    "$$\\pi_i(a_i | o_i) = \\text{softmax}(f_i(o_i))$$\n",
    "\n",
    "## 3.3 Communication in MARL\n",
    "\n",
    "### 1. Explicit Communication\n",
    "Agents exchange messages to coordinate:\n",
    "$$m_i^t = f_{comm}(o_i^t, h_i^{t-1})$$\n",
    "$$h_i^t = f_{update}(o_i^t, m_{-i}^t, h_i^{t-1})$$\n",
    "\n",
    "### 2. Implicit Communication\n",
    "Coordination through shared representations or attention mechanisms.\n",
    "\n",
    "### 3. Emergent Communication\n",
    "Communication protocols emerge through learning:\n",
    "$$\\mathcal{L}_{comm} = \\mathcal{L}_{task} + \\lambda \\mathcal{L}_{communication}$$\n",
    "\n",
    "## 3.4 Applications of MARL\n",
    "\n",
    "### Autonomous Vehicle Coordination\n",
    "- **Agents**: Individual vehicles\n",
    "- **Objective**: Safe and efficient traffic flow\n",
    "- **Challenges**: Real-time coordination, safety constraints\n",
    "\n",
    "### Multi-Robot Systems\n",
    "- **Agents**: Individual robots\n",
    "- **Objective**: Collaborative task completion\n",
    "- **Challenges**: Partial observability, communication constraints\n",
    "\n",
    "### Financial Trading\n",
    "- **Agents**: Individual traders/algorithms\n",
    "- **Objective**: Profit maximization\n",
    "- **Challenges**: Market manipulation, information asymmetry\n",
    "\n",
    "### Game Playing\n",
    "- **Agents**: Individual players\n",
    "- **Objective**: Win/score maximization\n",
    "- **Challenges**: Opponent modeling, strategy adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a340760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentEnvironment:\n    def __init__(self, grid_size=8, num_agents=4, num_targets=3):\n        self.grid_size = grid_size\n        self.num_agents = num_agents\n        self.num_targets = num_targets\n        self.max_episode_steps = 100\n        self.reset()\n        self.action_space = 5\n        self.observation_space = 2 + 2 * num_agents + 2 * num_targets\n    def reset(self):\n        self.current_step = 0\n        self.agent_positions = []\n        for _ in range(self.num_agents):\n            while True:\n                pos = [np.random.randint(0, self.grid_size), np.random.randint(0, self.grid_size)]\n                if pos not in self.agent_positions:\n                    self.agent_positions.append(pos)\n                    break\n        self.target_positions = []\n        for _ in range(self.num_targets):\n            while True:\n                pos = [np.random.randint(0, self.grid_size), np.random.randint(0, self.grid_size)]\n                if pos not in self.agent_positions and pos not in self.target_positions:\n                    self.target_positions.append(pos)\n                    break\n        self.targets_collected = [False] * self.num_targets\n        return self.get_observations()\n    def get_observations(self):\n        observations = []\n        for i in range(self.num_agents):\n            obs = []\n            obs.extend([self.agent_positions[i][0] / self.grid_size, \n                       self.agent_positions[i][1] / self.grid_size])\n            for j in range(self.num_agents):\n                if i != j:\n                    rel_pos = [(self.agent_positions[j][0] - self.agent_positions[i][0]) / self.grid_size,\n                              (self.agent_positions[j][1] - self.agent_positions[i][1]) / self.grid_size]\n                    obs.extend(rel_pos)\n            for k, target_pos in enumerate(self.target_positions):\n                if not self.targets_collected[k]:\n                    rel_pos = [(target_pos[0] - self.agent_positions[i][0]) / self.grid_size,\n                              (target_pos[1] - self.agent_positions[i][1]) / self.grid_size]\n                    obs.extend(rel_pos)\n                else:\n                    obs.extend([0.0, 0.0])\n            observations.append(np.array(obs, dtype=np.float32))\n        return observations\n    def step(self, actions):\n        self.current_step += 1\n        rewards = [0.0] * self.num_agents\n        new_positions = []\n        for i, action in enumerate(actions):\n            pos = self.agent_positions[i].copy()\n            if action == 1 and pos[1] < self.grid_size - 1:\n                pos[1] += 1\n            elif action == 2 and pos[1] > 0:\n                pos[1] -= 1\n            elif action == 3 and pos[0] > 0:\n                pos[0] -= 1\n            elif action == 4 and pos[0] < self.grid_size - 1:\n                pos[0] += 1\n            new_positions.append(pos)\n        collision_agents = set()\n        for i in range(self.num_agents):\n            for j in range(i + 1, self.num_agents):\n                if new_positions[i] == new_positions[j]:\n                    collision_agents.add(i)\n                    collision_agents.add(j)\n        for i in range(self.num_agents):\n            if i not in collision_agents:\n                self.agent_positions[i] = new_positions[i]\n            else:\n                rewards[i] -= 0.5\n        targets_collected_this_step = []\n        for i in range(self.num_agents):\n            for j, target_pos in enumerate(self.target_positions):\n                if (not self.targets_collected[j] and \n                    self.agent_positions[i] == target_pos):\n                    self.targets_collected[j] = True\n                    rewards[i] += 10.0\n                    targets_collected_this_step.append(j)\n        if targets_collected_this_step:\n            team_bonus = 2.0 * len(targets_collected_this_step)\n            for i in range(self.num_agents):\n                rewards[i] += team_bonus / self.num_agents\n        for i in range(self.num_agents):\n            rewards[i] -= 0.1\n        done = (all(self.targets_collected) or \n                self.current_step >= self.max_episode_steps)\n        observations = self.get_observations()\n        info = {\n            'targets_collected': sum(self.targets_collected),\n            'total_targets': self.num_targets,\n            'collisions': len(collision_agents) // 2\n        }\n        return observations, rewards, done, info\nclass MADDPGAgent:\n    def __init__(self, obs_dim, action_dim, num_agents, agent_id, lr=1e-3):\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.num_agents = num_agents\n        self.agent_id = agent_id\n        self.actor = nn.Sequential(\n            nn.Linear(obs_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 128),\n            nn.ReLU(),\n            nn.Linear(128, action_dim),\n            nn.Softmax(dim=-1)\n        ).to(device)\n        global_obs_dim = obs_dim * num_agents\n        global_action_dim = action_dim * num_agents\n        self.critic = nn.Sequential(\n            nn.Linear(global_obs_dim + global_action_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        ).to(device)\n        self.actor_target = copy.deepcopy(self.actor)\n        self.critic_target = copy.deepcopy(self.critic)\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n        self.gamma = 0.95\n        self.tau = 0.01\n        self.actor_losses = []\n        self.critic_losses = []\n    def get_action(self, observation, exploration_noise=0.1):\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(device)\n            action_probs = self.actor(obs_tensor)\n            if exploration_noise > 0:\n                noise = torch.randn_like(action_probs) * exploration_noise\n                action_probs = torch.softmax(action_probs + noise, dim=-1)\n            action_dist = Categorical(action_probs)\n            action = action_dist.sample()\n        return action.item()\n    def update(self, batch, other_agents):\n        states, actions, rewards, next_states, dones = batch\n        states = torch.FloatTensor(states).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards[:, self.agent_id]).to(device)\n        next_states = torch.FloatTensor(next_states).to(device)\n        dones = torch.BoolTensor(dones).to(device)\n        batch_size = states.shape[0]\n        states_flat = states.view(batch_size, -1)\n        next_states_flat = next_states.view(batch_size, -1)\n        actions_onehot = F.one_hot(actions, num_classes=self.action_dim).float()\n        actions_flat = actions_onehot.view(batch_size, -1)\n        next_actions = []\n        with torch.no_grad():\n            for i in range(self.num_agents):\n                if i == self.agent_id:\n                    next_action_probs = self.actor_target(next_states[:, i])\\n                else:\\n                    next_action_probs = other_agents[i].actor_target(next_states[:, i])\n                next_actions.append(next_action_probs)\n        next_actions_concat = torch.cat(next_actions, dim=-1)\n        with torch.no_grad():\n            critic_input = torch.cat([next_states_flat, next_actions_concat], dim=-1)\n            target_q_values = self.critic_target(critic_input).squeeze()\n            target_q_values = rewards + self.gamma * target_q_values * (~dones)\n        current_q_input = torch.cat([states_flat, actions_flat], dim=-1)\n        current_q_values = self.critic(current_q_input).squeeze()\n        critic_loss = F.mse_loss(current_q_values, target_q_values)\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n        self.critic_optimizer.step()\n        current_actions = []\n        for i in range(self.num_agents):\n            if i == self.agent_id:\n                current_actions.append(self.actor(states[:, i]))\n            else:\n                with torch.no_grad():\n                    current_actions.append(other_agents[i].actor(states[:, i]))\n        current_actions_concat = torch.cat(current_actions, dim=-1)\n        actor_critic_input = torch.cat([states_flat, current_actions_concat], dim=-1)\n        actor_loss = -self.critic(actor_critic_input).mean()\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n        self.actor_optimizer.step()\n        self.soft_update()\n        self.actor_losses.append(actor_loss.item())\n        self.critic_losses.append(critic_loss.item())\n        return {\n            'actor_loss': actor_loss.item(),\n            'critic_loss': critic_loss.item()\n        }\n    def soft_update(self):\n        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\nclass QMIXAgent:\n    def __init__(self, obs_dim, action_dim, num_agents, state_dim, lr=1e-3):\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.num_agents = num_agents\n        self.state_dim = state_dim\n        self.q_networks = nn.ModuleList([\n            nn.Sequential(\n                nn.Linear(obs_dim, 128),\n                nn.ReLU(),\n                nn.Linear(128, 128),\n                nn.ReLU(),\n                nn.Linear(128, action_dim)\n            ).to(device) for _ in range(num_agents)\n        ])\n        self.mixing_network = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, num_agents * 32),\n            nn.ReLU()\n        ).to(device)\n        self.final_layer = nn.Sequential(\n            nn.Linear(32, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1)\n        ).to(device)\n        self.target_q_networks = copy.deepcopy(self.q_networks)\n        self.target_mixing_network = copy.deepcopy(self.mixing_network)\n        self.target_final_layer = copy.deepcopy(self.final_layer)\n        all_params = (list(self.q_networks.parameters()) + \n                     list(self.mixing_network.parameters()) + \n                     list(self.final_layer.parameters()))\n        self.optimizer = optim.Adam(all_params, lr=lr)\n        self.gamma = 0.95\n        self.tau = 0.01\n        self.epsilon = 1.0\n        self.epsilon_decay = 0.995\n        self.epsilon_min = 0.1\n        self.losses = []\n        self.team_rewards = []\n    def get_actions(self, observations):\n        actions = []\n        with torch.no_grad():\n            for i, obs in enumerate(observations):\n                obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n                q_values = self.q_networks[i](obs_tensor)\n                if np.random.random() < self.epsilon:\n                    action = np.random.randint(self.action_dim)\n                else:\n                    action = q_values.argmax().item()\n                actions.append(action)\n        return actions\n    def mixing_forward(self, individual_q_values, state):\n        mixing_weights = self.mixing_network(state)\n        mixing_weights = mixing_weights.view(-1, self.num_agents, 32)\n        mixing_weights = torch.abs(mixing_weights)\n        individual_q_values = individual_q_values.unsqueeze(-1)\n        mixed_values = torch.bmm(mixing_weights.transpose(1, 2), individual_q_values)\n        mixed_values = mixed_values.squeeze(-1)\n        team_q_value = self.final_layer(mixed_values)\n        return team_q_value\n    def update(self, batch):\n        states, actions, rewards, next_states, dones = batch\n        batch_size = len(states)\n        states_tensor = torch.FloatTensor(states).to(device)\n        actions_tensor = torch.LongTensor(actions).to(device)\n        team_rewards = torch.FloatTensor([sum(r) for r in rewards]).to(device)\n        next_states_tensor = torch.FloatTensor(next_states).to(device)\n        dones_tensor = torch.BoolTensor(dones).to(device)\n        states_flat = states_tensor.view(batch_size, -1)\n        next_states_flat = next_states_tensor.view(batch_size, -1)\n        individual_q_values = []\n        for i in range(self.num_agents):\n            q_vals = self.q_networks[i](states_tensor[:, i])\n            chosen_q_vals = q_vals.gather(1, actions_tensor[:, i].unsqueeze(1)).squeeze()\n            individual_q_values.append(chosen_q_vals)\n        individual_q_values = torch.stack(individual_q_values, dim=1)\n        team_q_values = self.mixing_forward(individual_q_values, states_flat).squeeze()\n        with torch.no_grad():\n            next_individual_q_values = []\n            for i in range(self.num_agents):\n                next_q_vals = self.target_q_networks[i](next_states_tensor[:, i])\n                max_next_q_vals = next_q_vals.max(1)[0]\n                next_individual_q_values.append(max_next_q_vals)\n            next_individual_q_values = torch.stack(next_individual_q_values, dim=1)\n            target_mixing_weights = self.target_mixing_network(next_states_flat)\n            target_mixing_weights = target_mixing_weights.view(-1, self.num_agents, 32)\n            target_mixing_weights = torch.abs(target_mixing_weights)\n            next_individual_q_values_expanded = next_individual_q_values.unsqueeze(-1)\n            target_mixed_values = torch.bmm(\n                target_mixing_weights.transpose(1, 2), \n                next_individual_q_values_expanded\n            ).squeeze(-1)\n            target_team_q_values = self.target_final_layer(target_mixed_values).squeeze()\n            target_team_q_values = team_rewards + self.gamma * target_team_q_values * (~dones_tensor)\n        loss = F.mse_loss(team_q_values, target_team_q_values)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(\n            list(self.q_networks.parameters()) + \n            list(self.mixing_network.parameters()) + \n            list(self.final_layer.parameters()), \n            max_norm=1.0\n        )\n        self.optimizer.step()\n        if len(self.losses) % 100 == 0:\n            self.soft_update_targets()\n        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n        self.losses.append(loss.item())\n        self.team_rewards.append(team_rewards.mean().item())\n        return {\n            'loss': loss.item(),\n            'team_reward': team_rewards.mean().item(),\n            'epsilon': self.epsilon\n        }\n    def soft_update_targets(self):\n        for target, source in zip(self.target_q_networks, self.q_networks):\n            for target_param, param in zip(target.parameters(), source.parameters()):\n                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n        for target_param, param in zip(self.target_mixing_network.parameters(), \n                                      self.mixing_network.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n        for target_param, param in zip(self.target_final_layer.parameters(), \n                                      self.final_layer.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\nclass MultiAgentReplayBuffer:\n    def __init__(self, capacity, num_agents, obs_dim):\n        self.capacity = capacity\n        self.num_agents = num_agents\n        self.obs_dim = obs_dim\n        self.states = []\n        self.actions = []\n        self.rewards = []\n        self.next_states = []\n        self.dones = []\n        self.position = 0\n        self.size = 0\n    def push(self, state, action, reward, next_state, done):\n        if len(self.states) < self.capacity:\n            self.states.append(None)\n            self.actions.append(None)\n            self.rewards.append(None)\n            self.next_states.append(None)\n            self.dones.append(None)\n        self.states[self.position] = state\n        self.actions[self.position] = action\n        self.rewards[self.position] = reward\n        self.next_states[self.position] = next_state\n        self.dones[self.position] = done\n        self.position = (self.position + 1) % self.capacity\n        self.size = min(self.size + 1, self.capacity)\n    def sample(self, batch_size):\n        if self.size < batch_size:\n            return None\n        indices = np.random.choice(self.size, batch_size, replace=False)\n        batch_states = [self.states[i] for i in indices]\n        batch_actions = [self.actions[i] for i in indices]\n        batch_rewards = [self.rewards[i] for i in indices]\n        batch_next_states = [self.next_states[i] for i in indices]\n        batch_dones = [self.dones[i] for i in indices]\n        return (batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\ndef demonstrate_multi_agent_rl():\n    print(\"ðŸ¤ Demonstrating Multi-Agent Reinforcement Learning\")\n    print(\"=\" * 60)\n    env = MultiAgentEnvironment(grid_size=8, num_agents=4, num_targets=3)\n    obs_dim = env.observation_space\n    action_dim = env.action_space\n    num_agents = env.num_agents\n    maddpg_agents = [\n        MADDPGAgent(obs_dim, action_dim, num_agents, i) \n        for i in range(num_agents)\n    ]\n    state_dim = obs_dim * num_agents\n    qmix_agent = QMIXAgent(obs_dim, action_dim, num_agents, state_dim)\n    maddpg_buffer = MultiAgentReplayBuffer(capacity=50000, num_agents=num_agents, obs_dim=obs_dim)\n    qmix_buffer = MultiAgentReplayBuffer(capacity=50000, num_agents=num_agents, obs_dim=obs_dim)\n    results = {\n        'MADDPG': {'rewards': [], 'targets_collected': [], 'cooperation_rate': []},\n        'QMIX': {'rewards': [], 'targets_collected': [], 'cooperation_rate': []}\n    }\n    num_episodes = 500\n    batch_size = 32\n    for episode in range(num_episodes):\n        observations = env.reset()\n        episode_reward = 0\n        targets_collected = 0\n        cooperation_events = 0\n        for step in range(100):\n            actions = []\n            for i, agent in enumerate(maddpg_agents):\n                action = agent.get_action(observations[i], exploration_noise=0.1)\n                actions.append(action)\n            next_observations, rewards, done, info = env.step(actions)\n            maddpg_buffer.push(observations, actions, rewards, next_observations, done)\n            episode_reward += sum(rewards)\n            targets_collected = info['targets_collected']\n            if info['targets_collected'] > 0:\n                cooperation_events += 1\n            observations = next_observations\n            if done:\n                break\n        if maddpg_buffer.size > batch_size:\n            batch = maddpg_buffer.sample(batch_size)\n            for agent in maddpg_agents:\n                agent.update(batch, maddpg_agents)\n        results['MADDPG']['rewards'].append(episode_reward)\n        results['MADDPG']['targets_collected'].append(targets_collected)\n        results['MADDPG']['cooperation_rate'].append(cooperation_events / max(1, step + 1))\n        observations = env.reset()\n        episode_reward = 0\n        targets_collected = 0\n        cooperation_events = 0\n        for step in range(100):\n            actions = qmix_agent.get_actions(observations)\n            next_observations, rewards, done, info = env.step(actions)\n            qmix_buffer.push(observations, actions, rewards, next_observations, done)\n            episode_reward += sum(rewards)\n            targets_collected = info['targets_collected']\n            if info['targets_collected'] > 0:\n                cooperation_events += 1\n            observations = next_observations\n            if done:\n                break\n        if qmix_buffer.size > batch_size:\n            batch = qmix_buffer.sample(batch_size)\n            qmix_agent.update(batch)\n        results['QMIX']['rewards'].append(episode_reward)\n        results['QMIX']['targets_collected'].append(targets_collected)\n        results['QMIX']['cooperation_rate'].append(cooperation_events / max(1, step + 1))\n        if episode % 100 == 0:\n            print(f\"\\nEpisode {episode}:\")\n            for algo_name in ['MADDPG', 'QMIX']:\n                recent_rewards = np.mean(results[algo_name]['rewards'][-50:])\n                recent_targets = np.mean(results[algo_name]['targets_collected'][-50:])\n                recent_coop = np.mean(results[algo_name]['cooperation_rate'][-50:])\n                print(f\"  {algo_name}: Reward={recent_rewards:.2f}, Targets={recent_targets:.1f}, Cooperation={recent_coop:.3f}\")\n    return results, maddpg_agents, qmix_agent, env\nprint(\"ðŸš€ Starting Multi-Agent RL Training...\")\nmarl_results, maddpg_agents, qmix_agent, marl_env = demonstrate_multi_agent_rl()\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nfor algo_name, data in marl_results.items():\n    window_size = 20\n    if len(data['rewards']) >= window_size:\n        smoothed_rewards = pd.Series(data['rewards']).rolling(window_size).mean()\n        axes[0, 0].plot(smoothed_rewards, label=algo_name, linewidth=2)\naxes[0, 0].set_title('Multi-Agent RL Learning Curves')\naxes[0, 0].set_xlabel('Episode')\naxes[0, 0].set_ylabel('Total Team Reward (Smoothed)')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\nfor algo_name, data in marl_results.items():\n    window_size = 20\n    if len(data['targets_collected']) >= window_size:\n        smoothed_targets = pd.Series(data['targets_collected']).rolling(window_size).mean()\n        axes[0, 1].plot(smoothed_targets, label=algo_name, linewidth=2)\naxes[0, 1].set_title('Target Collection Efficiency')\naxes[0, 1].set_xlabel('Episode')\naxes[0, 1].set_ylabel('Targets Collected per Episode (Smoothed)')\naxes[0, 1].axhline(y=marl_env.num_targets, color='red', linestyle='--', label='Max Targets')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\nfor algo_name, data in marl_results.items():\n    window_size = 20\n    if len(data['cooperation_rate']) >= window_size:\n        smoothed_coop = pd.Series(data['cooperation_rate']).rolling(window_size).mean()\n        axes[1, 0].plot(smoothed_coop, label=algo_name, linewidth=2)\naxes[1, 0].set_title('Cooperation Rate Over Time')\naxes[1, 0].set_xlabel('Episode')\naxes[1, 0].set_ylabel('Cooperation Events per Step (Smoothed)')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\nfinal_performance = {}\nfor algo_name, data in marl_results.items():\n    final_reward = np.mean(data['rewards'][-50:])\n    final_targets = np.mean(data['targets_collected'][-50:])\n    final_coop = np.mean(data['cooperation_rate'][-50:])\n    axes[1, 1].bar(algo_name, final_reward, alpha=0.7)\n    final_performance[algo_name] = {\n        'reward': final_reward,\n        'targets': final_targets,\n        'cooperation': final_coop\n    }\naxes[1, 1].set_title('Final Performance Comparison')\naxes[1, 1].set_ylabel('Average Team Reward (Last 50 Episodes)')\naxes[1, 1].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint(\"\\nðŸ“Š Multi-Agent RL Results Summary:\")\nprint(\"=\" * 60)\nfor algo_name, performance in final_performance.items():\n    print(f\"\\n{algo_name}:\")\n    print(f\"  Final Team Reward: {performance['reward']:.2f}\")\n    print(f\"  Target Collection Rate: {performance['targets']:.2f}/{marl_env.num_targets}\")\n    print(f\"  Cooperation Rate: {performance['cooperation']:.3f}\")\n    print(f\"  Success Rate: {performance['targets']/marl_env.num_targets:.1%}\")\nprint(\"\\nðŸ’¡ Key Insights:\")\nprint(\"  â€¢ MADDPG uses centralized training with decentralized execution\")\nprint(\"  â€¢ QMIX factorizes team value function while maintaining monotonicity\")\nprint(\"  â€¢ Cooperation emerges through reward structure and learning\")\nprint(\"  â€¢ Multi-agent coordination improves with experience\")\nprint(\"\\nðŸ¤ Multi-Agent RL demonstration completed!\")\nprint(\"ðŸ”„ Ready for Robust RL implementation...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd8428",
   "metadata": {},
   "source": [
    "# Section 4: Robust Reinforcement Learning\n",
    "\n",
    "## 4.1 Theory: Handling Uncertainty and Adversarial Conditions\n",
    "\n",
    "Robust Reinforcement Learning addresses the challenge of learning policies that perform well under uncertainty, distributional shifts, and adversarial conditions. This is crucial for deploying RL agents in real-world environments where training and testing conditions may differ significantly.\n",
    "\n",
    "### Sources of Uncertainty in RL\n",
    "\n",
    "#### 1. Model Uncertainty\n",
    "- **Transition Dynamics**: $P(s'|s,a)$ may be unknown or changing\n",
    "- **Reward Function**: $R(s,a)$ may be noisy or non-stationary\n",
    "- **Initial State Distribution**: $\\mu_0(s)$ may vary between episodes\n",
    "\n",
    "#### 2. Environmental Uncertainty\n",
    "- **Observation Noise**: $o_t = s_t + \\epsilon_t$ where $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "- **Partial Observability**: Agent only observes $o_t$ instead of full state $s_t$\n",
    "- **Dynamic Environments**: Environment parameters change over time\n",
    "\n",
    "#### 3. Distributional Shift\n",
    "- **Covariate Shift**: $P_{train}(s) \\neq P_{test}(s)$\n",
    "- **Concept Drift**: $P_{train}(s'|s,a) \\neq P_{test}(s'|s,a)$\n",
    "- **Domain Gap**: Training and deployment environments differ\n",
    "\n",
    "### Mathematical Framework for Robust RL\n",
    "\n",
    "#### Robust Markov Decision Process (RMDP)\n",
    "An RMDP extends the standard MDP to handle uncertainty:\n",
    "$$\\text{RMDP} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma, \\mu_0)$$\n",
    "\n",
    "Where:\n",
    "- **$\\mathcal{P}$**: Uncertainty set of transition kernels\n",
    "- **$\\mathcal{R}$**: Uncertainty set of reward functions\n",
    "\n",
    "#### Robust Value Function\n",
    "The robust value function considers worst-case scenarios:\n",
    "$$V^{\\pi}_{robust}(s) = \\min_{P \\in \\mathcal{P}, R \\in \\mathcal{R}} V^{\\pi}_{P,R}(s)$$\n",
    "\n",
    "#### Distributionally Robust Optimization (DRO)\n",
    "Optimize performance over a set of probability distributions:\n",
    "$$\\max_\\pi \\min_{P \\in \\mathcal{P}} \\mathbb{E}_{P}[\\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t)]$$\n",
    "\n",
    "### Approaches to Robust RL\n",
    "\n",
    "#### 1. Domain Randomization\n",
    "**Idea**: Train on diverse environments to improve generalization\n",
    "\n",
    "**Implementation**:\n",
    "- Randomize environment parameters during training\n",
    "- Sample from distribution: $\\theta \\sim p(\\theta)$\n",
    "- Train policy to work across parameter space\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$J_{DR}(\\pi) = \\mathbb{E}_{\\theta \\sim p(\\theta)}[J_\\theta(\\pi)]$$\n",
    "\n",
    "#### 2. Adversarial Training\n",
    "**Idea**: Train against adversarial perturbations\n",
    "\n",
    "**Min-Max Objective**:\n",
    "$$\\max_\\pi \\min_{\\delta} \\mathbb{E}[R(s + \\delta, \\pi(s + \\delta))]$$\n",
    "\n",
    "Subject to: $||\\delta|| \\leq \\epsilon$\n",
    "\n",
    "#### 3. Distributional RL for Robustness\n",
    "**Idea**: Model full return distribution instead of expected value\n",
    "\n",
    "**Quantile Regression**:\n",
    "$$\\mathcal{L}(\\tau, \\hat{Z}) = \\mathbb{E}[(\\tau - \\mathbb{1}_{z < \\hat{Z}(\\tau)})(z - \\hat{Z}(\\tau))]$$\n",
    "\n",
    "#### 4. Bayesian RL\n",
    "**Idea**: Maintain uncertainty over model parameters\n",
    "\n",
    "**Posterior Update**:\n",
    "$$P(\\theta|D) \\propto P(D|\\theta)P(\\theta)$$\n",
    "\n",
    "**Thompson Sampling**:\n",
    "$$\\pi_t = \\arg\\max_\\pi \\mathbb{E}_{\\theta \\sim P(\\theta|D_t)}[V^\\pi_\\theta]$$\n",
    "\n",
    "## 4.2 Risk Measures in Robust RL\n",
    "\n",
    "### 1. Conditional Value at Risk (CVaR)\n",
    "Optimize worst-case expected returns:\n",
    "$$\\text{CVaR}_\\alpha(Z) = \\mathbb{E}[Z | Z \\leq \\text{VaR}_\\alpha(Z)]$$\n",
    "\n",
    "### 2. Coherent Risk Measures\n",
    "Risk measure $\\rho$ is coherent if it satisfies:\n",
    "- **Monotonicity**: $X \\geq Y \\Rightarrow \\rho(X) \\leq \\rho(Y)$\n",
    "- **Translation Invariance**: $\\rho(X + c) = \\rho(X) - c$\n",
    "- **Positive Homogeneity**: $\\rho(cX) = c\\rho(X)$ for $c \\geq 0$\n",
    "- **Subadditivity**: $\\rho(X + Y) \\leq \\rho(X) + \\rho(Y)$\n",
    "\n",
    "### 3. Entropic Risk Measure\n",
    "$$\\rho_\\beta(Z) = \\frac{1}{\\beta} \\log \\mathbb{E}[e^{-\\beta Z}]$$\n",
    "\n",
    "## 4.3 Uncertainty Quantification\n",
    "\n",
    "### 1. Epistemic vs Aleatoric Uncertainty\n",
    "- **Epistemic**: Model uncertainty (reducible with more data)\n",
    "- **Aleatoric**: Data uncertainty (irreducible noise)\n",
    "\n",
    "### 2. Ensemble Methods\n",
    "Maintain multiple models and aggregate predictions:\n",
    "$$\\mu(x) = \\frac{1}{M} \\sum_{i=1}^M f_i(x)$$\n",
    "$$\\sigma^2(x) = \\frac{1}{M} \\sum_{i=1}^M (f_i(x) - \\mu(x))^2$$\n",
    "\n",
    "### 3. Dropout-based Uncertainty\n",
    "Use Monte Carlo dropout for uncertainty estimation:\n",
    "$$\\mu(x) = \\frac{1}{T} \\sum_{t=1}^T f(x, \\epsilon_t)$$\n",
    "\n",
    "## 4.4 Applications of Robust RL\n",
    "\n",
    "### Autonomous Driving\n",
    "- **Uncertainties**: Weather conditions, other drivers' behavior\n",
    "- **Robustness**: Safe driving across diverse conditions\n",
    "- **Methods**: Domain randomization, distributional RL\n",
    "\n",
    "### Financial Trading\n",
    "- **Uncertainties**: Market volatility, regime changes\n",
    "- **Robustness**: Consistent performance across market conditions\n",
    "- **Methods**: Risk-sensitive RL, robust optimization\n",
    "\n",
    "### Healthcare\n",
    "- **Uncertainties**: Patient variability, measurement noise\n",
    "- **Robustness**: Safe treatment across patient populations\n",
    "- **Methods**: Bayesian RL, conservative policy optimization\n",
    "\n",
    "### Robotics\n",
    "- **Uncertainties**: Sensor noise, actuator failures, environmental changes\n",
    "- **Robustness**: Reliable operation in unstructured environments\n",
    "- **Methods**: Adaptive control, robust MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobustEnvironment:\n    def __init__(self, base_size=6, uncertainty_level=0.1, dynamic_obstacles=True):\n        self.base_size = base_size\n        self.uncertainty_level = uncertainty_level\n        self.dynamic_obstacles = dynamic_obstacles\n        self.current_size = base_size\n        self.noise_std = 0.0\n        self.action_failure_prob = 0.0\n        self.reward_noise_std = 0.0\n        self.reset()\n        self.action_space = 4\n        self.max_episode_steps = 100\n    def randomize_parameters(self):\n        size_variation = max(1, int(self.base_size * self.uncertainty_level))\n        self.current_size = np.random.randint(\n            max(3, self.base_size - size_variation),\n            self.base_size + size_variation + 1\n        )\n        self.noise_std = np.random.uniform(0, self.uncertainty_level)\n        self.action_failure_prob = np.random.uniform(0, self.uncertainty_level)\n        self.reward_noise_std = np.random.uniform(0, self.uncertainty_level * 5)\n        if self.dynamic_obstacles:\n            num_obstacles = np.random.randint(0, max(1, self.current_size // 2))\n            self.obstacles = []\n            for _ in range(num_obstacles):\n                obs_pos = [np.random.randint(1, self.current_size-1), \n                          np.random.randint(1, self.current_size-1)]\n                if obs_pos not in self.obstacles:\n                    self.obstacles.append(obs_pos)\n    def reset(self):\n        self.randomize_parameters()\n        self.agent_pos = [0, 0]\n        self.goal_pos = [self.current_size-1, self.current_size-1]\n        self.current_step = 0\n        if not hasattr(self, 'obstacles'):\n            self.obstacles = []\n        return self.get_observation()\n    def get_observation(self):\n        obs = np.array([\n            self.agent_pos[0] / self.current_size,\n            self.agent_pos[1] / self.current_size,\n            (self.goal_pos[0] - self.agent_pos[0]) / self.current_size,\n            (self.goal_pos[1] - self.agent_pos[1]) / self.current_size,\n            self.current_size / 10.0,\n            len(self.obstacles) / 10.0\n        ], dtype=np.float32)\n        if self.noise_std > 0:\n            noise = np.random.normal(0, self.noise_std, obs.shape)\n            obs += noise\n        return obs\n    def step(self, action):\n        self.current_step += 1\n        if np.random.random() < self.action_failure_prob:\n            action = 4\n        prev_pos = self.agent_pos.copy()\n        if action == 0 and self.agent_pos[1] < self.current_size - 1:\n            self.agent_pos[1] += 1\n        elif action == 1 and self.agent_pos[1] > 0:\n            self.agent_pos[1] -= 1\n        elif action == 2 and self.agent_pos[0] > 0:\n            self.agent_pos[0] -= 1\n        elif action == 3 and self.agent_pos[0] < self.current_size - 1:\n            self.agent_pos[0] += 1\n        if self.agent_pos in self.obstacles:\n            self.agent_pos = prev_pos\n            reward = -5.0\n        else:\n            done = (self.agent_pos == self.goal_pos)\n            if done:\n                reward = 10.0\n            else:\n                dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n                reward = -0.1 - 0.01 * dist\n        if self.reward_noise_std > 0:\n            reward += np.random.normal(0, self.reward_noise_std)\n        done = (self.agent_pos == self.goal_pos) or (self.current_step >= self.max_episode_steps)\n        info = {\n            'environment_size': self.current_size,\n            'noise_level': self.noise_std,\n            'action_failure_prob': self.action_failure_prob,\n            'obstacles': len(self.obstacles)\n        }\n        return self.get_observation(), reward, done, info\nclass DomainRandomizationAgent:\n    def __init__(self, obs_dim, action_dim, lr=3e-4):\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.policy_network = nn.Sequential(\n            nn.Linear(obs_dim, 256),\n            nn.LayerNorm(256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 256),\n            nn.LayerNorm(256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, action_dim),\n            nn.Softmax(dim=-1)\n        ).to(device)\n        self.value_network = nn.Sequential(\n            nn.Linear(obs_dim, 256),\n            nn.LayerNorm(256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 256),\n            nn.LayerNorm(256),\n            nn.ReLU(),\n            nn.Dropout(0.1),\n            nn.Linear(256, 1)\n        ).to(device)\n        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n        self.gamma = 0.99\n        self.clip_ratio = 0.2\n        self.policy_losses = []\n        self.value_losses = []\n        self.environment_diversity = []\n    def get_action(self, observation):\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(device)\n            action_probs = self.policy_network(obs_tensor)\n            action_dist = Categorical(action_probs)\n            action = action_dist.sample()\n            log_prob = action_dist.log_prob(action)\n            value = self.value_network(obs_tensor)\n        return action.item(), log_prob.item(), value.item()\n    def update(self, trajectories):\n        if not trajectories:\n            return None\n        all_obs, all_actions, all_rewards, all_log_probs, all_values = [], [], [], [], []\n        environment_params = []\n        for trajectory in trajectories:\n            obs, actions, rewards, log_probs, values, env_params = zip(*trajectory)\n            all_obs.extend(obs)\n            all_actions.extend(actions)\n            all_rewards.extend(rewards)\n            all_log_probs.extend(log_probs)\n            all_values.extend(values)\n            environment_params.extend(env_params)\n        observations = torch.FloatTensor(all_obs).to(device)\n        actions = torch.LongTensor(all_actions).to(device)\n        old_log_probs = torch.FloatTensor(all_log_probs).to(device)\n        values = torch.FloatTensor(all_values).to(device)\n        returns = self.compute_returns(trajectories)\n        advantages = returns - values\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        for _ in range(4):\n            action_probs = self.policy_network(observations)\n            action_dist = Categorical(action_probs)\\n            new_log_probs = action_dist.log_prob(actions)\n            ratio = torch.exp(new_log_probs - old_log_probs)\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n            policy_loss = -torch.min(surr1, surr2).mean()\n            self.policy_optimizer.zero_grad()\n            policy_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=0.5)\n            self.policy_optimizer.step()\n            new_values = self.value_network(observations).squeeze()\n            value_loss = F.mse_loss(new_values, returns)\n            self.value_optimizer.zero_grad()\n            value_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.value_network.parameters(), max_norm=0.5)\n            self.value_optimizer.step()\n        self.policy_losses.append(policy_loss.item())\n        self.value_losses.append(value_loss.item())\n        unique_sizes = len(set([params['environment_size'] for params in environment_params]))\n        avg_noise = np.mean([params['noise_level'] for params in environment_params])\n        self.environment_diversity.append({'unique_sizes': unique_sizes, 'avg_noise': avg_noise})\n        return {\n            'policy_loss': policy_loss.item(),\n            'value_loss': value_loss.item(),\n            'environment_diversity': unique_sizes\n        }\n    def compute_returns(self, trajectories):\n        all_returns = []\n        for trajectory in trajectories:\n            rewards = [step[2] for step in trajectory]\n            returns = []\n            G = 0\n            for reward in reversed(rewards):\n                G = reward + self.gamma * G\n                returns.insert(0, G)\n            all_returns.extend(returns)\n        return torch.FloatTensor(all_returns).to(device)\nclass AdversarialRobustAgent:\n    def __init__(self, obs_dim, action_dim, lr=3e-4, adversarial_strength=0.1):\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.adversarial_strength = adversarial_strength\n        self.policy_network = nn.Sequential(\n            nn.Linear(obs_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim),\n            nn.Softmax(dim=-1)\n        ).to(device)\n        self.value_network = nn.Sequential(\n            nn.Linear(obs_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        ).to(device)\n        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n        self.gamma = 0.99\n        self.robust_losses = []\n        self.adversarial_losses = []\n        self.perturbation_norms = []\n    def generate_adversarial_observation(self, observation):\n        obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(device)\n        obs_tensor.requires_grad_(True)\n        action_probs = self.policy_network(obs_tensor)\n        entropy_loss = -(action_probs * torch.log(action_probs + 1e-8)).sum()\n        entropy_loss.backward()\n        with torch.no_grad():\n            gradient = obs_tensor.grad.data\n            perturbation = self.adversarial_strength * torch.sign(gradient)\n            adversarial_obs = obs_tensor + perturbation\n            adversarial_obs = torch.clamp(adversarial_obs, -2.0, 2.0)\n            self.perturbation_norms.append(torch.norm(perturbation).item())\n        return adversarial_obs.squeeze().cpu().numpy()\n    def get_action(self, observation, use_adversarial=True):\n        with torch.no_grad():\n            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(device)\n            action_probs = self.policy_network(obs_tensor)\n            action_dist = Categorical(action_probs)\n            action = action_dist.sample()\n            log_prob = action_dist.log_prob(action)\n            value = self.value_network(obs_tensor)\n        original_action = action.item()\n        if use_adversarial:\n            adversarial_obs = self.generate_adversarial_observation(observation)\n            with torch.no_grad():\n                adv_obs_tensor = torch.FloatTensor(adversarial_obs).unsqueeze(0).to(device)\n                adv_action_probs = self.policy_network(adv_obs_tensor)\n                adv_action_dist = Categorical(adv_action_probs)\n                adv_action = adv_action_dist.sample()\n            return original_action, log_prob.item(), value.item()\n        return original_action, log_prob.item(), value.item()\n    def update(self, trajectories):\n        if not trajectories:\n            return None\n        all_obs, all_actions, all_rewards, all_log_probs, all_values = [], [], [], [], []\n        for trajectory in trajectories:\n            obs, actions, rewards, log_probs, values, _ = zip(*trajectory)\n            all_obs.extend(obs)\n            all_actions.extend(actions)\n            all_rewards.extend(rewards)\n            all_log_probs.extend(log_probs)\n            all_values.extend(values)\n        observations = torch.FloatTensor(all_obs).to(device)\n        actions = torch.LongTensor(all_actions).to(device)\n        all_returns = []\n        for trajectory in trajectories:\n            rewards = [step[2] for step in trajectory]\n            returns = []\n            G = 0\n            for reward in reversed(rewards):\n                G = reward + self.gamma * G\n                returns.insert(0, G)\n            all_returns.extend(returns)\n        returns = torch.FloatTensor(all_returns).to(device)\n        values = torch.FloatTensor(all_values).to(device)\n        advantages = returns - values\n        action_probs = self.policy_network(observations)\n        action_dist = Categorical(action_probs)\n        log_probs = action_dist.log_prob(actions)\n        policy_loss = -(log_probs * advantages.detach()).mean()\n        value_loss = F.mse_loss(values, returns)\n        adversarial_loss = 0\n        for i in range(min(32, len(observations))):\n            obs = observations[i]\n            obs_adv = obs.clone().detach()\n            obs_adv.requires_grad_(True)\n            action_probs_adv = self.policy_network(obs_adv.unsqueeze(0))\n            entropy = -(action_probs_adv * torch.log(action_probs_adv + 1e-8)).sum()\n            grad = torch.autograd.grad(entropy, obs_adv, create_graph=True)[0]\n            perturbation = self.adversarial_strength * torch.sign(grad)\n            obs_adversarial = obs + perturbation\n            action_probs_original = self.policy_network(obs.unsqueeze(0))\n            action_probs_adversarial = self.policy_network(obs_adversarial.unsqueeze(0))\n            kl_loss = F.kl_div(\n                torch.log(action_probs_adversarial + 1e-8),\n                action_probs_original,\n                reduction='batchmean'\n            )\n            adversarial_loss += kl_loss\n        adversarial_loss /= min(32, len(observations))\n        total_policy_loss = policy_loss + 0.1 * adversarial_loss\n        self.policy_optimizer.zero_grad()\n        total_policy_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=0.5)\n        self.policy_optimizer.step()\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.value_network.parameters(), max_norm=0.5)\n        self.value_optimizer.step()\n        self.robust_losses.append(total_policy_loss.item())\n        self.adversarial_losses.append(adversarial_loss.item())\n        return {\n            'policy_loss': policy_loss.item(),\n            'value_loss': value_loss.item(),\n            'adversarial_loss': adversarial_loss.item(),\n            'total_loss': total_policy_loss.item()\n        }\ndef collect_robust_trajectory(env, agent, max_steps=100):\n    trajectory = []\n    observation = env.reset()\n    for step in range(max_steps):\n        action, log_prob, value = agent.get_action(observation)\n        next_observation, reward, done, info = env.step(action)\n        trajectory.append((\n            observation.copy(), action, reward, log_prob, value, info.copy()\n        ))\n        if done:\n            break\n        observation = next_observation\n    return trajectory\ndef demonstrate_robust_rl():\n    print(\"ðŸ›¡ï¸ Demonstrating Robust Reinforcement Learning\")\n    print(\"=\" * 60)\n    environments = {\n        'low_uncertainty': RobustEnvironment(base_size=6, uncertainty_level=0.1),\n        'medium_uncertainty': RobustEnvironment(base_size=6, uncertainty_level=0.3),\n        'high_uncertainty': RobustEnvironment(base_size=6, uncertainty_level=0.5)\n    }\n    obs_dim = 6\n    action_dim = 4\n    agents = {\n        'Domain_Randomization': DomainRandomizationAgent(obs_dim, action_dim),\n        'Adversarial_Training': AdversarialRobustAgent(obs_dim, action_dim, adversarial_strength=0.1)\n    }\n    results = {name: {\n        'rewards': {env_name: [] for env_name in environments.keys()},\n        'robustness_score': [],\n        'adaptation_rate': []\n    } for name in agents.keys()}\n    num_episodes = 400\n    trajectories_per_update = 5\n    for episode in range(num_episodes):\n        for agent_name, agent in agents.items():\n            all_trajectories = []\n            episode_rewards = {env_name: [] for env_name in environments.keys()}\n            for env_name, env in environments.items():\n                env_trajectories = []\n                for _ in range(trajectories_per_update):\n                    trajectory = collect_robust_trajectory(env, agent)\n                    env_trajectories.append(trajectory)\n                    episode_reward = sum(step[2] for step in trajectory)\n                    episode_rewards[env_name].append(episode_reward)\n                all_trajectories.extend(env_trajectories)\n            if all_trajectories:\n                update_info = agent.update(all_trajectories)\n            for env_name in environments.keys():\n                results[agent_name]['rewards'][env_name].extend(episode_rewards[env_name])\n            if episode_rewards['low_uncertainty'] and episode_rewards['high_uncertainty']:\n                low_perf = np.mean(episode_rewards['low_uncertainty'])\n                high_perf = np.mean(episode_rewards['high_uncertainty'])\n                robustness_score = high_perf / (low_perf + 1e-8)\n                results[agent_name]['robustness_score'].append(robustness_score)\n        if episode % 100 == 0:\n            print(f\"\\nEpisode {episode}:\")\n            for agent_name in agents.keys():\n                if results[agent_name]['robustness_score']:\n                    recent_robustness = np.mean(results[agent_name]['robustness_score'][-10:])\n                    low_perf = np.mean(results[agent_name]['rewards']['low_uncertainty'][-20:])\n                    high_perf = np.mean(results[agent_name]['rewards']['high_uncertainty'][-20:])\n                    print(f\"  {agent_name}:\")\n                    print(f\"    Low Uncertainty: {low_perf:.2f}\")\n                    print(f\"    High Uncertainty: {high_perf:.2f}\")\n                    print(f\"    Robustness Score: {recent_robustness:.3f}\")\n    return results, agents, environments\nprint(\"ðŸš€ Starting Robust RL Training...\")\nrobust_results, robust_agents, robust_environments = demonstrate_robust_rl()\nfig, axes = plt.subplots(2, 2, figsize=(15, 12))\nuncertainty_levels = ['low_uncertainty', 'medium_uncertainty', 'high_uncertainty']\ncolors = ['green', 'orange', 'red']\nfor i, agent_name in enumerate(robust_agents.keys()):\n    for j, (env_name, color) in enumerate(zip(uncertainty_levels, colors)):\n        window_size = 20\n        rewards = robust_results[agent_name]['rewards'][env_name]\n        if len(rewards) >= window_size:\n            smoothed_rewards = pd.Series(rewards).rolling(window_size).mean()\n            axes[i, 0].plot(smoothed_rewards, label=f'{env_name.replace(\"_\", \" \").title()}', \n                          color=color, linewidth=2)\n    axes[i, 0].set_title(f'{agent_name} - Performance vs Uncertainty')\n    axes[i, 0].set_xlabel('Episode')\n    axes[i, 0].set_ylabel('Episode Reward (Smoothed)')\n    axes[i, 0].legend()\n    axes[i, 0].grid(True, alpha=0.3)\nfor agent_name in robust_agents.keys():\n    window_size = 10\n    robustness_scores = robust_results[agent_name]['robustness_score']\n    if len(robustness_scores) >= window_size:\n        smoothed_robustness = pd.Series(robustness_scores).rolling(window_size).mean()\n        axes[0, 1].plot(smoothed_robustness, label=agent_name, linewidth=2)\naxes[0, 1].set_title('Robustness Score Over Time')\naxes[0, 1].set_xlabel('Episode')\naxes[0, 1].set_ylabel('Robustness Score (High/Low Performance)')\naxes[0, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Perfect Robustness')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\nfinal_performance = {}\nfor agent_name in robust_agents.keys():\n    final_low = np.mean(robust_results[agent_name]['rewards']['low_uncertainty'][-50:])\n    final_high = np.mean(robust_results[agent_name]['rewards']['high_uncertainty'][-50:])\n    final_performance[agent_name] = {'low': final_low, 'high': final_high}\nagents = list(final_performance.keys())\nx = np.arange(len(agents))\nwidth = 0.35\nbars1 = axes[1, 1].bar(x - width/2, [final_performance[agent]['low'] for agent in agents], \n                       width, label='Low Uncertainty', alpha=0.8, color='green')\nbars2 = axes[1, 1].bar(x + width/2, [final_performance[agent]['high'] for agent in agents], \n                       width, label='High Uncertainty', alpha=0.8, color='red')\naxes[1, 1].set_title('Final Performance Comparison')\naxes[1, 1].set_ylabel('Average Reward (Last 50 Episodes)')\naxes[1, 1].set_xticks(x)\naxes[1, 1].set_xticklabels([name.replace('_', '\\n') for name in agents])\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint(\"\\nðŸ“Š Robust RL Results Summary:\")\nprint(\"=\" * 60)\nfor agent_name in robust_agents.keys():\n    print(f\"\\n{agent_name}:\")\n    low_perf = final_performance[agent_name]['low']\n    high_perf = final_performance[agent_name]['high']\n    performance_drop = (low_perf - high_perf) / low_perf * 100\n    print(f\"  Low Uncertainty Performance: {low_perf:.2f}\")\n    print(f\"  High Uncertainty Performance: {high_perf:.2f}\")\n    print(f\"  Performance Drop: {performance_drop:.1f}%\")\n    print(f\"  Robustness Score: {high_perf/low_perf:.3f}\")\nif 'Adversarial_Training' in robust_agents:\n    adv_agent = robust_agents['Adversarial_Training']\n    if hasattr(adv_agent, 'perturbation_norms') and adv_agent.perturbation_norms:\n        avg_perturbation = np.mean(adv_agent.perturbation_norms[-100:])\n        print(f\"\\nAdversarial Training Statistics:\")\n        print(f\"  Average Perturbation Norm: {avg_perturbation:.4f}\")\n        print(f\"  Adversarial Strength: {adv_agent.adversarial_strength:.3f}\")\nprint(\"\\nðŸ’¡ Key Insights:\")\nprint(\"  â€¢ Domain randomization improves generalization across environments\")\nprint(\"  â€¢ Adversarial training enhances robustness to input perturbations\")\nprint(\"  â€¢ Robustness often comes at the cost of peak performance\")\nprint(\"  â€¢ Uncertainty quantification helps assess model confidence\")\nprint(\"  â€¢ Real-world deployment requires robust policies\")\nprint(\"\\nðŸ›¡ï¸ Robust RL demonstration completed!\")\nprint(\"ðŸŽ¯ Advanced Deep RL CA14 implementation finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95638aa8",
   "metadata": {},
   "source": [
    "# Section 5: Comprehensive Evaluation and Real-World Applications\n",
    "\n",
    "This final section provides a comprehensive evaluation framework for comparing all advanced Deep RL methods and discusses real-world deployment considerations.\n",
    "\n",
    "## 5.1 Comprehensive Evaluation Framework\n",
    "\n",
    "Advanced Deep RL methods must be evaluated across multiple dimensions to understand their practical utility:\n",
    "\n",
    "### Performance Metrics\n",
    "- **Sample Efficiency**: How quickly algorithms learn from data\n",
    "- **Asymptotic Performance**: Final performance after convergence\n",
    "- **Robustness**: Performance under distribution shift and uncertainty\n",
    "- **Safety**: Constraint satisfaction and risk mitigation\n",
    "- **Scalability**: Performance in large-scale multi-agent settings\n",
    "\n",
    "### Evaluation Dimensions\n",
    "1. **Data Efficiency**: Offline vs. online learning requirements\n",
    "2. **Safety Constraints**: Hard vs. soft constraint satisfaction\n",
    "3. **Multi-Agent Coordination**: Centralized vs. decentralized approaches\n",
    "4. **Robustness**: Uncertainty handling and domain transfer\n",
    "5. **Computational Requirements**: Training and inference costs\n",
    "\n",
    "## 5.2 Real-World Deployment Considerations\n",
    "\n",
    "### Critical Factors for Practical Applications\n",
    "1. **Safety First**: Hard safety constraints in critical systems\n",
    "2. **Data Availability**: Leveraging existing datasets vs. online exploration\n",
    "3. **Coordination Requirements**: Multi-agent collaboration and competition\n",
    "4. **Environment Uncertainty**: Handling model mismatch and distribution shift\n",
    "5. **Regulatory Compliance**: Meeting industry standards and regulations\n",
    "\n",
    "### Application Domains\n",
    "- **Autonomous Vehicles**: Safe navigation with offline learning from driving data\n",
    "- **Financial Trading**: Multi-agent market interactions with risk constraints\n",
    "- **Healthcare**: Safe treatment optimization with limited data\n",
    "- **Robotics**: Robust manipulation under environmental uncertainty\n",
    "- **Energy Management**: Multi-agent coordination in smart grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bfc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComprehensiveEvaluator:\n    def __init__(self):\n        self.evaluation_metrics = {\n            'sample_efficiency': [],\n            'asymptotic_performance': [],\n            'robustness_score': [],\n            'safety_violations': [],\n            'coordination_effectiveness': [],\n            'computational_cost': []\n        }\n        self.method_results = {}\n    def evaluate_sample_efficiency(self, training_curves, convergence_threshold=0.8):\n        efficiency_scores = {}\n        for method_name, rewards in training_curves.items():\n            if not rewards:\n                efficiency_scores[method_name] = float('inf')\n                continue\n            max_reward = max(rewards)\n            target_reward = convergence_threshold * max_reward\n            convergence_episode = len(rewards)\n            for i, reward in enumerate(rewards):\n                if reward >= target_reward:\n                    convergence_episode = i\n                    break\n            efficiency_scores[method_name] = convergence_episode\n        return efficiency_scores\n    def evaluate_asymptotic_performance(self, training_curves, final_episodes=50):\n        asymptotic_scores = {}\n        for method_name, rewards in training_curves.items():\n            if len(rewards) >= final_episodes:\n                asymptotic_scores[method_name] = np.mean(rewards[-final_episodes:])\n            else:\n                asymptotic_scores[method_name] = np.mean(rewards) if rewards else 0.0\n        return asymptotic_scores\n    def evaluate_robustness(self, agents, test_environments, num_episodes=50):\n        robustness_scores = {}\n        for agent_name, agent in agents.items():\n            environment_performances = []\n            for env_name, env in test_environments.items():\n                episode_rewards = []\n                for episode in range(num_episodes):\n                    obs = env.reset()\n                    total_reward = 0\n                    done = False\n                    while not done:\n                        if hasattr(agent, 'get_action'):\n                            if len(inspect.signature(agent.get_action).parameters) > 1:\n                                action, _, _ = agent.get_action(obs)\n                            else:\n                                action = agent.get_action(obs)\n                        else:\n                            action = np.random.randint(env.action_space)\n                        obs, reward, done, _ = env.step(action)\n                        total_reward += reward\n                    episode_rewards.append(total_reward)\n                environment_performances.append(np.mean(episode_rewards))\n            if environment_performances:\n                min_perf = min(environment_performances)\n                max_perf = max(environment_performances)\n                robustness_scores[agent_name] = min_perf / max_perf if max_perf > 0 else 0.0\n            else:\n                robustness_scores[agent_name] = 0.0\n        return robustness_scores\n    def evaluate_safety(self, agents, safe_environment, num_episodes=100):\n        safety_scores = {}\n        for agent_name, agent in agents.items():\n            violations = 0\n            total_steps = 0\n            for episode in range(num_episodes):\n                obs = safe_environment.reset()\n                done = False\n                while not done:\n                    if hasattr(agent, 'get_action'):\n                        if len(inspect.signature(agent.get_action).parameters) > 1:\n                            action, _, _ = agent.get_action(obs)\n                        else:\n                            action = agent.get_action(obs)\n                    else:\n                        action = np.random.randint(safe_environment.action_space)\n                    obs, reward, done, info = safe_environment.step(action)\n                    total_steps += 1\n                    if hasattr(safe_environment, 'constraint_violation'):\n                        if safe_environment.constraint_violation:\n                            violations += 1\n                    elif 'constraint_violation' in info:\n                        if info['constraint_violation']:\n                            violations += 1\n                    elif reward < -1.0:\n                        violations += 1\n            safety_scores[agent_name] = violations / total_steps if total_steps > 0 else 1.0\n        return safety_scores\n    def evaluate_coordination(self, multi_agent_results):\n        coordination_scores = {}\n        for method_name, results in multi_agent_results.items():\n            if 'coordination_rewards' in results:\n                individual_perf = results.get('individual_performance', 0)\n                coordinated_perf = np.mean(results['coordination_rewards'][-50:])\n                coordination_scores[method_name] = coordinated_perf - individual_perf\n            else:\n                coordination_scores[method_name] = 0.0\n        return coordination_scores\n    def compute_comprehensive_score(self, method_results):\n        comprehensive_scores = {}\n        metrics = ['sample_efficiency', 'asymptotic_performance', 'robustness_score', \n                  'safety_score', 'coordination_effectiveness']\n        normalized_scores = {}\n        for metric in metrics:\n            if metric in method_results:\n                values = list(method_results[metric].values())\n                if values:\n                    if metric == 'sample_efficiency':\n                        min_val, max_val = min(values), max(values)\n                        normalized_scores[metric] = {\n                            method: 1 - (score - min_val) / (max_val - min_val) if max_val > min_val else 1.0\n                            for method, score in method_results[metric].items()\n                        }\n                    elif metric == 'safety_score':\n                        min_val, max_val = min(values), max(values)\n                        normalized_scores[metric] = {\n                            method: 1 - (score - min_val) / (max_val - min_val) if max_val > min_val else 1.0\n                            for method, score in method_results[metric].items()\n                        }\n                    else:\n                        min_val, max_val = min(values), max(values)\n                        normalized_scores[metric] = {\n                            method: (score - min_val) / (max_val - min_val) if max_val > min_val else 1.0\n                            for method, score in method_results[metric].items()\n                        }\n        weights = {\n            'sample_efficiency': 0.2,\n            'asymptotic_performance': 0.25,\n            'robustness_score': 0.25,\n            'safety_score': 0.2,\n            'coordination_effectiveness': 0.1\n        }\n        methods = set()\n        for metric_scores in normalized_scores.values():\n            methods.update(metric_scores.keys())\n        for method in methods:\n            score = 0\n            weight_sum = 0\n            for metric, weight in weights.items():\n                if metric in normalized_scores and method in normalized_scores[metric]:\n                    score += weight * normalized_scores[metric][method]\n                    weight_sum += weight\n            comprehensive_scores[method] = score / weight_sum if weight_sum > 0 else 0.0\n        return comprehensive_scores, normalized_scores\ndef create_evaluation_environments():\n    environments = {\n        'standard': RobustEnvironment(base_size=6, uncertainty_level=0.0),\n        'noisy': RobustEnvironment(base_size=6, uncertainty_level=0.2),\n        'large': RobustEnvironment(base_size=8, uncertainty_level=0.1),\n        'obstacles': RobustEnvironment(base_size=6, uncertainty_level=0.1, dynamic_obstacles=True)\n    }\n    return environments\ndef run_comprehensive_evaluation():\n    print(\"ðŸ” Starting Comprehensive Evaluation of Advanced Deep RL Methods\")\n    print(\"=\" * 70)\n    evaluator = ComprehensiveEvaluator()\n    test_environments = create_evaluation_environments()\n    all_agents = {}\n    if 'offline_agents' in globals():\n        for name, agent in offline_agents.items():\n            all_agents[f'Offline_{name}'] = agent\n    if 'safe_agents' in globals():\n        for name, agent in safe_agents.items():\n            all_agents[f'Safe_{name}'] = agent\n    if 'ma_agents' in globals():\n        for name, agent_list in ma_agents.items():\n            if isinstance(agent_list, list) and len(agent_list) > 0:\n                all_agents[f'MultiAgent_{name}'] = agent_list[0]\n            else:\n                all_agents[f'MultiAgent_{name}'] = agent_list\n    if 'robust_agents' in globals():\n        for name, agent in robust_agents.items():\n            all_agents[f'Robust_{name}'] = agent\n    training_curves = {}\n    if 'offline_results' in globals():\n        for name in offline_results.keys():\n            if 'episode_rewards' in offline_results[name]:\n                training_curves[f'Offline_{name}'] = offline_results[name]['episode_rewards']\n    if 'safe_results' in globals():\n        for name in safe_results.keys():\n            if 'rewards' in safe_results[name]:\n                training_curves[f'Safe_{name}'] = safe_results[name]['rewards']\n    if 'ma_results' in globals():\n        for name in ma_results.keys():\n            if 'episode_rewards' in ma_results[name]:\n                training_curves[f'MultiAgent_{name}'] = ma_results[name]['episode_rewards']\n    if 'robust_results' in globals():\n        for name in robust_results.keys():\n            if 'rewards' in robust_results[name] and 'low_uncertainty' in robust_results[name]['rewards']:\n                training_curves[f'Robust_{name}'] = robust_results[name]['rewards']['low_uncertainty']\n    print(f\"ðŸ“Š Evaluating {len(all_agents)} methods across {len(test_environments)} environments\")\n    evaluation_results = {}\n    print(\"âš¡ Evaluating sample efficiency...\")\n    efficiency_scores = evaluator.evaluate_sample_efficiency(training_curves)\n    evaluation_results['sample_efficiency'] = efficiency_scores\n    print(\"ðŸŽ¯ Evaluating asymptotic performance...\")\n    asymptotic_scores = evaluator.evaluate_asymptotic_performance(training_curves)\n    evaluation_results['asymptotic_performance'] = asymptotic_scores\n    print(\"ðŸ›¡ï¸ Evaluating robustness...\")\n    robustness_scores = evaluator.evaluate_robustness(all_agents, test_environments)\n    evaluation_results['robustness_score'] = robustness_scores\n    print(\"ðŸš¨ Evaluating safety...\")\n    if 'safe_envs' in globals():\n        safe_env = list(safe_envs.values())[0] if safe_envs else test_environments['standard']\n    else:\n        safe_env = test_environments['standard']\n    safety_scores = evaluator.evaluate_safety(all_agents, safe_env)\n    evaluation_results['safety_score'] = safety_scores\n    print(\"ðŸ¤ Evaluating coordination...\")\n    coordination_scores = {}\n    if 'ma_results' in globals():\n        coordination_scores = evaluator.evaluate_coordination(ma_results)\n    evaluation_results['coordination_effectiveness'] = coordination_scores\n    print(\"ðŸ“ˆ Computing comprehensive scores...\")\n    comprehensive_scores, normalized_scores = evaluator.compute_comprehensive_score(evaluation_results)\n    return evaluation_results, comprehensive_scores, normalized_scores\nprint(\"ðŸš€ Starting Comprehensive Evaluation...\")\neval_results, comprehensive_scores, normalized_scores = run_comprehensive_evaluation()\nfig, axes = plt.subplots(2, 3, figsize=(18, 12))\nif 'sample_efficiency' in eval_results and eval_results['sample_efficiency']:\n    methods = list(eval_results['sample_efficiency'].keys())\n    scores = list(eval_results['sample_efficiency'].values())\n    bars = axes[0, 0].bar(range(len(methods)), scores, alpha=0.8, color='skyblue')\n    axes[0, 0].set_title('Sample Efficiency\\n(Episodes to Convergence)')\n    axes[0, 0].set_xlabel('Methods')\n    axes[0, 0].set_ylabel('Episodes')\n    axes[0, 0].set_xticks(range(len(methods)))\n    axes[0, 0].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n    axes[0, 0].grid(True, alpha=0.3)\nif 'asymptotic_performance' in eval_results and eval_results['asymptotic_performance']:\n    methods = list(eval_results['asymptotic_performance'].keys())\n    scores = list(eval_results['asymptotic_performance'].values())\n    bars = axes[0, 1].bar(range(len(methods)), scores, alpha=0.8, color='lightgreen')\n    axes[0, 1].set_title('Asymptotic Performance\\n(Final Reward)')\n    axes[0, 1].set_xlabel('Methods')\n    axes[0, 1].set_ylabel('Average Reward')\n    axes[0, 1].set_xticks(range(len(methods)))\n    axes[0, 1].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n    axes[0, 1].grid(True, alpha=0.3)\nif 'robustness_score' in eval_results and eval_results['robustness_score']:\n    methods = list(eval_results['robustness_score'].keys())\n    scores = list(eval_results['robustness_score'].values())\n    bars = axes[0, 2].bar(range(len(methods)), scores, alpha=0.8, color='orange')\n    axes[0, 2].set_title('Robustness Score\\n(Min/Max Performance)')\n    axes[0, 2].set_xlabel('Methods')\n    axes[0, 2].set_ylabel('Robustness Score')\n    axes[0, 2].set_xticks(range(len(methods)))\n    axes[0, 2].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n    axes[0, 2].axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Perfect Robustness')\n    axes[0, 2].legend()\n    axes[0, 2].grid(True, alpha=0.3)\nif 'safety_score' in eval_results and eval_results['safety_score']:\n    methods = list(eval_results['safety_score'].keys())\n    scores = [1 - score for score in eval_results['safety_score'].values()]\n    bars = axes[1, 0].bar(range(len(methods)), scores, alpha=0.8, color='lightcoral')\n    axes[1, 0].set_title('Safety Score\\n(1 - Violation Rate)')\n    axes[1, 0].set_xlabel('Methods')\n    axes[1, 0].set_ylabel('Safety Score')\n    axes[1, 0].set_xticks(range(len(methods)))\n    axes[1, 0].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n    axes[1, 0].grid(True, alpha=0.3)\nif comprehensive_scores:\n    methods = list(comprehensive_scores.keys())\n    scores = list(comprehensive_scores.values())\n    colors = plt.cm.viridis(np.linspace(0, 1, len(methods)))\n    bars = axes[1, 1].bar(range(len(methods)), scores, alpha=0.8, color=colors)\n    axes[1, 1].set_title('Comprehensive Score\\n(Weighted Average)')\n    axes[1, 1].set_xlabel('Methods')\n    axes[1, 1].set_ylabel('Comprehensive Score')\n    axes[1, 1].set_xticks(range(len(methods)))\n    axes[1, 1].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n    axes[1, 1].grid(True, alpha=0.3)\nif comprehensive_scores and len(comprehensive_scores) >= 3:\n    top_methods = sorted(comprehensive_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n    categories = ['Sample\\nEfficiency', 'Asymptotic\\nPerformance', 'Robustness', \n                 'Safety', 'Coordination']\n    N = len(categories)\n    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n    angles += angles[:1]\n    ax_radar = plt.subplot(2, 3, 6, projection='polar')\n    colors = ['red', 'green', 'blue']\n    for i, (method, _) in enumerate(top_methods):\n        values = []\n        for metric in ['sample_efficiency', 'asymptotic_performance', 'robustness_score', \n                      'safety_score', 'coordination_effectiveness']:\n            if metric in normalized_scores and method in normalized_scores[metric]:\n                values.append(normalized_scores[metric][method])\n            else:\n                values.append(0.0)\n        values += values[:1]\n        ax_radar.plot(angles, values, 'o-', linewidth=2, label=method.replace('_', ' '), color=colors[i])\n        ax_radar.fill(angles, values, alpha=0.25, color=colors[i])\n    ax_radar.set_xticks(angles[:-1])\n    ax_radar.set_xticklabels(categories)\n    ax_radar.set_ylim(0, 1)\n    ax_radar.set_title('Top 3 Methods Comparison', y=1.08)\n    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n    ax_radar.grid(True)\nplt.tight_layout()\nplt.show()\nprint(\"\\nðŸ“Š Comprehensive Evaluation Results:\")\nprint(\"=\" * 70)\nprint(\"\\nðŸ† Top 3 Overall Methods:\")\nif comprehensive_scores:\n    sorted_methods = sorted(comprehensive_scores.items(), key=lambda x: x[1], reverse=True)\n    for i, (method, score) in enumerate(sorted_methods[:3]):\n        print(f\"  {i+1}. {method}: {score:.3f}\")\nprint(\"\\nðŸ“ˆ Detailed Metrics:\")\nfor metric_name, metric_results in eval_results.items():\n    if metric_results:\n        print(f\"\\n{metric_name.replace('_', ' ').title()}:\")\n        sorted_results = sorted(metric_results.items(), key=lambda x: x[1], \n                              reverse=(metric_name not in ['sample_efficiency', 'safety_score']))\n        for method, score in sorted_results:\n            if metric_name == 'sample_efficiency':\n                print(f\"  {method}: {score:.0f} episodes\")\n            elif metric_name == 'safety_score':\n                print(f\"  {method}: {(1-score)*100:.1f}% safety rate\")\n            else:\n                print(f\"  {method}: {score:.3f}\")\nprint(\"\\nðŸ’¡ Key Insights:\")\nprint(\"  â€¢ Offline RL excels in data efficiency but may lack adaptability\")\nprint(\"  â€¢ Safe RL provides constraint satisfaction at performance cost\")\nprint(\"  â€¢ Multi-agent RL enables coordination but increases complexity\")\nprint(\"  â€¢ Robust RL handles uncertainty but requires more computation\")\nprint(\"  â€¢ Real-world applications require careful method selection\")\nprint(\"  â€¢ Hybrid approaches often provide best overall performance\")\nprint(\"\\nðŸŒŸ Recommendations for Deployment:\")\nprint(\"  â€¢ Safety-critical: Prioritize Safe RL methods\")\nprint(\"  â€¢ Limited data: Use Offline RL with safety constraints\")\nprint(\"  â€¢ Multi-agent settings: MADDPG for continuous, QMIX for discrete\")\nprint(\"  â€¢ Uncertain environments: Domain randomization + adversarial training\")\nprint(\"  â€¢ Production systems: Comprehensive evaluation before deployment\")\nprint(\"\\nðŸŽ¯ Comprehensive evaluation completed!\")\nprint(\"ðŸ“š Advanced Deep RL CA14 notebook fully implemented!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753945c",
   "metadata": {},
   "source": [
    "# Summary and Conclusions\n",
    "\n",
    "## Key Takeaways from Advanced Deep RL\n",
    "\n",
    "This comprehensive exploration of advanced Deep Reinforcement Learning has covered the cutting-edge methods essential for real-world deployment:\n",
    "\n",
    "### ðŸŽ¯ Core Advanced RL Paradigms\n",
    "\n",
    "1. **Offline Reinforcement Learning**\n",
    "   - **Conservative Q-Learning (CQL)**: Addresses overestimation bias in offline settings\n",
    "   - **Implicit Q-Learning (IQL)**: Avoids distributional shift through expectile regression\n",
    "   - **Key Insight**: Essential for domains with existing data but limited online interaction\n",
    "\n",
    "2. **Safe Reinforcement Learning**\n",
    "   - **Constrained Policy Optimization (CPO)**: Hard constraint satisfaction\n",
    "   - **Lagrangian Methods**: Adaptive penalty balancing performance and safety\n",
    "   - **Key Insight**: Critical for safety-critical applications where violations are unacceptable\n",
    "\n",
    "3. **Multi-Agent Reinforcement Learning**\n",
    "   - **MADDPG**: Centralized training, decentralized execution for continuous control\n",
    "   - **QMIX**: Value function factorization for discrete action coordination\n",
    "   - **Key Insight**: Enables coordination in complex multi-agent environments\n",
    "\n",
    "4. **Robust Reinforcement Learning**\n",
    "   - **Domain Randomization**: Training across diverse environment configurations\n",
    "   - **Adversarial Training**: Robustness to input perturbations and model uncertainty\n",
    "   - **Key Insight**: Essential for deployment in uncertain, dynamic real-world environments\n",
    "\n",
    "### ðŸŒŸ Practical Implementation Insights\n",
    "\n",
    "- **Hyperparameter Sensitivity**: Advanced methods often require careful tuning\n",
    "- **Computational Requirements**: Robust methods need more resources but provide better generalization\n",
    "- **Data Requirements**: Offline methods leverage existing data, online methods need exploration\n",
    "- **Safety Trade-offs**: Safe methods may sacrifice peak performance for constraint satisfaction\n",
    "- **Scalability Considerations**: Multi-agent methods face coordination complexity\n",
    "\n",
    "### ðŸš€ Real-World Applications\n",
    "\n",
    "Advanced Deep RL methods are revolutionizing multiple domains:\n",
    "\n",
    "- **Autonomous Systems**: Safe navigation with offline learning from human demonstrations\n",
    "- **Financial Markets**: Multi-agent trading with risk constraints and robustness\n",
    "- **Healthcare**: Safe treatment optimization with limited data and safety constraints\n",
    "- **Robotics**: Robust manipulation under environmental uncertainty\n",
    "- **Resource Management**: Multi-agent coordination in smart grids and logistics\n",
    "\n",
    "### ðŸ”¬ Future Directions\n",
    "\n",
    "The field continues evolving toward:\n",
    "\n",
    "1. **Hybrid Approaches**: Combining offline, safe, multi-agent, and robust techniques\n",
    "2. **Foundation Models**: Pre-trained RL models for downstream adaptation\n",
    "3. **Neurosymbolic RL**: Incorporating symbolic reasoning for interpretability\n",
    "4. **Continual Learning**: Adaptation without catastrophic forgetting\n",
    "5. **Human-AI Collaboration**: Interactive learning from human feedback\n",
    "\n",
    "### ðŸ“š Educational Impact\n",
    "\n",
    "This CA14 demonstrates that mastering advanced Deep RL requires:\n",
    "\n",
    "- **Theoretical Understanding**: Mathematical foundations of each paradigm\n",
    "- **Practical Implementation**: Hands-on experience with state-of-the-art algorithms\n",
    "- **Critical Evaluation**: Comprehensive assessment across multiple metrics\n",
    "- **Real-world Perspective**: Understanding deployment challenges and trade-offs\n",
    "\n",
    "### ðŸŽ–ï¸ Final Reflection\n",
    "\n",
    "Advanced Deep Reinforcement Learning represents the frontier of artificial intelligence, enabling autonomous agents to learn, adapt, and operate safely in complex, uncertain, and multi-agent environments. The methods explored in this comprehensive study provide the tools necessary for the next generation of intelligent systems that will transform industries and improve human lives.\n",
    "\n",
    "The journey from basic RL to these advanced paradigms illustrates the rapid evolution of the field and highlights the importance of continuous learning and adaptation in both our algorithms and our understanding of intelligence itself.\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ¯ Congratulations on completing this comprehensive exploration of Advanced Deep Reinforcement Learning!**\n",
    "\n",
    "*This marks the culmination of your journey through cutting-edge RL methods. The knowledge and skills gained here will serve as a foundation for tackling the most challenging problems in artificial intelligence and machine learning.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pythonProject1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}