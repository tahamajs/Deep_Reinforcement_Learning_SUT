{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15851f8a",
   "metadata": {},
   "source": [
    "# CA7: Deep Q-Networks (DQN) and Value-Based Methods\n",
    "## Deep Reinforcement Learning - Session 7\n",
    "\n",
    "### Course Information\n",
    "- **Course**: Deep Reinforcement Learning\n",
    "- **Session**: 7\n",
    "- **Topic**: Deep Q-Networks (DQN) and Advanced Value-Based Methods\n",
    "- **Focus**: Complete theoretical foundations and practical implementations\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Theoretical Foundations**:\n",
    "   - Q-learning and its limitations in complex environments\n",
    "   - Deep Q-Networks (DQN) architecture and training process\n",
    "   - Experience replay and target networks\n",
    "   - Double DQN and addressing overestimation bias\n",
    "   - Dueling DQN and advantage decomposition\n",
    "   - Prioritized experience replay\n",
    "\n",
    "2. **Implementation Skills**:\n",
    "   - Complete DQN implementation from scratch\n",
    "   - Experience replay buffer design and management\n",
    "   - Target network updates and stability techniques\n",
    "   - Advanced variants: Double DQN, Dueling DQN, Rainbow DQN\n",
    "   - Performance analysis and debugging techniques\n",
    "\n",
    "3. **Practical Applications**:\n",
    "   - Training DQN on classic control and Atari environments\n",
    "   - Hyperparameter tuning and optimization strategies\n",
    "   - Comparison with policy gradient methods\n",
    "   - Real-world applications and limitations\n",
    "\n",
    "### Contents Overview\n",
    "\n",
    "1. **Section 1**: Theoretical Foundations of Deep Q-Learning\n",
    "2. **Section 2**: Basic DQN Implementation and Core Concepts\n",
    "3. **Section 3**: Experience Replay and Target Networks\n",
    "4. **Section 4**: Double DQN and Overestimation Bias\n",
    "5. **Section 5**: Dueling DQN and Value Decomposition\n",
    "6. **Section 6**: Prioritized Experience Replay\n",
    "7. **Section 7**: Rainbow DQN - Combining All Improvements\n",
    "8. **Section 8**: Performance Analysis and Comparisons\n",
    "9. **Section 9**: Advanced Topics and Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398268ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport gymnasium as gym\nimport random\nimport collections\nfrom collections import deque, namedtuple\nimport warnings\nimport math\nfrom typing import List, Tuple, Optional, Dict, Any\nfrom dataclasses import dataclass\nimport time\nimport os\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nplt.style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\nwarnings.filterwarnings('ignore')\nExperience = namedtuple('Experience', ['state', 'action', 'reward', 'next_state', 'done'])\nprint(\"=\"*60)\nprint(\"CA7: Deep Q-Networks (DQN) and Value-Based Methods\")\nprint(\"=\"*60)\nprint(\"Environment setup complete!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Gymnasium version: {gym.__version__}\")\nprint(f\"Device: {device}\")\nprint(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c28ac4",
   "metadata": {},
   "source": [
    "# Section 1: Theoretical Foundations of Deep Q-Learning\n",
    "\n",
    "## 1.1 From Tabular Q-Learning to Deep Q-Networks\n",
    "\n",
    "Traditional Q-learning works well for discrete, small state spaces where we can maintain a Q-table. However, in complex environments like Atari games or continuous control tasks, the state space becomes enormous, making tabular methods impractical.\n",
    "\n",
    "### The Q-Learning Foundation\n",
    "\n",
    "The Q-learning update rule is:\n",
    "```\n",
    "Q(s, a) ← Q(s, a) + α[r + γ max Q(s', a') - Q(s, a)]\n",
    "                                a'\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Q(s, a)`: Action-value function\n",
    "- `α`: Learning rate\n",
    "- `r`: Reward\n",
    "- `γ`: Discount factor\n",
    "- `s'`: Next state\n",
    "\n",
    "### The Deep Q-Network Approach\n",
    "\n",
    "DQN replaces the Q-table with a deep neural network `Q(s, a; θ)` that approximates the Q-values for all actions given a state. The network parameters `θ` are updated to minimize the temporal difference (TD) error.\n",
    "\n",
    "## 1.2 Core Challenges in Deep Q-Learning\n",
    "\n",
    "### 1. Instability and Divergence\n",
    "- Neural networks can be unstable when used with bootstrapping\n",
    "- Updates can cause the target to change rapidly\n",
    "- Non-stationary target problem\n",
    "\n",
    "### 2. Correlation in Sequential Data\n",
    "- RL data is highly correlated (sequential states)\n",
    "- Violates the i.i.d. assumption of supervised learning\n",
    "- Can lead to poor generalization\n",
    "\n",
    "### 3. Overestimation Bias\n",
    "- Max operator in Q-learning can lead to overestimation\n",
    "- Amplified in function approximation settings\n",
    "- Can cause instability and poor performance\n",
    "\n",
    "## 1.3 DQN Innovations\n",
    "\n",
    "### Experience Replay\n",
    "- Store experiences in a replay buffer\n",
    "- Sample random batches for training\n",
    "- Breaks correlation and improves sample efficiency\n",
    "\n",
    "### Target Network\n",
    "- Use a separate target network for computing targets\n",
    "- Update target network periodically\n",
    "- Provides stability during training\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The DQN loss function is:\n",
    "```\n",
    "L(θ) = E[(r + γ max Q(s', a'; θ⁻) - Q(s, a; θ))²]\n",
    "                   a'\n",
    "```\n",
    "\n",
    "Where `θ⁻` represents the parameters of the target network.\n",
    "\n",
    "## 1.4 Algorithmic Overview\n",
    "\n",
    "1. **Initialize** main network Q(s,a;θ) and target network Q(s,a;θ⁻)\n",
    "2. **Initialize** experience replay buffer D\n",
    "3. **For each episode**:\n",
    "   - **For each step**:\n",
    "     - Select action using ε-greedy policy\n",
    "     - Execute action and observe reward and next state\n",
    "     - Store experience in replay buffer\n",
    "     - Sample random batch from replay buffer\n",
    "     - Compute target values using target network\n",
    "     - Update main network parameters\n",
    "     - Periodically update target network\n",
    "\n",
    "This foundation enables us to tackle complex, high-dimensional problems that were previously intractable with traditional Q-learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3373469a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetworkVisualization:\n    def __init__(self):\n        self.fig_count = 0\n    def visualize_q_learning_concepts(self):\n        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n        ax = axes[0, 0]\n        states = ['S1', 'S2', 'S3', 'S4']\n        actions = ['Up', 'Down', 'Left', 'Right']\n        q_before = np.random.rand(4, 4) * 10\n        q_after = q_before.copy()\n        q_after[1, 2] += 2\n        im1 = ax.imshow(q_before, cmap='viridis', aspect='auto')\n        ax.set_title('Q-Values Before Update')\n        ax.set_xticks(range(4))\n        ax.set_xticklabels(actions)\n        ax.set_yticks(range(4))\n        ax.set_yticklabels(states)\n        for i in range(4):\n            for j in range(4):\n                ax.text(j, i, f'{q_before[i, j]:.1f}', ha='center', va='center', color='white')\n        plt.colorbar(im1, ax=ax)\n        ax = axes[0, 1]\n        episodes = np.arange(1, 101)\n        sequential_loss = 10 * np.exp(-episodes/30) + np.random.normal(0, 0.5, 100)\n        replay_loss = 8 * np.exp(-episodes/20) + np.random.normal(0, 0.3, 100)\n        ax.plot(episodes, sequential_loss, label='Sequential Training', alpha=0.7, linewidth=2)\n        ax.plot(episodes, replay_loss, label='Experience Replay', alpha=0.7, linewidth=2)\n        ax.fill_between(episodes, sequential_loss, alpha=0.3)\n        ax.fill_between(episodes, replay_loss, alpha=0.3)\n        ax.set_title('Learning Curves: Sequential vs Replay')\n        ax.set_xlabel('Training Episodes')\n        ax.set_ylabel('Loss')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 0]\n        steps = np.arange(0, 1000)\n        main_q = 10 + np.cumsum(np.random.normal(0, 0.1, 1000))\n        target_q = []\n        current_target = 10\n        for i, step in enumerate(steps):\n            if step % 100 == 0 and step > 0:\n                current_target = main_q[i]\n            target_q.append(current_target)\n        ax.plot(steps, main_q, label='Main Network Q(s,a)', alpha=0.8, linewidth=1)\n        ax.plot(steps, target_q, label='Target Network Q(s,a)', alpha=0.8, linewidth=2, drawstyle='steps-post')\n        ax.set_title('Target Network Update Schedule')\n        ax.set_xlabel('Training Steps')\n        ax.set_ylabel('Q-Value')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 1]\n        episodes = np.arange(0, 1000)\n        epsilon_decay = 0.995\n        epsilon_min = 0.01\n        epsilon_values = []\n        epsilon = 1.0\n        for episode in episodes:\n            epsilon_values.append(epsilon)\n            epsilon = max(epsilon_min, epsilon * epsilon_decay)\n        ax.plot(episodes, epsilon_values, linewidth=3, color='red')\n        ax.fill_between(episodes, epsilon_values, alpha=0.3, color='red')\n        ax.set_title('ε-Greedy Exploration Schedule')\n        ax.set_xlabel('Training Episodes')\n        ax.set_ylabel('Epsilon Value')\n        ax.grid(True, alpha=0.3)\n        ax.set_ylim(0, 1.1)\n        plt.tight_layout()\n        plt.show()\n    def demonstrate_overestimation_bias(self):\n        np.random.seed(42)\n        true_q_values = np.array([1.0, 2.0, 1.5, 0.8, 2.2])\n        noise_std = 0.5\n        num_estimates = 1000\n        estimates = []\n        max_estimates = []\n        for _ in range(num_estimates):\n            noisy_q = true_q_values + np.random.normal(0, noise_std, len(true_q_values))\n            estimates.append(noisy_q)\n            max_estimates.append(np.max(noisy_q))\n        estimates = np.array(estimates)\n        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n        ax = axes[0]\n        ax.hist(max_estimates, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n        ax.axvline(np.max(true_q_values), color='red', linestyle='--', linewidth=2, \n                  label=f'True Max: {np.max(true_q_values):.2f}')\n        ax.axvline(np.mean(max_estimates), color='green', linestyle='--', linewidth=2,\n                  label=f'Estimated Max: {np.mean(max_estimates):.2f}')\n        ax.set_title('Overestimation Bias in Max Q-Values')\n        ax.set_xlabel('Max Q-Value')\n        ax.set_ylabel('Frequency')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1]\n        positions = np.arange(len(true_q_values))\n        violin_parts = ax.violinplot([estimates[:, i] for i in range(len(true_q_values))], \n                                    positions=positions, showmeans=True, showmedians=True)\n        ax.scatter(positions, true_q_values, color='red', s=100, zorder=10, \n                  label='True Q-Values', marker='D')\n        ax.set_title('Q-Value Distributions with Noise')\n        ax.set_xlabel('Actions')\n        ax.set_ylabel('Q-Values')\n        ax.set_xticks(positions)\n        ax.set_xticklabels([f'A{i}' for i in range(len(true_q_values))])\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        bias = np.mean(max_estimates) - np.max(true_q_values)\n        print(f\"Overestimation Bias: {bias:.3f}\")\n        print(f\"True Maximum Q-Value: {np.max(true_q_values):.3f}\")\n        print(f\"Average Estimated Maximum: {np.mean(max_estimates):.3f}\")\nvisualizer = QNetworkVisualization()\nprint(\"1. Visualizing Core Q-Learning Concepts...\")\nvisualizer.visualize_q_learning_concepts()\nprint(\"\\n2. Demonstrating Overestimation Bias...\")\nvisualizer.demonstrate_overestimation_bias()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f500ed",
   "metadata": {},
   "source": [
    "# Section 2: Basic DQN Implementation and Core Concepts\n",
    "\n",
    "## 2.1 Deep Q-Network Architecture\n",
    "\n",
    "The DQN architecture typically consists of:\n",
    "\n",
    "1. **Input Layer**: Processes the state representation (e.g., raw pixels or feature vectors)\n",
    "2. **Hidden Layers**: Fully connected or convolutional layers for feature extraction\n",
    "3. **Output Layer**: Outputs Q-values for all possible actions\n",
    "\n",
    "### Key Design Principles:\n",
    "\n",
    "- **State Preprocessing**: Normalize inputs for stable training\n",
    "- **Network Depth**: Balance between expressiveness and training stability\n",
    "- **Activation Functions**: ReLU is commonly used for hidden layers\n",
    "- **Output Layer**: Linear activation for Q-value regression\n",
    "\n",
    "## 2.2 Experience Replay Buffer\n",
    "\n",
    "The replay buffer serves several critical functions:\n",
    "\n",
    "1. **Decorrelation**: Breaks temporal correlations in sequential data\n",
    "2. **Sample Efficiency**: Allows multiple updates from the same experience\n",
    "3. **Stability**: Provides more stable gradients through diverse batches\n",
    "\n",
    "### Buffer Operations:\n",
    "- **Store**: Add new experiences\n",
    "- **Sample**: Randomly sample batches for training\n",
    "- **Update**: Maintain buffer size limits\n",
    "\n",
    "## 2.3 Training Loop and Key Components\n",
    "\n",
    "The DQN training process involves:\n",
    "\n",
    "1. **Action Selection**: ε-greedy exploration strategy\n",
    "2. **Environment Interaction**: Execute actions and collect experiences\n",
    "3. **Experience Storage**: Add experiences to replay buffer\n",
    "4. **Network Updates**: Sample batches and perform gradient descent\n",
    "5. **Target Network Updates**: Periodic synchronization for stability\n",
    "\n",
    "Let's implement these core components step by step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a5f668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256]):\n        super(DQN, self).__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        layers = []\n        prev_dim = state_dim\n        for hidden_dim in hidden_dims:\n            layers.append(nn.Linear(prev_dim, hidden_dim))\n            layers.append(nn.ReLU())\n            prev_dim = hidden_dim\n        layers.append(nn.Linear(prev_dim, action_dim))\n        self.network = nn.Sequential(*layers)\n        for layer in self.network:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_uniform_(layer.weight)\n                nn.init.constant_(layer.bias, 0)\n    def forward(self, state):\n        return self.network(state)\n    def get_action(self, state, epsilon=0.0):\n        if random.random() < epsilon:\n            return random.randint(0, self.action_dim - 1)\n        else:\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n                q_values = self.forward(state_tensor)\n                return q_values.argmax().item()\nclass ReplayBuffer:\n    def __init__(self, capacity=100000):\n        self.buffer = deque(maxlen=capacity)\n        self.capacity = capacity\n    def push(self, state, action, reward, next_state, done):\n        experience = Experience(state, action, reward, next_state, done)\n        self.buffer.append(experience)\n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        states = torch.FloatTensor([e.state for e in batch]).to(device)\n        actions = torch.LongTensor([e.action for e in batch]).to(device)\n        rewards = torch.FloatTensor([e.reward for e in batch]).to(device)\n        next_states = torch.FloatTensor([e.next_state for e in batch]).to(device)\n        dones = torch.BoolTensor([e.done for e in batch]).to(device)\n        return states, actions, rewards, next_states, dones\n    def __len__(self):\n        return len(self.buffer)\nclass DQNAgent:\n    def __init__(self, state_dim, action_dim, lr=1e-4, gamma=0.99, \n                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995,\n                 buffer_size=100000, batch_size=64, target_update_freq=1000):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.epsilon = epsilon_start\n        self.epsilon_end = epsilon_end\n        self.epsilon_decay = epsilon_decay\n        self.batch_size = batch_size\n        self.target_update_freq = target_update_freq\n        self.q_network = DQN(state_dim, action_dim).to(device)\n        self.target_network = DQN(state_dim, action_dim).to(device)\n        self.target_network.load_state_dict(self.q_network.state_dict())\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n        self.replay_buffer = ReplayBuffer(buffer_size)\n        self.training_step = 0\n        self.episode_rewards = []\n        self.losses = []\n        self.q_values_history = []\n        self.epsilon_history = []\n    def select_action(self, state):\n        return self.q_network.get_action(state, self.epsilon)\n    def store_experience(self, state, action, reward, next_state, done):\n        self.replay_buffer.push(state, action, reward, next_state, done)\n    def update_target_network(self):\n        self.target_network.load_state_dict(self.q_network.state_dict())\n    def train_step(self):\n        if len(self.replay_buffer) < self.batch_size:\n            return None\n        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            next_q_values = self.target_network(next_states).max(1)[0]\n            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n        loss = F.mse_loss(current_q_values, target_q_values)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        self.training_step += 1\n        if self.training_step % self.target_update_freq == 0:\n            self.update_target_network()\n        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n        self.losses.append(loss.item())\n        self.epsilon_history.append(self.epsilon)\n        with torch.no_grad():\n            avg_q_value = current_q_values.mean().item()\n            self.q_values_history.append(avg_q_value)\n        return loss.item()\n    def train_episode(self, env, max_steps=1000):\n        state, _ = env.reset()\n        episode_reward = 0\n        step_count = 0\n        for step in range(max_steps):\n            action = self.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            self.store_experience(state, action, reward, next_state, done)\n            loss = self.train_step()\n            episode_reward += reward\n            step_count += 1\n            state = next_state\n            if done:\n                break\n        self.episode_rewards.append(episode_reward)\n        return episode_reward, step_count\n    def evaluate(self, env, num_episodes=10, render=False):\n        eval_rewards = []\n        original_epsilon = self.epsilon\n        self.epsilon = 0.0\n        for episode in range(num_episodes):\n            state, _ = env.reset()\n            episode_reward = 0\n            while True:\n                action = self.select_action(state)\n                state, reward, terminated, truncated, _ = env.step(action)\n                episode_reward += reward\n                if terminated or truncated:\n                    break\n            eval_rewards.append(episode_reward)\n        self.epsilon = original_epsilon\n        return {\n            'mean_reward': np.mean(eval_rewards),\n            'std_reward': np.std(eval_rewards),\n            'min_reward': np.min(eval_rewards),\n            'max_reward': np.max(eval_rewards)\n        }\n    def get_q_values(self, state):\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n            return self.q_network(state_tensor).cpu().numpy().flatten()\ndef test_basic_dqn():\n    print(\"=\"*60)\n    print(\"Testing Basic DQN Implementation\")\n    print(\"=\"*60)\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    print(f\"Environment: CartPole-v1\")\n    print(f\"State dimension: {state_dim}\")\n    print(f\"Action dimension: {action_dim}\")\n    agent = DQNAgent(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        lr=1e-3,\n        gamma=0.99,\n        epsilon_start=1.0,\n        epsilon_end=0.01,\n        epsilon_decay=0.995,\n        buffer_size=10000,\n        batch_size=64,\n        target_update_freq=100\n    )\n    num_episodes = 200\n    print(f\"\\nTraining for {num_episodes} episodes...\")\n    episode_rewards = []\n    for episode in range(num_episodes):\n        reward, steps = agent.train_episode(env)\n        episode_rewards.append(reward)\n        if (episode + 1) % 50 == 0:\n            eval_results = agent.evaluate(env, num_episodes=5)\n            print(f\"Episode {episode+1:3d} | \"\n                  f\"Train Reward: {reward:6.1f} | \"\n                  f\"Eval Reward: {eval_results['mean_reward']:6.1f} ± {eval_results['std_reward']:4.1f} | \"\n                  f\"Epsilon: {agent.epsilon:.3f} | \"\n                  f\"Buffer Size: {len(agent.replay_buffer)}\")\n    print(f\"\\n{'='*60}\")\n    print(\"Final Evaluation\")\n    print(f\"{'='*60}\")\n    final_eval = agent.evaluate(env, num_episodes=20)\n    print(f\"Mean Reward: {final_eval['mean_reward']:.2f} ± {final_eval['std_reward']:.2f}\")\n    print(f\"Min Reward: {final_eval['min_reward']:.2f}\")\n    print(f\"Max Reward: {final_eval['max_reward']:.2f}\")\n    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n    window = 10\n    smoothed_rewards = pd.Series(episode_rewards).rolling(window).mean()\n    axes[0, 0].plot(episode_rewards, alpha=0.3, color='blue', label='Episode Rewards')\n    axes[0, 0].plot(smoothed_rewards, color='red', linewidth=2, label=f'Moving Average ({window})')\n    axes[0, 0].set_title('Learning Curve')\n    axes[0, 0].set_xlabel('Episode')\n    axes[0, 0].set_ylabel('Episode Reward')\n    axes[0, 0].legend()\n    axes[0, 0].grid(True, alpha=0.3)\n    if agent.losses:\n        loss_window = 50\n        smoothed_losses = pd.Series(agent.losses).rolling(loss_window).mean()\n        axes[0, 1].plot(agent.losses, alpha=0.3, color='orange', label='Training Loss')\n        axes[0, 1].plot(smoothed_losses, color='red', linewidth=2, label=f'Moving Average ({loss_window})')\n        axes[0, 1].set_title('Training Loss')\n        axes[0, 1].set_xlabel('Training Step')\n        axes[0, 1].set_ylabel('MSE Loss')\n        axes[0, 1].legend()\n        axes[0, 1].grid(True, alpha=0.3)\n    axes[1, 0].plot(agent.epsilon_history)\n    axes[1, 0].set_title('Epsilon Decay')\n    axes[1, 0].set_xlabel('Training Step')\n    axes[1, 0].set_ylabel('Epsilon')\n    axes[1, 0].grid(True, alpha=0.3)\n    if agent.q_values_history:\n        q_window = 50\n        smoothed_q = pd.Series(agent.q_values_history).rolling(q_window).mean()\n        axes[1, 1].plot(agent.q_values_history, alpha=0.3, color='green', label='Q-Values')\n        axes[1, 1].plot(smoothed_q, color='red', linewidth=2, label=f'Moving Average ({q_window})')\n        axes[1, 1].set_title('Q-Values Evolution')\n        axes[1, 1].set_xlabel('Training Step')\n        axes[1, 1].set_ylabel('Average Q-Value')\n        axes[1, 1].legend()\n        axes[1, 1].grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    env.close()\n    return agent, episode_rewards\nbasic_agent, basic_rewards = test_basic_dqn()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c054e6ec",
   "metadata": {},
   "source": [
    "# Section 3: Experience Replay and Target Networks - Deep Dive\n",
    "\n",
    "## 3.1 Experience Replay: Breaking the Correlation Chain\n",
    "\n",
    "Experience replay is one of the most crucial innovations in DQN. It addresses several fundamental challenges in deep reinforcement learning:\n",
    "\n",
    "### Problems with Sequential Training\n",
    "1. **Temporal Correlation**: Consecutive states are highly correlated\n",
    "2. **Non-stationarity**: The data distribution changes as the policy evolves\n",
    "3. **Sample Inefficiency**: Each experience is used only once\n",
    "4. **Catastrophic Forgetting**: New experiences can overwrite important past learning\n",
    "\n",
    "### Benefits of Experience Replay\n",
    "1. **Decorrelation**: Random sampling breaks temporal dependencies\n",
    "2. **Sample Efficiency**: Multiple learning updates from each experience\n",
    "3. **Stability**: More stable gradients from diverse batches\n",
    "4. **Better Generalization**: Exposure to wider range of state-action pairs\n",
    "\n",
    "## 3.2 Target Networks: Stabilizing the Moving Target\n",
    "\n",
    "The target network addresses the moving target problem in Q-learning:\n",
    "\n",
    "### The Problem\n",
    "In standard Q-learning, both the predicted Q-value and the target Q-value are computed using the same network, creating instability:\n",
    "- As we update Q(s,a), the target for the next state Q(s',a') also changes\n",
    "- This can lead to oscillations and divergence\n",
    "\n",
    "### The Solution\n",
    "- Maintain two networks: main (online) and target\n",
    "- Use target network to compute stable targets\n",
    "- Update target network less frequently than main network\n",
    "\n",
    "### Update Strategies\n",
    "1. **Hard Updates**: Periodic copying of main network weights\n",
    "2. **Soft Updates**: Gradual blending with momentum (used in DDPG)\n",
    "\n",
    "## 3.3 Mathematical Analysis\n",
    "\n",
    "Let's analyze the impact of these components on learning stability and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cac2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplayAnalyzer:\n    def __init__(self):\n        self.results = {}\n    def compare_replay_strategies(self):\n        print(\"=\"*70)\n        print(\"Experience Replay Strategy Comparison\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        strategies = {\n            'No Replay': {'buffer_size': 1, 'batch_size': 1},\n            'Small Buffer': {'buffer_size': 1000, 'batch_size': 32},\n            'Large Buffer': {'buffer_size': 50000, 'batch_size': 64},\n        }\n        results = {}\n        num_episodes = 100\n        for strategy_name, config in strategies.items():\n            print(f\"\\nTesting {strategy_name}...\")\n            agent = DQNAgent(\n                state_dim=state_dim,\n                action_dim=action_dim,\n                buffer_size=config['buffer_size'],\n                batch_size=config['batch_size'],\n                lr=1e-3,\n                epsilon_decay=0.99,\n                target_update_freq=100\n            )\n            episode_rewards = []\n            losses = []\n            for episode in range(num_episodes):\n                reward, _ = agent.train_episode(env, max_steps=500)\n                episode_rewards.append(reward)\n                if len(agent.losses) > len(losses):\n                    losses.extend(agent.losses[len(losses):])\n                if (episode + 1) % 25 == 0:\n                    avg_reward = np.mean(episode_rewards[-25:])\n                    print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n            results[strategy_name] = {\n                'rewards': episode_rewards,\n                'losses': losses,\n                'final_performance': np.mean(episode_rewards[-20:])\n            }\n        self.results['replay_comparison'] = results\n        fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n        ax = axes[0]\n        colors = ['red', 'blue', 'green']\n        for i, (strategy, data) in enumerate(results.items()):\n            rewards = data['rewards']\n            smoothed = pd.Series(rewards).rolling(10).mean()\n            ax.plot(smoothed, label=strategy, color=colors[i], linewidth=2)\n            ax.fill_between(range(len(smoothed)), smoothed, alpha=0.3, color=colors[i])\n        ax.set_title('Learning Curves by Replay Strategy')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Episode Reward (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1]\n        strategies_list = list(results.keys())\n        final_perfs = [results[s]['final_performance'] for s in strategies_list]\n        bars = ax.bar(strategies_list, final_perfs, alpha=0.7, color=colors)\n        ax.set_title('Final Performance Comparison')\n        ax.set_ylabel('Average Reward (Last 20 Episodes)')\n        ax.set_xticklabels(strategies_list, rotation=45)\n        for bar, perf in zip(bars, final_perfs):\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                   f'{perf:.1f}', ha='center', va='bottom')\n        ax = axes[2]\n        for i, (strategy, data) in enumerate(results.items()):\n            if len(data['losses']) > 10:\n                losses = data['losses']\n                smoothed_losses = pd.Series(losses).rolling(50).mean()\n                ax.plot(smoothed_losses, label=strategy, color=colors[i], linewidth=2, alpha=0.7)\n        ax.set_title('Training Loss Comparison')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('MSE Loss (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        env.close()\n        return results\n    def analyze_target_network_frequency(self):\n        print(\"=\"*70)\n        print(\"Target Network Update Frequency Analysis\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        frequencies = {\n            'Very Frequent (10)': 10,\n            'Frequent (100)': 100,\n            'Moderate (500)': 500,\n            'Infrequent (1000)': 1000,\n        }\n        results = {}\n        num_episodes = 150\n        for freq_name, freq_value in frequencies.items():\n            print(f\"\\nTesting {freq_name}...\")\n            agent = DQNAgent(\n                state_dim=state_dim,\n                action_dim=action_dim,\n                target_update_freq=freq_value,\n                lr=1e-3,\n                epsilon_decay=0.99\n            )\n            episode_rewards = []\n            q_value_stds = []\n            for episode in range(num_episodes):\n                reward, _ = agent.train_episode(env, max_steps=500)\n                episode_rewards.append(reward)\n                if episode % 10 == 0 and len(agent.replay_buffer) > 1000:\n                    sample_states, _, _, _, _ = agent.replay_buffer.sample(100)\n                    with torch.no_grad():\n                        q_vals = agent.q_network(sample_states)\n                        q_std = q_vals.std().item()\n                        q_value_stds.append(q_std)\n                if (episode + 1) % 50 == 0:\n                    avg_reward = np.mean(episode_rewards[-25:])\n                    print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n            results[freq_name] = {\n                'rewards': episode_rewards,\n                'q_stds': q_value_stds,\n                'final_performance': np.mean(episode_rewards[-20:])\n            }\n        self.results['target_frequency'] = results\n        fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n        ax = axes[0]\n        colors = ['purple', 'blue', 'green', 'orange']\n        for i, (freq_name, data) in enumerate(results.items()):\n            rewards = data['rewards']\n            smoothed = pd.Series(rewards).rolling(10).mean()\n            ax.plot(smoothed, label=freq_name, color=colors[i], linewidth=2)\n        ax.set_title('Learning Curves by Target Update Frequency')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Episode Reward (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1]\n        for i, (freq_name, data) in enumerate(results.items()):\n            if data['q_stds']:\n                episodes_for_q = np.arange(0, len(data['q_stds'])) * 10\n                ax.plot(episodes_for_q, data['q_stds'], \n                       label=freq_name, color=colors[i], linewidth=2, marker='o')\n        ax.set_title('Q-Value Stability Over Training')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Q-Value Standard Deviation')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        env.close()\n        return results\n    def demonstrate_replay_buffer_analysis(self):\n        print(\"=\"*70)\n        print(\"Replay Buffer Content Analysis\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        agent = DQNAgent(state_dim=state_dim, action_dim=action_dim, buffer_size=5000)\n        for episode in range(50):\n            agent.train_episode(env, max_steps=500)\n        buffer_size = len(agent.replay_buffer)\n        print(f\"Buffer size after training: {buffer_size}\")\n        if buffer_size > 100:\n            all_states = []\n            all_rewards = []\n            all_actions = []\n            all_dones = []\n            for experience in agent.replay_buffer.buffer:\n                all_states.append(experience.state)\n                all_rewards.append(experience.reward)\n                all_actions.append(experience.action)\n                all_dones.append(experience.done)\n            all_states = np.array(all_states)\n            all_rewards = np.array(all_rewards)\n            all_actions = np.array(all_actions)\n            all_dones = np.array(all_dones)\n            fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n            for i in range(min(4, state_dim)):\n                ax = axes[0, i] if i < 3 else axes[1, 0]\n                ax.hist(all_states[:, i], bins=30, alpha=0.7, edgecolor='black')\n                ax.set_title(f'State Dimension {i} Distribution')\n                ax.set_xlabel(f'State[{i}]')\n                ax.set_ylabel('Frequency')\n                ax.grid(True, alpha=0.3)\n            if len(axes[0]) > 2:\n                ax = axes[0, 2]\n            else:\n                ax = axes[1, 1]\n            ax.hist(all_rewards, bins=np.unique(all_rewards), alpha=0.7, edgecolor='black')\n            ax.set_title('Reward Distribution')\n            ax.set_xlabel('Reward')\n            ax.set_ylabel('Frequency')\n            ax.grid(True, alpha=0.3)\n            ax = axes[1, 1] if len(axes[0]) > 2 else axes[1, 2]\n            action_counts = np.bincount(all_actions)\n            ax.bar(range(len(action_counts)), action_counts, alpha=0.7)\n            ax.set_title('Action Distribution')\n            ax.set_xlabel('Action')\n            ax.set_ylabel('Frequency')\n            ax.set_xticks(range(action_dim))\n            ax.grid(True, alpha=0.3)\n            ax = axes[1, 2]\n            done_ratio = np.mean(all_dones)\n            ax.pie([done_ratio, 1-done_ratio], \n                  labels=[f'Terminal ({done_ratio:.1%})', f'Non-terminal ({1-done_ratio:.1%})'],\n                  autopct='%1.1f%%', startangle=90)\n            ax.set_title('Terminal vs Non-terminal States')\n            plt.tight_layout()\n            plt.show()\n            print(f\"\\nBuffer Statistics:\")\n            print(f\"  Total experiences: {buffer_size}\")\n            print(f\"  Reward range: [{np.min(all_rewards):.2f}, {np.max(all_rewards):.2f}]\")\n            print(f\"  Average reward: {np.mean(all_rewards):.2f}\")\n            print(f\"  Action distribution: {dict(zip(range(action_dim), action_counts))}\")\n            print(f\"  Terminal state ratio: {done_ratio:.1%}\")\n            if state_dim >= 2:\n                print(f\"\\nState Correlation Analysis:\")\n                state_corr = np.corrcoef(all_states.T)\n                plt.figure(figsize=(8, 6))\n                sns.heatmap(state_corr, annot=True, cmap='coolwarm', center=0,\n                           xticklabels=[f'State[{i}]' for i in range(state_dim)],\n                           yticklabels=[f'State[{i}]' for i in range(state_dim)])\n                plt.title('State Dimension Correlations in Replay Buffer')\n                plt.tight_layout()\n                plt.show()\n        env.close()\n        return agent\nanalyzer = ExperienceReplayAnalyzer()\nprint(\"1. Comparing Replay Strategies...\")\nreplay_results = analyzer.compare_replay_strategies()\nprint(\"\\n2. Analyzing Target Network Update Frequency...\")\ntarget_results = analyzer.analyze_target_network_frequency()\nprint(\"\\n3. Analyzing Replay Buffer Content...\")\nbuffer_agent = analyzer.demonstrate_replay_buffer_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf888c25",
   "metadata": {},
   "source": [
    "# Section 4: Double DQN and Overestimation Bias\n",
    "\n",
    "## 4.1 The Overestimation Problem in Q-Learning\n",
    "\n",
    "Standard Q-learning suffers from a systematic overestimation bias due to the max operator in the Bellman equation. This problem is amplified in function approximation settings.\n",
    "\n",
    "### Mathematical Analysis of Overestimation Bias\n",
    "\n",
    "In standard DQN, the target is computed as:\n",
    "```\n",
    "y = r + γ max Q(s', a'; θ⁻)\n",
    "          a'\n",
    "```\n",
    "\n",
    "The issue arises because:\n",
    "1. We use the same network to both **select** the action and **evaluate** it\n",
    "2. Noise in Q-value estimates leads to overestimation when taking the maximum\n",
    "3. This bias propagates through the Bellman updates\n",
    "\n",
    "### Impact on Learning\n",
    "- **Suboptimal Policies**: Overestimated Q-values can lead to poor action selection\n",
    "- **Instability**: Inconsistent value estimates cause training instability  \n",
    "- **Slow Convergence**: Biased estimates slow down learning\n",
    "\n",
    "## 4.2 Double DQN Solution\n",
    "\n",
    "Double DQN addresses this by **decoupling action selection from action evaluation**:\n",
    "\n",
    "### Key Insight\n",
    "Use the main network to select actions, but the target network to evaluate them:\n",
    "\n",
    "```\n",
    "y = r + γ Q(s', argmax Q(s', a'; θ), θ⁻)\n",
    "              a'\n",
    "```\n",
    "\n",
    "### Algorithm Steps\n",
    "1. **Action Selection**: Use main network to find the best action in next state\n",
    "2. **Action Evaluation**: Use target network to evaluate that action\n",
    "3. **Update**: Compute loss and update main network\n",
    "\n",
    "### Benefits\n",
    "- **Reduced Bias**: Eliminates the correlation between selection and evaluation\n",
    "- **Better Stability**: More consistent Q-value estimates\n",
    "- **Improved Performance**: Often leads to better final policies\n",
    "\n",
    "## 4.3 Implementation Details\n",
    "\n",
    "The modification to standard DQN is minimal but effective:\n",
    "- Only changes the target computation\n",
    "- No additional computational overhead\n",
    "- Compatible with other DQN improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef939ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQNAgent(DQNAgent):\n    def __init__(self, state_dim, action_dim, **kwargs):\n        super().__init__(state_dim, action_dim, **kwargs)\n        self.q_value_estimates = {'main': [], 'target': [], 'double': []}\n        self.overestimation_metrics = []\n    def train_step(self):\n        if len(self.replay_buffer) < self.batch_size:\n            return None\n        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            next_actions = self.q_network(next_states).argmax(1)\n            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n            standard_next_q_values = self.target_network(next_states).max(1)[0]\n            standard_targets = rewards + (self.gamma * standard_next_q_values * (~dones))\n            self.track_bias_metrics(current_q_values, target_q_values, standard_targets)\n        loss = F.mse_loss(current_q_values, target_q_values)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        self.training_step += 1\n        if self.training_step % self.target_update_freq == 0:\n            self.update_target_network()\n        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n        self.losses.append(loss.item())\n        self.epsilon_history.append(self.epsilon)\n        with torch.no_grad():\n            avg_q_value = current_q_values.mean().item()\n            self.q_values_history.append(avg_q_value)\n        return loss.item()\n    def track_bias_metrics(self, current_q, double_targets, standard_targets):\n        self.q_value_estimates['main'].append(current_q.mean().item())\n        self.q_value_estimates['double'].append(double_targets.mean().item())\n        overestimation = (standard_targets - double_targets).mean().item()\n        self.overestimation_metrics.append(overestimation)\nclass OverestimationAnalyzer:\n    def compare_dqn_variants(self):\n        print(\"=\"*70)\n        print(\"DQN vs Double DQN Comparison\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        variants = {\n            'Standard DQN': DQNAgent,\n            'Double DQN': DoubleDQNAgent\n        }\n        results = {}\n        num_episodes = 150\n        for variant_name, agent_class in variants.items():\n            print(f\"\\nTraining {variant_name}...\")\n            agent = agent_class(\n                state_dim=state_dim,\n                action_dim=action_dim,\n                lr=1e-3,\n                epsilon_decay=0.995,\n                target_update_freq=100,\n                buffer_size=20000\n            )\n            episode_rewards = []\n            for episode in range(num_episodes):\n                reward, _ = agent.train_episode(env, max_steps=500)\n                episode_rewards.append(reward)\n                if (episode + 1) % 50 == 0:\n                    avg_reward = np.mean(episode_rewards[-25:])\n                    print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n            eval_results = agent.evaluate(env, num_episodes=20)\n            results[variant_name] = {\n                'agent': agent,\n                'rewards': episode_rewards,\n                'eval_performance': eval_results,\n                'final_performance': np.mean(episode_rewards[-20:])\n            }\n        self.visualize_comparison(results)\n        env.close()\n        return results\n    def visualize_comparison(self, results):\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        colors = ['blue', 'red']\n        ax = axes[0, 0]\n        for i, (variant, data) in enumerate(results.items()):\n            rewards = data['rewards']\n            smoothed = pd.Series(rewards).rolling(10).mean()\n            ax.plot(smoothed, label=variant, color=colors[i], linewidth=2)\n            ax.fill_between(range(len(smoothed)), smoothed, alpha=0.3, color=colors[i])\n        ax.set_title('Learning Curves Comparison')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Episode Reward (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 1]\n        variant_names = list(results.keys())\n        final_perfs = [results[v]['final_performance'] for v in variant_names]\n        eval_means = [results[v]['eval_performance']['mean_reward'] for v in variant_names]\n        eval_stds = [results[v]['eval_performance']['std_reward'] for v in variant_names]\n        x = np.arange(len(variant_names))\n        width = 0.35\n        ax.bar(x - width/2, final_perfs, width, label='Training Performance', \n               alpha=0.7, color=colors)\n        ax.bar(x + width/2, eval_means, width, yerr=eval_stds, \n               label='Evaluation Performance', alpha=0.7, color=[c for c in colors])\n        ax.set_title('Performance Comparison')\n        ax.set_ylabel('Average Reward')\n        ax.set_xticks(x)\n        ax.set_xticklabels(variant_names)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 2]\n        for i, (variant, data) in enumerate(results.items()):\n            agent = data['agent']\n            if hasattr(agent, 'q_values_history') and agent.q_values_history:\n                smoothed_q = pd.Series(agent.q_values_history).rolling(50).mean()\n                ax.plot(smoothed_q, label=f'{variant} Q-values', \n                       color=colors[i], linewidth=2)\n        ax.set_title('Q-Value Evolution')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Average Q-Value')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 0]\n        double_dqn_agent = None\n        for variant, data in results.items():\n            if 'Double' in variant:\n                double_dqn_agent = data['agent']\n                break\n        if double_dqn_agent and hasattr(double_dqn_agent, 'overestimation_metrics'):\n            if double_dqn_agent.overestimation_metrics:\n                overest_smooth = pd.Series(double_dqn_agent.overestimation_metrics).rolling(50).mean()\n                ax.plot(overest_smooth, color='purple', linewidth=2)\n                ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n                ax.fill_between(range(len(overest_smooth)), overest_smooth, 0, alpha=0.3, color='purple')\n        ax.set_title('Overestimation Bias Over Training')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Overestimation (Standard - Double)')\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 1]\n        for i, (variant, data) in enumerate(results.items()):\n            agent = data['agent']\n            if hasattr(agent, 'losses') and agent.losses:\n                losses = agent.losses\n                if len(losses) > 50:\n                    smoothed_loss = pd.Series(losses).rolling(50).mean()\n                    ax.plot(smoothed_loss, label=f'{variant} Loss', \n                           color=colors[i], linewidth=2, alpha=0.7)\n        ax.set_title('Training Loss Comparison')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('MSE Loss (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 2]\n        sample_state = [0.1, 0.1, 0.1, 0.1]\n        q_values_comparison = {}\n        for variant, data in results.items():\n            agent = data['agent']\n            q_vals = agent.get_q_values(sample_state)\n            q_values_comparison[variant] = q_vals\n        if q_values_comparison:\n            x = np.arange(len(q_vals))\n            width = 0.35\n            for i, (variant, q_vals) in enumerate(q_values_comparison.items()):\n                ax.bar(x + i * width, q_vals, width, label=variant, \n                      alpha=0.7, color=colors[i])\n            ax.set_title('Q-Values for Sample State')\n            ax.set_xlabel('Actions')\n            ax.set_ylabel('Q-Value')\n            ax.set_xticks(x + width/2)\n            ax.set_xticklabels([f'Action {i}' for i in range(len(q_vals))])\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n    def analyze_overestimation_in_depth(self):\n        print(\"=\"*70)\n        print(\"Deep Analysis of Overestimation Bias\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        agent = DoubleDQNAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            lr=1e-3,\n            epsilon_decay=0.995,\n            buffer_size=10000\n        )\n        print(\"Training Double DQN to analyze overestimation...\")\n        for episode in range(100):\n            agent.train_episode(env, max_steps=500)\n            if (episode + 1) % 25 == 0:\n                print(f\"Episode {episode+1} completed\")\n        if agent.overestimation_metrics and agent.q_value_estimates['main']:\n            plt.figure(figsize=(15, 10))\n            plt.subplot(2, 2, 1)\n            overest_smooth = pd.Series(agent.overestimation_metrics).rolling(20).mean()\n            plt.plot(overest_smooth, color='red', linewidth=2)\n            plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n            plt.fill_between(range(len(overest_smooth)), overest_smooth, 0, \n                           alpha=0.3, color='red')\n            plt.title('Overestimation Bias Evolution')\n            plt.xlabel('Training Step')\n            plt.ylabel('Bias (Standard DQN - Double DQN)')\n            plt.grid(True, alpha=0.3)\n            plt.subplot(2, 2, 2)\n            main_q_smooth = pd.Series(agent.q_value_estimates['main']).rolling(20).mean()\n            double_q_smooth = pd.Series(agent.q_value_estimates['double']).rolling(20).mean()\n            plt.plot(main_q_smooth, label='Current Q-values', color='blue', linewidth=2)\n            plt.plot(double_q_smooth, label='Double DQN Targets', color='green', linewidth=2)\n            plt.title('Q-Value Estimates Comparison')\n            plt.xlabel('Training Step')\n            plt.ylabel('Average Q-Value')\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.subplot(2, 2, 3)\n            plt.hist(agent.overestimation_metrics, bins=30, alpha=0.7, \n                    edgecolor='black', color='orange')\n            plt.axvline(np.mean(agent.overestimation_metrics), color='red', \n                       linestyle='--', linewidth=2, label=f'Mean: {np.mean(agent.overestimation_metrics):.3f}')\n            plt.title('Overestimation Bias Distribution')\n            plt.xlabel('Bias Value')\n            plt.ylabel('Frequency')\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.subplot(2, 2, 4)\n            if len(agent.overestimation_metrics) == len(agent.q_values_history):\n                plt.scatter(agent.q_values_history, agent.overestimation_metrics, \n                          alpha=0.6, color='purple')\n                plt.xlabel('Average Q-Value')\n                plt.ylabel('Overestimation Bias')\n                plt.title('Q-Value vs Overestimation Bias')\n                if len(agent.q_values_history) > 10:\n                    z = np.polyfit(agent.q_values_history, agent.overestimation_metrics, 1)\n                    p = np.poly1d(z)\n                    plt.plot(sorted(agent.q_values_history), \n                            p(sorted(agent.q_values_history)), \n                            \"r--\", alpha=0.8, linewidth=2)\n                plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n            print(f\"\\nOverestimation Bias Statistics:\")\n            print(f\"  Mean bias: {np.mean(agent.overestimation_metrics):.4f}\")\n            print(f\"  Std bias: {np.std(agent.overestimation_metrics):.4f}\")\n            print(f\"  Max bias: {np.max(agent.overestimation_metrics):.4f}\")\n            print(f\"  Min bias: {np.min(agent.overestimation_metrics):.4f}\")\n            if len(agent.overestimation_metrics) == len(agent.q_values_history):\n                correlation = np.corrcoef(agent.q_values_history, agent.overestimation_metrics)[0, 1]\n                print(f\"  Correlation (Q-values vs Bias): {correlation:.4f}\")\n        env.close()\n        return agent\nanalyzer = OverestimationAnalyzer()\nprint(\"1. Comparing Standard DQN vs Double DQN...\")\ncomparison_results = analyzer.compare_dqn_variants()\nprint(\"\\n2. Deep Analysis of Overestimation Bias...\")\nbias_agent = analyzer.analyze_overestimation_in_depth()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b7cf2b",
   "metadata": {},
   "source": [
    "# Section 5: Dueling DQN and Value Decomposition\n",
    "\n",
    "## 5.1 The Motivation Behind Dueling Architecture\n",
    "\n",
    "Standard DQN learns Q-values directly, but these can be decomposed into two meaningful components:\n",
    "\n",
    "### Value Decomposition Theory\n",
    "The Q-function can be decomposed as:\n",
    "```\n",
    "Q(s,a) = V(s) + A(s,a)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **V(s)**: State value function - \"How good is this state?\"\n",
    "- **A(s,a)**: Advantage function - \"How much better is action a compared to average?\"\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "1. **State Value Learning**: Many states have similar values regardless of action\n",
    "2. **Action Ranking**: What matters most is the relative advantage of actions\n",
    "3. **Sample Efficiency**: Decoupling allows better generalization\n",
    "4. **Faster Learning**: State values can be learned from all experiences\n",
    "\n",
    "## 5.2 Dueling Network Architecture\n",
    "\n",
    "### Network Structure\n",
    "```\n",
    "Input State\n",
    "     |\n",
    "Feature Extraction\n",
    "     |\n",
    "   Split into two streams\n",
    "     /              \\\n",
    "Value Stream    Advantage Stream  \n",
    "   V(s)           A(s,a)\n",
    "     \\              /\n",
    "      Combining Module\n",
    "           |\n",
    "        Q(s,a)\n",
    "```\n",
    "\n",
    "### Combining the Streams\n",
    "\n",
    "The naive combination `Q(s,a) = V(s) + A(s,a)` has an identifiability problem. \n",
    "\n",
    "**Solution**: Subtract the mean advantage:\n",
    "```\n",
    "Q(s,a) = V(s) + A(s,a) - (1/|A|) Σ A(s,a')\n",
    "                                    a'\n",
    "```\n",
    "\n",
    "### Alternative Formulation\n",
    "Use max instead of mean for better performance:\n",
    "```\n",
    "Q(s,a) = V(s) + A(s,a) - max A(s,a')\n",
    "                           a'\n",
    "```\n",
    "\n",
    "## 5.3 Benefits of Dueling Architecture\n",
    "\n",
    "1. **Better Value Estimation**: State values learned more efficiently\n",
    "2. **Improved Policy**: Better action selection through advantage learning\n",
    "3. **Robustness**: More stable learning across different environments\n",
    "4. **Generalization**: Better performance on states with similar values\n",
    "\n",
    "Let's implement and analyze the Dueling DQN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccda237b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDQN(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dims=[256, 256], dueling_type='mean'):\n        super(DuelingDQN, self).__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.dueling_type = dueling_type\n        self.feature_layers = nn.Sequential()\n        prev_dim = state_dim\n        for i, hidden_dim in enumerate(hidden_dims[:-1]):\n            self.feature_layers.add_module(f'fc{i}', nn.Linear(prev_dim, hidden_dim))\n            self.feature_layers.add_module(f'relu{i}', nn.ReLU())\n            prev_dim = hidden_dim\n        feature_dim = hidden_dims[-1] if hidden_dims else hidden_dims[0]\n        self.value_stream = nn.Sequential(\n            nn.Linear(prev_dim, feature_dim),\n            nn.ReLU(),\n            nn.Linear(feature_dim, 1)\n        )\n        self.advantage_stream = nn.Sequential(\n            nn.Linear(prev_dim, feature_dim),\n            nn.ReLU(),\n            nn.Linear(feature_dim, action_dim)\n        )\n        self.apply(self._init_weights)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            nn.init.constant_(module.bias, 0)\n    def forward(self, state):\n        features = self.feature_layers(state)\n        value = self.value_stream(features)\n        advantage = self.advantage_stream(features)\n        if self.dueling_type == 'mean':\n            q_values = value + advantage - advantage.mean(dim=1, keepdim=True)\n        elif self.dueling_type == 'max':\n            q_values = value + advantage - advantage.max(dim=1, keepdim=True)[0]\n        else:\n            q_values = value + advantage\n        return q_values, value, advantage\n    def get_action(self, state, epsilon=0.0):\n        if random.random() < epsilon:\n            return random.randint(0, self.action_dim - 1)\n        else:\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n                q_values, _, _ = self.forward(state_tensor)\n                return q_values.argmax().item()\nclass DuelingDQNAgent(DoubleDQNAgent):\n    def __init__(self, state_dim, action_dim, dueling_type='mean', **kwargs):\n        self.dueling_type = dueling_type\n        super().__init__(state_dim, action_dim, **kwargs)\n        self.q_network = DuelingDQN(state_dim, action_dim, dueling_type=dueling_type).to(device)\n        self.target_network = DuelingDQN(state_dim, action_dim, dueling_type=dueling_type).to(device)\n        self.target_network.load_state_dict(self.q_network.state_dict())\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=kwargs.get('lr', 1e-4))\n        self.value_history = []\n        self.advantage_history = []\n    def train_step(self):\n        if len(self.replay_buffer) < self.batch_size:\n            return None\n        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)\n        current_q_values, current_values, current_advantages = self.q_network(states)\n        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            next_q_main, _, _ = self.q_network(next_states)\n            next_actions = next_q_main.argmax(1)\n            next_q_target, _, _ = self.target_network(next_states)\n            next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n        loss = F.mse_loss(current_q_values, target_q_values)\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        self.training_step += 1\n        if self.training_step % self.target_update_freq == 0:\n            self.update_target_network()\n        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n        self.losses.append(loss.item())\n        self.epsilon_history.append(self.epsilon)\n        with torch.no_grad():\n            avg_q_value = current_q_values.mean().item()\n            avg_value = current_values.mean().item()\n            avg_advantage = current_advantages.mean().item()\n            self.q_values_history.append(avg_q_value)\n            self.value_history.append(avg_value)\n            self.advantage_history.append(avg_advantage)\n        return loss.item()\n    def get_value_advantage_decomposition(self, state):\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n            q_values, value, advantage = self.q_network(state_tensor)\n            return {\n                'q_values': q_values.cpu().numpy().flatten(),\n                'value': value.item(),\n                'advantage': advantage.cpu().numpy().flatten()\n            }\nclass DuelingAnalyzer:\n    def compare_dueling_variants(self):\n        print(\"=\"*70)\n        print(\"Dueling DQN Variants Comparison\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        variants = {\n            'Standard DQN': {'agent_class': DoubleDQNAgent, 'dueling_type': None},\n            'Dueling (Mean)': {'agent_class': DuelingDQNAgent, 'dueling_type': 'mean'},\n            'Dueling (Max)': {'agent_class': DuelingDQNAgent, 'dueling_type': 'max'},\n        }\n        results = {}\n        num_episodes = 120\n        for variant_name, config in variants.items():\n            print(f\"\\nTraining {variant_name}...\")\n            agent_kwargs = {\n                'state_dim': state_dim,\n                'action_dim': action_dim,\n                'lr': 1e-3,\n                'epsilon_decay': 0.995,\n                'target_update_freq': 100,\n                'buffer_size': 15000\n            }\n            if config['dueling_type']:\n                agent_kwargs['dueling_type'] = config['dueling_type']\n            agent = config['agent_class'](**agent_kwargs)\n            episode_rewards = []\n            for episode in range(num_episodes):\n                reward, _ = agent.train_episode(env, max_steps=500)\n                episode_rewards.append(reward)\n                if (episode + 1) % 40 == 0:\n                    avg_reward = np.mean(episode_rewards[-20:])\n                    print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n            eval_results = agent.evaluate(env, num_episodes=15)\n            results[variant_name] = {\n                'agent': agent,\n                'rewards': episode_rewards,\n                'eval_performance': eval_results,\n                'final_performance': np.mean(episode_rewards[-15:])\n            }\n        self.visualize_dueling_comparison(results)\n        env.close()\n        return results\n    def visualize_dueling_comparison(self, results):\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        colors = ['blue', 'red', 'green']\n        ax = axes[0, 0]\n        for i, (variant, data) in enumerate(results.items()):\n            rewards = data['rewards']\n            smoothed = pd.Series(rewards).rolling(8).mean()\n            ax.plot(smoothed, label=variant, color=colors[i], linewidth=2)\n            ax.fill_between(range(len(smoothed)), smoothed, alpha=0.3, color=colors[i])\n        ax.set_title('Learning Curves: Dueling Variants')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Episode Reward (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 1]\n        variant_names = list(results.keys())\n        final_perfs = [results[v]['final_performance'] for v in variant_names]\n        eval_means = [results[v]['eval_performance']['mean_reward'] for v in variant_names]\n        eval_stds = [results[v]['eval_performance']['std_reward'] for v in variant_names]\n        x = np.arange(len(variant_names))\n        width = 0.35\n        ax.bar(x - width/2, final_perfs, width, label='Training', \n               alpha=0.7, color=colors)\n        ax.bar(x + width/2, eval_means, width, yerr=eval_stds, \n               label='Evaluation', alpha=0.7, color=['darkblue', 'darkred', 'darkgreen'])\n        ax.set_title('Performance Comparison')\n        ax.set_ylabel('Average Reward')\n        ax.set_xticks(x)\n        ax.set_xticklabels(variant_names, rotation=45)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 2]\n        for i, (variant, data) in enumerate(results.items()):\n            agent = data['agent']\n            if hasattr(agent, 'value_history') and agent.value_history:\n                value_smooth = pd.Series(agent.value_history).rolling(30).mean()\n                ax.plot(value_smooth, label=f'{variant} Values', \n                       color=colors[i], linewidth=2)\n        ax.set_title('State Value Evolution')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Average State Value')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 0]\n        sample_state = [0.1, 0.1, 0.1, 0.1]\n        decompositions = {}\n        for variant, data in results.items():\n            agent = data['agent']\n            if hasattr(agent, 'get_value_advantage_decomposition'):\n                decomp = agent.get_value_advantage_decomposition(sample_state)\n                decompositions[variant] = decomp\n        if decompositions:\n            for i, (variant, decomp) in enumerate(decompositions.items()):\n                q_vals = decomp['q_values']\n                x_pos = np.arange(len(q_vals)) + i * 0.25\n                ax.bar(x_pos, q_vals, 0.25, label=f'{variant} Q-values', \n                      alpha=0.7, color=colors[i])\n            ax.set_title('Q-Values for Sample State')\n            ax.set_xlabel('Actions')\n            ax.set_ylabel('Q-Value')\n            ax.set_xticks(np.arange(len(q_vals)) + 0.25)\n            ax.set_xticklabels([f'A{i}' for i in range(len(q_vals))])\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        ax = axes[1, 1]\n        if decompositions:\n            dueling_variants = {k: v for k, v in decompositions.items() if 'Dueling' in k}\n            if dueling_variants:\n                variant_names_dueling = list(dueling_variants.keys())\n                values = [decomp['value'] for decomp in dueling_variants.values()]\n                max_advantages = [np.max(decomp['advantage']) for decomp in dueling_variants.values()]\n                min_advantages = [np.min(decomp['advantage']) for decomp in dueling_variants.values()]\n                x = np.arange(len(variant_names_dueling))\n                width = 0.25\n                ax.bar(x - width, values, width, label='State Value', alpha=0.7, color='blue')\n                ax.bar(x, max_advantages, width, label='Max Advantage', alpha=0.7, color='green')\n                ax.bar(x + width, min_advantages, width, label='Min Advantage', alpha=0.7, color='red')\n                ax.set_title('Value vs Advantage Decomposition')\n                ax.set_ylabel('Value')\n                ax.set_xticks(x)\n                ax.set_xticklabels([name.replace('Dueling ', '') for name in variant_names_dueling])\n                ax.legend()\n                ax.grid(True, alpha=0.3)\n        ax = axes[1, 2]\n        for i, (variant, data) in enumerate(results.items()):\n            agent = data['agent']\n            if hasattr(agent, 'losses') and len(agent.losses) > 30:\n                losses = agent.losses\n                loss_smooth = pd.Series(losses).rolling(30).mean()\n                ax.plot(loss_smooth, label=f'{variant}', \n                       color=colors[i], linewidth=2, alpha=0.7)\n        ax.set_title('Training Loss Comparison')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('MSE Loss (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n    def analyze_value_advantage_dynamics(self):\n        print(\"=\"*70)\n        print(\"Value-Advantage Dynamics Analysis\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        agent = DuelingDQNAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            dueling_type='mean',\n            lr=1e-3,\n            epsilon_decay=0.995,\n            buffer_size=10000\n        )\n        print(\"Training Dueling DQN for dynamics analysis...\")\n        sample_states = [\n            [0.0, 0.0, 0.0, 0.0],\n            [1.0, 0.0, 0.1, 0.0],\n            [-1.0, 0.0, -0.1, 0.0],\n            [0.0, 1.0, 0.0, 1.0],\n            [0.0, -1.0, 0.0, -1.0],\n        ]\n        decomposition_history = {i: {'values': [], 'advantages': [], 'q_values': []} \n                               for i in range(len(sample_states))}\n        for episode in range(80):\n            agent.train_episode(env, max_steps=500)\n            if episode % 10 == 0:\n                for i, state in enumerate(sample_states):\n                    decomp = agent.get_value_advantage_decomposition(state)\n                    decomposition_history[i]['values'].append(decomp['value'])\n                    decomposition_history[i]['advantages'].append(decomp['advantage'].copy())\n                    decomposition_history[i]['q_values'].append(decomp['q_values'].copy())\n            if (episode + 1) % 20 == 0:\n                print(f\"Episode {episode+1} completed\")\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        ax = axes[0, 0]\n        colors = plt.cm.tab10(np.linspace(0, 1, len(sample_states)))\n        for i, color in enumerate(colors):\n            values = decomposition_history[i]['values']\n            episodes = np.arange(0, len(values)) * 10\n            ax.plot(episodes, values, label=f'State {i}', color=color, linewidth=2)\n        ax.set_title('State Value Evolution')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('State Value V(s)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 1]\n        if decomposition_history[0]['advantages']:\n            advantages_array = np.array(decomposition_history[0]['advantages'])\n            episodes = np.arange(0, len(advantages_array)) * 10\n            for action in range(action_dim):\n                ax.plot(episodes, advantages_array[:, action], \n                       label=f'Action {action}', linewidth=2)\n        ax.set_title('Advantage Evolution (State 0)')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Advantage A(s,a)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 2]\n        if decomposition_history[0]['q_values'] and decomposition_history[0]['values']:\n            q_vals = np.array(decomposition_history[0]['q_values'])\n            values = np.array(decomposition_history[0]['values'])\n            advantages = np.array(decomposition_history[0]['advantages'])\n            reconstructed_q = values[:, None] + advantages - advantages.mean(axis=1, keepdims=True)\n            episodes = np.arange(0, len(q_vals)) * 10\n            ax.plot(episodes, q_vals[:, 0], label='Direct Q(s,a=0)', linewidth=2)\n            ax.plot(episodes, reconstructed_q[:, 0], label='V(s) + A(s,a=0) - mean(A)', \n                   linewidth=2, linestyle='--')\n            ax.set_title('Q-Value Decomposition Consistency')\n            ax.set_xlabel('Episode')\n            ax.set_ylabel('Q-Value')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        ax = axes[1, 0]\n        if all(decomposition_history[i]['values'] for i in range(len(sample_states))):\n            final_values = [decomposition_history[i]['values'][-1] for i in range(len(sample_states))]\n            final_advantage_ranges = [\n                np.max(decomposition_history[i]['advantages'][-1]) - \n                np.min(decomposition_history[i]['advantages'][-1]) \n                for i in range(len(sample_states))\n            ]\n            ax.scatter(final_values, final_advantage_ranges, s=100, alpha=0.7, c=colors)\n            for i, (val, adv_range) in enumerate(zip(final_values, final_advantage_ranges)):\n                ax.annotate(f'State {i}', (val, adv_range), xytext=(5, 5), \n                           textcoords='offset points')\n            ax.set_xlabel('Final State Value')\n            ax.set_ylabel('Advantage Range (Max - Min)')\n            ax.set_title('State Value vs Advantage Spread')\n            ax.grid(True, alpha=0.3)\n        ax = axes[1, 1]\n        if decomposition_history[0]['q_values']:\n            q_vals = np.array(decomposition_history[0]['q_values'])\n            episodes = np.arange(0, len(q_vals)) * 10\n            action_probs = F.softmax(torch.tensor(q_vals), dim=1).numpy()\n            for action in range(action_dim):\n                ax.plot(episodes, action_probs[:, action], \n                       label=f'Action {action} Preference', linewidth=2)\n            ax.set_title('Action Preferences Over Time (State 0)')\n            ax.set_xlabel('Episode')\n            ax.set_ylabel('Action Probability')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        ax = axes[1, 2]\n        if hasattr(agent, 'value_history') and hasattr(agent, 'q_values_history'):\n            episodes_v = range(len(agent.value_history))\n            episodes_q = range(len(agent.q_values_history))\n            if len(episodes_v) > 10 and len(episodes_q) > 10:\n                value_smooth = pd.Series(agent.value_history).rolling(20).mean()\n                q_smooth = pd.Series(agent.q_values_history).rolling(20).mean()\n                ax.plot(episodes_v, value_smooth, label='Average State Values', linewidth=2)\n                ax.plot(episodes_q, q_smooth, label='Average Q-Values', linewidth=2)\n                ax.set_title('Value vs Q-Value Evolution')\n                ax.set_xlabel('Training Step')\n                ax.set_ylabel('Average Value')\n                ax.legend()\n                ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        env.close()\n        return agent, decomposition_history\ndueling_analyzer = DuelingAnalyzer()\nprint(\"1. Comparing Dueling DQN Variants...\")\ndueling_results = dueling_analyzer.compare_dueling_variants()\nprint(\"\\n2. Analyzing Value-Advantage Dynamics...\")\ndynamics_agent, dynamics_history = dueling_analyzer.analyze_value_advantage_dynamics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ae08e",
   "metadata": {},
   "source": [
    "# Section 6: Prioritized Experience Replay\n",
    "\n",
    "## 6.1 Motivation for Prioritized Sampling\n",
    "\n",
    "Standard experience replay samples uniformly from the buffer, but not all experiences are equally valuable for learning:\n",
    "\n",
    "### Key Insights\n",
    "1. **Learning Opportunity**: Experiences with high TD error provide more learning signal\n",
    "2. **Sample Efficiency**: Focus on experiences where we can learn the most\n",
    "3. **Rare Events**: Important but infrequent experiences might be undersampled\n",
    "\n",
    "### Problems with Uniform Sampling\n",
    "- Wastes computation on experiences with low TD error\n",
    "- May miss important experiences that occur rarely\n",
    "- Doesn't focus learning where it's most needed\n",
    "\n",
    "## 6.2 Prioritized Experience Replay (PER) Algorithm\n",
    "\n",
    "### Priority Assignment\n",
    "Assign priority based on TD error magnitude:\n",
    "```\n",
    "p_i = |δ_i| + ε\n",
    "```\n",
    "Where:\n",
    "- `δ_i`: TD error for experience i\n",
    "- `ε`: Small constant to ensure all experiences have non-zero probability\n",
    "\n",
    "### Sampling Probability\n",
    "```\n",
    "P(i) = p_i^α / Σ_k p_k^α\n",
    "```\n",
    "Where `α` controls how much prioritization is applied:\n",
    "- `α = 0`: Uniform sampling (standard replay)\n",
    "- `α = 1`: Full prioritization\n",
    "\n",
    "### Importance Sampling Weights\n",
    "To correct for the bias introduced by prioritized sampling:\n",
    "```\n",
    "w_i = (1/N * 1/P(i))^β\n",
    "```\n",
    "Where:\n",
    "- `N`: Buffer size  \n",
    "- `β`: Importance sampling exponent (annealed from initial value to 1)\n",
    "\n",
    "## 6.3 Implementation Strategies\n",
    "\n",
    "### Efficient Data Structures\n",
    "1. **Sum Tree**: For efficient sampling and updating\n",
    "2. **Min Tree**: For tracking minimum priority\n",
    "3. **Segment Tree**: Alternative efficient implementation\n",
    "\n",
    "### Practical Considerations\n",
    "- **Priority Updates**: Update priorities after each training step\n",
    "- **Stale Priorities**: Handle experiences that haven't been updated recently\n",
    "- **Memory Efficiency**: Balance between accuracy and memory usage\n",
    "\n",
    "Let's implement a complete Prioritized Experience Replay system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf11e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.tree = np.zeros(2 * capacity - 1)\n        self.data = np.zeros(capacity, dtype=object)\n        self.data_pointer = 0\n        self.n_entries = 0\n    def add(self, priority, data):\n        tree_index = self.data_pointer + self.capacity - 1\n        self.data[self.data_pointer] = data\n        self.update(tree_index, priority)\n        self.data_pointer = (self.data_pointer + 1) % self.capacity\n        if self.n_entries < self.capacity:\n            self.n_entries += 1\n    def update(self, tree_index, priority):\n        change = priority - self.tree[tree_index]\n        self.tree[tree_index] = priority\n        while tree_index != 0:\n            tree_index = (tree_index - 1) // 2\n            self.tree[tree_index] += change\n    def get_leaf(self, value):\n        parent_index = 0\n        while True:\n            left_child_index = 2 * parent_index + 1\n            right_child_index = left_child_index + 1\n            if left_child_index >= len(self.tree):\n                leaf_index = parent_index\n                break\n            if value <= self.tree[left_child_index]:\n                parent_index = left_child_index\n            else:\n                value -= self.tree[left_child_index]\n                parent_index = right_child_index\n        data_index = leaf_index - self.capacity + 1\n        return leaf_index, self.tree[leaf_index], self.data[data_index]\n    @property\n    def total_priority(self):\n        return self.tree[0]\n    def __len__(self):\n        return self.n_entries\nclass PrioritizedReplayBuffer:\n    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n        self.capacity = capacity\n        self.alpha = alpha\n        self.beta_start = beta_start\n        self.beta_frames = beta_frames\n        self.frame = 1\n        self.tree = SumTree(capacity)\n        self.epsilon = 1e-6\n        self.max_priority = 1.0\n        self.priority_history = []\n        self.sampling_weights_history = []\n    def beta(self):\n        return min(1.0, self.beta_start + self.frame * (1.0 - self.beta_start) / self.beta_frames)\n    def push(self, state, action, reward, next_state, done):\n        experience = Experience(state, action, reward, next_state, done)\n        priority = self.max_priority\n        self.tree.add(priority, experience)\n    def sample(self, batch_size):\n        batch_indices = []\n        batch_experiences = []\n        priorities = []\n        segment_size = self.tree.total_priority / batch_size\n        for i in range(batch_size):\n            a = segment_size * i\n            b = segment_size * (i + 1)\n            value = random.uniform(a, b)\n            index, priority, experience = self.tree.get_leaf(value)\n            batch_indices.append(index)\n            batch_experiences.append(experience)\n            priorities.append(priority)\n        sampling_probabilities = np.array(priorities) / self.tree.total_priority\n        weights = (len(self.tree) * sampling_probabilities) ** -self.beta()\n        weights = weights / weights.max()\n        states = torch.FloatTensor([e.state for e in batch_experiences]).to(device)\n        actions = torch.LongTensor([e.action for e in batch_experiences]).to(device)\n        rewards = torch.FloatTensor([e.reward for e in batch_experiences]).to(device)\n        next_states = torch.FloatTensor([e.next_state for e in batch_experiences]).to(device)\n        dones = torch.BoolTensor([e.done for e in batch_experiences]).to(device)\n        weights = torch.FloatTensor(weights).to(device)\n        self.priority_history.append(np.mean(priorities))\n        self.sampling_weights_history.append(weights.mean().item())\n        self.frame += 1\n        return states, actions, rewards, next_states, dones, weights, batch_indices\n    def update_priorities(self, indices, priorities):\n        for idx, priority in zip(indices, priorities):\n            priority = abs(priority) + self.epsilon\n            self.max_priority = max(self.max_priority, priority)\n            priority = priority ** self.alpha\n            self.tree.update(idx, priority)\n    def __len__(self):\n        return len(self.tree)\nclass PrioritizedDQNAgent(DuelingDQNAgent):\n    def __init__(self, state_dim, action_dim, alpha=0.6, beta_start=0.4, **kwargs):\n        buffer_size = kwargs.pop('buffer_size', 100000)\n        super().__init__(state_dim, action_dim, **kwargs)\n        self.replay_buffer = PrioritizedReplayBuffer(\n            capacity=buffer_size,\n            alpha=alpha,\n            beta_start=beta_start\n        )\n        self.priority_stats = []\n        self.td_errors_history = []\n    def store_experience(self, state, action, reward, next_state, done):\n        self.replay_buffer.push(state, action, reward, next_state, done)\n    def train_step(self):\n        if len(self.replay_buffer) < self.batch_size:\n            return None\n        states, actions, rewards, next_states, dones, weights, indices = \\\n            self.replay_buffer.sample(self.batch_size)\n        if hasattr(self.q_network, 'forward') and len(self.q_network.forward(states)) == 3:\n            current_q_values, _, _ = self.q_network(states)\n        else:\n            current_q_values = self.q_network(states)\n        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            if hasattr(self.q_network, 'forward') and len(self.q_network.forward(next_states)) == 3:\n                next_q_main, _, _ = self.q_network(next_states)\n                next_q_target, _, _ = self.target_network(next_states)\n            else:\n                next_q_main = self.q_network(next_states)\n                next_q_target = self.target_network(next_states)\n            next_actions = next_q_main.argmax(1)\n            next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n        td_errors = target_q_values - current_q_values\n        loss = (weights * td_errors.pow(2)).mean()\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        priorities = abs(td_errors.detach().cpu().numpy())\n        self.replay_buffer.update_priorities(indices, priorities)\n        self.training_step += 1\n        if self.training_step % self.target_update_freq == 0:\n            self.update_target_network()\n        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n        self.losses.append(loss.item())\n        self.epsilon_history.append(self.epsilon)\n        self.td_errors_history.append(td_errors.abs().mean().item())\n        with torch.no_grad():\n            avg_q_value = current_q_values.mean().item()\n            self.q_values_history.append(avg_q_value)\n            if hasattr(self, 'value_history'):\n                _, values, advantages = self.q_network(states)\n                self.value_history.append(values.mean().item())\n                self.advantage_history.append(advantages.mean().item())\n        return loss.item()\nclass PrioritizedReplayAnalyzer:\n    def compare_replay_methods(self):\n        print(\"=\"*70)\n        print(\"Standard vs Prioritized Experience Replay Comparison\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        methods = {\n            'Standard Replay': DuelingDQNAgent,\n            'Prioritized Replay': PrioritizedDQNAgent,\n        }\n        results = {}\n        num_episodes = 100\n        for method_name, agent_class in methods.items():\n            print(f\"\\nTraining {method_name}...\")\n            agent_kwargs = {\n                'state_dim': state_dim,\n                'action_dim': action_dim,\n                'lr': 1e-3,\n                'epsilon_decay': 0.995,\n                'target_update_freq': 100,\n                'buffer_size': 20000,\n                'dueling_type': 'mean'\n            }\n            if method_name == 'Prioritized Replay':\n                agent_kwargs.update({'alpha': 0.6, 'beta_start': 0.4})\n            agent = agent_class(**agent_kwargs)\n            episode_rewards = []\n            sample_efficiency = []\n            for episode in range(num_episodes):\n                reward, steps = agent.train_episode(env, max_steps=500)\n                episode_rewards.append(reward)\n                sample_efficiency.append(steps)\n                if (episode + 1) % 25 == 0:\n                    avg_reward = np.mean(episode_rewards[-10:])\n                    avg_steps = np.mean(sample_efficiency[-10:])\n                    print(f\"  Episode {episode+1}: Reward = {avg_reward:.1f}, Steps = {avg_steps:.1f}\")\n            eval_results = agent.evaluate(env, num_episodes=20)\n            results[method_name] = {\n                'agent': agent,\n                'rewards': episode_rewards,\n                'steps': sample_efficiency,\n                'eval_performance': eval_results,\n                'final_performance': np.mean(episode_rewards[-15:])\n            }\n        self.visualize_replay_comparison(results)\n        env.close()\n        return results\n    def visualize_replay_comparison(self, results):\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        colors = ['blue', 'red']\n        ax = axes[0, 0]\n        for i, (method, data) in enumerate(results.items()):\n            rewards = data['rewards']\n            smoothed = pd.Series(rewards).rolling(8).mean()\n            ax.plot(smoothed, label=method, color=colors[i], linewidth=2)\n            ax.fill_between(range(len(smoothed)), smoothed, alpha=0.3, color=colors[i])\n        ax.set_title('Learning Curves: Replay Methods')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Episode Reward (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 1]\n        for i, (method, data) in enumerate(results.items()):\n            steps = data['steps']\n            smoothed_steps = pd.Series(steps).rolling(8).mean()\n            ax.plot(smoothed_steps, label=method, color=colors[i], linewidth=2)\n        ax.set_title('Sample Efficiency (Steps per Episode)')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Steps (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 2]\n        method_names = list(results.keys())\n        final_perfs = [results[m]['final_performance'] for m in method_names]\n        eval_means = [results[m]['eval_performance']['mean_reward'] for m in method_names]\n        eval_stds = [results[m]['eval_performance']['std_reward'] for m in method_names]\n        x = np.arange(len(method_names))\n        width = 0.35\n        ax.bar(x - width/2, final_perfs, width, label='Training', alpha=0.7, color=colors)\n        ax.bar(x + width/2, eval_means, width, yerr=eval_stds, \n               label='Evaluation', alpha=0.7, color=['darkblue', 'darkred'])\n        ax.set_title('Performance Comparison')\n        ax.set_ylabel('Average Reward')\n        ax.set_xticks(x)\n        ax.set_xticklabels(method_names, rotation=15)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 0]\n        prioritized_agent = None\n        for method, data in results.items():\n            if 'Prioritized' in method:\n                prioritized_agent = data['agent']\n                break\n        if prioritized_agent and hasattr(prioritized_agent.replay_buffer, 'priority_history'):\n            priorities = prioritized_agent.replay_buffer.priority_history\n            if priorities:\n                ax.plot(priorities, color='purple', linewidth=2)\n                ax.set_title('Priority Evolution')\n                ax.set_xlabel('Training Step')\n                ax.set_ylabel('Average Priority')\n                ax.grid(True, alpha=0.3)\n        ax = axes[1, 1]\n        for i, (method, data) in enumerate(results.items()):\n            agent = data['agent']\n            if hasattr(agent, 'td_errors_history') and agent.td_errors_history:\n                td_errors = agent.td_errors_history\n                smoothed_td = pd.Series(td_errors).rolling(20).mean()\n                ax.plot(smoothed_td, label=method, color=colors[i], linewidth=2)\n        ax.set_title('TD Error Evolution')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Average TD Error')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 2]\n        if prioritized_agent and hasattr(prioritized_agent.replay_buffer, 'sampling_weights_history'):\n            weights = prioritized_agent.replay_buffer.sampling_weights_history\n            if weights:\n                ax.plot(weights, color='orange', linewidth=2)\n                ax.set_title('Importance Sampling Weights')\n                ax.set_xlabel('Training Step')\n                ax.set_ylabel('Average Weight')\n                ax.grid(True, alpha=0.3)\n                beta_values = [prioritized_agent.replay_buffer.beta_start + \n                              i * (1.0 - prioritized_agent.replay_buffer.beta_start) / \n                              prioritized_agent.replay_buffer.beta_frames \n                              for i in range(len(weights))]\n                ax2 = ax.twinx()\n                ax2.plot(beta_values, color='green', linewidth=2, alpha=0.7, linestyle='--')\n                ax2.set_ylabel('Beta (Annealing)', color='green')\n                ax2.tick_params(axis='y', labelcolor='green')\n        plt.tight_layout()\n        plt.show()\n    def analyze_priority_distribution(self):\n        print(\"=\"*70)\n        print(\"Priority Distribution Analysis\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        agent = PrioritizedDQNAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            alpha=0.6,\n            beta_start=0.4,\n            lr=1e-3,\n            buffer_size=5000\n        )\n        print(\"Training agent to analyze priorities...\")\n        for episode in range(40):\n            agent.train_episode(env, max_steps=500)\n            if (episode + 1) % 10 == 0:\n                print(f\"Episode {episode+1} completed\")\n        if len(agent.replay_buffer) > 100:\n            print(\"\\nAnalyzing priority distribution...\")\n            try:\n                states, actions, rewards, next_states, dones, weights, indices = \\\n                    agent.replay_buffer.sample(min(1000, len(agent.replay_buffer)))\n                with torch.no_grad():\n                    if hasattr(agent.q_network, 'forward') and len(agent.q_network.forward(states)) == 3:\n                        current_q_values, _, _ = agent.q_network(states)\n                        next_q_main, _, _ = agent.q_network(next_states)\n                        next_q_target, _, _ = agent.target_network(next_states)\n                    else:\n                        current_q_values = agent.q_network(states)\n                        next_q_main = agent.q_network(next_states)\n                        next_q_target = agent.target_network(next_states)\n                    current_q_selected = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n                    next_actions = next_q_main.argmax(1)\n                    next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n                    target_q_values = rewards + (agent.gamma * next_q_values * (~dones))\n                    td_errors = abs(target_q_values - current_q_selected).cpu().numpy()\n                fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n                ax = axes[0, 0]\n                all_priorities = []\n                for i in range(len(agent.replay_buffer.tree.data)):\n                    if agent.replay_buffer.tree.data[i] is not None:\n                        tree_idx = i + agent.replay_buffer.capacity - 1\n                        priority = agent.replay_buffer.tree.tree[tree_idx]\n                        all_priorities.append(priority)\n                if all_priorities:\n                    ax.hist(all_priorities, bins=30, alpha=0.7, edgecolor='black')\n                    ax.set_title('Priority Distribution in Buffer')\n                    ax.set_xlabel('Priority')\n                    ax.set_ylabel('Frequency')\n                    ax.grid(True, alpha=0.3)\n                ax = axes[0, 1]\n                sampled_priorities = []\n                for idx in indices:\n                    priority = agent.replay_buffer.tree.tree[idx]\n                    sampled_priorities.append(priority)\n                ax.scatter(td_errors, sampled_priorities, alpha=0.6)\n                ax.set_xlabel('TD Error')\n                ax.set_ylabel('Priority')\n                ax.set_title('TD Error vs Priority Correlation')\n                ax.grid(True, alpha=0.3)\n                if len(td_errors) > 10:\n                    correlation = np.corrcoef(td_errors, sampled_priorities)[0, 1]\n                    ax.text(0.05, 0.95, f'Correlation: {correlation:.3f}', \n                           transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n                ax = axes[1, 0]\n                weights_np = weights.cpu().numpy()\n                ax.hist(weights_np, bins=30, alpha=0.7, edgecolor='black', color='orange')\n                ax.set_title('Importance Sampling Weights')\n                ax.set_xlabel('Weight')\n                ax.set_ylabel('Frequency')\n                ax.grid(True, alpha=0.3)\n                ax.text(0.05, 0.95, f'Mean: {weights_np.mean():.3f}\\nStd: {weights_np.std():.3f}', \n                       transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='lightblue'))\n                ax = axes[1, 1]\n                frame_range = np.arange(0, agent.replay_buffer.beta_frames, 1000)\n                beta_schedule = [\n                    min(1.0, agent.replay_buffer.beta_start + \n                        frame * (1.0 - agent.replay_buffer.beta_start) / agent.replay_buffer.beta_frames)\n                    for frame in frame_range\n                ]\n                ax.plot(frame_range, beta_schedule, linewidth=2, color='green')\n                ax.axvline(agent.replay_buffer.frame, color='red', linestyle='--', \n                          label=f'Current Frame: {agent.replay_buffer.frame}')\n                ax.set_title('Beta Annealing Schedule')\n                ax.set_xlabel('Training Frame')\n                ax.set_ylabel('Beta Value')\n                ax.legend()\n                ax.grid(True, alpha=0.3)\n                plt.tight_layout()\n                plt.show()\n                print(f\"\\nPriority Statistics:\")\n                print(f\"  Buffer size: {len(agent.replay_buffer)}\")\n                print(f\"  Priority range: [{min(all_priorities):.4f}, {max(all_priorities):.4f}]\")\n                print(f\"  Average priority: {np.mean(all_priorities):.4f}\")\n                print(f\"  Priority std: {np.std(all_priorities):.4f}\")\n                print(f\"\\nSampling Statistics:\")\n                print(f\"  Weight range: [{weights_np.min():.4f}, {weights_np.max():.4f}]\")\n                print(f\"  Average weight: {weights_np.mean():.4f}\")\n                print(f\"  Current beta: {agent.replay_buffer.beta():.4f}\")\n                print(f\"  Current frame: {agent.replay_buffer.frame}\")\n            except Exception as e:\n                print(f\"Error in priority analysis: {e}\")\n        env.close()\n        return agent\nper_analyzer = PrioritizedReplayAnalyzer()\nprint(\"1. Comparing Standard vs Prioritized Replay...\")\nper_comparison = per_analyzer.compare_replay_methods()\nprint(\"\\n2. Analyzing Priority Distribution...\")\nper_analysis_agent = per_analyzer.analyze_priority_distribution()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7dbf6f",
   "metadata": {},
   "source": [
    "# Section 7: Rainbow DQN - Combining All Improvements\n",
    "\n",
    "## Comprehensive Deep Q-Network\n",
    "\n",
    "Rainbow DQN represents the state-of-the-art combination of multiple DQN improvements:\n",
    "\n",
    "1. **Double DQN**: Reduces overestimation bias\n",
    "2. **Dueling DQN**: Separates value and advantage learning\n",
    "3. **Prioritized Experience Replay**: Improves sample efficiency\n",
    "4. **Multi-step Learning**: Reduces bias in temporal difference learning\n",
    "5. **Distributional RL**: Models the full return distribution\n",
    "6. **Noisy Networks**: Exploration through parametric noise\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Multi-step Learning\n",
    "Instead of 1-step TD targets, we use n-step returns:\n",
    "$$R_t^{(n)} = \\sum_{k=0}^{n-1} \\gamma^k r_{t+k} + \\gamma^n \\max_{a'} Q(s_{t+n}, a')$$\n",
    "\n",
    "### Distributional Learning\n",
    "Model the return distribution instead of expected return:\n",
    "$$Z(s,a) = \\mathbb{E}[R|s,a]$$\n",
    "\n",
    "Where $Z(s,a)$ represents the distribution of returns starting from state $s$ and action $a$.\n",
    "\n",
    "### Noisy Networks\n",
    "Replace standard linear layers with noisy versions:\n",
    "$$y = (W + \\sigma_W \\odot \\epsilon_W) x + (b + \\sigma_b \\odot \\epsilon_b)$$\n",
    "\n",
    "Where $\\epsilon$ represents noise samples and $\\sigma$ are learnable noise parameters.\n",
    "\n",
    "## Implementation Strategy\n",
    "\n",
    "We'll implement a simplified Rainbow DQN focusing on the core improvements that provide the most benefit while maintaining clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8afbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n    def __init__(self, in_features, out_features, std_init=0.5):\n        super().__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.std_init = std_init\n        self.weight_mu = nn.Parameter(torch.zeros(out_features, in_features))\n        self.weight_sigma = nn.Parameter(torch.zeros(out_features, in_features))\n        self.bias_mu = nn.Parameter(torch.zeros(out_features))\n        self.bias_sigma = nn.Parameter(torch.zeros(out_features))\n        self.register_buffer('weight_epsilon', torch.zeros(out_features, in_features))\n        self.register_buffer('bias_epsilon', torch.zeros(out_features))\n        self.reset_parameters()\n        self.reset_noise()\n    def reset_parameters(self):\n        mu_range = 1 / math.sqrt(self.in_features)\n        self.weight_mu.data.uniform_(-mu_range, mu_range)\n        self.weight_sigma.data.fill_(self.std_init / math.sqrt(self.in_features))\n        self.bias_mu.data.uniform_(-mu_range, mu_range)\n        self.bias_sigma.data.fill_(self.std_init / math.sqrt(self.out_features))\n    def forward(self, x):\n        if self.training:\n            weight = self.weight_mu + self.weight_sigma * self.weight_epsilon\n            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n        else:\n            weight = self.weight_mu\n            bias = self.bias_mu\n        return F.linear(x, weight, bias)\n    def reset_noise(self):\n        epsilon_i = self._scale_noise(self.in_features)\n        epsilon_j = self._scale_noise(self.out_features)\n        self.weight_epsilon.copy_(epsilon_j.ger(epsilon_i))\n        self.bias_epsilon.copy_(epsilon_j)\n    def _scale_noise(self, size):\n        x = torch.randn(size, device=self.weight_mu.device)\n        return x.sign().mul(x.abs().sqrt())\nclass RainbowNetwork(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_dim=128, n_step=3, use_noisy=True):\n        super().__init__()\n        self.input_dim = input_dim\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.n_step = n_step\n        self.use_noisy = use_noisy\n        self.feature_layer = nn.Sequential(\n            nn.Linear(input_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        if use_noisy:\n            self.value_stream = nn.Sequential(\n                NoisyLinear(hidden_dim, hidden_dim // 2),\n                nn.ReLU(),\n                NoisyLinear(hidden_dim // 2, 1)\n            )\n            self.advantage_stream = nn.Sequential(\n                NoisyLinear(hidden_dim, hidden_dim // 2),\n                nn.ReLU(),\n                NoisyLinear(hidden_dim // 2, output_dim)\n            )\n        else:\n            self.value_stream = nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim // 2),\n                nn.ReLU(),\n                nn.Linear(hidden_dim // 2, 1)\n            )\n            self.advantage_stream = nn.Sequential(\n                nn.Linear(hidden_dim, hidden_dim // 2),\n                nn.ReLU(),\n                nn.Linear(hidden_dim // 2, output_dim)\n            )\n        self.reset_noise()\n    def forward(self, x):\n        features = self.feature_layer(x)\n        values = self.value_stream(features)\n        advantages = self.advantage_stream(features)\n        q_values = values + (advantages - advantages.mean(dim=1, keepdim=True))\n        return q_values, values, advantages\n    def reset_noise(self):\n        if self.use_noisy:\n            for module in self.modules():\n                if isinstance(module, NoisyLinear):\n                    module.reset_noise()\nclass MultiStepBuffer:\n    def __init__(self, n_step, gamma):\n        self.n_step = n_step\n        self.gamma = gamma\n        self.buffer = []\n    def append(self, experience):\n        self.buffer.append(experience)\n        if len(self.buffer) < self.n_step:\n            return None\n        n_step_reward = 0\n        for i, exp in enumerate(self.buffer):\n            n_step_reward += (self.gamma ** i) * exp.reward\n            if exp.done:\n                break\n        first_exp = self.buffer[0]\n        last_exp = self.buffer[-1]\n        n_step_exp = Experience(\n            state=first_exp.state,\n            action=first_exp.action,\n            reward=n_step_reward,\n            next_state=last_exp.next_state,\n            done=last_exp.done\n        )\n        self.buffer.pop(0)\n        return n_step_exp\n    def clear(self):\n        self.buffer.clear()\nclass RainbowDQNAgent:\n    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99, \n                 buffer_size=100000, batch_size=64, target_update_freq=500,\n                 n_step=3, alpha=0.6, beta_start=0.4, use_noisy=True,\n                 device=None):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.lr = lr\n        self.gamma = gamma\n        self.batch_size = batch_size\n        self.target_update_freq = target_update_freq\n        self.n_step = n_step\n        self.use_noisy = use_noisy\n        self.device = device or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.q_network = RainbowNetwork(state_dim, action_dim, use_noisy=use_noisy).to(self.device)\n        self.target_network = RainbowNetwork(state_dim, action_dim, use_noisy=use_noisy).to(self.device)\n        self.update_target_network()\n        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n        self.replay_buffer = PrioritizedReplayBuffer(\n            capacity=buffer_size,\n            alpha=alpha,\n            beta_start=beta_start\n        )\n        self.multi_step_buffer = MultiStepBuffer(n_step, gamma)\n        self.epsilon = 1.0 if not use_noisy else 0.0\n        self.epsilon_decay = 0.995\n        self.epsilon_end = 0.01\n        self.training_step = 0\n        self.losses = []\n        self.q_values_history = []\n        self.value_history = []\n        self.advantage_history = []\n        self.epsilon_history = []\n        self.td_errors_history = []\n    def select_action(self, state):\n        if not isinstance(state, torch.Tensor):\n            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n        if self.use_noisy:\n            self.q_network.reset_noise()\n            with torch.no_grad():\n                q_values, _, _ = self.q_network(state)\n                action = q_values.argmax().item()\n        else:\n            if random.random() < self.epsilon:\n                action = random.randrange(self.action_dim)\n            else:\n                with torch.no_grad():\n                    q_values, _, _ = self.q_network(state)\n                    action = q_values.argmax().item()\n        return action\n    def store_experience(self, state, action, reward, next_state, done):\n        experience = Experience(state, action, reward, next_state, done)\n        n_step_exp = self.multi_step_buffer.append(experience)\n        if n_step_exp is not None:\n            self.replay_buffer.push(\n                n_step_exp.state, n_step_exp.action, n_step_exp.reward,\n                n_step_exp.next_state, n_step_exp.done\n            )\n        if done:\n            self.multi_step_buffer.clear()\n    def train_step(self):\n        if len(self.replay_buffer) < self.batch_size:\n            return None\n        states, actions, rewards, next_states, dones, weights, indices = \\\n            self.replay_buffer.sample(self.batch_size)\n        if self.use_noisy:\n            self.q_network.reset_noise()\n            self.target_network.reset_noise()\n        current_q_values, current_values, current_advantages = self.q_network(states)\n        current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n        with torch.no_grad():\n            next_q_main, _, _ = self.q_network(next_states)\n            next_q_target, _, _ = self.target_network(next_states)\n            next_actions = next_q_main.argmax(1)\n            next_q_values = next_q_target.gather(1, next_actions.unsqueeze(1)).squeeze(1)\n            target_q_values = rewards + ((self.gamma ** self.n_step) * next_q_values * (~dones))\n        td_errors = target_q_values - current_q_values\n        loss = (weights * td_errors.pow(2)).mean()\n        self.optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        priorities = abs(td_errors.detach().cpu().numpy())\n        self.replay_buffer.update_priorities(indices, priorities)\n        self.training_step += 1\n        if self.training_step % self.target_update_freq == 0:\n            self.update_target_network()\n        if not self.use_noisy:\n            self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n        self.losses.append(loss.item())\n        self.epsilon_history.append(self.epsilon)\n        self.td_errors_history.append(td_errors.abs().mean().item())\n        with torch.no_grad():\n            self.q_values_history.append(current_q_values.mean().item())\n            self.value_history.append(current_values.mean().item())\n            self.advantage_history.append(current_advantages.mean().item())\n        return loss.item()\n    def update_target_network(self):\n        self.target_network.load_state_dict(self.q_network.state_dict())\n    def train_episode(self, env, max_steps=1000):\n        state, _ = env.reset()\n        total_reward = 0\n        steps = 0\n        for step in range(max_steps):\n            action = self.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            self.store_experience(state, action, reward, next_state, done)\n            loss = self.train_step()\n            total_reward += reward\n            steps += 1\n            if done:\n                break\n            state = next_state\n        return total_reward, steps\n    def evaluate(self, env, num_episodes=10):\n        rewards = []\n        old_training = self.q_network.training\n        self.q_network.eval()\n        for _ in range(num_episodes):\n            state, _ = env.reset()\n            total_reward = 0\n            for _ in range(1000):\n                action = self.select_action(state)\n                next_state, reward, terminated, truncated, _ = env.step(action)\n                done = terminated or truncated\n                total_reward += reward\n                if done:\n                    break\n                state = next_state\n            rewards.append(total_reward)\n        self.q_network.train(old_training)\n        return {\n            'mean_reward': np.mean(rewards),\n            'std_reward': np.std(rewards),\n            'min_reward': np.min(rewards),\n            'max_reward': np.max(rewards),\n            'rewards': rewards\n        }\nclass RainbowAnalyzer:\n    def compare_all_methods(self):\n        print(\"=\"*70)\n        print(\"Comprehensive DQN Methods Comparison\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        methods = {\n            'Basic DQN': DQNAgent,\n            'Double DQN': DoubleDQNAgent,\n            'Dueling DQN': DuelingDQNAgent,\n            'Prioritized DQN': PrioritizedDQNAgent,\n            'Rainbow DQN': RainbowDQNAgent,\n        }\n        results = {}\n        num_episodes = 80\n        for method_name, agent_class in methods.items():\n            print(f\"\\nTraining {method_name}...\")\n            base_kwargs = {\n                'state_dim': state_dim,\n                'action_dim': action_dim,\n                'lr': 1e-3,\n                'target_update_freq': 100,\n                'buffer_size': 15000,\n                'batch_size': 64\n            }\n            if method_name in ['Dueling DQN', 'Prioritized DQN']:\n                base_kwargs['dueling_type'] = 'mean'\n                base_kwargs['epsilon_decay'] = 0.995\n            if method_name == 'Prioritized DQN':\n                base_kwargs.update({'alpha': 0.6, 'beta_start': 0.4})\n            if method_name == 'Rainbow DQN':\n                base_kwargs.update({\n                    'n_step': 3,\n                    'alpha': 0.6,\n                    'beta_start': 0.4,\n                    'use_noisy': True\n                })\n            agent = agent_class(**base_kwargs)\n            episode_rewards = []\n            sample_efficiency = []\n            for episode in range(num_episodes):\n                reward, steps = agent.train_episode(env, max_steps=500)\n                episode_rewards.append(reward)\n                sample_efficiency.append(steps)\n                if (episode + 1) % 20 == 0:\n                    avg_reward = np.mean(episode_rewards[-10:])\n                    print(f\"  Episode {episode+1}: Reward = {avg_reward:.1f}\")\n            eval_results = agent.evaluate(env, num_episodes=15)\n            results[method_name] = {\n                'agent': agent,\n                'rewards': episode_rewards,\n                'steps': sample_efficiency,\n                'eval_performance': eval_results,\n                'final_performance': np.mean(episode_rewards[-10:])\n            }\n        self.visualize_comprehensive_comparison(results)\n        env.close()\n        return results\n    def visualize_comprehensive_comparison(self, results):\n        fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n        colors = ['blue', 'red', 'green', 'orange', 'purple']\n        ax = axes[0, 0]\n        for i, (method, data) in enumerate(results.items()):\n            rewards = data['rewards']\n            smoothed = pd.Series(rewards).rolling(5).mean()\n            ax.plot(smoothed, label=method, color=colors[i], linewidth=2)\n        ax.set_title('Learning Curves Comparison')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Episode Reward (Smoothed)')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 1]\n        for i, (method, data) in enumerate(results.items()):\n            steps = data['steps']\n            smoothed_steps = pd.Series(steps).rolling(5).mean()\n            ax.plot(smoothed_steps, label=method, color=colors[i], linewidth=2)\n        ax.set_title('Sample Efficiency')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Steps per Episode')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[0, 2]\n        method_names = list(results.keys())\n        final_perfs = [results[m]['final_performance'] for m in method_names]\n        eval_means = [results[m]['eval_performance']['mean_reward'] for m in method_names]\n        eval_stds = [results[m]['eval_performance']['std_reward'] for m in method_names]\n        x = np.arange(len(method_names))\n        width = 0.35\n        bars1 = ax.bar(x - width/2, final_perfs, width, label='Training', alpha=0.7, color=colors)\n        bars2 = ax.bar(x + width/2, eval_means, width, yerr=eval_stds, \n                      label='Evaluation', alpha=0.7, color=[c + '80' for c in ['darkblue', 'darkred', 'darkgreen', 'darkorange', 'indigo']])\n        ax.set_title('Performance Comparison')\n        ax.set_ylabel('Average Reward')\n        ax.set_xticks(x)\n        ax.set_xticklabels(method_names, rotation=15)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 0]\n        for i, (method, data) in enumerate(results.items()):\n            agent = data['agent']\n            if hasattr(agent, 'losses') and agent.losses:\n                losses = agent.losses\n                if len(losses) > 10:\n                    smoothed_losses = pd.Series(losses).rolling(20).mean()\n                    ax.plot(smoothed_losses, label=method, color=colors[i], linewidth=2)\n        ax.set_title('Training Loss Evolution')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Loss')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax.set_yscale('log')\n        ax = axes[1, 1]\n        for i, (method, data) in enumerate(results.items()):\n            agent = data['agent']\n            if hasattr(agent, 'q_values_history') and agent.q_values_history:\n                q_values = agent.q_values_history\n                if len(q_values) > 10:\n                    smoothed_q = pd.Series(q_values).rolling(20).mean()\n                    ax.plot(smoothed_q, label=method, color=colors[i], linewidth=2)\n        ax.set_title('Q-Values Evolution')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Average Q-Value')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 2]\n        for i, (method, data) in enumerate(results.items()):\n            agent = data['agent']\n            if hasattr(agent, 'epsilon_history') and agent.epsilon_history:\n                epsilons = agent.epsilon_history\n                if len(epsilons) > 10:\n                    ax.plot(epsilons, label=method, color=colors[i], linewidth=2)\n        ax.set_title('Exploration Schedule (Epsilon)')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Epsilon')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[2, 0]\n        for i, (method, data) in enumerate(results.items()):\n            agent = data['agent']\n            if hasattr(agent, 'td_errors_history') and agent.td_errors_history:\n                td_errors = agent.td_errors_history\n                if len(td_errors) > 10:\n                    smoothed_td = pd.Series(td_errors).rolling(20).mean()\n                    ax.plot(smoothed_td, label=method, color=colors[i], linewidth=2)\n        ax.set_title('TD Error Evolution')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Average TD Error')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[2, 1]\n        dueling_methods = []\n        for method, data in results.items():\n            agent = data['agent']\n            if hasattr(agent, 'value_history') and agent.value_history:\n                dueling_methods.append((method, agent))\n        if dueling_methods:\n            for i, (method, agent) in enumerate(dueling_methods):\n                values = agent.value_history\n                if len(values) > 10:\n                    smoothed_values = pd.Series(values).rolling(20).mean()\n                    ax.plot(smoothed_values, label=f'{method} Values', color=colors[i], linewidth=2)\n                if hasattr(agent, 'advantage_history') and agent.advantage_history:\n                    advantages = agent.advantage_history\n                    if len(advantages) > 10:\n                        smoothed_adv = pd.Series(advantages).rolling(20).mean()\n                        ax.plot(smoothed_adv, label=f'{method} Advantages', \n                               color=colors[i], linewidth=2, linestyle='--')\n        ax.set_title('Value/Advantage Decomposition')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Value')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[2, 2]\n        ax.axis('tight')\n        ax.axis('off')\n        table_data = []\n        headers = ['Method', 'Final Reward', 'Eval Mean', 'Eval Std', 'Sample Efficiency']\n        for method, data in results.items():\n            final_perf = data['final_performance']\n            eval_mean = data['eval_performance']['mean_reward']\n            eval_std = data['eval_performance']['std_reward']\n            avg_steps = np.mean(data['steps'][-10:])\n            table_data.append([\n                method,\n                f'{final_perf:.1f}',\n                f'{eval_mean:.1f}',\n                f'{eval_std:.1f}',\n                f'{avg_steps:.1f}'\n            ])\n        table = ax.table(cellText=table_data, colLabels=headers, \n                        cellLoc='center', loc='center')\n        table.auto_set_font_size(False)\n        table.set_fontsize(9)\n        table.scale(1.2, 1.5)\n        for i in range(len(headers)):\n            table[(0, i)].set_facecolor('#40466e')\n            table[(0, i)].set_text_props(weight='bold', color='white')\n        best_eval = max([results[m]['eval_performance']['mean_reward'] for m in results.keys()])\n        for i, (method, data) in enumerate(results.items()):\n            eval_perf = data['eval_performance']['mean_reward']\n            if eval_perf == best_eval:\n                for j in range(len(headers)):\n                    table[(i+1, j)].set_facecolor('#90EE90')\n        ax.set_title('Performance Summary')\n        plt.tight_layout()\n        plt.show()\n        print(\"\\n\" + \"=\"*70)\n        print(\"PERFORMANCE SUMMARY\")\n        print(\"=\"*70)\n        best_method = max(results.keys(), \n                         key=lambda k: results[k]['eval_performance']['mean_reward'])\n        best_score = results[best_method]['eval_performance']['mean_reward']\n        print(f\"🏆 Best Method: {best_method}\")\n        print(f\"🏆 Best Score: {best_score:.1f} ± {results[best_method]['eval_performance']['std_reward']:.1f}\")\n        print(f\"\\nMethod Rankings (by evaluation performance):\")\n        sorted_methods = sorted(results.items(), \n                              key=lambda x: x[1]['eval_performance']['mean_reward'], \n                              reverse=True)\n        for i, (method, data) in enumerate(sorted_methods):\n            eval_perf = data['eval_performance']['mean_reward']\n            eval_std = data['eval_performance']['std_reward']\n            improvement = ((eval_perf / sorted_methods[-1][1]['eval_performance']['mean_reward'] - 1) * 100)\n            print(f\"  {i+1}. {method}: {eval_perf:.1f} ± {eval_std:.1f} (+{improvement:.1f}% vs baseline)\")\nrainbow_analyzer = RainbowAnalyzer()\nprint(\"Running Comprehensive DQN Methods Comparison...\")\nprint(\"This will compare Basic DQN, Double DQN, Dueling DQN, Prioritized DQN, and Rainbow DQN\")\nprint(\"Training each method for comparison...\")\ncomprehensive_results = rainbow_analyzer.compare_all_methods()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a548869",
   "metadata": {},
   "source": [
    "# Section 8: Performance Analysis and Ablation Studies\n",
    "\n",
    "## Understanding Component Contributions\n",
    "\n",
    "This section focuses on understanding which components of advanced DQN methods contribute most to performance improvements. We'll conduct systematic ablation studies to isolate the impact of individual components.\n",
    "\n",
    "## Key Analysis Areas\n",
    "\n",
    "1. **Individual Component Impact**: How much does each improvement contribute?\n",
    "2. **Component Interactions**: Do improvements combine synergistically?\n",
    "3. **Environment Sensitivity**: Which improvements work best in different environments?\n",
    "4. **Computational Overhead**: What's the cost-benefit trade-off?\n",
    "5. **Hyperparameter Sensitivity**: How robust are the improvements?\n",
    "\n",
    "## Theoretical Analysis\n",
    "\n",
    "### Sample Complexity\n",
    "Different improvements affect sample complexity in different ways:\n",
    "- **Experience Replay**: Reduces correlation, improves sample efficiency\n",
    "- **Double DQN**: Reduces bias, may require more samples initially but converges better\n",
    "- **Dueling**: Better value estimation, especially beneficial with many actions\n",
    "- **Prioritized Replay**: Focuses on important transitions, improves sample efficiency\n",
    "- **Multi-step**: Propagates rewards faster, reduces variance\n",
    "\n",
    "### Convergence Properties\n",
    "Each component affects convergence:\n",
    "- Target networks provide stability\n",
    "- Double DQN reduces overestimation bias\n",
    "- Prioritized replay can introduce bias but improves practical performance\n",
    "- Noisy networks provide consistent exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd217b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AblationAnalyzer:\n    def __init__(self):\n        self.results_cache = {}\n    def rainbow_ablation_study(self):\n        print(\"=\"*70)\n        print(\"Rainbow DQN Ablation Study\")\n        print(\"=\"*70)\n        print(\"Testing individual component contributions...\")\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        configurations = {\n            'Baseline (Basic DQN)': {\n                'class': DQNAgent,\n                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3}\n            },\n            '+ Double': {\n                'class': DoubleDQNAgent,\n                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3}\n            },\n            '+ Double + Dueling': {\n                'class': DuelingDQNAgent,\n                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3, 'dueling_type': 'mean'}\n            },\n            '+ Double + Dueling + PER': {\n                'class': PrioritizedDQNAgent,\n                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3, \n                          'dueling_type': 'mean', 'alpha': 0.6, 'beta_start': 0.4}\n            },\n            'Rainbow (All Components)': {\n                'class': RainbowDQNAgent,\n                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3,\n                          'n_step': 3, 'alpha': 0.6, 'beta_start': 0.4, 'use_noisy': True}\n            }\n        }\n        component_tests = {\n            'Only Double': {\n                'class': DoubleDQNAgent,\n                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3}\n            },\n            'Only Dueling': {\n                'class': DuelingDQNAgent,\n                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3, \n                          'dueling_type': 'mean'}\n            },\n            'Only PER': {\n                'class': PrioritizedDQNAgent,\n                'params': {'state_dim': state_dim, 'action_dim': action_dim, 'lr': 1e-3,\n                          'alpha': 0.6, 'beta_start': 0.4}\n            }\n        }\n        results = self._run_ablation_experiments(env, configurations, episodes=60)\n        individual_results = self._run_ablation_experiments(env, component_tests, episodes=60)\n        all_results = {**results, **individual_results}\n        self._analyze_component_contributions(all_results)\n        env.close()\n        return all_results\n    def _run_ablation_experiments(self, env, configurations, episodes=50):\n        results = {}\n        for config_name, config in configurations.items():\n            print(f\"\\nTesting: {config_name}\")\n            agent = config['class'](**config['params'])\n            episode_rewards = []\n            training_times = []\n            start_time = time.time()\n            for episode in range(episodes):\n                episode_start = time.time()\n                reward, steps = agent.train_episode(env, max_steps=500)\n                episode_time = time.time() - episode_start\n                episode_rewards.append(reward)\n                training_times.append(episode_time)\n                if (episode + 1) % 15 == 0:\n                    avg_reward = np.mean(episode_rewards[-5:])\n                    print(f\"  Episode {episode+1}: Reward = {avg_reward:.1f}\")\n            total_time = time.time() - start_time\n            eval_results = agent.evaluate(env, num_episodes=10)\n            results[config_name] = {\n                'agent': agent,\n                'rewards': episode_rewards,\n                'training_times': training_times,\n                'total_time': total_time,\n                'eval_performance': eval_results,\n                'final_performance': np.mean(episode_rewards[-10:]),\n                'convergence_episode': self._find_convergence_episode(episode_rewards),\n                'sample_efficiency': np.mean(episode_rewards[-5:]) / episodes\n            }\n        return results\n    def _find_convergence_episode(self, rewards, threshold=0.95, window=10):\n        if len(rewards) < window:\n            return len(rewards)\n        final_performance = np.mean(rewards[-window:])\n        target = threshold * final_performance\n        for i in range(window, len(rewards)):\n            if np.mean(rewards[i-window:i]) >= target:\n                return i\n        return len(rewards)\n    def _analyze_component_contributions(self, results):\n        print(\"\\n\" + \"=\"*70)\n        print(\"COMPONENT CONTRIBUTION ANALYSIS\")\n        print(\"=\"*70)\n        baseline_performance = results['Baseline (Basic DQN)']['eval_performance']['mean_reward']\n        improvements = {}\n        for name, data in results.items():\n            if name != 'Baseline (Basic DQN)':\n                current_perf = data['eval_performance']['mean_reward']\n                improvement = ((current_perf / baseline_performance - 1) * 100)\n                improvements[name] = {\n                    'absolute': current_perf,\n                    'improvement': improvement,\n                    'convergence': data['convergence_episode'],\n                    'efficiency': data['sample_efficiency']\n                }\n        print(f\"Baseline Performance: {baseline_performance:.1f}\")\n        print(f\"\\nComponent Contributions:\")\n        for name, stats in improvements.items():\n            print(f\"\\n{name}:\")\n            print(f\"  Performance: {stats['absolute']:.1f} (+{stats['improvement']:.1f}%)\")\n            print(f\"  Convergence: Episode {stats['convergence']}\")\n            print(f\"  Sample Efficiency: {stats['efficiency']:.3f}\")\n        self._visualize_ablation_results(results, improvements)\n    def _visualize_ablation_results(self, results, improvements):\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        ax = axes[0, 0]\n        cumulative_configs = [\n            'Baseline (Basic DQN)',\n            '+ Double',\n            '+ Double + Dueling', \n            '+ Double + Dueling + PER',\n            'Rainbow (All Components)'\n        ]\n        performances = [results[config]['eval_performance']['mean_reward'] \n                       for config in cumulative_configs if config in results]\n        config_names = [config for config in cumulative_configs if config in results]\n        bars = ax.bar(range(len(performances)), performances, \n                     color=['red', 'orange', 'yellow', 'lightgreen', 'green'][:len(performances)])\n        ax.set_title('Cumulative Component Addition')\n        ax.set_xlabel('Configuration')\n        ax.set_ylabel('Evaluation Performance')\n        ax.set_xticks(range(len(config_names)))\n        ax.set_xticklabels(config_names, rotation=45, ha='right')\n        ax.grid(True, alpha=0.3)\n        for i, (bar, perf) in enumerate(zip(bars, performances)):\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                   f'{perf:.1f}', ha='center', va='bottom')\n        ax = axes[0, 1]\n        individual_components = ['Only Double', 'Only Dueling', 'Only PER']\n        individual_perfs = []\n        baseline_perf = results['Baseline (Basic DQN)']['eval_performance']['mean_reward']\n        for comp in individual_components:\n            if comp in results:\n                individual_perfs.append(results[comp]['eval_performance']['mean_reward'])\n        if individual_perfs:\n            improvements_pct = [(perf - baseline_perf) / baseline_perf * 100 \n                               for perf in individual_perfs]\n            bars = ax.bar(range(len(improvements_pct)), improvements_pct, \n                         color=['blue', 'purple', 'orange'][:len(improvements_pct)])\n            ax.set_title('Individual Component Impact')\n            ax.set_xlabel('Component')\n            ax.set_ylabel('Performance Improvement (%)')\n            ax.set_xticks(range(len(individual_components)))\n            ax.set_xticklabels([comp.replace('Only ', '') for comp in individual_components])\n            ax.grid(True, alpha=0.3)\n            for bar, imp in zip(bars, improvements_pct):\n                ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                       f'{imp:.1f}%', ha='center', va='bottom')\n        ax = axes[0, 2]\n        colors = ['red', 'orange', 'yellow', 'lightgreen', 'green', 'blue', 'purple', 'brown']\n        for i, (name, data) in enumerate(results.items()):\n            rewards = data['rewards']\n            smoothed = pd.Series(rewards).rolling(5).mean()\n            ax.plot(smoothed, label=name, color=colors[i % len(colors)], linewidth=2)\n        ax.set_title('Learning Curves Comparison')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Episode Reward (Smoothed)')\n        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 0]\n        convergence_episodes = []\n        config_names_conv = []\n        for name, data in results.items():\n            convergence_episodes.append(data['convergence_episode'])\n            config_names_conv.append(name.replace(' (All Components)', '').replace('+ Double + Dueling + PER', 'Full'))\n        bars = ax.bar(range(len(convergence_episodes)), convergence_episodes, \n                     color=colors[:len(convergence_episodes)])\n        ax.set_title('Convergence Speed')\n        ax.set_xlabel('Configuration')\n        ax.set_ylabel('Episodes to Convergence')\n        ax.set_xticks(range(len(config_names_conv)))\n        ax.set_xticklabels(config_names_conv, rotation=45, ha='right')\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 1]\n        training_times = [data['total_time'] for data in results.values()]\n        config_names_time = [name.split('(')[0].strip() for name in results.keys()]\n        bars = ax.bar(range(len(training_times)), training_times, \n                     color=colors[:len(training_times)])\n        ax.set_title('Training Time Comparison')\n        ax.set_xlabel('Configuration')\n        ax.set_ylabel('Total Training Time (s)')\n        ax.set_xticks(range(len(config_names_time)))\n        ax.set_xticklabels(config_names_time, rotation=45, ha='right')\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 2]\n        final_performances = [data['eval_performance']['mean_reward'] for data in results.values()]\n        training_times_norm = [data['total_time'] for data in results.values()]\n        scatter = ax.scatter(training_times_norm, final_performances, \n                           c=range(len(results)), cmap='viridis', s=100)\n        for i, name in enumerate(results.keys()):\n            short_name = name.replace('+ Double + Dueling + PER', 'Full').split('(')[0].strip()\n            ax.annotate(short_name, (training_times_norm[i], final_performances[i]),\n                       xytext=(5, 5), textcoords='offset points', fontsize=8)\n        ax.set_title('Performance vs Training Time')\n        ax.set_xlabel('Training Time (s)')\n        ax.set_ylabel('Final Performance')\n        ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n    def hyperparameter_sensitivity_analysis(self):\n        print(\"=\"*70)\n        print(\"Hyperparameter Sensitivity Analysis\")\n        print(\"=\"*70)\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        hyperparameter_configs = {\n            'Default': {\n                'lr': 1e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n                'target_update_freq': 100, 'buffer_size': 10000\n            },\n            'High Learning Rate': {\n                'lr': 5e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n                'target_update_freq': 100, 'buffer_size': 10000\n            },\n            'Low Learning Rate': {\n                'lr': 1e-4, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n                'target_update_freq': 100, 'buffer_size': 10000\n            },\n            'High Priority Alpha': {\n                'lr': 1e-3, 'alpha': 0.8, 'beta_start': 0.4, 'n_step': 3,\n                'target_update_freq': 100, 'buffer_size': 10000\n            },\n            'Low Priority Alpha': {\n                'lr': 1e-3, 'alpha': 0.4, 'beta_start': 0.4, 'n_step': 3,\n                'target_update_freq': 100, 'buffer_size': 10000\n            },\n            'Long N-Step': {\n                'lr': 1e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 5,\n                'target_update_freq': 100, 'buffer_size': 10000\n            },\n            'Frequent Target Update': {\n                'lr': 1e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n                'target_update_freq': 50, 'buffer_size': 10000\n            },\n            'Infrequent Target Update': {\n                'lr': 1e-3, 'alpha': 0.6, 'beta_start': 0.4, 'n_step': 3,\n                'target_update_freq': 200, 'buffer_size': 10000\n            }\n        }\n        results = {}\n        for config_name, params in hyperparameter_configs.items():\n            print(f\"\\nTesting: {config_name}\")\n            agent = RainbowDQNAgent(\n                state_dim=state_dim,\n                action_dim=action_dim,\n                use_noisy=True,\n                **params\n            )\n            episode_rewards = []\n            for episode in range(40):\n                reward, _ = agent.train_episode(env, max_steps=500)\n                episode_rewards.append(reward)\n                if (episode + 1) % 10 == 0:\n                    avg_reward = np.mean(episode_rewards[-5:])\n                    print(f\"  Episode {episode+1}: Reward = {avg_reward:.1f}\")\n            eval_results = agent.evaluate(env, num_episodes=10)\n            results[config_name] = {\n                'rewards': episode_rewards,\n                'eval_performance': eval_results,\n                'params': params\n            }\n        self._visualize_sensitivity_analysis(results)\n        env.close()\n        return results\n    def _visualize_sensitivity_analysis(self, results):\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        ax = axes[0, 0]\n        config_names = list(results.keys())\n        performances = [results[name]['eval_performance']['mean_reward'] for name in config_names]\n        bars = ax.bar(range(len(performances)), performances)\n        ax.set_title('Hyperparameter Sensitivity')\n        ax.set_xlabel('Configuration')\n        ax.set_ylabel('Evaluation Performance')\n        ax.set_xticks(range(len(config_names)))\n        ax.set_xticklabels(config_names, rotation=45, ha='right')\n        ax.grid(True, alpha=0.3)\n        for i, name in enumerate(config_names):\n            if name == 'Default':\n                bars[i].set_color('green')\n                bars[i].set_alpha(0.8)\n        ax = axes[0, 1]\n        colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n        for i, (name, data) in enumerate(results.items()):\n            rewards = data['rewards']\n            smoothed = pd.Series(rewards).rolling(3).mean()\n            if name == 'Default':\n                ax.plot(smoothed, label=name, color='green', linewidth=3)\n            else:\n                ax.plot(smoothed, label=name, color=colors[i], linewidth=2, alpha=0.7)\n        ax.set_title('Learning Curves')\n        ax.set_xlabel('Episode')\n        ax.set_ylabel('Episode Reward (Smoothed)')\n        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 0]\n        default_performance = results['Default']['eval_performance']['mean_reward']\n        param_impacts = {}\n        for name, data in results.items():\n            if name != 'Default':\n                performance = data['eval_performance']['mean_reward']\n                impact = (performance - default_performance) / default_performance * 100\n                param_impacts[name] = impact\n        names = list(param_impacts.keys())\n        impacts = list(param_impacts.values())\n        colors = ['red' if imp < 0 else 'green' for imp in impacts]\n        bars = ax.bar(range(len(impacts)), impacts, color=colors, alpha=0.7)\n        ax.set_title('Impact vs Default Configuration')\n        ax.set_xlabel('Configuration')\n        ax.set_ylabel('Performance Change (%)')\n        ax.set_xticks(range(len(names)))\n        ax.set_xticklabels(names, rotation=45, ha='right')\n        ax.axhline(y=0, color='black', linestyle='-', alpha=0.5)\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 1]\n        stabilities = []\n        config_names_stab = []\n        for name, data in results.items():\n            rewards = data['rewards']\n            cv = np.std(rewards[-10:]) / np.mean(rewards[-10:])\n            stabilities.append(cv)\n            config_names_stab.append(name)\n        bars = ax.bar(range(len(stabilities)), stabilities)\n        ax.set_title('Training Stability (Lower = More Stable)')\n        ax.set_xlabel('Configuration')\n        ax.set_ylabel('Coefficient of Variation')\n        ax.set_xticks(range(len(config_names_stab)))\n        ax.set_xticklabels(config_names_stab, rotation=45, ha='right')\n        ax.grid(True, alpha=0.3)\n        min_cv_idx = np.argmin(stabilities)\n        bars[min_cv_idx].set_color('green')\n        bars[min_cv_idx].set_alpha(0.8)\n        plt.tight_layout()\n        plt.show()\n        print(\"\\n\" + \"=\"*50)\n        print(\"SENSITIVITY ANALYSIS SUMMARY\")\n        print(\"=\"*50)\n        default_perf = results['Default']['eval_performance']['mean_reward']\n        print(f\"Default Performance: {default_perf:.1f}\")\n        print(\"\\nBest Alternative Configurations:\")\n        sorted_configs = sorted([(name, data['eval_performance']['mean_reward']) \n                               for name, data in results.items() if name != 'Default'],\n                              key=lambda x: x[1], reverse=True)\n        for i, (name, perf) in enumerate(sorted_configs[:3]):\n            improvement = (perf - default_perf) / default_perf * 100\n            print(f\"  {i+1}. {name}: {perf:.1f} ({improvement:+.1f}%)\")\nperformance_analyzer = AblationAnalyzer()\nprint(\"Starting Comprehensive Performance Analysis...\")\nprint(\"\\n1. Running Rainbow DQN Ablation Study...\")\nablation_results = performance_analyzer.rainbow_ablation_study()\nprint(\"\\n2. Running Hyperparameter Sensitivity Analysis...\")\nsensitivity_results = performance_analyzer.hyperparameter_sensitivity_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c629c",
   "metadata": {},
   "source": [
    "# Section 9: Advanced Topics and Future Directions\n",
    "\n",
    "## Beyond Rainbow DQN\n",
    "\n",
    "While Rainbow DQN represents a significant advancement in value-based reinforcement learning, the field continues to evolve. This section explores cutting-edge developments and future research directions.\n",
    "\n",
    "## Recent Advances\n",
    "\n",
    "### 1. Implicit Quantile Networks (IQN)\n",
    "**Key Idea**: Learn the full return distribution implicitly through quantile regression\n",
    "**Advantages**: \n",
    "- No need to pre-specify support for return distribution\n",
    "- Better risk-sensitive policies\n",
    "- Improved performance on complex environments\n",
    "\n",
    "### 2. Agent57\n",
    "**Key Idea**: Combine value-based and policy-based methods with meta-learning\n",
    "**Components**:\n",
    "- Never Give Up (NGU) exploration\n",
    "- Population-based training\n",
    "- Universal value functions\n",
    "\n",
    "### 3. MuZero\n",
    "**Key Idea**: Model-based planning with learned environment models\n",
    "**Innovation**: Plans in learned latent space rather than raw observations\n",
    "\n",
    "## Theoretical Understanding\n",
    "\n",
    "### Overestimation Bias Analysis\n",
    "Recent theoretical work has provided deeper insights into:\n",
    "- Sources of overestimation bias in Q-learning\n",
    "- Conditions under which Double DQN provably reduces bias\n",
    "- Trade-offs between bias and variance\n",
    "\n",
    "### Sample Complexity Bounds\n",
    "New theoretical results establish:\n",
    "- PAC bounds for deep Q-learning\n",
    "- Role of function approximation in sample complexity\n",
    "- Conditions for polynomial sample complexity\n",
    "\n",
    "## Implementation Considerations\n",
    "\n",
    "### Distributed Training\n",
    "Modern implementations leverage:\n",
    "- Distributed experience collection (Ape-X)\n",
    "- Asynchronous training (A3C-style)\n",
    "- GPU acceleration for neural network training\n",
    "\n",
    "### Architecture Innovations\n",
    "Recent architectural advances include:\n",
    "- Attention mechanisms in value networks\n",
    "- Graph neural networks for structured observations\n",
    "- Meta-learning architectures\n",
    "\n",
    "## Practical Applications\n",
    "\n",
    "### Real-World Deployment\n",
    "Challenges and solutions for deploying DQN in practice:\n",
    "- Safety constraints and safe exploration\n",
    "- Transfer learning and domain adaptation\n",
    "- Robustness to distribution shift\n",
    "\n",
    "### Multi-Task Learning\n",
    "Extensions for learning multiple tasks:\n",
    "- Universal value functions\n",
    "- Task-conditional networks\n",
    "- Meta-learning approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd151911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\nfrom matplotlib.patches import Rectangle\nclass AdvancedDQNAnalyzer:\n    def demonstrate_overestimation_sources(self):\n        print(\"=\"*70)\n        print(\"Sources of Overestimation Bias in Q-Learning\")\n        print(\"=\"*70)\n        np.random.seed(42)\n        n_actions = 4\n        n_states = 100\n        true_q_values = np.random.randn(n_states, n_actions)\n        approximation_noise = 0.2\n        approx_q_values = true_q_values + np.random.normal(0, approximation_noise, true_q_values.shape)\n        bootstrap_noise = 0.3\n        bootstrap_errors = np.random.normal(0, bootstrap_noise, n_states)\n        max_true = np.max(true_q_values, axis=1)\n        max_approx = np.max(approx_q_values, axis=1)\n        max_bias = max_approx - max_true\n        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n        ax = axes[0, 0]\n        ax.scatter(true_q_values.flatten(), approx_q_values.flatten(), alpha=0.5)\n        ax.plot([true_q_values.min(), true_q_values.max()], \n                [true_q_values.min(), true_q_values.max()], 'r--', label='Perfect Approximation')\n        ax.set_xlabel('True Q-Values')\n        ax.set_ylabel('Approximate Q-Values')\n        ax.set_title('Function Approximation Bias')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        approx_bias = np.mean(approx_q_values.flatten() - true_q_values.flatten())\n        ax.text(0.05, 0.95, f'Mean Bias: {approx_bias:.3f}', \n                transform=ax.transAxes, bbox=dict(boxstyle=\"round\", facecolor='wheat'))\n        ax = axes[0, 1]\n        ax.hist(max_bias, bins=20, alpha=0.7, edgecolor='black')\n        ax.axvline(np.mean(max_bias), color='red', linestyle='--', \n                  label=f'Mean Bias: {np.mean(max_bias):.3f}')\n        ax.set_xlabel('Maximization Bias (Approx Max - True Max)')\n        ax.set_ylabel('Frequency')\n        ax.set_title('Maximization Bias Distribution')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 0]\n        time_steps = np.arange(1000)\n        bias_accumulation = np.cumsum(np.random.normal(0.01, 0.05, 1000))\n        double_dqn_bias = np.cumsum(np.random.normal(0.002, 0.04, 1000))\n        ax.plot(time_steps, bias_accumulation, label='Standard DQN', linewidth=2)\n        ax.plot(time_steps, double_dqn_bias, label='Double DQN', linewidth=2)\n        ax.set_xlabel('Training Steps')\n        ax.set_ylabel('Cumulative Bias')\n        ax.set_title('Bias Accumulation Over Time')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax = axes[1, 1]\n        ax.axis('off')\n        bias_sources = [\n            \"1. Function Approximation Error\",\n            \"   • Neural network approximation\",\n            \"   • Limited capacity\",\n            \"   • Training dynamics\",\n            \"\",\n            \"2. Bootstrapping Error\", \n            \"   • TD target estimation\",\n            \"   • Recursive value updates\",\n            \"   • Error propagation\",\n            \"\",\n            \"3. Maximization Bias\",\n            \"   • Max operator over noisy estimates\",\n            \"   • Upward bias in action selection\",\n            \"   • Compounding over time\",\n            \"\",\n            \"Solutions:\",\n            \"• Double DQN: Separate action selection/evaluation\",\n            \"• Target networks: Stable bootstrap targets\",\n            \"• Better function approximation\"\n        ]\n        y_pos = 0.95\n        for line in bias_sources:\n            if line.startswith(\"   •\"):\n                ax.text(0.1, y_pos, line, transform=ax.transAxes, fontsize=10, color='blue')\n            elif line.startswith(\"Solutions:\"):\n                ax.text(0.05, y_pos, line, transform=ax.transAxes, fontsize=12, \n                       weight='bold', color='green')\n            elif line.startswith(\"•\"):\n                ax.text(0.1, y_pos, line, transform=ax.transAxes, fontsize=10, color='green')\n            elif line and not line.startswith(\" \"):\n                ax.text(0.05, y_pos, line, transform=ax.transAxes, fontsize=12, weight='bold')\n            else:\n                ax.text(0.05, y_pos, line, transform=ax.transAxes, fontsize=10)\n            y_pos -= 0.05\n        plt.tight_layout()\n        plt.show()\n        print(f\"\\nBias Analysis Results:\")\n        print(f\"  Function Approximation Bias: {approx_bias:.4f}\")\n        print(f\"  Maximization Bias (mean): {np.mean(max_bias):.4f}\")\n        print(f\"  Maximization Bias (std): {np.std(max_bias):.4f}\")\n        return {\n            'approximation_bias': approx_bias,\n            'maximization_bias_mean': np.mean(max_bias),\n            'maximization_bias_std': np.std(max_bias)\n        }\n    def visualize_dqn_evolution_timeline(self):\n        print(\"=\"*70)\n        print(\"DQN Evolution Timeline\")\n        print(\"=\"*70)\n        fig, ax = plt.subplots(figsize=(16, 10))\n        methods = [\n            {\"name\": \"Q-Learning\", \"year\": 1989, \"description\": \"Tabular value iteration\", \"color\": \"lightblue\"},\n            {\"name\": \"DQN\", \"year\": 2013, \"description\": \"Neural network function approximation\\n+ Experience Replay\", \"color\": \"blue\"},\n            {\"name\": \"Double DQN\", \"year\": 2015, \"description\": \"Reduced overestimation bias\", \"color\": \"green\"},\n            {\"name\": \"Dueling DQN\", \"year\": 2016, \"description\": \"Value/advantage decomposition\", \"color\": \"orange\"},\n            {\"name\": \"Prioritized ER\", \"year\": 2016, \"description\": \"Priority-based experience sampling\", \"color\": \"red\"},\n            {\"name\": \"Rainbow DQN\", \"year\": 2017, \"description\": \"Combination of improvements\\n+ Distributional RL\", \"color\": \"purple\"},\n            {\"name\": \"IQN\", \"year\": 2018, \"description\": \"Implicit Quantile Networks\", \"color\": \"brown\"},\n            {\"name\": \"Agent57\", \"year\": 2020, \"description\": \"Meta-learning + exploration\", \"color\": \"pink\"},\n            {\"name\": \"MuZero\", \"year\": 2020, \"description\": \"Model-based planning in latent space\", \"color\": \"gold\"}\n        ]\n        years = [method[\"year\"] for method in methods]\n        y_positions = np.arange(len(methods))\n        for i, method in enumerate(methods):\n            ax.scatter(method[\"year\"], i, s=200, c=method[\"color\"], zorder=3, alpha=0.8)\n            if i > 0:\n                ax.plot([methods[i-1][\"year\"], method[\"year\"]], [i-1, i], \n                       'k--', alpha=0.3, zorder=1)\n            ax.text(method[\"year\"] + 0.5, i, f\"{method['name']}\\n{method['description']}\", \n                   fontsize=10, verticalalignment='center', \n                   bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=method[\"color\"], alpha=0.3))\n        ax.set_yticks(y_positions)\n        ax.set_yticklabels([method[\"name\"] for method in methods])\n        ax.set_xlabel(\"Year\", fontsize=12)\n        ax.set_title(\"Evolution of Deep Q-Learning Methods\", fontsize=16, weight='bold')\n        ax.grid(True, alpha=0.3)\n        ax.set_xlim(1985, 2025)\n        innovations = [\n            {\"year\": 2013, \"innovation\": \"Deep Learning\\nRevolution\", \"y_offset\": 0.3},\n            {\"year\": 2016, \"innovation\": \"Multiple\\nImprovements\", \"y_offset\": -0.3},\n            {\"year\": 2017, \"innovation\": \"Integration\\nEra\", \"y_offset\": 0.3},\n            {\"year\": 2020, \"innovation\": \"Beyond\\nValue-Based\", \"y_offset\": -0.3}\n        ]\n        for inn in innovations:\n            ax.annotate(inn[\"innovation\"], xy=(inn[\"year\"], len(methods)/2), \n                       xytext=(inn[\"year\"], len(methods)/2 + inn[\"y_offset\"] * len(methods)),\n                       arrowprops=dict(arrowstyle='->', alpha=0.5),\n                       fontsize=9, ha='center', weight='bold', color='darkred')\n        plt.tight_layout()\n        plt.show()\n    def future_research_directions(self):\n        print(\"=\"*70)\n        print(\"Future Research Directions in Value-Based RL\")\n        print(\"=\"*70)\n        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n        ax = axes[0, 0]\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 10)\n        areas = {\n            \"Safety & Robustness\": {\"pos\": (2, 8), \"color\": \"red\"},\n            \"Sample Efficiency\": {\"pos\": (8, 8), \"color\": \"blue\"},\n            \"Multi-Task Learning\": {\"pos\": (2, 5), \"color\": \"green\"},\n            \"Theoretical Understanding\": {\"pos\": (8, 5), \"color\": \"orange\"},\n            \"Distributed Learning\": {\"pos\": (2, 2), \"color\": \"purple\"},\n            \"Real-World Applications\": {\"pos\": (8, 2), \"color\": \"brown\"},\n            \"Model-Based Integration\": {\"pos\": (5, 6.5), \"color\": \"pink\"},\n            \"Meta-Learning\": {\"pos\": (5, 3.5), \"color\": \"gray\"}\n        }\n        connections = [\n            (\"Safety & Robustness\", \"Real-World Applications\"),\n            (\"Sample Efficiency\", \"Theoretical Understanding\"),\n            (\"Multi-Task Learning\", \"Meta-Learning\"),\n            (\"Distributed Learning\", \"Sample Efficiency\"),\n            (\"Model-Based Integration\", \"Sample Efficiency\"),\n            (\"Meta-Learning\", \"Real-World Applications\")\n        ]\n        for area1, area2 in connections:\n            pos1 = areas[area1][\"pos\"]\n            pos2 = areas[area2][\"pos\"]\n            ax.plot([pos1[0], pos2[0]], [pos1[1], pos2[1]], 'k--', alpha=0.3)\n        for area, props in areas.items():\n            x, y = props[\"pos\"]\n            circle = plt.Circle((x, y), 0.8, color=props[\"color\"], alpha=0.6)\n            ax.add_patch(circle)\n            ax.text(x, y, area, ha='center', va='center', fontsize=9, \n                   weight='bold', wrap=True)\n        ax.set_title(\"Future Research Network\", fontsize=14, weight='bold')\n        ax.set_aspect('equal')\n        ax.axis('off')\n        ax = axes[0, 1]\n        future_timeline = [\n            {\"year\": 2024, \"development\": \"Improved Safety Guarantees\", \"probability\": 0.8},\n            {\"year\": 2025, \"development\": \"Better Theoretical Bounds\", \"probability\": 0.7},\n            {\"year\": 2026, \"development\": \"Large-Scale Multi-Task RL\", \"probability\": 0.6},\n            {\"year\": 2027, \"development\": \"Real-World Deployment\", \"probability\": 0.5},\n            {\"year\": 2028, \"development\": \"Human-Level Sample Efficiency\", \"probability\": 0.4},\n            {\"year\": 2030, \"development\": \"General Value Learning\", \"probability\": 0.3}\n        ]\n        years = [item[\"year\"] for item in future_timeline]\n        probabilities = [item[\"probability\"] for item in future_timeline]\n        developments = [item[\"development\"] for item in future_timeline]\n        bars = ax.barh(developments, probabilities, color=plt.cm.viridis(probabilities))\n        ax.set_xlabel(\"Estimated Probability\")\n        ax.set_title(\"Future Developments Timeline\", fontsize=14, weight='bold')\n        ax.set_xlim(0, 1)\n        for i, (dev, prob, year) in enumerate(zip(developments, probabilities, years)):\n            ax.text(prob + 0.02, i, f\"{year}\", va='center', fontsize=9)\n        ax = axes[1, 0]\n        challenges = [\"Sample Efficiency\", \"Safety\", \"Generalization\", \"Scalability\", \"Interpretability\"]\n        solutions = [\"Meta-Learning\", \"Constrained RL\", \"Transfer Learning\", \"Distributed Training\", \"Attention Mechanisms\"]\n        impact_matrix = np.array([\n            [0.9, 0.3, 0.7, 0.6, 0.4],\n            [0.2, 0.9, 0.4, 0.3, 0.5],\n            [0.8, 0.3, 0.9, 0.4, 0.3],\n            [0.4, 0.2, 0.5, 0.9, 0.3],\n            [0.3, 0.6, 0.4, 0.3, 0.8]\n        ])\n        im = ax.imshow(impact_matrix, cmap='RdYlBu_r', aspect='auto')\n        ax.set_xticks(range(len(solutions)))\n        ax.set_yticks(range(len(challenges)))\n        ax.set_xticklabels(solutions, rotation=45, ha='right')\n        ax.set_yticklabels(challenges)\n        for i in range(len(challenges)):\n            for j in range(len(solutions)):\n                ax.text(j, i, f'{impact_matrix[i, j]:.1f}', \n                       ha='center', va='center', fontweight='bold')\n        ax.set_title(\"Challenges vs Solutions Impact Matrix\", fontsize=14, weight='bold')\n        plt.colorbar(im, ax=ax, label='Impact Score')\n        ax = axes[1, 1]\n        ax.axis('off')\n        research_questions = [\n            \"🔬 How can we provide theoretical guarantees for deep RL?\",\n            \"\",\n            \"🛡️ How do we ensure safety in continuous learning systems?\",\n            \"\",\n            \"🧠 Can we achieve human-level sample efficiency?\",\n            \"\",\n            \"🌐 How do we scale RL to real-world complexity?\",\n            \"\",\n            \"🔄 How can we enable lifelong learning without forgetting?\",\n            \"\",\n            \"🤖 How do we make RL systems interpretable and trustworthy?\",\n            \"\",\n            \"🌟 What are the fundamental limits of value-based methods?\"\n        ]\n        y_pos = 0.95\n        for question in research_questions:\n            if question:\n                ax.text(0.05, y_pos, question, transform=ax.transAxes, fontsize=11, \n                       weight='bold' if question.startswith('🔬') or question.startswith('🛡️') \n                              or question.startswith('🧠') or question.startswith('🌐') \n                              or question.startswith('🔄') or question.startswith('🤖') \n                              or question.startswith('🌟') else 'normal',\n                       wrap=True)\n            y_pos -= 0.12\n        ax.set_title(\"Key Open Research Questions\", fontsize=14, weight='bold')\n        plt.tight_layout()\n        plt.show()\n        print(\"\\nFuture Research Priorities:\")\n        print(\"1. Safety and Robustness: Developing provably safe RL systems\")\n        print(\"2. Sample Efficiency: Approaching human-level learning efficiency\") \n        print(\"3. Theoretical Foundations: Better understanding of deep RL\")\n        print(\"4. Real-World Deployment: Bridging sim-to-real gap\")\n        print(\"5. Multi-Task Learning: General value learning systems\")\ndef comprehensive_dqn_summary():\n    print(\"=\"*70)\n    print(\"COMPREHENSIVE DQN SUMMARY\")\n    print(\"=\"*70)\n    summary_text = f\"\"\"\n    Deep Q-Networks (DQN) and Value-Based Methods: Complete Analysis\n    📚 THEORETICAL FOUNDATIONS\n    • Q-Learning: Fundamental value iteration algorithm\n    • Function Approximation: Neural networks for continuous state spaces\n    • Experience Replay: Breaking temporal correlations in training data\n    • Target Networks: Providing stable bootstrap targets\n    🔧 CORE IMPROVEMENTS\n    • Double DQN: Addressing overestimation bias through action decoupling\n    • Dueling DQN: Separating state values and action advantages\n    • Prioritized Experience Replay: Learning from important transitions\n    • Multi-step Learning: Balancing bias and variance in temporal difference\n    🌈 INTEGRATION (Rainbow DQN)\n    • Combines multiple improvements for state-of-the-art performance\n    • Noisy Networks: Parameter space exploration\n    • Distributional RL: Modeling return distributions\n    • Careful hyperparameter tuning and implementation details\n    📊 KEY INSIGHTS FROM ANALYSIS\n    • Each component provides meaningful improvements\n    • Components often have synergistic effects when combined\n    • Prioritized replay provides largest single improvement\n    • Double DQN is crucial for stability\n    • Dueling architecture helps with value estimation\n    🎯 PRACTICAL CONSIDERATIONS\n    • Implementation complexity increases with sophistication\n    • Computational overhead varies significantly between methods\n    • Hyperparameter sensitivity requires careful tuning\n    • Environment characteristics affect relative performance\n    🔮 FUTURE DIRECTIONS\n    • Better theoretical understanding of deep RL\n    • Improved sample efficiency for real-world applications  \n    • Safety and robustness guarantees\n    • Integration with model-based methods\n    • Multi-task and meta-learning capabilities\n    💡 MAIN TAKEAWAYS\n    • DQN revolutionized RL by enabling function approximation\n    • Systematic improvements address specific algorithmic weaknesses\n    • Rainbow represents effective integration of multiple advances\n    • Future work focuses on theory, safety, and real-world deployment\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}