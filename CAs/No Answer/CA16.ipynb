{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85a1e82",
   "metadata": {},
   "source": [
    "# CA16: Cutting-Edge Deep Reinforcement Learning - Foundation Models, Neurosymbolic RL, and Future Paradigms\n",
    "\n",
    "## Deep Reinforcement Learning - Advanced Topics and Emerging Paradigms\n",
    "\n",
    "This comprehensive notebook explores the latest frontiers in Deep Reinforcement Learning, covering foundation models, neurosymbolic approaches, continual learning, human-AI collaboration, and emerging paradigms that will shape the future of intelligent agents.\n",
    "\n",
    "## Topics Covered:\n",
    "\n",
    "### ðŸ§  **Foundation Models in RL**\n",
    "- Large-scale pre-trained RL models\n",
    "- Decision Transformer and Trajectory Transformers\n",
    "- Multi-task and multi-modal RL agents\n",
    "- In-context learning for RL\n",
    "\n",
    "### ðŸ”¬ **Neurosymbolic Reinforcement Learning**\n",
    "- Symbolic reasoning integration\n",
    "- Logic-guided policy learning\n",
    "- Interpretable and explainable RL\n",
    "- Causal reasoning in RL\n",
    "\n",
    "### ðŸ”„ **Continual and Lifelong Learning**\n",
    "- Catastrophic forgetting in RL\n",
    "- Meta-learning and adaptation\n",
    "- Progressive neural networks\n",
    "- Memory systems for continual RL\n",
    "\n",
    "### ðŸ¤ **Human-AI Collaborative RL**\n",
    "- Learning from human feedback (RLHF)\n",
    "- Interactive learning and teaching\n",
    "- Preference learning and reward modeling\n",
    "- Constitutional AI and value alignment\n",
    "\n",
    "### âš¡ **Advanced Computational Methods**\n",
    "- Quantum-inspired RL algorithms\n",
    "- Neuromorphic computing for RL\n",
    "- Distributed and federated RL\n",
    "- Energy-efficient RL architectures\n",
    "\n",
    "### ðŸŒ **Real-World Deployment and Ethics**\n",
    "- Production RL systems\n",
    "- Ethical considerations and fairness\n",
    "- Robustness and reliability\n",
    "- Regulatory compliance and safety\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Master foundation model architectures for reinforcement learning\n",
    "2. Implement neurosymbolic RL algorithms with interpretability\n",
    "3. Design continual learning systems that avoid catastrophic forgetting\n",
    "4. Build human-AI collaborative learning frameworks\n",
    "5. Explore quantum and neuromorphic computing paradigms\n",
    "6. Apply advanced RL to real-world production systems\n",
    "7. Address ethical considerations and societal impact\n",
    "8. Analyze emerging paradigms and future research directions\n",
    "\n",
    "### Session Structure:\n",
    "- **Section 1**: Foundation Models and Large-Scale RL\n",
    "- **Section 2**: Neurosymbolic RL and Interpretability\n",
    "- **Section 3**: Continual Learning and Meta-Learning\n",
    "- **Section 4**: Human-AI Collaborative Learning\n",
    "- **Section 5**: Advanced Computational Paradigms\n",
    "- **Section 6**: Real-World Deployment and Ethics\n",
    "- **Section 7**: Future Directions and Research Frontiers\n",
    "\n",
    "---\n",
    "**Assignment Date**: Cutting-Edge Deep RL - Lesson 16  \n",
    "**Estimated Time**: 4-5 hours  \n",
    "**Difficulty**: Research-Level Advanced  \n",
    "**Prerequisites**: CA1-CA15 completed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc93e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Normal, Categorical, MultivariateNormal\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import deque, namedtuple, OrderedDict\nimport random\nimport copy\nimport math\nimport time\nimport gym\nfrom typing import List, Dict, Tuple, Optional, Union, Any, Callable\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom dataclasses import dataclass\nfrom abc import ABC, abstractmethod\nimport json\nimport pickle\nfrom datetime import datetime\nimport logging\nfrom pathlib import Path\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nprint(f\"ðŸš€ Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"ðŸ’« GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"ðŸ”¢ CUDA Version: {torch.version.cuda}\")\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.alpha'] = 0.3\nplt.rcParams['figure.dpi'] = 100\nFOUNDATION_MODEL_CONFIG = {\n    'model_dim': 512,\n    'num_heads': 8,\n    'num_layers': 6,\n    'context_length': 1024,\n    'dropout': 0.1,\n    'layer_norm_eps': 1e-5,\n    'max_position_embeddings': 2048\n}\nNEUROSYMBOLIC_CONFIG = {\n    'logic_embedding_dim': 128,\n    'symbolic_vocab_size': 1000,\n    'reasoning_steps': 5,\n    'symbolic_weight': 0.3,\n    'neural_weight': 0.7,\n    'interpretability_threshold': 0.8\n}\nCONTINUAL_LEARNING_CONFIG = {\n    'ewc_lambda': 1000,\n    'memory_size': 10000,\n    'num_tasks': 10,\n    'adaptation_lr': 1e-4,\n    'meta_lr': 1e-3,\n    'forgetting_threshold': 0.1\n}\nHUMAN_AI_CONFIG = {\n    'preference_model_dim': 256,\n    'reward_model_lr': 3e-4,\n    'human_feedback_ratio': 0.1,\n    'preference_batch_size': 64,\n    'kl_penalty': 0.1,\n    'value_alignment_weight': 1.0\n}\nQUANTUM_RL_CONFIG = {\n    'num_qubits': 8,\n    'circuit_depth': 10,\n    'quantum_lr': 0.01,\n    'entanglement_layers': 3,\n    'measurement_shots': 1024,\n    'quantum_advantage_threshold': 1.5\n}\nprint(\"\\nðŸ§  Cutting-Edge Deep RL Environment Initialized!\")\nprint(\"ðŸ”¬ Advanced Topics: Foundation Models, Neurosymbolic RL, Continual Learning\")\nprint(\"ðŸ¤ Human-AI Collaboration, Quantum RL, Ethics & Future Paradigms\")\nprint(\"âš¡ Ready for next-generation reinforcement learning research!\")\nprint(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57300682",
   "metadata": {},
   "source": [
    "# Section 1: Foundation Models in Reinforcement Learning\n",
    "\n",
    "Foundation models represent a paradigm shift in AI, where large-scale pre-trained models can be adapted to various downstream tasks. In RL, this concept translates to training massive models on diverse experiences that can then be fine-tuned for specific tasks.\n",
    "\n",
    "## 1.1 Theoretical Foundations\n",
    "\n",
    "### Decision Transformers\n",
    "The Decision Transformer reframes RL as a sequence modeling problem, where the goal is to generate actions conditioned on desired returns.\n",
    "\n",
    "**Key Insight**: Instead of learning value functions or policy gradients, we model:\n",
    "$$P(a_t | s_{1:t}, a_{1:t-1}, R_{t:T})$$\n",
    "\n",
    "Where $R_{t:T}$ represents the desired return-to-go from time $t$ to episode end $T$.\n",
    "\n",
    "### Trajectory Transformers\n",
    "Extend transformers to model entire trajectories:\n",
    "$$P(\\tau | g) = \\prod_{t=0}^{T} P(s_{t+1}, r_t, a_t | s_{1:t}, a_{1:t-1}, g)$$\n",
    "\n",
    "Where $g$ represents the goal or task specification.\n",
    "\n",
    "### Multi-Task Pre-training\n",
    "Foundation models in RL are trained on massive datasets containing:\n",
    "- Multiple environments and tasks\n",
    "- Diverse behavioral policies\n",
    "- Various skill demonstrations\n",
    "- Cross-modal experiences (vision, language, control)\n",
    "\n",
    "**Training Objective**:\n",
    "$$\\mathcal{L} = \\sum_{\\mathcal{D}_i} \\mathbb{E}_{\\tau \\sim \\mathcal{D}_i} [-\\log P(\\tau | \\text{context}_i)]$$\n",
    "\n",
    "### In-Context Learning for RL\n",
    "Similar to language models, RL foundation models can adapt to new tasks through in-context learning:\n",
    "- Provide few-shot demonstrations\n",
    "- Model infers task structure and optimal behavior\n",
    "- No gradient updates required\n",
    "\n",
    "## 1.2 Advantages and Challenges\n",
    "\n",
    "### Advantages:\n",
    "1. **Sample Efficiency**: Leverage pre-training for rapid adaptation\n",
    "2. **Generalization**: Transfer knowledge across diverse tasks\n",
    "3. **Few-Shot Learning**: Adapt to new tasks with minimal data\n",
    "4. **Unified Architecture**: Single model for multiple domains\n",
    "\n",
    "### Challenges:\n",
    "1. **Computational Requirements**: Massive models need significant resources\n",
    "2. **Data Requirements**: Need diverse, high-quality training data\n",
    "3. **Task Distribution**: Performance depends on training task diversity\n",
    "4. **Fine-tuning Complexity**: Avoiding catastrophic forgetting during adaptation\n",
    "\n",
    "### Scaling Laws in RL\n",
    "Similar to language models, RL foundation models exhibit scaling laws:\n",
    "- **Model Size**: Larger models achieve better performance\n",
    "- **Data Scale**: More diverse training data improves generalization\n",
    "- **Compute**: Increased training compute enables larger models\n",
    "\n",
    "**Empirical Scaling Relationship**:\n",
    "$$\\text{Performance} \\propto \\alpha N^{\\beta} D^{\\gamma} C^{\\delta}$$\n",
    "\n",
    "Where $N$ = model parameters, $D$ = dataset size, $C$ = compute budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33514387",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]\nclass DecisionTransformer(nn.Module):\n    def __init__(self, state_dim, action_dim, model_dim=512, num_heads=8, num_layers=6, \n                 max_length=1024, dropout=0.1):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.model_dim = model_dim\n        self.max_length = max_length\n        self.state_embedding = nn.Linear(state_dim, model_dim)\n        self.action_embedding = nn.Linear(action_dim, model_dim)\n        self.return_embedding = nn.Linear(1, model_dim)\n        self.timestep_embedding = nn.Embedding(max_length, model_dim)\n        self.pos_encoding = PositionalEncoding(model_dim, max_length * 3)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=model_dim,\n            nhead=num_heads,\n            dim_feedforward=4 * model_dim,\n            dropout=dropout,\n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n        self.layer_norm = nn.LayerNorm(model_dim)\n        self.action_head = nn.Linear(model_dim, action_dim)\n        self.value_head = nn.Linear(model_dim, 1)\n        self.return_head = nn.Linear(model_dim, 1)\n        self.dropout = nn.Dropout(dropout)\n        self.apply(self._init_weights)\n    def _init_weights(self, module):\n        if isinstance(module, (nn.Linear, nn.Embedding)):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if isinstance(module, nn.Linear) and module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n    def forward(self, states, actions, returns_to_go, timesteps, attention_mask=None):\n        batch_size, seq_len = states.shape[0], states.shape[1]\n        state_embeddings = self.state_embedding(states)\n        action_embeddings = self.action_embedding(actions)\n        return_embeddings = self.return_embedding(returns_to_go)\n        time_embeddings = self.timestep_embedding(timesteps)\n        state_embeddings += time_embeddings\n        action_embeddings += time_embeddings\n        return_embeddings += time_embeddings\n        stacked_inputs = torch.stack([\n            return_embeddings, state_embeddings, action_embeddings\n        ], dim=2).reshape(batch_size, 3 * seq_len, self.model_dim)\n        stacked_inputs = self.pos_encoding(stacked_inputs.transpose(0, 1)).transpose(0, 1)\n        stacked_inputs = self.layer_norm(stacked_inputs)\n        stacked_inputs = self.dropout(stacked_inputs)\n        transformer_output = self.transformer(stacked_inputs, src_key_padding_mask=attention_mask)\n        transformer_output = transformer_output.reshape(batch_size, seq_len, 3, self.model_dim)\n        return_preds = self.return_head(transformer_output[:, :, 0])\n        state_preds = transformer_output[:, :, 1]\n        action_preds = self.action_head(transformer_output[:, :, 2])\n        value_preds = self.value_head(transformer_output[:, :, 1])\n        return {\n            'action_preds': action_preds,\n            'value_preds': value_preds,\n            'return_preds': return_preds,\n            'state_representations': state_preds\n        }\n    def get_action(self, states, actions, returns_to_go, timesteps, temperature=1.0):\n        self.eval()\n        with torch.no_grad():\n            outputs = self.forward(states, actions, returns_to_go, timesteps)\n            action_logits = outputs['action_preds'][:, -1] / temperature\n            if self.action_dim > 1:\n                action_probs = F.softmax(action_logits, dim=-1)\n                action = torch.multinomial(action_probs, 1)\n            else:\n                action = torch.tanh(action_logits)\n            return action\nclass MultiTaskRLFoundationModel(nn.Module):\n    def __init__(self, state_dim, action_dim, task_dim, model_dim=512, num_heads=8, num_layers=6):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.task_dim = task_dim\n        self.model_dim = model_dim\n        self.task_embedding = nn.Embedding(task_dim, model_dim)\n        self.decision_transformer = DecisionTransformer(\n            state_dim, action_dim, model_dim, num_heads, num_layers\n        )\n        self.task_heads = nn.ModuleDict({\n            f'task_{i}': nn.Linear(model_dim, action_dim)\n            for i in range(task_dim)\n        })\n        self.context_encoder = nn.LSTM(model_dim, model_dim, batch_first=True)\n        self.adaptation_network = nn.Sequential(\n            nn.Linear(model_dim, model_dim),\n            nn.ReLU(),\n            nn.Linear(model_dim, model_dim)\n        )\n    def forward(self, states, actions, returns_to_go, timesteps, task_ids, context_length=10):\n        batch_size = states.shape[0]\n        task_embeds = self.task_embedding(task_ids)\n        task_embeds = task_embeds.unsqueeze(1).expand(-1, states.shape[1], -1)\n        conditioned_states = states + task_embeds[:, :, :self.state_dim]\n        outputs = self.decision_transformer(conditioned_states, actions, returns_to_go, timesteps)\n        state_representations = outputs['state_representations']\n        task_specific_actions = []\n        for i, task_id in enumerate(task_ids):\n            task_head = self.task_heads[f'task_{task_id.item()}']\n            task_action = task_head(state_representations[i])\n            task_specific_actions.append(task_action)\n        outputs['task_specific_actions'] = torch.stack(task_specific_actions)\n        return outputs\n    def adapt_to_new_task(self, context_trajectories, num_adaptation_steps=5):\n        context_features = []\n        for trajectory in context_trajectories:\n            states, actions, returns = trajectory['states'], trajectory['actions'], trajectory['returns']\n            timesteps = torch.arange(len(states))\n            with torch.no_grad():\n                outputs = self.decision_transformer(states, actions, returns, timesteps)\n                context_features.append(outputs['state_representations'].mean(dim=1))\n        context_features = torch.stack(context_features)\n        context_encoding, _ = self.context_encoder(context_features.unsqueeze(0))\n        adaptation_params = self.adaptation_network(context_encoding.squeeze(0).mean(dim=0))\n        return adaptation_params\nclass InContextLearningRL:\n    def __init__(self, foundation_model, context_length=50):\n        self.foundation_model = foundation_model\n        self.context_length = context_length\n        self.context_buffer = deque(maxlen=context_length)\n    def add_context(self, state, action, reward, next_state, done):\n        self.context_buffer.append({\n            'state': state,\n            'action': action,\n            'reward': reward,\n            'next_state': next_state,\n            'done': done\n        })\n    def get_action(self, current_state, desired_return, temperature=1.0):\n        if len(self.context_buffer) == 0:\n            return np.random.randint(self.foundation_model.action_dim)\n        context_states = []\n        context_actions = []\n        context_returns = []\n        context_timesteps = []\n        cumulative_return = 0\n        for i, exp in enumerate(reversed(list(self.context_buffer))):\n            context_states.append(exp['state'])\n            context_actions.append(exp['action'])\n            cumulative_return += exp['reward']\n            context_returns.append([cumulative_return])\n            context_timesteps.append(len(self.context_buffer) - i - 1)\n        context_states.reverse()\n        context_actions.reverse()\n        context_returns.reverse()\n        context_timesteps.reverse()\n        context_states.append(current_state)\n        context_actions.append(np.zeros(self.foundation_model.action_dim))\n        context_returns.append([desired_return])\n        context_timesteps.append(len(self.context_buffer))\n        states = torch.FloatTensor(context_states).unsqueeze(0).to(device)\n        actions = torch.FloatTensor(context_actions).unsqueeze(0).to(device)\n        returns_to_go = torch.FloatTensor(context_returns).unsqueeze(0).to(device)\n        timesteps = torch.LongTensor(context_timesteps).unsqueeze(0).to(device)\n        with torch.no_grad():\n            action = self.foundation_model.get_action(states, actions, returns_to_go, timesteps, temperature)\n        return action.cpu().numpy().flatten()\nclass FoundationModelTrainer:\n    def __init__(self, model, learning_rate=1e-4, weight_decay=1e-2):\n        self.model = model\n        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1000)\n        self.training_stats = {\n            'losses': [],\n            'action_losses': [],\n            'value_losses': [],\n            'return_losses': []\n        }\n    def train_step(self, batch):\n        self.model.train()\n        self.optimizer.zero_grad()\n        states = batch['states'].to(device)\n        actions = batch['actions'].to(device)\n        returns_to_go = batch['returns_to_go'].to(device)\n        timesteps = batch['timesteps'].to(device)\n        target_actions = batch['target_actions'].to(device)\n        target_returns = batch['target_returns'].to(device)\n        outputs = self.model(states, actions, returns_to_go, timesteps)\n        action_loss = F.mse_loss(outputs['action_preds'], target_actions)\n        value_loss = F.mse_loss(outputs['value_preds'], target_returns)\n        return_loss = F.mse_loss(outputs['return_preds'], target_returns)\n        total_loss = action_loss + 0.5 * value_loss + 0.1 * return_loss\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        self.scheduler.step()\n        self.training_stats['losses'].append(total_loss.item())\n        self.training_stats['action_losses'].append(action_loss.item())\n        self.training_stats['value_losses'].append(value_loss.item())\n        self.training_stats['return_losses'].append(return_loss.item())\n        return total_loss.item()\nprint(\"ðŸ§  Foundation Models Implementation Complete!\")\nprint(\"ðŸ“Š Key Components:\")\nprint(\"  â€¢ DecisionTransformer: Sequence-based RL with transformers\")\nprint(\"  â€¢ MultiTaskRLFoundationModel: Multi-task pre-training framework\")\nprint(\"  â€¢ InContextLearningRL: Few-shot adaptation without gradient updates\")\nprint(\"  â€¢ FoundationModelTrainer: Scalable training infrastructure\")\nprint(\"\\nâœ¨ Ready for large-scale RL foundation model training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383bdef",
   "metadata": {},
   "source": [
    "# Section 2: Neurosymbolic Reinforcement Learning\n",
    "\n",
    "Neurosymbolic RL combines the learning capabilities of neural networks with the reasoning power of symbolic systems, creating interpretable and more robust intelligent agents.\n",
    "\n",
    "## 2.1 Theoretical Foundations\n",
    "\n",
    "### The Neurosymbolic Paradigm\n",
    "Traditional RL systems struggle with:\n",
    "- **Interpretability**: Understanding why decisions were made\n",
    "- **Compositional Reasoning**: Combining learned concepts systematically\n",
    "- **Sample Efficiency**: Learning abstract rules from limited data\n",
    "- **Transfer**: Applying learned knowledge to new domains\n",
    "\n",
    "**Neurosymbolic RL** addresses these challenges by integrating:\n",
    "- **Neural Components**: Learning from raw sensory data\n",
    "- **Symbolic Components**: Logical reasoning and rule-based inference\n",
    "- **Hybrid Architectures**: Seamless integration of both paradigms\n",
    "\n",
    "### Core Components\n",
    "\n",
    "#### 1. Symbolic Knowledge Representation\n",
    "Represent environment knowledge using formal logic:\n",
    "- **Predicate Logic**: $\\text{at}(\\text{agent}, x, y) \\land \\text{obstacle}(x+1, y) \\rightarrow \\neg \\text{move\\_right}$\n",
    "- **Temporal Logic**: $\\square (\\text{goal\\_reached} \\rightarrow \\Diamond \\text{reward})$\n",
    "- **Probabilistic Logic**: $P(\\text{success} | \\text{action}, \\text{state}) = 0.8$\n",
    "\n",
    "#### 2. Neural-Symbolic Integration Patterns\n",
    "\n",
    "**Pattern 1: Neural Perception + Symbolic Reasoning**\n",
    "$$\\pi(a|s) = \\text{SymbolicPlanner}(\\text{NeuralPerception}(s))$$\n",
    "\n",
    "**Pattern 2: Symbolic-Guided Neural Learning**\n",
    "$$\\mathcal{L} = \\mathcal{L}_{\\text{RL}} + \\lambda \\mathcal{L}_{\\text{logic}}$$\n",
    "\n",
    "**Pattern 3: Hybrid Representations**\n",
    "$$h = \\text{Combine}(h_{\\text{neural}}, h_{\\text{symbolic}})$$\n",
    "\n",
    "### Logical Policy Learning\n",
    "Learn policies that satisfy logical constraints:\n",
    "\n",
    "**Constraint Satisfaction**:\n",
    "$$\\pi^* = \\arg\\max_\\pi \\mathbb{E}_\\pi[R] \\text{ subject to } \\phi \\models \\psi$$\n",
    "\n",
    "Where $\\phi$ represents the policy behavior and $\\psi$ represents logical constraints.\n",
    "\n",
    "**Logic-Regularized RL**:\n",
    "$$\\mathcal{L} = -\\mathbb{E}_\\pi[R] + \\alpha \\cdot \\text{LogicViolation}(\\pi, \\psi)$$\n",
    "\n",
    "### Compositional Learning\n",
    "Enable agents to compose learned primitives:\n",
    "\n",
    "**Hierarchical Composition**:\n",
    "- **Skills**: $\\pi_1, \\pi_2, \\ldots, \\pi_k$\n",
    "- **Meta-Policy**: $\\pi_{\\text{meta}}(k|s)$\n",
    "- **Composition Rule**: $\\pi(a|s) = \\sum_k \\pi_{\\text{meta}}(k|s) \\pi_k(a|s)$\n",
    "\n",
    "**Logical Composition**:\n",
    "- **Primitive Predicates**: $p_1, p_2, \\ldots, p_n$\n",
    "- **Logical Operators**: $\\land, \\lor, \\neg, \\rightarrow$\n",
    "- **Complex Behaviors**: $\\psi = p_1 \\land (p_2 \\lor \\neg p_3) \\rightarrow p_4$\n",
    "\n",
    "## 2.2 Interpretability and Explainability\n",
    "\n",
    "### Attention-Based Explanations\n",
    "Use attention mechanisms to highlight decision factors:\n",
    "$$\\alpha_i = \\frac{\\exp(e_i)}{\\sum_j \\exp(e_j)}, \\quad e_i = f_{\\text{att}}(h_i)$$\n",
    "\n",
    "### Counterfactual Reasoning\n",
    "Generate explanations through counterfactuals:\n",
    "- **Question**: \"What if state $s$ were different?\"\n",
    "- **Counterfactual State**: $s' = s + \\delta$\n",
    "- **Action Change**: $\\Delta a = \\pi(s') - \\pi(s)$\n",
    "- **Explanation**: \"If $x$ were true, agent would do $y$ instead\"\n",
    "\n",
    "### Causal Discovery in RL\n",
    "Learn causal relationships between variables:\n",
    "$$X \\rightarrow Y \\text{ if } I(Y; \\text{do}(X)) > 0$$\n",
    "\n",
    "Where $I$ is mutual information and $\\text{do}(X)$ represents intervention.\n",
    "\n",
    "### Logical Rule Extraction\n",
    "Extract interpretable rules from trained policies:\n",
    "1. **State Abstraction**: Group similar states\n",
    "2. **Action Patterns**: Identify consistent action choices\n",
    "3. **Rule Formation**: Convert patterns to logical rules\n",
    "4. **Rule Validation**: Test rules on new data\n",
    "\n",
    "## 2.3 Advanced Neurosymbolic Architectures\n",
    "\n",
    "### Differentiable Neural Module Networks (dNMNs)\n",
    "Compose neural modules based on language instructions:\n",
    "- **Modules**: $\\{m_1, m_2, \\ldots, m_k\\}$\n",
    "- **Composition**: Dynamic module assembly\n",
    "- **Training**: End-to-end differentiable\n",
    "\n",
    "### Graph Neural Networks for Symbolic Reasoning\n",
    "Represent knowledge as graphs and use GNNs:\n",
    "- **Nodes**: Entities, concepts, states\n",
    "- **Edges**: Relations, transitions, dependencies\n",
    "- **Message Passing**: Propagate information through graph\n",
    "- **Reasoning**: Multi-hop inference over graph structure\n",
    "\n",
    "### Memory-Augmented Networks\n",
    "External memory for symbolic knowledge storage:\n",
    "- **Memory Matrix**: $M \\in \\mathbb{R}^{N \\times D}$\n",
    "- **Attention**: $w = \\text{softmax}(q^T M)$\n",
    "- **Read**: $r = w^T M$\n",
    "- **Write**: $M \\leftarrow M + w \\odot \\text{update}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2474f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import List, Dict, Tuple, Any\nfrom dataclasses import dataclass\nfrom enum import Enum\nimport networkx as nx\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict\nclass LogicalOperator(Enum):\n    AND = \"and\"\n    OR = \"or\"\n    NOT = \"not\"\n    IMPLIES = \"implies\"\n@dataclass\nclass LogicalPredicate:\n    name: str\n    args: List[str]\n    truth_value: float = 0.0\n    def __str__(self):\n        if self.args:\n            return f\"{self.name}({', '.join(self.args)})\"\n        return self.name\n@dataclass\nclass LogicalRule:\n    premises: List[LogicalPredicate]\n    conclusion: LogicalPredicate\n    operator: LogicalOperator\n    confidence: float = 1.0\n    def evaluate(self, facts: Dict[str, float]) -> float:\n        premise_values = []\n        for premise in self.premises:\n            key = str(premise)\n            premise_values.append(facts.get(key, 0.0))\n        if self.operator == LogicalOperator.AND:\n            premise_truth = min(premise_values) if premise_values else 0.0\n        elif self.operator == LogicalOperator.OR:\n            premise_truth = max(premise_values) if premise_values else 0.0\n        elif self.operator == LogicalOperator.NOT:\n            premise_truth = 1.0 - max(premise_values) if premise_values else 1.0\n        elif self.operator == LogicalOperator.IMPLIES:\n            premise_truth = min(premise_values) if premise_values else 0.0\n        conclusion_key = str(self.conclusion)\n        current_conclusion = facts.get(conclusion_key, 0.0)\n        if self.operator == LogicalOperator.IMPLIES:\n            return min(1.0, 1.0 - premise_truth + current_conclusion) * self.confidence\n        return premise_truth * self.confidence\nclass SymbolicKnowledgeBase:\n    def __init__(self):\n        self.rules: List[LogicalRule] = []\n        self.facts: Dict[str, float] = {}\n        self.predicates: Dict[str, LogicalPredicate] = {}\n    def add_rule(self, rule: LogicalRule):\n        self.rules.append(rule)\n    def add_fact(self, predicate: LogicalPredicate, truth_value: float):\n        key = str(predicate)\n        self.facts[key] = truth_value\n        self.predicates[key] = predicate\n    def forward_chain(self, max_iterations: int = 10) -> Dict[str, float]:\n        for iteration in range(max_iterations):\n            changed = False\n            for rule in self.rules:\n                rule_activation = rule.evaluate(self.facts)\n                conclusion_key = str(rule.conclusion)\n                old_value = self.facts.get(conclusion_key, 0.0)\n                new_value = max(old_value, rule_activation)\n                if new_value != old_value:\n                    self.facts[conclusion_key] = new_value\n                    changed = True\n            if not changed:\n                break\n        return self.facts\n    def explain_decision(self, query: str) -> List[str]:\n        explanations = []\n        for rule in self.rules:\n            if str(rule.conclusion) == query:\n                activation = rule.evaluate(self.facts)\n                if activation > 0.1:\n                    premise_str = f\" {rule.operator.value} \".join([str(p) for p in rule.premises])\n                    explanations.append(f\"{query} because {premise_str} (confidence: {activation:.2f})\")\n        return explanations\nclass NeuralPerceptionModule(nn.Module):\n    def __init__(self, state_dim: int, predicate_dim: int, hidden_dim: int = 64):\n        super().__init__()\n        self.state_dim = state_dim\n        self.predicate_dim = predicate_dim\n        self.encoder = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, predicate_dim),\n            nn.Sigmoid()\n        )\n        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        features = self.encoder[:-1](state)\n        features_expanded = features.unsqueeze(1)\n        attended_features, attention_weights = self.attention(\n            features_expanded, features_expanded, features_expanded\n        )\n        attended_features = attended_features.squeeze(1)\n        predicates = torch.sigmoid(self.encoder[-1](attended_features))\n        return predicates, attention_weights\nclass SymbolicReasoningModule:\n    def __init__(self, knowledge_base: SymbolicKnowledgeBase):\n        self.kb = knowledge_base\n        self.predicate_names = [\n            \"near_goal\", \"obstacle_ahead\", \"low_energy\", \"high_reward_area\",\n            \"safe_position\", \"explored_area\", \"time_pressure\", \"resource_available\"\n        ]\n    def reason(self, neural_predicates: torch.Tensor) -> Dict[str, float]:\n        self.kb.facts.clear()\n        for i, pred_name in enumerate(self.predicate_names):\n            if i < len(neural_predicates):\n                pred = LogicalPredicate(pred_name, [])\n                self.kb.add_fact(pred, float(neural_predicates[i]))\n        inferred_facts = self.kb.forward_chain()\n        return inferred_facts\nclass NeurosymbolicPolicy(nn.Module):\n    def __init__(self, state_dim: int, action_dim: int, predicate_dim: int = 8):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.predicate_dim = predicate_dim\n        self.perception = NeuralPerceptionModule(state_dim, predicate_dim)\n        self.kb = SymbolicKnowledgeBase()\n        self._initialize_domain_knowledge()\n        self.reasoning = SymbolicReasoningModule(self.kb)\n        self.action_net = nn.Sequential(\n            nn.Linear(predicate_dim * 2, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, action_dim)\n        )\n        self.value_net = nn.Sequential(\n            nn.Linear(predicate_dim * 2, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.Linear(32, 1)\n        )\n    def _initialize_domain_knowledge(self):\n        obstacle_pred = LogicalPredicate(\"obstacle_ahead\", [])\n        safe_pred = LogicalPredicate(\"safe_position\", [])\n        avoid_pred = LogicalPredicate(\"avoid_forward\", [])\n        rule1 = LogicalRule(\n            premises=[obstacle_pred, LogicalPredicate(\"safe_position\", [])],\n            conclusion=avoid_pred,\n            operator=LogicalOperator.AND,\n            confidence=0.9\n        )\n        self.kb.add_rule(rule1)\n        near_goal_pred = LogicalPredicate(\"near_goal\", [])\n        high_reward_pred = LogicalPredicate(\"high_reward_area\", [])\n        approach_pred = LogicalPredicate(\"approach_goal\", [])\n        rule2 = LogicalRule(\n            premises=[near_goal_pred, high_reward_pred],\n            conclusion=approach_pred,\n            operator=LogicalOperator.AND,\n            confidence=0.95\n        )\n        self.kb.add_rule(rule2)\n        low_energy_pred = LogicalPredicate(\"low_energy\", [])\n        resource_pred = LogicalPredicate(\"resource_available\", [])\n        collect_pred = LogicalPredicate(\"collect_resource\", [])\n        rule3 = LogicalRule(\n            premises=[low_energy_pred, resource_pred],\n            conclusion=collect_pred,\n            operator=LogicalOperator.AND,\n            confidence=0.85\n        )\n        self.kb.add_rule(rule3)\n        time_pred = LogicalPredicate(\"time_pressure\", [])\n        explored_pred = LogicalPredicate(\"explored_area\", [])\n        explore_pred = LogicalPredicate(\"explore_quickly\", [])\n        rule4 = LogicalRule(\n            premises=[time_pred, explored_pred],\n            conclusion=explore_pred,\n            operator=LogicalOperator.AND,\n            confidence=0.8\n        )\n        self.kb.add_rule(rule4)\n    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n        batch_size = state.shape[0]\n        neural_predicates, attention_weights = self.perception(state)\n        symbolic_features_list = []\n        explanations = {\"neural_predicates\": [], \"symbolic_inferences\": [], \"explanations\": []}\n        for i in range(batch_size):\n            symbolic_facts = self.reasoning.reason(neural_predicates[i])\n            symbolic_features = torch.zeros(self.predicate_dim)\n            for j, pred_name in enumerate(self.reasoning.predicate_names):\n                if j < self.predicate_dim:\n                    symbolic_features[j] = symbolic_facts.get(pred_name, 0.0)\n            symbolic_features_list.append(symbolic_features)\n            explanations[\"neural_predicates\"].append(neural_predicates[i].detach())\n            explanations[\"symbolic_inferences\"].append(symbolic_features)\n            sample_explanations = []\n            for fact_name, truth_value in symbolic_facts.items():\n                if truth_value > 0.5:\n                    fact_explanations = self.kb.explain_decision(fact_name)\n                    sample_explanations.extend(fact_explanations)\n            explanations[\"explanations\"].append(sample_explanations)\n        symbolic_features = torch.stack(symbolic_features_list).to(state.device)\n        combined_features = torch.cat([neural_predicates, symbolic_features], dim=1)\n        action_logits = self.action_net(combined_features)\n        values = self.value_net(combined_features)\n        explanations[\"attention_weights\"] = attention_weights\n        return action_logits, values, explanations\n    def get_action(self, state: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, Dict]:\n        action_logits, values, explanations = self.forward(state)\n        if deterministic:\n            actions = torch.argmax(action_logits, dim=-1)\n        else:\n            action_dist = torch.distributions.Categorical(logits=action_logits)\n            actions = action_dist.sample()\n        return actions, explanations\nclass NeurosymbolicAgent:\n    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4):\n        self.policy = NeurosymbolicPolicy(state_dim, action_dim)\n        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n        self.training_history = {\n            'rewards': [],\n            'losses': [],\n            'explanations': []\n        }\n    def train_step(self, states: torch.Tensor, actions: torch.Tensor, \n                   rewards: torch.Tensor, next_states: torch.Tensor, \n                   dones: torch.Tensor) -> Dict[str, float]:\n        action_logits, values, explanations = self.policy(states)\n        next_action_logits, next_values, _ = self.policy(next_states)\n        with torch.no_grad():\n            targets = rewards + 0.99 * next_values.squeeze() * (1 - dones.float())\n            advantages = targets - values.squeeze()\n        action_dist = torch.distributions.Categorical(logits=action_logits)\n        log_probs = action_dist.log_prob(actions)\n        policy_loss = -(log_probs * advantages.detach()).mean()\n        value_loss = F.mse_loss(values.squeeze(), targets.detach())\n        entropy = action_dist.entropy().mean()\n        entropy_bonus = 0.01 * entropy\n        total_loss = policy_loss + 0.5 * value_loss - entropy_bonus\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n        self.optimizer.step()\n        train_info = {\n            'total_loss': total_loss.item(),\n            'policy_loss': policy_loss.item(),\n            'value_loss': value_loss.item(),\n            'entropy': entropy.item(),\n            'avg_value': values.mean().item(),\n            'explanations': explanations\n        }\n        self.training_history['losses'].append(total_loss.item())\n        return train_info\nprint(\"âœ… Neurosymbolic RL classes implemented successfully!\")\nprint(\"Components: LogicalPredicate, LogicalRule, SymbolicKnowledgeBase\")\nprint(\"Neural modules: NeuralPerceptionModule, SymbolicReasoningModule\") \nprint(\"Policy: NeurosymbolicPolicy with interpretable reasoning\")\nprint(\"Agent: NeurosymbolicAgent with training capabilities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2703f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\nfrom gymnasium import spaces\nimport random\nfrom typing import Tuple, List\nimport seaborn as sns\nclass SymbolicGridWorld(gym.Env):\n    def __init__(self, size=8):\n        super().__init__()\n        self.size = size\n        self.agent_pos = [0, 0]\n        self.goal_pos = [size-1, size-1]\n        self.obstacles = set()\n        for _ in range(size // 2):\n            x, y = random.randint(1, size-2), random.randint(1, size-2)\n            if [x, y] != self.goal_pos:\n                self.obstacles.add((x, y))\n        self.resources = set()\n        for _ in range(size // 3):\n            x, y = random.randint(0, size-1), random.randint(0, size-1)\n            if [x, y] != self.goal_pos and (x, y) not in self.obstacles:\n                self.resources.add((x, y))\n        self.energy = 10\n        self.max_energy = 10\n        self.time_step = 0\n        self.max_time = size * size\n        self.collected_resources = set()\n        self.visited_positions = set()\n        self.action_space = spaces.Discrete(4)\n        self.observation_space = spaces.Box(\n            low=0, high=1, shape=(12,), dtype=np.float32\n        )\n        self.actions = {\n            0: [-1, 0],\n            1: [1, 0],\n            2: [0, -1],\n            3: [0, 1]\n        }\n    def reset(self, seed=None):\n        super().reset(seed=seed)\n        self.agent_pos = [0, 0]\n        self.energy = self.max_energy\n        self.time_step = 0\n        self.collected_resources = set()\n        self.visited_positions = {tuple(self.agent_pos)}\n        return self._get_observation(), {}\n    def step(self, action):\n        old_pos = self.agent_pos.copy()\n        new_pos = [\n            self.agent_pos[0] + self.actions[action][0],\n            self.agent_pos[1] + self.actions[action][1]\n        ]\n        if 0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size:\n            if tuple(new_pos) not in self.obstacles:\n                self.agent_pos = new_pos\n                self.energy -= 1\n        self.time_step += 1\n        self.visited_positions.add(tuple(self.agent_pos))\n        if tuple(self.agent_pos) in self.resources and tuple(self.agent_pos) not in self.collected_resources:\n            self.collected_resources.add(tuple(self.agent_pos))\n            self.energy = min(self.max_energy, self.energy + 3)\n        reward = self._calculate_reward()\n        terminated = (self.agent_pos == self.goal_pos or \n                     self.energy <= 0 or \n                     self.time_step >= self.max_time)\n        return self._get_observation(), reward, terminated, False, {}\n    def _calculate_reward(self):\n        reward = 0\n        if self.agent_pos == self.goal_pos:\n            reward += 100\n        goal_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n        reward -= goal_dist * 0.1\n        if self.energy <= 0:\n            reward -= 50\n        if tuple(self.agent_pos) in self.resources and tuple(self.agent_pos) not in self.collected_resources:\n            reward += 10\n        if tuple(self.agent_pos) not in self.visited_positions:\n            reward += 1\n        reward -= 0.01\n        return reward\n    def _get_observation(self):\n        obs = np.zeros(12, dtype=np.float32)\n        obs[0] = self.agent_pos[0] / self.size\n        obs[1] = self.agent_pos[1] / self.size\n        obs[2] = self._near_goal()\n        obs[3] = self._obstacle_ahead()\n        obs[4] = self._low_energy()\n        obs[5] = self._high_reward_area()\n        obs[6] = self._safe_position()\n        obs[7] = self._explored_area()\n        obs[8] = self._time_pressure()\n        obs[9] = self._resource_available()\n        obs[10] = self.energy / self.max_energy\n        obs[11] = self.time_step / self.max_time\n        return obs\n    def _near_goal(self) -> float:\n        dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n        return max(0, 1.0 - dist / (2 * self.size))\n    def _obstacle_ahead(self) -> float:\n        for action in range(4):\n            new_pos = [\n                self.agent_pos[0] + self.actions[action][0],\n                self.agent_pos[1] + self.actions[action][1]\n            ]\n            if (0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and \n                tuple(new_pos) in self.obstacles):\n                return 1.0\n        return 0.0\n    def _low_energy(self) -> float:\n        return max(0, 1.0 - self.energy / (self.max_energy * 0.3))\n    def _high_reward_area(self) -> float:\n        goal_reward = self._near_goal()\n        resource_reward = 0.0\n        for resource in self.resources:\n            dist = abs(self.agent_pos[0] - resource[0]) + abs(self.agent_pos[1] - resource[1])\n            resource_reward = max(resource_reward, max(0, 1.0 - dist / 3))\n        return max(goal_reward, resource_reward)\n    def _safe_position(self) -> float:\n        min_dist = float('inf')\n        for obstacle in self.obstacles:\n            dist = abs(self.agent_pos[0] - obstacle[0]) + abs(self.agent_pos[1] - obstacle[1])\n            min_dist = min(min_dist, dist)\n        if min_dist == float('inf'):\n            return 1.0\n        return min(1.0, min_dist / 3)\n    def _explored_area(self) -> float:\n        return 1.0 if tuple(self.agent_pos) in self.visited_positions else 0.0\n    def _time_pressure(self) -> float:\n        return max(0, (self.time_step - self.max_time * 0.7) / (self.max_time * 0.3))\n    def _resource_available(self) -> float:\n        return 1.0 if (tuple(self.agent_pos) in self.resources and \n                      tuple(self.agent_pos) not in self.collected_resources) else 0.0\n    def render(self, mode='human'):\n        grid = np.zeros((self.size, self.size))\n        for obs in self.obstacles:\n            grid[obs[0], obs[1]] = -1\n        for res in self.resources:\n            if res not in self.collected_resources:\n                grid[res[0], res[1]] = 0.5\n        for res in self.collected_resources:\n            grid[res[0], res[1]] = 0.3\n        for pos in self.visited_positions:\n            if grid[pos[0], pos[1]] == 0:\n                grid[pos[0], pos[1]] = 0.1\n        grid[self.goal_pos[0], self.goal_pos[1]] = 2\n        grid[self.agent_pos[0], self.agent_pos[1]] = 1\n        plt.figure(figsize=(8, 8))\n        plt.imshow(grid, cmap='RdYlBu', vmin=-1, vmax=2)\n        plt.colorbar(label='Cell Type')\n        plt.title(f'Neurosymbolic GridWorld (Step: {self.time_step}, Energy: {self.energy})')\n        legend_elements = [\n            plt.Rectangle((0,0),1,1, facecolor='darkred', label='Obstacle'),\n            plt.Rectangle((0,0),1,1, facecolor='orange', label='Resource'),\n            plt.Rectangle((0,0),1,1, facecolor='yellow', label='Collected Resource'),\n            plt.Rectangle((0,0),1,1, facecolor='lightblue', label='Visited'),\n            plt.Rectangle((0,0),1,1, facecolor='blue', label='Goal'),\n            plt.Rectangle((0,0),1,1, facecolor='red', label='Agent')\n        ]\n        plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n        plt.grid(True, alpha=0.3)\n        plt.show()\ndef train_neurosymbolic_agent(env, agent, episodes=1000, render_every=200):\n    episode_rewards = []\n    episode_explanations = []\n    for episode in range(episodes):\n        state, _ = env.reset()\n        episode_reward = 0\n        episode_explanation_log = []\n        done = False\n        while not done:\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n            action, explanations = agent.policy.get_action(state_tensor, deterministic=False)\n            action = action.item()\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            episode_reward += reward\n            if explanations['explanations'][0]:\n                episode_explanation_log.append({\n                    'step': env.time_step,\n                    'state': state.copy(),\n                    'action': action,\n                    'reward': reward,\n                    'explanations': explanations['explanations'][0].copy(),\n                    'neural_predicates': explanations['neural_predicates'][0].numpy().copy(),\n                    'symbolic_inferences': explanations['symbolic_inferences'][0].numpy().copy()\n                })\n            state = next_state\n        episode_rewards.append(episode_reward)\n        episode_explanations.append(episode_explanation_log)\n        if episode % render_every == 0:\n            print(f\\\"Episode {episode}: Reward = {episode_reward:.2f}\\\")\\\n            if episode_explanation_log:\n                print(\\\"Sample explanations:\\\")\n                for exp in episode_explanation_log[:3]:\n                    if exp['explanations']:\n                        print(f\\\"  Step {exp['step']}: {exp['explanations'][0]}\\\"\")\n            print()\n        if episode > 10 and episode % 10 == 0:\n            train_states, train_actions, train_rewards, train_next_states, train_dones = [], [], [], [], []\n            for _ in range(32):\n                state, _ = env.reset()\n                for _ in range(10):\n                    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n                    action, _ = agent.policy.get_action(state_tensor)\n                    action = action.item()\n                    next_state, reward, terminated, truncated, _ = env.step(action)\n                    done = terminated or truncated\n                    train_states.append(state)\n                    train_actions.append(action)\n                    train_rewards.append(reward)\n                    train_next_states.append(next_state)\n                    train_dones.append(done)\n                    if done:\n                        break\n                    state = next_state\n            train_states = torch.FloatTensor(np.array(train_states))\n            train_actions = torch.LongTensor(train_actions)\n            train_rewards = torch.FloatTensor(train_rewards)\n            train_next_states = torch.FloatTensor(np.array(train_next_states))\n            train_dones = torch.BoolTensor(train_dones)\n            train_info = agent.train_step(train_states, train_actions, train_rewards, train_next_states, train_dones)\n            agent.training_history['rewards'].append(np.mean(episode_rewards[-10:]))\n    return episode_rewards, episode_explanations\nprint(\"Creating Symbolic GridWorld Environment...\")\nenv = SymbolicGridWorld(size=6)\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nprint(f\"Environment created with state_dim={state_dim}, action_dim={action_dim}\")\nprint(\"Creating Neurosymbolic Agent...\")\nagent = NeurosymbolicAgent(state_dim, action_dim, lr=1e-3)\nprint(\"âœ… Environment and Agent ready!\")\nprint(\"Next: Run training to see neurosymbolic reasoning in action\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c7bdf",
   "metadata": {},
   "source": [
    "# Section 3: Human-AI Collaborative Learning\n",
    "\n",
    "Human-AI collaborative learning represents a paradigm where AI agents learn not just from environment interaction, but also from human guidance, feedback, and collaboration to achieve superhuman performance.\n",
    "\n",
    "## 3.1 Theoretical Foundations\n",
    "\n",
    "### The Human-AI Collaboration Paradigm\n",
    "\n",
    "Traditional RL assumes agents learn independently from environment feedback. **Human-AI Collaborative Learning** extends this by incorporating human intelligence:\n",
    "\n",
    "- **Human Expertise Integration**: Leverage human domain knowledge and intuition\n",
    "- **Interactive Learning**: Real-time human feedback during agent training\n",
    "- **Shared Control**: Dynamic handoff between human and AI decision-making\n",
    "- **Explanatory AI**: AI explains decisions to humans for better collaboration\n",
    "\n",
    "### Learning from Human Feedback (RLHF)\n",
    "\n",
    "**Preference-Based Learning**:\n",
    "Instead of engineering reward functions, learn from human preferences:\n",
    "\n",
    "$$r_{\\theta}(s, a) = \\text{RewardModel}_{\\theta}(s, a)$$\n",
    "\n",
    "Where the reward model is trained on human preference data:\n",
    "$$\\mathcal{D} = \\{(s_i, a_i^1, a_i^2, y_i)\\}$$\n",
    "\n",
    "Where $y_i \\in \\{0, 1\\}$ indicates whether human prefers action $a_i^1$ over $a_i^2$ in state $s_i$.\n",
    "\n",
    "**Bradley-Terry Model** for preferences:\n",
    "$$P(a^1 \\succ a^2 | s) = \\frac{\\exp(r_{\\theta}(s, a^1))}{\\exp(r_{\\theta}(s, a^1)) + \\exp(r_{\\theta}(s, a^2))}$$\n",
    "\n",
    "**Training Objective**:\n",
    "$$\\mathcal{L}(\\theta) = -\\mathbb{E}_{(s,a^1,a^2,y) \\sim \\mathcal{D}}[y \\log P(a^1 \\succ a^2 | s) + (1-y) \\log P(a^2 \\succ a^1 | s)]$$\n",
    "\n",
    "### Interactive Imitation Learning\n",
    "\n",
    "**DAgger (Dataset Aggregation)**:\n",
    "Iteratively collect expert demonstrations on learned policy trajectories:\n",
    "\n",
    "1. Train policy $\\pi_i$ on current dataset $\\mathcal{D}_i$\n",
    "2. Execute $\\pi_i$ to collect states $\\{s_t\\}$\n",
    "3. Query expert for optimal actions $\\{a_t^*\\}$ on $\\{s_t\\}$\n",
    "4. Aggregate: $\\mathcal{D}_{i+1} = \\mathcal{D}_i \\cup \\{(s_t, a_t^*)\\}$\n",
    "\n",
    "**SMILe (Safe Multi-agent Imitation Learning)**:\n",
    "Learn from multiple human experts with safety constraints:\n",
    "$$\\pi^* = \\arg\\min_\\pi \\sum_i w_i \\mathcal{L}_{\\text{imitation}}(\\pi, \\pi_i^{\\text{expert}}) + \\lambda \\mathcal{L}_{\\text{safety}}(\\pi)$$\n",
    "\n",
    "### Shared Autonomy and Control\n",
    "\n",
    "**Arbitration Between Human and AI**:\n",
    "Dynamic switching between human and AI control:\n",
    "\n",
    "$$a_t = \\begin{cases}\n",
    "a_t^{\\text{human}} & \\text{if } \\alpha_t > \\tau \\\\\n",
    "a_t^{\\text{AI}} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $\\alpha_t$ represents human authority level at time $t$.\n",
    "\n",
    "**Confidence-Based Handoff**:\n",
    "$$\\alpha_t = f(\\text{confidence}_{\\text{AI}}(s_t), \\text{urgency}(s_t), \\text{human\\_availability}(t))$$\n",
    "\n",
    "**Blended Control**:\n",
    "Combine human and AI actions based on context:\n",
    "$$a_t = w_t \\cdot a_t^{\\text{human}} + (1 - w_t) \\cdot a_t^{\\text{AI}}$$\n",
    "\n",
    "### Trust and Calibration\n",
    "\n",
    "**Trust Modeling**:\n",
    "Model human trust in AI decisions:\n",
    "$$T_{t+1} = T_t + \\alpha \\cdot (\\text{outcome}_t - T_t) \\cdot \\text{surprise}_t$$\n",
    "\n",
    "Where:\n",
    "- $T_t$: Trust level at time $t$\n",
    "- $\\text{outcome}_t$: Actual performance outcome\n",
    "- $\\text{surprise}_t$: Difference between expected and actual outcome\n",
    "\n",
    "**Calibrated Confidence**:\n",
    "Ensure AI confidence matches actual performance:\n",
    "$$\\text{Calibration Error} = \\mathbb{E}[|\\text{Confidence} - \\text{Accuracy}|]$$\n",
    "\n",
    "**Trust-Aware Policy**:\n",
    "Modify policy to maintain appropriate human trust:\n",
    "$$\\pi_{\\text{trust}}(a|s) = \\pi(a|s) \\cdot f_{\\text{trust}}(a, s, T_t)$$\n",
    "\n",
    "## 3.2 Human Feedback Integration Methods\n",
    "\n",
    "### Critiquing and Advice\n",
    "Allow humans to provide structured feedback:\n",
    "\n",
    "**Action Critiquing**:\n",
    "- Human observes AI action and provides feedback\n",
    "- Types: \"Good action\", \"Bad action\", \"Better action would be...\"\n",
    "- Update policy based on critique\n",
    "\n",
    "**State-Action Advice**:\n",
    "$$\\mathcal{L}_{\\text{advice}} = -\\log \\pi(a_{\\text{advised}} | s) \\cdot w_{\\text{confidence}}$$\n",
    "\n",
    "### Demonstration and Intervention\n",
    "\n",
    "**Human Demonstrations**:\n",
    "- Collect expert trajectories: $\\tau_{\\text{expert}} = \\{(s_0, a_0), (s_1, a_1), \\ldots\\}$\n",
    "- Learn via behavioral cloning or inverse RL\n",
    "- Active learning: query human on uncertain states\n",
    "\n",
    "**Intervention Learning**:\n",
    "- Human takes control when AI makes mistakes\n",
    "- Learn from intervention patterns\n",
    "- Identify failure modes and correction strategies\n",
    "\n",
    "### Preference Learning and Ranking\n",
    "\n",
    "**Pairwise Preferences**:\n",
    "Show human two action sequences and ask for preference\n",
    "$$\\mathcal{P} = \\{(\\tau_1, \\tau_2, \\text{preference})\\}$$\n",
    "\n",
    "**Trajectory Ranking**:\n",
    "Rank multiple trajectories by performance\n",
    "$$\\tau_1 \\succ \\tau_2 \\succ \\ldots \\succ \\tau_k$$\n",
    "\n",
    "**Active Preference Learning**:\n",
    "Intelligently select which comparisons to show human:\n",
    "$$\\text{query}^* = \\arg\\max_{\\text{query}} \\text{InformationGain}(\\text{query})$$\n",
    "\n",
    "## 3.3 Collaborative Decision Making\n",
    "\n",
    "### Shared Mental Models\n",
    "Align human and AI understanding of the task:\n",
    "\n",
    "**Common Ground**:\n",
    "- Shared representation of environment\n",
    "- Agreed-upon goal decomposition  \n",
    "- Common terminology and concepts\n",
    "\n",
    "**Theory of Mind**:\n",
    "AI models human beliefs, intentions, and capabilities:\n",
    "$$\\text{AI\\_Model}(\\text{human\\_belief}(s_t), \\text{human\\_goal}, \\text{human\\_capability})$$\n",
    "\n",
    "### Communication Protocols\n",
    "\n",
    "**Natural Language Interface**:\n",
    "- AI explains decisions in natural language\n",
    "- Human provides feedback via natural language\n",
    "- Bidirectional communication for coordination\n",
    "\n",
    "**Multimodal Communication**:\n",
    "- Visual indicators (attention, confidence)\n",
    "- Gestural input from humans\n",
    "- Audio feedback and alerts\n",
    "\n",
    "### Coordination Strategies\n",
    "\n",
    "**Task Allocation**:\n",
    "Divide tasks based on comparative advantage:\n",
    "$$\\text{Assign}(T_i) = \\begin{cases}\n",
    "\\text{Human} & \\text{if } \\text{Advantage}_{\\text{human}}(T_i) > \\text{Advantage}_{\\text{AI}}(T_i) \\\\\n",
    "\\text{AI} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Dynamic Role Assignment**:\n",
    "Roles change based on context, performance, and availability:\n",
    "- **Leader-Follower**: One party leads, other assists\n",
    "- **Peer Collaboration**: Equal partnership with negotiation\n",
    "- **Hierarchical**: Clear command structure with delegation\n",
    "\n",
    "## 3.4 Advanced Collaborative Learning Paradigms\n",
    "\n",
    "### Constitutional AI\n",
    "Train AI systems to follow high-level principles:\n",
    "\n",
    "1. **Constitutional Training**: Define principles in natural language\n",
    "2. **Self-Critiquing**: AI evaluates its own responses against principles\n",
    "3. **Iterative Refinement**: Improve responses based on principle violations\n",
    "\n",
    "**Constitutional Loss**:\n",
    "$$\\mathcal{L}_{\\text{constitutional}} = \\mathcal{L}_{\\text{task}} + \\lambda \\sum_i \\text{Violation}(\\text{principle}_i)$$\n",
    "\n",
    "### Cooperative Inverse Reinforcement Learning (Co-IRL)\n",
    "Learn shared reward functions through interaction:\n",
    "\n",
    "$$R^* = \\arg\\max_R \\log P(\\tau_{\\text{human}} | R) + \\log P(\\tau_{\\text{AI}} | R) + \\text{Cooperation}(R)$$\n",
    "\n",
    "### Multi-Agent Human-AI Teams\n",
    "Extend collaboration to multi-agent settings:\n",
    "\n",
    "**Team Formation**:\n",
    "- Optimal team composition (humans + AI agents)\n",
    "- Role specialization and capability matching\n",
    "- Communication network topology\n",
    "\n",
    "**Collective Intelligence**:\n",
    "$$\\text{Team\\_Performance} > \\max(\\text{Individual\\_Performance})$$\n",
    "\n",
    "### Continual Human-AI Co-Evolution\n",
    "Humans and AI systems improve together over time:\n",
    "\n",
    "**Co-Adaptation**:\n",
    "- AI adapts to human preferences and style\n",
    "- Humans develop better collaboration skills with AI\n",
    "- Mutual model updates and learning\n",
    "\n",
    "**Lifelong Collaboration**:\n",
    "- Maintain collaboration quality over extended periods\n",
    "- Handle changes in human capabilities and preferences\n",
    "- Evolve communication and coordination protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161eb87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional, Callable\nfrom dataclasses import dataclass\nfrom collections import deque\nimport matplotlib.pyplot as plt\nimport random\n@dataclass\nclass HumanPreference:\n    state: np.ndarray\n    action1: int\n    action2: int \n    preference: int\n    confidence: float = 1.0\n    timestamp: float = 0.0\n@dataclass \nclass HumanFeedback:\n    feedback_type: str\n    content: any\n    confidence: float = 1.0\n    context: Dict = None\nclass PreferenceRewardModel(nn.Module):\n    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n        super().__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.encoder = nn.Sequential(\n            nn.Linear(state_dim + 1, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, 1)\n        )\n        self.confidence_net = nn.Sequential(\n            nn.Linear(state_dim + 1, hidden_dim // 2),\n            nn.ReLU(),\n            nn.Linear(hidden_dim // 2, 1),\n            nn.Sigmoid()\n        )\n    def forward(self, states: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n        actions_normalized = actions.float().unsqueeze(1) / self.action_dim\n        state_action = torch.cat([states, actions_normalized], dim=1)\n        rewards = self.encoder(state_action).squeeze(-1)\n        confidences = self.confidence_net(state_action).squeeze(-1)\n        return rewards, confidences\n    def preference_probability(self, state: torch.Tensor, action1: torch.Tensor, action2: torch.Tensor) -> torch.Tensor:\n        reward1, conf1 = self.forward(state, action1)\n        reward2, conf2 = self.forward(state, action2)\n        prob = torch.sigmoid(reward1 - reward2)\n        return prob, (conf1 + conf2) / 2\nclass HumanFeedbackCollector:\n    def __init__(self, true_reward_fn: Optional[Callable] = None):\n        self.preferences: List[HumanPreference] = []\n        self.feedback_history: List[HumanFeedback] = []\n        self.true_reward_fn = true_reward_fn\n        self.noise_level = 0.1\n    def collect_preference(self, state: np.ndarray, action1: int, action2: int, \n                          use_true_reward: bool = True) -> HumanPreference:\n        if use_true_reward and self.true_reward_fn is not None:\n            reward1 = self.true_reward_fn(state, action1)\n            reward2 = self.true_reward_fn(state, action2)\n            reward1 += np.random.normal(0, self.noise_level)\n            reward2 += np.random.normal(0, self.noise_level)\n            preference = 0 if reward1 > reward2 else 1\n            confidence = min(1.0, max(0.1, abs(reward1 - reward2)))\n        else:\n            preference = random.choice([0, 1])\n            confidence = random.uniform(0.5, 1.0)\n        pref = HumanPreference(\n            state=state,\n            action1=action1,\n            action2=action2,\n            preference=preference,\n            confidence=confidence\n        )\n        self.preferences.append(pref)\n        return pref\n    def collect_critique(self, state: np.ndarray, action: int, ai_reward: float) -> HumanFeedback:\n        if self.true_reward_fn is not None:\n            true_reward = self.true_reward_fn(state, action)\n            reward_diff = true_reward - ai_reward\n            if reward_diff > 0.5:\n                critique = \"good_action\"\n                confidence = min(1.0, reward_diff)\n            elif reward_diff < -0.5:\n                critique = \"bad_action\"\n                confidence = min(1.0, abs(reward_diff))\n            else:\n                critique = \"neutral\"\n                confidence = 0.5\n        else:\n            critique = random.choice([\"good_action\", \"bad_action\", \"neutral\"])\n            confidence = random.uniform(0.3, 1.0)\n        feedback = HumanFeedback(\n            feedback_type=\"critique\",\n            content=critique,\n            confidence=confidence,\n            context={\"state\": state, \"action\": action, \"ai_reward\": ai_reward}\n        )\n        self.feedback_history.append(feedback)\n        return feedback\n    def get_preference_dataset(self) -> List[HumanPreference]:\n        return self.preferences\n    def clear_history(self):\n        self.preferences.clear()\n        self.feedback_history.clear()\nclass CollaborativeAgent:\n    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.policy = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim)\n        )\n        self.value_net = nn.Sequential(\n            nn.Linear(state_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n        self.reward_model = PreferenceRewardModel(state_dim, action_dim)\n        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=lr)\n        self.reward_optimizer = torch.optim.Adam(self.reward_model.parameters(), lr=lr)\n        self.human_trust = 0.8\n        self.ai_confidence_history = deque(maxlen=100)\n        self.collaboration_history = []\n    def get_action(self, state: torch.Tensor, use_learned_reward: bool = True) -> Tuple[int, Dict]:\n        with torch.no_grad():\n            logits = self.policy(state)\n            action_probs = F.softmax(logits, dim=-1)\n            action_dist = torch.distributions.Categorical(action_probs)\n            action = action_dist.sample().item()\n            if use_learned_reward:\n                reward, confidence = self.reward_model(state.unsqueeze(0), torch.tensor([action]))\n                ai_confidence = confidence.item()\n                predicted_reward = reward.item()\n            else:\n                ai_confidence = action_probs.max().item()\n                predicted_reward = None\n            self.ai_confidence_history.append(ai_confidence)\n            intervention_threshold = self._compute_intervention_threshold()\n            should_request_human = ai_confidence < intervention_threshold\n            collab_info = {\n                'action': action,\n                'ai_confidence': ai_confidence,\n                'predicted_reward': predicted_reward,\n                'action_probs': action_probs.numpy(),\n                'should_request_human': should_request_human,\n                'human_trust': self.human_trust,\n                'intervention_threshold': intervention_threshold\n            }\n            return action, collab_info\n    def _compute_intervention_threshold(self) -> float:\n        base_threshold = 0.5\n        trust_adjustment = (1.0 - self.human_trust) * 0.3\n        if len(self.ai_confidence_history) > 10:\n            recent_avg_confidence = np.mean(list(self.ai_confidence_history)[-10:])\n            performance_adjustment = (0.7 - recent_avg_confidence) * 0.2\n        else:\n            performance_adjustment = 0\n        threshold = base_threshold + trust_adjustment + performance_adjustment\n        return np.clip(threshold, 0.2, 0.8)\n    def train_reward_model(self, preferences: List[HumanPreference], epochs: int = 10):\n        if len(preferences) < 2:\n            return\n        total_loss = 0\n        for epoch in range(epochs):\n            epoch_loss = 0\n            random.shuffle(preferences)\n            for pref in preferences:\n                state = torch.FloatTensor(pref.state).unsqueeze(0)\n                action1 = torch.tensor([pref.action1])\n                action2 = torch.tensor([pref.action2])\n                prob, avg_conf = self.reward_model.preference_probability(state, action1, action2)\n                if pref.preference == 0:\n                    loss = -torch.log(prob + 1e-8)\n                else:\n                    loss = -torch.log(1 - prob + 1e-8)\n                loss = loss * pref.confidence\n                self.reward_optimizer.zero_grad()\n                loss.backward()\n                self.reward_optimizer.step()\n                epoch_loss += loss.item()\n            total_loss += epoch_loss\n        return total_loss / (len(preferences) * epochs)\n    def update_trust(self, predicted_outcome: float, actual_outcome: float, surprise_factor: float = 1.0):\n        learning_rate = 0.1\n        prediction_error = actual_outcome - predicted_outcome\n        normalized_error = np.tanh(prediction_error / 2.0)\n        trust_update = learning_rate * normalized_error * surprise_factor\n        self.human_trust = np.clip(self.human_trust + trust_update, 0.0, 1.0)\n        self.collaboration_history.append({\n            'predicted_outcome': predicted_outcome,\n            'actual_outcome': actual_outcome,\n            'prediction_error': prediction_error,\n            'trust_update': trust_update,\n            'new_trust': self.human_trust\n        })\n    def train_policy_from_rewards(self, states: torch.Tensor, actions: torch.Tensor, \n                                 rewards: torch.Tensor, next_states: torch.Tensor, \n                                 dones: torch.Tensor) -> Dict[str, float]:\n        action_logits = self.policy(states)\n        action_dist = torch.distributions.Categorical(logits=action_logits)\n        log_probs = action_dist.log_prob(actions)\n        values = self.value_net(states).squeeze()\n        next_values = self.value_net(next_states).squeeze()\n        with torch.no_grad():\n            targets = rewards + 0.99 * next_values * (1 - dones.float())\n            advantages = targets - values\n        policy_loss = -(log_probs * advantages.detach()).mean()\n        value_loss = F.mse_loss(values, targets.detach())\n        entropy = action_dist.entropy().mean()\n        entropy_bonus = 0.01 * entropy\n        total_loss = policy_loss + 0.5 * value_loss - entropy_bonus\n        self.policy_optimizer.zero_grad()\n        total_loss.backward()\n        self.policy_optimizer.step()\n        value_loss_separate = F.mse_loss(self.value_net(states).squeeze(), targets.detach())\n        self.value_optimizer.zero_grad()\n        value_loss_separate.backward()\n        self.value_optimizer.step()\n        return {\n            'policy_loss': policy_loss.item(),\n            'value_loss': value_loss.item(),\n            'entropy': entropy.item(),\n            'total_loss': total_loss.item()\n        }\n    def get_collaboration_stats(self) -> Dict:\n        if not self.collaboration_history:\n            return {}\n        recent_history = self.collaboration_history[-50:]\n        return {\n            'current_trust': self.human_trust,\n            'avg_prediction_error': np.mean([h['prediction_error'] for h in recent_history]),\n            'trust_volatility': np.std([h['new_trust'] for h in recent_history]),\n            'collaboration_count': len(self.collaboration_history),\n            'recent_performance': np.mean([1 - abs(h['prediction_error']) for h in recent_history])\n        }\nprint(\"âœ… Human-AI Collaborative Learning implementation complete!\")\nprint(\"Components implemented:\")\nprint(\"- PreferenceRewardModel: Learn from human preferences\") \nprint(\"- HumanFeedbackCollector: Simulate human feedback\")\nprint(\"- CollaborativeAgent: Agent that learns from human feedback\")\nprint(\"- Trust modeling and intervention mechanisms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eae2719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\nfrom matplotlib.patches import Rectangle\nimport seaborn as sns\nclass CollaborativeGridWorld(gym.Env):\n    def __init__(self, size=6):\n        super().__init__()\n        self.size = size\n        self.reset()\n        self.action_space = spaces.Discrete(4)\n        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n        self.actions = {0: [-1, 0], 1: [1, 0], 2: [0, -1], 3: [0, 1]}\n        self.true_reward_weights = {\n            'goal_distance': -0.1,\n            'obstacle_penalty': -5.0,\n            'goal_reward': 10.0,\n            'efficiency_bonus': 0.5,\n            'exploration_bonus': 0.1\n        }\n    def reset(self, seed=None):\n        super().reset(seed=seed)\n        self.agent_pos = [0, 0]\n        self.goal_pos = [self.size-1, self.size-1]\n        self.obstacles = set()\n        for i in range(2, 5):\n            self.obstacles.add((i, 2))\n        for j in range(1, 4):\n            self.obstacles.add((2, j))\n        self.obstacles.discard(tuple(self.goal_pos))\n        self.visited_positions = {tuple(self.agent_pos)}\n        self.step_count = 0\n        self.max_steps = self.size * self.size * 2\n        return self._get_observation(), {}\n    def step(self, action):\n        old_pos = self.agent_pos.copy()\n        new_pos = [\n            self.agent_pos[0] + self.actions[action][0],\n            self.agent_pos[1] + self.actions[action][1]\n        ]\n        reward = 0\n        if (0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and \n            tuple(new_pos) not in self.obstacles):\n            self.agent_pos = new_pos\n            old_dist = abs(old_pos[0] - self.goal_pos[0]) + abs(old_pos[1] - self.goal_pos[1])\n            new_dist = abs(new_pos[0] - self.goal_pos[0]) + abs(new_pos[1] - self.goal_pos[1])\n            reward += self.true_reward_weights['goal_distance'] * (new_dist - old_dist)\n            if tuple(new_pos) not in self.visited_positions:\n                reward += self.true_reward_weights['exploration_bonus']\n                self.visited_positions.add(tuple(new_pos))\n        else:\n            reward += self.true_reward_weights['obstacle_penalty']\n        if self.agent_pos == self.goal_pos:\n            reward += self.true_reward_weights['goal_reward']\n            efficiency = max(0, 1 - self.step_count / (self.size * 2))\n            reward += self.true_reward_weights['efficiency_bonus'] * efficiency\n        self.step_count += 1\n        terminated = (self.agent_pos == self.goal_pos or self.step_count >= self.max_steps)\n        return self._get_observation(), reward, terminated, False, {}\n    def _get_observation(self):\n        obs = np.zeros(8, dtype=np.float32)\n        obs[0] = self.agent_pos[0] / self.size\n        obs[1] = self.agent_pos[1] / self.size\n        goal_dx = (self.goal_pos[0] - self.agent_pos[0]) / self.size\n        goal_dy = (self.goal_pos[1] - self.agent_pos[1]) / self.size\n        obs[2] = goal_dx\n        obs[3] = goal_dy\n        goal_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n        obs[4] = goal_dist / (2 * self.size)\n        for i, (dx, dy) in enumerate([[0, 1], [0, -1], [1, 0], [-1, 0]]):\n            next_pos = [self.agent_pos[0] + dx, self.agent_pos[1] + dy]\n            if (next_pos[0] < 0 or next_pos[0] >= self.size or \n                next_pos[1] < 0 or next_pos[1] >= self.size or\n                tuple(next_pos) in self.obstacles):\n                obs[5] = 1.0\n                break\n        obs[6] = self.step_count / self.max_steps\n        obs[7] = len(self.visited_positions) / (self.size * self.size)\n        return obs\n    def true_reward_function(self, state, action):\n        old_pos = self.agent_pos.copy()\n        old_step = self.step_count\n        old_visited = self.visited_positions.copy()\n        obs, reward, done, truncated, _ = self.step(action)\n        self.agent_pos = old_pos\n        self.step_count = old_step\n        self.visited_positions = old_visited\n        return reward\n    def render_collaboration(self, collab_info=None):\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        grid = np.zeros((self.size, self.size))\n        for obs in self.obstacles:\n            grid[obs[0], obs[1]] = -1\n        for pos in self.visited_positions:\n            if grid[pos[0], pos[1]] == 0:\n                grid[pos[0], pos[1]] = 0.3\n        grid[self.goal_pos[0], self.goal_pos[1]] = 2\n        grid[self.agent_pos[0], self.agent_pos[1]] = 1\n        im = ax1.imshow(grid, cmap='RdYlBu', vmin=-1, vmax=2)\n        ax1.set_title(f'Collaborative GridWorld (Step: {self.step_count})')\n        ax1.grid(True, alpha=0.3)\n        legend_elements = [\n            Rectangle((0,0),1,1, facecolor='darkred', label='Obstacle'),\n            Rectangle((0,0),1,1, facecolor='lightblue', label='Visited'),\n            Rectangle((0,0),1,1, facecolor='blue', label='Goal'),\n            Rectangle((0,0),1,1, facecolor='red', label='Agent')\n        ]\n        ax1.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n        if collab_info:\n            ax2.axis('off')\n            info_text = f\"\"\"Collaboration Status:\nAI Confidence: {collab_info['ai_confidence']:.3f}\nHuman Trust: {collab_info['human_trust']:.3f}\nIntervention Threshold: {collab_info['intervention_threshold']:.3f}\nRequest Human Help: {collab_info['should_request_human']}\nAction Probabilities:\n    episode_rewards = []\n    trust_history = []\n    collaboration_events = []\n    print(\"Starting Collaborative Training...\")\n    print(f\"Episodes: {episodes}, Feedback every: {feedback_frequency} episodes\")\n    for episode in range(episodes):\n        state, _ = env.reset()\n        episode_reward = 0\n        episode_steps = []\n        done = False\n        while not done:\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n            action, collab_info = agent.get_action(state_tensor, use_learned_reward=True)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            episode_reward += reward\n            episode_steps.append({\n                'state': state.copy(),\n                'action': action,\n                'reward': reward,\n                'next_state': next_state.copy(),\n                'done': done,\n                'collab_info': collab_info\n            })\n            if collab_info['predicted_reward'] is not None:\n                agent.update_trust(\n                    predicted_outcome=collab_info['predicted_reward'],\n                    actual_outcome=reward,\n                    surprise_factor=1.0 - collab_info['ai_confidence']\n                )\n            state = next_state\n        episode_rewards.append(episode_reward)\n        trust_history.append(agent.human_trust)\n        if episode % feedback_frequency == 0 and episode > 0:\n            print(f\"\\n--- Episode {episode}: Collecting Human Feedback ---\")\n            feedback_count = 0\n            for step_data in episode_steps[-10:]:\n                if random.random() < 0.3:\n                    available_actions = list(range(env.action_space.n))\n                    if step_data['action'] in available_actions:\n                        available_actions.remove(step_data['action'])\n                    if available_actions:\n                        alt_action = random.choice(available_actions)\n                        pref = feedback_collector.collect_preference(\n                            state=step_data['state'],\n                            action1=step_data['action'],\n                            action2=alt_action,\n                            use_true_reward=True\n                        )\n                        feedback_count += 1\n                        critique = feedback_collector.collect_critique(\n                            state=step_data['state'],\n                            action=step_data['action'],\n                            ai_reward=step_data['collab_info'].get('predicted_reward', 0)\n                        )\n            print(f\"Collected {feedback_count} preference comparisons\")\n            if len(feedback_collector.preferences) > 5:\n                reward_loss = agent.train_reward_model(\n                    feedback_collector.preferences,\n                    epochs=5\n                )\n                print(f\"Reward model loss: {reward_loss:.4f}\")\n            if len(episode_steps) > 10:\n                states = torch.FloatTensor([step['state'] for step in episode_steps])\n                actions = torch.LongTensor([step['action'] for step in episode_steps])\n                next_states = torch.FloatTensor([step['next_state'] for step in episode_steps])\n                dones = torch.BoolTensor([step['done'] for step in episode_steps])\n                with torch.no_grad():\n                    learned_rewards, _ = agent.reward_model(states, actions)\n                policy_stats = agent.train_policy_from_rewards(\n                    states, actions, learned_rewards, next_states, dones\n                )\n                print(f\"Policy loss: {policy_stats['policy_loss']:.4f}\")\n            collaboration_events.append({\n                'episode': episode,\n                'reward': episode_reward,\n                'trust': agent.human_trust,\n                'feedback_count': feedback_count,\n                'total_preferences': len(feedback_collector.preferences)\n            })\n        if episode % render_frequency == 0 and episode > 0:\n            print(f\"\\nEpisode {episode}:\")\n            print(f\"  Reward: {episode_reward:.2f}\")\n            print(f\"  Human Trust: {agent.human_trust:.3f}\")\n            print(f\"  Total Preferences Collected: {len(feedback_collector.preferences)}\")\n            collab_stats = agent.get_collaboration_stats()\n            if collab_stats:\n                print(f\"  Avg Prediction Error: {collab_stats['avg_prediction_error']:.3f}\")\n                print(f\"  Recent Performance: {collab_stats['recent_performance']:.3f}\")\n    return {\n        'episode_rewards': episode_rewards,\n        'trust_history': trust_history,\n        'collaboration_events': collaboration_events,\n        'final_preferences': feedback_collector.preferences\n    }\ndef setup_collaborative_experiment():\n    print(\"Setting up Collaborative Learning Experiment...\")\n    env = CollaborativeGridWorld(size=6)\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    agent = CollaborativeAgent(state_dim, action_dim, lr=1e-3)\n    feedback_collector = HumanFeedbackCollector(\n        true_reward_fn=lambda state, action: env.true_reward_function(state, action)\n    )\n    print(f\"Environment: {state_dim}D state, {action_dim} actions\")\n    print(f\"Agent: CollaborativeAgent with preference learning\")\n    print(f\"Feedback: Simulated human with {feedback_collector.noise_level} noise level\")\n    return env, agent, feedback_collector\nenv, agent, feedback_collector = setup_collaborative_experiment()\nprint(\"\\nðŸ¤– Testing single episode with collaboration...\")\nstate, _ = env.reset()\naction, collab_info = agent.get_action(torch.FloatTensor(state).unsqueeze(0))\nprint(f\"AI Action: {action}, Confidence: {collab_info['ai_confidence']:.3f}\")\nprint(f\"Should request human help: {collab_info['should_request_human']}\")\nprint(\"\\nâœ… Collaborative Learning setup complete!\")\nprint(\"Ready to run: train_collaborative_agent(env, agent, feedback_collector)\")\nprint(\"This will train the agent with simulated human feedback\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493b884",
   "metadata": {},
   "source": [
    "# Section 4: Foundation Models in Reinforcement Learning\n",
    "\n",
    "Foundation models represent a paradigm shift in RL, leveraging pre-trained large models to achieve sample-efficient learning and strong generalization across diverse tasks and domains.\n",
    "\n",
    "## 4.1 Theoretical Foundations\n",
    "\n",
    "### The Foundation Model Paradigm in RL\n",
    "\n",
    "**Traditional RL Limitations**:\n",
    "- **Sample Inefficiency**: Learning from scratch on each task\n",
    "- **Poor Generalization**: Overfitting to specific environments\n",
    "- **Limited Transfer**: Difficulty sharing knowledge across domains\n",
    "- **Representation Learning**: Learning both policy and representations simultaneously\n",
    "\n",
    "**Foundation Model Advantages**:\n",
    "- **Pre-trained Representations**: Rich features learned from large datasets\n",
    "- **Few-Shot Learning**: Rapid adaptation to new tasks with minimal data\n",
    "- **Cross-Domain Transfer**: Knowledge sharing across different environments\n",
    "- **Compositional Reasoning**: Understanding of complex task structures\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "**Foundation Model as Universal Approximator**:\n",
    "$$f_{\\theta}: \\mathcal{X} \\rightarrow \\mathcal{Z}$$\n",
    "\n",
    "Where $\\mathcal{X}$ is input space (observations, language, etc.) and $\\mathcal{Z}$ is latent representation space.\n",
    "\n",
    "**Task-Specific Adaptation**:\n",
    "$$\\pi_{\\phi}^{(i)}(a|s) = g_{\\phi}(f_{\\theta}(s), \\text{context}_i)$$\n",
    "\n",
    "Where $g_{\\phi}$ is a task-specific head and $\\text{context}_i$ provides task information.\n",
    "\n",
    "**Multi-Task Objective**:\n",
    "$$\\mathcal{L} = \\sum_{i=1}^{T} w_i \\mathcal{L}_i(\\pi_{\\phi}^{(i)}) + \\lambda \\mathcal{L}_{\\text{reg}}(\\theta, \\phi)$$\n",
    "\n",
    "Where $T$ is number of tasks, $w_i$ are task weights, and $\\mathcal{L}_{\\text{reg}}$ is regularization.\n",
    "\n",
    "### Transfer Learning in RL\n",
    "\n",
    "**Three Paradigms**:\n",
    "\n",
    "1. **Feature Transfer**: Use pre-trained features\n",
    "   $$\\pi(a|s) = \\text{Head}(\\text{FrozenFoundationModel}(s))$$\n",
    "\n",
    "2. **Fine-Tuning**: Adapt entire model\n",
    "   $$\\theta^{*} = \\arg\\min_{\\theta} \\mathcal{L}_{\\text{task}}(\\theta) + \\lambda ||\\theta - \\theta_0||^2$$\n",
    "\n",
    "3. **Prompt-Based Learning**: Task specification through prompts\n",
    "   $$\\pi(a|s, p) = \\text{FoundationModel}(s, p)$$\n",
    "   \n",
    "   Where $p$ is a task-specific prompt.\n",
    "\n",
    "### Cross-Modal Learning\n",
    "\n",
    "**Vision-Language-Action Models**:\n",
    "$$\\pi(a|v, l) = f(v, l) \\text{ where } v \\in \\mathcal{V}, l \\in \\mathcal{L}, a \\in \\mathcal{A}$$\n",
    "\n",
    "**Unified Representations**:\n",
    "- Visual observations $\\rightarrow$ Vision transformer features\n",
    "- Language instructions $\\rightarrow$ Language model embeddings  \n",
    "- Actions $\\rightarrow$ Shared action space representations\n",
    "\n",
    "**Cross-Modal Alignment**:\n",
    "$$\\mathcal{L}_{\\text{align}} = ||\\text{Embed}_V(v) - \\text{Embed}_L(\\text{describe}(v))||^2$$\n",
    "\n",
    "## 4.2 Large Language Models for RL\n",
    "\n",
    "### LLMs as World Models\n",
    "\n",
    "**Chain-of-Thought Reasoning**:\n",
    "```\n",
    "Thought: I need to navigate to the goal while avoiding obstacles.\n",
    "Action: Move right to avoid the wall on the left.\n",
    "Observation: I see a clear path ahead.\n",
    "Thought: The goal is north of my position.\n",
    "Action: Move up toward the goal.\n",
    "```\n",
    "\n",
    "**Structured Reasoning**:\n",
    "$$\\text{Action} = \\text{LLM}(\\text{State}, \\text{Goal}, \\text{History}, \\text{Reasoning Template})$$\n",
    "\n",
    "### Prompt Engineering for RL\n",
    "\n",
    "**Task Specification Prompts**:\n",
    "```\n",
    "Task: Navigate a robot to collect all gems in a maze.\n",
    "Rules: \n",
    "- Avoid obstacles (marked as #)\n",
    "- Collect gems (marked as *)  \n",
    "- Reach exit (marked as E)\n",
    "Current state: [ASCII representation]\n",
    "Choose action: [up, down, left, right]\n",
    "```\n",
    "\n",
    "**Few-Shot Learning Prompts**:\n",
    "```\n",
    "Example 1:\n",
    "State: Agent at (0,0), Goal at (1,1), No obstacles\n",
    "Action: right (move toward goal)\n",
    "Result: Reached (1,0)\n",
    "\n",
    "Example 2: \n",
    "State: Agent at (1,0), Goal at (1,1)\n",
    "Action: up (move toward goal)\n",
    "Result: Reached goal, +10 reward\n",
    "\n",
    "Current situation:\n",
    "State: [current state]\n",
    "Action: [your choice]\n",
    "```\n",
    "\n",
    "### LLM-Based Hierarchical Planning\n",
    "\n",
    "**High-Level Planning**:\n",
    "$$\\text{Subgoals} = \\text{LLM}_{\\text{planner}}(\\text{Task}, \\text{Environment})$$\n",
    "\n",
    "**Low-Level Execution**:\n",
    "$$a_t = \\pi_{\\text{low}}(s_t, \\text{current\\_subgoal})$$\n",
    "\n",
    "**Plan Refinement**:\n",
    "$$\\text{Updated\\_Plan} = \\text{LLM}_{\\text{planner}}(\\text{Original\\_Plan}, \\text{Execution\\_Feedback})$$\n",
    "\n",
    "## 4.3 Vision Transformers in RL\n",
    "\n",
    "### ViT for State Representation\n",
    "\n",
    "**Patch Embedding**:\n",
    "$$\\text{Patches} = \\text{Reshape}(\\text{Image}_{H \\times W \\times C}) \\rightarrow \\mathbb{R}^{N \\times P^2 \\cdot C}$$\n",
    "\n",
    "Where $N = HW/P^2$ is number of patches and $P$ is patch size.\n",
    "\n",
    "**Spatial-Temporal Attention**:\n",
    "- **Spatial**: Attend to important regions in current frame\n",
    "- **Temporal**: Attend to relevant frames in history\n",
    "- **Action**: Attend to action-relevant features\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Action Prediction Head**:\n",
    "$$\\pi(a|s) = \\text{MLP}(\\text{ViT}(s)[\\text{CLS}])$$\n",
    "\n",
    "Where $[\\text{CLS}]$ is the classification token embedding.\n",
    "\n",
    "### Multi-Modal Fusion\n",
    "\n",
    "**Visual-Language Fusion**:\n",
    "$$h_{\\text{fused}} = \\text{Attention}(h_{\\text{vision}}, h_{\\text{language}}, h_{\\text{language}})$$\n",
    "\n",
    "**Hierarchical Feature Integration**:\n",
    "- **Low-level**: Pixel features, edge detection\n",
    "- **Mid-level**: Objects, spatial relationships  \n",
    "- **High-level**: Scene understanding, semantic concepts\n",
    "\n",
    "### Attention-Based Policy Networks\n",
    "\n",
    "**Self-Attention for State Processing**:\n",
    "$$A_{\\text{state}} = \\text{SelfAttention}(\\text{StateFeatures})$$\n",
    "\n",
    "**Cross-Attention for Action Selection**:\n",
    "$$A_{\\text{action}} = \\text{CrossAttention}(\\text{ActionQueries}, \\text{StateFeatures})$$\n",
    "\n",
    "**Multi-Head Architecture**:\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "## 4.4 Foundation Model Training Strategies\n",
    "\n",
    "### Pre-Training Objectives\n",
    "\n",
    "**Masked Language Modeling (MLM)**:\n",
    "$$\\mathcal{L}_{\\text{MLM}} = -\\sum_{i \\in \\text{masked}} \\log p(x_i | x_{\\setminus i})$$\n",
    "\n",
    "**Masked Image Modeling (MIM)**:  \n",
    "$$\\mathcal{L}_{\\text{MIM}} = ||\\text{Reconstruct}(\\text{Mask}(\\text{Image})) - \\text{Image}||^2$$\n",
    "\n",
    "**Contrastive Learning**:\n",
    "$$\\mathcal{L}_{\\text{contrastive}} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k} \\exp(\\text{sim}(z_i, z_k)/\\tau)}$$\n",
    "\n",
    "### Multi-Task Pre-Training\n",
    "\n",
    "**Joint Training Objective**:\n",
    "$$\\mathcal{L}_{\\text{joint}} = \\sum_{t=1}^{T} \\lambda_t \\mathcal{L}_t + \\mathcal{L}_{\\text{reg}}$$\n",
    "\n",
    "**Task Sampling Strategies**:\n",
    "- **Uniform Sampling**: Equal probability for all tasks\n",
    "- **Importance Sampling**: Weight by task difficulty/importance\n",
    "- **Curriculum Learning**: Gradually increase task complexity\n",
    "\n",
    "**Parameter Sharing Strategies**:\n",
    "- **Shared Encoder**: Common feature extraction\n",
    "- **Task-Specific Heads**: Specialized output layers\n",
    "- **Adapter Layers**: Small task-specific modifications\n",
    "\n",
    "### Fine-Tuning Approaches\n",
    "\n",
    "**Full Fine-Tuning**:\n",
    "- Update all parameters for target task\n",
    "- Risk of catastrophic forgetting\n",
    "- Requires substantial computational resources\n",
    "\n",
    "**Parameter-Efficient Fine-Tuning**:\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)**:\n",
    "$$W' = W + AB$$\n",
    "where $A \\in \\mathbb{R}^{d \\times r}$, $B \\in \\mathbb{R}^{r \\times d}$ with $r << d$.\n",
    "\n",
    "**Adapter Layers**:\n",
    "$$h' = h + \\text{Adapter}(h) = h + W_2 \\sigma(W_1 h + b_1) + b_2$$\n",
    "\n",
    "**Prefix Tuning**:\n",
    "Add learnable prefix vectors to transformer inputs.\n",
    "\n",
    "### Continual Learning for Foundation Models\n",
    "\n",
    "**Elastic Weight Consolidation (EWC)**:\n",
    "$$\\mathcal{L}_{\\text{EWC}} = \\mathcal{L}_{\\text{task}} + \\lambda \\sum_i F_i (\\theta_i - \\theta_i^*)^2$$\n",
    "\n",
    "Where $F_i$ is Fisher information matrix diagonal.\n",
    "\n",
    "**Progressive Networks**:\n",
    "- Freeze previous task parameters\n",
    "- Add new columns for new tasks\n",
    "- Lateral connections for knowledge transfer\n",
    "\n",
    "**Meta-Learning for Rapid Adaptation**:\n",
    "$$\\theta' = \\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\text{support}}(\\theta)$$\n",
    "$$\\mathcal{L}_{\\text{meta}} = \\mathbb{E}_{\\text{tasks}} [\\mathcal{L}_{\\text{query}}(\\theta')]$$\n",
    "\n",
    "## 4.5 Emergent Capabilities\n",
    "\n",
    "### Few-Shot Task Learning\n",
    "Foundation models demonstrate remarkable ability to adapt to new tasks with minimal examples:\n",
    "\n",
    "**In-Context Learning**:\n",
    "- Provide examples in input prompt\n",
    "- Model adapts without parameter updates\n",
    "- Emergent capability from scale and diversity\n",
    "\n",
    "**Meta-Learning Through Pre-Training**:\n",
    "- Learn to learn from pre-training data distribution\n",
    "- Transfer learning strategies emerge naturally\n",
    "- Rapid adaptation to distribution shifts\n",
    "\n",
    "### Compositional Reasoning\n",
    "Combine primitive skills to solve complex tasks:\n",
    "\n",
    "**Skill Composition**:\n",
    "$$\\text{ComplexTask} = \\text{Compose}(\\text{Skill}_1, \\text{Skill}_2, \\ldots, \\text{Skill}_k)$$\n",
    "\n",
    "**Hierarchical Planning**:\n",
    "- Decompose complex goals into subgoals\n",
    "- Learn primitive skills for subgoal achievement\n",
    "- Compose skills dynamically based on context\n",
    "\n",
    "### Cross-Domain Transfer\n",
    "Knowledge learned in one domain transfers to related domains:\n",
    "\n",
    "**Domain Adaptation**:\n",
    "$$\\mathcal{L}_{\\text{adapt}} = \\mathcal{L}_{\\text{target}} + \\lambda \\mathcal{L}_{\\text{domain}}$$\n",
    "\n",
    "**Universal Policies**:\n",
    "Single policy that works across multiple environments with different dynamics, observation spaces, and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c888848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import MultiheadAttention, LayerNorm\nimport math\nimport numpy as np\nfrom typing import List, Dict, Tuple, Optional\nfrom dataclasses import dataclass\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, max_len: int = 5000):\n        super().__init__()\n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-math.log(10000.0) / d_model))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        self.register_buffer('pe', pe)\n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]\nclass TransformerBlock(nn.Module):\n    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n        super().__init__()\n        self.attention = MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.norm1 = LayerNorm(d_model)\n        self.norm2 = LayerNorm(d_model)\n        self.feedforward = nn.Sequential(\n            nn.Linear(d_model, d_ff),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_ff, d_model),\n            nn.Dropout(dropout)\n        )\n    def forward(self, x, mask=None):\n        attn_out, attn_weights = self.attention(x, x, x, attn_mask=mask)\n        x = self.norm1(x + attn_out)\n        ff_out = self.feedforward(x)\n        x = self.norm2(x + ff_out)\n        return x, attn_weights\nclass VisionTransformer(nn.Module):\n    def __init__(self, img_size: int = 84, patch_size: int = 16, in_channels: int = 3,\n                 d_model: int = 256, n_heads: int = 8, n_layers: int = 6, \n                 d_ff: int = 1024, dropout: float = 0.1):\n        super().__init__()\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.n_patches = (img_size // patch_size) ** 2\n        self.d_model = d_model\n        self.patch_embed = nn.Conv2d(in_channels, d_model, \n                                   kernel_size=patch_size, stride=patch_size)\n        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, d_model))\n        self.layers = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_ff, dropout)\n            for _ in range(n_layers)\n        ])\n        self.norm = LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = self.patch_embed(x)\n        x = x.flatten(2).transpose(1, 2)\n        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n        x = torch.cat([cls_tokens, x], dim=1)\n        x = x + self.pos_embed\n        x = self.dropout(x)\n        attentions = []\n        for layer in self.layers:\n            x, attn = layer(x)\n            attentions.append(attn)\n        x = self.norm(x)\n        return x, attentions\nclass LanguageEncoder(nn.Module):\n    def __init__(self, vocab_size: int, d_model: int = 256, n_heads: int = 8,\n                 n_layers: int = 4, max_seq_len: int = 128, dropout: float = 0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.embedding = nn.Embedding(vocab_size, d_model)\n        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n        self.layers = nn.ModuleList([\n            TransformerBlock(d_model, n_heads, d_model * 4, dropout)\n            for _ in range(n_layers)\n        ])\n        self.norm = LayerNorm(d_model)\n    def forward(self, tokens, attention_mask=None):\n        x = self.embedding(tokens) * math.sqrt(self.d_model)\n        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)\n        if attention_mask is not None:\n            mask = attention_mask.unsqueeze(1).repeat(1, attention_mask.size(1), 1)\n            mask = mask.masked_fill(mask == 0, float('-inf'))\n        else:\n            mask = None\n        for layer in self.layers:\n            x, _ = layer(x, mask)\n        x = self.norm(x)\n        return x\nclass CrossModalFusion(nn.Module):\n    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.1):\n        super().__init__()\n        self.vision_to_lang = MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.lang_to_vision = MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n        self.norm1 = LayerNorm(d_model)\n        self.norm2 = LayerNorm(d_model)\n        self.fusion_net = nn.Sequential(\n            nn.Linear(d_model * 2, d_model),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model, d_model)\n        )\n    def forward(self, vision_features, lang_features, lang_mask=None):\n        vision_attended, _ = self.vision_to_lang(\n            vision_features, lang_features, lang_features, key_padding_mask=lang_mask\n        )\n        vision_features = self.norm1(vision_features + vision_attended)\n        lang_attended, _ = self.lang_to_vision(\n            lang_features, vision_features, vision_features\n        )\n        lang_features = self.norm2(lang_features + lang_attended)\n        vision_pooled = vision_features[:, 0]\n        lang_pooled = lang_features.mean(dim=1)\n        combined = torch.cat([vision_pooled, lang_pooled], dim=1)\n        fused = self.fusion_net(combined)\n        return fused\nclass FoundationPolicy(nn.Module):\n    def __init__(self, \n                 img_size: int = 84,\n                 patch_size: int = 16, \n                 in_channels: int = 3,\n                 vocab_size: int = 1000,\n                 action_dim: int = 4,\n                 d_model: int = 256,\n                 n_heads: int = 8,\n                 n_layers: int = 6,\n                 max_seq_len: int = 64,\n                 dropout: float = 0.1):\n        super().__init__()\n        self.d_model = d_model\n        self.action_dim = action_dim\n        self.vision_encoder = VisionTransformer(\n            img_size, patch_size, in_channels, d_model, n_heads, n_layers, \n            d_model * 4, dropout\n        )\n        self.language_encoder = LanguageEncoder(\n            vocab_size, d_model, n_heads, n_layers // 2, max_seq_len, dropout\n        )\n        self.fusion = CrossModalFusion(d_model, n_heads, dropout)\n        self.policy_head = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model // 2, action_dim)\n        )\n        self.value_head = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(d_model // 2, 1)\n        )\n        self.apply(self._init_weights)\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n    def forward(self, images, instructions=None, instruction_mask=None):\n        vision_features, vision_attentions = self.vision_encoder(images)\n        if instructions is not None:\n            lang_features = self.language_encoder(instructions, instruction_mask)\n            fused_features = self.fusion(vision_features, lang_features, instruction_mask)\n        else:\n            fused_features = vision_features[:, 0]\n        action_logits = self.policy_head(fused_features)\n        values = self.value_head(fused_features).squeeze(-1)\n        attention_info = {\n            'vision_attentions': vision_attentions,\n            'fused_features': fused_features\n        }\n        return action_logits, values, attention_info\n    def get_action(self, images, instructions=None, instruction_mask=None, \n                   deterministic=False):\n        with torch.no_grad():\n            action_logits, values, attention_info = self.forward(\n                images, instructions, instruction_mask\n            )\n            if deterministic:\n                actions = torch.argmax(action_logits, dim=-1)\n            else:\n                action_dist = torch.distributions.Categorical(logits=action_logits)\n                actions = action_dist.sample()\n        return actions, values, attention_info\nclass FewShotLearner:\n    def __init__(self, foundation_model: FoundationPolicy):\n        self.foundation_model = foundation_model\n        self.task_examples = []\n    def add_example(self, image, instruction, action, reward):\n        self.task_examples.append({\n            'image': image,\n            'instruction': instruction,\n            'action': action,\n            'reward': reward\n        })\n    def adapt_to_task(self, support_data, lr=1e-4, steps=10):\n        optimizer = torch.optim.Adam(self.foundation_model.parameters(), lr=lr)\n        for step in range(steps):\n            total_loss = 0\n            for example in support_data:\n                images = example['images'].unsqueeze(0)\n                instructions = example['instructions'].unsqueeze(0)\n                actions = example['actions'].unsqueeze(0)\n                rewards = example['rewards'].unsqueeze(0)\n                action_logits, values, _ = self.foundation_model(images, instructions)\n                action_loss = F.cross_entropy(action_logits, actions)\n                value_loss = F.mse_loss(values, rewards)\n                loss = action_loss + 0.5 * value_loss\n                total_loss += loss\n            optimizer.zero_grad()\n            total_loss.backward()\n            optimizer.step()\n        return total_loss.item()\nclass PromptTemplate:\n    def __init__(self, task_type: str):\n        self.task_type = task_type\n        self.templates = {\n            'navigation': \"Navigate to {goal} while avoiding {obstacles}. Current position: {position}.\",\n            'collection': \"Collect all {objects} in the environment. Collected: {collected}/{total}.\",\n            'interaction': \"Interact with {target} to {action}. Available actions: {actions}.\",\n            'puzzle': \"Solve the puzzle by {instruction}. Current state: {state}.\"\n        }\n    def generate_prompt(self, **kwargs) -> str:\n        if self.task_type in self.templates:\n            return self.templates[self.task_type].format(**kwargs)\n        else:\n            return f\"Complete the task: {kwargs.get('instruction', 'Unknown task')}\"\n    def tokenize_prompt(self, prompt: str, tokenizer, max_length: int = 64) -> Dict:\n        words = prompt.lower().split()\n        vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3}\n        for word in words:\n            if word not in vocab:\n                vocab[word] = len(vocab)\n        tokens = [vocab.get(word, vocab['[UNK]']) for word in words]\n        if len(tokens) < max_length:\n            tokens = tokens + [vocab['[PAD]']] * (max_length - len(tokens))\n            mask = [1] * len(words) + [0] * (max_length - len(words))\n        else:\n            tokens = tokens[:max_length]\n            mask = [1] * max_length\n        return {\n            'tokens': torch.tensor(tokens),\n            'mask': torch.tensor(mask, dtype=torch.bool),\n            'vocab': vocab\n        }\nprint(\"âœ… Foundation Models in RL implementation complete!\")\nprint(\"Components implemented:\")\nprint(\"- VisionTransformer: Process visual observations with attention\")\nprint(\"- LanguageEncoder: Process text instructions\") \nprint(\"- CrossModalFusion: Fuse vision and language representations\")\nprint(\"- FoundationPolicy: Multi-modal policy with interpretable attention\")\nprint(\"- FewShotLearner: Rapid task adaptation\")\nprint(\"- PromptTemplate: Task specification through natural language\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df536c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalGridWorld(gym.Env):\n    def __init__(self, size=8, render_size=84):\n        super().__init__()\n        self.size = size\n        self.render_size = render_size\n        self.reset()\n        self.action_space = spaces.Discrete(4)\n        self.observation_space = spaces.Dict({\n            'image': spaces.Box(low=0, high=255, shape=(3, render_size, render_size), dtype=np.uint8),\n            'instruction': spaces.Box(low=0, high=1000, shape=(64,), dtype=np.int32),\n            'instruction_mask': spaces.Box(low=0, high=1, shape=(64,), dtype=np.bool_)\n        })\n        self.actions = {0: [-1, 0], 1: [1, 0], 2: [0, -1], 3: [0, 1]}\n        self.action_names = ['Up', 'Down', 'Left', 'Right']\n        self.tasks = [\n            'navigation', 'collection', 'avoidance', 'exploration'\n        ]\n        self.prompt_template = PromptTemplate('navigation')\n    def reset(self, task_type='navigation', seed=None):\n        super().reset(seed=seed)\n        self.agent_pos = [0, 0]\n        self.goal_pos = [self.size-1, self.size-1]\n        self.obstacles = set()\n        self.treasures = set()\n        self.visited = {tuple(self.agent_pos)}\n        for _ in range(self.size // 2):\n            x, y = np.random.randint(1, self.size-1), np.random.randint(1, self.size-1)\n            if [x, y] not in [self.agent_pos, self.goal_pos]:\n                self.obstacles.add((x, y))\n        if task_type == 'collection':\n            for _ in range(3):\n                x, y = np.random.randint(0, self.size), np.random.randint(0, self.size)\n                if ([x, y] not in [self.agent_pos, self.goal_pos] and \n                    (x, y) not in self.obstacles):\n                    self.treasures.add((x, y))\n        self.collected_treasures = set()\n        self.step_count = 0\n        self.max_steps = self.size * self.size\n        self.task_type = task_type\n        return self._get_observation(), {}\n    def step(self, action):\n        old_pos = self.agent_pos.copy()\n        new_pos = [\n            self.agent_pos[0] + self.actions[action][0],\n            self.agent_pos[1] + self.actions[action][1]\n        ]\n        reward = -0.1\n        if (0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and \n            tuple(new_pos) not in self.obstacles):\n            self.agent_pos = new_pos\n            self.visited.add(tuple(new_pos))\n            if self.task_type == 'navigation':\n                old_dist = abs(old_pos[0] - self.goal_pos[0]) + abs(old_pos[1] - self.goal_pos[1])\n                new_dist = abs(new_pos[0] - self.goal_pos[0]) + abs(new_pos[1] - self.goal_pos[1])\n                reward += (old_dist - new_dist) * 0.1\n                if new_pos == self.goal_pos:\n                    reward += 10\n            elif self.task_type == 'collection':\n                if tuple(new_pos) in self.treasures and tuple(new_pos) not in self.collected_treasures:\n                    self.collected_treasures.add(tuple(new_pos))\n                    reward += 5\n                if len(self.collected_treasures) == len(self.treasures):\n                    reward += 20\n            elif self.task_type == 'exploration':\n                if tuple(new_pos) not in self.visited:\n                    reward += 1\n        else:\n            reward -= 1\n        self.step_count += 1\n        terminated = self._check_termination()\n        return self._get_observation(), reward, terminated, False, {}\n    def _check_termination(self):\n        if self.task_type == 'navigation':\n            return self.agent_pos == self.goal_pos or self.step_count >= self.max_steps\n        elif self.task_type == 'collection':\n            return (len(self.collected_treasures) == len(self.treasures) or \n                   self.step_count >= self.max_steps)\n        elif self.task_type == 'exploration':\n            return (len(self.visited) >= self.size * self.size * 0.8 or \n                   self.step_count >= self.max_steps)\n        return self.step_count >= self.max_steps\n    def _get_observation(self):\n        image = self._render_image()\n        instruction_text = self._generate_instruction()\n        instruction_data = self.prompt_template.tokenize_prompt(\n            instruction_text, None, max_length=64\n        )\n        return {\n            'image': image,\n            'instruction': instruction_data['tokens'],\n            'instruction_mask': instruction_data['mask']\n        }\n    def _render_image(self):\n        image = np.ones((3, self.render_size, self.render_size), dtype=np.uint8) * 255\n        cell_size = self.render_size // self.size\n        for i in range(self.size + 1):\n            x = i * cell_size\n            image[:, x:x+1, :] = 200\n            y = i * cell_size\n            image[:, :, y:y+1] = 200\n        for obs_x, obs_y in self.obstacles:\n            x1, x2 = obs_x * cell_size, (obs_x + 1) * cell_size\n            y1, y2 = obs_y * cell_size, (obs_y + 1) * cell_size\n            image[:, x1:x2, y1:y2] = 0\n        for treasure_x, treasure_y in self.treasures:\n            if (treasure_x, treasure_y) not in self.collected_treasures:\n                x1, x2 = treasure_x * cell_size, (treasure_x + 1) * cell_size\n                y1, y2 = treasure_y * cell_size, (treasure_y + 1) * cell_size\n                image[0, x1:x2, y1:y2] = 255\n                image[1, x1:x2, y1:y2] = 255\n                image[2, x1:x2, y1:y2] = 0\n        goal_x, goal_y = self.goal_pos\n        x1, x2 = goal_x * cell_size, (goal_x + 1) * cell_size\n        y1, y2 = goal_y * cell_size, (goal_y + 1) * cell_size\n        image[0, x1:x2, y1:y2] = 0\n        image[1, x1:x2, y1:y2] = 255\n        image[2, x1:x2, y1:y2] = 0\n        agent_x, agent_y = self.agent_pos\n        x1, x2 = agent_x * cell_size, (agent_x + 1) * cell_size  \n        y1, y2 = agent_y * cell_size, (agent_y + 1) * cell_size\n        image[0, x1:x2, y1:y2] = 255\n        image[1, x1:x2, y1:y2] = 0\n        image[2, x1:x2, y1:y2] = 0\n        return image\n    def _generate_instruction(self):\n        if self.task_type == 'navigation':\n            return f\"Navigate to the goal at position ({self.goal_pos[0]}, {self.goal_pos[1]}). \" \\\n                   f\"Current position: ({self.agent_pos[0]}, {self.agent_pos[1]}). \" \\\n                   f\"Avoid obstacles and find the shortest path.\"\n        elif self.task_type == 'collection':\n            total = len(self.treasures)\n            collected = len(self.collected_treasures)\n            return f\"Collect all {total} treasures. Progress: {collected}/{total} collected. \" \\\n                   f\"Yellow squares are treasures. Current position: ({self.agent_pos[0]}, {self.agent_pos[1]}).\"\n        elif self.task_type == 'exploration':\n            explored = len(self.visited)\n            total_cells = self.size * self.size\n            return f\"Explore the environment. Visit at least 80% of cells. \" \\\n                   f\"Progress: {explored}/{total_cells} cells visited.\"\n        else:\n            return f\"Complete the task. Current position: ({self.agent_pos[0]}, {self.agent_pos[1]}).\"\ndef visualize_attention_maps(model, observation, save_path=None):\n    with torch.no_grad():\n        images = observation['image'].unsqueeze(0).float() / 255.0\n        instructions = observation['instruction'].unsqueeze(0)\n        instruction_mask = observation['instruction_mask'].unsqueeze(0)\n        action_logits, values, attention_info = model(images, instructions, instruction_mask)\n        vision_attention = attention_info['vision_attentions'][-1][0]\n        cls_attention = vision_attention.mean(0)[0, 1:]\n        n_patches_per_dim = int(np.sqrt(len(cls_attention)))\n        attention_map = cls_attention.reshape(n_patches_per_dim, n_patches_per_dim)\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n        original_image = images[0].permute(1, 2, 0).numpy()\n        axes[0].imshow(original_image)\n        axes[0].set_title('Original Image')\n        axes[0].axis('off')\n        im = axes[1].imshow(attention_map.numpy(), cmap='hot', interpolation='bilinear')\n        axes[1].set_title('Vision Attention Map')\n        axes[1].axis('off')\n        plt.colorbar(im, ax=axes[1])\n        from scipy.ndimage import zoom\n        attention_resized = zoom(attention_map.numpy(), \n                               (original_image.shape[0] / attention_map.shape[0],\n                                original_image.shape[1] / attention_map.shape[1]))\n        axes[2].imshow(original_image)\n        axes[2].imshow(attention_resized, alpha=0.6, cmap='hot')\n        axes[2].set_title('Attention Overlay')\n        axes[2].axis('off')\n        plt.tight_layout()\n        if save_path:\n            plt.savefig(save_path)\n        plt.show()\n        action_probs = F.softmax(action_logits[0], dim=0)\n        action_names = ['Up', 'Down', 'Left', 'Right']\n        print(\"Action Predictions:\")\n        for i, (action, prob) in enumerate(zip(action_names, action_probs)):\n            print(f\"  {action}: {prob:.3f}\")\n        print(f\"Predicted Value: {values[0]:.3f}\")\ndef compare_models_performance(environments, models, episodes_per_env=100):\n    results = {model_name: {env_name: [] for env_name in environments.keys()} \n              for model_name in models.keys()}\n    for env_name, env in environments.items():\n        print(f\"\\nTesting environment: {env_name}\")\n        for model_name, model in models.items():\n            print(f\"  Testing model: {model_name}\")\n            episode_rewards = []\n            for episode in range(episodes_per_env):\n                obs, _ = env.reset()\n                episode_reward = 0\n                done = False\n                while not done:\n                    if hasattr(model, 'get_action'):\n                        images = torch.FloatTensor(obs['image']).unsqueeze(0) / 255.0\n                        instructions = obs['instruction'].unsqueeze(0)\n                        instruction_mask = obs['instruction_mask'].unsqueeze(0)\n                        action, _, _ = model.get_action(images, instructions, instruction_mask)\n                        action = action.item()\n                    else:\n                        action = env.action_space.sample()\n                    obs, reward, terminated, truncated, _ = env.step(action)\n                    done = terminated or truncated\n                    episode_reward += reward\n                episode_rewards.append(episode_reward)\n            results[model_name][env_name] = episode_rewards\n            avg_reward = np.mean(episode_rewards)\n            std_reward = np.std(episode_rewards)\n            print(f\"    Average reward: {avg_reward:.2f} Â± {std_reward:.2f}\")\n    return results\ndef plot_learning_curves(training_histories, save_path=None):\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    for name, history in training_histories.items():\n        episodes = range(len(history['rewards']))\n        plt.plot(episodes, history['rewards'], label=name, alpha=0.7)\n        if len(history['rewards']) > 10:\n            window = min(50, len(history['rewards']) // 10)\n            moving_avg = np.convolve(history['rewards'], np.ones(window)/window, mode='valid')\n            plt.plot(range(window-1, len(history['rewards'])), moving_avg, \n                    label=f'{name} (MA)', linewidth=2)\n    plt.xlabel('Episode')\n    plt.ylabel('Reward')\n    plt.title('Learning Curves')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.subplot(1, 3, 2)\n    for name, history in training_histories.items():\n        if 'losses' in history and history['losses']:\n            episodes = range(len(history['losses']))\n            plt.plot(episodes, history['losses'], label=name, alpha=0.7)\n    plt.xlabel('Training Step')\n    plt.ylabel('Loss')\n    plt.title('Training Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.yscale('log')\n    plt.subplot(1, 3, 3)\n    for name, history in training_histories.items():\n        if 'success_rate' in history and history['success_rate']:\n            episodes = range(len(history['success_rate']))\n            plt.plot(episodes, history['success_rate'], label=name, alpha=0.7)\n    plt.xlabel('Episode')\n    plt.ylabel('Success Rate')\n    plt.title('Success Rate Over Time')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    plt.tight_layout()\n    if save_path:\n        plt.savefig(save_path)\n    plt.show()\nprint(\"Setting up comprehensive experiments...\")\nmm_env = MultiModalGridWorld(size=6, render_size=84)\nprint(\"Testing multi-modal environment...\")\nobs, _ = mm_env.reset(task_type='navigation')\nprint(f\"Image shape: {obs['image'].shape}\")\nprint(f\"Instruction tokens: {obs['instruction'].shape}\")\nprint(f\"Instruction mask: {obs['instruction_mask'].shape}\")\nprint(\"Creating foundation model...\")\nfoundation_model = FoundationPolicy(\n    img_size=84,\n    patch_size=16,\n    in_channels=3,\n    vocab_size=1000,\n    action_dim=4,\n    d_model=128,\n    n_heads=4,\n    n_layers=3,\n    max_seq_len=64,\n    dropout=0.1\n)\nprint(\"Testing foundation model...\")\nimages = torch.FloatTensor(obs['image']).unsqueeze(0) / 255.0\ninstructions = obs['instruction'].unsqueeze(0)\ninstruction_mask = obs['instruction_mask'].unsqueeze(0)\naction_logits, values, attention_info = foundation_model(images, instructions, instruction_mask)\nprint(f\"Action logits shape: {action_logits.shape}\")\nprint(f\"Values shape: {values.shape}\")\nprint(f\"Number of attention layers: {len(attention_info['vision_attentions'])}\")\nprint(\"\\nâœ… Comprehensive experimental setup complete!\")\nprint(\"Available functions:\")\nprint(\"- visualize_attention_maps(): Visualize model attention\")  \nprint(\"- compare_models_performance(): Compare different approaches\")\nprint(\"- plot_learning_curves(): Plot training progress\")\nprint(\"\\nEnvironment supports multiple task types: 'navigation', 'collection', 'exploration'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dba8e3",
   "metadata": {},
   "source": [
    "# Conclusion and Future Directions\n",
    "\n",
    "## Summary of Advanced Deep RL Concepts\n",
    "\n",
    "This notebook has explored cutting-edge topics in Deep Reinforcement Learning that represent the current frontier of research and applications. We covered four major paradigms:\n",
    "\n",
    "### 1. Continual Learning in RL\n",
    "- **Key Insight**: Agents must learn new tasks while retaining knowledge from previous experiences\n",
    "- **Main Challenges**: Catastrophic forgetting, interference between tasks, scalability\n",
    "- **Solutions**: Elastic Weight Consolidation, Progressive Networks, Meta-learning approaches\n",
    "- **Applications**: Robotics, adaptive systems, lifelong learning agents\n",
    "\n",
    "### 2. Neurosymbolic Reinforcement Learning  \n",
    "- **Key Insight**: Combining neural learning with symbolic reasoning for interpretable and robust agents\n",
    "- **Main Challenges**: Integration of continuous and discrete representations, knowledge representation\n",
    "- **Solutions**: Differentiable programming, logic-based constraints, hybrid architectures\n",
    "- **Applications**: Autonomous systems, healthcare, safety-critical domains\n",
    "\n",
    "### 3. Human-AI Collaborative Learning\n",
    "- **Key Insight**: Leverage human expertise and feedback to improve agent learning and performance\n",
    "- **Main Challenges**: Trust modeling, preference learning, real-time collaboration\n",
    "- **Solutions**: RLHF, preference-based rewards, shared autonomy frameworks\n",
    "- **Applications**: Human-robot interaction, personalized AI, assisted decision-making\n",
    "\n",
    "### 4. Foundation Models in RL\n",
    "- **Key Insight**: Pre-trained large models enable sample-efficient learning and strong generalization\n",
    "- **Main Challenges**: Transfer learning, multi-modal integration, computational efficiency\n",
    "- **Solutions**: Vision transformers, cross-modal attention, prompt engineering\n",
    "- **Applications**: General-purpose AI agents, few-shot learning, multi-task systems\n",
    "\n",
    "## Interconnections Between Paradigms\n",
    "\n",
    "These four approaches are not isolated but can be combined synergistically:\n",
    "\n",
    "**Continual + Neurosymbolic**: Symbolic knowledge provides structure for continual learning, preventing catastrophic forgetting through logical constraints.\n",
    "\n",
    "**Human-AI + Foundation Models**: Foundation models provide better initialization for human-AI collaboration, while human feedback can guide foundation model fine-tuning.\n",
    "\n",
    "**Neurosymbolic + Foundation Models**: Foundation models can learn to perform symbolic reasoning, while symbolic structures can guide foundation model architectures.\n",
    "\n",
    "**All Four Combined**: A truly advanced RL system might use foundation models as initialization, incorporate human feedback for alignment, use symbolic reasoning for interpretability, and support continual learning for adaptation.\n",
    "\n",
    "## Current Research Frontiers\n",
    "\n",
    "### Emerging Challenges\n",
    "1. **Scalability**: How do these methods scale to real-world complexity?\n",
    "2. **Sample Efficiency**: Can we achieve superhuman performance with minimal data?\n",
    "3. **Robustness**: How do agents handle distribution shifts and adversarial conditions?\n",
    "4. **Alignment**: How do we ensure AI systems pursue intended objectives?\n",
    "5. **Interpretability**: Can we understand and verify agent decision-making?\n",
    "\n",
    "### Promising Directions\n",
    "1. **Unified Architectures**: Single models that combine multiple paradigms\n",
    "2. **Meta-Learning**: Learning to learn across paradigms and domains\n",
    "3. **Causal Reasoning**: Understanding cause-and-effect relationships\n",
    "4. **Compositional Learning**: Building complex behaviors from simple primitives\n",
    "5. **Multi-Agent Collaboration**: Scaling human-AI collaboration to teams\n",
    "\n",
    "## Practical Implementation Insights\n",
    "\n",
    "### Key Lessons Learned\n",
    "1. **Start Simple**: Begin with simplified versions before adding complexity\n",
    "2. **Modular Design**: Build components that can be combined and reused\n",
    "3. **Interpretability First**: Design for explainability from the beginning\n",
    "4. **Human-Centered**: Consider human factors in system design\n",
    "5. **Robust Evaluation**: Test across diverse scenarios and failure modes\n",
    "\n",
    "### Implementation Best Practices\n",
    "1. **Gradual Integration**: Introduce new paradigms incrementally\n",
    "2. **Ablation Studies**: Understand the contribution of each component\n",
    "3. **Multi-Metric Evaluation**: Use diverse evaluation criteria beyond reward\n",
    "4. **Failure Analysis**: Learn from failures and edge cases\n",
    "5. **Ethical Considerations**: Address bias, fairness, and safety concerns\n",
    "\n",
    "## Future Applications\n",
    "\n",
    "### Near-Term (1-3 years)\n",
    "- **Personalized AI Assistants**: Agents that adapt to individual preferences and learn continuously\n",
    "- **Robotic Process Automation**: Intelligent automation that can handle exceptions and learn from feedback\n",
    "- **Educational AI**: Tutoring systems that adapt teaching strategies based on student progress\n",
    "- **Healthcare Support**: AI systems that assist medical professionals with decision-making\n",
    "\n",
    "### Medium-Term (3-7 years)\n",
    "- **Autonomous Vehicles**: Self-driving cars that learn from human drivers and adapt to new environments\n",
    "- **Smart Cities**: Urban systems that optimize resource allocation through continuous learning\n",
    "- **Scientific Discovery**: AI agents that collaborate with researchers to generate and test hypotheses\n",
    "- **Creative AI**: Systems that collaborate with humans in creative endeavors\n",
    "\n",
    "### Long-Term (7+ years)\n",
    "- **General Intelligence**: AI systems that can perform any cognitive task that humans can do\n",
    "- **Scientific AI**: Autonomous systems capable of conducting independent scientific research\n",
    "- **Collaborative Societies**: Seamless integration of human and AI capabilities in all aspects of society\n",
    "- **Space Exploration**: AI systems capable of autonomous operation in extreme and unknown environments\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The field of Deep Reinforcement Learning continues to evolve rapidly, with these advanced paradigms representing the current cutting edge. Each approach addresses fundamental limitations of traditional RL and opens new possibilities for creating more capable, reliable, and aligned AI systems.\n",
    "\n",
    "The key to success in this field is not just understanding individual techniques, but recognizing how they can be combined to create systems that are greater than the sum of their parts. As we move forward, the most impactful advances will likely come from principled integration of these paradigms with careful attention to real-world constraints and human values.\n",
    "\n",
    "### Final Recommendations for Further Learning\n",
    "\n",
    "1. **Hands-On Implementation**: Build and experiment with these systems yourself\n",
    "2. **Stay Current**: Follow recent papers and conferences (NeurIPS, ICML, ICLR, AAAI)\n",
    "3. **Interdisciplinary Learning**: Study cognitive science, philosophy, and domain-specific knowledge\n",
    "4. **Community Engagement**: Participate in research communities and open-source projects\n",
    "5. **Ethical Reflection**: Consider the societal implications of your work\n",
    "\n",
    "The future of AI lies not just in more powerful algorithms, but in systems that can learn, reason, collaborate, and adapt in ways that align with human values and capabilities. These advanced RL paradigms provide the building blocks for that future.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You have completed CA16 - Advanced Topics in Deep Reinforcement Learning**\n",
    "\n",
    "This comprehensive exploration has covered the most cutting-edge approaches in modern RL research. You now have the theoretical foundations and practical implementation skills to contribute to the next generation of intelligent systems.\n",
    "\n",
    "*\"The best way to predict the future is to invent it.\"* - Alan Kay"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}