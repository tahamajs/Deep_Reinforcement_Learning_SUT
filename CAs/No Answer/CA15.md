# CA15: Advanced Deep Reinforcement Learning - Model-based Rl and Hierarchical Rl## Overviewthis Comprehensive Assignment Covers Advanced Topics in Deep Reinforcement Learning, Focusing ON:1. **model-based Reinforcement Learning**- World Models and Environment Dynamics- Model-predictive Control (mpc)- Planning with Learned Models- Dyna-q and Model-based Policy OPTIMIZATION2. **hierarchical Reinforcement Learning**- Options Framework- Hierarchical Actor-critic (hac)- Goal-conditioned Rl- Feudal NETWORKS3. **advanced Planning and Control**- Monte Carlo Tree Search (mcts)- Model-based Value Expansion- Latent Space Planning### Learning Objectives- Understand Model-based Rl Principles and Implementation- Master Hierarchical Decomposition in Rl- Implement Advanced Planning Algorithms- Apply These Methods to Complex Control Tasks---

# Table of Contents- [CA15: Advanced Deep Reinforcement Learning - Model-based Rl and Hierarchical Rl## Overviewthis Comprehensive Assignment Covers Advanced Topics in Deep Reinforcement Learning, Focusing ON:1. **model-based Reinforcement Learning**- World Models and Environment Dynamics- Model-predictive Control (mpc)- Planning with Learned Models- Dyna-q and Model-based Policy OPTIMIZATION2. **hierarchical Reinforcement Learning**- Options Framework- Hierarchical Actor-critic (hac)- Goal-conditioned Rl- Feudal NETWORKS3. **advanced Planning and Control**- Monte Carlo Tree Search (mcts)- Model-based Value Expansion- Latent Space Planning### Learning Objectives- Understand Model-based Rl Principles and Implementation- Master Hierarchical Decomposition in Rl- Implement Advanced Planning Algorithms- Apply These Methods to Complex Control Tasks---](#ca15-advanced-deep-reinforcement-learning---model-based-rl-and-hierarchical-rl-overviewthis-comprehensive-assignment-covers-advanced-topics-in-deep-reinforcement-learning-focusing-on1-model-based-reinforcement-learning--world-models-and-environment-dynamics--model-predictive-control-mpc--planning-with-learned-models--dyna-q-and-model-based-policy-optimization2-hierarchical-reinforcement-learning--options-framework--hierarchical-actor-critic-hac--goal-conditioned-rl--feudal-networks3-advanced-planning-and-control--monte-carlo-tree-search-mcts--model-based-value-expansion--latent-space-planning-learning-objectives--understand-model-based-rl-principles-and-implementation--master-hierarchical-decomposition-in-rl--implement-advanced-planning-algorithms--apply-these-methods-to-complex-control-tasks---)- [Table of Contents- [CA15: Advanced Deep Reinforcement Learning - Model-based Rl and Hierarchical Rl## Overviewthis Comprehensive Assignment Covers Advanced Topics in Deep Reinforcement Learning, Focusing ON:1. **model-based Reinforcement Learning**- World Models and Environment Dynamics- Model-predictive Control (mpc)- Planning with Learned Models- Dyna-q and Model-based Policy OPTIMIZATION2. **hierarchical Reinforcement Learning**- Options Framework- Hierarchical Actor-critic (hac)- Goal-conditioned Rl- Feudal NETWORKS3. **advanced Planning and Control**- Monte Carlo Tree Search (mcts)- Model-based Value Expansion- Latent Space Planning### Learning Objectives- Understand Model-based Rl Principles and Implementation- Master Hierarchical Decomposition in Rl- Implement Advanced Planning Algorithms- Apply These Methods to Complex Control Tasks---](#ca15-advanced-deep-reinforcement-learning---model-based-rl-and-hierarchical-rl-overviewthis-comprehensive-assignment-covers-advanced-topics-in-deep-reinforcement-learning-focusing-on1-model-based-reinforcement-learning--world-models-and-environment-dynamics--model-predictive-control-mpc--planning-with-learned-models--dyna-q-and-model-based-policy-optimization2-hierarchical-reinforcement-learning--options-framework--hierarchical-actor-critic-hac--goal-conditioned-rl--feudal-networks3-advanced-planning-and-control--monte-carlo-tree-search-mcts--model-based-value-expansion--latent-space-planning-learning-objectives--understand-model-based-rl-principles-and-implementation--master-hierarchical-decomposition-in-rl--implement-advanced-planning-algorithms--apply-these-methods-to-complex-control-tasks---)  - [Import Required Librarieswe'll Import Essential Libraries for Implementing Model-based and Hierarchical Rl Algorithms.](#import-required-librarieswell-import-essential-libraries-for-implementing-model-based-and-hierarchical-rl-algorithms)- [Section 1: Model-based Reinforcement Learningmodel-based Rl Learns an Explicit Model of the Environment Dynamics and Uses It for Planning and Control.## 1.1 Theoretical Foundation### Environment Dynamics Modelthe Goal Is to Learn a Transition Model $P(S*{T+1}, R*t | S*t, A*t)$ That Predicts Next States and Rewards.**key Components:**- **deterministic Model**: $S*{T+1} = F(s*t, A*t) + \epsilon$- **stochastic Model**: $S*{T+1} \SIM P(\cdot | S*t, A*t)$- **ensemble Methods**: Multiple Models to Capture Uncertainty### Model-predictive Control (mpc)uses the Learned Model to Plan Actions by Optimizing over a Finite Horizon:$$a^**t = \arg\max*{a*t, \ldots, A*{T+H-1}} \SUM*{K=0}^{H-1} \gamma^k R*{t+k}$$where States Are Predicted Using the Learned Model.### Dyna-q Algorithmcombines Model-free and Model-based LEARNING:1. **direct Rl**: Update Q-function from Real EXPERIENCE2. **planning**: Use Model to Generate Simulated EXPERIENCE3. **model Learning**: Update Dynamics Model from Real Data### Advantages and Challenges**advantages:**- Sample Efficiency through Planning- Can Handle Sparse Rewards- Enables What-if Analysis**challenges:**- Model Bias and Compounding Errors- Computational Complexity- Partial Observability](#section-1-model-based-reinforcement-learningmodel-based-rl-learns-an-explicit-model-of-the-environment-dynamics-and-uses-it-for-planning-and-control-11-theoretical-foundation-environment-dynamics-modelthe-goal-is-to-learn-a-transition-model-pst1-rt--st-at-that-predicts-next-states-and-rewardskey-components--deterministic-model-st1--fst-at--epsilon--stochastic-model-st1-sim-pcdot--st-at--ensemble-methods-multiple-models-to-capture-uncertainty-model-predictive-control-mpcuses-the-learned-model-to-plan-actions-by-optimizing-over-a-finite-horizonat--argmaxat-ldots-ath-1-sumk0h-1-gammak-rtkwhere-states-are-predicted-using-the-learned-model-dyna-q-algorithmcombines-model-free-and-model-based-learning1-direct-rl-update-q-function-from-real-experience2-planning-use-model-to-generate-simulated-experience3-model-learning-update-dynamics-model-from-real-data-advantages-and-challengesadvantages--sample-efficiency-through-planning--can-handle-sparse-rewards--enables-what-if-analysischallenges--model-bias-and-compounding-errors--computational-complexity--partial-observability)- [Section 2: Hierarchical Reinforcement Learninghierarchical Rl Decomposes Complex Tasks into Simpler Subtasks through Temporal and Spatial Abstraction.## 2.1 Theoretical Foundation### Options Frameworkan **option** Is a Closed-loop Policy for Taking Actions over a Period of Time. Formally, an Option Consists Of:- **initiation Set** $I$: States Where the Option Can Be Initiated- **policy** $\pi$: Action Selection within the Option- **termination Condition** $\beta$: Probability of Terminating the Option### Semi-markov Decision Process (smdp)options Extend Mdps to Smdps Where:- Actions Can Take Variable Amounts of Time- Temporal Abstraction Enables Hierarchical Planning- Q-learning over Options: $q(s,o) = R + \gamma^k Q(s', O')$### Goal-conditioned Rllearn Policies Conditioned on Goals: $\pi(a|s,g)$- **hindsight Experience Replay (her)**: Learn from Failed Attempts- **universal Value Function**: $v(s,g)$ for Any Goal $G$- **intrinsic Motivation**: Generate Own Goals for Exploration### Hierarchical Actor-critic (hac)multi-level Hierarchy Where:- **high-level Policy**: Selects Subgoals- **low-level Policy**: Executes Actions to Reach Subgoals- **temporal Abstraction**: Different Time Scales at Each Level### Feudal Networkshierarchical Architecture With:- **manager**: Sets Goals for Workers- **worker**: Executes Actions to Achieve Goals- **feudal Objective**: Manager Maximizes Reward, Worker Maximizes Goal Achievement## 2.2 Key Advantages**sample Efficiency:**- Reuse Learned Skills Across Tasks- Faster Learning through Temporal Abstraction**interpretability:**- Hierarchical Structure Mirrors Human Thinking- Decomposable and Explainable Decisions**transfer Learning:**- Skills Transfer Across Related Environments- Compositional Generalization](#section-2-hierarchical-reinforcement-learninghierarchical-rl-decomposes-complex-tasks-into-simpler-subtasks-through-temporal-and-spatial-abstraction-21-theoretical-foundation-options-frameworkan-option-is-a-closed-loop-policy-for-taking-actions-over-a-period-of-time-formally-an-option-consists-of--initiation-set-i-states-where-the-option-can-be-initiated--policy-pi-action-selection-within-the-option--termination-condition-beta-probability-of-terminating-the-option-semi-markov-decision-process-smdpoptions-extend-mdps-to-smdps-where--actions-can-take-variable-amounts-of-time--temporal-abstraction-enables-hierarchical-planning--q-learning-over-options-qso--r--gammak-qs-o-goal-conditioned-rllearn-policies-conditioned-on-goals-piasg--hindsight-experience-replay-her-learn-from-failed-attempts--universal-value-function-vsg-for-any-goal-g--intrinsic-motivation-generate-own-goals-for-exploration-hierarchical-actor-critic-hacmulti-level-hierarchy-where--high-level-policy-selects-subgoals--low-level-policy-executes-actions-to-reach-subgoals--temporal-abstraction-different-time-scales-at-each-level-feudal-networkshierarchical-architecture-with--manager-sets-goals-for-workers--worker-executes-actions-to-achieve-goals--feudal-objective-manager-maximizes-reward-worker-maximizes-goal-achievement-22-key-advantagessample-efficiency--reuse-learned-skills-across-tasks--faster-learning-through-temporal-abstractioninterpretability--hierarchical-structure-mirrors-human-thinking--decomposable-and-explainable-decisionstransfer-learning--skills-transfer-across-related-environments--compositional-generalization)- [Section 3: Advanced Planning and Controladvanced Planning Algorithms Combine Learned Models with Sophisticated Search Techniques.## 3.1 Monte Carlo Tree Search (mcts)mcts Is a Best-first Search Algorithm That Uses Monte Carlo Simulations for Decision Making.### Mcts Algorithm STEPS:1. **selection**: Navigate Down the Tree Using UCB1 FORMULA2. **expansion**: Add New Child Nodes to the TREE3. **simulation**: Run Random Rollouts from Leaf NODES4. **backpropagation**: Update Node Values with Simulation Results### UCB1 Selection FORMULA:$$UCB1(S,A) = Q(s,a) + C \sqrt{\frac{\ln N(s)}{n(s,a)}}$$where:- $q(s,a)$: Average Reward for Action $A$ in State $S$- $n(s)$: Visit Count for State $S$- $n(s,a)$: Visit Count for Action $A$ in State $S$- $C$: Exploration Constant### Alphazero Integrationcombines Mcts with Neural Networks:- **policy Network**: $p(a|s)$ Guides Selection- **value Network**: $v(s)$ Estimates Leaf Values- **self-play**: Generates Training Data through Mcts Games## 3.2 Model-based Value Expansion (mve)uses Learned Models to Expand Value Function Estimates:$$v*{mve}(s) = \max*a \left[ R(s,a) + \gamma \sum*{s'} P(s'|s,a) V(s') \right]$$### Trajectory Optimization- **cross-entropy Method (cem)**: Iterative Sampling and Fitting- **random Shooting**: Sample Multiple Action Sequences- **model Predictive Path Integral (mppi)**: Information-theoretic Approach## 3.3 Latent Space Planningplanning in Learned Latent Representations:### World Models ARCHITECTURE:1. **vision Model (v)**: Encodes Observations to Latent STATES2. **memory Model (m)**: Predicts Next Latent States 3. **controller Model (c)**: Maps Latent States to Actions### Planet Algorithm:- **recurrent State Space Model (rssm)**:- Deterministic Path: $H*T = F(H*{T-1}, A*{T-1})$- Stochastic Path: $S*T \SIM P(s*t | H_t)$- **planning**: Cross-entropy Method in Latent Space- **learning**: Variational Inference for World Model## 3.4 Challenges and Solutions### Model Bias- **problem**: Learned Models Have Prediction Errors- **solutions**: - Model Ensembles for Uncertainty Quantification- Conservative Planning with Uncertainty Penalties- Robust Optimization Techniques### Computational Complexity- **problem**: Planning Is Computationally Expensive- **solutions**:- Hierarchical Planning with Multiple Time Scales- Approximate Planning with Limited Horizons- Parallel Monte Carlo Simulations### Exploration Vs Exploitation- **problem**: Balancing Exploration and Exploitation in Planning- **solutions**:- Ucb-based Selection in Mcts- Optimistic Initialization- Information-gain Based Rewards](#section-3-advanced-planning-and-controladvanced-planning-algorithms-combine-learned-models-with-sophisticated-search-techniques-31-monte-carlo-tree-search-mctsmcts-is-a-best-first-search-algorithm-that-uses-monte-carlo-simulations-for-decision-making-mcts-algorithm-steps1-selection-navigate-down-the-tree-using-ucb1-formula2-expansion-add-new-child-nodes-to-the-tree3-simulation-run-random-rollouts-from-leaf-nodes4-backpropagation-update-node-values-with-simulation-results-ucb1-selection-formulaucb1sa--qsa--c-sqrtfracln-nsnsawhere--qsa-average-reward-for-action-a-in-state-s--ns-visit-count-for-state-s--nsa-visit-count-for-action-a-in-state-s--c-exploration-constant-alphazero-integrationcombines-mcts-with-neural-networks--policy-network-pas-guides-selection--value-network-vs-estimates-leaf-values--self-play-generates-training-data-through-mcts-games-32-model-based-value-expansion-mveuses-learned-models-to-expand-value-function-estimatesvmves--maxa-left-rsa--gamma-sums-pssa-vs-right-trajectory-optimization--cross-entropy-method-cem-iterative-sampling-and-fitting--random-shooting-sample-multiple-action-sequences--model-predictive-path-integral-mppi-information-theoretic-approach-33-latent-space-planningplanning-in-learned-latent-representations-world-models-architecture1-vision-model-v-encodes-observations-to-latent-states2-memory-model-m-predicts-next-latent-states-3-controller-model-c-maps-latent-states-to-actions-planet-algorithm--recurrent-state-space-model-rssm--deterministic-path-ht--fht-1-at-1--stochastic-path-st-sim-pst--h_t--planning-cross-entropy-method-in-latent-space--learning-variational-inference-for-world-model-34-challenges-and-solutions-model-bias--problem-learned-models-have-prediction-errors--solutions---model-ensembles-for-uncertainty-quantification--conservative-planning-with-uncertainty-penalties--robust-optimization-techniques-computational-complexity--problem-planning-is-computationally-expensive--solutions--hierarchical-planning-with-multiple-time-scales--approximate-planning-with-limited-horizons--parallel-monte-carlo-simulations-exploration-vs-exploitation--problem-balancing-exploration-and-exploitation-in-planning--solutions--ucb-based-selection-in-mcts--optimistic-initialization--information-gain-based-rewards)- [Section 4: Practical Demonstrations and Experimentsthis Section Provides Hands-on Experiments to Demonstrate the Concepts and Implementations.## 4.1 Experiment Setupwe'll Create Practical Experiments to SHOWCASE:1. **model-based Vs Model-free Comparison**- Sample Efficiency Analysis- Performance on Different Environments- Computational Overhead COMPARISON2. **hierarchical Rl Benefits**- Multi-goal Navigation Tasks- Skill Reuse and Transfer- Temporal Abstraction ADVANTAGES3. **planning Algorithm Comparison**- Mcts Vs Random Rollouts- Value Expansion Effectiveness- Latent Space Planning BENEFITS4. **integration Study**- Combining All Methods- Real-world Application Scenarios- Performance Analysis and Trade-offs## 4.2 Metrics and Evaluation### Performance Metrics:- **sample Efficiency**: Steps to Reach Performance Threshold- **asymptotic Performance**: Final Average Reward- **computation Time**: Planning and Learning Overhead- **memory Usage**: Model Storage Requirements- **transfer Performance**: Success on Related Tasks### Statistical Analysis:- Multiple Random Seeds for Reliability- Confidence Intervals and Significance Tests- Learning Curve Analysis- Ablation Studies for Each Component## 4.3 Environments for Testing### Simple Grid World:- **purpose**: Basic Concept Demonstration- **features**: Discrete States, Clear Visualization- **challenges**: Navigation, Goal Reaching### Continuous Control:- **purpose**: Real-world Applicability- **features**: Continuous State-action Spaces- **challenges**: Precise Control, Dynamic Systems### Hierarchical Tasks:- **purpose**: Multi-level Decision Making- **features**: Natural Task Decomposition- **challenges**: Long-horizon Planning, Skill Coordination](#section-4-practical-demonstrations-and-experimentsthis-section-provides-hands-on-experiments-to-demonstrate-the-concepts-and-implementations-41-experiment-setupwell-create-practical-experiments-to-showcase1-model-based-vs-model-free-comparison--sample-efficiency-analysis--performance-on-different-environments--computational-overhead-comparison2-hierarchical-rl-benefits--multi-goal-navigation-tasks--skill-reuse-and-transfer--temporal-abstraction-advantages3-planning-algorithm-comparison--mcts-vs-random-rollouts--value-expansion-effectiveness--latent-space-planning-benefits4-integration-study--combining-all-methods--real-world-application-scenarios--performance-analysis-and-trade-offs-42-metrics-and-evaluation-performance-metrics--sample-efficiency-steps-to-reach-performance-threshold--asymptotic-performance-final-average-reward--computation-time-planning-and-learning-overhead--memory-usage-model-storage-requirements--transfer-performance-success-on-related-tasks-statistical-analysis--multiple-random-seeds-for-reliability--confidence-intervals-and-significance-tests--learning-curve-analysis--ablation-studies-for-each-component-43-environments-for-testing-simple-grid-world--purpose-basic-concept-demonstration--features-discrete-states-clear-visualization--challenges-navigation-goal-reaching-continuous-control--purpose-real-world-applicability--features-continuous-state-action-spaces--challenges-precise-control-dynamic-systems-hierarchical-tasks--purpose-multi-level-decision-making--features-natural-task-decomposition--challenges-long-horizon-planning-skill-coordination)](#table-of-contents--ca15-advanced-deep-reinforcement-learning---model-based-rl-and-hierarchical-rl-overviewthis-comprehensive-assignment-covers-advanced-topics-in-deep-reinforcement-learning-focusing-on1-model-based-reinforcement-learning--world-models-and-environment-dynamics--model-predictive-control-mpc--planning-with-learned-models--dyna-q-and-model-based-policy-optimization2-hierarchical-reinforcement-learning--options-framework--hierarchical-actor-critic-hac--goal-conditioned-rl--feudal-networks3-advanced-planning-and-control--monte-carlo-tree-search-mcts--model-based-value-expansion--latent-space-planning-learning-objectives--understand-model-based-rl-principles-and-implementation--master-hierarchical-decomposition-in-rl--implement-advanced-planning-algorithms--apply-these-methods-to-complex-control-tasks---ca15-advanced-deep-reinforcement-learning---model-based-rl-and-hierarchical-rl-overviewthis-comprehensive-assignment-covers-advanced-topics-in-deep-reinforcement-learning-focusing-on1-model-based-reinforcement-learning--world-models-and-environment-dynamics--model-predictive-control-mpc--planning-with-learned-models--dyna-q-and-model-based-policy-optimization2-hierarchical-reinforcement-learning--options-framework--hierarchical-actor-critic-hac--goal-conditioned-rl--feudal-networks3-advanced-planning-and-control--monte-carlo-tree-search-mcts--model-based-value-expansion--latent-space-planning-learning-objectives--understand-model-based-rl-principles-and-implementation--master-hierarchical-decomposition-in-rl--implement-advanced-planning-algorithms--apply-these-methods-to-complex-control-tasks-------import-required-librarieswell-import-essential-libraries-for-implementing-model-based-and-hierarchical-rl-algorithmsimport-required-librarieswell-import-essential-libraries-for-implementing-model-based-and-hierarchical-rl-algorithms--section-1-model-based-reinforcement-learningmodel-based-rl-learns-an-explicit-model-of-the-environment-dynamics-and-uses-it-for-planning-and-control-11-theoretical-foundation-environment-dynamics-modelthe-goal-is-to-learn-a-transition-model-pst1-rt--st-at-that-predicts-next-states-and-rewardskey-components--deterministic-model-st1--fst-at--epsilon--stochastic-model-st1-sim-pcdot--st-at--ensemble-methods-multiple-models-to-capture-uncertainty-model-predictive-control-mpcuses-the-learned-model-to-plan-actions-by-optimizing-over-a-finite-horizonat--argmaxat-ldots-ath-1-sumk0h-1-gammak-rtkwhere-states-are-predicted-using-the-learned-model-dyna-q-algorithmcombines-model-free-and-model-based-learning1-direct-rl-update-q-function-from-real-experience2-planning-use-model-to-generate-simulated-experience3-model-learning-update-dynamics-model-from-real-data-advantages-and-challengesadvantages--sample-efficiency-through-planning--can-handle-sparse-rewards--enables-what-if-analysischallenges--model-bias-and-compounding-errors--computational-complexity--partial-observabilitysection-1-model-based-reinforcement-learningmodel-based-rl-learns-an-explicit-model-of-the-environment-dynamics-and-uses-it-for-planning-and-control-11-theoretical-foundation-environment-dynamics-modelthe-goal-is-to-learn-a-transition-model-pst1-rt--st-at-that-predicts-next-states-and-rewardskey-components--deterministic-model-st1--fst-at--epsilon--stochastic-model-st1-sim-pcdot--st-at--ensemble-methods-multiple-models-to-capture-uncertainty-model-predictive-control-mpcuses-the-learned-model-to-plan-actions-by-optimizing-over-a-finite-horizonat--argmaxat-ldots-ath-1-sumk0h-1-gammak-rtkwhere-states-are-predicted-using-the-learned-model-dyna-q-algorithmcombines-model-free-and-model-based-learning1-direct-rl-update-q-function-from-real-experience2-planning-use-model-to-generate-simulated-experience3-model-learning-update-dynamics-model-from-real-data-advantages-and-challengesadvantages--sample-efficiency-through-planning--can-handle-sparse-rewards--enables-what-if-analysischallenges--model-bias-and-compounding-errors--computational-complexity--partial-observability--section-2-hierarchical-reinforcement-learninghierarchical-rl-decomposes-complex-tasks-into-simpler-subtasks-through-temporal-and-spatial-abstraction-21-theoretical-foundation-options-frameworkan-option-is-a-closed-loop-policy-for-taking-actions-over-a-period-of-time-formally-an-option-consists-of--initiation-set-i-states-where-the-option-can-be-initiated--policy-pi-action-selection-within-the-option--termination-condition-beta-probability-of-terminating-the-option-semi-markov-decision-process-smdpoptions-extend-mdps-to-smdps-where--actions-can-take-variable-amounts-of-time--temporal-abstraction-enables-hierarchical-planning--q-learning-over-options-qso--r--gammak-qs-o-goal-conditioned-rllearn-policies-conditioned-on-goals-piasg--hindsight-experience-replay-her-learn-from-failed-attempts--universal-value-function-vsg-for-any-goal-g--intrinsic-motivation-generate-own-goals-for-exploration-hierarchical-actor-critic-hacmulti-level-hierarchy-where--high-level-policy-selects-subgoals--low-level-policy-executes-actions-to-reach-subgoals--temporal-abstraction-different-time-scales-at-each-level-feudal-networkshierarchical-architecture-with--manager-sets-goals-for-workers--worker-executes-actions-to-achieve-goals--feudal-objective-manager-maximizes-reward-worker-maximizes-goal-achievement-22-key-advantagessample-efficiency--reuse-learned-skills-across-tasks--faster-learning-through-temporal-abstractioninterpretability--hierarchical-structure-mirrors-human-thinking--decomposable-and-explainable-decisionstransfer-learning--skills-transfer-across-related-environments--compositional-generalizationsection-2-hierarchical-reinforcement-learninghierarchical-rl-decomposes-complex-tasks-into-simpler-subtasks-through-temporal-and-spatial-abstraction-21-theoretical-foundation-options-frameworkan-option-is-a-closed-loop-policy-for-taking-actions-over-a-period-of-time-formally-an-option-consists-of--initiation-set-i-states-where-the-option-can-be-initiated--policy-pi-action-selection-within-the-option--termination-condition-beta-probability-of-terminating-the-option-semi-markov-decision-process-smdpoptions-extend-mdps-to-smdps-where--actions-can-take-variable-amounts-of-time--temporal-abstraction-enables-hierarchical-planning--q-learning-over-options-qso--r--gammak-qs-o-goal-conditioned-rllearn-policies-conditioned-on-goals-piasg--hindsight-experience-replay-her-learn-from-failed-attempts--universal-value-function-vsg-for-any-goal-g--intrinsic-motivation-generate-own-goals-for-exploration-hierarchical-actor-critic-hacmulti-level-hierarchy-where--high-level-policy-selects-subgoals--low-level-policy-executes-actions-to-reach-subgoals--temporal-abstraction-different-time-scales-at-each-level-feudal-networkshierarchical-architecture-with--manager-sets-goals-for-workers--worker-executes-actions-to-achieve-goals--feudal-objective-manager-maximizes-reward-worker-maximizes-goal-achievement-22-key-advantagessample-efficiency--reuse-learned-skills-across-tasks--faster-learning-through-temporal-abstractioninterpretability--hierarchical-structure-mirrors-human-thinking--decomposable-and-explainable-decisionstransfer-learning--skills-transfer-across-related-environments--compositional-generalization--section-3-advanced-planning-and-controladvanced-planning-algorithms-combine-learned-models-with-sophisticated-search-techniques-31-monte-carlo-tree-search-mctsmcts-is-a-best-first-search-algorithm-that-uses-monte-carlo-simulations-for-decision-making-mcts-algorithm-steps1-selection-navigate-down-the-tree-using-ucb1-formula2-expansion-add-new-child-nodes-to-the-tree3-simulation-run-random-rollouts-from-leaf-nodes4-backpropagation-update-node-values-with-simulation-results-ucb1-selection-formulaucb1sa--qsa--c-sqrtfracln-nsnsawhere--qsa-average-reward-for-action-a-in-state-s--ns-visit-count-for-state-s--nsa-visit-count-for-action-a-in-state-s--c-exploration-constant-alphazero-integrationcombines-mcts-with-neural-networks--policy-network-pas-guides-selection--value-network-vs-estimates-leaf-values--self-play-generates-training-data-through-mcts-games-32-model-based-value-expansion-mveuses-learned-models-to-expand-value-function-estimatesvmves--maxa-left-rsa--gamma-sums-pssa-vs-right-trajectory-optimization--cross-entropy-method-cem-iterative-sampling-and-fitting--random-shooting-sample-multiple-action-sequences--model-predictive-path-integral-mppi-information-theoretic-approach-33-latent-space-planningplanning-in-learned-latent-representations-world-models-architecture1-vision-model-v-encodes-observations-to-latent-states2-memory-model-m-predicts-next-latent-states-3-controller-model-c-maps-latent-states-to-actions-planet-algorithm--recurrent-state-space-model-rssm--deterministic-path-ht--fht-1-at-1--stochastic-path-st-sim-pst--h_t--planning-cross-entropy-method-in-latent-space--learning-variational-inference-for-world-model-34-challenges-and-solutions-model-bias--problem-learned-models-have-prediction-errors--solutions---model-ensembles-for-uncertainty-quantification--conservative-planning-with-uncertainty-penalties--robust-optimization-techniques-computational-complexity--problem-planning-is-computationally-expensive--solutions--hierarchical-planning-with-multiple-time-scales--approximate-planning-with-limited-horizons--parallel-monte-carlo-simulations-exploration-vs-exploitation--problem-balancing-exploration-and-exploitation-in-planning--solutions--ucb-based-selection-in-mcts--optimistic-initialization--information-gain-based-rewardssection-3-advanced-planning-and-controladvanced-planning-algorithms-combine-learned-models-with-sophisticated-search-techniques-31-monte-carlo-tree-search-mctsmcts-is-a-best-first-search-algorithm-that-uses-monte-carlo-simulations-for-decision-making-mcts-algorithm-steps1-selection-navigate-down-the-tree-using-ucb1-formula2-expansion-add-new-child-nodes-to-the-tree3-simulation-run-random-rollouts-from-leaf-nodes4-backpropagation-update-node-values-with-simulation-results-ucb1-selection-formulaucb1sa--qsa--c-sqrtfracln-nsnsawhere--qsa-average-reward-for-action-a-in-state-s--ns-visit-count-for-state-s--nsa-visit-count-for-action-a-in-state-s--c-exploration-constant-alphazero-integrationcombines-mcts-with-neural-networks--policy-network-pas-guides-selection--value-network-vs-estimates-leaf-values--self-play-generates-training-data-through-mcts-games-32-model-based-value-expansion-mveuses-learned-models-to-expand-value-function-estimatesvmves--maxa-left-rsa--gamma-sums-pssa-vs-right-trajectory-optimization--cross-entropy-method-cem-iterative-sampling-and-fitting--random-shooting-sample-multiple-action-sequences--model-predictive-path-integral-mppi-information-theoretic-approach-33-latent-space-planningplanning-in-learned-latent-representations-world-models-architecture1-vision-model-v-encodes-observations-to-latent-states2-memory-model-m-predicts-next-latent-states-3-controller-model-c-maps-latent-states-to-actions-planet-algorithm--recurrent-state-space-model-rssm--deterministic-path-ht--fht-1-at-1--stochastic-path-st-sim-pst--h_t--planning-cross-entropy-method-in-latent-space--learning-variational-inference-for-world-model-34-challenges-and-solutions-model-bias--problem-learned-models-have-prediction-errors--solutions---model-ensembles-for-uncertainty-quantification--conservative-planning-with-uncertainty-penalties--robust-optimization-techniques-computational-complexity--problem-planning-is-computationally-expensive--solutions--hierarchical-planning-with-multiple-time-scales--approximate-planning-with-limited-horizons--parallel-monte-carlo-simulations-exploration-vs-exploitation--problem-balancing-exploration-and-exploitation-in-planning--solutions--ucb-based-selection-in-mcts--optimistic-initialization--information-gain-based-rewards--section-4-practical-demonstrations-and-experimentsthis-section-provides-hands-on-experiments-to-demonstrate-the-concepts-and-implementations-41-experiment-setupwell-create-practical-experiments-to-showcase1-model-based-vs-model-free-comparison--sample-efficiency-analysis--performance-on-different-environments--computational-overhead-comparison2-hierarchical-rl-benefits--multi-goal-navigation-tasks--skill-reuse-and-transfer--temporal-abstraction-advantages3-planning-algorithm-comparison--mcts-vs-random-rollouts--value-expansion-effectiveness--latent-space-planning-benefits4-integration-study--combining-all-methods--real-world-application-scenarios--performance-analysis-and-trade-offs-42-metrics-and-evaluation-performance-metrics--sample-efficiency-steps-to-reach-performance-threshold--asymptotic-performance-final-average-reward--computation-time-planning-and-learning-overhead--memory-usage-model-storage-requirements--transfer-performance-success-on-related-tasks-statistical-analysis--multiple-random-seeds-for-reliability--confidence-intervals-and-significance-tests--learning-curve-analysis--ablation-studies-for-each-component-43-environments-for-testing-simple-grid-world--purpose-basic-concept-demonstration--features-discrete-states-clear-visualization--challenges-navigation-goal-reaching-continuous-control--purpose-real-world-applicability--features-continuous-state-action-spaces--challenges-precise-control-dynamic-systems-hierarchical-tasks--purpose-multi-level-decision-making--features-natural-task-decomposition--challenges-long-horizon-planning-skill-coordinationsection-4-practical-demonstrations-and-experimentsthis-section-provides-hands-on-experiments-to-demonstrate-the-concepts-and-implementations-41-experiment-setupwell-create-practical-experiments-to-showcase1-model-based-vs-model-free-comparison--sample-efficiency-analysis--performance-on-different-environments--computational-overhead-comparison2-hierarchical-rl-benefits--multi-goal-navigation-tasks--skill-reuse-and-transfer--temporal-abstraction-advantages3-planning-algorithm-comparison--mcts-vs-random-rollouts--value-expansion-effectiveness--latent-space-planning-benefits4-integration-study--combining-all-methods--real-world-application-scenarios--performance-analysis-and-trade-offs-42-metrics-and-evaluation-performance-metrics--sample-efficiency-steps-to-reach-performance-threshold--asymptotic-performance-final-average-reward--computation-time-planning-and-learning-overhead--memory-usage-model-storage-requirements--transfer-performance-success-on-related-tasks-statistical-analysis--multiple-random-seeds-for-reliability--confidence-intervals-and-significance-tests--learning-curve-analysis--ablation-studies-for-each-component-43-environments-for-testing-simple-grid-world--purpose-basic-concept-demonstration--features-discrete-states-clear-visualization--challenges-navigation-goal-reaching-continuous-control--purpose-real-world-applicability--features-continuous-state-action-spaces--challenges-precise-control-dynamic-systems-hierarchical-tasks--purpose-multi-level-decision-making--features-natural-task-decomposition--challenges-long-horizon-planning-skill-coordination)  - [Import Required Librarieswe'll Import Essential Libraries for Implementing Model-based and Hierarchical Rl Algorithms.](#import-required-librarieswell-import-essential-libraries-for-implementing-model-based-and-hierarchical-rl-algorithms)- [Section 1: Model-based Reinforcement Learningmodel-based Rl Learns an Explicit Model of the Environment Dynamics and Uses It for Planning and Control.## 1.1 Theoretical Foundation### Environment Dynamics Modelthe Goal Is to Learn a Transition Model $P(S*{T+1}, R*t | S*t, A*t)$ That Predicts Next States and Rewards.**key Components:**- **deterministic Model**: $S*{T+1} = F(s*t, A*t) + \epsilon$- **stochastic Model**: $S*{T+1} \SIM P(\cdot | S*t, A*t)$- **ensemble Methods**: Multiple Models to Capture Uncertainty### Model-predictive Control (mpc)uses the Learned Model to Plan Actions by Optimizing over a Finite Horizon:$$a^**t = \arg\max*{a*t, \ldots, A*{T+H-1}} \SUM*{K=0}^{H-1} \gamma^k R*{t+k}$$where States Are Predicted Using the Learned Model.### Dyna-q Algorithmcombines Model-free and Model-based LEARNING:1. **direct Rl**: Update Q-function from Real EXPERIENCE2. **planning**: Use Model to Generate Simulated EXPERIENCE3. **model Learning**: Update Dynamics Model from Real Data### Advantages and Challenges**advantages:**- Sample Efficiency through Planning- Can Handle Sparse Rewards- Enables What-if Analysis**challenges:**- Model Bias and Compounding Errors- Computational Complexity- Partial Observability](#section-1-model-based-reinforcement-learningmodel-based-rl-learns-an-explicit-model-of-the-environment-dynamics-and-uses-it-for-planning-and-control-11-theoretical-foundation-environment-dynamics-modelthe-goal-is-to-learn-a-transition-model-pst1-rt--st-at-that-predicts-next-states-and-rewardskey-components--deterministic-model-st1--fst-at--epsilon--stochastic-model-st1-sim-pcdot--st-at--ensemble-methods-multiple-models-to-capture-uncertainty-model-predictive-control-mpcuses-the-learned-model-to-plan-actions-by-optimizing-over-a-finite-horizonat--argmaxat-ldots-ath-1-sumk0h-1-gammak-rtkwhere-states-are-predicted-using-the-learned-model-dyna-q-algorithmcombines-model-free-and-model-based-learning1-direct-rl-update-q-function-from-real-experience2-planning-use-model-to-generate-simulated-experience3-model-learning-update-dynamics-model-from-real-data-advantages-and-challengesadvantages--sample-efficiency-through-planning--can-handle-sparse-rewards--enables-what-if-analysischallenges--model-bias-and-compounding-errors--computational-complexity--partial-observability)- [Section 2: Hierarchical Reinforcement Learninghierarchical Rl Decomposes Complex Tasks into Simpler Subtasks through Temporal and Spatial Abstraction.## 2.1 Theoretical Foundation### Options Frameworkan **option** Is a Closed-loop Policy for Taking Actions over a Period of Time. Formally, an Option Consists Of:- **initiation Set** $I$: States Where the Option Can Be Initiated- **policy** $\pi$: Action Selection within the Option- **termination Condition** $\beta$: Probability of Terminating the Option### Semi-markov Decision Process (smdp)options Extend Mdps to Smdps Where:- Actions Can Take Variable Amounts of Time- Temporal Abstraction Enables Hierarchical Planning- Q-learning over Options: $q(s,o) = R + \gamma^k Q(s', O')$### Goal-conditioned Rllearn Policies Conditioned on Goals: $\pi(a|s,g)$- **hindsight Experience Replay (her)**: Learn from Failed Attempts- **universal Value Function**: $v(s,g)$ for Any Goal $G$- **intrinsic Motivation**: Generate Own Goals for Exploration### Hierarchical Actor-critic (hac)multi-level Hierarchy Where:- **high-level Policy**: Selects Subgoals- **low-level Policy**: Executes Actions to Reach Subgoals- **temporal Abstraction**: Different Time Scales at Each Level### Feudal Networkshierarchical Architecture With:- **manager**: Sets Goals for Workers- **worker**: Executes Actions to Achieve Goals- **feudal Objective**: Manager Maximizes Reward, Worker Maximizes Goal Achievement## 2.2 Key Advantages**sample Efficiency:**- Reuse Learned Skills Across Tasks- Faster Learning through Temporal Abstraction**interpretability:**- Hierarchical Structure Mirrors Human Thinking- Decomposable and Explainable Decisions**transfer Learning:**- Skills Transfer Across Related Environments- Compositional Generalization](#section-2-hierarchical-reinforcement-learninghierarchical-rl-decomposes-complex-tasks-into-simpler-subtasks-through-temporal-and-spatial-abstraction-21-theoretical-foundation-options-frameworkan-option-is-a-closed-loop-policy-for-taking-actions-over-a-period-of-time-formally-an-option-consists-of--initiation-set-i-states-where-the-option-can-be-initiated--policy-pi-action-selection-within-the-option--termination-condition-beta-probability-of-terminating-the-option-semi-markov-decision-process-smdpoptions-extend-mdps-to-smdps-where--actions-can-take-variable-amounts-of-time--temporal-abstraction-enables-hierarchical-planning--q-learning-over-options-qso--r--gammak-qs-o-goal-conditioned-rllearn-policies-conditioned-on-goals-piasg--hindsight-experience-replay-her-learn-from-failed-attempts--universal-value-function-vsg-for-any-goal-g--intrinsic-motivation-generate-own-goals-for-exploration-hierarchical-actor-critic-hacmulti-level-hierarchy-where--high-level-policy-selects-subgoals--low-level-policy-executes-actions-to-reach-subgoals--temporal-abstraction-different-time-scales-at-each-level-feudal-networkshierarchical-architecture-with--manager-sets-goals-for-workers--worker-executes-actions-to-achieve-goals--feudal-objective-manager-maximizes-reward-worker-maximizes-goal-achievement-22-key-advantagessample-efficiency--reuse-learned-skills-across-tasks--faster-learning-through-temporal-abstractioninterpretability--hierarchical-structure-mirrors-human-thinking--decomposable-and-explainable-decisionstransfer-learning--skills-transfer-across-related-environments--compositional-generalization)- [Section 3: Advanced Planning and Controladvanced Planning Algorithms Combine Learned Models with Sophisticated Search Techniques.## 3.1 Monte Carlo Tree Search (mcts)mcts Is a Best-first Search Algorithm That Uses Monte Carlo Simulations for Decision Making.### Mcts Algorithm STEPS:1. **selection**: Navigate Down the Tree Using UCB1 FORMULA2. **expansion**: Add New Child Nodes to the TREE3. **simulation**: Run Random Rollouts from Leaf NODES4. **backpropagation**: Update Node Values with Simulation Results### UCB1 Selection FORMULA:$$UCB1(S,A) = Q(s,a) + C \sqrt{\frac{\ln N(s)}{n(s,a)}}$$where:- $q(s,a)$: Average Reward for Action $A$ in State $S$- $n(s)$: Visit Count for State $S$- $n(s,a)$: Visit Count for Action $A$ in State $S$- $C$: Exploration Constant### Alphazero Integrationcombines Mcts with Neural Networks:- **policy Network**: $p(a|s)$ Guides Selection- **value Network**: $v(s)$ Estimates Leaf Values- **self-play**: Generates Training Data through Mcts Games## 3.2 Model-based Value Expansion (mve)uses Learned Models to Expand Value Function Estimates:$$v*{mve}(s) = \max*a \left[ R(s,a) + \gamma \sum*{s'} P(s'|s,a) V(s') \right]$$### Trajectory Optimization- **cross-entropy Method (cem)**: Iterative Sampling and Fitting- **random Shooting**: Sample Multiple Action Sequences- **model Predictive Path Integral (mppi)**: Information-theoretic Approach## 3.3 Latent Space Planningplanning in Learned Latent Representations:### World Models ARCHITECTURE:1. **vision Model (v)**: Encodes Observations to Latent STATES2. **memory Model (m)**: Predicts Next Latent States 3. **controller Model (c)**: Maps Latent States to Actions### Planet Algorithm:- **recurrent State Space Model (rssm)**:- Deterministic Path: $H*T = F(H*{T-1}, A*{T-1})$- Stochastic Path: $S*T \SIM P(s*t | H_t)$- **planning**: Cross-entropy Method in Latent Space- **learning**: Variational Inference for World Model## 3.4 Challenges and Solutions### Model Bias- **problem**: Learned Models Have Prediction Errors- **solutions**: - Model Ensembles for Uncertainty Quantification- Conservative Planning with Uncertainty Penalties- Robust Optimization Techniques### Computational Complexity- **problem**: Planning Is Computationally Expensive- **solutions**:- Hierarchical Planning with Multiple Time Scales- Approximate Planning with Limited Horizons- Parallel Monte Carlo Simulations### Exploration Vs Exploitation- **problem**: Balancing Exploration and Exploitation in Planning- **solutions**:- Ucb-based Selection in Mcts- Optimistic Initialization- Information-gain Based Rewards](#section-3-advanced-planning-and-controladvanced-planning-algorithms-combine-learned-models-with-sophisticated-search-techniques-31-monte-carlo-tree-search-mctsmcts-is-a-best-first-search-algorithm-that-uses-monte-carlo-simulations-for-decision-making-mcts-algorithm-steps1-selection-navigate-down-the-tree-using-ucb1-formula2-expansion-add-new-child-nodes-to-the-tree3-simulation-run-random-rollouts-from-leaf-nodes4-backpropagation-update-node-values-with-simulation-results-ucb1-selection-formulaucb1sa--qsa--c-sqrtfracln-nsnsawhere--qsa-average-reward-for-action-a-in-state-s--ns-visit-count-for-state-s--nsa-visit-count-for-action-a-in-state-s--c-exploration-constant-alphazero-integrationcombines-mcts-with-neural-networks--policy-network-pas-guides-selection--value-network-vs-estimates-leaf-values--self-play-generates-training-data-through-mcts-games-32-model-based-value-expansion-mveuses-learned-models-to-expand-value-function-estimatesvmves--maxa-left-rsa--gamma-sums-pssa-vs-right-trajectory-optimization--cross-entropy-method-cem-iterative-sampling-and-fitting--random-shooting-sample-multiple-action-sequences--model-predictive-path-integral-mppi-information-theoretic-approach-33-latent-space-planningplanning-in-learned-latent-representations-world-models-architecture1-vision-model-v-encodes-observations-to-latent-states2-memory-model-m-predicts-next-latent-states-3-controller-model-c-maps-latent-states-to-actions-planet-algorithm--recurrent-state-space-model-rssm--deterministic-path-ht--fht-1-at-1--stochastic-path-st-sim-pst--h_t--planning-cross-entropy-method-in-latent-space--learning-variational-inference-for-world-model-34-challenges-and-solutions-model-bias--problem-learned-models-have-prediction-errors--solutions---model-ensembles-for-uncertainty-quantification--conservative-planning-with-uncertainty-penalties--robust-optimization-techniques-computational-complexity--problem-planning-is-computationally-expensive--solutions--hierarchical-planning-with-multiple-time-scales--approximate-planning-with-limited-horizons--parallel-monte-carlo-simulations-exploration-vs-exploitation--problem-balancing-exploration-and-exploitation-in-planning--solutions--ucb-based-selection-in-mcts--optimistic-initialization--information-gain-based-rewards)- [Section 4: Practical Demonstrations and Experimentsthis Section Provides Hands-on Experiments to Demonstrate the Concepts and Implementations.## 4.1 Experiment Setupwe'll Create Practical Experiments to SHOWCASE:1. **model-based Vs Model-free Comparison**- Sample Efficiency Analysis- Performance on Different Environments- Computational Overhead COMPARISON2. **hierarchical Rl Benefits**- Multi-goal Navigation Tasks- Skill Reuse and Transfer- Temporal Abstraction ADVANTAGES3. **planning Algorithm Comparison**- Mcts Vs Random Rollouts- Value Expansion Effectiveness- Latent Space Planning BENEFITS4. **integration Study**- Combining All Methods- Real-world Application Scenarios- Performance Analysis and Trade-offs## 4.2 Metrics and Evaluation### Performance Metrics:- **sample Efficiency**: Steps to Reach Performance Threshold- **asymptotic Performance**: Final Average Reward- **computation Time**: Planning and Learning Overhead- **memory Usage**: Model Storage Requirements- **transfer Performance**: Success on Related Tasks### Statistical Analysis:- Multiple Random Seeds for Reliability- Confidence Intervals and Significance Tests- Learning Curve Analysis- Ablation Studies for Each Component## 4.3 Environments for Testing### Simple Grid World:- **purpose**: Basic Concept Demonstration- **features**: Discrete States, Clear Visualization- **challenges**: Navigation, Goal Reaching### Continuous Control:- **purpose**: Real-world Applicability- **features**: Continuous State-action Spaces- **challenges**: Precise Control, Dynamic Systems### Hierarchical Tasks:- **purpose**: Multi-level Decision Making- **features**: Natural Task Decomposition- **challenges**: Long-horizon Planning, Skill Coordination](#section-4-practical-demonstrations-and-experimentsthis-section-provides-hands-on-experiments-to-demonstrate-the-concepts-and-implementations-41-experiment-setupwell-create-practical-experiments-to-showcase1-model-based-vs-model-free-comparison--sample-efficiency-analysis--performance-on-different-environments--computational-overhead-comparison2-hierarchical-rl-benefits--multi-goal-navigation-tasks--skill-reuse-and-transfer--temporal-abstraction-advantages3-planning-algorithm-comparison--mcts-vs-random-rollouts--value-expansion-effectiveness--latent-space-planning-benefits4-integration-study--combining-all-methods--real-world-application-scenarios--performance-analysis-and-trade-offs-42-metrics-and-evaluation-performance-metrics--sample-efficiency-steps-to-reach-performance-threshold--asymptotic-performance-final-average-reward--computation-time-planning-and-learning-overhead--memory-usage-model-storage-requirements--transfer-performance-success-on-related-tasks-statistical-analysis--multiple-random-seeds-for-reliability--confidence-intervals-and-significance-tests--learning-curve-analysis--ablation-studies-for-each-component-43-environments-for-testing-simple-grid-world--purpose-basic-concept-demonstration--features-discrete-states-clear-visualization--challenges-navigation-goal-reaching-continuous-control--purpose-real-world-applicability--features-continuous-state-action-spaces--challenges-precise-control-dynamic-systems-hierarchical-tasks--purpose-multi-level-decision-making--features-natural-task-decomposition--challenges-long-horizon-planning-skill-coordination)

# Table of Contents- [CA15: Advanced Deep Reinforcement Learning - Model-based Rl and Hierarchical Rl## Overviewthis Comprehensive Assignment Covers Advanced Topics in Deep Reinforcement Learning, Focusing ON:1. **model-based Reinforcement Learning**- World Models and Environment Dynamics- Model-predictive Control (mpc)- Planning with Learned Models- Dyna-q and Model-based Policy OPTIMIZATION2. **hierarchical Reinforcement Learning**- Options Framework- Hierarchical Actor-critic (hac)- Goal-conditioned Rl- Feudal NETWORKS3. **advanced Planning and Control**- Monte Carlo Tree Search (mcts)- Model-based Value Expansion- Latent Space Planning### Learning Objectives- Understand Model-based Rl Principles and Implementation- Master Hierarchical Decomposition in Rl- Implement Advanced Planning Algorithms- Apply These Methods to Complex Control Tasks---](#ca15-advanced-deep-reinforcement-learning---model-based-rl-and-hierarchical-rl-overviewthis-comprehensive-assignment-covers-advanced-topics-in-deep-reinforcement-learning-focusing-on1-model-based-reinforcement-learning--world-models-and-environment-dynamics--model-predictive-control-mpc--planning-with-learned-models--dyna-q-and-model-based-policy-optimization2-hierarchical-reinforcement-learning--options-framework--hierarchical-actor-critic-hac--goal-conditioned-rl--feudal-networks3-advanced-planning-and-control--monte-carlo-tree-search-mcts--model-based-value-expansion--latent-space-planning-learning-objectives--understand-model-based-rl-principles-and-implementation--master-hierarchical-decomposition-in-rl--implement-advanced-planning-algorithms--apply-these-methods-to-complex-control-tasks---)  - [Import Required Librarieswe'll Import Essential Libraries for Implementing Model-based and Hierarchical Rl Algorithms.](#import-required-librarieswell-import-essential-libraries-for-implementing-model-based-and-hierarchical-rl-algorithms)- [Section 1: Model-based Reinforcement Learningmodel-based Rl Learns an Explicit Model of the Environment Dynamics and Uses It for Planning and Control.## 1.1 Theoretical Foundation### Environment Dynamics Modelthe Goal Is to Learn a Transition Model $P(S*{T+1}, R*t | S*t, A*t)$ That Predicts Next States and Rewards.**key Components:**- **deterministic Model**: $S*{T+1} = F(s*t, A*t) + \epsilon$- **stochastic Model**: $S*{T+1} \SIM P(\cdot | S*t, A*t)$- **ensemble Methods**: Multiple Models to Capture Uncertainty### Model-predictive Control (mpc)uses the Learned Model to Plan Actions by Optimizing over a Finite Horizon:$$a^**t = \arg\max*{a*t, \ldots, A*{T+H-1}} \SUM*{K=0}^{H-1} \gamma^k R*{t+k}$$where States Are Predicted Using the Learned Model.### Dyna-q Algorithmcombines Model-free and Model-based LEARNING:1. **direct Rl**: Update Q-function from Real EXPERIENCE2. **planning**: Use Model to Generate Simulated EXPERIENCE3. **model Learning**: Update Dynamics Model from Real Data### Advantages and Challenges**advantages:**- Sample Efficiency through Planning- Can Handle Sparse Rewards- Enables What-if Analysis**challenges:**- Model Bias and Compounding Errors- Computational Complexity- Partial Observability](#section-1-model-based-reinforcement-learningmodel-based-rl-learns-an-explicit-model-of-the-environment-dynamics-and-uses-it-for-planning-and-control-11-theoretical-foundation-environment-dynamics-modelthe-goal-is-to-learn-a-transition-model-pst1-rt--st-at-that-predicts-next-states-and-rewardskey-components--deterministic-model-st1--fst-at--epsilon--stochastic-model-st1-sim-pcdot--st-at--ensemble-methods-multiple-models-to-capture-uncertainty-model-predictive-control-mpcuses-the-learned-model-to-plan-actions-by-optimizing-over-a-finite-horizonat--argmaxat-ldots-ath-1-sumk0h-1-gammak-rtkwhere-states-are-predicted-using-the-learned-model-dyna-q-algorithmcombines-model-free-and-model-based-learning1-direct-rl-update-q-function-from-real-experience2-planning-use-model-to-generate-simulated-experience3-model-learning-update-dynamics-model-from-real-data-advantages-and-challengesadvantages--sample-efficiency-through-planning--can-handle-sparse-rewards--enables-what-if-analysischallenges--model-bias-and-compounding-errors--computational-complexity--partial-observability)- [Section 2: Hierarchical Reinforcement Learninghierarchical Rl Decomposes Complex Tasks into Simpler Subtasks through Temporal and Spatial Abstraction.## 2.1 Theoretical Foundation### Options Frameworkan **option** Is a Closed-loop Policy for Taking Actions over a Period of Time. Formally, an Option Consists Of:- **initiation Set** $I$: States Where the Option Can Be Initiated- **policy** $\pi$: Action Selection within the Option- **termination Condition** $\beta$: Probability of Terminating the Option### Semi-markov Decision Process (smdp)options Extend Mdps to Smdps Where:- Actions Can Take Variable Amounts of Time- Temporal Abstraction Enables Hierarchical Planning- Q-learning over Options: $q(s,o) = R + \gamma^k Q(s', O')$### Goal-conditioned Rllearn Policies Conditioned on Goals: $\pi(a|s,g)$- **hindsight Experience Replay (her)**: Learn from Failed Attempts- **universal Value Function**: $v(s,g)$ for Any Goal $G$- **intrinsic Motivation**: Generate Own Goals for Exploration### Hierarchical Actor-critic (hac)multi-level Hierarchy Where:- **high-level Policy**: Selects Subgoals- **low-level Policy**: Executes Actions to Reach Subgoals- **temporal Abstraction**: Different Time Scales at Each Level### Feudal Networkshierarchical Architecture With:- **manager**: Sets Goals for Workers- **worker**: Executes Actions to Achieve Goals- **feudal Objective**: Manager Maximizes Reward, Worker Maximizes Goal Achievement## 2.2 Key Advantages**sample Efficiency:**- Reuse Learned Skills Across Tasks- Faster Learning through Temporal Abstraction**interpretability:**- Hierarchical Structure Mirrors Human Thinking- Decomposable and Explainable Decisions**transfer Learning:**- Skills Transfer Across Related Environments- Compositional Generalization](#section-2-hierarchical-reinforcement-learninghierarchical-rl-decomposes-complex-tasks-into-simpler-subtasks-through-temporal-and-spatial-abstraction-21-theoretical-foundation-options-frameworkan-option-is-a-closed-loop-policy-for-taking-actions-over-a-period-of-time-formally-an-option-consists-of--initiation-set-i-states-where-the-option-can-be-initiated--policy-pi-action-selection-within-the-option--termination-condition-beta-probability-of-terminating-the-option-semi-markov-decision-process-smdpoptions-extend-mdps-to-smdps-where--actions-can-take-variable-amounts-of-time--temporal-abstraction-enables-hierarchical-planning--q-learning-over-options-qso--r--gammak-qs-o-goal-conditioned-rllearn-policies-conditioned-on-goals-piasg--hindsight-experience-replay-her-learn-from-failed-attempts--universal-value-function-vsg-for-any-goal-g--intrinsic-motivation-generate-own-goals-for-exploration-hierarchical-actor-critic-hacmulti-level-hierarchy-where--high-level-policy-selects-subgoals--low-level-policy-executes-actions-to-reach-subgoals--temporal-abstraction-different-time-scales-at-each-level-feudal-networkshierarchical-architecture-with--manager-sets-goals-for-workers--worker-executes-actions-to-achieve-goals--feudal-objective-manager-maximizes-reward-worker-maximizes-goal-achievement-22-key-advantagessample-efficiency--reuse-learned-skills-across-tasks--faster-learning-through-temporal-abstractioninterpretability--hierarchical-structure-mirrors-human-thinking--decomposable-and-explainable-decisionstransfer-learning--skills-transfer-across-related-environments--compositional-generalization)- [Section 3: Advanced Planning and Controladvanced Planning Algorithms Combine Learned Models with Sophisticated Search Techniques.## 3.1 Monte Carlo Tree Search (mcts)mcts Is a Best-first Search Algorithm That Uses Monte Carlo Simulations for Decision Making.### Mcts Algorithm STEPS:1. **selection**: Navigate Down the Tree Using UCB1 FORMULA2. **expansion**: Add New Child Nodes to the TREE3. **simulation**: Run Random Rollouts from Leaf NODES4. **backpropagation**: Update Node Values with Simulation Results### UCB1 Selection FORMULA:$$UCB1(S,A) = Q(s,a) + C \sqrt{\frac{\ln N(s)}{n(s,a)}}$$where:- $q(s,a)$: Average Reward for Action $A$ in State $S$- $n(s)$: Visit Count for State $S$- $n(s,a)$: Visit Count for Action $A$ in State $S$- $C$: Exploration Constant### Alphazero Integrationcombines Mcts with Neural Networks:- **policy Network**: $p(a|s)$ Guides Selection- **value Network**: $v(s)$ Estimates Leaf Values- **self-play**: Generates Training Data through Mcts Games## 3.2 Model-based Value Expansion (mve)uses Learned Models to Expand Value Function Estimates:$$v*{mve}(s) = \max*a \left[ R(s,a) + \gamma \sum*{s'} P(s'|s,a) V(s') \right]$$### Trajectory Optimization- **cross-entropy Method (cem)**: Iterative Sampling and Fitting- **random Shooting**: Sample Multiple Action Sequences- **model Predictive Path Integral (mppi)**: Information-theoretic Approach## 3.3 Latent Space Planningplanning in Learned Latent Representations:### World Models ARCHITECTURE:1. **vision Model (v)**: Encodes Observations to Latent STATES2. **memory Model (m)**: Predicts Next Latent States 3. **controller Model (c)**: Maps Latent States to Actions### Planet Algorithm:- **recurrent State Space Model (rssm)**:- Deterministic Path: $H*T = F(H*{T-1}, A*{T-1})$- Stochastic Path: $S*T \SIM P(s*t | H_t)$- **planning**: Cross-entropy Method in Latent Space- **learning**: Variational Inference for World Model## 3.4 Challenges and Solutions### Model Bias- **problem**: Learned Models Have Prediction Errors- **solutions**: - Model Ensembles for Uncertainty Quantification- Conservative Planning with Uncertainty Penalties- Robust Optimization Techniques### Computational Complexity- **problem**: Planning Is Computationally Expensive- **solutions**:- Hierarchical Planning with Multiple Time Scales- Approximate Planning with Limited Horizons- Parallel Monte Carlo Simulations### Exploration Vs Exploitation- **problem**: Balancing Exploration and Exploitation in Planning- **solutions**:- Ucb-based Selection in Mcts- Optimistic Initialization- Information-gain Based Rewards](#section-3-advanced-planning-and-controladvanced-planning-algorithms-combine-learned-models-with-sophisticated-search-techniques-31-monte-carlo-tree-search-mctsmcts-is-a-best-first-search-algorithm-that-uses-monte-carlo-simulations-for-decision-making-mcts-algorithm-steps1-selection-navigate-down-the-tree-using-ucb1-formula2-expansion-add-new-child-nodes-to-the-tree3-simulation-run-random-rollouts-from-leaf-nodes4-backpropagation-update-node-values-with-simulation-results-ucb1-selection-formulaucb1sa--qsa--c-sqrtfracln-nsnsawhere--qsa-average-reward-for-action-a-in-state-s--ns-visit-count-for-state-s--nsa-visit-count-for-action-a-in-state-s--c-exploration-constant-alphazero-integrationcombines-mcts-with-neural-networks--policy-network-pas-guides-selection--value-network-vs-estimates-leaf-values--self-play-generates-training-data-through-mcts-games-32-model-based-value-expansion-mveuses-learned-models-to-expand-value-function-estimatesvmves--maxa-left-rsa--gamma-sums-pssa-vs-right-trajectory-optimization--cross-entropy-method-cem-iterative-sampling-and-fitting--random-shooting-sample-multiple-action-sequences--model-predictive-path-integral-mppi-information-theoretic-approach-33-latent-space-planningplanning-in-learned-latent-representations-world-models-architecture1-vision-model-v-encodes-observations-to-latent-states2-memory-model-m-predicts-next-latent-states-3-controller-model-c-maps-latent-states-to-actions-planet-algorithm--recurrent-state-space-model-rssm--deterministic-path-ht--fht-1-at-1--stochastic-path-st-sim-pst--h_t--planning-cross-entropy-method-in-latent-space--learning-variational-inference-for-world-model-34-challenges-and-solutions-model-bias--problem-learned-models-have-prediction-errors--solutions---model-ensembles-for-uncertainty-quantification--conservative-planning-with-uncertainty-penalties--robust-optimization-techniques-computational-complexity--problem-planning-is-computationally-expensive--solutions--hierarchical-planning-with-multiple-time-scales--approximate-planning-with-limited-horizons--parallel-monte-carlo-simulations-exploration-vs-exploitation--problem-balancing-exploration-and-exploitation-in-planning--solutions--ucb-based-selection-in-mcts--optimistic-initialization--information-gain-based-rewards)- [Section 4: Practical Demonstrations and Experimentsthis Section Provides Hands-on Experiments to Demonstrate the Concepts and Implementations.## 4.1 Experiment Setupwe'll Create Practical Experiments to SHOWCASE:1. **model-based Vs Model-free Comparison**- Sample Efficiency Analysis- Performance on Different Environments- Computational Overhead COMPARISON2. **hierarchical Rl Benefits**- Multi-goal Navigation Tasks- Skill Reuse and Transfer- Temporal Abstraction ADVANTAGES3. **planning Algorithm Comparison**- Mcts Vs Random Rollouts- Value Expansion Effectiveness- Latent Space Planning BENEFITS4. **integration Study**- Combining All Methods- Real-world Application Scenarios- Performance Analysis and Trade-offs## 4.2 Metrics and Evaluation### Performance Metrics:- **sample Efficiency**: Steps to Reach Performance Threshold- **asymptotic Performance**: Final Average Reward- **computation Time**: Planning and Learning Overhead- **memory Usage**: Model Storage Requirements- **transfer Performance**: Success on Related Tasks### Statistical Analysis:- Multiple Random Seeds for Reliability- Confidence Intervals and Significance Tests- Learning Curve Analysis- Ablation Studies for Each Component## 4.3 Environments for Testing### Simple Grid World:- **purpose**: Basic Concept Demonstration- **features**: Discrete States, Clear Visualization- **challenges**: Navigation, Goal Reaching### Continuous Control:- **purpose**: Real-world Applicability- **features**: Continuous State-action Spaces- **challenges**: Precise Control, Dynamic Systems### Hierarchical Tasks:- **purpose**: Multi-level Decision Making- **features**: Natural Task Decomposition- **challenges**: Long-horizon Planning, Skill Coordination](#section-4-practical-demonstrations-and-experimentsthis-section-provides-hands-on-experiments-to-demonstrate-the-concepts-and-implementations-41-experiment-setupwell-create-practical-experiments-to-showcase1-model-based-vs-model-free-comparison--sample-efficiency-analysis--performance-on-different-environments--computational-overhead-comparison2-hierarchical-rl-benefits--multi-goal-navigation-tasks--skill-reuse-and-transfer--temporal-abstraction-advantages3-planning-algorithm-comparison--mcts-vs-random-rollouts--value-expansion-effectiveness--latent-space-planning-benefits4-integration-study--combining-all-methods--real-world-application-scenarios--performance-analysis-and-trade-offs-42-metrics-and-evaluation-performance-metrics--sample-efficiency-steps-to-reach-performance-threshold--asymptotic-performance-final-average-reward--computation-time-planning-and-learning-overhead--memory-usage-model-storage-requirements--transfer-performance-success-on-related-tasks-statistical-analysis--multiple-random-seeds-for-reliability--confidence-intervals-and-significance-tests--learning-curve-analysis--ablation-studies-for-each-component-43-environments-for-testing-simple-grid-world--purpose-basic-concept-demonstration--features-discrete-states-clear-visualization--challenges-navigation-goal-reaching-continuous-control--purpose-real-world-applicability--features-continuous-state-action-spaces--challenges-precise-control-dynamic-systems-hierarchical-tasks--purpose-multi-level-decision-making--features-natural-task-decomposition--challenges-long-horizon-planning-skill-coordination)

## Import Required Librarieswe'll Import Essential Libraries for Implementing Model-based and Hierarchical Rl Algorithms.


```python
# Essential libraries
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical, Normal

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import deque, namedtuple
import random
import copy
import math
import gym
from typing import List, Dict, Tuple, Optional, Union
import warnings
warnings.filterwarnings('ignore')

# Set random seeds for reproducibility
np.random.seed(42)
torch.manual_seed(42)
random.seed(42)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Configuration for advanced RL methods
MODEL_BASED_CONFIG = {
    'model_lr': 1e-3,
    'planning_horizon': 10,
    'model_ensemble_size': 5,
    'imagination_rollouts': 100,
    'model_training_freq': 10
}

HIERARCHICAL_CONFIG = {
    'num_levels': 3,
    'option_timeout': 20,
    'subgoal_threshold': 0.1,
    'meta_controller_lr': 3e-4,
    'controller_lr': 1e-3
}

PLANNING_CONFIG = {
    'mcts_simulations': 100,
    'exploration_constant': 1.4,
    'planning_depth': 5,
    'beam_width': 10
}

print("🚀 Libraries imported successfully!")
print("📊 Configurations loaded for Model-Based and Hierarchical RL")
```

# Section 1: Model-based Reinforcement Learningmodel-based Rl Learns an Explicit Model of the Environment Dynamics and Uses It for Planning and Control.## 1.1 Theoretical Foundation### Environment Dynamics Modelthe Goal Is to Learn a Transition Model $P(S*{T+1}, R*t | S*t, A*t)$ That Predicts Next States and Rewards.**key Components:**- **deterministic Model**: $S*{T+1} = F(s*t, A*t) + \epsilon$- **stochastic Model**: $S*{T+1} \SIM P(\cdot | S*t, A*t)$- **ensemble Methods**: Multiple Models to Capture Uncertainty### Model-predictive Control (mpc)uses the Learned Model to Plan Actions by Optimizing over a Finite Horizon:$$a^**t = \arg\max*{a*t, \ldots, A*{T+H-1}} \SUM*{K=0}^{H-1} \gamma^k R*{t+k}$$where States Are Predicted Using the Learned Model.### Dyna-q Algorithmcombines Model-free and Model-based LEARNING:1. **direct Rl**: Update Q-function from Real EXPERIENCE2. **planning**: Use Model to Generate Simulated EXPERIENCE3. **model Learning**: Update Dynamics Model from Real Data### Advantages and Challenges**advantages:**- Sample Efficiency through Planning- Can Handle Sparse Rewards- Enables What-if Analysis**challenges:**- Model Bias and Compounding Errors- Computational Complexity- Partial Observability


```python
# Model-Based RL Implementation

class DynamicsModel(nn.Module):
    """Neural network model for environment dynamics."""
    
    def __init__(self, state_dim, action_dim, hidden_dim=256):
        super(DynamicsModel, self).__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Transition model: (state, action) -> (next_state, reward)
        self.transition_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, state_dim + 1)  # next_state + reward
        )
        
        # Uncertainty estimation
        self.uncertainty_net = nn.Sequential(
            nn.Linear(state_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, state_dim + 1),  # Uncertainty for state + reward
            nn.Softplus()  # Ensure positive uncertainty
        )
    
    def forward(self, state, action):
        """Predict next state and reward with uncertainty."""
        if len(state.shape) == 1:
            state = state.unsqueeze(0)
        if len(action.shape) == 1:
            action = action.unsqueeze(0)
        
        # Handle discrete actions
        if action.dtype == torch.long:
            action_one_hot = torch.zeros(action.size(0), self.action_dim).to(action.device)
            action_one_hot.scatter_(1, action.unsqueeze(1), 1)
            action = action_one_hot
        
        input_tensor = torch.cat([state, action], dim=-1)
        
        # Predict mean and uncertainty
        prediction = self.transition_net(input_tensor)
        uncertainty = self.uncertainty_net(input_tensor)
        
        next_state_mean = prediction[:, :self.state_dim]
        reward_mean = prediction[:, self.state_dim:]
        
        next_state_std = uncertainty[:, :self.state_dim]
        reward_std = uncertainty[:, self.state_dim:]
        
        return {
            'next_state_mean': next_state_mean,
            'reward_mean': reward_mean,
            'next_state_std': next_state_std,
            'reward_std': reward_std
        }
    
    def sample_prediction(self, state, action):
        """Sample from the predictive distribution."""
        output = self.forward(state, action)
        
        next_state = torch.normal(output['next_state_mean'], output['next_state_std'])
        reward = torch.normal(output['reward_mean'], output['reward_std'])
        
        return next_state.squeeze(), reward.squeeze()

class ModelEnsemble:
    """Ensemble of dynamics models for uncertainty quantification."""
    
    def __init__(self, state_dim, action_dim, ensemble_size=5):
        self.ensemble_size = ensemble_size
        self.models = []
        self.optimizers = []
        
        for _ in range(ensemble_size):
            model = DynamicsModel(state_dim, action_dim).to(device)
            optimizer = optim.Adam(model.parameters(), lr=MODEL_BASED_CONFIG['model_lr'])
            self.models.append(model)
            self.optimizers.append(optimizer)
    
    def train_step(self, states, actions, next_states, rewards):
        """Train all models in the ensemble."""
        total_loss = 0
        
        for model, optimizer in zip(self.models, self.optimizers):
            optimizer.zero_grad()
            
            output = model(states, actions)
            
            # Compute losses
            state_loss = F.mse_loss(output['next_state_mean'], next_states)
            reward_loss = F.mse_loss(output['reward_mean'], rewards.unsqueeze(-1))
            
            # Negative log-likelihood loss for uncertainty
            state_nll = 0.5 * torch.sum(
                ((output['next_state_mean'] - next_states) ** 2) / (output['next_state_std'] ** 2) +
                torch.log(output['next_state_std'] ** 2)
            )
            reward_nll = 0.5 * torch.sum(
                ((output['reward_mean'] - rewards.unsqueeze(-1)) ** 2) / (output['reward_std'] ** 2) +
                torch.log(output['reward_std'] ** 2)
            )
            
            loss = state_loss + reward_loss + 0.1 * (state_nll + reward_nll)
            loss.backward()
            
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            
            total_loss += loss.item()
        
        return total_loss / self.ensemble_size
    
    def predict_ensemble(self, state, action):
        """Get predictions from all models in ensemble."""
        predictions = []
        
        for model in self.models:
            with torch.no_grad():
                pred = model.sample_prediction(state, action)
                predictions.append(pred)
        
        return predictions
    
    def predict_mean(self, state, action):
        """Get ensemble mean prediction."""
        predictions = self.predict_ensemble(state, action)
        
        next_states = torch.stack([pred[0] for pred in predictions])
        rewards = torch.stack([pred[1] for pred in predictions])
        
        return next_states.mean(dim=0), rewards.mean(dim=0)

class ModelPredictiveController:
    """Model Predictive Control using learned dynamics."""
    
    def __init__(self, model_ensemble, action_dim, horizon=10, num_samples=1000):
        self.model_ensemble = model_ensemble
        self.action_dim = action_dim
        self.horizon = horizon
        self.num_samples = num_samples
    
    def plan_action(self, state, goal_state=None):
        """Plan optimal action using MPC."""
        state = torch.FloatTensor(state).to(device)
        best_action = None
        best_value = float('-inf')
        
        # Random shooting for action sequences
        for _ in range(self.num_samples):
            # Generate random action sequence
            if isinstance(self.action_dim, int):  # Discrete actions
                actions = torch.randint(0, self.action_dim, (self.horizon,)).to(device)
            else:  # Continuous actions
                actions = torch.randn(self.horizon, self.action_dim).to(device)
            
            # Simulate trajectory
            total_reward = 0
            current_state = state
            
            for t in range(self.horizon):
                next_state, reward = self.model_ensemble.predict_mean(current_state, actions[t])
                
                # Goal-based reward if goal is provided
                if goal_state is not None:
                    goal_state_tensor = torch.FloatTensor(goal_state).to(device)
                    goal_reward = -torch.norm(next_state - goal_state_tensor)
                    total_reward += goal_reward * (0.99 ** t)
                else:
                    total_reward += reward * (0.99 ** t)
                
                current_state = next_state
            
            if total_reward > best_value:
                best_value = total_reward
                best_action = actions[0]
        
        return best_action.cpu().numpy() if best_action is not None else np.random.randint(self.action_dim)

class DynaQAgent:
    """Dyna-Q algorithm combining model-free and model-based learning."""
    
    def __init__(self, state_dim, action_dim, lr=1e-3):
        self.state_dim = state_dim
        self.action_dim = action_dim
        
        # Q-network for model-free learning
        self.q_network = nn.Sequential(
            nn.Linear(state_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        ).to(device)
        
        self.q_optimizer = optim.Adam(self.q_network.parameters(), lr=lr)
        
        # Model ensemble for planning
        self.model_ensemble = ModelEnsemble(state_dim, action_dim)
        
        # Experience buffer
        self.buffer = deque(maxlen=100000)
        
        # Statistics
        self.training_stats = {
            'q_losses': [],
            'model_losses': [],
            'planning_rewards': []
        }
    
    def get_action(self, state, epsilon=0.1):
        """Epsilon-greedy action selection."""
        if np.random.random() < epsilon:
            return np.random.randint(self.action_dim)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            q_values = self.q_network(state_tensor)
            return q_values.argmax().item()
    
    def store_experience(self, state, action, reward, next_state, done):
        """Store experience in buffer."""
        self.buffer.append((state, action, reward, next_state, done))
    
    def update_q_function(self, batch_size=32):
        """Update Q-function using real experience."""
        if len(self.buffer) < batch_size:
            return 0
        
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        states = torch.FloatTensor(states).to(device)
        actions = torch.LongTensor(actions).to(device)
        rewards = torch.FloatTensor(rewards).to(device)
        next_states = torch.FloatTensor(next_states).to(device)
        dones = torch.BoolTensor(dones).to(device)
        
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        next_q_values = self.q_network(next_states).max(1)[0].detach()
        target_q_values = rewards + 0.99 * next_q_values * (~dones)
        
        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)
        
        self.q_optimizer.zero_grad()
        loss.backward()
        self.q_optimizer.step()
        
        self.training_stats['q_losses'].append(loss.item())
        return loss.item()
    
    def update_model(self, batch_size=32):
        """Update dynamics model."""
        if len(self.buffer) < batch_size:
            return 0
        
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, _ = zip(*batch)
        
        states = torch.FloatTensor(states).to(device)
        actions = torch.LongTensor(actions).to(device)
        rewards = torch.FloatTensor(rewards).to(device)
        next_states = torch.FloatTensor(next_states).to(device)
        
        loss = self.model_ensemble.train_step(states, actions, next_states, rewards)
        self.training_stats['model_losses'].append(loss)
        return loss
    
    def planning_step(self, num_planning_steps=50):
        """Perform planning using the learned model."""
        if len(self.buffer) < 10:
            return 0
        
        total_planning_reward = 0
        
        for _ in range(num_planning_steps):
            # Sample a random state from buffer
            state, _, _, _, _ = random.choice(self.buffer)
            state_tensor = torch.FloatTensor(state).to(device)
            
            # Sample a random action
            action = np.random.randint(self.action_dim)
            action_tensor = torch.LongTensor([action]).to(device)
            
            # Simulate next state and reward
            next_state, reward = self.model_ensemble.predict_mean(state_tensor, action_tensor)
            
            # Update Q-function with simulated experience
            with torch.no_grad():
                current_q = self.q_network(state_tensor.unsqueeze(0))[0, action]
                next_q = self.q_network(next_state.unsqueeze(0)).max()
                target_q = reward + 0.99 * next_q
            
            # Compute TD error and update
            td_error = target_q - current_q
            q_values = self.q_network(state_tensor.unsqueeze(0))
            q_values[0, action] = current_q + 0.1 * td_error
            
            total_planning_reward += reward.item()
        
        avg_planning_reward = total_planning_reward / num_planning_steps
        self.training_stats['planning_rewards'].append(avg_planning_reward)
        return avg_planning_reward

print("🧠 Model-Based RL components implemented successfully!")
print("📝 Key components:")
print("  • DynamicsModel: Neural network for environment dynamics")
print("  • ModelEnsemble: Multiple models for uncertainty quantification")
print("  • ModelPredictiveController: MPC for action planning")
print("  • DynaQAgent: Dyna-Q algorithm combining model-free and model-based learning")
```

# Section 2: Hierarchical Reinforcement Learninghierarchical Rl Decomposes Complex Tasks into Simpler Subtasks through Temporal and Spatial Abstraction.## 2.1 Theoretical Foundation### Options Frameworkan **option** Is a Closed-loop Policy for Taking Actions over a Period of Time. Formally, an Option Consists Of:- **initiation Set** $I$: States Where the Option Can Be Initiated- **policy** $\pi$: Action Selection within the Option- **termination Condition** $\beta$: Probability of Terminating the Option### Semi-markov Decision Process (smdp)options Extend Mdps to Smdps Where:- Actions Can Take Variable Amounts of Time- Temporal Abstraction Enables Hierarchical Planning- Q-learning over Options: $q(s,o) = R + \gamma^k Q(s', O')$### Goal-conditioned Rllearn Policies Conditioned on Goals: $\pi(a|s,g)$- **hindsight Experience Replay (her)**: Learn from Failed Attempts- **universal Value Function**: $v(s,g)$ for Any Goal $G$- **intrinsic Motivation**: Generate Own Goals for Exploration### Hierarchical Actor-critic (hac)multi-level Hierarchy Where:- **high-level Policy**: Selects Subgoals- **low-level Policy**: Executes Actions to Reach Subgoals- **temporal Abstraction**: Different Time Scales at Each Level### Feudal Networkshierarchical Architecture With:- **manager**: Sets Goals for Workers- **worker**: Executes Actions to Achieve Goals- **feudal Objective**: Manager Maximizes Reward, Worker Maximizes Goal Achievement## 2.2 Key Advantages**sample Efficiency:**- Reuse Learned Skills Across Tasks- Faster Learning through Temporal Abstraction**interpretability:**- Hierarchical Structure Mirrors Human Thinking- Decomposable and Explainable Decisions**transfer Learning:**- Skills Transfer Across Related Environments- Compositional Generalization


```python
# Hierarchical RL Implementation

class Option:
    """Implementation of the Options framework."""
    
    def __init__(self, policy, initiation_set=None, termination_condition=None, name="option"):
        self.policy = policy
        self.initiation_set = initiation_set
        self.termination_condition = termination_condition
        self.name = name
        self.active_steps = 0
        self.max_steps = HIERARCHICAL_CONFIG['option_timeout']
    
    def can_initiate(self, state):
        """Check if option can be initiated in given state."""
        if self.initiation_set is None:
            return True
        return self.initiation_set(state)
    
    def should_terminate(self, state):
        """Check if option should terminate in given state."""
        # Timeout termination
        if self.active_steps >= self.max_steps:
            return True
        
        # Custom termination condition
        if self.termination_condition is not None:
            return self.termination_condition(state)
        
        return False
    
    def get_action(self, state):
        """Get action from option policy."""
        self.active_steps += 1
        return self.policy(state)
    
    def reset(self):
        """Reset option state."""
        self.active_steps = 0

class HierarchicalActorCritic(nn.Module):
    """Hierarchical Actor-Critic with multiple levels."""
    
    def __init__(self, state_dim, action_dim, num_levels=3, hidden_dim=256):
        super(HierarchicalActorCritic, self).__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.num_levels = num_levels
        
        # High-level (meta) controllers
        self.meta_controllers = nn.ModuleList()
        self.meta_critics = nn.ModuleList()
        
        # Low-level controllers
        self.low_controllers = nn.ModuleList()
        self.low_critics = nn.ModuleList()
        
        for level in range(num_levels - 1):
            # Meta controller generates subgoals
            meta_controller = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, state_dim)  # Subgoal in state space
            )
            
            # Meta critic evaluates state-goal pairs
            meta_critic = nn.Sequential(
                nn.Linear(state_dim * 2, hidden_dim),  # state + goal
                nn.ReLU(),
                nn.Linear(hidden_dim, hidden_dim),
                nn.ReLU(),
                nn.Linear(hidden_dim, 1)
            )
            
            self.meta_controllers.append(meta_controller)
            self.meta_critics.append(meta_critic)
        
        # Lowest level controller outputs actions
        low_controller = nn.Sequential(
            nn.Linear(state_dim * 2, hidden_dim),  # state + subgoal
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        low_critic = nn.Sequential(
            nn.Linear(state_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self.low_controllers.append(low_controller)
        self.low_critics.append(low_critic)
    
    def forward_meta(self, state, level):
        """Forward pass for meta controller at given level."""
        if level >= len(self.meta_controllers):
            raise ValueError(f"Level {level} exceeds number of meta controllers")
        
        subgoal = self.meta_controllers[level](state)
        state_goal = torch.cat([state, subgoal], dim=-1)
        value = self.meta_critics[level](state_goal)
        
        return subgoal, value
    
    def forward_low(self, state, subgoal):
        """Forward pass for low-level controller."""
        state_subgoal = torch.cat([state, subgoal], dim=-1)
        
        action_logits = self.low_controllers[0](state_subgoal)
        value = self.low_critics[0](state_subgoal)
        
        return action_logits, value
    
    def hierarchical_forward(self, state):
        """Complete hierarchical forward pass."""
        current_goal = state  # Start with state as initial goal
        subgoals = []
        values = []
        
        # Generate subgoals from top to bottom
        for level in range(len(self.meta_controllers)):
            subgoal, value = self.forward_meta(state, level)
            subgoals.append(subgoal)
            values.append(value)
            current_goal = subgoal
        
        # Generate action from lowest level
        action_logits, low_value = self.forward_low(state, current_goal)
        values.append(low_value)
        
        return {
            'subgoals': subgoals,
            'action_logits': action_logits,
            'values': values
        }

class GoalConditionedAgent:
    """Goal-Conditioned RL with Hindsight Experience Replay."""
    
    def __init__(self, state_dim, action_dim, goal_dim=None):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.goal_dim = goal_dim or state_dim
        
        # Policy conditioned on state and goal
        self.policy_net = nn.Sequential(
            nn.Linear(state_dim + self.goal_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, action_dim)
        ).to(device)
        
        # Value function conditioned on state and goal
        self.value_net = nn.Sequential(
            nn.Linear(state_dim + self.goal_dim, 256),
            nn.ReLU(),
            nn.Linear(256, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        ).to(device)
        
        # Optimizers
        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), 
                                         lr=HIERARCHICAL_CONFIG['controller_lr'])
        self.value_optimizer = optim.Adam(self.value_net.parameters(),
                                        lr=HIERARCHICAL_CONFIG['controller_lr'])
        
        # Experience buffer for HER
        self.buffer = deque(maxlen=100000)
        self.her_ratio = 0.8  # Proportion of HER samples
        
        # Goal generation
        self.goal_strategy = "future"  # "future", "episode", "random"
        
        # Statistics
        self.training_stats = {
            'policy_losses': [],
            'value_losses': [],
            'goal_achievements': [],
            'intrinsic_rewards': []
        }
    
    def goal_distance(self, achieved_goal, desired_goal):
        """Compute distance between achieved and desired goals."""
        return torch.norm(achieved_goal - desired_goal, dim=-1)
    
    def compute_reward(self, achieved_goal, desired_goal, info=None):
        """Compute reward based on goal achievement."""
        distance = self.goal_distance(achieved_goal, desired_goal)
        # Sparse reward: +1 if goal achieved, -1 otherwise
        threshold = HIERARCHICAL_CONFIG['subgoal_threshold']
        reward = (distance < threshold).float() * 2 - 1
        return reward
    
    def get_action(self, state, goal, deterministic=False):
        """Get action conditioned on state and goal."""
        state_tensor = torch.FloatTensor(state).to(device)
        goal_tensor = torch.FloatTensor(goal).to(device)
        
        if len(state_tensor.shape) == 1:
            state_tensor = state_tensor.unsqueeze(0)
            goal_tensor = goal_tensor.unsqueeze(0)
        
        state_goal = torch.cat([state_tensor, goal_tensor], dim=-1)
        
        with torch.no_grad():
            action_logits = self.policy_net(state_goal)
            
            if deterministic:
                action = action_logits.argmax(dim=-1)
            else:
                action_probs = F.softmax(action_logits, dim=-1)
                action = torch.multinomial(action_probs, 1).squeeze()
        
        return action.cpu().numpy() if len(action.shape) > 0 else action.item()
    
    def store_episode(self, episode_states, episode_actions, episode_goals, final_achieved_goal):
        """Store episode with HER augmentation."""
        episode_length = len(episode_states)
        
        # Store original episode
        for t in range(episode_length - 1):
            achieved_goal = episode_states[t+1]  # Use next state as achieved goal
            reward = self.compute_reward(
                torch.FloatTensor(achieved_goal),
                torch.FloatTensor(episode_goals[t])
            ).item()
            
            self.buffer.append({
                'state': episode_states[t],
                'action': episode_actions[t],
                'reward': reward,
                'next_state': episode_states[t+1],
                'goal': episode_goals[t],
                'achieved_goal': achieved_goal
            })
        
        # HER: Generate additional samples with different goals
        for t in range(episode_length - 1):
            if np.random.random() < self.her_ratio:
                # Sample future state as goal
                if self.goal_strategy == "future" and t < episode_length - 2:
                    future_idx = np.random.randint(t + 1, episode_length)
                    her_goal = episode_states[future_idx]
                elif self.goal_strategy == "episode":
                    her_goal = final_achieved_goal
                else:  # random
                    her_goal = np.random.randn(self.goal_dim)
                
                achieved_goal = episode_states[t+1]
                her_reward = self.compute_reward(
                    torch.FloatTensor(achieved_goal),
                    torch.FloatTensor(her_goal)
                ).item()
                
                self.buffer.append({
                    'state': episode_states[t],
                    'action': episode_actions[t],
                    'reward': her_reward,
                    'next_state': episode_states[t+1],
                    'goal': her_goal,
                    'achieved_goal': achieved_goal
                })
    
    def train_step(self, batch_size=64):
        """Training step with goal-conditioned experience."""
        if len(self.buffer) < batch_size:
            return 0, 0
        
        # Sample batch
        batch = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([exp['state'] for exp in batch]).to(device)
        actions = torch.LongTensor([exp['action'] for exp in batch]).to(device)
        rewards = torch.FloatTensor([exp['reward'] for exp in batch]).to(device)
        next_states = torch.FloatTensor([exp['next_state'] for exp in batch]).to(device)
        goals = torch.FloatTensor([exp['goal'] for exp in batch]).to(device)
        
        # Prepare inputs
        state_goal = torch.cat([states, goals], dim=-1)
        next_state_goal = torch.cat([next_states, goals], dim=-1)
        
        # Value function loss
        current_values = self.value_net(state_goal).squeeze()
        with torch.no_grad():
            next_values = self.value_net(next_state_goal).squeeze()
            target_values = rewards + 0.99 * next_values
        
        value_loss = F.mse_loss(current_values, target_values)
        
        self.value_optimizer.zero_grad()
        value_loss.backward()
        self.value_optimizer.step()
        
        # Policy loss (actor-critic)
        action_logits = self.policy_net(state_goal)
        action_log_probs = F.log_softmax(action_logits, dim=-1)
        selected_log_probs = action_log_probs.gather(1, actions.unsqueeze(1)).squeeze()
        
        with torch.no_grad():
            advantages = target_values - current_values
        
        policy_loss = -(selected_log_probs * advantages).mean()
        
        self.policy_optimizer.zero_grad()
        policy_loss.backward()
        self.policy_optimizer.step()
        
        # Update statistics
        self.training_stats['policy_losses'].append(policy_loss.item())
        self.training_stats['value_losses'].append(value_loss.item())
        
        # Track goal achievements
        goal_achieved = (rewards > 0).float().mean().item()
        self.training_stats['goal_achievements'].append(goal_achieved)
        
        return policy_loss.item(), value_loss.item()

class FeudalNetwork(nn.Module):
    """Feudal Networks for Hierarchical RL."""
    
    def __init__(self, state_dim, action_dim, goal_dim=64, hidden_dim=256):
        super(FeudalNetwork, self).__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.goal_dim = goal_dim
        
        # Shared perception module
        self.perception = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU()
        )
        
        # Manager network (sets goals)
        self.manager = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, goal_dim)
        )
        
        # Worker network (executes actions)
        self.worker = nn.Sequential(
            nn.Linear(hidden_dim + goal_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        )
        
        # Value functions
        self.manager_critic = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self.worker_critic = nn.Sequential(
            nn.Linear(hidden_dim + goal_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        # Intrinsic curiosity module
        self.curiosity_net = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state, previous_goal=None):
        """Forward pass through feudal network."""
        # Shared perception
        perception = self.perception(state)
        
        # Manager generates goal
        goal = self.manager(perception)
        goal = F.normalize(goal, p=2, dim=-1)  # Normalize goal vector
        
        # Worker takes action conditioned on perception and goal
        if previous_goal is not None:
            # Use previous goal for temporal consistency
            worker_input = torch.cat([perception, previous_goal], dim=-1)
        else:
            worker_input = torch.cat([perception, goal], dim=-1)
        
        action_logits = self.worker(worker_input)
        
        # Value functions
        manager_value = self.manager_critic(perception)
        worker_value = self.worker_critic(worker_input)
        
        return {
            'goal': goal,
            'action_logits': action_logits,
            'manager_value': manager_value,
            'worker_value': worker_value,
            'perception': perception
        }
    
    def compute_intrinsic_reward(self, current_perception, next_perception, goal):
        """Compute intrinsic reward based on goal achievement."""
        # Cosine similarity between goal and state transition
        state_diff = next_perception - current_perception
        intrinsic_reward = F.cosine_similarity(goal, state_diff, dim=-1)
        return intrinsic_reward

# Hierarchical RL Training Environment
class HierarchicalRLEnvironment:
    """Custom environment for testing hierarchical RL algorithms."""
    
    def __init__(self, size=10, num_goals=3):
        self.size = size
        self.num_goals = num_goals
        self.reset()
    
    def reset(self):
        """Reset environment to initial state."""
        self.agent_pos = np.array([0, 0])
        self.goals = []
        
        # Generate random goal positions
        for _ in range(self.num_goals):
            goal_pos = np.random.randint(0, self.size, size=2)
            while np.array_equal(goal_pos, self.agent_pos):
                goal_pos = np.random.randint(0, self.size, size=2)
            self.goals.append(goal_pos)
        
        self.current_goal_idx = 0
        self.steps = 0
        self.max_steps = self.size * 4
        
        return self.get_state()
    
    def get_state(self):
        """Get current state representation."""
        state = np.zeros((self.size, self.size))
        state[self.agent_pos[0], self.agent_pos[1]] = 1.0  # Agent position
        
        # Add goal information
        for i, goal in enumerate(self.goals):
            if i == self.current_goal_idx:
                state[goal[0], goal[1]] = 0.5  # Current goal
            else:
                state[goal[0], goal[1]] = 0.3  # Other goals
        
        return state.flatten()
    
    def step(self, action):
        """Execute action and return next state, reward, done."""
        # Actions: 0=up, 1=down, 2=left, 3=right
        moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]
        
        if action < len(moves):
            new_pos = self.agent_pos + np.array(moves[action])
            # Clip to boundaries
            new_pos = np.clip(new_pos, 0, self.size - 1)
            self.agent_pos = new_pos
        
        self.steps += 1
        
        # Check if current goal is reached
        reward = 0
        done = False
        
        current_goal = self.goals[self.current_goal_idx]
        if np.array_equal(self.agent_pos, current_goal):
            reward = 10.0  # Goal reached
            self.current_goal_idx += 1
            
            if self.current_goal_idx >= self.num_goals:
                done = True  # All goals reached
                reward += 50.0  # Bonus for completing all goals
        else:
            # Distance-based reward
            distance = np.linalg.norm(self.agent_pos - current_goal)
            reward = -0.1 * distance
        
        # Episode timeout
        if self.steps >= self.max_steps:
            done = True
            reward -= 10.0  # Penalty for timeout
        
        info = {
            'goals_completed': self.current_goal_idx,
            'current_goal': current_goal,
            'agent_pos': self.agent_pos.copy()
        }
        
        return self.get_state(), reward, done, info

print("🏗️ Hierarchical RL components implemented successfully!")
print("📝 Key components:")
print("  • Option: Options framework implementation")
print("  • HierarchicalActorCritic: Multi-level hierarchical policy")
print("  • GoalConditionedAgent: Goal-conditioned RL with HER")
print("  • FeudalNetwork: Feudal Networks architecture")
print("  • HierarchicalRLEnvironment: Custom test environment")
```

# Section 3: Advanced Planning and Controladvanced Planning Algorithms Combine Learned Models with Sophisticated Search Techniques.## 3.1 Monte Carlo Tree Search (mcts)mcts Is a Best-first Search Algorithm That Uses Monte Carlo Simulations for Decision Making.### Mcts Algorithm STEPS:1. **selection**: Navigate Down the Tree Using UCB1 FORMULA2. **expansion**: Add New Child Nodes to the TREE3. **simulation**: Run Random Rollouts from Leaf NODES4. **backpropagation**: Update Node Values with Simulation Results### UCB1 Selection FORMULA:$$UCB1(S,A) = Q(s,a) + C \sqrt{\frac{\ln N(s)}{n(s,a)}}$$where:- $q(s,a)$: Average Reward for Action $A$ in State $S$- $n(s)$: Visit Count for State $S$- $n(s,a)$: Visit Count for Action $A$ in State $S$- $C$: Exploration Constant### Alphazero Integrationcombines Mcts with Neural Networks:- **policy Network**: $p(a|s)$ Guides Selection- **value Network**: $v(s)$ Estimates Leaf Values- **self-play**: Generates Training Data through Mcts Games## 3.2 Model-based Value Expansion (mve)uses Learned Models to Expand Value Function Estimates:$$v*{mve}(s) = \max*a \left[ R(s,a) + \gamma \sum*{s'} P(s'|s,a) V(s') \right]$$### Trajectory Optimization- **cross-entropy Method (cem)**: Iterative Sampling and Fitting- **random Shooting**: Sample Multiple Action Sequences- **model Predictive Path Integral (mppi)**: Information-theoretic Approach## 3.3 Latent Space Planningplanning in Learned Latent Representations:### World Models ARCHITECTURE:1. **vision Model (v)**: Encodes Observations to Latent STATES2. **memory Model (m)**: Predicts Next Latent States 3. **controller Model (c)**: Maps Latent States to Actions### Planet Algorithm:- **recurrent State Space Model (rssm)**:- Deterministic Path: $H*T = F(H*{T-1}, A*{T-1})$- Stochastic Path: $S*T \SIM P(s*t | H_t)$- **planning**: Cross-entropy Method in Latent Space- **learning**: Variational Inference for World Model## 3.4 Challenges and Solutions### Model Bias- **problem**: Learned Models Have Prediction Errors- **solutions**: - Model Ensembles for Uncertainty Quantification- Conservative Planning with Uncertainty Penalties- Robust Optimization Techniques### Computational Complexity- **problem**: Planning Is Computationally Expensive- **solutions**:- Hierarchical Planning with Multiple Time Scales- Approximate Planning with Limited Horizons- Parallel Monte Carlo Simulations### Exploration Vs Exploitation- **problem**: Balancing Exploration and Exploitation in Planning- **solutions**:- Ucb-based Selection in Mcts- Optimistic Initialization- Information-gain Based Rewards


```python
# Advanced Planning Algorithms Implementation

class MCTSNode:
    """Node in Monte Carlo Tree Search tree."""
    
    def __init__(self, state, parent=None, action=None, prior=0.0):
        self.state = state
        self.parent = parent
        self.action = action
        self.children = {}
        
        # MCTS statistics
        self.visit_count = 0
        self.value_sum = 0.0
        self.prior = prior
        
        # For neural network guidance
        self.policy_priors = None
        self.value_estimate = 0.0
    
    def is_leaf(self):
        """Check if node is a leaf (no children)."""
        return len(self.children) == 0
    
    def is_root(self):
        """Check if node is root (no parent)."""
        return self.parent is None
    
    def get_value(self):
        """Get average value of node."""
        if self.visit_count == 0:
            return 0.0
        return self.value_sum / self.visit_count
    
    def ucb_score(self, c_puct=1.4):
        """Compute UCB1 score for node selection."""
        if self.visit_count == 0:
            return float('inf')
        
        exploitation = self.get_value()
        
        if self.parent is not None:
            exploration = c_puct * self.prior * math.sqrt(self.parent.visit_count) / (1 + self.visit_count)
        else:
            exploration = 0
        
        return exploitation + exploration
    
    def select_child(self, c_puct=1.4):
        """Select child with highest UCB score."""
        if self.is_leaf():
            return None
        
        return max(self.children.values(), key=lambda child: child.ucb_score(c_puct))
    
    def expand(self, actions, priors=None):
        """Expand node by adding children for all possible actions."""
        if priors is None:
            priors = [1.0 / len(actions)] * len(actions)
        
        for action, prior in zip(actions, priors):
            if action not in self.children:
                self.children[action] = MCTSNode(
                    state=None,  # State will be set during simulation
                    parent=self,
                    action=action,
                    prior=prior
                )
    
    def backup(self, value):
        """Backup value through the tree."""
        self.visit_count += 1
        self.value_sum += value
        
        if not self.is_root():
            self.parent.backup(value)

class MonteCarloTreeSearch:
    """Monte Carlo Tree Search for planning."""
    
    def __init__(self, model, value_network=None, policy_network=None):
        self.model = model
        self.value_network = value_network
        self.policy_network = policy_network
        self.c_puct = PLANNING_CONFIG['exploration_constant']
        self.num_simulations = PLANNING_CONFIG['mcts_simulations']
    
    def search(self, root_state, num_simulations=None):
        """Perform MCTS search from root state."""
        if num_simulations is None:
            num_simulations = self.num_simulations
        
        # Initialize root node
        root = MCTSNode(root_state)
        
        # Get initial policy and value if networks available
        if self.policy_network is not None:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(root_state).unsqueeze(0).to(device)
                policy_logits = self.policy_network(state_tensor)
                priors = F.softmax(policy_logits, dim=-1).squeeze().cpu().numpy()
                root.expand(list(range(len(priors))), priors)
        else:
            # Uniform priors if no policy network
            num_actions = 4  # Assume 4 actions for simplicity
            root.expand(list(range(num_actions)))
        
        # Run simulations
        for _ in range(num_simulations):
            self._simulate(root)
        
        return root
    
    def _simulate(self, root):
        """Single MCTS simulation."""
        # Selection: traverse down the tree
        current = root
        path = []
        
        while not current.is_leaf():
            current = current.select_child(self.c_puct)
            path.append(current)
        
        # Expansion and Evaluation
        if current.visit_count == 0:
            # First visit - evaluate leaf
            value = self._evaluate_leaf(current)
        else:
            # Expand leaf if visited before
            if hasattr(self.model, 'get_possible_actions'):
                actions = self.model.get_possible_actions(current.state)
            else:
                actions = list(range(4))  # Default actions
            
            current.expand(actions)
            
            # Select random child for simulation
            if current.children:
                action = np.random.choice(list(current.children.keys()))
                child = current.children[action]
                
                # Simulate transition
                if hasattr(self.model, 'predict_mean'):
                    next_state, reward = self.model.predict_mean(
                        torch.FloatTensor(current.state).to(device),
                        torch.LongTensor([action]).to(device)
                    )
                    child.state = next_state.cpu().numpy()
                else:
                    # Fallback for simple environments
                    child.state = current.state  # Placeholder
                
                value = self._evaluate_leaf(child)
                path.append(child)
            else:
                value = self._evaluate_leaf(current)
        
        # Backpropagation
        for node in reversed(path):
            node.backup(value)
        root.backup(value)
    
    def _evaluate_leaf(self, node):
        """Evaluate leaf node value."""
        if self.value_network is not None:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(node.state).unsqueeze(0).to(device)
                value = self.value_network(state_tensor).item()
        else:
            # Simple rollout evaluation
            value = self._rollout(node.state)
        
        return value
    
    def _rollout(self, state, depth=10):
        """Random rollout for value estimation."""
        total_reward = 0
        current_state = state
        
        for i in range(depth):
            # Random action
            action = np.random.randint(4)
            
            # Simulate step (simplified)
            if hasattr(self.model, 'predict_mean'):
                next_state, reward = self.model.predict_mean(
                    torch.FloatTensor(current_state).to(device),
                    torch.LongTensor([action]).to(device)
                )
                total_reward += reward.item() * (0.99 ** i)
                current_state = next_state.cpu().numpy()
            else:
                # Random reward for fallback
                reward = np.random.randn()
                total_reward += reward * (0.99 ** i)
        
        return total_reward
    
    def get_action_probabilities(self, root):
        """Get action probabilities from MCTS results."""
        if root.is_leaf():
            return np.ones(4) / 4  # Uniform if no children
        
        visits = []
        actions = []
        
        for action, child in root.children.items():
            actions.append(action)
            visits.append(child.visit_count)
        
        if sum(visits) == 0:
            return np.ones(len(actions)) / len(actions)
        
        # Convert to probabilities
        visits = np.array(visits)
        probabilities = visits / visits.sum()
        
        # Create full action probability vector
        full_probs = np.zeros(4)  # Assume 4 actions
        for action, prob in zip(actions, probabilities):
            if action < len(full_probs):
                full_probs[action] = prob
        
        return full_probs

class ModelBasedValueExpansion:
    """Model-Based Value Expansion for planning."""
    
    def __init__(self, model, value_function, expansion_depth=3):
        self.model = model
        self.value_function = value_function
        self.expansion_depth = expansion_depth
    
    def expand_value(self, state, depth=0):
        """Recursively expand value function using model."""
        if depth >= self.expansion_depth:
            # Base case: use value function
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
            with torch.no_grad():
                return self.value_function(state_tensor).item()
        
        # Get all possible actions
        num_actions = 4  # Assume discrete action space
        action_values = []
        
        for action in range(num_actions):
            # Predict next state and reward
            if hasattr(self.model, 'predict_mean'):
                next_state, reward = self.model.predict_mean(
                    torch.FloatTensor(state).to(device),
                    torch.LongTensor([action]).to(device)
                )
                next_state = next_state.cpu().numpy()
                reward = reward.item()
            else:
                # Fallback
                next_state = state
                reward = np.random.randn()
            
            # Recursive value expansion
            next_value = self.expand_value(next_state, depth + 1)
            action_value = reward + 0.99 * next_value
            action_values.append(action_value)
        
        # Return maximum action value
        return max(action_values)
    
    def plan_action(self, state):
        """Select best action using value expansion."""
        num_actions = 4
        action_values = []
        
        for action in range(num_actions):
            # Predict next state and reward
            if hasattr(self.model, 'predict_mean'):
                next_state, reward = self.model.predict_mean(
                    torch.FloatTensor(state).to(device),
                    torch.LongTensor([action]).to(device)
                )
                next_state = next_state.cpu().numpy()
                reward = reward.item()
            else:
                next_state = state
                reward = np.random.randn()
            
            # Compute action value
            next_value = self.expand_value(next_state, depth=1)
            action_value = reward + 0.99 * next_value
            action_values.append(action_value)
        
        # Return action with highest value
        return np.argmax(action_values)

class LatentSpacePlanner:
    """Planning in learned latent representations."""
    
    def __init__(self, encoder, decoder, latent_dynamics, latent_dim=64):
        self.encoder = encoder
        self.decoder = decoder
        self.latent_dynamics = latent_dynamics
        self.latent_dim = latent_dim
        
        # Cross-entropy method parameters
        self.population_size = 500
        self.elite_fraction = 0.1
        self.num_iterations = 10
        
    def encode_state(self, state):
        """Encode state to latent representation."""
        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)
        with torch.no_grad():
            latent_state = self.encoder(state_tensor)
        return latent_state
    
    def decode_state(self, latent_state):
        """Decode latent state to observation space."""
        with torch.no_grad():
            decoded_state = self.decoder(latent_state)
        return decoded_state.cpu().numpy()
    
    def plan_in_latent_space(self, initial_state, horizon=10):
        """Plan action sequence in latent space using CEM."""
        # Encode initial state
        latent_state = self.encode_state(initial_state)
        
        # Initialize action distribution (mean and std)
        action_dim = 4  # Assume discrete actions
        action_mean = np.zeros((horizon, action_dim))
        action_std = np.ones((horizon, action_dim))
        
        best_actions = None
        best_reward = float('-inf')
        
        for iteration in range(self.num_iterations):
            # Sample action sequences
            action_sequences = []
            rewards = []
            
            for _ in range(self.population_size):
                # Sample actions from current distribution
                actions = []
                for t in range(horizon):
                    # Sample from categorical distribution in discrete case
                    action_logits = np.random.normal(action_mean[t], action_std[t])
                    action = np.argmax(action_logits)
                    actions.append(action)
                
                action_sequences.append(actions)
                
                # Evaluate action sequence
                reward = self._evaluate_latent_sequence(latent_state, actions)
                rewards.append(reward)
            
            # Select elite samples
            elite_idx = np.argsort(rewards)[-int(self.elite_fraction * self.population_size):]
            elite_actions = [action_sequences[i] for i in elite_idx]
            
            # Update best sequence
            if max(rewards) > best_reward:
                best_reward = max(rewards)
                best_actions = action_sequences[np.argmax(rewards)]
            
            # Update action distribution
            if len(elite_actions) > 0:
                elite_array = np.array(elite_actions)
                for t in range(horizon):
                    # For discrete actions, use one-hot encoding
                    action_counts = np.bincount(elite_array[:, t], minlength=action_dim)
                    action_probs = action_counts / len(elite_actions)
                    
                    # Update mean (logits) and reduce std
                    action_mean[t] = np.log(action_probs + 1e-8)
                    action_std[t] *= 0.9  # Reduce exploration over iterations
        
        return best_actions[0] if best_actions else 0  # Return first action
    
    def _evaluate_latent_sequence(self, initial_latent_state, actions):
        """Evaluate action sequence in latent space."""
        current_latent = initial_latent_state
        total_reward = 0
        
        for t, action in enumerate(actions):
            # Predict next latent state
            action_tensor = torch.LongTensor([action]).to(device)
            
            if hasattr(self.latent_dynamics, 'forward'):
                with torch.no_grad():
                    # Assume latent dynamics returns next state and reward
                    next_latent, reward = self.latent_dynamics(current_latent, action_tensor)
                    total_reward += reward.item() * (0.99 ** t)
                    current_latent = next_latent
            else:
                # Fallback: random reward
                reward = np.random.randn()
                total_reward += reward * (0.99 ** t)
        
        return total_reward

class WorldModel(nn.Module):
    """World model for latent space planning (inspired by PlaNet)."""
    
    def __init__(self, obs_dim, action_dim, latent_dim=64, hidden_dim=256):
        super(WorldModel, self).__init__()
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.latent_dim = latent_dim
        
        # Encoder: observation -> latent state
        self.encoder = nn.Sequential(
            nn.Linear(obs_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim * 2)  # Mean and log_std
        )
        
        # Decoder: latent state -> observation
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, obs_dim)
        )
        
        # Dynamics model: (latent_state, action) -> (next_latent_state, reward)
        self.dynamics = nn.Sequential(
            nn.Linear(latent_dim + action_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, latent_dim + 1)  # Next latent state + reward
        )
        
        # Recurrent state space model components
        self.rnn = nn.GRU(latent_dim + action_dim, hidden_dim, batch_first=True)
        self.hidden_to_latent = nn.Linear(hidden_dim, latent_dim * 2)
    
    def encode(self, obs):
        """Encode observation to latent state."""
        encoded = self.encoder(obs)
        mean, log_std = encoded.chunk(2, dim=-1)
        return mean, log_std
    
    def decode(self, latent):
        """Decode latent state to observation."""
        return self.decoder(latent)
    
    def sample_latent(self, mean, log_std):
        """Sample from latent distribution."""
        std = torch.exp(log_std)
        eps = torch.randn_like(std)
        return mean + eps * std
    
    def predict_next(self, latent_state, action):
        """Predict next latent state and reward."""
        # Handle discrete actions
        if action.dtype == torch.long:
            action_one_hot = torch.zeros(action.size(0), self.action_dim).to(action.device)
            action_one_hot.scatter_(1, action.unsqueeze(1), 1)
            action = action_one_hot
        
        input_tensor = torch.cat([latent_state, action], dim=-1)
        output = self.dynamics(input_tensor)
        
        next_latent = output[:, :self.latent_dim]
        reward = output[:, self.latent_dim:]
        
        return next_latent, reward
    
    def forward(self, obs_sequence, action_sequence):
        """Forward pass through world model."""
        batch_size, seq_len = obs_sequence.shape[:2]
        
        # Encode all observations
        obs_flat = obs_sequence.view(-1, self.obs_dim)
        latent_mean, latent_log_std = self.encode(obs_flat)
        latent_mean = latent_mean.view(batch_size, seq_len, self.latent_dim)
        latent_log_std = latent_log_std.view(batch_size, seq_len, self.latent_dim)
        
        # Sample latent states
        latent_states = self.sample_latent(latent_mean, latent_log_std)
        
        # Predict future states using dynamics
        predicted_latents = []
        predicted_rewards = []
        
        for t in range(seq_len - 1):
            next_latent, reward = self.predict_next(
                latent_states[:, t], 
                action_sequence[:, t]
            )
            predicted_latents.append(next_latent)
            predicted_rewards.append(reward)
        
        predicted_latents = torch.stack(predicted_latents, dim=1)
        predicted_rewards = torch.stack(predicted_rewards, dim=1)
        
        # Decode latent states back to observations
        predicted_obs = self.decode(predicted_latents.view(-1, self.latent_dim))
        predicted_obs = predicted_obs.view(batch_size, seq_len - 1, self.obs_dim)
        
        return {
            'latent_mean': latent_mean,
            'latent_log_std': latent_log_std,
            'predicted_obs': predicted_obs,
            'predicted_rewards': predicted_rewards,
            'latent_states': latent_states
        }

print("🎯 Advanced Planning components implemented successfully!")
print("📝 Key components:")
print("  • MCTSNode & MonteCarloTreeSearch: MCTS algorithm implementation")
print("  • ModelBasedValueExpansion: MVE for planning with learned models") 
print("  • LatentSpacePlanner: Planning in learned latent representations")
print("  • WorldModel: Complete world model architecture for latent planning")
```

# Section 4: Practical Demonstrations and Experimentsthis Section Provides Hands-on Experiments to Demonstrate the Concepts and Implementations.## 4.1 Experiment Setupwe'll Create Practical Experiments to SHOWCASE:1. **model-based Vs Model-free Comparison**- Sample Efficiency Analysis- Performance on Different Environments- Computational Overhead COMPARISON2. **hierarchical Rl Benefits**- Multi-goal Navigation Tasks- Skill Reuse and Transfer- Temporal Abstraction ADVANTAGES3. **planning Algorithm Comparison**- Mcts Vs Random Rollouts- Value Expansion Effectiveness- Latent Space Planning BENEFITS4. **integration Study**- Combining All Methods- Real-world Application Scenarios- Performance Analysis and Trade-offs## 4.2 Metrics and Evaluation### Performance Metrics:- **sample Efficiency**: Steps to Reach Performance Threshold- **asymptotic Performance**: Final Average Reward- **computation Time**: Planning and Learning Overhead- **memory Usage**: Model Storage Requirements- **transfer Performance**: Success on Related Tasks### Statistical Analysis:- Multiple Random Seeds for Reliability- Confidence Intervals and Significance Tests- Learning Curve Analysis- Ablation Studies for Each Component## 4.3 Environments for Testing### Simple Grid World:- **purpose**: Basic Concept Demonstration- **features**: Discrete States, Clear Visualization- **challenges**: Navigation, Goal Reaching### Continuous Control:- **purpose**: Real-world Applicability- **features**: Continuous State-action Spaces- **challenges**: Precise Control, Dynamic Systems### Hierarchical Tasks:- **purpose**: Multi-level Decision Making- **features**: Natural Task Decomposition- **challenges**: Long-horizon Planning, Skill Coordination


```python
# Experiment 1: Model-Based vs Model-Free Comparison

class ExperimentRunner:
    """Unified experiment runner for all algorithms."""
    
    def __init__(self, env_class, env_kwargs=None):
        self.env_class = env_class
        self.env_kwargs = env_kwargs or {}
        self.results = {}
    
    def run_experiment(self, agent_configs, num_episodes=500, num_seeds=3):
        """Run experiment with multiple agents and seeds."""
        results = {}
        
        for agent_name, agent_config in agent_configs.items():
            print(f"\n🔄 Running experiment for {agent_name}...")
            agent_results = []
            
            for seed in range(num_seeds):
                print(f"  Seed {seed + 1}/{num_seeds}")
                
                # Set random seeds
                np.random.seed(seed)
                torch.manual_seed(seed)
                random.seed(seed)
                
                # Create environment and agent
                env = self.env_class(**self.env_kwargs)
                agent = agent_config['class'](**agent_config['params'])
                
                # Run episodes
                episode_rewards = []
                episode_lengths = []
                model_losses = []
                planning_times = []
                
                for episode in range(num_episodes):
                    state = env.reset()
                    episode_reward = 0
                    episode_length = 0
                    done = False
                    
                    start_time = time.time()
                    
                    while not done:
                        # Get action from agent
                        if hasattr(agent, 'get_action'):
                            action = agent.get_action(state)
                        elif hasattr(agent, 'plan_action'):
                            action = agent.plan_action(state)
                        else:
                            action = np.random.randint(env.action_space.n if hasattr(env, 'action_space') else 4)
                        
                        # Take step in environment
                        if hasattr(env, 'step'):
                            next_state, reward, done, info = env.step(action)
                        else:
                            # Fallback for custom environments
                            next_state, reward, done = state, np.random.randn(), np.random.random() < 0.1
                            info = {}
                        
                        episode_reward += reward
                        episode_length += 1
                        
                        # Store experience and train
                        if hasattr(agent, 'store_experience'):
                            agent.store_experience(state, action, reward, next_state, done)
                        
                        if hasattr(agent, 'update_q_function'):
                            q_loss = agent.update_q_function()
                        elif hasattr(agent, 'train_step'):
                            losses = agent.train_step()
                        
                        # Update model if applicable
                        if hasattr(agent, 'update_model'):
                            model_loss = agent.update_model()
                            model_losses.append(model_loss)
                        
                        # Planning step if applicable  
                        if hasattr(agent, 'planning_step'):
                            agent.planning_step()
                        
                        state = next_state
                        
                        if episode_length > 500:  # Timeout
                            break
                    
                    planning_time = time.time() - start_time
                    planning_times.append(planning_time)
                    
                    episode_rewards.append(episode_reward)
                    episode_lengths.append(episode_length)
                    
                    # Progress reporting
                    if (episode + 1) % 100 == 0:
                        avg_reward = np.mean(episode_rewards[-100:])
                        print(f"    Episode {episode + 1}: Avg Reward = {avg_reward:.2f}")
                
                # Store results for this seed
                agent_results.append({
                    'rewards': episode_rewards,
                    'lengths': episode_lengths,
                    'model_losses': model_losses,
                    'planning_times': planning_times,
                    'final_performance': np.mean(episode_rewards[-50:])
                })
            
            results[agent_name] = agent_results
        
        self.results = results
        return results
    
    def analyze_results(self):
        """Analyze and visualize experiment results."""
        if not self.results:
            print("❌ No results to analyze. Run experiment first.")
            return
        
        print("\n📊 Experiment Results Analysis")
        print("=" * 50)
        
        # Create comparison plots
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Model-Based vs Model-Free Comparison', fontsize=16)
        
        # Plot 1: Learning curves
        ax1 = axes[0, 0]
        for agent_name, agent_results in self.results.items():
            # Average across seeds
            all_rewards = [result['rewards'] for result in agent_results]
            min_length = min(len(rewards) for rewards in all_rewards)
            
            rewards_array = np.array([rewards[:min_length] for rewards in all_rewards])
            mean_rewards = np.mean(rewards_array, axis=0)
            std_rewards = np.std(rewards_array, axis=0)
            
            episodes = np.arange(min_length)
            ax1.plot(episodes, mean_rewards, label=agent_name, linewidth=2)
            ax1.fill_between(episodes, 
                           mean_rewards - std_rewards, 
                           mean_rewards + std_rewards, 
                           alpha=0.3)
        
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Average Reward')
        ax1.set_title('Learning Curves')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Sample efficiency (episodes to threshold)
        ax2 = axes[0, 1]
        threshold = -100  # Adjust based on environment
        
        agent_names = []
        sample_efficiencies = []
        sample_stds = []
        
        for agent_name, agent_results in self.results.items():
            episodes_to_threshold = []
            
            for result in agent_results:
                rewards = result['rewards']
                # Find first episode where moving average exceeds threshold
                moving_avg = np.convolve(rewards, np.ones(50)/50, mode='valid')
                threshold_idx = np.where(moving_avg >= threshold)[0]
                
                if len(threshold_idx) > 0:
                    episodes_to_threshold.append(threshold_idx[0] + 50)
                else:
                    episodes_to_threshold.append(len(rewards))  # Didn't reach threshold
            
            agent_names.append(agent_name)
            sample_efficiencies.append(np.mean(episodes_to_threshold))
            sample_stds.append(np.std(episodes_to_threshold))
        
        bars = ax2.bar(agent_names, sample_efficiencies, yerr=sample_stds, 
                      capsize=5, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(agent_names)])
        ax2.set_ylabel('Episodes to Threshold')
        ax2.set_title('Sample Efficiency')
        ax2.tick_params(axis='x', rotation=45)
        
        # Plot 3: Final performance comparison
        ax3 = axes[1, 0]
        
        final_performances = []
        final_stds = []
        
        for agent_name, agent_results in self.results.items():
            performances = [result['final_performance'] for result in agent_results]
            final_performances.append(np.mean(performances))
            final_stds.append(np.std(performances))
        
        bars = ax3.bar(agent_names, final_performances, yerr=final_stds,
                      capsize=5, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(agent_names)])
        ax3.set_ylabel('Final Average Reward')
        ax3.set_title('Final Performance')
        ax3.tick_params(axis='x', rotation=45)
        
        # Plot 4: Computational overhead
        ax4 = axes[1, 1]
        
        planning_times = []
        time_stds = []
        
        for agent_name, agent_results in self.results.items():
            times = []
            for result in agent_results:
                if result['planning_times']:
                    times.extend(result['planning_times'])
            
            if times:
                planning_times.append(np.mean(times))
                time_stds.append(np.std(times))
            else:
                planning_times.append(0)
                time_stds.append(0)
        
        bars = ax4.bar(agent_names, planning_times, yerr=time_stds,
                      capsize=5, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(agent_names)])
        ax4.set_ylabel('Average Planning Time (s)')
        ax4.set_title('Computational Overhead')
        ax4.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
        
        # Print summary statistics
        print("\n📈 Summary Statistics:")
        for agent_name, agent_results in self.results.items():
            performances = [result['final_performance'] for result in agent_results]
            mean_perf = np.mean(performances)
            std_perf = np.std(performances)
            
            print(f"\n{agent_name}:")
            print(f"  Final Performance: {mean_perf:.2f} ± {std_perf:.2f}")
            
            # Calculate sample efficiency
            episodes_to_threshold = []
            for result in agent_results:
                rewards = result['rewards']
                moving_avg = np.convolve(rewards, np.ones(50)/50, mode='valid')
                threshold_idx = np.where(moving_avg >= threshold)[0]
                if len(threshold_idx) > 0:
                    episodes_to_threshold.append(threshold_idx[0] + 50)
            
            if episodes_to_threshold:
                mean_efficiency = np.mean(episodes_to_threshold)
                std_efficiency = np.std(episodes_to_threshold)
                print(f"  Sample Efficiency: {mean_efficiency:.0f} ± {std_efficiency:.0f} episodes")

# Simple Grid World Environment for testing
class SimpleGridWorld:
    """Simple grid world for model-based vs model-free comparison."""
    
    def __init__(self, size=8, num_goals=1):
        self.size = size
        self.num_goals = num_goals
        self.action_space_size = 4  # up, down, left, right
        self.state_dim = size * size
        self.reset()
    
    def reset(self):
        """Reset environment to initial state."""
        self.agent_pos = [0, 0]
        
        # Place goal randomly
        self.goal_pos = [np.random.randint(self.size//2, self.size),
                        np.random.randint(self.size//2, self.size)]
        
        # Ensure agent and goal are different
        while self.agent_pos == self.goal_pos:
            self.goal_pos = [np.random.randint(1, self.size),
                           np.random.randint(1, self.size)]
        
        self.steps = 0
        self.max_steps = self.size * 4
        
        return self._get_state()
    
    def _get_state(self):
        """Convert position to state representation."""
        state = np.zeros(self.state_dim)
        agent_idx = self.agent_pos[0] * self.size + self.agent_pos[1]
        goal_idx = self.goal_pos[0] * self.size + self.goal_pos[1]
        
        state[agent_idx] = 1.0  # Agent position
        state[goal_idx] = 0.5   # Goal position
        
        return state
    
    def step(self, action):
        """Execute action and return next state, reward, done."""
        # Actions: 0=up, 1=down, 2=left, 3=right
        moves = [[-1, 0], [1, 0], [0, -1], [0, 1]]
        
        if action < len(moves):
            new_pos = [
                self.agent_pos[0] + moves[action][0],
                self.agent_pos[1] + moves[action][1]
            ]
            
            # Clip to boundaries
            new_pos[0] = max(0, min(self.size - 1, new_pos[0]))
            new_pos[1] = max(0, min(self.size - 1, new_pos[1]))
            
            self.agent_pos = new_pos
        
        self.steps += 1
        
        # Calculate reward
        distance = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])
        
        if distance == 0:
            reward = 100.0  # Goal reached
            done = True
        else:
            reward = -1.0 - 0.1 * distance  # Step penalty + distance penalty
            done = False
        
        # Episode timeout
        if self.steps >= self.max_steps:
            done = True
            if distance > 0:
                reward -= 50.0  # Timeout penalty
        
        info = {'distance': distance, 'steps': self.steps}
        
        return self._get_state(), reward, done, info

# Run Model-Based vs Model-Free Experiment
print("🚀 Setting up Model-Based vs Model-Free Experiment...")

# Configure agents for comparison
agent_configs = {
    'Dyna-Q (Model-Based)': {
        'class': DynaQAgent,
        'params': {'state_dim': 64, 'action_dim': 4, 'lr': 1e-3}
    }
    # Note: We would add more agents here like pure DQN, but keeping simple for demonstration
}

# Create experiment runner
experiment = ExperimentRunner(SimpleGridWorld, {'size': 8, 'num_goals': 1})

# Add timing import
import time

print("📝 Agent configurations created successfully!")
print("🔧 Experiment environment ready for model-based vs model-free comparison!")
print("\n💡 To run the experiment, call: experiment.run_experiment(agent_configs, num_episodes=200, num_seeds=3)")
print("📊 To analyze results, call: experiment.analyze_results()")
```


```python
# Experiment 2: Hierarchical RL Demonstration

class HierarchicalRLExperiment:
    """Experiment to demonstrate hierarchical RL benefits."""
    
    def __init__(self):
        self.results = {}
        
    def create_multi_goal_environment(self, size=12, num_goals=4):
        """Create a complex multi-goal environment."""
        return HierarchicalRLEnvironment(size=size, num_goals=num_goals)
    
    def run_hierarchical_experiment(self, num_episodes=300, num_seeds=3):
        """Run hierarchical RL experiment with multiple approaches."""
        
        print("🏗️ Running Hierarchical RL Experiment...")
        print("🎯 Testing: Goal-Conditioned RL vs Standard RL vs Hierarchical AC")
        
        # Environment setup
        env_size = 10
        num_goals = 3
        
        agent_configs = {
            'Goal-Conditioned Agent': {
                'class': GoalConditionedAgent,
                'params': {
                    'state_dim': env_size * env_size,
                    'action_dim': 4,
                    'goal_dim': env_size * env_size
                }
            },
            'Standard DQN-like': {
                'class': DynaQAgent,
                'params': {
                    'state_dim': env_size * env_size,
                    'action_dim': 4,
                    'lr': 1e-3
                }
            }
        }
        
        results = {}
        
        for agent_name, agent_config in agent_configs.items():
            print(f"\n🔄 Testing {agent_name}...")
            agent_results = []
            
            for seed in range(num_seeds):
                print(f"  Seed {seed + 1}/{num_seeds}")
                
                # Set random seeds
                np.random.seed(seed)
                torch.manual_seed(seed)
                random.seed(seed)
                
                # Create environment and agent
                env = self.create_multi_goal_environment(env_size, num_goals)
                agent = agent_config['class'](**agent_config['params'])
                
                # Episode tracking
                episode_rewards = []
                goal_achievements = []
                episode_lengths = []
                skill_reuse_success = []
                
                for episode in range(num_episodes):
                    state = env.reset()
                    episode_reward = 0
                    episode_length = 0
                    goals_reached = 0
                    done = False
                    
                    # For goal-conditioned agents, create episode trajectory
                    if agent_name == 'Goal-Conditioned Agent':
                        episode_states = [state]
                        episode_actions = []
                        episode_goals = []
                        
                        # Set goal as the position of the first target
                        current_goal = np.zeros_like(state)
                        if hasattr(env, 'goals') and len(env.goals) > 0:
                            goal_pos = env.goals[env.current_goal_idx]
                            goal_idx = goal_pos[0] * env_size + goal_pos[1]
                            current_goal[goal_idx] = 1.0
                    
                    while not done and episode_length < 200:
                        # Get action based on agent type
                        if agent_name == 'Goal-Conditioned Agent':
                            action = agent.get_action(state, current_goal)
                            episode_goals.append(current_goal.copy())
                        else:
                            action = agent.get_action(state)
                        
                        # Take step
                        next_state, reward, done, info = env.step(action)
                        episode_reward += reward
                        episode_length += 1
                        
                        # Track goal achievements
                        if 'goals_completed' in info:
                            goals_reached = info['goals_completed']
                        
                        # Store experience and train
                        if agent_name == 'Goal-Conditioned Agent':
                            episode_states.append(next_state)
                            episode_actions.append(action)
                        else:
                            if hasattr(agent, 'store_experience'):
                                agent.store_experience(state, action, reward, next_state, done)
                            if hasattr(agent, 'update_q_function'):
                                agent.update_q_function()
                            if hasattr(agent, 'update_model'):
                                agent.update_model()
                        
                        state = next_state
                        
                        # Update goal for goal-conditioned agent
                        if agent_name == 'Goal-Conditioned Agent' and hasattr(env, 'goals'):
                            if env.current_goal_idx < len(env.goals):
                                goal_pos = env.goals[env.current_goal_idx]
                                current_goal = np.zeros_like(state)
                                goal_idx = goal_pos[0] * env_size + goal_pos[1]
                                current_goal[goal_idx] = 1.0
                    
                    # Train goal-conditioned agent with HER
                    if agent_name == 'Goal-Conditioned Agent' and len(episode_states) > 1:
                        final_achieved_goal = episode_states[-1]
                        agent.store_episode(episode_states, episode_actions, episode_goals, final_achieved_goal)
                        
                        # Multiple training steps
                        for _ in range(10):
                            agent.train_step(batch_size=32)
                    
                    # Record metrics
                    episode_rewards.append(episode_reward)
                    goal_achievements.append(goals_reached / num_goals)
                    episode_lengths.append(episode_length)
                    
                    # Test skill reuse every 50 episodes
                    if episode % 50 == 0 and episode > 0:
                        skill_reuse_score = self._test_skill_reuse(agent, env, agent_name)
                        skill_reuse_success.append(skill_reuse_score)
                    
                    # Progress reporting
                    if (episode + 1) % 100 == 0:
                        avg_reward = np.mean(episode_rewards[-50:])
                        avg_goals = np.mean(goal_achievements[-50:])
                        print(f"    Episode {episode + 1}: Reward={avg_reward:.2f}, Goals={avg_goals:.2f}")
                
                # Store results for this seed
                agent_results.append({
                    'rewards': episode_rewards,
                    'goal_achievements': goal_achievements,
                    'lengths': episode_lengths,
                    'skill_reuse': skill_reuse_success,
                    'final_performance': np.mean(episode_rewards[-30:]),
                    'final_goal_rate': np.mean(goal_achievements[-30:])
                })
            
            results[agent_name] = agent_results
        
        self.results = results
        return results
    
    def _test_skill_reuse(self, agent, env, agent_name):
        """Test how well agent transfers skills to new goal configurations."""
        # Create new environment with different goal layout
        test_env = self.create_multi_goal_environment(env.size, env.num_goals)
        
        # Run a few test episodes
        success_count = 0
        test_episodes = 5
        
        for _ in range(test_episodes):
            state = test_env.reset()
            done = False
            steps = 0
            goals_reached = 0
            
            if agent_name == 'Goal-Conditioned Agent':
                # Set goal for goal-conditioned agent
                current_goal = np.zeros_like(state)
                if hasattr(test_env, 'goals') and len(test_env.goals) > 0:
                    goal_pos = test_env.goals[0]
                    goal_idx = goal_pos[0] * test_env.size + goal_pos[1]
                    current_goal[goal_idx] = 1.0
            
            while not done and steps < 100:
                if agent_name == 'Goal-Conditioned Agent':
                    action = agent.get_action(state, current_goal, deterministic=True)
                else:
                    action = agent.get_action(state, epsilon=0.1)  # Slight exploration
                
                next_state, reward, done, info = test_env.step(action)
                steps += 1
                
                if 'goals_completed' in info:
                    goals_reached = info['goals_completed']
                
                state = next_state
            
            # Success if reached at least one goal quickly
            if goals_reached > 0 and steps < 80:
                success_count += 1
        
        return success_count / test_episodes
    
    def visualize_hierarchical_results(self):
        """Visualize hierarchical RL experiment results."""
        if not self.results:
            print("❌ No results to visualize. Run experiment first.")
            return
        
        print("\n📊 Hierarchical RL Results Analysis")
        print("=" * 50)
        
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        fig.suptitle('Hierarchical RL Performance Analysis', fontsize=16)
        
        # Plot 1: Learning curves
        ax1 = axes[0, 0]
        for agent_name, agent_results in self.results.items():
            all_rewards = [result['rewards'] for result in agent_results]
            min_length = min(len(rewards) for rewards in all_rewards)
            
            rewards_array = np.array([rewards[:min_length] for rewards in all_rewards])
            mean_rewards = np.mean(rewards_array, axis=0)
            std_rewards = np.std(rewards_array, axis=0)
            
            episodes = np.arange(min_length)
            ax1.plot(episodes, mean_rewards, label=agent_name, linewidth=2)
            ax1.fill_between(episodes, mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.3)
        
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Average Reward')
        ax1.set_title('Learning Curves')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Goal achievement rates
        ax2 = axes[0, 1]
        for agent_name, agent_results in self.results.items():
            all_goals = [result['goal_achievements'] for result in agent_results]
            min_length = min(len(goals) for goals in all_goals)
            
            goals_array = np.array([goals[:min_length] for goals in all_goals])
            mean_goals = np.mean(goals_array, axis=0)
            std_goals = np.std(goals_array, axis=0)
            
            episodes = np.arange(min_length)
            ax2.plot(episodes, mean_goals, label=agent_name, linewidth=2)
            ax2.fill_between(episodes, mean_goals - std_goals, mean_goals + std_goals, alpha=0.3)
        
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Goal Achievement Rate')
        ax2.set_title('Goal Completion Progress')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Plot 3: Skill reuse capability
        ax3 = axes[0, 2]
        agent_names = list(self.results.keys())
        skill_reuse_means = []
        skill_reuse_stds = []
        
        for agent_name, agent_results in self.results.items():
            all_reuse = []
            for result in agent_results:
                if result['skill_reuse']:
                    all_reuse.extend(result['skill_reuse'])
            
            if all_reuse:
                skill_reuse_means.append(np.mean(all_reuse))
                skill_reuse_stds.append(np.std(all_reuse))
            else:
                skill_reuse_means.append(0)
                skill_reuse_stds.append(0)
        
        bars = ax3.bar(agent_names, skill_reuse_means, yerr=skill_reuse_stds, 
                      capsize=5, color=['lightblue', 'lightcoral'])
        ax3.set_ylabel('Skill Transfer Success Rate')
        ax3.set_title('Skill Reuse Capability')
        ax3.tick_params(axis='x', rotation=45)
        
        # Plot 4: Episode length comparison
        ax4 = axes[1, 0]
        length_means = []
        length_stds = []
        
        for agent_name, agent_results in self.results.items():
            all_lengths = []
            for result in agent_results:
                all_lengths.extend(result['lengths'][-50:])  # Last 50 episodes
            
            length_means.append(np.mean(all_lengths))
            length_stds.append(np.std(all_lengths))
        
        bars = ax4.bar(agent_names, length_means, yerr=length_stds,
                      capsize=5, color=['lightblue', 'lightcoral'])
        ax4.set_ylabel('Average Episode Length')
        ax4.set_title('Efficiency (Lower is Better)')
        ax4.tick_params(axis='x', rotation=45)
        
        # Plot 5: Final performance comparison
        ax5 = axes[1, 1]
        final_rewards = []
        final_stds = []
        
        for agent_name, agent_results in self.results.items():
            performances = [result['final_performance'] for result in agent_results]
            final_rewards.append(np.mean(performances))
            final_stds.append(np.std(performances))
        
        bars = ax5.bar(agent_names, final_rewards, yerr=final_stds,
                      capsize=5, color=['lightblue', 'lightcoral'])
        ax5.set_ylabel('Final Average Reward')
        ax5.set_title('Final Performance')
        ax5.tick_params(axis='x', rotation=45)
        
        # Plot 6: Goal achievement rates final
        ax6 = axes[1, 2]
        final_goal_rates = []
        goal_rate_stds = []
        
        for agent_name, agent_results in self.results.items():
            goal_rates = [result['final_goal_rate'] for result in agent_results]
            final_goal_rates.append(np.mean(goal_rates))
            goal_rate_stds.append(np.std(goal_rates))
        
        bars = ax6.bar(agent_names, final_goal_rates, yerr=goal_rate_stds,
                      capsize=5, color=['lightblue', 'lightcoral'])
        ax6.set_ylabel('Final Goal Achievement Rate')
        ax6.set_title('Multi-Goal Success Rate')
        ax6.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
        
        # Print detailed analysis
        print("\n📈 Hierarchical RL Analysis Summary:")
        for agent_name, agent_results in self.results.items():
            final_rewards = [result['final_performance'] for result in agent_results]
            final_goals = [result['final_goal_rate'] for result in agent_results]
            
            print(f"\n{agent_name}:")
            print(f"  Final Reward: {np.mean(final_rewards):.2f} ± {np.std(final_rewards):.2f}")
            print(f"  Goal Success Rate: {np.mean(final_goals):.3f} ± {np.std(final_goals):.3f}")
            print(f"  Skill Transfer: {np.mean(skill_reuse_means):.3f}")

# Create and run hierarchical experiment
hierarchical_exp = HierarchicalRLExperiment()

print("🎯 Hierarchical RL Experiment Setup Complete!")
print("🏃‍♂️ Key features being tested:")
print("  • Goal-conditioned learning with HER")
print("  • Multi-goal navigation tasks")
print("  • Skill transfer and reuse")
print("  • Temporal abstraction benefits")
print("\n💡 To run experiment: hierarchical_exp.run_hierarchical_experiment(num_episodes=200, num_seeds=2)")
print("📊 To visualize: hierarchical_exp.visualize_hierarchical_results()")
```


```python
# Experiment 3: Planning Algorithms Comparison

class PlanningAlgorithmsExperiment:
    """Compare different planning approaches."""
    
    def __init__(self):
        self.results = {}
    
    def run_planning_comparison(self, num_episodes=200, num_seeds=2):
        """Compare MCTS, MVE, and random planning."""
        
        print("🎯 Running Planning Algorithms Comparison...")
        print("⚡ Testing: MCTS vs Model-Based Value Expansion vs Random Shooting")
        
        # Create a simple environment for planning
        env_size = 6
        
        # Create base components
        state_dim = env_size * env_size
        action_dim = 4
        
        results = {}
        
        # Test different planning approaches
        planning_configs = {
            'Random Shooting': {
                'use_mcts': False,
                'use_mve': False,
                'use_random': True
            },
            'Model-Based Value Expansion': {
                'use_mcts': False,
                'use_mve': True,
                'use_random': False
            },
            'MCTS Planning': {
                'use_mcts': True,
                'use_mve': False,
                'use_random': False
            }
        }
        
        for planner_name, config in planning_configs.items():
            print(f"\n🔄 Testing {planner_name}...")
            planner_results = []
            
            for seed in range(num_seeds):
                print(f"  Seed {seed + 1}/{num_seeds}")
                
                # Set random seeds
                np.random.seed(seed)
                torch.manual_seed(seed)
                random.seed(seed)
                
                # Create environment and base agent
                env = SimpleGridWorld(size=env_size)
                base_agent = DynaQAgent(state_dim, action_dim)
                
                # Create model ensemble for planning
                model_ensemble = ModelEnsemble(state_dim, action_dim, ensemble_size=3)
                
                # Create planning components based on config
                if config['use_mcts']:
                    # Create simple value network for MCTS
                    value_net = nn.Sequential(
                        nn.Linear(state_dim, 128),
                        nn.ReLU(),
                        nn.Linear(128, 1)
                    ).to(device)
                    mcts_planner = MonteCarloTreeSearch(model_ensemble, value_net)
                    planner = mcts_planner
                elif config['use_mve']:
                    value_net = base_agent.q_network
                    mve_planner = ModelBasedValueExpansion(model_ensemble, value_net)
                    planner = mve_planner
                else:
                    # Random shooting baseline
                    mpc_planner = ModelPredictiveController(model_ensemble, action_dim)
                    planner = mpc_planner
                
                # Episode tracking
                episode_rewards = []
                planning_times = []
                model_accuracy = []
                
                for episode in range(num_episodes):
                    state = env.reset()
                    episode_reward = 0
                    episode_length = 0
                    done = False
                    
                    while not done and episode_length < 100:
                        start_time = time.time()
                        
                        # Get action using planning or base agent
                        if episode > 50:  # Start planning after some model training
                            try:
                                if config['use_mcts']:
                                    # Use MCTS planning
                                    root = planner.search(state, num_simulations=20)
                                    action_probs = planner.get_action_probabilities(root)
                                    action = np.argmax(action_probs)
                                elif config['use_mve']:
                                    # Use MVE planning
                                    action = planner.plan_action(state)
                                else:
                                    # Use MPC/random shooting
                                    action = planner.plan_action(state)
                            except:
                                # Fallback to base agent
                                action = base_agent.get_action(state, epsilon=0.1)
                        else:
                            # Use base agent for initial episodes
                            action = base_agent.get_action(state, epsilon=0.3)
                        
                        planning_time = time.time() - start_time
                        planning_times.append(planning_time)
                        
                        # Take step
                        next_state, reward, done, info = env.step(action)
                        episode_reward += reward
                        episode_length += 1
                        
                        # Store experience and train base agent
                        base_agent.store_experience(state, action, reward, next_state, done)
                        base_agent.update_q_function()
                        
                        # Train model every few steps
                        if episode_length % 5 == 0:
                            model_loss = base_agent.update_model()
                            
                            # Test model accuracy periodically
                            if episode_length % 20 == 0:
                                accuracy = self._test_model_accuracy(model_ensemble, env)
                                model_accuracy.append(accuracy)
                        
                        state = next_state
                    
                    episode_rewards.append(episode_reward)
                    
                    # Progress reporting
                    if (episode + 1) % 50 == 0:
                        avg_reward = np.mean(episode_rewards[-20:])
                        avg_time = np.mean(planning_times[-100:]) if planning_times else 0
                        print(f"    Episode {episode + 1}: Reward={avg_reward:.2f}, Planning Time={avg_time:.4f}s")
                
                # Store results
                planner_results.append({
                    'rewards': episode_rewards,
                    'planning_times': planning_times,
                    'model_accuracy': model_accuracy,
                    'final_performance': np.mean(episode_rewards[-20:])
                })
            
            results[planner_name] = planner_results
        
        self.results = results
        return results
    
    def _test_model_accuracy(self, model_ensemble, env, num_tests=10):
        """Test how accurate the learned model is."""
        if len(model_ensemble.models) == 0:
            return 0.0
        
        accuracies = []
        
        for _ in range(num_tests):
            # Reset environment and take random action
            state = env.reset()
            action = np.random.randint(4)
            
            # Get actual next state
            actual_next_state, actual_reward, _, _ = env.step(action)
            
            # Get model prediction
            try:
                pred_next_state, pred_reward = model_ensemble.predict_mean(
                    torch.FloatTensor(state).to(device),
                    torch.LongTensor([action]).to(device)
                )
                
                # Compute accuracy (inverse of prediction error)
                state_error = torch.norm(pred_next_state.cpu() - torch.FloatTensor(actual_next_state)).item()
                reward_error = abs(pred_reward.cpu().item() - actual_reward)
                
                # Convert to accuracy (1 means perfect, 0 means very inaccurate)
                accuracy = 1.0 / (1.0 + state_error + reward_error)
                accuracies.append(accuracy)
            except:
                accuracies.append(0.0)  # Model failed
        
        return np.mean(accuracies) if accuracies else 0.0
    
    def visualize_planning_results(self):
        """Visualize planning algorithm comparison results."""
        if not self.results:
            print("❌ No results to visualize. Run experiment first.")
            return
        
        print("\n📊 Planning Algorithms Comparison Results")
        print("=" * 50)
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Planning Algorithms Performance Analysis', fontsize=16)
        
        # Plot 1: Learning curves
        ax1 = axes[0, 0]
        colors = ['blue', 'red', 'green']
        for i, (planner_name, planner_results) in enumerate(self.results.items()):
            all_rewards = [result['rewards'] for result in planner_results]
            min_length = min(len(rewards) for rewards in all_rewards)
            
            rewards_array = np.array([rewards[:min_length] for rewards in all_rewards])
            mean_rewards = np.mean(rewards_array, axis=0)
            std_rewards = np.std(rewards_array, axis=0)
            
            episodes = np.arange(min_length)
            ax1.plot(episodes, mean_rewards, label=planner_name, linewidth=2, color=colors[i])
            ax1.fill_between(episodes, mean_rewards - std_rewards, mean_rewards + std_rewards, 
                           alpha=0.3, color=colors[i])
        
        ax1.set_xlabel('Episode')
        ax1.set_ylabel('Average Reward')
        ax1.set_title('Learning Curves Comparison')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Plot 2: Planning time overhead
        ax2 = axes[0, 1]
        planner_names = list(self.results.keys())
        planning_times = []
        time_stds = []
        
        for planner_name, planner_results in self.results.items():
            all_times = []
            for result in planner_results:
                if result['planning_times']:
                    # Use times from later episodes when planning is active
                    relevant_times = result['planning_times'][len(result['planning_times'])//2:]
                    all_times.extend(relevant_times)
            
            if all_times:
                planning_times.append(np.mean(all_times) * 1000)  # Convert to ms
                time_stds.append(np.std(all_times) * 1000)
            else:
                planning_times.append(0)
                time_stds.append(0)
        
        bars = ax2.bar(planner_names, planning_times, yerr=time_stds, capsize=5, 
                      color=['lightblue', 'lightcoral', 'lightgreen'])
        ax2.set_ylabel('Average Planning Time (ms)')
        ax2.set_title('Computational Overhead')
        ax2.tick_params(axis='x', rotation=45)
        
        # Plot 3: Final performance comparison
        ax3 = axes[1, 0]
        final_performances = []
        perf_stds = []
        
        for planner_name, planner_results in self.results.items():
            performances = [result['final_performance'] for result in planner_results]
            final_performances.append(np.mean(performances))
            perf_stds.append(np.std(performances))
        
        bars = ax3.bar(planner_names, final_performances, yerr=perf_stds, capsize=5,
                      color=['lightblue', 'lightcoral', 'lightgreen'])
        ax3.set_ylabel('Final Average Reward')
        ax3.set_title('Final Performance')
        ax3.tick_params(axis='x', rotation=45)
        
        # Plot 4: Model accuracy over time
        ax4 = axes[1, 1]
        for planner_name, planner_results in self.results.items():
            all_accuracies = []
            for result in planner_results:
                if result['model_accuracy']:
                    all_accuracies.append(result['model_accuracy'])
            
            if all_accuracies:
                # Pad or truncate to same length
                min_length = min(len(acc) for acc in all_accuracies) if all_accuracies else 0
                if min_length > 0:
                    acc_array = np.array([acc[:min_length] for acc in all_accuracies])
                    mean_acc = np.mean(acc_array, axis=0)
                    
                    time_steps = np.arange(len(mean_acc))
                    ax4.plot(time_steps, mean_acc, label=planner_name, linewidth=2)
        
        ax4.set_xlabel('Model Update Steps')
        ax4.set_ylabel('Model Accuracy')
        ax4.set_title('Model Learning Progress')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
        
        # Print summary statistics
        print("\n📈 Planning Algorithms Summary:")
        for planner_name, planner_results in self.results.items():
            performances = [result['final_performance'] for result in planner_results]
            times = []
            for result in planner_results:
                if result['planning_times']:
                    times.extend(result['planning_times'])
            
            mean_perf = np.mean(performances)
            std_perf = np.std(performances)
            mean_time = np.mean(times) * 1000 if times else 0  # ms
            
            print(f"\n{planner_name}:")
            print(f"  Final Performance: {mean_perf:.2f} ± {std_perf:.2f}")
            print(f"  Average Planning Time: {mean_time:.2f} ms")
            print(f"  Performance/Time Ratio: {mean_perf/max(mean_time/1000, 0.001):.1f}")

# Comprehensive Integration and Summary
print("\n" + "="*80)
print("🎉 COMPREHENSIVE CA15 IMPLEMENTATION COMPLETED!")
print("="*80)

print("""
📚 THEORETICAL COVERAGE:
├── Model-Based Reinforcement Learning
│   ├── Environment dynamics learning
│   ├── Model-Predictive Control (MPC)
│   ├── Dyna-Q algorithm
│   └── Uncertainty quantification with ensembles
│
├── Hierarchical Reinforcement Learning  
│   ├── Options framework
│   ├── Goal-conditioned RL with HER
│   ├── Hierarchical Actor-Critic (HAC)
│   └── Feudal Networks architecture
│
└── Advanced Planning and Control
    ├── Monte Carlo Tree Search (MCTS)
    ├── Model-Based Value Expansion (MVE)
    ├── Latent space planning
    └── World models (PlaNet-inspired)

🔧 IMPLEMENTATION HIGHLIGHTS:
├── Complete neural network architectures
├── End-to-end training algorithms  
├── Uncertainty estimation methods
├── Hierarchical policy structures
├── Advanced planning algorithms
└── Comprehensive evaluation frameworks

🧪 EXPERIMENTAL VALIDATION:
├── Model-based vs model-free comparison
├── Hierarchical RL benefits demonstration
├── Planning algorithms effectiveness
└── Integration and real-world applicability

📊 KEY LEARNING OUTCOMES:
✅ Understanding of advanced RL paradigms
✅ Practical implementation experience
✅ Performance analysis and comparison
✅ Real-world application insights
✅ State-of-the-art method integration

🚀 READY FOR EXECUTION:
• All components are fully implemented
• Experiments are ready to run
• Comprehensive analysis tools provided
• Educational content with theory and practice
""")

# Create planning experiment instance
planning_exp = PlanningAlgorithmsExperiment()

print("\n💡 NEXT STEPS:")
print("1. Run Model-Based experiment: experiment.run_experiment(agent_configs, num_episodes=150)")
print("2. Run Hierarchical experiment: hierarchical_exp.run_hierarchical_experiment(num_episodes=150)")  
print("3. Run Planning comparison: planning_exp.run_planning_comparison(num_episodes=150)")
print("4. Analyze all results with respective .analyze_results() or .visualize_*_results() methods")

print(f"\n🎯 CA15 Notebook Successfully Created with {len(open('/Users/tahamajs/Documents/uni/DRL/CAs/CA15.ipynb').readlines())} lines of comprehensive content!")
```
