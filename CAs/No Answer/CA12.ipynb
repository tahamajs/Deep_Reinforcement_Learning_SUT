{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c444804b",
   "metadata": {},
   "source": [
    "# CA12: Multi-Agent Reinforcement Learning and Advanced Policy Methods\n",
    "\n",
    "## Deep Reinforcement Learning - Session 12\n",
    "\n",
    "**Multi-Agent Reinforcement Learning (MARL), Advanced Policy Gradient Methods, and Distributed Training**\n",
    "\n",
    "This notebook explores advanced reinforcement learning topics including multi-agent systems, sophisticated policy gradient methods, distributed training techniques, and modern approaches to collaborative and competitive learning environments.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand multi-agent reinforcement learning fundamentals\n",
    "2. Implement cooperative and competitive MARL algorithms\n",
    "3. Master advanced policy gradient methods (PPO, TRPO, SAC variants)\n",
    "4. Explore distributed training and asynchronous methods\n",
    "5. Implement communication and coordination mechanisms\n",
    "6. Understand game-theoretic foundations of MARL\n",
    "7. Apply meta-learning and few-shot adaptation\n",
    "8. Analyze emergent behaviors in multi-agent systems\n",
    "\n",
    "### Notebook Structure:\n",
    "1. **Multi-Agent Foundations** - Game theory and MARL basics\n",
    "2. **Cooperative Multi-Agent Learning** - Centralized training, decentralized execution\n",
    "3. **Competitive and Mixed-Motive Systems** - Self-play and adversarial training\n",
    "4. **Advanced Policy Methods** - PPO variants, SAC improvements, TRPO\n",
    "5. **Distributed Reinforcement Learning** - A3C, IMPALA, and modern distributed methods\n",
    "6. **Communication and Coordination** - Message passing and emergent communication\n",
    "7. **Meta-Learning in RL** - Few-shot adaptation and transfer learning\n",
    "8. **Comprehensive Applications** - Real-world multi-agent scenarios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0788ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Normal, Categorical, MultivariateNormal, kl_divergence\nimport torch.multiprocessing as mp\nimport gymnasium as gym\nfrom gymnasium import spaces\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict, deque, namedtuple\nimport random\nimport pickle\nimport json\nimport copy\nimport time\nimport threading\nfrom typing import Tuple, List, Dict, Optional, Union, NamedTuple, Any\nimport warnings\nfrom dataclasses import dataclass, field\nimport math\nfrom tqdm import tqdm\nfrom abc import ABC, abstractmethod\nimport itertools\nwarnings.filterwarnings('ignore')\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.distributed as dist\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.utils.data import DataLoader, Dataset\nimport networkx as nx\nfrom scipy.spatial.distance import pdist, squareform\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nfrom scipy.optimize import minimize, linprog\nfrom scipy.special import softmax\nimport cvxpy as cp\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nrandom.seed(SEED)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nn_gpus = torch.cuda.device_count()\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nprint(f\"🤖 Multi-Agent Reinforcement Learning Environment Setup\")\nprint(f\"Device: {device}\")\nprint(f\"Available GPUs: {n_gpus}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\nplt.style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (16, 10)\nplt.rcParams['font.size'] = 12\nplt.rcParams['axes.titlesize'] = 16\nplt.rcParams['axes.labelsize'] = 14\nplt.rcParams['xtick.labelsize'] = 11\nplt.rcParams['ytick.labelsize'] = 11\nplt.rcParams['legend.fontsize'] = 11\nagent_colors = sns.color_palette(\"Set2\", 8)\nperformance_colors = sns.color_palette(\"viridis\", 6)\nsns.set_palette(agent_colors)\n@dataclass\nclass MultiAgentConfig:\n    n_agents: int = 2\n    state_dim: int = 10\n    action_dim: int = 4\n    hidden_dim: int = 128\n    lr: float = 3e-4\n    gamma: float = 0.99\n    tau: float = 0.005\n    batch_size: int = 256\n    buffer_size: int = 100000\n    update_freq: int = 10\n    communication: bool = False\n    message_dim: int = 32\n    coordination_mechanism: str = \"centralized\"\n@dataclass \nclass PolicyConfig:\n    algorithm: str = \"PPO\"\n    clip_ratio: float = 0.2\n    target_kl: float = 0.01\n    entropy_coef: float = 0.01\n    value_coef: float = 0.5\n    max_grad_norm: float = 0.5\n    n_epochs: int = 10\n    minibatch_size: int = 64\n    use_gae: bool = True\n    gae_lambda: float = 0.95\nma_config = MultiAgentConfig()\npolicy_config = PolicyConfig()\nprint(\"✅ Multi-Agent RL environment setup complete!\")\nprint(f\"🎯 Configuration: {ma_config.n_agents} agents, {ma_config.coordination_mechanism} coordination\")\nprint(\"🚀 Ready for advanced multi-agent reinforcement learning!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e214d5e",
   "metadata": {},
   "source": [
    "# Section 1: Multi-Agent Foundations and Game Theory\n",
    "\n",
    "## 1.1 Theoretical Foundation\n",
    "\n",
    "### Multi-Agent Reinforcement Learning (MARL)\n",
    "\n",
    "Multi-Agent Reinforcement Learning extends single-agent RL to environments with multiple learning agents. Key challenges include:\n",
    "\n",
    "1. **Non-stationarity**: The environment appears non-stationary from each agent's perspective as other agents learn\n",
    "2. **Partial observability**: Agents may have limited information about others' actions and observations\n",
    "3. **Credit assignment**: Determining individual contributions to team rewards\n",
    "4. **Scalability**: Computational complexity grows exponentially with number of agents\n",
    "5. **Equilibrium concepts**: Finding stable solutions in multi-agent settings\n",
    "\n",
    "### Game-Theoretic Foundations\n",
    "\n",
    "**Nash Equilibrium**: A strategy profile where no agent can improve by unilaterally changing strategy.\n",
    "\n",
    "For agents $i = 1, ..., n$ with strategy spaces $S_i$ and utility functions $u_i(s_1, ..., s_n)$:\n",
    "$$s^* = (s_1^*, ..., s_n^*) \\text{ is a Nash equilibrium if } \\forall i, s_i: u_i(s_i^*, s_{-i}^*) \\geq u_i(s_i, s_{-i}^*)$$\n",
    "\n",
    "**Pareto Optimality**: A strategy profile is Pareto optimal if no other profile improves at least one agent's utility without decreasing another's.\n",
    "\n",
    "**Stackelberg Equilibrium**: Leader-follower game structure where one agent commits to a strategy first.\n",
    "\n",
    "### MARL Paradigms\n",
    "\n",
    "1. **Independent Learning**: Each agent treats others as part of the environment\n",
    "2. **Joint Action Learning**: Agents learn about others' actions and adapt accordingly  \n",
    "3. **Multi-Agent Actor-Critic (MAAC)**: Centralized training with decentralized execution\n",
    "4. **Communication-Based Learning**: Agents exchange information to coordinate\n",
    "\n",
    "### Cooperation vs Competition Spectrum\n",
    "\n",
    "- **Fully Cooperative**: Shared reward, common goal (e.g., team sports)\n",
    "- **Fully Competitive**: Zero-sum game (e.g., adversarial settings)\n",
    "- **Mixed-Motive**: Partially cooperative and competitive (e.g., resource sharing)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Multi-Agent MDP (MMDP)**:\n",
    "- State space: $\\mathcal{S}$\n",
    "- Joint action space: $\\mathcal{A} = \\mathcal{A}_1 \\times ... \\times \\mathcal{A}_n$\n",
    "- Transition dynamics: $P(s'|s, a_1, ..., a_n)$\n",
    "- Reward functions: $R_i(s, a_1, ..., a_n, s')$ for each agent $i$\n",
    "- Discount factor: $\\gamma \\in [0, 1)$\n",
    "\n",
    "**Policy Gradient in MARL**:\n",
    "$$\\nabla_{\\theta_i} J_i(\\theta_i) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\sum_{t=0}^T \\nabla_{\\theta_i} \\log \\pi_{\\theta_i}(a_{i,t}|o_{i,t}) A_i^t]$$\n",
    "\n",
    "Where $A_i^t$ is agent $i$'s advantage at time $t$, which can be computed using various methods including multi-agent value functions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GameTheoryUtils:\n    @staticmethod\n    def find_nash_equilibria(payoff_matrices):\n        n_players = len(payoff_matrices)\n        if n_players != 2:\n            raise NotImplementedError(\"Only 2-player games supported\")\n        matrix_a, matrix_b = payoff_matrices[0], payoff_matrices[1]\n        nash_equilibria = []\n        rows, cols = matrix_a.shape\n        for i in range(rows):\n            for j in range(cols):\n                is_nash = True\n                for i_prime in range(rows):\n                    if matrix_a[i_prime, j] > matrix_a[i, j]:\n                        is_nash = False\n                        break\n                if is_nash:\n                    for j_prime in range(cols):\n                        if matrix_b[i, j_prime] > matrix_b[i, j]:\n                            is_nash = False\n                            break\n                if is_nash:\n                    nash_equilibria.append((i, j))\n        return nash_equilibria\n    @staticmethod\n    def is_pareto_optimal(payoff_matrices, strategy_profile):\n        current_payoffs = [matrix[strategy_profile] for matrix in payoff_matrices]\n        for profile in itertools.product(*[range(matrix.shape[i]) for i, matrix in enumerate(payoff_matrices)]):\n            if profile == strategy_profile:\n                continue\n            candidate_payoffs = [matrix[profile] for matrix in payoff_matrices]\n            dominates = True\n            strictly_better = False\n            for i in range(len(current_payoffs)):\n                if candidate_payoffs[i] < current_payoffs[i]:\n                    dominates = False\n                    break\n                elif candidate_payoffs[i] > current_payoffs[i]:\n                    strictly_better = True\n            if dominates and strictly_better:\n                return False\n        return True\n    @staticmethod\n    def compute_best_response(payoff_matrix, opponent_strategy):\n        expected_payoffs = payoff_matrix @ opponent_strategy\n        return np.zeros_like(expected_payoffs).at[np.argmax(expected_payoffs)].set(1.0)\nclass MultiAgentEnvironment:\n    def __init__(self, n_agents, state_dim, action_dim, cooperative=True):\n        self.n_agents = n_agents\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.cooperative = cooperative\n        self.state = None\n        self.step_count = 0\n        self.max_steps = 200\n    def reset(self):\n        self.state = np.random.randn(self.state_dim)\n        self.step_count = 0\n        return [self.state.copy() for _ in range(self.n_agents)]\n    def step(self, actions):\n        self.step_count += 1\n        joint_action = np.mean(actions, axis=0)\n        noise = np.random.randn(self.state_dim) * 0.1\n        self.state = 0.9 * self.state + 0.1 * joint_action[:self.state_dim] + noise\n        if self.cooperative:\n            coordination_bonus = -np.mean([np.linalg.norm(actions[i] - joint_action) for i in range(self.n_agents)])\n            base_reward = -np.linalg.norm(self.state)\n            rewards = [base_reward + coordination_bonus] * self.n_agents\n        else:\n            rewards = []\n            for i in range(self.n_agents):\n                individual_reward = -np.linalg.norm(self.state - actions[i][:self.state_dim])\n                competition_penalty = sum([np.linalg.norm(actions[i] - actions[j]) \n                                         for j in range(self.n_agents) if j != i]) * 0.1\n                rewards.append(individual_reward - competition_penalty)\n        done = self.step_count >= self.max_steps\n        next_states = [self.state.copy() for _ in range(self.n_agents)]\n        return next_states, rewards, done\n    def render(self):\n        pass\ndef demonstrate_game_theory():\n    print(\"🎯 Game Theory Analysis Demo\")\n    print(\"\\n1. Prisoner's Dilemma:\")\n    prisoner_a = np.array([[-1, -3], [0, -2]])\n    prisoner_b = np.array([[-1, 0], [-3, -2]])\n    print(\"Player 1 payoff matrix:\")\n    print(prisoner_a)\n    print(\"Player 2 payoff matrix:\")\n    print(prisoner_b)\n    nash_eq = GameTheoryUtils.find_nash_equilibria([prisoner_a, prisoner_b])\n    print(f\"Nash equilibria: {nash_eq}\")\n    for eq in nash_eq:\n        is_pareto = GameTheoryUtils.is_pareto_optimal([prisoner_a, prisoner_b], eq)\n        print(f\"Strategy {eq}: Pareto optimal = {is_pareto}\")\n    print(\"\\n2. Coordination Game:\")\n    coord_a = np.array([[2, 0], [0, 1]])\n    coord_b = np.array([[2, 0], [0, 1]])\n    print(\"Coordination game (both players have same payoffs):\")\n    print(coord_a)\n    nash_eq = GameTheoryUtils.find_nash_equilibria([coord_a, coord_b])\n    print(f\"Nash equilibria: {nash_eq}\")\n    return prisoner_a, prisoner_b, coord_a, coord_b\ndef test_multi_agent_env():\n    print(\"\\n🤖 Multi-Agent Environment Test\")\n    print(\"Testing cooperative environment:\")\n    coop_env = MultiAgentEnvironment(n_agents=3, state_dim=4, action_dim=4, cooperative=True)\n    states = coop_env.reset()\n    print(f\"Initial states shape: {[s.shape for s in states]}\")\n    actions = [np.random.randn(coop_env.action_dim) for _ in range(coop_env.n_agents)]\n    next_states, rewards, done = coop_env.step(actions)\n    print(f\"Rewards (cooperative): {rewards}\")\n    print(f\"All agents get same reward: {len(set(rewards)) == 1}\")\n    print(\"\\nTesting competitive environment:\")\n    comp_env = MultiAgentEnvironment(n_agents=3, state_dim=4, action_dim=4, cooperative=False)\n    states = comp_env.reset()\n    next_states, rewards, done = comp_env.step(actions)\n    print(f\"Rewards (competitive): {rewards}\")\n    print(f\"Agents get different rewards: {len(set(rewards)) > 1}\")\n    return coop_env, comp_env\ngame_matrices = demonstrate_game_theory()\nenvironments = test_multi_agent_env()\nprint(\"\\n✅ Game theory and multi-agent foundations implemented successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9507ce",
   "metadata": {},
   "source": [
    "# Section 2: Cooperative Multi-Agent Learning\n",
    "\n",
    "## 2.1 Centralized Training, Decentralized Execution (CTDE)\n",
    "\n",
    "The CTDE paradigm is fundamental to modern cooperative MARL:\n",
    "\n",
    "**Training Phase**: \n",
    "- Central coordinator has access to global information\n",
    "- Can compute joint value functions and coordinate policy updates\n",
    "- Addresses non-stationarity through centralized critic\n",
    "\n",
    "**Execution Phase**:\n",
    "- Each agent acts based on local observations only\n",
    "- No communication required during deployment\n",
    "- Maintains scalability and robustness\n",
    "\n",
    "### Multi-Agent Actor-Critic (MAAC)\n",
    "\n",
    "**Centralized Critic**: Estimates joint action-value function $Q(s, a_1, ..., a_n)$\n",
    "\n",
    "**Actor Update**: Each agent $i$ updates policy using centralized critic:\n",
    "$$\\nabla_{\\theta_i} J_i = \\mathbb{E}[\\nabla_{\\theta_i} \\log \\pi_{\\theta_i}(a_i|o_i) \\cdot Q^{\\pi}(s, a_1, ..., a_n)]$$\n",
    "\n",
    "**Critic Update**: Minimize joint TD error:\n",
    "$$L(\\phi) = \\mathbb{E}[(Q_{\\phi}(s, a_1, ..., a_n) - y)^2]$$\n",
    "$$y = r + \\gamma Q_{\\phi'}(s', \\pi_{\\theta_1'}(o_1'), ..., \\pi_{\\theta_n'}(o_n'))$$\n",
    "\n",
    "### Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "\n",
    "Extension of DDPG to multi-agent settings:\n",
    "\n",
    "1. **Centralized Critics**: Each agent maintains its own critic that uses global information\n",
    "2. **Experience Replay**: Shared replay buffer with transitions $(s, a_1, ..., a_n, r_1, ..., r_n, s')$\n",
    "3. **Target Networks**: Slow-updating target networks for stability\n",
    "\n",
    "**Critic Loss for Agent $i$**:\n",
    "$$L_i(\\phi_i) = \\mathbb{E}[(Q_{\\phi_i}(s, a_1, ..., a_n) - y_i)^2]$$\n",
    "$$y_i = r_i + \\gamma Q_{\\phi_i'}(s', \\mu_{\\theta_1'}(o_1'), ..., \\mu_{\\theta_n'}(o_n'))$$\n",
    "\n",
    "**Actor Loss for Agent $i$**:\n",
    "$$L_i(\\theta_i) = -\\mathbb{E}[Q_{\\phi_i}(s, a_1|_{a_i=\\mu_{\\theta_i}(o_i)}, ..., a_n)]$$\n",
    "\n",
    "### Counterfactual Multi-Agent Policy Gradients (COMA)\n",
    "\n",
    "Uses counterfactual reasoning for credit assignment:\n",
    "\n",
    "**Counterfactual Baseline**:\n",
    "$$A_i(s, a) = Q(s, a) - \\sum_{a_i'} \\pi_i(a_i'|o_i) Q(s, a_{-i}, a_i')$$\n",
    "\n",
    "This baseline removes the effect of agent $i$'s action, isolating its contribution to the team reward.\n",
    "\n",
    "### Value Decomposition Networks (VDN)\n",
    "\n",
    "Decomposes team value function into individual components:\n",
    "$$Q_{tot}(s, a) = \\sum_{i=1}^n Q_i(o_i, a_i)$$\n",
    "\n",
    "**Advantages**:\n",
    "- Individual value functions can be learned independently\n",
    "- Naturally handles partial observability\n",
    "- Maintains convergence guarantees under certain conditions\n",
    "\n",
    "**Limitations**:\n",
    "- Additivity assumption may be too restrictive\n",
    "- Cannot represent complex coordination patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa73cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n    def __init__(self, obs_dim, action_dim, hidden_dim=128, max_action=1.0):\n        super(Actor, self).__init__()\n        self.max_action = max_action\n        self.net = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim),\n            nn.Tanh()\n        )\n    def forward(self, obs):\n        return self.max_action * self.net(obs)\nclass Critic(nn.Module):\n    def __init__(self, total_obs_dim, total_action_dim, hidden_dim=128):\n        super(Critic, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(total_obs_dim + total_action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def forward(self, obs, actions):\n        return self.net(torch.cat([obs, actions], dim=-1))\nclass MADDPGAgent:\n    def __init__(self, agent_id, obs_dim, action_dim, total_obs_dim, total_action_dim,\n                 lr_actor=1e-4, lr_critic=1e-3, gamma=0.99, tau=0.005):\n        self.agent_id = agent_id\n        self.gamma = gamma\n        self.tau = tau\n        self.actor = Actor(obs_dim, action_dim).to(device)\n        self.critic = Critic(total_obs_dim, total_action_dim).to(device)\n        self.target_actor = Actor(obs_dim, action_dim).to(device)\n        self.target_critic = Critic(total_obs_dim, total_action_dim).to(device)\n        self.target_actor.load_state_dict(self.actor.state_dict())\n        self.target_critic.load_state_dict(self.critic.state_dict())\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n        self.noise_scale = 0.1\n        self.noise_decay = 0.9999\n    def act(self, obs, add_noise=True):\n        obs = torch.FloatTensor(obs).to(device)\n        action = self.actor(obs).cpu().data.numpy()\n        if add_noise:\n            noise = np.random.normal(0, self.noise_scale, size=action.shape)\n            action += noise\n            self.noise_scale *= self.noise_decay\n        return np.clip(action, -1, 1)\n    def update_critic(self, obs, actions, rewards, next_obs, next_actions, dones):\n        obs = torch.FloatTensor(obs).to(device)\n        actions = torch.FloatTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_obs = torch.FloatTensor(next_obs).to(device)\n        next_actions = torch.FloatTensor(next_actions).to(device)\n        dones = torch.BoolTensor(dones).to(device)\n        current_q = self.critic(obs, actions).squeeze()\n        with torch.no_grad():\n            target_q = self.target_critic(next_obs, next_actions).squeeze()\n            target_q = rewards + self.gamma * target_q * ~dones\n        critic_loss = F.mse_loss(current_q, target_q)\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)\n        self.critic_optimizer.step()\n        return critic_loss.item()\n    def update_actor(self, obs, actions):\n        obs = torch.FloatTensor(obs).to(device)\n        actions = torch.FloatTensor(actions).to(device)\n        actions_pred = actions.clone()\n        agent_obs = obs[:, self.agent_id]\n        actions_pred[:, self.agent_id] = self.actor(agent_obs)\n        actor_loss = -self.critic(obs.view(obs.size(0), -1), \n                                 actions_pred.view(actions_pred.size(0), -1)).mean()\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n        self.actor_optimizer.step()\n        return actor_loss.item()\n    def soft_update(self):\n        for target, source in zip(self.target_actor.parameters(), self.actor.parameters()):\n            target.data.copy_(self.tau * source.data + (1.0 - self.tau) * target.data)\n        for target, source in zip(self.target_critic.parameters(), self.critic.parameters()):\n            target.data.copy_(self.tau * source.data + (1.0 - self.tau) * target.data)\nclass MADDPG:\n    def __init__(self, n_agents, obs_dim, action_dim, buffer_size=100000):\n        self.n_agents = n_agents\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        total_obs_dim = n_agents * obs_dim\n        total_action_dim = n_agents * action_dim\n        self.agents = [\n            MADDPGAgent(i, obs_dim, action_dim, total_obs_dim, total_action_dim)\n            for i in range(n_agents)\n        ]\n        self.replay_buffer = ReplayBuffer(buffer_size)\n    def act(self, observations, add_noise=True):\n        actions = []\n        for i, agent in enumerate(self.agents):\n            action = agent.act(observations[i], add_noise)\n            actions.append(action)\n        return actions\n    def step(self, states, actions, rewards, next_states, dones):\n        self.replay_buffer.push(states, actions, rewards, next_states, dones)\n        if len(self.replay_buffer) > ma_config.batch_size:\n            self.update()\n    def update(self):\n        batch = self.replay_buffer.sample(ma_config.batch_size)\n        states, actions, rewards, next_states, dones = batch\n        states_flat = np.array(states).reshape(len(states), -1)\n        actions_flat = np.array(actions).reshape(len(actions), -1)\n        next_states_flat = np.array(next_states).reshape(len(next_states), -1)\n        next_actions = []\n        for i, agent in enumerate(self.agents):\n            next_obs = torch.FloatTensor(next_states).to(device)[:, i]\n            next_action = agent.target_actor(next_obs)\n            next_actions.append(next_action)\n        next_actions_flat = torch.cat(next_actions, dim=-1).cpu().data.numpy()\n        losses = {'actor': [], 'critic': []}\n        for i, agent in enumerate(self.agents):\n            agent_rewards = np.array(rewards)[:, i]\n            agent_dones = np.array(dones)\n            critic_loss = agent.update_critic(\n                states_flat, actions_flat, agent_rewards,\n                next_states_flat, next_actions_flat, agent_dones\n            )\n            losses['critic'].append(critic_loss)\n            actor_loss = agent.update_actor(states, actions)\n            losses['actor'].append(actor_loss)\n            agent.soft_update()\n        return losses\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n    def push(self, states, actions, rewards, next_states, dones):\n        self.buffer.append((states, actions, rewards, next_states, dones))\n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        return states, actions, rewards, next_states, dones\n    def __len__(self):\n        return len(self.buffer)\nclass VDNAgent(nn.Module):\n    def __init__(self, obs_dim, action_dim, hidden_dim=64):\n        super(VDNAgent, self).__init__()\n        self.net = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n    def forward(self, obs):\n        return self.net(obs)\nclass VDN:\n    def __init__(self, n_agents, obs_dim, action_dim, lr=1e-3):\n        self.n_agents = n_agents\n        self.agents = [VDNAgent(obs_dim, action_dim).to(device) for _ in range(n_agents)]\n        self.target_agents = [VDNAgent(obs_dim, action_dim).to(device) for _ in range(n_agents)]\n        for agent, target in zip(self.agents, self.target_agents):\n            target.load_state_dict(agent.state_dict())\n        self.optimizers = [optim.Adam(agent.parameters(), lr=lr) for agent in self.agents]\n        self.replay_buffer = ReplayBuffer(10000)\n    def act(self, observations, epsilon=0.1):\n        actions = []\n        for i, agent in enumerate(self.agents):\n            if np.random.random() < epsilon:\n                action = np.random.randint(agent.net[-1].out_features)\n            else:\n                obs = torch.FloatTensor(observations[i]).to(device)\n                q_values = agent(obs)\n                action = q_values.argmax().item()\n            actions.append(action)\n        return actions\n    def update(self, batch_size=32):\n        if len(self.replay_buffer) < batch_size:\n            return\n        batch = self.replay_buffer.sample(batch_size)\n        states, actions, rewards, next_states, dones = batch\n        total_loss = 0\n        team_rewards = torch.FloatTensor([sum(r) for r in rewards]).to(device)\n        team_dones = torch.BoolTensor([any(d) for d in dones]).to(device)\n        for i, (agent, target_agent, optimizer) in enumerate(zip(self.agents, self.target_agents, self.optimizers)):\n            agent_states = torch.FloatTensor([s[i] for s in states]).to(device)\n            agent_actions = torch.LongTensor([a[i] for a in actions]).to(device)\n            agent_next_states = torch.FloatTensor([s[i] for s in next_states]).to(device)\n            q_values = agent(agent_states)\n            q_values = q_values.gather(1, agent_actions.unsqueeze(1)).squeeze()\n            with torch.no_grad():\n                next_q_values = target_agent(agent_next_states).max(1)[0]\n                target_q = team_rewards + 0.99 * next_q_values * ~team_dones\n            loss = F.mse_loss(q_values, target_q)\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n        tau = 0.005\n        for agent, target_agent in zip(self.agents, self.target_agents):\n            for param, target_param in zip(agent.parameters(), target_agent.parameters()):\n                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n        return total_loss / self.n_agents\nprint(\"🤖 Cooperative multi-agent algorithms implemented successfully!\")\nprint(\"✅ MADDPG, VDN, and supporting utilities ready for training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf213f",
   "metadata": {},
   "source": [
    "# Section 3: Advanced Policy Gradient Methods\n",
    "\n",
    "## 3.1 Proximal Policy Optimization (PPO)\n",
    "\n",
    "PPO addresses the challenge of step size in policy gradient methods through clipped objective functions.\n",
    "\n",
    "### PPO-Clip Objective\n",
    "\n",
    "**Probability Ratio**:\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
    "\n",
    "**Clipped Objective**:\n",
    "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[\\min(r_t(\\theta)A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]$$\n",
    "\n",
    "Where $\\epsilon$ is the clipping parameter (typically 0.1-0.3) and $A_t$ is the advantage estimate.\n",
    "\n",
    "### Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "TRPO constrains policy updates to stay within a trust region:\n",
    "\n",
    "**Objective**:\n",
    "$$\\max_\\theta \\hat{\\mathbb{E}}_t[\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}A_t]$$\n",
    "\n",
    "**Subject to**:\n",
    "$$\\hat{\\mathbb{E}}_t[KL[\\pi_{\\theta_{old}}(\\cdot|s_t), \\pi_\\theta(\\cdot|s_t)]] \\leq \\delta$$\n",
    "\n",
    "**Conjugate Gradient Solution**:\n",
    "TRPO uses conjugate gradient to solve the constrained optimization problem:\n",
    "$$g = \\nabla_\\theta L(\\theta_{old})$$\n",
    "$$H = \\nabla_\\theta^2 KL[\\pi_{\\theta_{old}}, \\pi_\\theta]$$\n",
    "$$\\theta_{new} = \\theta_{old} + \\sqrt{\\frac{2\\delta}{g^T H^{-1} g}} H^{-1} g$$\n",
    "\n",
    "### Soft Actor-Critic (SAC)\n",
    "\n",
    "SAC maximizes both expected return and entropy for better exploration:\n",
    "\n",
    "**Objective**:\n",
    "$$J(\\pi) = \\sum_{t=0}^T \\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi}[r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t))]$$\n",
    "\n",
    "Where $\\alpha$ is the temperature parameter controlling exploration-exploitation trade-off.\n",
    "\n",
    "**Soft Q-Function Updates**:\n",
    "$$J_Q(\\phi) = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \\sim \\mathcal{D}}[\\frac{1}{2}(Q_\\phi(s_t, a_t) - y_t)^2]$$\n",
    "$$y_t = r_t + \\gamma \\mathbb{E}_{a_{t+1} \\sim \\pi}[Q_{\\phi'}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi(a_{t+1}|s_{t+1})]$$\n",
    "\n",
    "**Policy Updates**:\n",
    "$$J_\\pi(\\theta) = \\mathbb{E}_{s_t \\sim \\mathcal{D}, a_t \\sim \\pi_\\theta}[\\alpha \\log \\pi_\\theta(a_t|s_t) - Q_\\phi(s_t, a_t)]$$\n",
    "\n",
    "### Advanced Advantage Estimation\n",
    "\n",
    "**Generalized Advantage Estimation (GAE)**:\n",
    "$$A_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^\\infty (\\gamma\\lambda)^l \\delta_{t+l}^V$$\n",
    "\n",
    "Where $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error.\n",
    "\n",
    "GAE balances bias and variance:\n",
    "- $\\lambda = 0$: Low variance, high bias (TD error)\n",
    "- $\\lambda = 1$: High variance, low bias (Monte Carlo)\n",
    "\n",
    "### Multi-Agent Policy Gradient Extensions\n",
    "\n",
    "**Multi-Agent PPO (MAPPO)**:\n",
    "- Centralized value function: $V(s_1, ..., s_n)$\n",
    "- Individual actor updates with shared value baseline\n",
    "- Addresses non-stationarity through centralized training\n",
    "\n",
    "**Multi-Agent SAC (MASAC)**:\n",
    "- Individual entropy regularization per agent\n",
    "- Shared experience replay buffer\n",
    "- Independent policy and Q-function updates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPONetwork(nn.Module):\n    def __init__(self, obs_dim, action_dim, hidden_dim=64, discrete=True):\n        super(PPONetwork, self).__init__()\n        self.discrete = discrete\n        self.shared = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        if discrete:\n            self.actor = nn.Linear(hidden_dim, action_dim)\n        else:\n            self.actor_mean = nn.Linear(hidden_dim, action_dim)\n            self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\n        self.critic = nn.Linear(hidden_dim, 1)\n    def forward(self, obs):\n        shared_features = self.shared(obs)\n        value = self.critic(shared_features)\n        if self.discrete:\n            action_logits = self.actor(shared_features)\n            return action_logits, value\n        else:\n            action_mean = self.actor_mean(shared_features)\n            action_std = torch.exp(self.actor_logstd.expand_as(action_mean))\n            return (action_mean, action_std), value\n    def get_action_and_value(self, obs, action=None):\n        if self.discrete:\n            logits, value = self.forward(obs)\n            probs = Categorical(logits=logits)\n            if action is None:\n                action = probs.sample()\n            return action, probs.log_prob(action), probs.entropy(), value\n        else:\n            (mean, std), value = self.forward(obs)\n            probs = Normal(mean, std)\n            if action is None:\n                action = probs.sample()\n            return action, probs.log_prob(action).sum(-1), probs.entropy().sum(-1), value\nclass PPOAgent:\n    def __init__(self, obs_dim, action_dim, lr=3e-4, discrete=True):\n        self.network = PPONetwork(obs_dim, action_dim, discrete=discrete).to(device)\n        self.optimizer = optim.Adam(self.network.parameters(), lr=lr, eps=1e-5)\n        self.discrete = discrete\n        self.clip_coef = 0.2\n        self.ent_coef = 0.01\n        self.vf_coef = 0.5\n        self.max_grad_norm = 0.5\n        self.target_kl = 0.01\n    def get_action_and_value(self, obs, action=None):\n        return self.network.get_action_and_value(obs, action)\n    def update(self, rollouts, n_epochs=10, minibatch_size=64):\n        obs, actions, logprobs, returns, values, advantages = rollouts\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        clipfracs = []\n        total_losses = []\n        for epoch in range(n_epochs):\n            indices = torch.randperm(len(obs))\n            for start in range(0, len(obs), minibatch_size):\n                end = start + minibatch_size\n                mb_indices = indices[start:end]\n                mb_obs = obs[mb_indices]\n                mb_actions = actions[mb_indices]\n                mb_logprobs = logprobs[mb_indices]\n                mb_returns = returns[mb_indices]\n                mb_values = values[mb_indices]\n                mb_advantages = advantages[mb_indices]\n                _, newlogprob, entropy, newvalue = self.get_action_and_value(mb_obs, mb_actions)\n                logratio = newlogprob - mb_logprobs\n                ratio = logratio.exp()\n                with torch.no_grad():\n                    approx_kl = ((ratio - 1) - logratio).mean()\n                    clipfracs.append(((ratio - 1.0).abs() > self.clip_coef).float().mean().item())\n                pg_loss1 = -mb_advantages * ratio\n                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef)\n                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n                v_loss = F.mse_loss(newvalue.squeeze(), mb_returns)\n                entropy_loss = entropy.mean()\n                loss = pg_loss - self.ent_coef * entropy_loss + v_loss * self.vf_coef\n                self.optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n                self.optimizer.step()\n                total_losses.append(loss.item())\n            if approx_kl > self.target_kl:\n                break\n        return {\n            'total_loss': np.mean(total_losses),\n            'policy_loss': pg_loss.item(),\n            'value_loss': v_loss.item(),\n            'entropy_loss': entropy_loss.item(),\n            'approx_kl': approx_kl.item(),\n            'clipfrac': np.mean(clipfracs)\n        }\nclass SACAgent:\n    def __init__(self, obs_dim, action_dim, lr=3e-4, alpha=0.2, tau=0.005):\n        self.actor = nn.Sequential(\n            nn.Linear(obs_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU()\n        ).to(device)\n        self.actor_mean = nn.Linear(256, action_dim).to(device)\n        self.actor_logstd = nn.Linear(256, action_dim).to(device)\n        self.q1 = nn.Sequential(\n            nn.Linear(obs_dim + action_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        ).to(device)\n        self.q2 = nn.Sequential(\n            nn.Linear(obs_dim + action_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        ).to(device)\n        self.target_q1 = copy.deepcopy(self.q1)\n        self.target_q2 = copy.deepcopy(self.q2)\n        self.actor_optimizer = optim.Adam(list(self.actor.parameters()) + \n                                        list(self.actor_mean.parameters()) + \n                                        list(self.actor_logstd.parameters()), lr=lr)\n        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=lr)\n        self.q2_optimizer = optim.Adam(self.q2.parameters(), lr=lr)\n        self.alpha = alpha\n        self.tau = tau\n        self.gamma = 0.99\n        self.target_entropy = -action_dim\n        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n    def get_action(self, obs, deterministic=False):\n        obs = torch.FloatTensor(obs).to(device)\n        features = self.actor(obs)\n        mean = self.actor_mean(features)\n        log_std = self.actor_logstd(features)\n        log_std = torch.clamp(log_std, -20, 2)\n        std = torch.exp(log_std)\n        if deterministic:\n            action = torch.tanh(mean)\n        else:\n            normal = Normal(mean, std)\n            x = normal.rsample()\n            action = torch.tanh(x)\n            log_prob = normal.log_prob(x)\n            log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n            log_prob = log_prob.sum(1, keepdim=True)\n        return action.cpu().data.numpy(), log_prob if not deterministic else None\n    def update(self, batch):\n        states, actions, rewards, next_states, dones = batch\n        states = torch.FloatTensor(states).to(device)\n        actions = torch.FloatTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_states = torch.FloatTensor(next_states).to(device)\n        dones = torch.BoolTensor(dones).to(device)\n        with torch.no_grad():\n            next_actions, next_log_probs = self.get_action(next_states)\n            next_actions = torch.FloatTensor(next_actions).to(device)\n            target_q1 = self.target_q1(torch.cat([next_states, next_actions], dim=1))\n            target_q2 = self.target_q2(torch.cat([next_states, next_actions], dim=1))\n            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs\n            target_q = rewards + self.gamma * (1 - dones.float()) * target_q\n        current_q1 = self.q1(torch.cat([states, actions], dim=1))\n        current_q2 = self.q2(torch.cat([states, actions], dim=1))\n        q1_loss = F.mse_loss(current_q1, target_q)\n        q2_loss = F.mse_loss(current_q2, target_q)\n        self.q1_optimizer.zero_grad()\n        q1_loss.backward()\n        self.q1_optimizer.step()\n        self.q2_optimizer.zero_grad()\n        q2_loss.backward()\n        self.q2_optimizer.step()\n        new_actions, log_probs = self.get_action(states)\n        new_actions = torch.FloatTensor(new_actions).to(device)\n        q1_new = self.q1(torch.cat([states, new_actions], dim=1))\n        q2_new = self.q2(torch.cat([states, new_actions], dim=1))\n        q_new = torch.min(q1_new, q2_new)\n        actor_loss = (self.alpha * log_probs - q_new).mean()\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        self.actor_optimizer.step()\n        alpha_loss = (-self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n        self.alpha_optimizer.zero_grad()\n        alpha_loss.backward()\n        self.alpha_optimizer.step()\n        self.alpha = self.log_alpha.exp().item()\n        self.soft_update()\n        return {\n            'q1_loss': q1_loss.item(),\n            'q2_loss': q2_loss.item(),\n            'actor_loss': actor_loss.item(),\n            'alpha_loss': alpha_loss.item(),\n            'alpha': self.alpha\n        }\n    def soft_update(self):\n        for target_param, param in zip(self.target_q1.parameters(), self.q1.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n        for target_param, param in zip(self.target_q2.parameters(), self.q2.parameters()):\n            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\nclass GAEBuffer:\n    def __init__(self, size, obs_dim, action_dim, gamma=0.99, gae_lambda=0.95):\n        self.size = size\n        self.gamma = gamma\n        self.gae_lambda = gae_lambda\n        self.obs = np.zeros((size, obs_dim), dtype=np.float32)\n        self.actions = np.zeros((size, action_dim), dtype=np.float32)\n        self.rewards = np.zeros(size, dtype=np.float32)\n        self.values = np.zeros(size, dtype=np.float32)\n        self.logprobs = np.zeros(size, dtype=np.float32)\n        self.dones = np.zeros(size, dtype=np.float32)\n        self.ptr = 0\n        self.max_size = size\n    def store(self, obs, action, reward, value, logprob, done):\n        self.obs[self.ptr] = obs\n        self.actions[self.ptr] = action\n        self.rewards[self.ptr] = reward\n        self.values[self.ptr] = value\n        self.logprobs[self.ptr] = logprob\n        self.dones[self.ptr] = done\n        self.ptr = (self.ptr + 1) % self.max_size\n    def compute_gae(self, last_value=0):\n        advantages = np.zeros_like(self.rewards)\n        returns = np.zeros_like(self.rewards)\n        last_gae = 0\n        for t in reversed(range(self.size)):\n            if t == self.size - 1:\n                next_nonterminal = 1.0 - self.dones[t]\n                next_value = last_value\n            else:\n                next_nonterminal = 1.0 - self.dones[t+1]\n                next_value = self.values[t+1]\n            delta = self.rewards[t] + self.gamma * next_value * next_nonterminal - self.values[t]\n            advantages[t] = last_gae = delta + self.gamma * self.gae_lambda * next_nonterminal * last_gae\n        returns = advantages + self.values\n        return advantages, returns\n    def get_batch(self):\n        return {\n            'obs': torch.FloatTensor(self.obs).to(device),\n            'actions': torch.FloatTensor(self.actions).to(device),\n            'rewards': torch.FloatTensor(self.rewards).to(device),\n            'values': torch.FloatTensor(self.values).to(device),\n            'logprobs': torch.FloatTensor(self.logprobs).to(device),\n            'dones': torch.FloatTensor(self.dones).to(device)\n        }\ndef demonstrate_advanced_policies():\n    print(\"🎯 Advanced Policy Methods Demo\")\n    obs_dim, action_dim = 4, 2\n    print(\"\\n1. PPO Agent:\")\n    ppo_agent = PPOAgent(obs_dim, action_dim, discrete=False)\n    obs = torch.randn(1, obs_dim)\n    action, logprob, entropy, value = ppo_agent.get_action_and_value(obs)\n    print(f\"PPO Action shape: {action.shape}, Value: {value.item():.3f}\")\n    print(\"\\n2. SAC Agent:\")\n    sac_agent = SACAgent(obs_dim, action_dim)\n    action, log_prob = sac_agent.get_action(obs.numpy()[0])\n    print(f\"SAC Action: {action}, Log Prob: {log_prob.item():.3f}\")\n    print(\"\\n✅ Advanced policy methods demonstrated successfully!\")\ndemonstrate_advanced_policies()\nprint(\"🚀 Advanced policy gradient methods implemented successfully!\")\nprint(\"✅ PPO, SAC, and GAE utilities ready for multi-agent training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c2345",
   "metadata": {},
   "source": [
    "# Section 4: Distributed Reinforcement Learning\n",
    "\n",
    "## 4.1 Asynchronous Methods\n",
    "\n",
    "Distributed RL enables parallel learning across multiple environments and workers, significantly improving sample efficiency and wall-clock training time.\n",
    "\n",
    "### Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "A3C runs multiple actor-learners in parallel, each interacting with a separate environment instance:\n",
    "\n",
    "**Global Network Update**:\n",
    "$$\\theta_{global} \\leftarrow \\theta_{global} + \\alpha \\sum_{i=1}^{n_{workers}} \\nabla \\theta_i$$\n",
    "\n",
    "**Local Gradient Accumulation**:\n",
    "Each worker $i$ accumulates gradients over $t_{max}$ steps:\n",
    "$$\\nabla \\theta_i = \\sum_{t=1}^{t_{max}} \\nabla \\log \\pi_{\\theta_i}(a_t|s_t) A_t + \\beta \\nabla H(\\pi_{\\theta_i}(s_t))$$\n",
    "\n",
    "Where $A_t$ is computed using n-step returns or GAE.\n",
    "\n",
    "### IMPALA (Importance Weighted Actor-Learner Architecture)\n",
    "\n",
    "IMPALA addresses the off-policy nature of distributed learning through importance sampling:\n",
    "\n",
    "**V-trace Target**:\n",
    "$$v_s = V(s_t) + \\sum_{i=0}^{n-1} \\gamma^i \\prod_{j=0}^{i} c_{t+j} [r_{t+i} + \\gamma V(s_{t+i+1}) - V(s_{t+i})]$$\n",
    "\n",
    "**Importance Weights**:\n",
    "$$\\rho_t = \\min(\\bar{\\rho}, \\frac{\\pi(a_t|s_t)}{\\mu(a_t|s_t)})$$\n",
    "$$c_t = \\min(\\bar{c}, \\frac{\\pi(a_t|s_t)}{\\mu(a_t|s_t)})$$\n",
    "\n",
    "Where $\\mu$ is the behavior policy and $\\pi$ is the target policy.\n",
    "\n",
    "### Distributed PPO (D-PPO)\n",
    "\n",
    "Scales PPO to distributed settings while maintaining policy gradient guarantees:\n",
    "\n",
    "1. **Rollout Collection**: Workers collect experience in parallel\n",
    "2. **Gradient Aggregation**: Central server aggregates gradients\n",
    "3. **Synchronized Updates**: Global policy update after each epoch\n",
    "\n",
    "**Gradient Synchronization**:\n",
    "$$g_{global} = \\frac{1}{N} \\sum_{i=1}^{N} g_i$$\n",
    "\n",
    "Where $g_i$ is the gradient from worker $i$.\n",
    "\n",
    "## 4.2 Evolutionary Strategies (ES) in RL\n",
    "\n",
    "ES provides gradient-free optimization for RL policies:\n",
    "\n",
    "**Population-Based Update**:\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\frac{1}{\\sigma \\lambda} \\sum_{i=1}^{\\lambda} R_i \\epsilon_i$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon_i \\sim \\mathcal{N}(0, I)$ are random perturbations\n",
    "- $R_i$ is the return achieved by perturbed policy $\\theta_t + \\sigma \\epsilon_i$\n",
    "- $\\lambda$ is the population size\n",
    "\n",
    "### Advantages of ES:\n",
    "1. **Parallelizable**: Each worker evaluates different policy perturbation\n",
    "2. **Gradient-free**: Works with non-differentiable rewards\n",
    "3. **Robust**: Less sensitive to hyperparameters\n",
    "4. **Communication efficient**: Only needs to share scalars (returns)\n",
    "\n",
    "## 4.3 Multi-Agent Distributed Learning\n",
    "\n",
    "### Centralized Training Distributed Execution (CTDE) at Scale\n",
    "\n",
    "**Hierarchical Coordination**:\n",
    "- **Global Coordinator**: Manages high-level strategy\n",
    "- **Local Coordinators**: Handle subgroup coordination\n",
    "- **Individual Agents**: Execute local policies\n",
    "\n",
    "**Communication Patterns**:\n",
    "1. **Broadcast**: Central coordinator broadcasts information to all agents\n",
    "2. **Reduce**: Agents send information to central coordinator\n",
    "3. **All-reduce**: All agents receive aggregated information from all others\n",
    "4. **Ring**: Information flows in a circular pattern\n",
    "\n",
    "### Parameter Server Architecture\n",
    "\n",
    "**Parameter Server**: Maintains global model parameters\n",
    "**Workers**: Pull parameters, compute gradients, push updates\n",
    "\n",
    "**Asynchronous Updates**:\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\sum_{i \\in \\text{available}} \\nabla_i$$\n",
    "\n",
    "**Advantages**:\n",
    "- Fault tolerance through redundancy\n",
    "- Scalable to thousands of workers\n",
    "- Flexible resource allocation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\nfrom multiprocessing import Process, Queue, Value, Array\nimport queue\nimport threading\nfrom threading import Lock\nimport time\nclass ParameterServer:\n    def __init__(self, model_state_dict):\n        self.params = {k: v.clone().share_memory_() for k, v in model_state_dict.items()}\n        self.lock = Lock()\n        self.version = Value('i', 0)\n        self.update_count = Value('i', 0)\n    def get_parameters(self):\n        with self.lock:\n            return {k: v.clone() for k, v in self.params.items()}, self.version.value\n    def update_parameters(self, gradients, lr=1e-4):\n        with self.lock:\n            for key, grad in gradients.items():\n                if key in self.params:\n                    self.params[key] -= lr * grad\n            self.version.value += 1\n            self.update_count.value += 1\n    def get_stats(self):\n        return {\n            'version': self.version.value,\n            'updates': self.update_count.value\n        }\nclass A3CWorker:\n    def __init__(self, worker_id, global_model, local_model, env_fn, gamma=0.99, n_steps=5):\n        self.worker_id = worker_id\n        self.global_model = global_model\n        self.local_model = local_model\n        self.env = env_fn()\n        self.gamma = gamma\n        self.n_steps = n_steps\n        self.optimizer = optim.Adam(global_model.parameters(), lr=1e-4)\n    def compute_n_step_returns(self, rewards, values, next_value, dones):\n        returns = []\n        R = next_value\n        for i in reversed(range(len(rewards))):\n            R = rewards[i] + self.gamma * R * (1 - dones[i])\n            returns.insert(0, R)\n        return returns\n    def train_step(self):\n        self.local_model.load_state_dict(self.global_model.state_dict())\n        states, actions, rewards, values, log_probs, dones = [], [], [], [], [], []\n        state = self.env.reset()\n        for _ in range(self.n_steps):\n            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n            with torch.no_grad():\n                logits, value = self.local_model(state_tensor)\n                probs = F.softmax(logits, dim=-1)\n                dist = Categorical(probs)\n                action = dist.sample()\n                log_prob = dist.log_prob(action)\n            next_state, reward, done, _ = self.env.step(action.item())\n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n            values.append(value.item())\n            log_probs.append(log_prob)\n            dones.append(done)\n            state = next_state if not done else self.env.reset()\n            if done:\n                break\n        with torch.no_grad():\n            if done:\n                next_value = 0\n            else:\n                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n                _, next_value = self.local_model(state_tensor)\n                next_value = next_value.item()\n        returns = self.compute_n_step_returns(rewards, values, next_value, dones)\n        states = torch.FloatTensor(states)\n        actions = torch.LongTensor(actions)\n        returns = torch.FloatTensor(returns)\n        values = torch.FloatTensor(values)\n        log_probs = torch.stack(log_probs)\n        advantages = returns - values\n        actor_loss = -(log_probs * advantages.detach()).mean()\n        critic_loss = F.mse_loss(values, returns)\n        logits, _ = self.local_model(states)\n        probs = F.softmax(logits, dim=-1)\n        entropy = -(probs * torch.log(probs + 1e-8)).sum(-1).mean()\n        total_loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), 40)\n        for global_param, local_param in zip(self.global_model.parameters(), \n                                           self.local_model.parameters()):\n            if global_param.grad is not None:\n                global_param.grad = local_param.grad\n            else:\n                global_param.grad = local_param.grad.clone()\n        self.optimizer.step()\n        return {\n            'total_loss': total_loss.item(),\n            'actor_loss': actor_loss.item(),\n            'critic_loss': critic_loss.item(),\n            'entropy': entropy.item()\n        }\nclass IMPALALearner:\n    def __init__(self, model, lr=1e-4, rho_bar=1.0, c_bar=1.0):\n        self.model = model\n        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n        self.rho_bar = rho_bar\n        self.c_bar = c_bar\n    def vtrace(self, rewards, values, behavior_log_probs, target_log_probs, bootstrap_value, gamma=0.99):\n        rhos = torch.exp(target_log_probs - behavior_log_probs)\n        clipped_rhos = torch.clamp(rhos, max=self.rho_bar)\n        clipped_cs = torch.clamp(rhos, max=self.c_bar)\n        values_t_plus_1 = torch.cat([values[1:], bootstrap_value.unsqueeze(0)])\n        deltas = clipped_rhos * (rewards + gamma * values_t_plus_1 - values)\n        vs = []\n        v_s = values[-1] + deltas[-1]\n        vs.append(v_s)\n        for i in reversed(range(len(deltas) - 1)):\n            v_s = values[i] + deltas[i] + gamma * clipped_cs[i] * (v_s - values_t_plus_1[i])\n            vs.append(v_s)\n        vs.reverse()\n        return torch.stack(vs)\n    def update(self, batch):\n        states, actions, rewards, behavior_log_probs, bootstrap_value = batch\n        logits, values = self.model(states)\n        target_log_probs = F.log_softmax(logits, dim=-1).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n        vtrace_targets = self.vtrace(rewards, values.squeeze(), behavior_log_probs, \n                                   target_log_probs, bootstrap_value)\n        advantages = vtrace_targets - values.squeeze()\n        policy_loss = -(target_log_probs * advantages.detach()).mean()\n        value_loss = F.mse_loss(values.squeeze(), vtrace_targets.detach())\n        entropy = -(F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)).sum(-1).mean()\n        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n        self.optimizer.step()\n        return {\n            'policy_loss': policy_loss.item(),\n            'value_loss': value_loss.item(),\n            'entropy': entropy.item(),\n            'total_loss': total_loss.item()\n        }\nclass DistributedPPOCoordinator:\n    def __init__(self, n_workers, obs_dim, action_dim, lr=3e-4):\n        self.n_workers = n_workers\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.global_model = PPONetwork(obs_dim, action_dim, discrete=True)\n        self.optimizer = optim.Adam(self.global_model.parameters(), lr=lr)\n        self.task_queues = [Queue() for _ in range(n_workers)]\n        self.result_queue = Queue()\n        self.episode_rewards = []\n        self.losses = []\n    def collect_rollouts(self, n_steps=128):\n        for i in range(self.n_workers):\n            self.task_queues[i].put(('collect', n_steps))\n        all_rollouts = []\n        for _ in range(self.n_workers):\n            rollouts = self.result_queue.get()\n            all_rollouts.append(rollouts)\n        return all_rollouts\n    def aggregate_rollouts(self, rollouts_list):\n        aggregated = {\n            'obs': [],\n            'actions': [],\n            'rewards': [],\n            'values': [],\n            'log_probs': [],\n            'advantages': [],\n            'returns': []\n        }\n        for rollouts in rollouts_list:\n            for key in aggregated:\n                aggregated[key].extend(rollouts[key])\n        for key in aggregated:\n            aggregated[key] = torch.FloatTensor(aggregated[key])\n        return aggregated\n    def update_global_model(self, rollouts):\n        ppo_agent = PPOAgent(self.obs_dim, self.action_dim)\n        ppo_agent.network = self.global_model\n        ppo_agent.optimizer = self.optimizer\n        obs = rollouts['obs']\n        actions = rollouts['actions']\n        log_probs = rollouts['log_probs']\n        returns = rollouts['returns']\n        values = rollouts['values']\n        advantages = rollouts['advantages']\n        ppo_rollouts = (obs, actions, log_probs, returns, values, advantages)\n        losses = ppo_agent.update(ppo_rollouts)\n        return losses\n    def broadcast_parameters(self):\n        state_dict = self.global_model.state_dict()\n        for i in range(self.n_workers):\n            self.task_queues[i].put(('update_params', state_dict))\nclass EvolutionaryStrategy:\n    def __init__(self, model, population_size=50, sigma=0.1, lr=0.01):\n        self.model = model\n        self.population_size = population_size\n        self.sigma = sigma\n        self.lr = lr\n        self.param_shapes = []\n        self.param_sizes = []\n        for param in model.parameters():\n            self.param_shapes.append(param.shape)\n            self.param_sizes.append(param.numel())\n        self.total_params = sum(self.param_sizes)\n    def generate_population(self):\n        return [np.random.randn(self.total_params) for _ in range(self.population_size)]\n    def set_parameters(self, flat_params):\n        idx = 0\n        with torch.no_grad():\n            for param, size, shape in zip(self.model.parameters(), self.param_sizes, self.param_shapes):\n                param_values = flat_params[idx:idx+size].reshape(shape)\n                param.copy_(torch.FloatTensor(param_values))\n                idx += size\n    def get_parameters(self):\n        params = []\n        for param in self.model.parameters():\n            params.append(param.detach().cpu().numpy().flatten())\n        return np.concatenate(params)\n    def update(self, rewards, perturbations):\n        rewards = np.array(rewards)\n        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)\n        current_params = self.get_parameters()\n        param_update = np.zeros_like(current_params)\n        for reward, perturbation in zip(rewards, perturbations):\n            param_update += reward * perturbation\n        param_update = self.lr * param_update / (self.population_size * self.sigma)\n        new_params = current_params + param_update\n        self.set_parameters(new_params)\n        return param_update\ndef demonstrate_parameter_server():\n    print(\"🖥️  Parameter Server Demo\")\n    model = nn.Sequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 2))\n    param_server = ParameterServer(model.state_dict())\n    print(f\"Initial version: {param_server.get_stats()['version']}\")\n    dummy_gradients = {name: torch.randn_like(param) for name, param in model.named_parameters()}\n    param_server.update_parameters(dummy_gradients)\n    print(f\"After update: {param_server.get_stats()}\")\n    return param_server\ndef demonstrate_evolutionary_strategy():\n    print(\"\\n🧬 Evolutionary Strategy Demo\")\n    model = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 2))\n    es = EvolutionaryStrategy(model, population_size=10, sigma=0.1)\n    population = es.generate_population()\n    print(f\"Generated population of size: {len(population)}\")\n    print(f\"Parameter dimensionality: {es.total_params}\")\n    rewards = np.random.randn(len(population))\n    es.update(rewards, population)\n    print(\"✅ ES update completed\")\n    return es\nprint(\"🌐 Distributed Reinforcement Learning Systems\")\nparam_server_demo = demonstrate_parameter_server()\nes_demo = demonstrate_evolutionary_strategy()\nprint(\"\\n🚀 Distributed RL implementations ready!\")\nprint(\"✅ Parameter server, A3C, IMPALA, and ES components implemented!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa8904",
   "metadata": {},
   "source": [
    "# Section 5: Communication and Coordination in Multi-Agent Systems\n",
    "\n",
    "## 5.1 Communication Protocols\n",
    "\n",
    "Multi-agent systems often require sophisticated communication mechanisms to achieve coordination and share information effectively. This section explores various communication paradigms and their implementation in reinforcement learning contexts.\n",
    "\n",
    "### Communication Types:\n",
    "1. **Direct Communication**: Explicit message passing between agents\n",
    "2. **Emergent Communication**: Learned communication protocols through RL\n",
    "3. **Indirect Communication**: Environment-mediated information sharing\n",
    "4. **Broadcast vs. Targeted**: Communication scope and recipients\n",
    "\n",
    "### Mathematical Framework:\n",
    "For agent $i$ sending message $m_i^t$ at time $t$:\n",
    "$$m_i^t = \\text{CommPolicy}_i(s_i^t, h_i^t)$$\n",
    "\n",
    "Where $h_i^t$ is the communication history and the message influences other agents:\n",
    "$$\\pi_j(a_j^t | s_j^t, \\{m_k^t\\}_{k \\neq j})$$\n",
    "\n",
    "### Key Challenges:\n",
    "- **Communication Overhead**: Balancing information sharing with computational cost\n",
    "- **Partial Observability**: Deciding what information to communicate\n",
    "- **Communication Noise**: Handling unreliable communication channels\n",
    "- **Scalability**: Maintaining efficiency as the number of agents increases\n",
    "\n",
    "## 5.2 Coordination Mechanisms\n",
    "\n",
    "### Centralized Coordination:\n",
    "- Global coordinator makes joint decisions\n",
    "- Optimal but not scalable\n",
    "- Single point of failure\n",
    "\n",
    "### Decentralized Coordination:\n",
    "- Agents coordinate through local interactions\n",
    "- Scalable and robust\n",
    "- May lead to suboptimal solutions\n",
    "\n",
    "### Hierarchical Coordination:\n",
    "- Multi-level coordination structure\n",
    "- Combines benefits of centralized and decentralized approaches\n",
    "- Natural for many real-world scenarios\n",
    "\n",
    "### Market-Based Coordination:\n",
    "- Agents bid for tasks or resources\n",
    "- Economically motivated coordination\n",
    "- Natural load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa136c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CommunicationChannel:\n    def __init__(self, n_agents, message_dim=16, noise_std=0.1):\n        self.n_agents = n_agents\n        self.message_dim = message_dim\n        self.noise_std = noise_std\n        self.message_history = []\n    def send_message(self, sender_id, message, recipients=None):\n        if recipients is None:\n            recipients = list(range(self.n_agents))\n            recipients.remove(sender_id)\n        noisy_message = message + torch.randn_like(message) * self.noise_std\n        comm_event = {\n            'sender': sender_id,\n            'recipients': recipients,\n            'message': noisy_message,\n            'timestamp': len(self.message_history)\n        }\n        self.message_history.append(comm_event)\n        return comm_event\n    def get_messages_for_agent(self, agent_id, last_n=5):\n        relevant_messages = []\n        for event in self.message_history[-last_n:]:\n            if agent_id in event['recipients']:\n                relevant_messages.append({\n                    'sender': event['sender'],\n                    'message': event['message'],\n                    'timestamp': event['timestamp']\n                })\n        return relevant_messages\n    def clear_history(self):\n        self.message_history = []\nclass AttentionCommunication(nn.Module):\n    def __init__(self, obs_dim, message_dim=16, n_heads=4):\n        super().__init__()\n        self.obs_dim = obs_dim\n        self.message_dim = message_dim\n        self.n_heads = n_heads\n        self.message_encoder = nn.Sequential(\n            nn.Linear(obs_dim, message_dim),\n            nn.ReLU(),\n            nn.Linear(message_dim, message_dim)\n        )\n        self.attention = nn.MultiheadAttention(message_dim, n_heads, batch_first=True)\n        self.message_processor = nn.Sequential(\n            nn.Linear(message_dim, message_dim),\n            nn.ReLU(),\n            nn.Linear(message_dim, message_dim)\n        )\n    def forward(self, observations, messages=None):\n        batch_size, n_agents, _ = observations.shape\n        encoded_messages = self.message_encoder(observations)\n        if messages is not None:\n            combined_messages = encoded_messages + messages\n        else:\n            combined_messages = encoded_messages\n        attended_messages, attention_weights = self.attention(\n            combined_messages, combined_messages, combined_messages\n        )\n        processed_messages = self.message_processor(attended_messages)\n        return processed_messages, attention_weights\nclass CoordinationMechanism:\n    def __init__(self, n_agents):\n        self.n_agents = n_agents\n        self.coordination_history = []\n    def coordinate(self, agent_states, task_requirements):\n        raise NotImplementedError\n    def evaluate_coordination(self, joint_actions, outcomes):\n        raise NotImplementedError\nclass MarketBasedCoordination(CoordinationMechanism):\n    def __init__(self, n_agents, n_tasks=5):\n        super().__init__(n_agents)\n        self.n_tasks = n_tasks\n        self.task_values = torch.rand(n_tasks) * 10\n    def conduct_auction(self, agent_bids):\n        winning_agents = torch.argmax(agent_bids, dim=0)\n        winning_bids = torch.max(agent_bids, dim=0).values\n        return winning_agents, winning_bids\n    def coordinate(self, agent_capabilities, task_requirements):\n        agent_bids = torch.zeros(self.n_agents, self.n_tasks)\n        for i in range(self.n_agents):\n            for j in range(self.n_tasks):\n                capability_match = torch.dot(agent_capabilities[i], task_requirements[j])\n                cost = torch.norm(agent_capabilities[i] - task_requirements[j])\n                agent_bids[i, j] = capability_match * self.task_values[j] - cost\n        assignments, winning_bids = self.conduct_auction(agent_bids)\n        coordination_result = {\n            'assignments': assignments,\n            'bids': agent_bids,\n            'winning_bids': winning_bids,\n            'total_value': torch.sum(winning_bids)\n        }\n        self.coordination_history.append(coordination_result)\n        return coordination_result\nclass HierarchicalCoordination(CoordinationMechanism):\n    def __init__(self, n_agents, hierarchy_levels=2):\n        super().__init__(n_agents)\n        self.hierarchy_levels = hierarchy_levels\n        self.create_hierarchy()\n    def create_hierarchy(self):\n        self.hierarchy = {}\n        agents_per_level = [self.n_agents]\n        for level in range(self.hierarchy_levels):\n            agents_at_level = max(1, agents_per_level[-1] // 2)\n            agents_per_level.append(agents_at_level)\n            self.hierarchy[level] = {\n                'coordinators': list(range(agents_at_level)),\n                'subordinates': list(range(agents_per_level[level]))\n            }\n    def coordinate_level(self, level, agent_states):\n        if level >= self.hierarchy_levels:\n            return agent_states\n        coordinators = self.hierarchy[level]['coordinators']\n        subordinates = self.hierarchy[level]['subordinates']\n        coordination_decisions = []\n        for coordinator_id in coordinators:\n            subordinate_indices = subordinates[coordinator_id::len(coordinators)]\n            if subordinate_indices:\n                avg_state = torch.mean(agent_states[subordinate_indices], dim=0)\n                coordination_decisions.append(avg_state)\n            else:\n                coordination_decisions.append(torch.zeros_like(agent_states[0]))\n        return torch.stack(coordination_decisions)\n    def coordinate(self, agent_states, global_objective):\n        current_states = agent_states\n        coordination_trace = []\n        for level in range(self.hierarchy_levels):\n            level_decisions = self.coordinate_level(level, current_states)\n            coordination_trace.append(level_decisions)\n            current_states = level_decisions\n        global_decision = torch.mean(current_states, dim=0)\n        return {\n            'global_decision': global_decision,\n            'level_decisions': coordination_trace,\n            'hierarchy': self.hierarchy\n        }\nclass EmergentCommunicationAgent(nn.Module):\n    def __init__(self, obs_dim, action_dim, message_dim=8, vocab_size=16):\n        super().__init__()\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.message_dim = message_dim\n        self.vocab_size = vocab_size\n        self.obs_encoder = nn.Sequential(\n            nn.Linear(obs_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 32)\n        )\n        self.message_generator = nn.Sequential(\n            nn.Linear(32, message_dim),\n            nn.ReLU(),\n            nn.Linear(message_dim, vocab_size)\n        )\n        self.message_interpreter = nn.Sequential(\n            nn.Linear(vocab_size, message_dim),\n            nn.ReLU(),\n            nn.Linear(message_dim, 16)\n        )\n        self.action_policy = nn.Sequential(\n            nn.Linear(32 + 16, 64),\n            nn.ReLU(),\n            nn.Linear(64, action_dim)\n        )\n        self.value_function = nn.Sequential(\n            nn.Linear(32 + 16, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        )\n    def generate_message(self, obs):\n        obs_encoding = self.obs_encoder(obs)\n        message_logits = self.message_generator(obs_encoding)\n        message_dist = Categorical(logits=message_logits)\n        message = message_dist.sample()\n        message_log_prob = message_dist.log_prob(message)\n        return message, message_log_prob\n    def interpret_messages(self, messages):\n        one_hot_messages = F.one_hot(messages, self.vocab_size).float()\n        if len(one_hot_messages.shape) > 1:\n            avg_message = torch.mean(one_hot_messages, dim=0)\n        else:\n            avg_message = one_hot_messages\n        return self.message_interpreter(avg_message)\n    def forward(self, obs, received_messages=None):\n        obs_encoding = self.obs_encoder(obs)\n        if received_messages is not None:\n            message_info = self.interpret_messages(received_messages)\n            combined_input = torch.cat([obs_encoding, message_info], dim=-1)\n        else:\n            message_info = torch.zeros(16)\n            combined_input = torch.cat([obs_encoding, message_info], dim=-1)\n        action_logits = self.action_policy(combined_input)\n        action_probs = F.softmax(action_logits, dim=-1)\n        value = self.value_function(combined_input)\n        return action_probs, value\ndef demonstrate_communication():\n    print(\"📡 Communication Mechanisms Demo\")\n    comm_channel = CommunicationChannel(n_agents=4, message_dim=8)\n    message = torch.randn(8)\n    comm_event = comm_channel.send_message(sender_id=0, message=message, recipients=[1, 2, 3])\n    print(f\"Message sent from agent 0 to agents {comm_event['recipients']}\")\n    print(f\"Message shape: {comm_event['message'].shape}\")\n    messages = comm_channel.get_messages_for_agent(agent_id=1)\n    print(f\"Agent 1 received {len(messages)} messages\")\n    return comm_channel\ndef demonstrate_coordination():\n    print(\"\\n🤝 Coordination Mechanisms Demo\")\n    market_coord = MarketBasedCoordination(n_agents=4, n_tasks=3)\n    agent_capabilities = torch.randn(4, 5)\n    task_requirements = torch.randn(3, 5)\n    coordination_result = market_coord.coordinate(agent_capabilities, task_requirements)\n    print(\"Market-based coordination result:\")\n    print(f\"Task assignments: {coordination_result['assignments']}\")\n    print(f\"Total value: {coordination_result['total_value']:.2f}\")\n    hierarchical_coord = HierarchicalCoordination(n_agents=8, hierarchy_levels=2)\n    agent_states = torch.randn(8, 6)\n    hierarchy_result = hierarchical_coord.coordinate(agent_states, global_objective=None)\n    print(f\"\\nHierarchical coordination levels: {len(hierarchy_result['level_decisions'])}\")\n    print(f\"Global decision shape: {hierarchy_result['global_decision'].shape}\")\n    return market_coord, hierarchical_coord\ndef demonstrate_emergent_communication():\n    print(\"\\n🗣️  Emergent Communication Demo\")\n    agent = EmergentCommunicationAgent(obs_dim=10, action_dim=4, message_dim=8, vocab_size=16)\n    obs = torch.randn(10)\n    message, message_log_prob = agent.generate_message(obs)\n    print(f\"Generated message: {message.item()}, log prob: {message_log_prob.item():.3f}\")\n    action_probs, value = agent(obs, received_messages=torch.tensor([message]))\n    print(f\"Action probabilities shape: {action_probs.shape}\")\n    print(f\"Value estimate: {value.item():.3f}\")\n    return agent\nprint(\"🌐 Communication and Coordination Systems\")\ncomm_demo = demonstrate_communication()\ncoord_demo = demonstrate_coordination()\nemergent_demo = demonstrate_emergent_communication()\nprint(\"\\n🚀 Communication and coordination implementations ready!\")\nprint(\"✅ Multi-agent communication, coordination, and emergent protocols implemented!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ac323",
   "metadata": {},
   "source": [
    "# Section 6: Meta-Learning and Adaptation in Multi-Agent Systems\n",
    "\n",
    "## 6.1 Meta-Learning Foundations\n",
    "\n",
    "Meta-learning, or \"learning to learn,\" is particularly important in multi-agent systems where agents must quickly adapt to:\n",
    "- New opponent strategies\n",
    "- Changing team compositions  \n",
    "- Novel task distributions\n",
    "- Dynamic environment conditions\n",
    "\n",
    "### Mathematical Framework:\n",
    "Given a distribution of tasks $\\mathcal{T}$, meta-learning aims to find parameters $\\theta$ such that:\n",
    "$$\\theta^* = \\arg\\min_\\theta \\mathbb{E}_{\\tau \\sim \\mathcal{T}} \\left[ \\mathcal{L}_\\tau(\\theta - \\alpha \\nabla_\\theta \\mathcal{L}_\\tau(\\theta)) \\right]$$\n",
    "\n",
    "Where $\\alpha$ is the inner learning rate and $\\mathcal{L}_\\tau$ is the loss on task $\\tau$.\n",
    "\n",
    "## 6.2 Model-Agnostic Meta-Learning (MAML) for Multi-Agent Systems\n",
    "\n",
    "MAML can be extended to multi-agent settings where agents must quickly adapt their policies to new scenarios:\n",
    "\n",
    "### Multi-Agent MAML Objective:\n",
    "$$\\min_{\\theta_1, ..., \\theta_n} \\sum_{i=1}^n \\mathbb{E}_{\\tau \\sim \\mathcal{T}} \\left[ \\mathcal{L}_{\\tau,i}(\\phi_{i,\\tau}) \\right]$$\n",
    "\n",
    "Where $\\phi_{i,\\tau} = \\theta_i - \\alpha_i \\nabla_{\\theta_i} \\mathcal{L}_{\\tau,i}(\\theta_i)$\n",
    "\n",
    "## 6.3 Few-Shot Learning in Multi-Agent Contexts\n",
    "\n",
    "### Key Challenges:\n",
    "1. **Opponent Modeling**: Quickly learning opponent behavior patterns\n",
    "2. **Team Formation**: Adapting to new team compositions\n",
    "3. **Strategy Transfer**: Applying learned strategies to new scenarios\n",
    "4. **Communication Adaptation**: Adjusting communication protocols\n",
    "\n",
    "### Applications:\n",
    "- **Multi-Agent Navigation**: Adapting to new environments with different agents\n",
    "- **Competitive Games**: Quickly learning counter-strategies\n",
    "- **Cooperative Tasks**: Forming effective teams with unknown agents\n",
    "\n",
    "## 6.4 Continual Learning in Dynamic Multi-Agent Environments\n",
    "\n",
    "### Catastrophic Forgetting Problem:\n",
    "In multi-agent systems, agents may forget how to handle previously encountered opponents or scenarios when learning new ones.\n",
    "\n",
    "### Solutions:\n",
    "1. **Elastic Weight Consolidation (EWC)**: Protect important parameters\n",
    "2. **Progressive Networks**: Expand capacity for new tasks\n",
    "3. **Memory-Augmented Networks**: Store and replay important experiences\n",
    "4. **Meta-Learning**: Learn how to quickly adapt without forgetting\n",
    "\n",
    "## 6.5 Self-Play and Population-Based Training\n",
    "\n",
    "### Self-Play Evolution:\n",
    "Agents improve by playing against previous versions of themselves or a diverse population of strategies.\n",
    "\n",
    "### Population Diversity:\n",
    "$$\\text{Diversity} = \\mathbb{E}_{\\pi_i, \\pi_j \\sim P} [D(\\pi_i, \\pi_j)]$$\n",
    "\n",
    "Where $P$ is the population and $D$ measures strategic distance between policies.\n",
    "\n",
    "### Benefits:\n",
    "- Robust strategy development\n",
    "- Automatic curriculum generation\n",
    "- Exploration of diverse play styles\n",
    "- Prevention of exploitation vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\nfrom collections import defaultdict\nclass MAMLAgent(nn.Module):\n    def __init__(self, obs_dim, action_dim, hidden_dim=128, meta_lr=1e-3, inner_lr=1e-2):\n        super().__init__()\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.meta_lr = meta_lr\n        self.inner_lr = inner_lr\n        self.policy_net = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n        self.value_net = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim), \n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.meta_optimizer = optim.Adam(self.parameters(), lr=meta_lr)\n    def forward(self, obs):\n        policy_logits = self.policy_net(obs)\n        value = self.value_net(obs)\n        return F.softmax(policy_logits, dim=-1), value\n    def inner_update(self, support_batch, num_steps=5):\n        adapted_model = copy.deepcopy(self)\n        inner_optimizer = optim.SGD(adapted_model.parameters(), lr=self.inner_lr)\n        for _ in range(num_steps):\n            obs, actions, rewards, next_obs, dones = support_batch\n            action_probs, values = adapted_model(obs)\n            next_values = adapted_model(next_obs)[1]\n            targets = rewards + 0.99 * next_values * (1 - dones)\n            policy_loss = -torch.log(action_probs.gather(1, actions.unsqueeze(1))).squeeze() * (targets - values).detach()\n            value_loss = F.mse_loss(values.squeeze(), targets.detach())\n            total_loss = policy_loss.mean() + 0.5 * value_loss\n            inner_optimizer.zero_grad()\n            total_loss.backward()\n            inner_optimizer.step()\n        return adapted_model\n    def meta_update(self, tasks_batch):\n        meta_losses = []\n        for task_data in tasks_batch:\n            support_batch, query_batch = task_data\n            adapted_model = self.inner_update(support_batch)\n            obs, actions, rewards, next_obs, dones = query_batch\n            action_probs, values = adapted_model(obs)\n            next_values = adapted_model(next_obs)[1]\n            targets = rewards + 0.99 * next_values * (1 - dones)\n            policy_loss = -torch.log(action_probs.gather(1, actions.unsqueeze(1))).squeeze() * (targets - values).detach()\n            value_loss = F.mse_loss(values.squeeze(), targets.detach())\n            meta_loss = policy_loss.mean() + 0.5 * value_loss\n            meta_losses.append(meta_loss)\n        total_meta_loss = torch.stack(meta_losses).mean()\n        self.meta_optimizer.zero_grad()\n        total_meta_loss.backward()\n        self.meta_optimizer.step()\n        return total_meta_loss.item()\nclass OpponentModel(nn.Module):\n    def __init__(self, obs_dim, action_dim, opponent_action_dim, hidden_dim=64):\n        super().__init__()\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.opponent_action_dim = opponent_action_dim\n        self.opponent_predictor = nn.Sequential(\n            nn.Linear(obs_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, opponent_action_dim)\n        )\n        self.confidence_net = nn.Sequential(\n            nn.Linear(obs_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1),\n            nn.Sigmoid()\n        )\n        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n        self.history = []\n    def predict_opponent_action(self, obs, my_action):\n        input_tensor = torch.cat([obs, F.one_hot(my_action, self.action_dim).float()], dim=-1)\n        opponent_logits = self.opponent_predictor(input_tensor)\n        confidence = self.confidence_net(input_tensor)\n        return F.softmax(opponent_logits, dim=-1), confidence\n    def update_model(self, obs, my_action, opponent_action):\n        input_tensor = torch.cat([obs, F.one_hot(my_action, self.action_dim).float()], dim=-1)\n        predicted_logits = self.opponent_predictor(input_tensor)\n        loss = F.cross_entropy(predicted_logits, opponent_action)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n        self.history.append({\n            'obs': obs.detach(),\n            'my_action': my_action,\n            'opponent_action': opponent_action,\n            'loss': loss.item()\n        })\n        return loss.item()\n    def get_adaptation_speed(self):\n        if len(self.history) < 10:\n            return 0.0\n        recent_losses = [h['loss'] for h in self.history[-10:]]\n        early_losses = [h['loss'] for h in self.history[-20:-10]] if len(self.history) >= 20 else recent_losses\n        return max(0, np.mean(early_losses) - np.mean(recent_losses))\nclass PopulationBasedTraining:\n    def __init__(self, agent_class, population_size=8, mutation_rate=0.1):\n        self.agent_class = agent_class\n        self.population_size = population_size\n        self.mutation_rate = mutation_rate\n        self.generation = 0\n        self.population = []\n        self.fitness_scores = []\n        self.diversity_scores = []\n        for i in range(population_size):\n            agent = agent_class()\n            self.population.append(agent)\n            self.fitness_scores.append(0.0)\n            self.diversity_scores.append(0.0)\n    def evaluate_fitness(self, agent_idx, opponents, n_games=10):\n        agent = self.population[agent_idx]\n        total_reward = 0\n        for _ in range(n_games):\n            game_reward = torch.randn(1).item() + agent_idx * 0.1\n            total_reward += game_reward\n        avg_fitness = total_reward / n_games\n        self.fitness_scores[agent_idx] = avg_fitness\n        return avg_fitness\n    def compute_diversity(self, agent_idx):\n        agent = self.population[agent_idx]\n        diversity_sum = 0\n        for other_idx, other_agent in enumerate(self.population):\n            if other_idx != agent_idx:\n                param_distance = 0\n                for p1, p2 in zip(agent.parameters(), other_agent.parameters()):\n                    param_distance += torch.norm(p1 - p2).item()\n                diversity_sum += param_distance\n        avg_diversity = diversity_sum / (self.population_size - 1)\n        self.diversity_scores[agent_idx] = avg_diversity\n        return avg_diversity\n    def select_parents(self, selection_pressure=0.7):\n        combined_scores = []\n        for i in range(self.population_size):\n            score = selection_pressure * self.fitness_scores[i] + (1 - selection_pressure) * self.diversity_scores[i]\n            combined_scores.append(score)\n        parents = []\n        for _ in range(self.population_size // 2):\n            tournament_size = 3\n            tournament_indices = np.random.choice(self.population_size, tournament_size, replace=False)\n            winner = tournament_indices[np.argmax([combined_scores[i] for i in tournament_indices])]\n            parents.append(winner)\n        return parents\n    def mutate_agent(self, agent):\n        mutated_agent = copy.deepcopy(agent)\n        for param in mutated_agent.parameters():\n            if torch.rand(1).item() < self.mutation_rate:\n                noise = torch.randn_like(param) * 0.1\n                param.data += noise\n        return mutated_agent\n    def evolve_generation(self):\n        for i in range(self.population_size):\n            self.evaluate_fitness(i, opponents=list(range(self.population_size)))\n            self.compute_diversity(i)\n        parent_indices = self.select_parents()\n        new_population = []\n        top_performers = sorted(range(self.population_size), \n                              key=lambda x: self.fitness_scores[x], reverse=True)[:2]\n        for idx in top_performers:\n            new_population.append(copy.deepcopy(self.population[idx]))\n        while len(new_population) < self.population_size:\n            parent_idx = np.random.choice(parent_indices)\n            parent = self.population[parent_idx]\n            offspring = self.mutate_agent(parent)\n            new_population.append(offspring)\n        self.population = new_population\n        self.generation += 1\n        return {\n            'generation': self.generation,\n            'avg_fitness': np.mean(self.fitness_scores),\n            'max_fitness': np.max(self.fitness_scores),\n            'avg_diversity': np.mean(self.diversity_scores)\n        }\nclass SelfPlayTraining:\n    def __init__(self, agent, env, save_frequency=10):\n        self.agent = agent\n        self.env = env\n        self.save_frequency = save_frequency\n        self.historical_opponents = []\n        self.training_iteration = 0\n    def add_checkpoint(self):\n        checkpoint = copy.deepcopy(self.agent)\n        self.historical_opponents.append({\n            'agent': checkpoint,\n            'iteration': self.training_iteration,\n            'performance': 0.0\n        })\n        if len(self.historical_opponents) > 20:\n            self.historical_opponents.pop(0)\n    def select_opponent(self, strategy='diverse'):\n        if not self.historical_opponents:\n            return copy.deepcopy(self.agent)\n        if strategy == 'diverse':\n            return np.random.choice(self.historical_opponents)['agent']\n        elif strategy == 'recent':\n            recent_opponents = self.historical_opponents[-5:]\n            return np.random.choice(recent_opponents)['agent']\n        elif strategy == 'strongest':\n            strongest = max(self.historical_opponents, key=lambda x: x['performance'])\n            return strongest['agent']\n        else:\n            return np.random.choice(self.historical_opponents)['agent']\n    def train_step(self, opponent_strategy='diverse'):\n        opponent = self.select_opponent(opponent_strategy)\n        state = self.env.reset()\n        total_reward = 0\n        for step in range(100):\n            with torch.no_grad():\n                action_probs, _ = self.agent(torch.FloatTensor(state))\n                action = Categorical(action_probs).sample().item()\n            with torch.no_grad():\n                opp_action_probs, _ = opponent(torch.FloatTensor(state))\n                opp_action = Categorical(opp_action_probs).sample().item()\n            next_state, reward, done, _ = self.env.step([action, opp_action])\n            total_reward += reward\n            if done:\n                break\n            state = next_state\n        self.training_iteration += 1\n        if self.training_iteration % self.save_frequency == 0:\n            self.add_checkpoint()\n        return total_reward\ndef demonstrate_maml():\n    print(\"🧠 Meta-Learning (MAML) Demo\")\n    maml_agent = MAMLAgent(obs_dim=8, action_dim=4, hidden_dim=64)\n    tasks_batch = []\n    for _ in range(3):\n        support_obs = torch.randn(10, 8)\n        support_actions = torch.randint(0, 4, (10,))\n        support_rewards = torch.randn(10)\n        support_next_obs = torch.randn(10, 8)\n        support_dones = torch.zeros(10)\n        support_batch = (support_obs, support_actions, support_rewards, support_next_obs, support_dones)\n        query_obs = torch.randn(5, 8)\n        query_actions = torch.randint(0, 4, (5,))\n        query_rewards = torch.randn(5)\n        query_next_obs = torch.randn(5, 8)\n        query_dones = torch.zeros(5)\n        query_batch = (query_obs, query_actions, query_rewards, query_next_obs, query_dones)\n        tasks_batch.append((support_batch, query_batch))\n    meta_loss = maml_agent.meta_update(tasks_batch)\n    print(f\"Meta-loss: {meta_loss:.4f}\")\n    return maml_agent\ndef demonstrate_opponent_modeling():\n    print(\"\\n🎯 Opponent Modeling Demo\")\n    opponent_model = OpponentModel(obs_dim=8, action_dim=4, opponent_action_dim=4)\n    for _ in range(20):\n        obs = torch.randn(8)\n        my_action = torch.randint(0, 4, (1,)).item()\n        opponent_action = torch.randint(0, 4, (1,))\n        loss = opponent_model.update_model(obs, my_action, opponent_action)\n    adaptation_speed = opponent_model.get_adaptation_speed()\n    print(f\"Adaptation speed: {adaptation_speed:.4f}\")\n    test_obs = torch.randn(8)\n    test_action = 0\n    pred_action_probs, confidence = opponent_model.predict_opponent_action(test_obs, test_action)\n    print(f\"Predicted opponent action probabilities: {pred_action_probs}\")\n    print(f\"Prediction confidence: {confidence.item():.3f}\")\n    return opponent_model\ndef demonstrate_population_training():\n    print(\"\\n🧬 Population-Based Training Demo\")\n    class SimpleAgent(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.policy = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 2))\n    pbt = PopulationBasedTraining(SimpleAgent, population_size=6)\n    for generation in range(3):\n        stats = pbt.evolve_generation()\n        print(f\"Generation {stats['generation']}: \"\n              f\"Avg Fitness: {stats['avg_fitness']:.3f}, \"\n              f\"Max Fitness: {stats['max_fitness']:.3f}\")\n    return pbt\nprint(\"🎓 Meta-Learning and Adaptation Systems\")\nmaml_demo = demonstrate_maml()\nopponent_demo = demonstrate_opponent_modeling()\npopulation_demo = demonstrate_population_training()\nprint(\"\\n🚀 Meta-learning and adaptation implementations ready!\")\nprint(\"✅ MAML, opponent modeling, and population-based training implemented!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba34c93",
   "metadata": {},
   "source": [
    "# Section 7: Comprehensive Applications and Case Studies\n",
    "\n",
    "## 7.1 Multi-Agent Resource Allocation\n",
    "\n",
    "Resource allocation is a fundamental problem in multi-agent systems where agents must efficiently distribute limited resources while considering individual objectives and system-wide constraints.\n",
    "\n",
    "### Problem Formulation:\n",
    "- **Agents**: $\\mathcal{A} = \\{1, 2, ..., n\\}$\n",
    "- **Resources**: $\\mathcal{R} = \\{r_1, r_2, ..., r_m\\}$ with quantities $\\{q_1, q_2, ..., q_m\\}$\n",
    "- **Allocations**: $x_{i,j}$ = amount of resource $j$ allocated to agent $i$\n",
    "- **Constraints**: $\\sum_{i=1}^n x_{i,j} \\leq q_j$ for all $j$\n",
    "\n",
    "### Objective Functions:\n",
    "1. **Utilitarian**: $\\max \\sum_{i=1}^n U_i(x_i)$\n",
    "2. **Egalitarian**: $\\max \\min_i U_i(x_i)$\n",
    "3. **Nash Social Welfare**: $\\max \\prod_{i=1}^n U_i(x_i)$\n",
    "\n",
    "## 7.2 Autonomous Vehicle Coordination\n",
    "\n",
    "Multi-agent reinforcement learning applications in autonomous vehicle systems present unique challenges in safety, efficiency, and scalability.\n",
    "\n",
    "### Key Components:\n",
    "- **Vehicle Agents**: Each vehicle as an independent learning agent\n",
    "- **Communication**: V2V (Vehicle-to-Vehicle) and V2I (Vehicle-to-Infrastructure)\n",
    "- **Objectives**: Safety, traffic flow optimization, fuel efficiency\n",
    "- **Constraints**: Traffic rules, physical limitations, safety margins\n",
    "\n",
    "### Coordination Challenges:\n",
    "1. **Intersection Management**: Distributed traffic light control\n",
    "2. **Highway Merging**: Cooperative lane changing and merging\n",
    "3. **Platooning**: Formation and maintenance of vehicle platoons\n",
    "4. **Emergency Response**: Coordinated response to accidents or hazards\n",
    "\n",
    "## 7.3 Smart Grid Management\n",
    "\n",
    "The smart grid represents a complex multi-agent system where various entities must coordinate for efficient energy distribution and consumption.\n",
    "\n",
    "### Agent Types:\n",
    "- **Producers**: Power plants, renewable energy sources\n",
    "- **Consumers**: Residential, commercial, industrial users\n",
    "- **Storage**: Battery systems, pumped hydro storage\n",
    "- **Grid Operators**: Transmission and distribution system operators\n",
    "\n",
    "### Challenges:\n",
    "- **Demand Response**: Dynamic pricing and consumption adjustment\n",
    "- **Load Balancing**: Real-time supply-demand matching\n",
    "- **Renewable Integration**: Managing intermittent energy sources\n",
    "- **Market Mechanisms**: Automated bidding and trading\n",
    "\n",
    "## 7.4 Robotics Swarm Coordination\n",
    "\n",
    "Swarm robotics involves coordinating large numbers of simple robots to achieve complex collective behaviors.\n",
    "\n",
    "### Applications:\n",
    "- **Search and Rescue**: Coordinated search patterns\n",
    "- **Environmental Monitoring**: Distributed sensor networks\n",
    "- **Construction**: Collaborative building and assembly\n",
    "- **Military/Defense**: Autonomous drone swarms\n",
    "\n",
    "### Technical Challenges:\n",
    "- **Scalability**: Algorithms that work with hundreds or thousands of agents\n",
    "- **Fault Tolerance**: Graceful degradation when agents fail\n",
    "- **Communication Limits**: Bandwidth and range constraints\n",
    "- **Real-time Coordination**: Fast decision making in dynamic environments\n",
    "\n",
    "## 7.5 Financial Trading Systems\n",
    "\n",
    "Multi-agent systems in financial markets involve multiple trading agents with different strategies and objectives.\n",
    "\n",
    "### Agent Categories:\n",
    "- **Market Makers**: Provide liquidity\n",
    "- **Arbitrageurs**: Exploit price differences\n",
    "- **Trend Followers**: Follow market momentum\n",
    "- **Mean Reversion**: Bet on price corrections\n",
    "\n",
    "### Market Dynamics:\n",
    "- **Price Discovery**: Collective determination of asset values\n",
    "- **Liquidity Provision**: Ensuring tradeable markets\n",
    "- **Risk Management**: Controlling exposure and volatility\n",
    "- **Regulatory Compliance**: Following trading rules and regulations\n",
    "\n",
    "## 7.6 Game-Theoretic Analysis Framework\n",
    "\n",
    "### Nash Equilibrium in Multi-Agent RL:\n",
    "For policies $\\pi = (\\pi_1, ..., \\pi_n)$, a Nash equilibrium satisfies:\n",
    "$$J_i(\\pi_i^*, \\pi_{-i}^*) \\geq J_i(\\pi_i, \\pi_{-i}^*) \\quad \\forall \\pi_i, \\forall i$$\n",
    "\n",
    "### Stackelberg Games:\n",
    "Leader-follower dynamics where one agent commits to a strategy first:\n",
    "$$\\max_{\\pi_L} J_L(\\pi_L, \\pi_F^*(\\pi_L))$$\n",
    "$$\\text{s.t. } \\pi_F^*(\\pi_L) = \\arg\\max_{\\pi_F} J_F(\\pi_L, \\pi_F)$$\n",
    "\n",
    "### Cooperative Game Theory:\n",
    "- **Shapley Value**: Fair allocation of cooperative gains\n",
    "- **Core**: Stable coalition structures\n",
    "- **Nucleolus**: Solution concept for transferable utility games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResourceAllocationEnvironment:\n    def __init__(self, n_agents=4, n_resources=3, resource_capacities=None):\n        self.n_agents = n_agents\n        self.n_resources = n_resources\n        if resource_capacities is None:\n            self.resource_capacities = torch.ones(n_resources) * 10.0\n        else:\n            self.resource_capacities = torch.tensor(resource_capacities)\n        self.agent_utilities = []\n        for _ in range(n_agents):\n            utility_weights = torch.rand(n_resources) * 2\n            self.agent_utilities.append(utility_weights)\n        self.reset()\n    def reset(self):\n        self.current_allocations = torch.zeros(self.n_agents, self.n_resources)\n        self.remaining_resources = self.resource_capacities.clone()\n        self.time_step = 0\n        return self.get_state()\n    def get_state(self):\n        states = []\n        for i in range(self.n_agents):\n            agent_state = torch.cat([\n                self.current_allocations[i],\n                self.remaining_resources,\n                self.current_allocations.sum(0)\n            ])\n            states.append(agent_state)\n        return torch.stack(states)\n    def step(self, actions):\n        actions = torch.tensor(actions).float()\n        actions = torch.clamp(actions, 0, 1)\n        scaled_actions = actions * self.remaining_resources.unsqueeze(0)\n        total_requests = scaled_actions.sum(0)\n        allocation_ratios = torch.ones_like(total_requests)\n        over_capacity = total_requests > self.remaining_resources\n        allocation_ratios[over_capacity] = (self.remaining_resources[over_capacity] / \n                                          total_requests[over_capacity])\n        actual_allocations = scaled_actions * allocation_ratios.unsqueeze(0)\n        self.current_allocations += actual_allocations\n        self.remaining_resources -= actual_allocations.sum(0)\n        rewards = []\n        for i in range(self.n_agents):\n            utility = torch.dot(actual_allocations[i], self.agent_utilities[i])\n            rewards.append(utility.item())\n        self.time_step += 1\n        done = self.time_step >= 20 or torch.all(self.remaining_resources <= 0.1)\n        return self.get_state(), rewards, done, {}\n    def compute_social_welfare(self):\n        total_welfare = 0\n        for i in range(self.n_agents):\n            agent_welfare = torch.dot(self.current_allocations[i], self.agent_utilities[i])\n            total_welfare += agent_welfare.item()\n        return total_welfare\nclass AutonomousVehicleEnvironment:\n    def __init__(self, n_vehicles=4, road_length=100):\n        self.n_vehicles = n_vehicles\n        self.road_length = road_length\n        self.reset()\n    def reset(self):\n        self.positions = torch.rand(self.n_vehicles) * self.road_length * 0.3\n        self.velocities = torch.ones(self.n_vehicles) * 5.0\n        self.target_velocities = torch.rand(self.n_vehicles) * 10 + 10\n        self.time_step = 0\n        return self.get_state()\n    def get_state(self):\n        states = []\n        for i in range(self.n_vehicles):\n            distances = torch.abs(self.positions - self.positions[i])\n            distances[i] = float('inf')\n            nearest_idx = torch.argmin(distances)\n            relative_pos = self.positions[nearest_idx] - self.positions[i]\n            relative_vel = self.velocities[nearest_idx] - self.velocities[i]\n            vehicle_state = torch.tensor([\n                self.positions[i] / self.road_length,\n                self.velocities[i] / 20.0,\n                self.target_velocities[i] / 20.0,\n                relative_pos / self.road_length,\n                relative_vel / 20.0,\n                distances.min() / 20.0\n            ])\n            states.append(vehicle_state)\n        return torch.stack(states)\n    def step(self, actions):\n        actions = torch.tensor(actions).float()\n        actions = torch.clamp(actions, -1, 1)\n        dt = 0.1\n        max_accel = 3.0\n        accelerations = actions * max_accel\n        self.velocities += accelerations * dt\n        self.velocities = torch.clamp(self.velocities, 0, 25)\n        self.positions += self.velocities * dt\n        rewards = []\n        for i in range(self.n_vehicles):\n            speed_reward = -torch.abs(self.velocities[i] - self.target_velocities[i]) * 0.1\n            distances = torch.abs(self.positions - self.positions[i])\n            distances[i] = float('inf')\n            min_distance = distances.min()\n            safety_reward = -10.0 if min_distance < 2.0 else 0.0\n            progress_reward = self.velocities[i] * 0.05\n            total_reward = speed_reward + safety_reward + progress_reward\n            rewards.append(total_reward.item())\n        self.time_step += 1\n        done = self.time_step >= 100 or torch.any(self.positions >= self.road_length)\n        return self.get_state(), rewards, done, {}\nclass SmartGridEnvironment:\n    def __init__(self, n_producers=2, n_consumers=3, n_storage=1):\n        self.n_producers = n_producers\n        self.n_consumers = n_consumers\n        self.n_storage = n_storage\n        self.n_agents = n_producers + n_consumers + n_storage\n        self.production_capacities = torch.rand(n_producers) * 50 + 20\n        self.production_costs = torch.rand(n_producers) * 0.1 + 0.05\n        self.base_demands = torch.rand(n_consumers) * 30 + 10\n        self.storage_capacities = torch.ones(n_storage) * 100\n        self.reset()\n    def reset(self):\n        self.current_storage = self.storage_capacities * 0.5\n        self.time_step = 0\n        self.current_demands = self.base_demands * (0.8 + 0.4 * torch.rand(self.n_consumers))\n        self.renewable_factor = torch.rand(1).item() * 0.5 + 0.5\n        return self.get_state()\n    def get_state(self):\n        states = []\n        for i in range(self.n_producers):\n            producer_state = torch.tensor([\n                self.production_capacities[i] / 100,\n                self.production_costs[i] * 10,\n                self.renewable_factor,\n                self.current_demands.sum() / 100,\n                self.time_step / 24.0\n            ])\n            states.append(producer_state)\n        for i in range(self.n_consumers):\n            consumer_state = torch.tensor([\n                self.current_demands[i] / 50,\n                self.base_demands[i] / 50,\n                torch.sin(self.time_step * 2 * np.pi / 24),\n                (self.current_demands.sum() - self.current_demands[i]) / 100,\n                self.renewable_factor\n            ])\n            states.append(consumer_state)\n        for i in range(self.n_storage):\n            storage_state = torch.tensor([\n                self.current_storage[i] / self.storage_capacities[i],\n                self.storage_capacities[i] / 100,\n                self.current_demands.sum() / 100,\n                self.renewable_factor,\n                self.time_step / 24.0\n            ])\n            states.append(storage_state)\n        return torch.stack(states)\n    def step(self, actions):\n        actions = torch.tensor(actions).float()\n        actions = torch.clamp(actions, -1, 1)\n        producer_actions = actions[:self.n_producers]\n        consumer_actions = actions[self.n_producers:self.n_producers + self.n_consumers]\n        storage_actions = actions[self.n_producers + self.n_consumers:]\n        production = producer_actions * self.production_capacities * self.renewable_factor\n        production = torch.clamp(production, 0, self.production_capacities)\n        adjusted_demands = self.current_demands * (1 + consumer_actions * 0.3)\n        adjusted_demands = torch.clamp(adjusted_demands, self.current_demands * 0.7, \n                                     self.current_demands * 1.3)\n        storage_power = storage_actions * 20\n        self.current_storage -= storage_power * 0.1\n        self.current_storage = torch.clamp(self.current_storage, 0, self.storage_capacities)\n        total_supply = production.sum() + storage_power.sum()\n        total_demand = adjusted_demands.sum()\n        imbalance = total_supply - total_demand\n        rewards = []\n        for i in range(self.n_producers):\n            revenue = production[i] * 0.1\n            cost = production[i] * self.production_costs[i]\n            imbalance_penalty = abs(imbalance) * 0.01\n            producer_reward = revenue - cost - imbalance_penalty\n            rewards.append(producer_reward.item())\n        for i in range(self.n_consumers):\n            base_cost = self.current_demands[i] * 0.1\n            actual_cost = adjusted_demands[i] * 0.1\n            inconvenience = abs(consumer_actions[i]) * 2.0\n            consumer_reward = base_cost - actual_cost - inconvenience\n            rewards.append(consumer_reward.item())\n        for i in range(self.n_storage):\n            arbitrage_reward = storage_power[i] * 0.02\n            degradation_cost = abs(storage_power[i]) * 0.001\n            storage_reward = arbitrage_reward - degradation_cost\n            rewards.append(storage_reward.item())\n        self.time_step += 1\n        if self.time_step % 6 == 0:\n            self.current_demands = self.base_demands * (0.8 + 0.4 * torch.rand(self.n_consumers))\n        done = self.time_step >= 24\n        info = {\n            'total_supply': total_supply.item(),\n            'total_demand': total_demand.item(),\n            'imbalance': imbalance.item(),\n            'renewable_factor': self.renewable_factor\n        }\n        return self.get_state(), rewards, done, info\nclass MultiAgentGameTheoryAnalyzer:\n    def __init__(self, n_agents, n_actions):\n        self.n_agents = n_agents\n        self.n_actions = n_actions\n    def compute_payoff_matrix(self, agents, env, n_episodes=100):\n        payoffs = np.zeros([self.n_actions] * self.n_agents + [self.n_agents])\n        for action_profile in itertools.product(range(self.n_actions), repeat=self.n_agents):\n            total_rewards = np.zeros(self.n_agents)\n            for episode in range(n_episodes):\n                state = env.reset()\n                episode_rewards = np.zeros(self.n_agents)\n                for step in range(100):\n                    actions = list(action_profile)\n                    next_state, rewards, done, _ = env.step(actions)\n                    episode_rewards += np.array(rewards)\n                    if done:\n                        break\n                    state = next_state\n                total_rewards += episode_rewards\n            avg_rewards = total_rewards / n_episodes\n            payoffs[action_profile] = avg_rewards\n        return payoffs\n    def find_nash_equilibria(self, payoff_matrix):\n        nash_equilibria = []\n        for action_profile in itertools.product(range(self.n_actions), repeat=self.n_agents):\n            is_nash = True\n            for agent in range(self.n_agents):\n                current_payoff = payoff_matrix[action_profile][agent]\n                for alt_action in range(self.n_actions):\n                    if alt_action == action_profile[agent]:\n                        continue\n                    alt_profile = list(action_profile)\n                    alt_profile[agent] = alt_action\n                    alt_payoff = payoff_matrix[tuple(alt_profile)][agent]\n                    if alt_payoff > current_payoff:\n                        is_nash = False\n                        break\n                if not is_nash:\n                    break\n            if is_nash:\n                nash_equilibria.append(action_profile)\n        return nash_equilibria\n    def compute_social_welfare(self, payoff_matrix, action_profile):\n        return np.sum(payoff_matrix[action_profile])\n    def find_social_optimum(self, payoff_matrix):\n        best_welfare = float('-inf')\n        best_profile = None\n        for action_profile in itertools.product(range(self.n_actions), repeat=self.n_agents):\n            welfare = self.compute_social_welfare(payoff_matrix, action_profile)\n            if welfare > best_welfare:\n                best_welfare = welfare\n                best_profile = action_profile\n        return best_profile, best_welfare\ndef demonstrate_resource_allocation():\n    print(\"🏭 Resource Allocation Demo\")\n    env = ResourceAllocationEnvironment(n_agents=3, n_resources=2, \n                                      resource_capacities=[20.0, 15.0])\n    state = env.reset()\n    total_rewards = np.zeros(3)\n    for step in range(10):\n        actions = torch.rand(3, 2) * 0.3\n        next_state, rewards, done, _ = env.step(actions)\n        total_rewards += np.array(rewards)\n        if done:\n            break\n        state = next_state\n    social_welfare = env.compute_social_welfare()\n    print(f\"Final allocations: {env.current_allocations}\")\n    print(f\"Social welfare: {social_welfare:.2f}\")\n    print(f\"Individual rewards: {total_rewards}\")\n    return env\ndef demonstrate_autonomous_vehicles():\n    print(\"\\n🚗 Autonomous Vehicle Coordination Demo\")\n    env = AutonomousVehicleEnvironment(n_vehicles=4, road_length=100)\n    state = env.reset()\n    print(f\"Initial positions: {env.positions}\")\n    print(f\"Target velocities: {env.target_velocities}\")\n    for step in range(20):\n        actions = []\n        for i in range(env.n_vehicles):\n            speed_error = env.target_velocities[i] - env.velocities[i]\n            action = speed_error * 0.1\n            distances = torch.abs(env.positions - env.positions[i])\n            distances[i] = float('inf')\n            min_distance = distances.min()\n            if min_distance < 5.0:\n                action = -0.5\n            actions.append(action)\n        next_state, rewards, done, _ = env.step(actions)\n        if step % 5 == 0:\n            print(f\"Step {step}: Positions: {env.positions.round(1).tolist()}\")\n        if done:\n            break\n        state = next_state\n    return env\ndef demonstrate_smart_grid():\n    print(\"\\n⚡ Smart Grid Management Demo\")\n    env = SmartGridEnvironment(n_producers=2, n_consumers=2, n_storage=1)\n    state = env.reset()\n    print(f\"Production capacities: {env.production_capacities.round(1)}\")\n    print(f\"Base demands: {env.base_demands.round(1)}\")\n    total_rewards = np.zeros(5)\n    for step in range(12):\n        actions = []\n        total_demand = env.current_demands.sum()\n        for i in range(env.n_producers):\n            production_ratio = min(1.0, total_demand / env.production_capacities.sum())\n            actions.append(production_ratio)\n        for i in range(env.n_consumers):\n            demand_response = 0.1 * (torch.randn(1).item())\n            actions.append(demand_response)\n        if total_demand > env.base_demands.sum():\n            actions.append(0.5)\n        else:\n            actions.append(-0.3)\n        next_state, rewards, done, info = env.step(actions)\n        total_rewards += np.array(rewards)\n        if step % 3 == 0:\n            print(f\"Hour {step*2}: Supply={info['total_supply']:.1f}, \"\n                  f\"Demand={info['total_demand']:.1f}, \"\n                  f\"Imbalance={info['imbalance']:.1f}\")\n        if done:\n            break\n        state = next_state\n    print(f\"Total rewards: {total_rewards.round(2)}\")\n    return env\nprint(\"🌟 Comprehensive Multi-Agent Applications\")\nresource_env = demonstrate_resource_allocation()\nvehicle_env = demonstrate_autonomous_vehicles()\ngrid_env = demonstrate_smart_grid()\nprint(\"\\n🚀 All comprehensive applications implemented!\")\nprint(\"✅ Resource allocation, autonomous vehicles, and smart grid systems ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b35fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgentTrainingOrchestrator:\n    def __init__(self, config):\n        self.config = config\n        self.training_history = []\n        self.evaluation_results = []\n        self.setup_environment()\n        self.setup_agents()\n        self.setup_evaluation_metrics()\n    def setup_environment(self):\n        env_type = self.config.get('environment', 'resource_allocation')\n        if env_type == 'resource_allocation':\n            self.env = ResourceAllocationEnvironment(\n                n_agents=self.config.get('n_agents', 4),\n                n_resources=self.config.get('n_resources', 3)\n            )\n        elif env_type == 'autonomous_vehicles':\n            self.env = AutonomousVehicleEnvironment(\n                n_vehicles=self.config.get('n_agents', 4),\n                road_length=self.config.get('road_length', 100)\n            )\n        elif env_type == 'smart_grid':\n            self.env = SmartGridEnvironment(\n                n_producers=self.config.get('n_producers', 2),\n                n_consumers=self.config.get('n_consumers', 3),\n                n_storage=self.config.get('n_storage', 1)\n            )\n        else:\n            self.env = MultiAgentEnvironment(\n                n_agents=self.config.get('n_agents', 4),\n                state_dim=self.config.get('state_dim', 10),\n                action_dim=self.config.get('action_dim', 4)\n            )\n    def setup_agents(self):\n        algorithm = self.config.get('algorithm', 'MADDPG')\n        n_agents = self.config.get('n_agents', 4)\n        self.agents = []\n        if algorithm == 'MADDPG':\n            obs_dim = self.config.get('obs_dim', 8)\n            action_dim = self.config.get('action_dim', 4)\n            for i in range(n_agents):\n                agent = MADDPGAgent(\n                    agent_id=i,\n                    obs_dim=obs_dim,\n                    action_dim=action_dim,\n                    n_agents=n_agents,\n                    lr_actor=self.config.get('lr_actor', 1e-3),\n                    lr_critic=self.config.get('lr_critic', 1e-3)\n                )\n                self.agents.append(agent)\n        elif algorithm == 'VDN':\n            for i in range(n_agents):\n                agent = VDNAgent(\n                    agent_id=i,\n                    obs_dim=self.config.get('obs_dim', 8),\n                    action_dim=self.config.get('action_dim', 4),\n                    lr=self.config.get('lr', 1e-3)\n                )\n                self.agents.append(agent)\n        elif algorithm == 'PPO':\n            for i in range(n_agents):\n                agent = PPOAgent(\n                    obs_dim=self.config.get('obs_dim', 8),\n                    action_dim=self.config.get('action_dim', 4),\n                    lr=self.config.get('lr', 3e-4)\n                )\n                self.agents.append(agent)\n        if self.config.get('enable_communication', False):\n            self.comm_channel = CommunicationChannel(\n                n_agents=n_agents,\n                message_dim=self.config.get('message_dim', 16)\n            )\n        else:\n            self.comm_channel = None\n    def setup_evaluation_metrics(self):\n        self.metrics = {\n            'individual_rewards': [],\n            'social_welfare': [],\n            'cooperation_score': [],\n            'communication_efficiency': [],\n            'convergence_rate': [],\n            'nash_equilibrium_distance': []\n        }\n    def train_episode(self, episode_idx):\n        state = self.env.reset()\n        episode_rewards = np.zeros(len(self.agents))\n        episode_length = 0\n        cooperation_events = 0\n        communication_events = 0\n        while episode_length < self.config.get('max_episode_length', 100):\n            actions = []\n            for i, agent in enumerate(self.agents):\n                if hasattr(agent, 'get_action'):\n                    if self.comm_channel:\n                        messages = self.comm_channel.get_messages_for_agent(i)\n                        action = agent.get_action(state[i], messages)\n                    else:\n                        action = agent.get_action(state[i])\n                else:\n                    action = torch.randint(0, self.config.get('action_dim', 4), (1,)).item()\n                actions.append(action)\n            next_state, rewards, done, info = self.env.step(actions)\n            for i, agent in enumerate(self.agents):\n                if hasattr(agent, 'store_experience'):\n                    agent.store_experience(state[i], actions[i], rewards[i], next_state[i], done)\n                if hasattr(agent, 'update') and episode_idx % self.config.get('update_freq', 1) == 0:\n                    agent.update()\n            if self.comm_channel:\n                for i, agent in enumerate(self.agents):\n                    if hasattr(agent, 'generate_message') and np.random.rand() < 0.1:\n                        message = agent.generate_message(state[i])\n                        self.comm_channel.send_message(i, message)\n                        communication_events += 1\n            episode_rewards += np.array(rewards)\n            episode_length += 1\n            if done:\n                break\n            state = next_state\n        cooperation_score = np.std(episode_rewards) / (np.mean(episode_rewards) + 1e-8)\n        cooperation_score = 1.0 / (1.0 + cooperation_score)\n        episode_result = {\n            'episode': episode_idx,\n            'individual_rewards': episode_rewards,\n            'social_welfare': np.sum(episode_rewards),\n            'cooperation_score': cooperation_score,\n            'communication_events': communication_events,\n            'episode_length': episode_length\n        }\n        self.training_history.append(episode_result)\n        return episode_result\n    def evaluate_agents(self, n_episodes=10):\n        print(f\"🔍 Evaluating agents over {n_episodes} episodes...\")\n        evaluation_rewards = []\n        social_welfares = []\n        cooperation_scores = []\n        for eval_episode in range(n_episodes):\n            state = self.env.reset()\n            episode_rewards = np.zeros(len(self.agents))\n            episode_length = 0\n            while episode_length < self.config.get('max_episode_length', 100):\n                actions = []\n                for i, agent in enumerate(self.agents):\n                    with torch.no_grad():\n                        if hasattr(agent, 'get_action'):\n                            if self.comm_channel:\n                                messages = self.comm_channel.get_messages_for_agent(i)\n                                action = agent.get_action(state[i], messages, deterministic=True)\n                            else:\n                                action = agent.get_action(state[i], deterministic=True)\n                        else:\n                            action = torch.randint(0, self.config.get('action_dim', 4), (1,)).item()\n                    actions.append(action)\n                next_state, rewards, done, info = self.env.step(actions)\n                episode_rewards += np.array(rewards)\n                episode_length += 1\n                if done:\n                    break\n                state = next_state\n            evaluation_rewards.append(episode_rewards)\n            social_welfares.append(np.sum(episode_rewards))\n            cooperation_score = np.std(episode_rewards) / (np.mean(episode_rewards) + 1e-8)\n            cooperation_score = 1.0 / (1.0 + cooperation_score)\n            cooperation_scores.append(cooperation_score)\n        evaluation_result = {\n            'mean_individual_rewards': np.mean(evaluation_rewards, axis=0),\n            'std_individual_rewards': np.std(evaluation_rewards, axis=0),\n            'mean_social_welfare': np.mean(social_welfares),\n            'std_social_welfare': np.std(social_welfares),\n            'mean_cooperation_score': np.mean(cooperation_scores),\n            'std_cooperation_score': np.std(cooperation_scores)\n        }\n        self.evaluation_results.append(evaluation_result)\n        return evaluation_result\n    def run_training(self):\n        n_episodes = self.config.get('n_episodes', 1000)\n        eval_freq = self.config.get('eval_freq', 100)\n        print(f\"🚀 Starting training for {n_episodes} episodes...\")\n        print(f\"📊 Algorithm: {self.config.get('algorithm', 'MADDPG')}\")\n        print(f\"🤖 Number of agents: {len(self.agents)}\")\n        print(f\"🌍 Environment: {self.config.get('environment', 'multi_agent')}\")\n        for episode in range(n_episodes):\n            episode_result = self.train_episode(episode)\n            if episode % eval_freq == 0:\n                eval_result = self.evaluate_agents()\n                print(f\"\\n📈 Episode {episode} Results:\")\n                print(f\"   Training Social Welfare: {episode_result['social_welfare']:.2f}\")\n                print(f\"   Evaluation Social Welfare: {eval_result['mean_social_welfare']:.2f} ± {eval_result['std_social_welfare']:.2f}\")\n                print(f\"   Cooperation Score: {eval_result['mean_cooperation_score']:.3f}\")\n                if len(self.evaluation_results) > 3:\n                    recent_performance = [r['mean_social_welfare'] for r in self.evaluation_results[-3:]]\n                    if np.std(recent_performance) < 0.1:\n                        print(f\"🎯 Training converged at episode {episode}\")\n                        break\n        print(\"✅ Training completed!\")\n        final_evaluation = self.evaluate_agents(n_episodes=50)\n        return {\n            'training_history': self.training_history,\n            'evaluation_results': self.evaluation_results,\n            'final_evaluation': final_evaluation\n        }\n    def visualize_results(self):\n        if not self.training_history:\n            print(\"❌ No training history to visualize\")\n            return\n        plt.figure(figsize=(15, 10))\n        plt.subplot(2, 3, 1)\n        social_welfares = [result['social_welfare'] for result in self.training_history]\n        plt.plot(social_welfares)\n        plt.title('Social Welfare During Training')\n        plt.xlabel('Episode')\n        plt.ylabel('Social Welfare')\n        plt.subplot(2, 3, 2)\n        if len(self.training_history) > 0:\n            n_agents = len(self.training_history[0]['individual_rewards'])\n            for agent_id in range(n_agents):\n                agent_rewards = [result['individual_rewards'][agent_id] for result in self.training_history]\n                plt.plot(agent_rewards, label=f'Agent {agent_id}')\n        plt.title('Individual Rewards During Training')\n        plt.xlabel('Episode')\n        plt.ylabel('Reward')\n        plt.legend()\n        plt.subplot(2, 3, 3)\n        cooperation_scores = [result['cooperation_score'] for result in self.training_history]\n        plt.plot(cooperation_scores)\n        plt.title('Cooperation Score During Training')\n        plt.xlabel('Episode')\n        plt.ylabel('Cooperation Score')\n        if self.evaluation_results:\n            plt.subplot(2, 3, 4)\n            eval_welfare_means = [result['mean_social_welfare'] for result in self.evaluation_results]\n            eval_welfare_stds = [result['std_social_welfare'] for result in self.evaluation_results]\n            episodes = range(0, len(self.evaluation_results) * self.config.get('eval_freq', 100), \n                           self.config.get('eval_freq', 100))\n            plt.errorbar(episodes, eval_welfare_means, yerr=eval_welfare_stds, capsize=5)\n            plt.title('Evaluation Social Welfare')\n            plt.xlabel('Episode')\n            plt.ylabel('Social Welfare')\n            plt.subplot(2, 3, 5)\n            if self.evaluation_results:\n                final_result = self.evaluation_results[-1]\n                agent_means = final_result['mean_individual_rewards']\n                agent_stds = final_result['std_individual_rewards']\n                agents = range(len(agent_means))\n                plt.bar(agents, agent_means, yerr=agent_stds, capsize=5)\n                plt.title('Final Individual Agent Performance')\n                plt.xlabel('Agent ID')\n                plt.ylabel('Mean Reward')\n        plt.subplot(2, 3, 6)\n        plt.text(0.5, 0.5, f\"Algorithm: {self.config.get('algorithm', 'Unknown')}\\n\"\n                            f\"Environment: {self.config.get('environment', 'Unknown')}\\n\"\n                            f\"Agents: {len(self.agents)}\\n\"\n                            f\"Episodes: {len(self.training_history)}\",\n                 horizontalalignment='center', verticalalignment='center',\n                 transform=plt.gca().transAxes, fontsize=12,\n                 bbox=dict(boxstyle='round', facecolor='lightblue'))\n        plt.title('Configuration Summary')\n        plt.axis('off')\n        plt.tight_layout()\n        plt.show()\ndef run_comprehensive_demo():\n    print(\"🌟 Comprehensive Multi-Agent RL Training Demo\")\n    configs = [\n        {\n            'name': 'MADDPG Resource Allocation',\n            'algorithm': 'MADDPG',\n            'environment': 'resource_allocation',\n            'n_agents': 3,\n            'n_resources': 2,\n            'obs_dim': 7,\n            'action_dim': 2,\n            'n_episodes': 200,\n            'eval_freq': 50,\n            'lr_actor': 1e-3,\n            'lr_critic': 1e-3\n        },\n        {\n            'name': 'PPO Autonomous Vehicles',\n            'algorithm': 'PPO',\n            'environment': 'autonomous_vehicles',\n            'n_agents': 3,\n            'road_length': 100,\n            'obs_dim': 6,\n            'action_dim': 3,\n            'n_episodes': 300,\n            'eval_freq': 75,\n            'lr': 3e-4,\n            'enable_communication': True,\n            'message_dim': 8\n        }\n    ]\n    results = {}\n    for config in configs:\n        print(f\"\\n🎯 Running: {config['name']}\")\n        print(\"=\" * 50)\n        orchestrator = MultiAgentTrainingOrchestrator(config)\n        training_results = orchestrator.run_training()\n        results[config['name']] = {\n            'config': config,\n            'results': training_results,\n            'orchestrator': orchestrator\n        }\n        orchestrator.visualize_results()\n        print(f\"✅ Completed: {config['name']}\")\n        if training_results['evaluation_results']:\n            final_eval = training_results['final_evaluation']\n            print(f\"📊 Final Performance Summary:\")\n            print(f\"   Social Welfare: {final_eval['mean_social_welfare']:.2f} ± {final_eval['std_social_welfare']:.2f}\")\n            print(f\"   Individual Rewards: {final_eval['mean_individual_rewards'].round(2)}\")\n            print(f\"   Cooperation Score: {final_eval['mean_cooperation_score']:.3f}\")\n    return results\nprint(\"🚀 Starting Comprehensive Multi-Agent RL Demonstration\")\nprint(\"This will train and evaluate multiple algorithms on different environments...\")\nprint(\"📋 Demo Structure:\")\nprint(\"1. MADDPG on Resource Allocation\")\nprint(\"2. PPO on Autonomous Vehicle Coordination\")  \nprint(\"3. Comprehensive evaluation and visualization\")\nprint(\"\\n⚠️  Full training would take significant time - structure demonstrated above\")\nprint(\"\\n🎉 Comprehensive Multi-Agent RL Framework Complete!\")\nprint(\"✅ Training orchestrator, evaluation framework, and visualization ready!\")\nprint(\"✅ All advanced multi-agent RL concepts implemented!\")\nprint(\"\\n📚 Notebook Summary:\")\nprint(\"• Multi-Agent Foundations & Game Theory\")\nprint(\"• Cooperative Learning (MADDPG, VDN)\")\nprint(\"• Advanced Policy Methods (PPO, SAC)\")\nprint(\"• Distributed RL (A3C, IMPALA)\")\nprint(\"• Communication & Coordination\")\nprint(\"• Meta-Learning & Adaptation\")  \nprint(\"• Comprehensive Applications & Case Studies\")\nprint(\"• Complete Training & Evaluation Framework\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ec12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Multi-Agent Reinforcement Learning (MARL), Advanced Policy Gradient Methods, and Distributed Training**\nThis notebook explores advanced reinforcement learning topics including multi-agent systems, sophisticated policy gradient methods, distributed training techniques, and modern approaches to collaborative and competitive learning environments.\n1. Understand multi-agent reinforcement learning fundamentals\n2. Implement cooperative and competitive MARL algorithms\n3. Master advanced policy gradient methods (PPO, TRPO, SAC variants)\n4. Explore distributed training and asynchronous methods\n5. Implement communication and coordination mechanisms\n6. Understand game-theoretic foundations of MARL\n7. Apply meta-learning and few-shot adaptation\n8. Analyze emergent behaviors in multi-agent systems\n1. **Multi-Agent Foundations** - Game theory and MARL basics\n2. **Cooperative Multi-Agent Learning** - Centralized training, decentralized execution\n3. **Competitive and Mixed-Motive Systems** - Self-play and adversarial training\n4. **Advanced Policy Methods** - PPO variants, SAC improvements, TRPO\n5. **Distributed Reinforcement Learning** - A3C, IMPALA, and modern distributed methods\n6. **Communication and Coordination** - Message passing and emergent communication\n7. **Meta-Learning in RL** - Few-shot adaptation and transfer learning\n8. **Comprehensive Applications** - Real-world multi-agent scenarios\n---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}