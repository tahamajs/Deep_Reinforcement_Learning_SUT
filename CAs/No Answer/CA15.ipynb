{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "913e82ce",
   "metadata": {},
   "source": [
    "# CA15: Advanced Deep Reinforcement Learning - Model-Based RL and Hierarchical RL\n",
    "\n",
    "## Overview\n",
    "\n",
    "This comprehensive assignment covers advanced topics in Deep Reinforcement Learning, focusing on:\n",
    "\n",
    "1. **Model-Based Reinforcement Learning**\n",
    "   - World Models and Environment Dynamics\n",
    "   - Model-Predictive Control (MPC)\n",
    "   - Planning with Learned Models\n",
    "   - Dyna-Q and Model-Based Policy Optimization\n",
    "\n",
    "2. **Hierarchical Reinforcement Learning**\n",
    "   - Options Framework\n",
    "   - Hierarchical Actor-Critic (HAC)\n",
    "   - Goal-Conditioned RL\n",
    "   - Feudal Networks\n",
    "\n",
    "3. **Advanced Planning and Control**\n",
    "   - Monte Carlo Tree Search (MCTS)\n",
    "   - Model-Based Value Expansion\n",
    "   - Latent Space Planning\n",
    "\n",
    "### Learning Objectives\n",
    "- Understand model-based RL principles and implementation\n",
    "- Master hierarchical decomposition in RL\n",
    "- Implement advanced planning algorithms\n",
    "- Apply these methods to complex control tasks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6747ed",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We'll import essential libraries for implementing model-based and hierarchical RL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d455e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.distributions import Categorical, Normal\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import deque, namedtuple\nimport random\nimport copy\nimport math\nimport gym\nfrom typing import List, Dict, Tuple, Optional, Union\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\ntorch.manual_seed(42)\nrandom.seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nMODEL_BASED_CONFIG = {\n    'model_lr': 1e-3,\n    'planning_horizon': 10,\n    'model_ensemble_size': 5,\n    'imagination_rollouts': 100,\n    'model_training_freq': 10\n}\nHIERARCHICAL_CONFIG = {\n    'num_levels': 3,\n    'option_timeout': 20,\n    'subgoal_threshold': 0.1,\n    'meta_controller_lr': 3e-4,\n    'controller_lr': 1e-3\n}\nPLANNING_CONFIG = {\n    'mcts_simulations': 100,\n    'exploration_constant': 1.4,\n    'planning_depth': 5,\n    'beam_width': 10\n}\nprint(\"üöÄ Libraries imported successfully!\")\nprint(\"üìä Configurations loaded for Model-Based and Hierarchical RL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de69bf0",
   "metadata": {},
   "source": [
    "# Section 1: Model-Based Reinforcement Learning\n",
    "\n",
    "Model-Based RL learns an explicit model of the environment dynamics and uses it for planning and control.\n",
    "\n",
    "## 1.1 Theoretical Foundation\n",
    "\n",
    "### Environment Dynamics Model\n",
    "The goal is to learn a transition model $p(s_{t+1}, r_t | s_t, a_t)$ that predicts next states and rewards.\n",
    "\n",
    "**Key Components:**\n",
    "- **Deterministic Model**: $s_{t+1} = f(s_t, a_t) + \\epsilon$\n",
    "- **Stochastic Model**: $s_{t+1} \\sim p(\\cdot | s_t, a_t)$\n",
    "- **Ensemble Methods**: Multiple models to capture uncertainty\n",
    "\n",
    "### Model-Predictive Control (MPC)\n",
    "Uses the learned model to plan actions by optimizing over a finite horizon:\n",
    "\n",
    "$$a^*_t = \\arg\\max_{a_t, \\ldots, a_{t+H-1}} \\sum_{k=0}^{H-1} \\gamma^k r_{t+k}$$\n",
    "\n",
    "where states are predicted using the learned model.\n",
    "\n",
    "### Dyna-Q Algorithm\n",
    "Combines model-free and model-based learning:\n",
    "1. **Direct RL**: Update Q-function from real experience\n",
    "2. **Planning**: Use model to generate simulated experience\n",
    "3. **Model Learning**: Update dynamics model from real data\n",
    "\n",
    "### Advantages and Challenges\n",
    "**Advantages:**\n",
    "- Sample efficiency through planning\n",
    "- Can handle sparse rewards\n",
    "- Enables what-if analysis\n",
    "\n",
    "**Challenges:**\n",
    "- Model bias and compounding errors\n",
    "- Computational complexity\n",
    "- Partial observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001221c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicsModel(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(DynamicsModel, self).__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.transition_net = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, state_dim + 1)\n        )\n        self.uncertainty_net = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, state_dim + 1),\n            nn.Softplus()\n        )\n    def forward(self, state, action):\n        if len(state.shape) == 1:\n            state = state.unsqueeze(0)\n        if len(action.shape) == 1:\n            action = action.unsqueeze(0)\n        if action.dtype == torch.long:\n            action_one_hot = torch.zeros(action.size(0), self.action_dim).to(action.device)\n            action_one_hot.scatter_(1, action.unsqueeze(1), 1)\n            action = action_one_hot\n        input_tensor = torch.cat([state, action], dim=-1)\n        prediction = self.transition_net(input_tensor)\n        uncertainty = self.uncertainty_net(input_tensor)\n        next_state_mean = prediction[:, :self.state_dim]\n        reward_mean = prediction[:, self.state_dim:]\n        next_state_std = uncertainty[:, :self.state_dim]\n        reward_std = uncertainty[:, self.state_dim:]\n        return {\n            'next_state_mean': next_state_mean,\n            'reward_mean': reward_mean,\n            'next_state_std': next_state_std,\n            'reward_std': reward_std\n        }\n    def sample_prediction(self, state, action):\n        output = self.forward(state, action)\n        next_state = torch.normal(output['next_state_mean'], output['next_state_std'])\n        reward = torch.normal(output['reward_mean'], output['reward_std'])\n        return next_state.squeeze(), reward.squeeze()\nclass ModelEnsemble:\n    def __init__(self, state_dim, action_dim, ensemble_size=5):\n        self.ensemble_size = ensemble_size\n        self.models = []\n        self.optimizers = []\n        for _ in range(ensemble_size):\n            model = DynamicsModel(state_dim, action_dim).to(device)\n            optimizer = optim.Adam(model.parameters(), lr=MODEL_BASED_CONFIG['model_lr'])\n            self.models.append(model)\n            self.optimizers.append(optimizer)\n    def train_step(self, states, actions, next_states, rewards):\n        total_loss = 0\n        for model, optimizer in zip(self.models, self.optimizers):\n            optimizer.zero_grad()\n            output = model(states, actions)\n            state_loss = F.mse_loss(output['next_state_mean'], next_states)\n            reward_loss = F.mse_loss(output['reward_mean'], rewards.unsqueeze(-1))\n            state_nll = 0.5 * torch.sum(\n                ((output['next_state_mean'] - next_states) ** 2) / (output['next_state_std'] ** 2) +\n                torch.log(output['next_state_std'] ** 2)\n            )\n            reward_nll = 0.5 * torch.sum(\n                ((output['reward_mean'] - rewards.unsqueeze(-1)) ** 2) / (output['reward_std'] ** 2) +\n                torch.log(output['reward_std'] ** 2)\n            )\n            loss = state_loss + reward_loss + 0.1 * (state_nll + reward_nll)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n            optimizer.step()\n            total_loss += loss.item()\n        return total_loss / self.ensemble_size\n    def predict_ensemble(self, state, action):\n        predictions = []\n        for model in self.models:\n            with torch.no_grad():\n                pred = model.sample_prediction(state, action)\n                predictions.append(pred)\n        return predictions\n    def predict_mean(self, state, action):\n        predictions = self.predict_ensemble(state, action)\n        next_states = torch.stack([pred[0] for pred in predictions])\n        rewards = torch.stack([pred[1] for pred in predictions])\n        return next_states.mean(dim=0), rewards.mean(dim=0)\nclass ModelPredictiveController:\n    def __init__(self, model_ensemble, action_dim, horizon=10, num_samples=1000):\n        self.model_ensemble = model_ensemble\n        self.action_dim = action_dim\n        self.horizon = horizon\n        self.num_samples = num_samples\n    def plan_action(self, state, goal_state=None):\n        state = torch.FloatTensor(state).to(device)\n        best_action = None\n        best_value = float('-inf')\n        for _ in range(self.num_samples):\n            if isinstance(self.action_dim, int):\n                actions = torch.randint(0, self.action_dim, (self.horizon,)).to(device)\n            else:\n                actions = torch.randn(self.horizon, self.action_dim).to(device)\n            total_reward = 0\n            current_state = state\n            for t in range(self.horizon):\n                next_state, reward = self.model_ensemble.predict_mean(current_state, actions[t])\n                if goal_state is not None:\n                    goal_state_tensor = torch.FloatTensor(goal_state).to(device)\n                    goal_reward = -torch.norm(next_state - goal_state_tensor)\n                    total_reward += goal_reward * (0.99 ** t)\n                else:\n                    total_reward += reward * (0.99 ** t)\n                current_state = next_state\n            if total_reward > best_value:\n                best_value = total_reward\n                best_action = actions[0]\n        return best_action.cpu().numpy() if best_action is not None else np.random.randint(self.action_dim)\nclass DynaQAgent:\n    def __init__(self, state_dim, action_dim, lr=1e-3):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.q_network = nn.Sequential(\n            nn.Linear(state_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim)\n        ).to(device)\n        self.q_optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n        self.model_ensemble = ModelEnsemble(state_dim, action_dim)\n        self.buffer = deque(maxlen=100000)\n        self.training_stats = {\n            'q_losses': [],\n            'model_losses': [],\n            'planning_rewards': []\n        }\n    def get_action(self, state, epsilon=0.1):\n        if np.random.random() < epsilon:\n            return np.random.randint(self.action_dim)\n        with torch.no_grad():\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n            q_values = self.q_network(state_tensor)\n            return q_values.argmax().item()\n    def store_experience(self, state, action, reward, next_state, done):\n        self.buffer.append((state, action, reward, next_state, done))\n    def update_q_function(self, batch_size=32):\n        if len(self.buffer) < batch_size:\n            return 0\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, dones = zip(*batch)\n        states = torch.FloatTensor(states).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_states = torch.FloatTensor(next_states).to(device)\n        dones = torch.BoolTensor(dones).to(device)\n        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))\n        next_q_values = self.q_network(next_states).max(1)[0].detach()\n        target_q_values = rewards + 0.99 * next_q_values * (~dones)\n        loss = F.mse_loss(current_q_values.squeeze(), target_q_values)\n        self.q_optimizer.zero_grad()\n        loss.backward()\n        self.q_optimizer.step()\n        self.training_stats['q_losses'].append(loss.item())\n        return loss.item()\n    def update_model(self, batch_size=32):\n        if len(self.buffer) < batch_size:\n            return 0\n        batch = random.sample(self.buffer, batch_size)\n        states, actions, rewards, next_states, _ = zip(*batch)\n        states = torch.FloatTensor(states).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        next_states = torch.FloatTensor(next_states).to(device)\n        loss = self.model_ensemble.train_step(states, actions, next_states, rewards)\n        self.training_stats['model_losses'].append(loss)\n        return loss\n    def planning_step(self, num_planning_steps=50):\n        if len(self.buffer) < 10:\n            return 0\n        total_planning_reward = 0\n        for _ in range(num_planning_steps):\n            state, _, _, _, _ = random.choice(self.buffer)\n            state_tensor = torch.FloatTensor(state).to(device)\n            action = np.random.randint(self.action_dim)\n            action_tensor = torch.LongTensor([action]).to(device)\n            next_state, reward = self.model_ensemble.predict_mean(state_tensor, action_tensor)\n            with torch.no_grad():\n                current_q = self.q_network(state_tensor.unsqueeze(0))[0, action]\n                next_q = self.q_network(next_state.unsqueeze(0)).max()\n                target_q = reward + 0.99 * next_q\n            td_error = target_q - current_q\n            q_values = self.q_network(state_tensor.unsqueeze(0))\n            q_values[0, action] = current_q + 0.1 * td_error\n            total_planning_reward += reward.item()\n        avg_planning_reward = total_planning_reward / num_planning_steps\n        self.training_stats['planning_rewards'].append(avg_planning_reward)\n        return avg_planning_reward\nprint(\"üß† Model-Based RL components implemented successfully!\")\nprint(\"üìù Key components:\")\nprint(\"  ‚Ä¢ DynamicsModel: Neural network for environment dynamics\")\nprint(\"  ‚Ä¢ ModelEnsemble: Multiple models for uncertainty quantification\")\nprint(\"  ‚Ä¢ ModelPredictiveController: MPC for action planning\")\nprint(\"  ‚Ä¢ DynaQAgent: Dyna-Q algorithm combining model-free and model-based learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e610b2",
   "metadata": {},
   "source": [
    "# Section 2: Hierarchical Reinforcement Learning\n",
    "\n",
    "Hierarchical RL decomposes complex tasks into simpler subtasks through temporal and spatial abstraction.\n",
    "\n",
    "## 2.1 Theoretical Foundation\n",
    "\n",
    "### Options Framework\n",
    "An **option** is a closed-loop policy for taking actions over a period of time. Formally, an option consists of:\n",
    "- **Initiation set** $I$: States where the option can be initiated\n",
    "- **Policy** $\\pi$: Action selection within the option\n",
    "- **Termination condition** $\\beta$: Probability of terminating the option\n",
    "\n",
    "### Semi-Markov Decision Process (SMDP)\n",
    "Options extend MDPs to SMDPs where:\n",
    "- Actions can take variable amounts of time\n",
    "- Temporal abstraction enables hierarchical planning\n",
    "- Q-learning over options: $Q(s,o) = r + \\gamma^k Q(s', o')$\n",
    "\n",
    "### Goal-Conditioned RL\n",
    "Learn policies conditioned on goals: $\\pi(a|s,g)$\n",
    "- **Hindsight Experience Replay (HER)**: Learn from failed attempts\n",
    "- **Universal Value Function**: $V(s,g)$ for any goal $g$\n",
    "- **Intrinsic Motivation**: Generate own goals for exploration\n",
    "\n",
    "### Hierarchical Actor-Critic (HAC)\n",
    "Multi-level hierarchy where:\n",
    "- **High-level policy**: Selects subgoals\n",
    "- **Low-level policy**: Executes actions to reach subgoals\n",
    "- **Temporal abstraction**: Different time scales at each level\n",
    "\n",
    "### Feudal Networks\n",
    "Hierarchical architecture with:\n",
    "- **Manager**: Sets goals for workers\n",
    "- **Worker**: Executes actions to achieve goals\n",
    "- **Feudal objective**: Manager maximizes reward, Worker maximizes goal achievement\n",
    "\n",
    "## 2.2 Key Advantages\n",
    "\n",
    "**Sample Efficiency:**\n",
    "- Reuse learned skills across tasks\n",
    "- Faster learning through temporal abstraction\n",
    "\n",
    "**Interpretability:**\n",
    "- Hierarchical structure mirrors human thinking\n",
    "- Decomposable and explainable decisions\n",
    "\n",
    "**Transfer Learning:**\n",
    "- Skills transfer across related environments\n",
    "- Compositional generalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfdf0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Option:\n    def __init__(self, policy, initiation_set=None, termination_condition=None, name=\"option\"):\n        self.policy = policy\n        self.initiation_set = initiation_set\n        self.termination_condition = termination_condition\n        self.name = name\n        self.active_steps = 0\n        self.max_steps = HIERARCHICAL_CONFIG['option_timeout']\n    def can_initiate(self, state):\n        if self.initiation_set is None:\n            return True\n        return self.initiation_set(state)\n    def should_terminate(self, state):\n        if self.active_steps >= self.max_steps:\n            return True\n        if self.termination_condition is not None:\n            return self.termination_condition(state)\n        return False\n    def get_action(self, state):\n        self.active_steps += 1\n        return self.policy(state)\n    def reset(self):\n        self.active_steps = 0\nclass HierarchicalActorCritic(nn.Module):\n    def __init__(self, state_dim, action_dim, num_levels=3, hidden_dim=256):\n        super(HierarchicalActorCritic, self).__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.num_levels = num_levels\n        self.meta_controllers = nn.ModuleList()\n        self.meta_critics = nn.ModuleList()\n        self.low_controllers = nn.ModuleList()\n        self.low_critics = nn.ModuleList()\n        for level in range(num_levels - 1):\n            meta_controller = nn.Sequential(\n                nn.Linear(state_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, state_dim)\n            )\n            meta_critic = nn.Sequential(\n                nn.Linear(state_dim * 2, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, 1)\n            )\n            self.meta_controllers.append(meta_controller)\n            self.meta_critics.append(meta_critic)\n        low_controller = nn.Sequential(\n            nn.Linear(state_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n        low_critic = nn.Sequential(\n            nn.Linear(state_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.low_controllers.append(low_controller)\n        self.low_critics.append(low_critic)\n    def forward_meta(self, state, level):\n        if level >= len(self.meta_controllers):\n            raise ValueError(f\"Level {level} exceeds number of meta controllers\")\n        subgoal = self.meta_controllers[level](state)\n        state_goal = torch.cat([state, subgoal], dim=-1)\n        value = self.meta_critics[level](state_goal)\n        return subgoal, value\n    def forward_low(self, state, subgoal):\n        state_subgoal = torch.cat([state, subgoal], dim=-1)\n        action_logits = self.low_controllers[0](state_subgoal)\n        value = self.low_critics[0](state_subgoal)\n        return action_logits, value\n    def hierarchical_forward(self, state):\n        current_goal = state\n        subgoals = []\n        values = []\n        for level in range(len(self.meta_controllers)):\n            subgoal, value = self.forward_meta(state, level)\n            subgoals.append(subgoal)\n            values.append(value)\n            current_goal = subgoal\n        action_logits, low_value = self.forward_low(state, current_goal)\n        values.append(low_value)\n        return {\n            'subgoals': subgoals,\n            'action_logits': action_logits,\n            'values': values\n        }\nclass GoalConditionedAgent:\n    def __init__(self, state_dim, action_dim, goal_dim=None):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.goal_dim = goal_dim or state_dim\n        self.policy_net = nn.Sequential(\n            nn.Linear(state_dim + self.goal_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, action_dim)\n        ).to(device)\n        self.value_net = nn.Sequential(\n            nn.Linear(state_dim + self.goal_dim, 256),\n            nn.ReLU(),\n            nn.Linear(256, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1)\n        ).to(device)\n        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), \n                                         lr=HIERARCHICAL_CONFIG['controller_lr'])\n        self.value_optimizer = optim.Adam(self.value_net.parameters(),\n                                        lr=HIERARCHICAL_CONFIG['controller_lr'])\n        self.buffer = deque(maxlen=100000)\n        self.her_ratio = 0.8\n        self.goal_strategy = \"future\"\n        self.training_stats = {\n            'policy_losses': [],\n            'value_losses': [],\n            'goal_achievements': [],\n            'intrinsic_rewards': []\n        }\n    def goal_distance(self, achieved_goal, desired_goal):\n        return torch.norm(achieved_goal - desired_goal, dim=-1)\n    def compute_reward(self, achieved_goal, desired_goal, info=None):\n        distance = self.goal_distance(achieved_goal, desired_goal)\n        threshold = HIERARCHICAL_CONFIG['subgoal_threshold']\n        reward = (distance < threshold).float() * 2 - 1\n        return reward\n    def get_action(self, state, goal, deterministic=False):\n        state_tensor = torch.FloatTensor(state).to(device)\n        goal_tensor = torch.FloatTensor(goal).to(device)\n        if len(state_tensor.shape) == 1:\n            state_tensor = state_tensor.unsqueeze(0)\n            goal_tensor = goal_tensor.unsqueeze(0)\n        state_goal = torch.cat([state_tensor, goal_tensor], dim=-1)\n        with torch.no_grad():\n            action_logits = self.policy_net(state_goal)\n            if deterministic:\n                action = action_logits.argmax(dim=-1)\n            else:\n                action_probs = F.softmax(action_logits, dim=-1)\n                action = torch.multinomial(action_probs, 1).squeeze()\n        return action.cpu().numpy() if len(action.shape) > 0 else action.item()\n    def store_episode(self, episode_states, episode_actions, episode_goals, final_achieved_goal):\n        episode_length = len(episode_states)\n        for t in range(episode_length - 1):\n            achieved_goal = episode_states[t+1]\n            reward = self.compute_reward(\n                torch.FloatTensor(achieved_goal),\n                torch.FloatTensor(episode_goals[t])\n            ).item()\n            self.buffer.append({\n                'state': episode_states[t],\n                'action': episode_actions[t],\n                'reward': reward,\n                'next_state': episode_states[t+1],\n                'goal': episode_goals[t],\n                'achieved_goal': achieved_goal\n            })\n        for t in range(episode_length - 1):\n            if np.random.random() < self.her_ratio:\n                if self.goal_strategy == \"future\" and t < episode_length - 2:\n                    future_idx = np.random.randint(t + 1, episode_length)\n                    her_goal = episode_states[future_idx]\n                elif self.goal_strategy == \"episode\":\n                    her_goal = final_achieved_goal\n                else:\n                    her_goal = np.random.randn(self.goal_dim)\n                achieved_goal = episode_states[t+1]\n                her_reward = self.compute_reward(\n                    torch.FloatTensor(achieved_goal),\n                    torch.FloatTensor(her_goal)\n                ).item()\n                self.buffer.append({\n                    'state': episode_states[t],\n                    'action': episode_actions[t],\n                    'reward': her_reward,\n                    'next_state': episode_states[t+1],\n                    'goal': her_goal,\n                    'achieved_goal': achieved_goal\n                })\n    def train_step(self, batch_size=64):\n        if len(self.buffer) < batch_size:\n            return 0, 0\n        batch = random.sample(self.buffer, batch_size)\n        states = torch.FloatTensor([exp['state'] for exp in batch]).to(device)\n        actions = torch.LongTensor([exp['action'] for exp in batch]).to(device)\n        rewards = torch.FloatTensor([exp['reward'] for exp in batch]).to(device)\n        next_states = torch.FloatTensor([exp['next_state'] for exp in batch]).to(device)\n        goals = torch.FloatTensor([exp['goal'] for exp in batch]).to(device)\n        state_goal = torch.cat([states, goals], dim=-1)\n        next_state_goal = torch.cat([next_states, goals], dim=-1)\n        current_values = self.value_net(state_goal).squeeze()\n        with torch.no_grad():\n            next_values = self.value_net(next_state_goal).squeeze()\n            target_values = rewards + 0.99 * next_values\n        value_loss = F.mse_loss(current_values, target_values)\n        self.value_optimizer.zero_grad()\n        value_loss.backward()\n        self.value_optimizer.step()\n        action_logits = self.policy_net(state_goal)\n        action_log_probs = F.log_softmax(action_logits, dim=-1)\n        selected_log_probs = action_log_probs.gather(1, actions.unsqueeze(1)).squeeze()\n        with torch.no_grad():\n            advantages = target_values - current_values\n        policy_loss = -(selected_log_probs * advantages).mean()\n        self.policy_optimizer.zero_grad()\n        policy_loss.backward()\n        self.policy_optimizer.step()\n        self.training_stats['policy_losses'].append(policy_loss.item())\n        self.training_stats['value_losses'].append(value_loss.item())\n        goal_achieved = (rewards > 0).float().mean().item()\n        self.training_stats['goal_achievements'].append(goal_achieved)\n        return policy_loss.item(), value_loss.item()\nclass FeudalNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim, goal_dim=64, hidden_dim=256):\n        super(FeudalNetwork, self).__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.goal_dim = goal_dim\n        self.perception = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.manager = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, goal_dim)\n        )\n        self.worker = nn.Sequential(\n            nn.Linear(hidden_dim + goal_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        )\n        self.manager_critic = nn.Sequential(\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.worker_critic = nn.Sequential(\n            nn.Linear(hidden_dim + goal_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.curiosity_net = nn.Sequential(\n            nn.Linear(hidden_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def forward(self, state, previous_goal=None):\n        perception = self.perception(state)\n        goal = self.manager(perception)\n        goal = F.normalize(goal, p=2, dim=-1)\n        if previous_goal is not None:\n            worker_input = torch.cat([perception, previous_goal], dim=-1)\n        else:\n            worker_input = torch.cat([perception, goal], dim=-1)\n        action_logits = self.worker(worker_input)\n        manager_value = self.manager_critic(perception)\n        worker_value = self.worker_critic(worker_input)\n        return {\n            'goal': goal,\n            'action_logits': action_logits,\n            'manager_value': manager_value,\n            'worker_value': worker_value,\n            'perception': perception\n        }\n    def compute_intrinsic_reward(self, current_perception, next_perception, goal):\n        state_diff = next_perception - current_perception\n        intrinsic_reward = F.cosine_similarity(goal, state_diff, dim=-1)\n        return intrinsic_reward\nclass HierarchicalRLEnvironment:\n    def __init__(self, size=10, num_goals=3):\n        self.size = size\n        self.num_goals = num_goals\n        self.reset()\n    def reset(self):\n        self.agent_pos = np.array([0, 0])\n        self.goals = []\n        for _ in range(self.num_goals):\n            goal_pos = np.random.randint(0, self.size, size=2)\n            while np.array_equal(goal_pos, self.agent_pos):\n                goal_pos = np.random.randint(0, self.size, size=2)\n            self.goals.append(goal_pos)\n        self.current_goal_idx = 0\n        self.steps = 0\n        self.max_steps = self.size * 4\n        return self.get_state()\n    def get_state(self):\n        state = np.zeros((self.size, self.size))\n        state[self.agent_pos[0], self.agent_pos[1]] = 1.0\n        for i, goal in enumerate(self.goals):\n            if i == self.current_goal_idx:\n                state[goal[0], goal[1]] = 0.5\n            else:\n                state[goal[0], goal[1]] = 0.3\n        return state.flatten()\n    def step(self, action):\n        moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]\n        if action < len(moves):\n            new_pos = self.agent_pos + np.array(moves[action])\n            new_pos = np.clip(new_pos, 0, self.size - 1)\n            self.agent_pos = new_pos\n        self.steps += 1\n        reward = 0\n        done = False\n        current_goal = self.goals[self.current_goal_idx]\n        if np.array_equal(self.agent_pos, current_goal):\n            reward = 10.0\n            self.current_goal_idx += 1\n            if self.current_goal_idx >= self.num_goals:\n                done = True\n                reward += 50.0\n        else:\n            distance = np.linalg.norm(self.agent_pos - current_goal)\n            reward = -0.1 * distance\n        if self.steps >= self.max_steps:\n            done = True\n            reward -= 10.0\n        info = {\n            'goals_completed': self.current_goal_idx,\n            'current_goal': current_goal,\n            'agent_pos': self.agent_pos.copy()\n        }\n        return self.get_state(), reward, done, info\nprint(\"üèóÔ∏è Hierarchical RL components implemented successfully!\")\nprint(\"üìù Key components:\")\nprint(\"  ‚Ä¢ Option: Options framework implementation\")\nprint(\"  ‚Ä¢ HierarchicalActorCritic: Multi-level hierarchical policy\")\nprint(\"  ‚Ä¢ GoalConditionedAgent: Goal-conditioned RL with HER\")\nprint(\"  ‚Ä¢ FeudalNetwork: Feudal Networks architecture\")\nprint(\"  ‚Ä¢ HierarchicalRLEnvironment: Custom test environment\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb4f2f",
   "metadata": {},
   "source": [
    "# Section 3: Advanced Planning and Control\n",
    "\n",
    "Advanced planning algorithms combine learned models with sophisticated search techniques.\n",
    "\n",
    "## 3.1 Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "MCTS is a best-first search algorithm that uses Monte Carlo simulations for decision making.\n",
    "\n",
    "### MCTS Algorithm Steps:\n",
    "1. **Selection**: Navigate down the tree using UCB1 formula\n",
    "2. **Expansion**: Add new child nodes to the tree\n",
    "3. **Simulation**: Run random rollouts from leaf nodes\n",
    "4. **Backpropagation**: Update node values with simulation results\n",
    "\n",
    "### UCB1 Selection Formula:\n",
    "$$UCB1(s,a) = Q(s,a) + c \\sqrt{\\frac{\\ln N(s)}{N(s,a)}}$$\n",
    "\n",
    "Where:\n",
    "- $Q(s,a)$: Average reward for action $a$ in state $s$\n",
    "- $N(s)$: Visit count for state $s$\n",
    "- $N(s,a)$: Visit count for action $a$ in state $s$\n",
    "- $c$: Exploration constant\n",
    "\n",
    "### AlphaZero Integration\n",
    "Combines MCTS with neural networks:\n",
    "- **Policy Network**: $p(a|s)$ guides selection\n",
    "- **Value Network**: $v(s)$ estimates leaf values\n",
    "- **Self-Play**: Generates training data through MCTS games\n",
    "\n",
    "## 3.2 Model-Based Value Expansion (MVE)\n",
    "\n",
    "Uses learned models to expand value function estimates:\n",
    "\n",
    "$$V_{MVE}(s) = \\max_a \\left[ r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s') \\right]$$\n",
    "\n",
    "### Trajectory Optimization\n",
    "- **Cross-Entropy Method (CEM)**: Iterative sampling and fitting\n",
    "- **Random Shooting**: Sample multiple action sequences\n",
    "- **Model Predictive Path Integral (MPPI)**: Information-theoretic approach\n",
    "\n",
    "## 3.3 Latent Space Planning\n",
    "\n",
    "Planning in learned latent representations:\n",
    "\n",
    "### World Models Architecture:\n",
    "1. **Vision Model (V)**: Encodes observations to latent states\n",
    "2. **Memory Model (M)**: Predicts next latent states  \n",
    "3. **Controller Model (C)**: Maps latent states to actions\n",
    "\n",
    "### PlaNet Algorithm:\n",
    "- **Recurrent State Space Model (RSSM)**:\n",
    "  - Deterministic path: $h_t = f(h_{t-1}, a_{t-1})$\n",
    "  - Stochastic path: $s_t \\sim p(s_t | h_t)$\n",
    "- **Planning**: Cross-entropy method in latent space\n",
    "- **Learning**: Variational inference for world model\n",
    "\n",
    "## 3.4 Challenges and Solutions\n",
    "\n",
    "### Model Bias\n",
    "- **Problem**: Learned models have prediction errors\n",
    "- **Solutions**: \n",
    "  - Model ensembles for uncertainty quantification\n",
    "  - Conservative planning with uncertainty penalties\n",
    "  - Robust optimization techniques\n",
    "\n",
    "### Computational Complexity\n",
    "- **Problem**: Planning is computationally expensive\n",
    "- **Solutions**:\n",
    "  - Hierarchical planning with multiple time scales\n",
    "  - Approximate planning with limited horizons\n",
    "  - Parallel Monte Carlo simulations\n",
    "\n",
    "### Exploration vs Exploitation\n",
    "- **Problem**: Balancing exploration and exploitation in planning\n",
    "- **Solutions**:\n",
    "  - UCB-based selection in MCTS\n",
    "  - Optimistic initialization\n",
    "  - Information-gain based rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1676bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n    def __init__(self, state, parent=None, action=None, prior=0.0):\n        self.state = state\n        self.parent = parent\n        self.action = action\n        self.children = {}\n        self.visit_count = 0\n        self.value_sum = 0.0\n        self.prior = prior\n        self.policy_priors = None\n        self.value_estimate = 0.0\n    def is_leaf(self):\n        return len(self.children) == 0\n    def is_root(self):\n        return self.parent is None\n    def get_value(self):\n        if self.visit_count == 0:\n            return 0.0\n        return self.value_sum / self.visit_count\n    def ucb_score(self, c_puct=1.4):\n        if self.visit_count == 0:\n            return float('inf')\n        exploitation = self.get_value()\n        if self.parent is not None:\n            exploration = c_puct * self.prior * math.sqrt(self.parent.visit_count) / (1 + self.visit_count)\n        else:\n            exploration = 0\n        return exploitation + exploration\n    def select_child(self, c_puct=1.4):\n        if self.is_leaf():\n            return None\n        return max(self.children.values(), key=lambda child: child.ucb_score(c_puct))\n    def expand(self, actions, priors=None):\n        if priors is None:\n            priors = [1.0 / len(actions)] * len(actions)\n        for action, prior in zip(actions, priors):\n            if action not in self.children:\n                self.children[action] = MCTSNode(\n                    state=None,\n                    parent=self,\n                    action=action,\n                    prior=prior\n                )\n    def backup(self, value):\n        self.visit_count += 1\n        self.value_sum += value\n        if not self.is_root():\n            self.parent.backup(value)\nclass MonteCarloTreeSearch:\n    def __init__(self, model, value_network=None, policy_network=None):\n        self.model = model\n        self.value_network = value_network\n        self.policy_network = policy_network\n        self.c_puct = PLANNING_CONFIG['exploration_constant']\n        self.num_simulations = PLANNING_CONFIG['mcts_simulations']\n    def search(self, root_state, num_simulations=None):\n        if num_simulations is None:\n            num_simulations = self.num_simulations\n        root = MCTSNode(root_state)\n        if self.policy_network is not None:\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(root_state).unsqueeze(0).to(device)\n                policy_logits = self.policy_network(state_tensor)\n                priors = F.softmax(policy_logits, dim=-1).squeeze().cpu().numpy()\n                root.expand(list(range(len(priors))), priors)\n        else:\n            num_actions = 4\n            root.expand(list(range(num_actions)))\n        for _ in range(num_simulations):\n            self._simulate(root)\n        return root\n    def _simulate(self, root):\n        current = root\n        path = []\n        while not current.is_leaf():\n            current = current.select_child(self.c_puct)\n            path.append(current)\n        if current.visit_count == 0:\n            value = self._evaluate_leaf(current)\n        else:\n            if hasattr(self.model, 'get_possible_actions'):\n                actions = self.model.get_possible_actions(current.state)\n            else:\n                actions = list(range(4))\n            current.expand(actions)\n            if current.children:\n                action = np.random.choice(list(current.children.keys()))\n                child = current.children[action]\n                if hasattr(self.model, 'predict_mean'):\n                    next_state, reward = self.model.predict_mean(\n                        torch.FloatTensor(current.state).to(device),\n                        torch.LongTensor([action]).to(device)\n                    )\n                    child.state = next_state.cpu().numpy()\n                else:\n                    child.state = current.state\n                value = self._evaluate_leaf(child)\n                path.append(child)\n            else:\n                value = self._evaluate_leaf(current)\n        for node in reversed(path):\n            node.backup(value)\n        root.backup(value)\n    def _evaluate_leaf(self, node):\n        if self.value_network is not None:\n            with torch.no_grad():\n                state_tensor = torch.FloatTensor(node.state).unsqueeze(0).to(device)\n                value = self.value_network(state_tensor).item()\n        else:\n            value = self._rollout(node.state)\n        return value\n    def _rollout(self, state, depth=10):\n        total_reward = 0\n        current_state = state\n        for i in range(depth):\n            action = np.random.randint(4)\n            if hasattr(self.model, 'predict_mean'):\n                next_state, reward = self.model.predict_mean(\n                    torch.FloatTensor(current_state).to(device),\n                    torch.LongTensor([action]).to(device)\n                )\n                total_reward += reward.item() * (0.99 ** i)\n                current_state = next_state.cpu().numpy()\n            else:\n                reward = np.random.randn()\n                total_reward += reward * (0.99 ** i)\n        return total_reward\n    def get_action_probabilities(self, root):\n        if root.is_leaf():\n            return np.ones(4) / 4\n        visits = []\n        actions = []\n        for action, child in root.children.items():\n            actions.append(action)\n            visits.append(child.visit_count)\n        if sum(visits) == 0:\n            return np.ones(len(actions)) / len(actions)\n        visits = np.array(visits)\n        probabilities = visits / visits.sum()\n        full_probs = np.zeros(4)\n        for action, prob in zip(actions, probabilities):\n            if action < len(full_probs):\n                full_probs[action] = prob\n        return full_probs\nclass ModelBasedValueExpansion:\n    def __init__(self, model, value_function, expansion_depth=3):\n        self.model = model\n        self.value_function = value_function\n        self.expansion_depth = expansion_depth\n    def expand_value(self, state, depth=0):\n        if depth >= self.expansion_depth:\n            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n            with torch.no_grad():\n                return self.value_function(state_tensor).item()\n        num_actions = 4\n        action_values = []\n        for action in range(num_actions):\n            if hasattr(self.model, 'predict_mean'):\n                next_state, reward = self.model.predict_mean(\n                    torch.FloatTensor(state).to(device),\n                    torch.LongTensor([action]).to(device)\n                )\n                next_state = next_state.cpu().numpy()\n                reward = reward.item()\n            else:\n                next_state = state\n                reward = np.random.randn()\n            next_value = self.expand_value(next_state, depth + 1)\n            action_value = reward + 0.99 * next_value\n            action_values.append(action_value)\n        return max(action_values)\n    def plan_action(self, state):\n        num_actions = 4\n        action_values = []\n        for action in range(num_actions):\n            if hasattr(self.model, 'predict_mean'):\n                next_state, reward = self.model.predict_mean(\n                    torch.FloatTensor(state).to(device),\n                    torch.LongTensor([action]).to(device)\n                )\n                next_state = next_state.cpu().numpy()\n                reward = reward.item()\n            else:\n                next_state = state\n                reward = np.random.randn()\n            next_value = self.expand_value(next_state, depth=1)\n            action_value = reward + 0.99 * next_value\n            action_values.append(action_value)\n        return np.argmax(action_values)\nclass LatentSpacePlanner:\n    def __init__(self, encoder, decoder, latent_dynamics, latent_dim=64):\n        self.encoder = encoder\n        self.decoder = decoder\n        self.latent_dynamics = latent_dynamics\n        self.latent_dim = latent_dim\n        self.population_size = 500\n        self.elite_fraction = 0.1\n        self.num_iterations = 10\n    def encode_state(self, state):\n        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            latent_state = self.encoder(state_tensor)\n        return latent_state\n    def decode_state(self, latent_state):\n        with torch.no_grad():\n            decoded_state = self.decoder(latent_state)\n        return decoded_state.cpu().numpy()\n    def plan_in_latent_space(self, initial_state, horizon=10):\n        latent_state = self.encode_state(initial_state)\n        action_dim = 4\n        action_mean = np.zeros((horizon, action_dim))\n        action_std = np.ones((horizon, action_dim))\n        best_actions = None\n        best_reward = float('-inf')\n        for iteration in range(self.num_iterations):\n            action_sequences = []\n            rewards = []\n            for _ in range(self.population_size):\n                actions = []\n                for t in range(horizon):\n                    action_logits = np.random.normal(action_mean[t], action_std[t])\n                    action = np.argmax(action_logits)\n                    actions.append(action)\n                action_sequences.append(actions)\n                reward = self._evaluate_latent_sequence(latent_state, actions)\n                rewards.append(reward)\n            elite_idx = np.argsort(rewards)[-int(self.elite_fraction * self.population_size):]\n            elite_actions = [action_sequences[i] for i in elite_idx]\n            if max(rewards) > best_reward:\n                best_reward = max(rewards)\n                best_actions = action_sequences[np.argmax(rewards)]\n            if len(elite_actions) > 0:\n                elite_array = np.array(elite_actions)\n                for t in range(horizon):\n                    action_counts = np.bincount(elite_array[:, t], minlength=action_dim)\n                    action_probs = action_counts / len(elite_actions)\n                    action_mean[t] = np.log(action_probs + 1e-8)\n                    action_std[t] *= 0.9\n        return best_actions[0] if best_actions else 0\n    def _evaluate_latent_sequence(self, initial_latent_state, actions):\n        current_latent = initial_latent_state\n        total_reward = 0\n        for t, action in enumerate(actions):\n            action_tensor = torch.LongTensor([action]).to(device)\n            if hasattr(self.latent_dynamics, 'forward'):\n                with torch.no_grad():\n                    next_latent, reward = self.latent_dynamics(current_latent, action_tensor)\n                    total_reward += reward.item() * (0.99 ** t)\n                    current_latent = next_latent\n            else:\n                reward = np.random.randn()\n                total_reward += reward * (0.99 ** t)\n        return total_reward\nclass WorldModel(nn.Module):\n    def __init__(self, obs_dim, action_dim, latent_dim=64, hidden_dim=256):\n        super(WorldModel, self).__init__()\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.latent_dim = latent_dim\n        self.encoder = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, latent_dim * 2)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, obs_dim)\n        )\n        self.dynamics = nn.Sequential(\n            nn.Linear(latent_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, latent_dim + 1)\n        )\n        self.rnn = nn.GRU(latent_dim + action_dim, hidden_dim, batch_first=True)\n        self.hidden_to_latent = nn.Linear(hidden_dim, latent_dim * 2)\n    def encode(self, obs):\n        encoded = self.encoder(obs)\n        mean, log_std = encoded.chunk(2, dim=-1)\n        return mean, log_std\n    def decode(self, latent):\n        return self.decoder(latent)\n    def sample_latent(self, mean, log_std):\n        std = torch.exp(log_std)\n        eps = torch.randn_like(std)\n        return mean + eps * std\n    def predict_next(self, latent_state, action):\n        if action.dtype == torch.long:\n            action_one_hot = torch.zeros(action.size(0), self.action_dim).to(action.device)\n            action_one_hot.scatter_(1, action.unsqueeze(1), 1)\n            action = action_one_hot\n        input_tensor = torch.cat([latent_state, action], dim=-1)\n        output = self.dynamics(input_tensor)\n        next_latent = output[:, :self.latent_dim]\n        reward = output[:, self.latent_dim:]\n        return next_latent, reward\n    def forward(self, obs_sequence, action_sequence):\n        batch_size, seq_len = obs_sequence.shape[:2]\n        obs_flat = obs_sequence.view(-1, self.obs_dim)\n        latent_mean, latent_log_std = self.encode(obs_flat)\n        latent_mean = latent_mean.view(batch_size, seq_len, self.latent_dim)\n        latent_log_std = latent_log_std.view(batch_size, seq_len, self.latent_dim)\n        latent_states = self.sample_latent(latent_mean, latent_log_std)\n        predicted_latents = []\n        predicted_rewards = []\n        for t in range(seq_len - 1):\n            next_latent, reward = self.predict_next(\n                latent_states[:, t], \n                action_sequence[:, t]\n            )\n            predicted_latents.append(next_latent)\n            predicted_rewards.append(reward)\n        predicted_latents = torch.stack(predicted_latents, dim=1)\n        predicted_rewards = torch.stack(predicted_rewards, dim=1)\n        predicted_obs = self.decode(predicted_latents.view(-1, self.latent_dim))\n        predicted_obs = predicted_obs.view(batch_size, seq_len - 1, self.obs_dim)\n        return {\n            'latent_mean': latent_mean,\n            'latent_log_std': latent_log_std,\n            'predicted_obs': predicted_obs,\n            'predicted_rewards': predicted_rewards,\n            'latent_states': latent_states\n        }\nprint(\"üéØ Advanced Planning components implemented successfully!\")\nprint(\"üìù Key components:\")\nprint(\"  ‚Ä¢ MCTSNode & MonteCarloTreeSearch: MCTS algorithm implementation\")\nprint(\"  ‚Ä¢ ModelBasedValueExpansion: MVE for planning with learned models\") \nprint(\"  ‚Ä¢ LatentSpacePlanner: Planning in learned latent representations\")\nprint(\"  ‚Ä¢ WorldModel: Complete world model architecture for latent planning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e4c08",
   "metadata": {},
   "source": [
    "# Section 4: Practical Demonstrations and Experiments\n",
    "\n",
    "This section provides hands-on experiments to demonstrate the concepts and implementations.\n",
    "\n",
    "## 4.1 Experiment Setup\n",
    "\n",
    "We'll create practical experiments to showcase:\n",
    "\n",
    "1. **Model-Based vs Model-Free Comparison**\n",
    "   - Sample efficiency analysis\n",
    "   - Performance on different environments\n",
    "   - Computational overhead comparison\n",
    "\n",
    "2. **Hierarchical RL Benefits**\n",
    "   - Multi-goal navigation tasks\n",
    "   - Skill reuse and transfer\n",
    "   - Temporal abstraction advantages\n",
    "\n",
    "3. **Planning Algorithm Comparison**\n",
    "   - MCTS vs random rollouts\n",
    "   - Value expansion effectiveness\n",
    "   - Latent space planning benefits\n",
    "\n",
    "4. **Integration Study**\n",
    "   - Combining all methods\n",
    "   - Real-world application scenarios\n",
    "   - Performance analysis and trade-offs\n",
    "\n",
    "## 4.2 Metrics and Evaluation\n",
    "\n",
    "### Performance Metrics:\n",
    "- **Sample Efficiency**: Steps to reach performance threshold\n",
    "- **Asymptotic Performance**: Final average reward\n",
    "- **Computation Time**: Planning and learning overhead\n",
    "- **Memory Usage**: Model storage requirements\n",
    "- **Transfer Performance**: Success on related tasks\n",
    "\n",
    "### Statistical Analysis:\n",
    "- Multiple random seeds for reliability\n",
    "- Confidence intervals and significance tests\n",
    "- Learning curve analysis\n",
    "- Ablation studies for each component\n",
    "\n",
    "## 4.3 Environments for Testing\n",
    "\n",
    "### Simple Grid World:\n",
    "- **Purpose**: Basic concept demonstration\n",
    "- **Features**: Discrete states, clear visualization\n",
    "- **Challenges**: Navigation, goal reaching\n",
    "\n",
    "### Continuous Control:\n",
    "- **Purpose**: Real-world applicability\n",
    "- **Features**: Continuous state-action spaces\n",
    "- **Challenges**: Precise control, dynamic systems\n",
    "\n",
    "### Hierarchical Tasks:\n",
    "- **Purpose**: Multi-level decision making\n",
    "- **Features**: Natural task decomposition\n",
    "- **Challenges**: Long-horizon planning, skill coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34dcf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperimentRunner:\n    def __init__(self, env_class, env_kwargs=None):\n        self.env_class = env_class\n        self.env_kwargs = env_kwargs or {}\n        self.results = {}\n    def run_experiment(self, agent_configs, num_episodes=500, num_seeds=3):\n        results = {}\n        for agent_name, agent_config in agent_configs.items():\n            print(f\"\\nüîÑ Running experiment for {agent_name}...\")\n            agent_results = []\n            for seed in range(num_seeds):\n                print(f\"  Seed {seed + 1}/{num_seeds}\")\n                np.random.seed(seed)\n                torch.manual_seed(seed)\n                random.seed(seed)\n                env = self.env_class(**self.env_kwargs)\n                agent = agent_config['class'](**agent_config['params'])\n                episode_rewards = []\n                episode_lengths = []\n                model_losses = []\n                planning_times = []\n                for episode in range(num_episodes):\n                    state = env.reset()\n                    episode_reward = 0\n                    episode_length = 0\n                    done = False\n                    start_time = time.time()\n                    while not done:\n                        if hasattr(agent, 'get_action'):\n                            action = agent.get_action(state)\n                        elif hasattr(agent, 'plan_action'):\n                            action = agent.plan_action(state)\n                        else:\n                            action = np.random.randint(env.action_space.n if hasattr(env, 'action_space') else 4)\n                        if hasattr(env, 'step'):\n                            next_state, reward, done, info = env.step(action)\n                        else:\n                            next_state, reward, done = state, np.random.randn(), np.random.random() < 0.1\n                            info = {}\n                        episode_reward += reward\n                        episode_length += 1\n                        if hasattr(agent, 'store_experience'):\n                            agent.store_experience(state, action, reward, next_state, done)\n                        if hasattr(agent, 'update_q_function'):\n                            q_loss = agent.update_q_function()\n                        elif hasattr(agent, 'train_step'):\n                            losses = agent.train_step()\n                        if hasattr(agent, 'update_model'):\n                            model_loss = agent.update_model()\n                            model_losses.append(model_loss)\n                        if hasattr(agent, 'planning_step'):\n                            agent.planning_step()\n                        state = next_state\n                        if episode_length > 500:\n                            break\n                    planning_time = time.time() - start_time\n                    planning_times.append(planning_time)\n                    episode_rewards.append(episode_reward)\n                    episode_lengths.append(episode_length)\n                    if (episode + 1) % 100 == 0:\n                        avg_reward = np.mean(episode_rewards[-100:])\n                        print(f\"    Episode {episode + 1}: Avg Reward = {avg_reward:.2f}\")\n                agent_results.append({\n                    'rewards': episode_rewards,\n                    'lengths': episode_lengths,\n                    'model_losses': model_losses,\n                    'planning_times': planning_times,\n                    'final_performance': np.mean(episode_rewards[-50:])\n                })\n            results[agent_name] = agent_results\n        self.results = results\n        return results\n    def analyze_results(self):\n        if not self.results:\n            print(\"‚ùå No results to analyze. Run experiment first.\")\n            return\n        print(\"\\nüìä Experiment Results Analysis\")\n        print(\"=\" * 50)\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        fig.suptitle('Model-Based vs Model-Free Comparison', fontsize=16)\n        ax1 = axes[0, 0]\n        for agent_name, agent_results in self.results.items():\n            all_rewards = [result['rewards'] for result in agent_results]\n            min_length = min(len(rewards) for rewards in all_rewards)\n            rewards_array = np.array([rewards[:min_length] for rewards in all_rewards])\n            mean_rewards = np.mean(rewards_array, axis=0)\n            std_rewards = np.std(rewards_array, axis=0)\n            episodes = np.arange(min_length)\n            ax1.plot(episodes, mean_rewards, label=agent_name, linewidth=2)\n            ax1.fill_between(episodes, \n                           mean_rewards - std_rewards, \n                           mean_rewards + std_rewards, \n                           alpha=0.3)\n        ax1.set_xlabel('Episode')\n        ax1.set_ylabel('Average Reward')\n        ax1.set_title('Learning Curves')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax2 = axes[0, 1]\n        threshold = -100\n        agent_names = []\n        sample_efficiencies = []\n        sample_stds = []\n        for agent_name, agent_results in self.results.items():\n            episodes_to_threshold = []\n            for result in agent_results:\n                rewards = result['rewards']\n                moving_avg = np.convolve(rewards, np.ones(50)/50, mode='valid')\n                threshold_idx = np.where(moving_avg >= threshold)[0]\n                if len(threshold_idx) > 0:\n                    episodes_to_threshold.append(threshold_idx[0] + 50)\n                else:\n                    episodes_to_threshold.append(len(rewards))\n            agent_names.append(agent_name)\n            sample_efficiencies.append(np.mean(episodes_to_threshold))\n            sample_stds.append(np.std(episodes_to_threshold))\n        bars = ax2.bar(agent_names, sample_efficiencies, yerr=sample_stds, \n                      capsize=5, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(agent_names)])\n        ax2.set_ylabel('Episodes to Threshold')\n        ax2.set_title('Sample Efficiency')\n        ax2.tick_params(axis='x', rotation=45)\n        ax3 = axes[1, 0]\n        final_performances = []\n        final_stds = []\n        for agent_name, agent_results in self.results.items():\n            performances = [result['final_performance'] for result in agent_results]\n            final_performances.append(np.mean(performances))\n            final_stds.append(np.std(performances))\n        bars = ax3.bar(agent_names, final_performances, yerr=final_stds,\n                      capsize=5, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(agent_names)])\n        ax3.set_ylabel('Final Average Reward')\n        ax3.set_title('Final Performance')\n        ax3.tick_params(axis='x', rotation=45)\n        ax4 = axes[1, 1]\n        planning_times = []\n        time_stds = []\n        for agent_name, agent_results in self.results.items():\n            times = []\n            for result in agent_results:\n                if result['planning_times']:\n                    times.extend(result['planning_times'])\n            if times:\n                planning_times.append(np.mean(times))\n                time_stds.append(np.std(times))\n            else:\n                planning_times.append(0)\n                time_stds.append(0)\n        bars = ax4.bar(agent_names, planning_times, yerr=time_stds,\n                      capsize=5, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(agent_names)])\n        ax4.set_ylabel('Average Planning Time (s)')\n        ax4.set_title('Computational Overhead')\n        ax4.tick_params(axis='x', rotation=45)\n        plt.tight_layout()\n        plt.show()\n        print(\"\\nüìà Summary Statistics:\")\n        for agent_name, agent_results in self.results.items():\n            performances = [result['final_performance'] for result in agent_results]\n            mean_perf = np.mean(performances)\n            std_perf = np.std(performances)\n            print(f\"\\n{agent_name}:\")\n            print(f\"  Final Performance: {mean_perf:.2f} ¬± {std_perf:.2f}\")\n            episodes_to_threshold = []\n            for result in agent_results:\n                rewards = result['rewards']\n                moving_avg = np.convolve(rewards, np.ones(50)/50, mode='valid')\n                threshold_idx = np.where(moving_avg >= threshold)[0]\n                if len(threshold_idx) > 0:\n                    episodes_to_threshold.append(threshold_idx[0] + 50)\n            if episodes_to_threshold:\n                mean_efficiency = np.mean(episodes_to_threshold)\n                std_efficiency = np.std(episodes_to_threshold)\n                print(f\"  Sample Efficiency: {mean_efficiency:.0f} ¬± {std_efficiency:.0f} episodes\")\nclass SimpleGridWorld:\n    def __init__(self, size=8, num_goals=1):\n        self.size = size\n        self.num_goals = num_goals\n        self.action_space_size = 4\n        self.state_dim = size * size\n        self.reset()\n    def reset(self):\n        self.agent_pos = [0, 0]\n        self.goal_pos = [np.random.randint(self.size//2, self.size),\n                        np.random.randint(self.size//2, self.size)]\n        while self.agent_pos == self.goal_pos:\n            self.goal_pos = [np.random.randint(1, self.size),\n                           np.random.randint(1, self.size)]\n        self.steps = 0\n        self.max_steps = self.size * 4\n        return self._get_state()\n    def _get_state(self):\n        state = np.zeros(self.state_dim)\n        agent_idx = self.agent_pos[0] * self.size + self.agent_pos[1]\n        goal_idx = self.goal_pos[0] * self.size + self.goal_pos[1]\n        state[agent_idx] = 1.0\n        state[goal_idx] = 0.5\n        return state\n    def step(self, action):\n        moves = [[-1, 0], [1, 0], [0, -1], [0, 1]]\n        if action < len(moves):\n            new_pos = [\n                self.agent_pos[0] + moves[action][0],\n                self.agent_pos[1] + moves[action][1]\n            ]\n            new_pos[0] = max(0, min(self.size - 1, new_pos[0]))\n            new_pos[1] = max(0, min(self.size - 1, new_pos[1]))\n            self.agent_pos = new_pos\n        self.steps += 1\n        distance = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n        if distance == 0:\n            reward = 100.0\n            done = True\n        else:\n            reward = -1.0 - 0.1 * distance\n            done = False\n        if self.steps >= self.max_steps:\n            done = True\n            if distance > 0:\n                reward -= 50.0\n        info = {'distance': distance, 'steps': self.steps}\n        return self._get_state(), reward, done, info\nprint(\"üöÄ Setting up Model-Based vs Model-Free Experiment...\")\nagent_configs = {\n    'Dyna-Q (Model-Based)': {\n        'class': DynaQAgent,\n        'params': {'state_dim': 64, 'action_dim': 4, 'lr': 1e-3}\n    }\n}\nexperiment = ExperimentRunner(SimpleGridWorld, {'size': 8, 'num_goals': 1})\nimport time\nprint(\"üìù Agent configurations created successfully!\")\nprint(\"üîß Experiment environment ready for model-based vs model-free comparison!\")\nprint(\"\\nüí° To run the experiment, call: experiment.run_experiment(agent_configs, num_episodes=200, num_seeds=3)\")\nprint(\"üìä To analyze results, call: experiment.analyze_results()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cc3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalRLExperiment:\n    def __init__(self):\n        self.results = {}\n    def create_multi_goal_environment(self, size=12, num_goals=4):\n        return HierarchicalRLEnvironment(size=size, num_goals=num_goals)\n    def run_hierarchical_experiment(self, num_episodes=300, num_seeds=3):\n        print(\"üèóÔ∏è Running Hierarchical RL Experiment...\")\n        print(\"üéØ Testing: Goal-Conditioned RL vs Standard RL vs Hierarchical AC\")\n        env_size = 10\n        num_goals = 3\n        agent_configs = {\n            'Goal-Conditioned Agent': {\n                'class': GoalConditionedAgent,\n                'params': {\n                    'state_dim': env_size * env_size,\n                    'action_dim': 4,\n                    'goal_dim': env_size * env_size\n                }\n            },\n            'Standard DQN-like': {\n                'class': DynaQAgent,\n                'params': {\n                    'state_dim': env_size * env_size,\n                    'action_dim': 4,\n                    'lr': 1e-3\n                }\n            }\n        }\n        results = {}\n        for agent_name, agent_config in agent_configs.items():\n            print(f\"\\nüîÑ Testing {agent_name}...\")\n            agent_results = []\n            for seed in range(num_seeds):\n                print(f\"  Seed {seed + 1}/{num_seeds}\")\n                np.random.seed(seed)\n                torch.manual_seed(seed)\n                random.seed(seed)\n                env = self.create_multi_goal_environment(env_size, num_goals)\n                agent = agent_config['class'](**agent_config['params'])\n                episode_rewards = []\n                goal_achievements = []\n                episode_lengths = []\n                skill_reuse_success = []\n                for episode in range(num_episodes):\n                    state = env.reset()\n                    episode_reward = 0\n                    episode_length = 0\n                    goals_reached = 0\n                    done = False\n                    if agent_name == 'Goal-Conditioned Agent':\n                        episode_states = [state]\n                        episode_actions = []\n                        episode_goals = []\n                        current_goal = np.zeros_like(state)\n                        if hasattr(env, 'goals') and len(env.goals) > 0:\n                            goal_pos = env.goals[env.current_goal_idx]\n                            goal_idx = goal_pos[0] * env_size + goal_pos[1]\n                            current_goal[goal_idx] = 1.0\n                    while not done and episode_length < 200:\n                        if agent_name == 'Goal-Conditioned Agent':\n                            action = agent.get_action(state, current_goal)\n                            episode_goals.append(current_goal.copy())\n                        else:\n                            action = agent.get_action(state)\n                        next_state, reward, done, info = env.step(action)\n                        episode_reward += reward\n                        episode_length += 1\n                        if 'goals_completed' in info:\n                            goals_reached = info['goals_completed']\n                        if agent_name == 'Goal-Conditioned Agent':\n                            episode_states.append(next_state)\n                            episode_actions.append(action)\n                        else:\n                            if hasattr(agent, 'store_experience'):\n                                agent.store_experience(state, action, reward, next_state, done)\n                            if hasattr(agent, 'update_q_function'):\n                                agent.update_q_function()\n                            if hasattr(agent, 'update_model'):\n                                agent.update_model()\n                        state = next_state\n                        if agent_name == 'Goal-Conditioned Agent' and hasattr(env, 'goals'):\n                            if env.current_goal_idx < len(env.goals):\n                                goal_pos = env.goals[env.current_goal_idx]\n                                current_goal = np.zeros_like(state)\n                                goal_idx = goal_pos[0] * env_size + goal_pos[1]\n                                current_goal[goal_idx] = 1.0\n                    if agent_name == 'Goal-Conditioned Agent' and len(episode_states) > 1:\n                        final_achieved_goal = episode_states[-1]\n                        agent.store_episode(episode_states, episode_actions, episode_goals, final_achieved_goal)\n                        for _ in range(10):\n                            agent.train_step(batch_size=32)\n                    episode_rewards.append(episode_reward)\n                    goal_achievements.append(goals_reached / num_goals)\n                    episode_lengths.append(episode_length)\n                    if episode % 50 == 0 and episode > 0:\n                        skill_reuse_score = self._test_skill_reuse(agent, env, agent_name)\n                        skill_reuse_success.append(skill_reuse_score)\n                    if (episode + 1) % 100 == 0:\n                        avg_reward = np.mean(episode_rewards[-50:])\n                        avg_goals = np.mean(goal_achievements[-50:])\n                        print(f\"    Episode {episode + 1}: Reward={avg_reward:.2f}, Goals={avg_goals:.2f}\")\n                agent_results.append({\n                    'rewards': episode_rewards,\n                    'goal_achievements': goal_achievements,\n                    'lengths': episode_lengths,\n                    'skill_reuse': skill_reuse_success,\n                    'final_performance': np.mean(episode_rewards[-30:]),\n                    'final_goal_rate': np.mean(goal_achievements[-30:])\n                })\n            results[agent_name] = agent_results\n        self.results = results\n        return results\n    def _test_skill_reuse(self, agent, env, agent_name):\n        test_env = self.create_multi_goal_environment(env.size, env.num_goals)\n        success_count = 0\n        test_episodes = 5\n        for _ in range(test_episodes):\n            state = test_env.reset()\n            done = False\n            steps = 0\n            goals_reached = 0\n            if agent_name == 'Goal-Conditioned Agent':\n                current_goal = np.zeros_like(state)\n                if hasattr(test_env, 'goals') and len(test_env.goals) > 0:\n                    goal_pos = test_env.goals[0]\n                    goal_idx = goal_pos[0] * test_env.size + goal_pos[1]\n                    current_goal[goal_idx] = 1.0\n            while not done and steps < 100:\n                if agent_name == 'Goal-Conditioned Agent':\n                    action = agent.get_action(state, current_goal, deterministic=True)\n                else:\n                    action = agent.get_action(state, epsilon=0.1)\n                next_state, reward, done, info = test_env.step(action)\n                steps += 1\n                if 'goals_completed' in info:\n                    goals_reached = info['goals_completed']\n                state = next_state\n            if goals_reached > 0 and steps < 80:\n                success_count += 1\n        return success_count / test_episodes\n    def visualize_hierarchical_results(self):\n        if not self.results:\n            print(\"‚ùå No results to visualize. Run experiment first.\")\n            return\n        print(\"\\nüìä Hierarchical RL Results Analysis\")\n        print(\"=\" * 50)\n        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n        fig.suptitle('Hierarchical RL Performance Analysis', fontsize=16)\n        ax1 = axes[0, 0]\n        for agent_name, agent_results in self.results.items():\n            all_rewards = [result['rewards'] for result in agent_results]\n            min_length = min(len(rewards) for rewards in all_rewards)\n            rewards_array = np.array([rewards[:min_length] for rewards in all_rewards])\n            mean_rewards = np.mean(rewards_array, axis=0)\n            std_rewards = np.std(rewards_array, axis=0)\n            episodes = np.arange(min_length)\n            ax1.plot(episodes, mean_rewards, label=agent_name, linewidth=2)\n            ax1.fill_between(episodes, mean_rewards - std_rewards, mean_rewards + std_rewards, alpha=0.3)\n        ax1.set_xlabel('Episode')\n        ax1.set_ylabel('Average Reward')\n        ax1.set_title('Learning Curves')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax2 = axes[0, 1]\n        for agent_name, agent_results in self.results.items():\n            all_goals = [result['goal_achievements'] for result in agent_results]\n            min_length = min(len(goals) for goals in all_goals)\n            goals_array = np.array([goals[:min_length] for goals in all_goals])\n            mean_goals = np.mean(goals_array, axis=0)\n            std_goals = np.std(goals_array, axis=0)\n            episodes = np.arange(min_length)\n            ax2.plot(episodes, mean_goals, label=agent_name, linewidth=2)\n            ax2.fill_between(episodes, mean_goals - std_goals, mean_goals + std_goals, alpha=0.3)\n        ax2.set_xlabel('Episode')\n        ax2.set_ylabel('Goal Achievement Rate')\n        ax2.set_title('Goal Completion Progress')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        ax3 = axes[0, 2]\n        agent_names = list(self.results.keys())\n        skill_reuse_means = []\n        skill_reuse_stds = []\n        for agent_name, agent_results in self.results.items():\n            all_reuse = []\n            for result in agent_results:\n                if result['skill_reuse']:\n                    all_reuse.extend(result['skill_reuse'])\n            if all_reuse:\n                skill_reuse_means.append(np.mean(all_reuse))\n                skill_reuse_stds.append(np.std(all_reuse))\n            else:\n                skill_reuse_means.append(0)\n                skill_reuse_stds.append(0)\n        bars = ax3.bar(agent_names, skill_reuse_means, yerr=skill_reuse_stds, \n                      capsize=5, color=['lightblue', 'lightcoral'])\n        ax3.set_ylabel('Skill Transfer Success Rate')\n        ax3.set_title('Skill Reuse Capability')\n        ax3.tick_params(axis='x', rotation=45)\n        ax4 = axes[1, 0]\n        length_means = []\n        length_stds = []\n        for agent_name, agent_results in self.results.items():\n            all_lengths = []\n            for result in agent_results:\n                all_lengths.extend(result['lengths'][-50:])\n            length_means.append(np.mean(all_lengths))\n            length_stds.append(np.std(all_lengths))\n        bars = ax4.bar(agent_names, length_means, yerr=length_stds,\n                      capsize=5, color=['lightblue', 'lightcoral'])\n        ax4.set_ylabel('Average Episode Length')\n        ax4.set_title('Efficiency (Lower is Better)')\n        ax4.tick_params(axis='x', rotation=45)\n        ax5 = axes[1, 1]\n        final_rewards = []\n        final_stds = []\n        for agent_name, agent_results in self.results.items():\n            performances = [result['final_performance'] for result in agent_results]\n            final_rewards.append(np.mean(performances))\n            final_stds.append(np.std(performances))\n        bars = ax5.bar(agent_names, final_rewards, yerr=final_stds,\n                      capsize=5, color=['lightblue', 'lightcoral'])\n        ax5.set_ylabel('Final Average Reward')\n        ax5.set_title('Final Performance')\n        ax5.tick_params(axis='x', rotation=45)\n        ax6 = axes[1, 2]\n        final_goal_rates = []\n        goal_rate_stds = []\n        for agent_name, agent_results in self.results.items():\n            goal_rates = [result['final_goal_rate'] for result in agent_results]\n            final_goal_rates.append(np.mean(goal_rates))\n            goal_rate_stds.append(np.std(goal_rates))\n        bars = ax6.bar(agent_names, final_goal_rates, yerr=goal_rate_stds,\n                      capsize=5, color=['lightblue', 'lightcoral'])\n        ax6.set_ylabel('Final Goal Achievement Rate')\n        ax6.set_title('Multi-Goal Success Rate')\n        ax6.tick_params(axis='x', rotation=45)\n        plt.tight_layout()\n        plt.show()\n        print(\"\\nüìà Hierarchical RL Analysis Summary:\")\n        for agent_name, agent_results in self.results.items():\n            final_rewards = [result['final_performance'] for result in agent_results]\n            final_goals = [result['final_goal_rate'] for result in agent_results]\n            print(f\"\\n{agent_name}:\")\n            print(f\"  Final Reward: {np.mean(final_rewards):.2f} ¬± {np.std(final_rewards):.2f}\")\n            print(f\"  Goal Success Rate: {np.mean(final_goals):.3f} ¬± {np.std(final_goals):.3f}\")\n            print(f\"  Skill Transfer: {np.mean(skill_reuse_means):.3f}\")\nhierarchical_exp = HierarchicalRLExperiment()\nprint(\"üéØ Hierarchical RL Experiment Setup Complete!\")\nprint(\"üèÉ‚Äç‚ôÇÔ∏è Key features being tested:\")\nprint(\"  ‚Ä¢ Goal-conditioned learning with HER\")\nprint(\"  ‚Ä¢ Multi-goal navigation tasks\")\nprint(\"  ‚Ä¢ Skill transfer and reuse\")\nprint(\"  ‚Ä¢ Temporal abstraction benefits\")\nprint(\"\\nüí° To run experiment: hierarchical_exp.run_hierarchical_experiment(num_episodes=200, num_seeds=2)\")\nprint(\"üìä To visualize: hierarchical_exp.visualize_hierarchical_results()\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8b195",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlanningAlgorithmsExperiment:\n    def __init__(self):\n        self.results = {}\n    def run_planning_comparison(self, num_episodes=200, num_seeds=2):\n        print(\"üéØ Running Planning Algorithms Comparison...\")\n        print(\"‚ö° Testing: MCTS vs Model-Based Value Expansion vs Random Shooting\")\n        env_size = 6\n        state_dim = env_size * env_size\n        action_dim = 4\n        results = {}\n        planning_configs = {\n            'Random Shooting': {\n                'use_mcts': False,\n                'use_mve': False,\n                'use_random': True\n            },\n            'Model-Based Value Expansion': {\n                'use_mcts': False,\n                'use_mve': True,\n                'use_random': False\n            },\n            'MCTS Planning': {\n                'use_mcts': True,\n                'use_mve': False,\n                'use_random': False\n            }\n        }\n        for planner_name, config in planning_configs.items():\n            print(f\"\\nüîÑ Testing {planner_name}...\")\n            planner_results = []\n            for seed in range(num_seeds):\n                print(f\"  Seed {seed + 1}/{num_seeds}\")\n                np.random.seed(seed)\n                torch.manual_seed(seed)\n                random.seed(seed)\n                env = SimpleGridWorld(size=env_size)\n                base_agent = DynaQAgent(state_dim, action_dim)\n                model_ensemble = ModelEnsemble(state_dim, action_dim, ensemble_size=3)\n                if config['use_mcts']:\n                    value_net = nn.Sequential(\n                        nn.Linear(state_dim, 128),\n                        nn.ReLU(),\n                        nn.Linear(128, 1)\n                    ).to(device)\n                    mcts_planner = MonteCarloTreeSearch(model_ensemble, value_net)\n                    planner = mcts_planner\n                elif config['use_mve']:\n                    value_net = base_agent.q_network\n                    mve_planner = ModelBasedValueExpansion(model_ensemble, value_net)\n                    planner = mve_planner\n                else:\n                    mpc_planner = ModelPredictiveController(model_ensemble, action_dim)\n                    planner = mpc_planner\n                episode_rewards = []\n                planning_times = []\n                model_accuracy = []\n                for episode in range(num_episodes):\n                    state = env.reset()\n                    episode_reward = 0\n                    episode_length = 0\n                    done = False\n                    while not done and episode_length < 100:\n                        start_time = time.time()\n                        if episode > 50:\n                            try:\n                                if config['use_mcts']:\n                                    root = planner.search(state, num_simulations=20)\n                                    action_probs = planner.get_action_probabilities(root)\n                                    action = np.argmax(action_probs)\n                                elif config['use_mve']:\n                                    action = planner.plan_action(state)\n                                else:\n                                    action = planner.plan_action(state)\n                            except:\n                                action = base_agent.get_action(state, epsilon=0.1)\n                        else:\n                            action = base_agent.get_action(state, epsilon=0.3)\n                        planning_time = time.time() - start_time\n                        planning_times.append(planning_time)\n                        next_state, reward, done, info = env.step(action)\n                        episode_reward += reward\n                        episode_length += 1\n                        base_agent.store_experience(state, action, reward, next_state, done)\n                        base_agent.update_q_function()\n                        if episode_length % 5 == 0:\n                            model_loss = base_agent.update_model()\n                            if episode_length % 20 == 0:\n                                accuracy = self._test_model_accuracy(model_ensemble, env)\n                                model_accuracy.append(accuracy)\n                        state = next_state\n                    episode_rewards.append(episode_reward)\n                    if (episode + 1) % 50 == 0:\n                        avg_reward = np.mean(episode_rewards[-20:])\n                        avg_time = np.mean(planning_times[-100:]) if planning_times else 0\n                        print(f\"    Episode {episode + 1}: Reward={avg_reward:.2f}, Planning Time={avg_time:.4f}s\")\n                planner_results.append({\n                    'rewards': episode_rewards,\n                    'planning_times': planning_times,\n                    'model_accuracy': model_accuracy,\n                    'final_performance': np.mean(episode_rewards[-20:])\n                })\n            results[planner_name] = planner_results\n        self.results = results\n        return results\n    def _test_model_accuracy(self, model_ensemble, env, num_tests=10):\n        if len(model_ensemble.models) == 0:\n            return 0.0\n        accuracies = []\n        for _ in range(num_tests):\n            state = env.reset()\n            action = np.random.randint(4)\n            actual_next_state, actual_reward, _, _ = env.step(action)\n            try:\n                pred_next_state, pred_reward = model_ensemble.predict_mean(\n                    torch.FloatTensor(state).to(device),\n                    torch.LongTensor([action]).to(device)\n                )\n                state_error = torch.norm(pred_next_state.cpu() - torch.FloatTensor(actual_next_state)).item()\n                reward_error = abs(pred_reward.cpu().item() - actual_reward)\n                accuracy = 1.0 / (1.0 + state_error + reward_error)\n                accuracies.append(accuracy)\n            except:\n                accuracies.append(0.0)\n        return np.mean(accuracies) if accuracies else 0.0\n    def visualize_planning_results(self):\n        if not self.results:\n            print(\"‚ùå No results to visualize. Run experiment first.\")\n            return\n        print(\"\\nüìä Planning Algorithms Comparison Results\")\n        print(\"=\" * 50)\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        fig.suptitle('Planning Algorithms Performance Analysis', fontsize=16)\n        ax1 = axes[0, 0]\n        colors = ['blue', 'red', 'green']\n        for i, (planner_name, planner_results) in enumerate(self.results.items()):\n            all_rewards = [result['rewards'] for result in planner_results]\n            min_length = min(len(rewards) for rewards in all_rewards)\n            rewards_array = np.array([rewards[:min_length] for rewards in all_rewards])\n            mean_rewards = np.mean(rewards_array, axis=0)\n            std_rewards = np.std(rewards_array, axis=0)\n            episodes = np.arange(min_length)\n            ax1.plot(episodes, mean_rewards, label=planner_name, linewidth=2, color=colors[i])\n            ax1.fill_between(episodes, mean_rewards - std_rewards, mean_rewards + std_rewards, \n                           alpha=0.3, color=colors[i])\n        ax1.set_xlabel('Episode')\n        ax1.set_ylabel('Average Reward')\n        ax1.set_title('Learning Curves Comparison')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax2 = axes[0, 1]\n        planner_names = list(self.results.keys())\n        planning_times = []\n        time_stds = []\n        for planner_name, planner_results in self.results.items():\n            all_times = []\n            for result in planner_results:\n                if result['planning_times']:\n                    relevant_times = result['planning_times'][len(result['planning_times'])//2:]\n                    all_times.extend(relevant_times)\n            if all_times:\n                planning_times.append(np.mean(all_times) * 1000)\n                time_stds.append(np.std(all_times) * 1000)\n            else:\n                planning_times.append(0)\n                time_stds.append(0)\n        bars = ax2.bar(planner_names, planning_times, yerr=time_stds, capsize=5, \n                      color=['lightblue', 'lightcoral', 'lightgreen'])\n        ax2.set_ylabel('Average Planning Time (ms)')\n        ax2.set_title('Computational Overhead')\n        ax2.tick_params(axis='x', rotation=45)\n        ax3 = axes[1, 0]\n        final_performances = []\n        perf_stds = []\n        for planner_name, planner_results in self.results.items():\n            performances = [result['final_performance'] for result in planner_results]\n            final_performances.append(np.mean(performances))\n            perf_stds.append(np.std(performances))\n        bars = ax3.bar(planner_names, final_performances, yerr=perf_stds, capsize=5,\n                      color=['lightblue', 'lightcoral', 'lightgreen'])\n        ax3.set_ylabel('Final Average Reward')\n        ax3.set_title('Final Performance')\n        ax3.tick_params(axis='x', rotation=45)\n        ax4 = axes[1, 1]\n        for planner_name, planner_results in self.results.items():\n            all_accuracies = []\n            for result in planner_results:\n                if result['model_accuracy']:\n                    all_accuracies.append(result['model_accuracy'])\n            if all_accuracies:\n                min_length = min(len(acc) for acc in all_accuracies) if all_accuracies else 0\n                if min_length > 0:\n                    acc_array = np.array([acc[:min_length] for acc in all_accuracies])\n                    mean_acc = np.mean(acc_array, axis=0)\n                    time_steps = np.arange(len(mean_acc))\n                    ax4.plot(time_steps, mean_acc, label=planner_name, linewidth=2)\n        ax4.set_xlabel('Model Update Steps')\n        ax4.set_ylabel('Model Accuracy')\n        ax4.set_title('Model Learning Progress')\n        ax4.legend()\n        ax4.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        print(\"\\nüìà Planning Algorithms Summary:\")\n        for planner_name, planner_results in self.results.items():\n            performances = [result['final_performance'] for result in planner_results]\n            times = []\n            for result in planner_results:\n                if result['planning_times']:\n                    times.extend(result['planning_times'])\n            mean_perf = np.mean(performances)\n            std_perf = np.std(performances)\n            mean_time = np.mean(times) * 1000 if times else 0\n            print(f\"\\n{planner_name}:\")\n            print(f\"  Final Performance: {mean_perf:.2f} ¬± {std_perf:.2f}\")\n            print(f\"  Average Planning Time: {mean_time:.2f} ms\")\n            print(f\"  Performance/Time Ratio: {mean_perf/max(mean_time/1000, 0.001):.1f}\")\nprint(\"\\n\" + \"=\"*80)\nprint(\"üéâ COMPREHENSIVE CA15 IMPLEMENTATION COMPLETED!\")\nprint(\"=\"*80)\nprint(\"\"\"\nüìö THEORETICAL COVERAGE:\n‚îú‚îÄ‚îÄ Model-Based Reinforcement Learning\n‚îÇ   ‚îú‚îÄ‚îÄ Environment dynamics learning\n‚îÇ   ‚îú‚îÄ‚îÄ Model-Predictive Control (MPC)\n‚îÇ   ‚îú‚îÄ‚îÄ Dyna-Q algorithm\n‚îÇ   ‚îî‚îÄ‚îÄ Uncertainty quantification with ensembles\n‚îÇ\n‚îú‚îÄ‚îÄ Hierarchical Reinforcement Learning  \n‚îÇ   ‚îú‚îÄ‚îÄ Options framework\n‚îÇ   ‚îú‚îÄ‚îÄ Goal-conditioned RL with HER\n‚îÇ   ‚îú‚îÄ‚îÄ Hierarchical Actor-Critic (HAC)\n‚îÇ   ‚îî‚îÄ‚îÄ Feudal Networks architecture\n‚îÇ\n‚îî‚îÄ‚îÄ Advanced Planning and Control\n    ‚îú‚îÄ‚îÄ Monte Carlo Tree Search (MCTS)\n    ‚îú‚îÄ‚îÄ Model-Based Value Expansion (MVE)\n    ‚îú‚îÄ‚îÄ Latent space planning\n    ‚îî‚îÄ‚îÄ World models (PlaNet-inspired)\nüîß IMPLEMENTATION HIGHLIGHTS:\n‚îú‚îÄ‚îÄ Complete neural network architectures\n‚îú‚îÄ‚îÄ End-to-end training algorithms  \n‚îú‚îÄ‚îÄ Uncertainty estimation methods\n‚îú‚îÄ‚îÄ Hierarchical policy structures\n‚îú‚îÄ‚îÄ Advanced planning algorithms\n‚îî‚îÄ‚îÄ Comprehensive evaluation frameworks\nüß™ EXPERIMENTAL VALIDATION:\n‚îú‚îÄ‚îÄ Model-based vs model-free comparison\n‚îú‚îÄ‚îÄ Hierarchical RL benefits demonstration\n‚îú‚îÄ‚îÄ Planning algorithms effectiveness\n‚îî‚îÄ‚îÄ Integration and real-world applicability\nüìä KEY LEARNING OUTCOMES:\n‚úÖ Understanding of advanced RL paradigms\n‚úÖ Practical implementation experience\n‚úÖ Performance analysis and comparison\n‚úÖ Real-world application insights\n‚úÖ State-of-the-art method integration\nüöÄ READY FOR EXECUTION:\n‚Ä¢ All components are fully implemented\n‚Ä¢ Experiments are ready to run\n‚Ä¢ Comprehensive analysis tools provided\n‚Ä¢ Educational content with theory and practice\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}