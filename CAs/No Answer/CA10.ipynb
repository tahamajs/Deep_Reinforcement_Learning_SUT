{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ec826b0",
   "metadata": {},
   "source": [
    "# CA10: Model-Based Reinforcement Learning and Planning Methods\n",
    "\n",
    "## Deep Reinforcement Learning - Session 10\n",
    "\n",
    "**Comprehensive Coverage of Model-Based Reinforcement Learning**\n",
    "\n",
    "This notebook provides a complete exploration of model-based reinforcement learning, covering theoretical foundations, planning algorithms, and practical implementations of various model-based approaches including Dyna-Q, Monte Carlo Tree Search (MCTS), Model Predictive Control (MPC), and modern neural approaches.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand the theoretical foundations of model-based reinforcement learning\n",
    "2. Implement classical planning algorithms: Dynamic Programming, Value Iteration\n",
    "3. Explore integrated planning and learning: Dyna-Q algorithm\n",
    "4. Master Monte Carlo Tree Search (MCTS) and its applications\n",
    "5. Implement Model Predictive Control (MPC) for continuous control\n",
    "6. Understand modern neural model-based approaches\n",
    "7. Compare model-based vs model-free methods\n",
    "8. Apply model-based methods to complex environments\n",
    "\n",
    "### Notebook Structure:\n",
    "1. **Theoretical Foundations** - Model-based RL theory and framework\n",
    "2. **Environment Models** - Learning and representing environment dynamics\n",
    "3. **Classical Planning** - Dynamic Programming and Value Iteration with learned models\n",
    "4. **Dyna-Q Algorithm** - Integrating planning and learning\n",
    "5. **Monte Carlo Tree Search** - MCTS algorithm and applications\n",
    "6. **Model Predictive Control** - MPC for continuous control problems\n",
    "7. **Modern Neural Methods** - World models and neural planning\n",
    "8. **Comparative Analysis** - Model-based vs model-free comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f1d43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical, Normal\nimport gymnasium as gym\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict, deque\nimport random\nimport pickle\nfrom typing import Tuple, List, Dict, Optional, Union\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\ntorch.manual_seed(42)\nrandom.seed(42)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nplt.style.use('default')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\nprint(\"Environment setup complete!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Gymnasium version: {gym.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d996f71",
   "metadata": {},
   "source": [
    "# Section 1: Theoretical Foundations of Model-Based Reinforcement Learning\n",
    "\n",
    "## 1.1 From Model-Free to Model-Based Learning\n",
    "\n",
    "In our journey through reinforcement learning, we have primarily focused on **model-free methods** such as Q-learning, SARSA, and policy gradient methods. These methods learn directly from experience without explicitly modeling the environment. However, there are fundamental advantages to learning and using environment models:\n",
    "\n",
    "### Model-Free vs Model-Based Comparison\n",
    "\n",
    "| Aspect | Model-Free Methods | Model-Based Methods |\n",
    "|--------|-------------------|--------------------|\n",
    "| **Learning** | Learn value functions or policies directly | Learn environment model first |\n",
    "| **Sample Efficiency** | Generally less sample efficient | Generally more sample efficient |\n",
    "| **Computational Cost** | Lower per-step computation | Higher per-step computation |\n",
    "| **Planning** | No explicit planning | Can plan with learned model |\n",
    "| **Robustness** | More robust to model errors | Sensitive to model inaccuracies |\n",
    "| **Interpretability** | Less interpretable | More interpretable (explicit model) |\n",
    "\n",
    "## 1.2 The Model-Based RL Framework\n",
    "\n",
    "The general model-based RL framework consists of three main components:\n",
    "\n",
    "1. **Model Learning**: Learn a model of the environment from experience\n",
    "   $$\\hat{P}(s'|s,a) \\approx P(s'|s,a)$$\n",
    "   $$\\hat{R}(s,a) \\approx E[R|s,a]$$\n",
    "\n",
    "2. **Planning**: Use the learned model to compute optimal policies\n",
    "   - Value iteration with learned model\n",
    "   - Policy iteration with learned model  \n",
    "   - Monte Carlo Tree Search\n",
    "   - Model Predictive Control\n",
    "\n",
    "3. **Acting**: Execute actions in the real environment\n",
    "   - Collect new experience\n",
    "   - Update the model\n",
    "   - Replan with improved model\n",
    "\n",
    "## 1.3 Advantages of Model-Based Methods\n",
    "\n",
    "**Sample Efficiency**: \n",
    "- Can generate synthetic experience using the learned model\n",
    "- Each real experience can be used multiple times for planning\n",
    "- Particularly important in expensive real-world applications\n",
    "\n",
    "**Transfer Learning**:\n",
    "- Models can transfer across different tasks in the same environment\n",
    "- Learned dynamics are often more general than policies\n",
    "\n",
    "**Interpretability and Safety**:\n",
    "- Explicit models provide insight into system behavior\n",
    "- Can simulate outcomes before taking actions\n",
    "- Enable safety verification and constraint checking\n",
    "\n",
    "**Planning Capabilities**:\n",
    "- Can look ahead and plan optimal sequences of actions\n",
    "- Adapt quickly to changes in rewards or goals\n",
    "- Enable hierarchical and long-term planning\n",
    "\n",
    "## 1.4 Challenges in Model-Based RL\n",
    "\n",
    "**Model Bias and Compounding Errors**:\n",
    "- Errors in the learned model can compound over time\n",
    "- Model bias can lead to suboptimal policies\n",
    "- Challenge: Learning accurate models in complex environments\n",
    "\n",
    "**Computational Complexity**:\n",
    "- Planning with models can be computationally expensive\n",
    "- Trade-off between planning depth and computational cost\n",
    "\n",
    "**Partial Observability**:\n",
    "- Real environments often have hidden state\n",
    "- Challenge: Learning models from partial observations\n",
    "\n",
    "**Stochastic Environments**:\n",
    "- Need to model uncertainty in transitions and rewards\n",
    "- Balance between model complexity and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bd0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedFrameworkVisualizer:\n    def visualize_framework_comparison(self):\n        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n        ax = axes[0]\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 10)\n        ax.set_title('Model-Free RL Framework', fontsize=16, fontweight='bold')\n        env_box = plt.Rectangle((1, 7), 8, 2, fill=True, facecolor='lightblue', edgecolor='black')\n        ax.add_patch(env_box)\n        ax.text(5, 8, 'Environment\\n(Unknown Dynamics)', ha='center', va='center', fontweight='bold')\n        agent_box = plt.Rectangle((3, 4), 4, 2, fill=True, facecolor='lightgreen', edgecolor='black')\n        ax.add_patch(agent_box)\n        ax.text(5, 5, 'Agent\\n(Policy/Value Function)', ha='center', va='center', fontweight='bold')\n        experience_box = plt.Rectangle((1, 1), 8, 1.5, fill=True, facecolor='lightyellow', edgecolor='black')\n        ax.add_patch(experience_box)\n        ax.text(5, 1.75, 'Direct Learning from Experience\\n(s, a, r, s\\')', ha='center', va='center')\n        ax.annotate('', xy=(5, 6.8), xytext=(5, 6.2), arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n        ax.text(5.5, 6.5, 'Actions', ha='left', va='center', color='red')\n        ax.annotate('', xy=(5, 3.8), xytext=(5, 4.2), arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n        ax.text(5.5, 3.5, 'Experience', ha='left', va='center', color='blue')\n        ax.annotate('', xy=(5, 2.8), xytext=(5, 3.5), arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n        ax.text(2, 3, 'Learning', ha='center', va='center', color='green')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        ax = axes[1]\n        ax.set_xlim(0, 10)\n        ax.set_ylim(0, 10)\n        ax.set_title('Model-Based RL Framework', fontsize=16, fontweight='bold')\n        env_box = plt.Rectangle((1, 8), 8, 1.5, fill=True, facecolor='lightblue', edgecolor='black')\n        ax.add_patch(env_box)\n        ax.text(5, 8.75, 'Real Environment', ha='center', va='center', fontweight='bold')\n        model_box = plt.Rectangle((1, 6), 3.5, 1.5, fill=True, facecolor='lightcoral', edgecolor='black')\n        ax.add_patch(model_box)\n        ax.text(2.75, 6.75, 'Learned\\nModel', ha='center', va='center', fontweight='bold')\n        planner_box = plt.Rectangle((5.5, 6), 3.5, 1.5, fill=True, facecolor='lightsalmon', edgecolor='black')\n        ax.add_patch(planner_box)\n        ax.text(7.25, 6.75, 'Planner', ha='center', va='center', fontweight='bold')\n        policy_box = plt.Rectangle((3, 3.5), 4, 1.5, fill=True, facecolor='lightgreen', edgecolor='black')\n        ax.add_patch(policy_box)\n        ax.text(5, 4.25, 'Policy', ha='center', va='center', fontweight='bold')\n        experience_box = plt.Rectangle((1, 1), 8, 1.5, fill=True, facecolor='lightyellow', edgecolor='black')\n        ax.add_patch(experience_box)\n        ax.text(5, 1.75, 'Experience Buffer\\n(s, a, r, s\\')', ha='center', va='center')\n        ax.annotate('', xy=(3, 7.8), xytext=(3, 7.2), arrowprops=dict(arrowstyle='->', lw=2, color='blue'))\n        ax.text(2, 7.5, 'Experience', ha='center', va='center', color='blue', rotation=90)\n        ax.annotate('', xy=(6, 6.8), xytext=(4, 6.8), arrowprops=dict(arrowstyle='->', lw=2, color='red'))\n        ax.text(5, 7.2, 'Model', ha='center', va='center', color='red')\n        ax.annotate('', xy=(6, 5.8), xytext=(6, 5.2), arrowprops=dict(arrowstyle='->', lw=2, color='green'))\n        ax.text(6.5, 5.5, 'Policy', ha='left', va='center', color='green', rotation=-90)\n        ax.annotate('', xy=(5, 7.8), xytext=(5, 5.2), arrowprops=dict(arrowstyle='->', lw=2, color='purple'))\n        ax.text(5.5, 6.5, 'Actions', ha='left', va='center', color='purple')\n        ax.annotate('', xy=(5, 2.8), xytext=(5, 3.2), arrowprops=dict(arrowstyle='->', lw=2, color='orange'))\n        ax.text(6, 3, 'Store', ha='center', va='center', color='orange')\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.spines['top'].set_visible(False)\n        ax.spines['right'].set_visible(False)\n        ax.spines['bottom'].set_visible(False)\n        ax.spines['left'].set_visible(False)\n        plt.tight_layout()\n        plt.show()\n    def visualize_sample_efficiency(self):\n        episodes = np.arange(1, 201)\n        np.random.seed(42)\n        model_free = 100 * (1 - np.exp(-episodes/50)) + np.random.normal(0, 5, len(episodes))\n        model_free = np.maximum(0, model_free)\n        model_based = 120 * (1 - np.exp(-episodes/20)) + np.random.normal(0, 3, len(episodes))\n        model_based = np.maximum(0, model_based)\n        model_based[100:] *= 0.95\n        plt.figure(figsize=(12, 8))\n        plt.plot(episodes, model_free, label='Model-Free RL', linewidth=2, color='blue', alpha=0.8)\n        plt.plot(episodes, model_based, label='Model-Based RL', linewidth=2, color='red', alpha=0.8)\n        window = 10\n        model_free_smooth = pd.Series(model_free).rolling(window).mean()\n        model_based_smooth = pd.Series(model_based).rolling(window).mean()\n        plt.plot(episodes, model_free_smooth, linewidth=3, color='darkblue', alpha=0.9)\n        plt.plot(episodes, model_based_smooth, linewidth=3, color='darkred', alpha=0.9)\n        plt.axvline(x=50, color='gray', linestyle='--', alpha=0.7)\n        plt.text(52, 80, 'Model-Based\\nAdvantage\\nRegion', fontsize=12, color='darkred')\n        plt.axvline(x=150, color='gray', linestyle='--', alpha=0.7)\n        plt.text(152, 80, 'Model-Free\\nCatches Up', fontsize=12, color='darkblue')\n        plt.xlabel('Training Episodes', fontsize=14)\n        plt.ylabel('Average Return', fontsize=14)\n        plt.title('Sample Efficiency: Model-Based vs Model-Free RL', fontsize=16, fontweight='bold')\n        plt.legend(fontsize=12)\n        plt.grid(True, alpha=0.3)\n        plt.annotate('Faster initial learning\\n(better sample efficiency)', \n                    xy=(30, model_based_smooth.iloc[30]), xytext=(70, 40),\n                    arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n                    fontsize=10, color='darkred')\n        plt.annotate('May plateau due to model bias', \n                    xy=(120, model_based_smooth.iloc[120]), xytext=(140, 60),\n                    arrowprops=dict(arrowstyle='->', color='red', lw=1.5),\n                    fontsize=10, color='darkred')\n        plt.tight_layout()\n        plt.show()\n    def create_summary_table(self):\n        comparison_data = {\n            'Characteristic': [\n                'Sample Efficiency',\n                'Computational Cost',\n                'Robustness to Model Errors',\n                'Planning Capability',\n                'Transfer Learning',\n                'Interpretability',\n                'Real-time Performance',\n                'Asymptotic Performance'\n            ],\n            'Model-Free': [\n                'Lower (more samples needed)',\n                'Lower (per interaction)',\n                'High (no model dependence)',\n                'None (reactive only)',\n                'Limited (policy/value specific)',\n                'Low (black box)',\n                'Fast (direct action)',\n                'High (no model bias)'\n            ],\n            'Model-Based': [\n                'Higher (reuse experience)',\n                'Higher (planning overhead)',\n                'Low (sensitive to model errors)',\n                'Excellent (explicit planning)',\n                'Excellent (model transfers)',\n                'High (interpretable model)',\n                'Variable (depends on planner)',\n                'Variable (depends on model accuracy)'\n            ]\n        }\n        df = pd.DataFrame(comparison_data)\n        fig, ax = plt.subplots(figsize=(16, 10))\n        ax.axis('tight')\n        ax.axis('off')\n        table = ax.table(cellText=df.values,\n                        colLabels=df.columns,\n                        cellLoc='left',\n                        loc='center',\n                        colWidths=[0.25, 0.375, 0.375])\n        table.auto_set_font_size(False)\n        table.set_fontsize(11)\n        table.scale(1, 2.5)\n        for i in range(len(df.columns)):\n            table[(0, i)].set_facecolor('#4CAF50')\n            table[(0, i)].set_text_props(weight='bold', color='white')\n        for i in range(1, len(df) + 1):\n            table[(i, 0)].set_facecolor('#E8F5E8')\n            table[(i, 0)].set_text_props(weight='bold')\n            if i % 2 == 0:\n                table[(i, 1)].set_facecolor('#F0F8FF')\n                table[(i, 2)].set_facecolor('#FFF8DC')\n            else:\n                table[(i, 1)].set_facecolor('#F8F8FF')\n                table[(i, 2)].set_facecolor('#FFFACD')\n        plt.title('Comprehensive Comparison: Model-Free vs Model-Based RL', \n                 fontsize=16, fontweight='bold', pad=20)\n        plt.show()\n        return df\nvisualizer = ModelBasedFrameworkVisualizer()\nprint(\"Model-Based RL Framework Visualizations\")\nprint(\"=\"*50)\nprint(\"\\n1. Framework Comparison:\")\nvisualizer.visualize_framework_comparison()\nprint(\"\\n2. Sample Efficiency Analysis:\")\nvisualizer.visualize_sample_efficiency()\nprint(\"\\n3. Comprehensive Comparison Table:\")\ncomparison_df = visualizer.create_summary_table()\nprint(\"\\n‚úÖ Theoretical foundations established!\")\nprint(\"üìä Next: Environment model learning and representation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8632a623",
   "metadata": {},
   "source": [
    "# Section 2: Environment Models and Model Learning\n",
    "\n",
    "## 2.1 Types of Environment Models\n",
    "\n",
    "Environment models can be categorized along several dimensions:\n",
    "\n",
    "### By Representation Type:\n",
    "\n",
    "**Tabular Models**:\n",
    "- Store explicit transition probabilities: $P(s'|s,a)$\n",
    "- Store explicit rewards: $R(s,a)$\n",
    "- Suitable for small, discrete state-action spaces\n",
    "- Example: Storing counts and computing maximum likelihood estimates\n",
    "\n",
    "**Function Approximation Models**:\n",
    "- Use neural networks to approximate dynamics\n",
    "- $s' = f_\\theta(s,a) + \\epsilon$ (deterministic + noise)\n",
    "- $P(s'|s,a) = \\pi_\\theta(s'|s,a)$ (stochastic)\n",
    "- Suitable for large, continuous state-action spaces\n",
    "\n",
    "### By Uncertainty Representation:\n",
    "\n",
    "**Deterministic Models**:\n",
    "- Predict single next state: $s' = f(s,a)$\n",
    "- Simple but ignores environment stochasticity\n",
    "- Can add noise independently\n",
    "\n",
    "**Stochastic Models**:\n",
    "- Predict distribution over next states: $P(s'|s,a)$\n",
    "- More accurate for stochastic environments\n",
    "- Can be parametric (Gaussian) or non-parametric\n",
    "\n",
    "**Ensemble Models**:\n",
    "- Multiple models trained on different data subsets\n",
    "- Uncertainty estimated from ensemble disagreement\n",
    "- More robust and better uncertainty quantification\n",
    "\n",
    "## 2.2 Model Learning Approaches\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "For tabular environments, we can use simple counting:\n",
    "$$\\hat{P}(s'|s,a) = \\frac{N(s,a,s')}{N(s,a)}$$\n",
    "$$\\hat{R}(s,a) = \\frac{1}{N(s,a)} \\sum_{i} R_i(s,a)$$\n",
    "\n",
    "### Neural Network Models\n",
    "\n",
    "For complex environments, use neural networks:\n",
    "- **Forward Model**: $(s,a) \\rightarrow (s', r)$\n",
    "- **Inverse Model**: $(s,s') \\rightarrow a$\n",
    "- **Combined**: Learn both forward and inverse models jointly\n",
    "\n",
    "### Training Objectives\n",
    "\n",
    "**Deterministic Dynamics**:\n",
    "$$L = \\mathbb{E}_{(s,a,s',r) \\sim D}[||s' - f_\\theta(s,a)||^2 + ||r - g_\\theta(s,a)||^2]$$\n",
    "\n",
    "**Stochastic Dynamics**:\n",
    "$$L = -\\mathbb{E}_{(s,a,s',r) \\sim D}[\\log P_\\theta(s'|s,a) + \\log P_\\theta(r|s,a)]$$\n",
    "\n",
    "## 2.3 Model Validation and Selection\n",
    "\n",
    "### Validation Strategies\n",
    "\n",
    "**Hold-out Validation**:\n",
    "- Split data into training and validation sets\n",
    "- Evaluate model accuracy on unseen transitions\n",
    "- Risk: May not reflect planning performance\n",
    "\n",
    "**Cross-Validation**:\n",
    "- Multiple train/validation splits\n",
    "- More robust estimate of model quality\n",
    "- Higher computational cost\n",
    "\n",
    "**Policy-Aware Validation**:\n",
    "- Evaluate model on states visited by current policy\n",
    "- More relevant for planning performance\n",
    "- Adapts as policy changes\n",
    "\n",
    "### Model Selection Criteria\n",
    "\n",
    "**Prediction Accuracy**:\n",
    "- Mean squared error for continuous states\n",
    "- Cross-entropy for discrete states\n",
    "- May not correlate with planning performance\n",
    "\n",
    "**Planning Performance**:\n",
    "- Evaluate policies learned with the model\n",
    "- More relevant but computationally expensive\n",
    "- Gold standard when feasible\n",
    "\n",
    "**Uncertainty Calibration**:\n",
    "- Ensure predicted uncertainty matches actual errors\n",
    "- Important for robust planning\n",
    "- Use reliability diagrams and calibration error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f487bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel:\n    def __init__(self, num_states, num_actions):\n        self.num_states = num_states\n        self.num_actions = num_actions\n        self.transition_counts = np.zeros((num_states, num_actions, num_states))\n        self.sa_counts = np.zeros((num_states, num_actions))\n        self.reward_sums = np.zeros((num_states, num_actions))\n        self.reward_counts = np.zeros((num_states, num_actions))\n    def update(self, state, action, next_state, reward):\n        self.transition_counts[state, action, next_state] += 1\n        self.sa_counts[state, action] += 1\n        self.reward_sums[state, action] += reward\n        self.reward_counts[state, action] += 1\n    def get_transition_prob(self, state, action, next_state):\n        if self.sa_counts[state, action] == 0:\n            return 1.0 / self.num_states\n        return self.transition_counts[state, action, next_state] / self.sa_counts[state, action]\n    def get_expected_reward(self, state, action):\n        if self.reward_counts[state, action] == 0:\n            return 0.0\n        return self.reward_sums[state, action] / self.reward_counts[state, action]\n    def sample_transition(self, state, action):\n        if self.sa_counts[state, action] == 0:\n            next_state = np.random.randint(self.num_states)\n        else:\n            probs = self.transition_counts[state, action] / self.sa_counts[state, action]\n            next_state = np.random.choice(self.num_states, p=probs)\n        reward = self.get_expected_reward(state, action)\n        return next_state, reward\n    def get_transition_matrix(self, action):\n        P = np.zeros((self.num_states, self.num_states))\n        for s in range(self.num_states):\n            if self.sa_counts[s, action] == 0:\n                P[s, :] = 1.0 / self.num_states\n            else:\n                P[s, :] = self.transition_counts[s, action, :] / self.sa_counts[s, action]\n        return P\n    def get_reward_vector(self, action):\n        R = np.zeros(self.num_states)\n        for s in range(self.num_states):\n            R[s] = self.get_expected_reward(s, action)\n        return R\nclass NeuralModel(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256, ensemble_size=1):\n        super(NeuralModel, self).__init__()\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.ensemble_size = ensemble_size\n        self.models = nn.ModuleList()\n        for _ in range(ensemble_size):\n            model = nn.Sequential(\n                nn.Linear(state_dim + action_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, hidden_dim),\n                nn.ReLU(),\n                nn.Linear(hidden_dim, state_dim + 1)\n            )\n            self.models.append(model)\n    def forward(self, state, action, model_idx=None):\n        if len(state.shape) == 1:\n            state = state.unsqueeze(0)\n        if len(action.shape) == 1:\n            action = action.unsqueeze(0)\n        if action.dtype == torch.long:\n            action_one_hot = torch.zeros(action.size(0), self.action_dim).to(action.device)\n            action_one_hot.scatter_(1, action.unsqueeze(1), 1)\n            action = action_one_hot\n        x = torch.cat([state, action], dim=1)\n        if model_idx is not None:\n            output = self.models[model_idx](x)\n        else:\n            outputs = torch.stack([model(x) for model in self.models])\n            output = outputs.mean(dim=0)\n        next_state = output[:, :self.state_dim]\n        reward = output[:, self.state_dim]\n        return next_state, reward\n    def predict_with_uncertainty(self, state, action):\n        outputs = []\n        for i in range(self.ensemble_size):\n            next_state, reward = self.forward(state, action, model_idx=i)\n            outputs.append(torch.cat([next_state, reward.unsqueeze(1)], dim=1))\n        outputs = torch.stack(outputs)\n        mean = outputs.mean(dim=0)\n        uncertainty = outputs.std(dim=0)\n        next_state_mean = mean[:, :self.state_dim]\n        reward_mean = mean[:, self.state_dim]\n        next_state_std = uncertainty[:, :self.state_dim]\n        reward_std = uncertainty[:, self.state_dim]\n        return next_state_mean, reward_mean, next_state_std, reward_std\n    def sample_from_model(self, state, action):\n        model_idx = np.random.randint(self.ensemble_size)\n        return self.forward(state, action, model_idx=model_idx)\nclass ModelTrainer:\n    def __init__(self, model, lr=1e-3):\n        self.model = model\n        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n        self.loss_history = []\n    def train_step(self, states, actions, next_states, rewards):\n        self.optimizer.zero_grad()\n        states = torch.FloatTensor(states).to(device)\n        actions = torch.LongTensor(actions).to(device) if len(actions.shape) == 1 else torch.FloatTensor(actions).to(device)\n        next_states = torch.FloatTensor(next_states).to(device)\n        rewards = torch.FloatTensor(rewards).to(device)\n        total_loss = 0\n        for i in range(self.model.ensemble_size):\n            pred_next_states, pred_rewards = self.model.forward(states, actions, model_idx=i)\n            state_loss = F.mse_loss(pred_next_states, next_states)\n            reward_loss = F.mse_loss(pred_rewards, rewards)\n            loss = state_loss + reward_loss\n            total_loss += loss\n        total_loss.backward()\n        self.optimizer.step()\n        return total_loss.item()\n    def train_batch(self, data, epochs=10, batch_size=32):\n        states, actions, next_states, rewards = data\n        n_samples = len(states)\n        for epoch in range(epochs):\n            epoch_loss = 0\n            n_batches = 0\n            indices = np.random.permutation(n_samples)\n            for i in range(0, n_samples, batch_size):\n                batch_indices = indices[i:i+batch_size]\n                batch_states = states[batch_indices]\n                batch_actions = actions[batch_indices]\n                batch_next_states = next_states[batch_indices]\n                batch_rewards = rewards[batch_indices]\n                loss = self.train_step(batch_states, batch_actions, batch_next_states, batch_rewards)\n                epoch_loss += loss\n                n_batches += 1\n            avg_loss = epoch_loss / n_batches\n            self.loss_history.append(avg_loss)\n            if (epoch + 1) % 5 == 0:\n                print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\nclass SimpleGridWorld:\n    def __init__(self, size=5):\n        self.size = size\n        self.num_states = size * size\n        self.num_actions = 4\n        self.reset()\n    def reset(self):\n        self.state = 0\n        return self.state\n    def step(self, action):\n        x, y = self.state % self.size, self.state // self.size\n        if action == 0 and y > 0:\n            y -= 1\n        elif action == 1 and y < self.size - 1:\n            y += 1\n        elif action == 2 and x > 0:\n            x -= 1\n        elif action == 3 and x < self.size - 1:\n            x += 1\n        self.state = y * self.size + x\n        if self.state == self.num_states - 1:\n            reward = 1.0\n            done = True\n        else:\n            reward = -0.01\n            done = False\n        return self.state, reward, done\nprint(\"Environment Model Learning Demonstration\")\nprint(\"=\" * 50)\nenv = SimpleGridWorld(size=4)\ntabular_model = TabularModel(env.num_states, env.num_actions)\nn_episodes = 1000\nexperience_data = []\nprint(\"\\n1. Collecting experience...\")\nfor episode in range(n_episodes):\n    state = env.reset()\n    done = False\n    while not done:\n        action = np.random.randint(env.num_actions)\n        next_state, reward, done = env.step(action)\n        tabular_model.update(state, action, next_state, reward)\n        experience_data.append((state, action, next_state, reward))\n        state = next_state\nprint(f\"Collected {len(experience_data)} transitions\")\nstates = np.array([exp[0] for exp in experience_data])\nactions = np.array([exp[1] for exp in experience_data])\nnext_states = np.array([exp[2] for exp in experience_data])\nrewards = np.array([exp[3] for exp in experience_data])\nstates_onehot = np.eye(env.num_states)[states]\nnext_states_onehot = np.eye(env.num_states)[next_states]\nprint(\"\\n2. Training neural model...\")\nneural_model = NeuralModel(env.num_states, env.num_actions, hidden_dim=64, ensemble_size=3).to(device)\ntrainer = ModelTrainer(neural_model, lr=1e-3)\ntrainer.train_batch((states_onehot, actions, next_states_onehot, rewards), epochs=50, batch_size=64)\nprint(\"\\n3. Model accuracy comparison:\")\ntest_states = np.random.randint(0, env.num_states, 100)\ntest_actions = np.random.randint(0, env.num_actions, 100)\ntabular_errors = []\nneural_errors = []\nfor s, a in zip(test_states, test_actions):\n    true_next_states = []\n    true_rewards = []\n    for _ in range(50):\n        env.state = s\n        next_state, reward, _ = env.step(a)\n        true_next_states.append(next_state)\n        true_rewards.append(reward)\n    true_reward = np.mean(true_rewards)\n    pred_reward_tabular = tabular_model.get_expected_reward(s, a)\n    tabular_errors.append(abs(true_reward - pred_reward_tabular))\n    state_tensor = torch.FloatTensor(np.eye(env.num_states)[s]).to(device)\n    action_tensor = torch.LongTensor([a]).to(device)\n    _, pred_reward_neural = neural_model(state_tensor, action_tensor)\n    pred_reward_neural = pred_reward_neural.cpu().item()\n    neural_errors.append(abs(true_reward - pred_reward_neural))\nprint(f\"Tabular model MAE: {np.mean(tabular_errors):.4f}\")\nprint(f\"Neural model MAE: {np.mean(neural_errors):.4f}\")\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(trainer.loss_history)\nplt.title('Neural Model Training Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.grid(True, alpha=0.3)\nplt.subplot(1, 2, 2)\nplt.bar(['Tabular Model', 'Neural Model'], [np.mean(tabular_errors), np.mean(neural_errors)], \n        color=['blue', 'red'], alpha=0.7)\nplt.title('Model Accuracy Comparison')\nplt.ylabel('Mean Absolute Error')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint(\"\\n‚úÖ Environment model learning complete!\")\nprint(\"üìä Next: Classical planning with learned models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc141ae",
   "metadata": {},
   "source": [
    "# Section 3: Classical Planning with Learned Models\n",
    "\n",
    "## 3.1 Dynamic Programming with Learned Models\n",
    "\n",
    "Once we have learned an environment model, we can use classical dynamic programming algorithms to compute optimal policies. This is one of the most straightforward applications of model-based RL.\n",
    "\n",
    "### Value Iteration with Learned Models\n",
    "\n",
    "The Value Iteration algorithm can be applied directly using our learned transition probabilities and rewards:\n",
    "\n",
    "$$V_{k+1}(s) = \\max_a \\sum_{s'} \\hat{P}(s'|s,a)[\\hat{R}(s,a,s') + \\gamma V_k(s')]$$\n",
    "\n",
    "**Key Advantages:**\n",
    "- Guaranteed convergence to optimal policy (if model is accurate)\n",
    "- Can compute policy for all states simultaneously\n",
    "- No need for exploration during planning phase\n",
    "\n",
    "**Potential Issues:**\n",
    "- Model errors compound over planning horizon\n",
    "- Assumes learned model is accurate\n",
    "- May overfit to limited experience\n",
    "\n",
    "### Policy Iteration with Learned Models\n",
    "\n",
    "Policy Iteration alternates between policy evaluation and policy improvement using the learned model:\n",
    "\n",
    "**Policy Evaluation:**\n",
    "$$V^\\pi(s) = \\sum_{s'} \\hat{P}(s'|s,\\pi(s))[\\hat{R}(s,\\pi(s),s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "**Policy Improvement:**\n",
    "$$\\pi'(s) = \\arg\\max_a \\sum_{s'} \\hat{P}(s'|s,a)[\\hat{R}(s,a,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "## 3.2 Handling Model Uncertainty\n",
    "\n",
    "Real learned models have uncertainty. Several approaches address this:\n",
    "\n",
    "### Pessimistic Planning\n",
    "- Use lower confidence bounds for model predictions\n",
    "- $\\hat{P}_{pessimistic}(s'|s,a) = \\hat{P}(s'|s,a) - \\beta \\sigma(s'|s,a)$\n",
    "- Leads to more robust but potentially conservative policies\n",
    "\n",
    "### Optimistic Planning  \n",
    "- Use upper confidence bounds for model predictions\n",
    "- Encourages exploration of uncertain regions\n",
    "- Can lead to more aggressive exploration policies\n",
    "\n",
    "### Robust Planning\n",
    "- Optimize for worst-case model within confidence region\n",
    "- $\\max_\\pi \\min_{M \\in \\mathcal{U}} V^\\pi_M$\n",
    "- Very conservative but safe approach\n",
    "\n",
    "## 3.3 Model-Based Policy Search\n",
    "\n",
    "Instead of computing value functions, we can directly search in policy space using the learned model:\n",
    "\n",
    "### Gradient-Based Policy Search\n",
    "- Use model to compute policy gradients\n",
    "- $\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta, M}[\\sum_t \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) R(\\tau)]$\n",
    "- Generate synthetic rollouts with learned model\n",
    "\n",
    "### Evolutionary Policy Search\n",
    "- Maintain population of policy parameters\n",
    "- Evaluate policies using learned model\n",
    "- Select and mutate best policies\n",
    "\n",
    "### Random Shooting\n",
    "- Sample random action sequences\n",
    "- Evaluate using learned model\n",
    "- Select best sequence and execute first action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285abb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBasedPlanner:\n    def __init__(self, model, num_states, num_actions, gamma=0.99):\n        self.model = model\n        self.num_states = num_states\n        self.num_actions = num_actions\n        self.gamma = gamma\n        self.V = np.zeros(num_states)\n        self.policy = np.zeros(num_states, dtype=int)\n        self.value_history = []\n        self.policy_history = []\n    def value_iteration(self, max_iterations=100, tolerance=1e-6):\n        print(f\"Running Value Iteration (max_iter={max_iterations}, tol={tolerance})\")\n        for iteration in range(max_iterations):\n            old_V = self.V.copy()\n            for state in range(self.num_states):\n                q_values = np.zeros(self.num_actions)\n                for action in range(self.num_actions):\n                    expected_value = 0\n                    for next_state in range(self.num_states):\n                        transition_prob = self.model.get_transition_prob(state, action, next_state)\n                        reward = self.model.get_expected_reward(state, action)\n                        expected_value += transition_prob * (reward + self.gamma * old_V[next_state])\n                    q_values[action] = expected_value\n                self.V[state] = np.max(q_values)\n                self.policy[state] = np.argmax(q_values)\n            self.value_history.append(self.V.copy())\n            self.policy_history.append(self.policy.copy())\n            if np.max(np.abs(self.V - old_V)) < tolerance:\n                print(f\"Converged after {iteration + 1} iterations\")\n                break\n        return self.V, self.policy\n    def policy_iteration(self, max_iterations=50, eval_max_iterations=100):\n        print(f\"Running Policy Iteration (max_iter={max_iterations})\")\n        self.policy = np.random.randint(0, self.num_actions, self.num_states)\n        for iteration in range(max_iterations):\n            old_policy = self.policy.copy()\n            self.V = self.policy_evaluation(self.policy, max_iterations=eval_max_iterations)\n            for state in range(self.num_states):\n                q_values = np.zeros(self.num_actions)\n                for action in range(self.num_actions):\n                    expected_value = 0\n                    for next_state in range(self.num_states):\n                        transition_prob = self.model.get_transition_prob(state, action, next_state)\n                        reward = self.model.get_expected_reward(state, action)\n                        expected_value += transition_prob * (reward + self.gamma * self.V[next_state])\n                    q_values[action] = expected_value\n                self.policy[state] = np.argmax(q_values)\n            self.value_history.append(self.V.copy())\n            self.policy_history.append(self.policy.copy())\n            if np.array_equal(self.policy, old_policy):\n                print(f\"Converged after {iteration + 1} iterations\")\n                break\n        return self.V, self.policy\n    def policy_evaluation(self, policy, max_iterations=100, tolerance=1e-6):\n        V = np.zeros(self.num_states)\n        for iteration in range(max_iterations):\n            old_V = V.copy()\n            for state in range(self.num_states):\n                action = policy[state]\n                expected_value = 0\n                for next_state in range(self.num_states):\n                    transition_prob = self.model.get_transition_prob(state, action, next_state)\n                    reward = self.model.get_expected_reward(state, action)\n                    expected_value += transition_prob * (reward + self.gamma * old_V[next_state])\n                V[state] = expected_value\n            if np.max(np.abs(V - old_V)) < tolerance:\n                break\n        return V\n    def compute_q_function(self):\n        Q = np.zeros((self.num_states, self.num_actions))\n        for state in range(self.num_states):\n            for action in range(self.num_actions):\n                expected_value = 0\n                for next_state in range(self.num_states):\n                    transition_prob = self.model.get_transition_prob(state, action, next_state)\n                    reward = self.model.get_expected_reward(state, action)\n                    expected_value += transition_prob * (reward + self.gamma * self.V[next_state])\n                Q[state, action] = expected_value\n        return Q\nclass UncertaintyAwarePlanner:\n    def __init__(self, ensemble_model, num_states, num_actions, gamma=0.99):\n        self.ensemble_model = ensemble_model\n        self.num_states = num_states\n        self.num_actions = num_actions\n        self.gamma = gamma\n    def pessimistic_value_iteration(self, beta=1.0, max_iterations=100):\n        V = np.zeros(self.num_states)\n        policy = np.zeros(self.num_states, dtype=int)\n        print(f\"Running Pessimistic Value Iteration (beta={beta})\")\n        for iteration in range(max_iterations):\n            old_V = V.copy()\n            for state in range(self.num_states):\n                q_values = np.zeros(self.num_actions)\n                for action in range(self.num_actions):\n                    state_onehot = np.eye(self.num_states)[state:state+1]\n                    action_tensor = np.array([action])\n                    state_tensor = torch.FloatTensor(state_onehot).to(device)\n                    action_tensor = torch.LongTensor(action_tensor).to(device)\n                    next_state_mean, reward_mean, next_state_std, reward_std = \\\n                        self.ensemble_model.predict_with_uncertainty(state_tensor, action_tensor)\\n\",\n                    \\n\",\n                    pessimistic_reward = reward_mean.cpu().item() - beta * reward_std.cpu().item()\\n\",\n                    \\n\",\n                    next_state_pred = next_state_mean.cpu().numpy()[0]\\n\",\n                    next_state_idx = np.argmax(next_state_pred)\n                    \\n\",\n                    q_values[action] = pessimistic_reward + self.gamma * old_V[next_state_idx]\\n\",\n                \\n\",\n                V[state] = np.max(q_values)\\n\",\n                policy[state] = np.argmax(q_values)\\n\",\n            \\n\",\n            if np.max(np.abs(V - old_V)) < 1e-6:\\n\",\n                print(f\\\"Converged after {iteration + 1} iterations\\\")\\n\",\n                break\\n\",\n        \\n\",\n        return V, policy\\n\",\n    \\n\",\n    def optimistic_value_iteration(self, beta=1.0, max_iterations=100):\\n\",\n        \\\"\\\"\\\"Value iteration with optimistic model estimates\\\"\\\"\\\"\\n\",\n        \\n\",\n        V = np.zeros(self.num_states)\\n\",\n        policy = np.zeros(self.num_states, dtype=int)\\n\",\n        \\n\",\n        print(f\\\"Running Optimistic Value Iteration (beta={beta})\\\")\\n\",\n        \\n\",\n        for iteration in range(max_iterations):\\n\",\n            old_V = V.copy()\\n\",\n            \\n\",\n            for state in range(self.num_states):\\n\",\n                q_values = np.zeros(self.num_actions)\\n\",\n                \\n\",\n                for action in range(self.num_actions):\\n\",\n                    state_onehot = np.eye(self.num_states)[state:state+1]\\n\",\n                    action_tensor = np.array([action])\\n\",\n                    \\n\",\n                    state_tensor = torch.FloatTensor(state_onehot).to(device)\\n\",\n                    action_tensor = torch.LongTensor(action_tensor).to(device)\\n\",\n                    \\n\",\n                    next_state_mean, reward_mean, next_state_std, reward_std = \\\\\\n\",\n                        self.ensemble_model.predict_with_uncertainty(state_tensor, action_tensor)\\n\",\n                    \\n\",\n                    optimistic_reward = reward_mean.cpu().item() + beta * reward_std.cpu().item()\\n\",\n                    \\n\",\n                    next_state_pred = next_state_mean.cpu().numpy()[0]\\n\",\n                    next_state_idx = np.argmax(next_state_pred)\\n\",\n                    \\n\",\n                    q_values[action] = optimistic_reward + self.gamma * old_V[next_state_idx]\\n\",\n                \\n\",\n                V[state] = np.max(q_values)\\n\",\n                policy[state] = np.argmax(q_values)\\n\",\n            \\n\",\n            if np.max(np.abs(V - old_V)) < 1e-6:\\n\",\n                print(f\\\"Converged after {iteration + 1} iterations\\\")\\n\",\n                break\\n\",\n        \\n\",\n        return V, policy\\n\",\n\"\\n\",\n\"class ModelBasedPolicySearch:\\n\",\n\"    \\\"\\\"\\\"Policy search using learned models\\\"\\\"\\\"\\n\",\n\"    \\n\",\n\"    def __init__(self, model, state_dim, action_dim, gamma=0.99):\\n\",\n\"        self.model = model\\n\",\n\"        self.state_dim = state_dim\\n\",\n\"        self.action_dim = action_dim\\n\",\n\"        self.gamma = gamma\\n\",\n\"    \\n\",\n\"    def random_shooting(self, initial_state, horizon=10, num_sequences=1000):\\n\",\n\"        \\\"\\\"\\\"Random shooting with learned model\\\"\\\"\\\"\\n\",\n\"        \\n\",\n\"        best_sequence = None\\n\",\n\"        best_value = -np.inf\\n\",\n\"        \\n\",\n\"        for _ in range(num_sequences):\\n\",\n\"            # Sample random action sequence\\n\",\n\"            action_sequence = np.random.randint(0, self.action_dim, horizon)\\n\",\n\"            \\n\",\n\"            # Evaluate sequence using model\\n\",\n\"            total_reward = 0\\n\",\n\"            current_state = initial_state\\n\",\n\"            discount = 1.0\\n\",\n\"            \\n\",\n\"            for action in action_sequence:\\n\",\n\"                next_state, reward = self.model.sample_transition(current_state, action)\\n\",\n\"                total_reward += discount * reward\\n\",\n\"                discount *= self.gamma\\n\",\n\"                current_state = next_state\\n\",\n\"            \\n\",\n\"            if total_reward > best_value:\\n\",\n\"                best_value = total_reward\\n\",\n\"                best_sequence = action_sequence\\n\",\n\"        \\n\",\n\"        return best_sequence, best_value\\n\",\n\"    \\n\",\n\"    def cross_entropy_method(self, initial_state, horizon=10, num_sequences=1000, \\n\",\n\"                           num_elite=100, num_iterations=10):\\n\",\n\"        \\\"\\\"\\\"Cross-entropy method for policy search\\\"\\\"\\\"\\n\",\n\"        \\n\",\n\"        # Initialize action probabilities (uniform)\\n\",\n\"        action_probs = np.ones((horizon, self.action_dim)) / self.action_dim\\n\",\n\"        \\n\",\n\"        for iteration in range(num_iterations):\\n\",\n\"            # Sample action sequences\\n\",\n\"            sequences = []\\n\",\n\"            values = []\\n\",\n\"            \\n\",\n\"            for _ in range(num_sequences):\\n\",\n\"                sequence = []\\n\",\n\"                for t in range(horizon):\\n\",\n\"                    action = np.random.choice(self.action_dim, p=action_probs[t])\\n\",\n\"                    sequence.append(action)\\n\",\n\"                \\n\",\n\"                # Evaluate sequence\\n\",\n\"                total_reward = 0\\n\",\n\"                current_state = initial_state\\n\",\n\"                discount = 1.0\\n\",\n\"                \\n\",\n\"                for action in sequence:\\n\",\n\"                    next_state, reward = self.model.sample_transition(current_state, action)\\n\",\n\"                    total_reward += discount * reward\\n\",\n\"                    discount *= self.gamma\\n\",\n\"                    current_state = next_state\\n\",\n\"                \\n\",\n\"                sequences.append(sequence)\\n\",\n\"                values.append(total_reward)\\n\",\n\"            \\n\",\n\"            # Select elite sequences\\n\",\n\"            elite_indices = np.argsort(values)[-num_elite:]\\n\",\n\"            elite_sequences = [sequences[i] for i in elite_indices]\\n\",\n\"            \\n\",\n\"            # Update action probabilities\\n\",\n\"            action_counts = np.zeros((horizon, self.action_dim))\\n\",\n\"            \\n\",\n\"            for sequence in elite_sequences:\\n\",\n\"                for t, action in enumerate(sequence):\\n\",\n\"                    action_counts[t, action] += 1\\n\",\n\"            \\n\",\n\"            # Smooth update\\n\",\n\"            alpha = 0.7\\n\",\n\"            new_probs = action_counts / num_elite\\n\",\n\"            action_probs = alpha * new_probs + (1 - alpha) * action_probs\\n\",\n\"            \\n\",\n\"            # Add small amount of noise for exploration\\n\",\n\"            action_probs += 0.01 / self.action_dim\\n\",\n\"            action_probs /= np.sum(action_probs, axis=1, keepdims=True)\\n\",\n\"        \\n\",\n\"        # Return best sequence\\n\",\n\"        best_sequence = elite_sequences[np.argmax([values[i] for i in elite_indices])]\\n\",\n\"        best_value = max([values[i] for i in elite_indices])\\n\",\n\"        \\n\",\n\"        return best_sequence, best_value\\n\",\n\"\\n\",\n\"# Demonstration of classical planning\\n\",\n\"print(\\\"Classical Planning with Learned Models\\\")\\n\",\n\"print(\\\"=\\\" * 50)\\n\",\n\"\\n\",\n\"# Use the tabular model we learned earlier\\n\",\n\"planner = ModelBasedPlanner(tabular_model, env.num_states, env.num_actions, gamma=0.95)\\n\",\n\"\\n\",\n\"print(\\\"\\\\n1. Value Iteration with Learned Model:\\\")\\n\",\n\"vi_values, vi_policy = planner.value_iteration(max_iterations=50)\\n\",\n\"\\n\",\n\"print(\\\"\\\\n2. Policy Iteration with Learned Model:\\\")\\n\",\n\"planner_pi = ModelBasedPlanner(tabular_model, env.num_states, env.num_actions, gamma=0.95)\\n\",\n\"pi_values, pi_policy = planner_pi.policy_iteration(max_iterations=20)\\n\",\n\"\\n\",\n\"# Compare with uncertainty-aware planning\\n\",\n\"print(\\\"\\\\n3. Uncertainty-Aware Planning:\\\")\\n\",\n\"uncertainty_planner = UncertaintyAwarePlanner(neural_model, env.num_states, env.num_actions)\\n\",\n\"pessimistic_V, pessimistic_policy = uncertainty_planner.pessimistic_value_iteration(beta=0.5)\\n\",\n\"optimistic_V, optimistic_policy = uncertainty_planner.optimistic_value_iteration(beta=0.5)\\n\",\n\"\\n\",\n\"# Visualize results\\n\",\n\"fig, axes = plt.subplots(2, 3, figsize=(18, 12))\\n\",\n\"\\n\",\n\"# Reshape values and policies for visualization\\n\",\n\"grid_size = int(np.sqrt(env.num_states))\\n\",\n\"\\n\",\n\"def plot_value_function(ax, values, title):\\n\",\n\"    value_grid = values.reshape(grid_size, grid_size)\\n\",\n\"    im = ax.imshow(value_grid, cmap='viridis')\\n\",\n\"    ax.set_title(title)\\n\",\n\"    plt.colorbar(im, ax=ax)\\n\",\n\"    \\n\",\n\"def plot_policy(ax, policy, title):\\n\",\n\"    policy_grid = policy.reshape(grid_size, grid_size)\\n\",\n\"    # Map actions to arrows: 0=‚Üë, 1=‚Üì, 2=‚Üê, 3=‚Üí\\n\",\n\"    arrow_map = {0: '‚Üë', 1: '‚Üì', 2: '‚Üê', 3: '‚Üí'}\\n\",\n\"    \\n\",\n\"    ax.imshow(np.zeros((grid_size, grid_size)), cmap='gray', alpha=0.3)\\n\",\n\"    \\n\",\n\"    for i in range(grid_size):\\n\",\n\"        for j in range(grid_size):\\n\",\n\"            action = policy_grid[i, j]\\n\",\n\"            ax.text(j, i, arrow_map[action], ha='center', va='center', \\n\",\n\"                   fontsize=20, fontweight='bold', color='blue')\\n\",\n\"    \\n\",\n\"    ax.set_title(title)\\n\",\n\"    ax.set_xticks([])\\n\",\n\"    ax.set_yticks([])\\n\",\n\"\\n\",\n\"# Plot value functions\\n\",\n\"plot_value_function(axes[0, 0], vi_values, 'Value Iteration - Values')\\n\",\n\"plot_value_function(axes[0, 1], pi_values, 'Policy Iteration - Values')\\n\",\n\"plot_value_function(axes[0, 2], pessimistic_V, 'Pessimistic Planning - Values')\\n\",\n\"\\n\",\n\"# Plot policies\\n\",\n\"plot_policy(axes[1, 0], vi_policy, 'Value Iteration - Policy')\\n\",\n\"plot_policy(axes[1, 1], pi_policy, 'Policy Iteration - Policy')\\n\",\n\"plot_policy(axes[1, 2], pessimistic_policy, 'Pessimistic Planning - Policy')\\n\",\n\"\\n\",\n\"plt.tight_layout()\\n\",\n\"plt.show()\\n\",\n\"\\n\",\n\"# Compare planning methods\\n\",\n\"print(\\\"\\\\n4. Planning Method Comparison:\\\")\\n\",\n\"print(f\\\"Value Iteration - Max Value: {np.max(vi_values):.3f}, Policy Changes: {len(planner.value_history)}\\\")\\n\",\n\"print(f\\\"Policy Iteration - Max Value: {np.max(pi_values):.3f}, Policy Changes: {len(planner_pi.value_history)}\\\")\\n\",\n\"print(f\\\"Pessimistic Planning - Max Value: {np.max(pessimistic_V):.3f}\\\")\\n\",\n\"print(f\\\"Optimistic Planning - Max Value: {np.max(optimistic_V):.3f}\\\")\\n\",\n\"\\n\",\n\"# Test policy search methods\\n\",\n\"print(\\\"\\\\n5. Model-Based Policy Search:\\\")\\n\",\n\"policy_searcher = ModelBasedPolicySearch(tabular_model, env.num_states, env.num_actions)\\n\",\n\"\\n\",\n\"# Random shooting\\n\",\n\"initial_state = 0\\n\",\n\"best_sequence_rs, best_value_rs = policy_searcher.random_shooting(initial_state, horizon=5, num_sequences=500)\\n\",\n\"print(f\\\"Random Shooting - Best Value: {best_value_rs:.3f}, Best Sequence: {best_sequence_rs}\\\")\\n\",\n\"\\n\",\n\"# Cross-entropy method\\n\",\n\"best_sequence_cem, best_value_cem = policy_searcher.cross_entropy_method(initial_state, horizon=5, \\n\",\n\"                                                                        num_sequences=200, num_elite=20)\\n\",\n\"print(f\\\"Cross-Entropy Method - Best Value: {best_value_cem:.3f}, Best Sequence: {best_sequence_cem}\\\")\\n\",\n\"\\n\",\n\"print(\\\"\\\\n‚úÖ Classical planning with learned models complete!\\\")\\n\",\n\"print(\\\"üìä Next: Dyna-Q algorithm - integrating planning and learning\\\")\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c05caa9",
   "metadata": {},
   "source": [
    "# Section 4: Dyna-Q Algorithm - Integrating Planning and Learning\n",
    "\n",
    "## 4.1 The Dyna Architecture\n",
    "\n",
    "Dyna-Q is a pioneering algorithm that integrates **direct reinforcement learning** with **planning** using a learned model. It represents one of the first successful attempts to combine model-free and model-based approaches.\n",
    "\n",
    "### The Dyna Framework Components:\n",
    "\n",
    "1. **Direct RL**: Learn from real experience using Q-learning\n",
    "2. **Model Learning**: Learn environment model from real experience  \n",
    "3. **Planning**: Use learned model to generate simulated experience\n",
    "4. **Policy**: Act using Œµ-greedy policy based on Q-values\n",
    "\n",
    "### Dyna-Q Algorithm Structure:\n",
    "\n",
    "```\n",
    "For each episode:\n",
    "    For each step:\n",
    "        1. Take action using Œµ-greedy policy\n",
    "        2. Observe reward and next state\n",
    "        3. Update Q-function with real experience\n",
    "        4. Update model with real experience\n",
    "        5. Planning: Do n planning updates using model\n",
    "            - Sample random state-action pair\n",
    "            - Use model to predict next state and reward\n",
    "            - Update Q-function with simulated experience\n",
    "```\n",
    "\n",
    "## 4.2 Mathematical Formulation\n",
    "\n",
    "### Direct Learning (Q-Learning):\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "### Model Learning:\n",
    "- **Transition Model**: $\\hat{P}(s'|s,a)$ learned from observed transitions\n",
    "- **Reward Model**: $\\hat{R}(s,a)$ learned from observed rewards\n",
    "\n",
    "### Planning Updates:\n",
    "For randomly sampled $(s,a)$ pairs:\n",
    "1. Generate simulated experience: $(s',r) \\sim \\hat{P}(\\cdot|s,a), \\hat{R}(s,a)$\n",
    "2. Update Q-function: $Q(s,a) \\leftarrow Q(s,a) + \\alpha[r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$\n",
    "\n",
    "## 4.3 Key Advantages of Dyna-Q\n",
    "\n",
    "**Sample Efficiency**:\n",
    "- Each real experience is used multiple times (once for direct learning, multiple times for planning)\n",
    "- Can learn much faster than pure model-free methods\n",
    "\n",
    "**Adaptability**:\n",
    "- Continues to work even if model is inaccurate\n",
    "- Direct learning corrects for model errors\n",
    "\n",
    "**Simplicity**:\n",
    "- Easy to implement and understand\n",
    "- Natural extension of Q-learning\n",
    "\n",
    "**Flexibility**:\n",
    "- Can adjust planning steps based on computational budget\n",
    "- Works with any model learning approach\n",
    "\n",
    "## 4.4 Dyna-Q+ Extensions\n",
    "\n",
    "**Exploration Bonus**:\n",
    "- Add bonus for actions not taken recently\n",
    "- Encourages exploration of potentially changed parts of environment\n",
    "\n",
    "**Prioritized Updates**:\n",
    "- Focus planning on states with largest Q-value changes\n",
    "- More efficient use of planning computation\n",
    "\n",
    "**Dyna-H**:\n",
    "- Use learned models for hierarchical planning\n",
    "- Plan at multiple temporal scales\n",
    "\n",
    "## 4.5 When Model Changes: The Blocking Maze\n",
    "\n",
    "A classic example where Dyna-Q demonstrates its adaptability is the \"blocking maze\" scenario:\n",
    "\n",
    "1. **Initial Phase**: Agent learns optimal path through maze\n",
    "2. **Environment Change**: Optimal path becomes blocked, new path opens\n",
    "3. **Adaptation**: Dyna-Q must discover the environment change and adapt\n",
    "\n",
    "This scenario highlights the importance of continued exploration even with a seemingly good model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2499f728",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynaQAgent:\n    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.95, epsilon=0.1, planning_steps=5):\n        self.num_states = num_states\n        self.num_actions = num_actions\n        self.alpha = alpha\n        self.gamma = gamma\n        self.epsilon = epsilon\n        self.planning_steps = planning_steps\n        self.Q = np.zeros((num_states, num_actions))\n        self.model = {}\n        self.visited_state_actions = set()\n        self.episode_rewards = []\n        self.episode_lengths = []\n        self.planning_updates = 0\n        self.direct_updates = 0\n    def select_action(self, state):\n        if np.random.random() < self.epsilon:\n            return np.random.randint(self.num_actions)\n        else:\n            return np.argmax(self.Q[state])\n    def update_q_function(self, state, action, reward, next_state):\n        td_target = reward + self.gamma * np.max(self.Q[next_state])\n        td_error = td_target - self.Q[state, action]\n        self.Q[state, action] += self.alpha * td_error\n        return td_error\n    def update_model(self, state, action, reward, next_state):\n        self.model[(state, action)] = (reward, next_state)\n        self.visited_state_actions.add((state, action))\n    def planning_update(self):\n        if len(self.visited_state_actions) == 0:\n            return\n        for _ in range(self.planning_steps):\n            state, action = random.choice(list(self.visited_state_actions))\n            if (state, action) in self.model:\n                reward, next_state = self.model[(state, action)]\n                self.update_q_function(state, action, reward, next_state)\n                self.planning_updates += 1\n    def train_episode(self, env, max_steps=200):\n        state = env.reset()\n        total_reward = 0\n        steps = 0\n        for step in range(max_steps):\n            action = self.select_action(state)\n            next_state, reward, done = env.step(action)\n            self.update_q_function(state, action, reward, next_state)\n            self.direct_updates += 1\n            self.update_model(state, action, reward, next_state)\n            self.planning_update()\\n\",\n            \\n\",\n            total_reward += reward\\n\",\n            steps += 1\\n\",\n            \\n\",\n            if done:\\n\",\n                break\\n\",\n                \\n\",\n            state = next_state\\n\",\n        \\n\",\n        self.episode_rewards.append(total_reward)\\n\",\n        self.episode_lengths.append(steps)\\n\",\n        \\n\",\n        return total_reward, steps\\n\",\n    \\n\",\n    def get_statistics(self):\\n\",\n        \\\"\\\"\\\"Get training statistics\\\"\\\"\\\"\\n\",\n        return {\\n\",\n            'direct_updates': self.direct_updates,\\n\",\n            'planning_updates': self.planning_updates,\\n\",\n            'model_size': len(self.model),\\n\",\n            'avg_episode_reward': np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0\\n\",\n        }\\n\",\n\\n\",\nclass DynaQPlusAgent(DynaQAgent):\\n\",\n    \\\"\\\"\\\"Dyna-Q+ with exploration bonus for changed environments\\\"\\\"\\\"\\n\",\n    \\n\",\n    def __init__(self, num_states, num_actions, alpha=0.1, gamma=0.95, epsilon=0.1, \\n\",\n                 planning_steps=5, kappa=0.001):\\n\",\n        super().__init__(num_states, num_actions, alpha, gamma, epsilon, planning_steps)\\n\",\n        \\n\",\n        self.kappa = kappa\n        self.last_visit_time = {}\n        self.current_time = 0\\n\",\n        \\n\",\n    def update_q_function(self, state, action, reward, next_state, is_real_experience=True):\\n\",\n        \\\"\\\"\\\"Enhanced Q-learning update with exploration bonus\\\"\\\"\\\"\\n\",\n        \\n\",\n        if is_real_experience:\\n\",\n            self.last_visit_time[(state, action)] = self.current_time\\n\",\n            self.current_time += 1\\n\",\n        \\n\",\n        exploration_bonus = 0\\n\",\n        if not is_real_experience and (state, action) in self.last_visit_time:\\n\",\n            time_since_visit = self.current_time - self.last_visit_time[(state, action)]\\n\",\n            exploration_bonus = self.kappa * np.sqrt(time_since_visit)\\n\",\n        \\n\",\n        td_target = reward + exploration_bonus + self.gamma * np.max(self.Q[next_state])\\n\",\n        td_error = td_target - self.Q[state, action]\\n\",\n        self.Q[state, action] += self.alpha * td_error\\n\",\n        \\n\",\n        return td_error\\n\",\n    \\n\",\n    def planning_update(self):\\n\",\n        \\\"\\\"\\\"Planning update with exploration bonus\\\"\\\"\\\"\\n\",\n        if len(self.visited_state_actions) == 0:\\n\",\n            return\\n\",\n        \\n\",\n        for _ in range(self.planning_steps):\\n\",\n            state, action = random.choice(list(self.visited_state_actions))\\n\",\n            \\n\",\n            if (state, action) in self.model:\\n\",\n                reward, next_state = self.model[(state, action)]\\n\",\n                \\n\",\n                self.update_q_function(state, action, reward, next_state, is_real_experience=False)\\n\",\n                self.planning_updates += 1\\n\",\n    \\n\",\n    def train_episode(self, env, max_steps=200):\\n\",\n        \\\"\\\"\\\"Training episode with proper experience tracking\\\"\\\"\\\"\\n\",\n        state = env.reset()\\n\",\n        total_reward = 0\\n\",\n        steps = 0\\n\",\n        \\n\",\n        for step in range(max_steps):\\n\",\n            action = self.select_action(state)\\n\",\n            next_state, reward, done = env.step(action)\\n\",\n            \\n\",\n            self.update_q_function(state, action, reward, next_state, is_real_experience=True)\\n\",\n            self.direct_updates += 1\\n\",\n            \\n\",\n            self.update_model(state, action, reward, next_state)\\n\",\n            self.planning_update()\\n\",\n            \\n\",\n            total_reward += reward\\n\",\n            steps += 1\\n\",\n            \\n\",\n            if done:\\n\",\n                break\\n\",\n                \\n\",\n            state = next_state\\n\",\n        \\n\",\n        self.episode_rewards.append(total_reward)\\n\",\n        self.episode_lengths.append(steps)\\n\",\n        \\n\",\n        return total_reward, steps\\n\",\n\\n\",\nclass BlockingMaze:\\n\",\n    \\\"\\\"\\\"Environment that changes to test Dyna-Q adaptability\\\"\\\"\\\"\\n\",\n    \\n\",\n    def __init__(self, width=9, height=6, change_episode=1000):\\n\",\n        self.width = width\\n\",\n        self.height = height\\n\",\n        self.num_states = width * height\\n\",\n        self.num_actions = 4\n        self.change_episode = change_episode\\n\",\n        self.episode_count = 0\\n\",\n        \\n\",\n        self.start_pos = (0, 3)\n        self.goal_pos = (8, 0)\n        \\n\",\n        self.blocked_cells = set()\\n\",\n        self.setup_initial_maze()\\n\",\n        \\n\",\n        self.state = self.pos_to_state(self.start_pos)\\n\",\n    \\n\",\n    def pos_to_state(self, pos):\\n\",\n        \\\"\\\"\\\"Convert (x, y) position to state index\\\"\\\"\\\"\\n\",\n        return pos[1] * self.width + pos[0]\\n\",\n    \\n\",\n    def state_to_pos(self, state):\\n\",\n        \\\"\\\"\\\"Convert state index to (x, y) position\\\"\\\"\\\"\\n\",\n        return (state % self.width, state // self.width)\\n\",\n    \\n\",\n    def setup_initial_maze(self):\\n\",\n        \\\"\\\"\\\"Setup initial maze with one path blocked\\\"\\\"\\\"\\n\",\n        for y in range(1, 4):\\n\",\n            self.blocked_cells.add((3, y))\\n\",\n        \\n\",\n        self.initial_blocks = self.blocked_cells.copy()\\n\",\n        \\n\",\n        self.changed_blocks = set()\\n\",\n        for x in range(1, 8):\\n\",\n            self.changed_blocks.add((x, 2))\\n\",\n    \\n\",\n    def reset(self):\\n\",\n        \\\"\\\"\\\"Reset environment\\\"\\\"\\\"\\n\",\n        self.episode_count += 1\\n\",\n        \\n\",\n        if self.episode_count == self.change_episode:\\n\",\n            self.blocked_cells = self.changed_blocks.copy()\\n\",\n            print(f\\\"\\\\n*** Environment changed at episode {self.episode_count} ***\\\")\\n\",\n        \\n\",\n        self.state = self.pos_to_state(self.start_pos)\\n\",\n        return self.state\\n\",\n    \\n\",\n    def step(self, action):\\n\",\n        \\\"\\\"\\\"Take action in environment\\\"\\\"\\\"\\n\",\n        current_pos = self.state_to_pos(self.state)\\n\",\n        x, y = current_pos\\n\",\n        \\n\",\n        if action == 0 and y > 0:\n            new_pos = (x, y - 1)\\n\",\n        elif action == 1 and y < self.height - 1:\n            new_pos = (x, y + 1)\\n\",\n        elif action == 2 and x > 0:\n            new_pos = (x - 1, y)\\n\",\n        elif action == 3 and x < self.width - 1:\n            new_pos = (x + 1, y)\\n\",\n        else:\\n\",\n            new_pos = current_pos\n        \\n\",\n        if new_pos in self.blocked_cells:\\n\",\n            new_pos = current_pos\n        \\n\",\n        self.state = self.pos_to_state(new_pos)\\n\",\n        \\n\",\n        if new_pos == self.goal_pos:\\n\",\n            reward = 1.0\\n\",\n            done = True\\n\",\n        else:\\n\",\n            reward = 0.0\\n\",\n            done = False\\n\",\n        \\n\",\n        return self.state, reward, done\\n\",\n    \\n\",\n    def render_maze(self):\\n\",\n        \\\"\\\"\\\"Render current maze state\\\"\\\"\\\"\\n\",\n        maze = np.zeros((self.height, self.width))\\n\",\n        \\n\",\n        for x, y in self.blocked_cells:\\n\",\n            maze[y, x] = -1\\n\",\n        \\n\",\n        maze[self.start_pos[1], self.start_pos[0]] = 2\\n\",\n        maze[self.goal_pos[1], self.goal_pos[0]] = 3\\n\",\n        \\n\",\n        current_pos = self.state_to_pos(self.state)\\n\",\n        if current_pos != self.start_pos and current_pos != self.goal_pos:\\n\",\n            maze[current_pos[1], current_pos[0]] = 1\\n\",\n        \\n\",\n        return maze\\n\",\n\\n\",\n\"# Comprehensive Dyna-Q Demonstration\\n\",\n\"print(\\\"Dyna-Q Algorithm Demonstration\\\")\\n\",\n\"print(\\\"=\\\" * 50)\\n\",\n\"\\n\",\n\"# Compare different agents\\n\",\n\"agents = {\\n\",\n\"    'Q-Learning': DynaQAgent(25, 4, planning_steps=0),  # No planning\\n\",\n\"    'Dyna-Q (n=5)': DynaQAgent(25, 4, planning_steps=5),\\n\",\n\"    'Dyna-Q (n=50)': DynaQAgent(25, 4, planning_steps=50),\\n\",\n\"    'Dyna-Q+ (n=5)': DynaQPlusAgent(25, 4, planning_steps=5, kappa=0.001)\\n\",\n\"}\\n\",\n\"\\n\",\n\"# Training on simple gridworld first\\n\",\n\"print(\\\"\\\\n1. Training on Simple GridWorld:\\\")\\n\",\n\"simple_env = SimpleGridWorld(size=5)\\n\",\n\"\\n\",\n\"results = {}\\n\",\n\"n_episodes = 200\\n\",\n\"\\n\",\n\"for name, agent in agents.items():\\n\",\n\"    print(f\\\"\\\\nTraining {name}...\\\")\\n\",\n\"    episode_rewards = []\\n\",\n\"    \\n\",\n\"    for episode in range(n_episodes):\\n\",\n\"        reward, _ = agent.train_episode(simple_env, max_steps=100)\\n\",\n\"        episode_rewards.append(reward)\\n\",\n\"        \\n\",\n\"        if (episode + 1) % 50 == 0:\\n\",\n\"            avg_reward = np.mean(episode_rewards[-10:])\\n\",\n\"            stats = agent.get_statistics()\\n\",\n\"            print(f\\\"  Episode {episode+1}: Avg Reward = {avg_reward:.3f}, \\\"\\n\",\n\"                  f\\\"Direct Updates = {stats['direct_updates']}, \\\"\\n\",\n\"                  f\\\"Planning Updates = {stats['planning_updates']}\\\")\\n\",\n\"    \\n\",\n\"    results[name] = {\\n\",\n\"        'episode_rewards': agent.episode_rewards.copy(),\\n\",\n\"        'statistics': agent.get_statistics()\\n\",\n\"    }\\n\",\n\"\\n\",\n\"# Visualize learning curves\\n\",\n\"plt.figure(figsize=(15, 10))\\n\",\n\"\\n\",\n\"plt.subplot(2, 2, 1)\\n\",\n\"colors = ['blue', 'red', 'green', 'orange']\\n\",\n\"for i, (name, data) in enumerate(results.items()):\\n\",\n\"    rewards = data['episode_rewards']\\n\",\n\"    # Smooth the rewards\\n\",\n\"    smoothed = pd.Series(rewards).rolling(window=10).mean()\\n\",\n\"    plt.plot(smoothed, label=name, color=colors[i], linewidth=2)\\n\",\n\"\\n\",\n\"plt.title('Learning Performance Comparison')\\n\",\n\"plt.xlabel('Episode')\\n\",\n\"plt.ylabel('Episode Reward (Smoothed)')\\n\",\n\"plt.legend()\\n\",\n\"plt.grid(True, alpha=0.3)\\n\",\n\"\\n\",\n\"# Show update statistics\\n\",\n\"plt.subplot(2, 2, 2)\\n\",\n\"agent_names = list(results.keys())\\n\",\n\"direct_updates = [results[name]['statistics']['direct_updates'] for name in agent_names]\\n\",\n\"planning_updates = [results[name]['statistics']['planning_updates'] for name in agent_names]\\n\",\n\"\\n\",\n\"x = np.arange(len(agent_names))\\n\",\n\"width = 0.35\\n\",\n\"\\n\",\n\"plt.bar(x - width/2, direct_updates, width, label='Direct Updates', alpha=0.7)\\n\",\n\"plt.bar(x + width/2, planning_updates, width, label='Planning Updates', alpha=0.7)\\n\",\n\"\\n\",\n\"plt.title('Update Statistics')\\n\",\n\"plt.xlabel('Agent')\\n\",\n\"plt.ylabel('Number of Updates')\\n\",\n\"plt.xticks(x, agent_names, rotation=45)\\n\",\n\"plt.legend()\\n\",\n\"plt.grid(True, alpha=0.3)\\n\",\n\"\\n\",\n\"# Test on blocking maze\\n\",\n\"print(\\\"\\\\n2. Testing on Blocking Maze (Environment Change):\\\")\\n\",\n\"maze_env = BlockingMaze(change_episode=100)\\n\",\n\"\\n\",\n\"# Create fresh agents for maze test\\n\",\n\"maze_agents = {\\n\",\n\"    'Dyna-Q': DynaQAgent(maze_env.num_states, maze_env.num_actions, planning_steps=50),\\n\",\n\"    'Dyna-Q+': DynaQPlusAgent(maze_env.num_states, maze_env.num_actions, planning_steps=50, kappa=0.01)\\n\",\n\"}\\n\",\n\"\\n\",\n\"maze_results = {}\\n\",\n\"n_episodes = 300\\n\",\n\"\\n\",\n\"for name, agent in maze_agents.items():\\n\",\n\"    print(f\\\"\\\\nTraining {name} on Blocking Maze...\\\")\\n\",\n\"    # Reset environment episode counter\\n\",\n\"    maze_env.episode_count = 0\\n\",\n\"    \\n\",\n\"    for episode in range(n_episodes):\\n\",\n\"        reward, steps = agent.train_episode(maze_env, max_steps=3000)\\n\",\n\"        \\n\",\n\"        if episode in [50, 99, 150, 200, 250]:\\n\",\n\"            print(f\\\"  Episode {episode+1}: Reward = {reward:.1f}, Steps = {steps}\\\")\\n\",\n\"    \\n\",\n\"    maze_results[name] = {\\n\",\n\"        'episode_rewards': agent.episode_rewards.copy(),\\n\",\n\"        'episode_lengths': agent.episode_lengths.copy()\\n\",\n\"    }\\n\",\n\"\\n\",\n\"# Plot maze results\\n\",\n\"plt.subplot(2, 2, 3)\\n\",\n\"for name, data in maze_results.items():\\n\",\n\"    rewards = data['episode_rewards']\\n\",\n\"    smoothed = pd.Series(rewards).rolling(window=20).mean()\\n\",\n\"    plt.plot(smoothed, label=name, linewidth=2)\\n\",\n\"\\n\",\n\"plt.axvline(x=100, color='red', linestyle='--', alpha=0.7, label='Environment Change')\\n\",\n\"plt.title('Blocking Maze Performance')\\n\",\n\"plt.xlabel('Episode')\\n\",\n\"plt.ylabel('Episode Reward (Smoothed)')\\n\",\n\"plt.legend()\\n\",\n\"plt.grid(True, alpha=0.3)\\n\",\n\"\\n\",\n\"plt.subplot(2, 2, 4)\\n\",\n\"for name, data in maze_results.items():\\n\",\n\"    lengths = data['episode_lengths']\\n\",\n\"    smoothed = pd.Series(lengths).rolling(window=20).mean()\\n\",\n\"    plt.plot(smoothed, label=name, linewidth=2)\\n\",\n\"\\n\",\n\"plt.axvline(x=100, color='red', linestyle='--', alpha=0.7, label='Environment Change')\\n\",\n\"plt.title('Episode Length (Steps to Goal)')\\n\",\n\"plt.xlabel('Episode')\\n\",\n\"plt.ylabel('Episode Length (Smoothed)')\\n\",\n\"plt.legend()\\n\",\n\"plt.grid(True, alpha=0.3)\\n\",\n\"\\n\",\n\"plt.tight_layout()\\n\",\n\"plt.show()\\n\",\n\"\\n\",\n\"# Analysis and insights\\n\",\n\"print(\\\"\\\\n3. Key Insights from Dyna-Q Experiments:\\\")\\n\",\n\"print(\\\"\\\\nSimple GridWorld Results:\\\")\\n\",\n\"for name, data in results.items():\\n\",\n\"    final_performance = np.mean(data['episode_rewards'][-20:])\\n\",\n\"    stats = data['statistics']\\n\",\n\"    efficiency = stats['planning_updates'] / max(stats['direct_updates'], 1)\\n\",\n\"    print(f\\\"  {name}: Final Performance = {final_performance:.3f}, \\\"\\n\",\n\"          f\\\"Planning Efficiency = {efficiency:.1f}x\\\")\\n\",\n\"\\n\",\n\"print(\\\"\\\\nBlocking Maze Results (Adaptability):\\\")\\n\",\n\"for name, data in maze_results.items():\\n\",\n\"    # Performance before and after change\\n\",\n\"    before_change = np.mean(data['episode_rewards'][80:100])\\n\",\n\"    after_change = np.mean(data['episode_rewards'][120:140])\\n\",\n\"    adaptation_speed = after_change - min(data['episode_rewards'][100:120])\\n\",\n\"    \\n\",\n\"    print(f\\\"  {name}: Performance before change = {before_change:.3f}, \\\"\\n\",\n\"          f\\\"after change = {after_change:.3f}, adaptation = {adaptation_speed:.3f}\\\")\\n\",\n\"\\n\",\n\"print(\\\"\\\\nüìä Key Takeaways:\\\")\\n\",\n\"print(\\\"‚Ä¢ Dyna-Q achieves better sample efficiency through planning\\\")\\n\",\n\"print(\\\"‚Ä¢ More planning steps generally improve performance\\\")\\n\",\n\"print(\\\"‚Ä¢ Dyna-Q+ adapts better to environment changes\\\")\\n\",\n\"print(\\\"‚Ä¢ Model-based methods excel when environment is stable\\\")\\n\",\n\"\\n\",\n\"print(\\\"\\\\n‚úÖ Dyna-Q algorithm demonstration complete!\\\")\\n\",\n\"print(\\\"üìä Next: Monte Carlo Tree Search (MCTS)\\\")\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816e0a69",
   "metadata": {},
   "source": [
    "# Section 5: Monte Carlo Tree Search (MCTS)\n",
    "\n",
    "Monte Carlo Tree Search is a powerful planning algorithm that has achieved remarkable success in games like Go and has been extended to general reinforcement learning problems.\n",
    "\n",
    "## 5.1 Theoretical Foundation\n",
    "\n",
    "MCTS combines:\n",
    "- **Tree Search**: Systematic exploration of possible future states\n",
    "- **Monte Carlo Simulation**: Random rollouts to estimate value\n",
    "- **Multi-Armed Bandit**: UCB for action selection in tree nodes\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Selection**: Navigate from root to leaf using bandit strategy\n",
    "2. **Expansion**: Add one or more child nodes\n",
    "3. **Simulation**: Random rollout from new node\n",
    "4. **Backpropagation**: Update all nodes on path with result\n",
    "\n",
    "### UCB1 Formula for Node Selection:\n",
    "\n",
    "$$UCB1(i) = \\overline{X_i} + C\\sqrt{\\frac{\\ln n}{n_i}}$$\n",
    "\n",
    "Where:\n",
    "- $\\overline{X_i}$ = average reward of action i\n",
    "- $n_i$ = number of times action i was selected\n",
    "- $n$ = total number of selections\n",
    "- $C$ = exploration parameter\n",
    "\n",
    "### MCTS in Model-Based RL:\n",
    "\n",
    "MCTS can be used with learned models to perform sophisticated planning by building search trees that explore promising action sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397012bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTSNode:\n    def __init__(self, state, parent=None, action=None):\n        self.state = state\n        self.parent = parent\n        self.action = action\n        self.children = {}\n        self.visits = 0\n        self.total_reward = 0.0\n        self.untried_actions = None\n    def is_fully_expanded(self):\n        return len(self.untried_actions) == 0\n    def is_terminal(self):\n        return len(self.children) == 0 and self.visits > 0 and self.untried_actions is not None and len(self.untried_actions) == 0\n    def get_ucb_value(self, exploration_weight=1.0):\n        if self.visits == 0:\n            return float('inf')\n        exploitation = self.total_reward / self.visits\n        exploration = exploration_weight * np.sqrt(np.log(self.parent.visits) / self.visits)\n        return exploitation + exploration\n    def select_child(self, exploration_weight=1.0):\n        return max(self.children.values(), \n                  key=lambda child: child.get_ucb_value(exploration_weight))\n    def expand(self, action, new_state):\n        if action in self.untried_actions:\n            self.untried_actions.remove(action)\n        child = MCTSNode(state=new_state, parent=self, action=action)\n        self.children[action] = child\n        return child\n    def update(self, reward):\n        self.visits += 1\n        self.total_reward += reward\n    def get_best_action(self):\n        if not self.children:\n            return None\n        return max(self.children.items(), \n                  key=lambda item: item[1].visits)[0]\nclass MCTS:\n    def __init__(self, model, num_actions, exploration_weight=1.0, max_depth=50):\n        self.model = model\n        self.num_actions = num_actions\n        self.exploration_weight = exploration_weight\n        self.max_depth = max_depth\n        self.gamma = 0.95\n    def search(self, root_state, num_simulations=1000):\n        root = MCTSNode(root_state)\n        root.untried_actions = list(range(self.num_actions))\n        for _ in range(num_simulations):\n            leaf = self._select_leaf(root)\n            if leaf.untried_actions and len(leaf.untried_actions) > 0:\n                action = np.random.choice(leaf.untried_actions)\n                next_state, reward, done = self._simulate_step(leaf.state, action)\n                child = leaf.expand(action, next_state)\n                child.untried_actions = list(range(self.num_actions)) if not done else []\n                leaf = child\n            simulation_reward = self._simulate_rollout(leaf.state)\n            self._backpropagate(leaf, simulation_reward)\n        return root.get_best_action(), root\n    def _select_leaf(self, node):\n        while node.is_fully_expanded() and node.children:\n            node = node.select_child(self.exploration_weight)\n        return node\n    def _simulate_step(self, state, action):\n        if hasattr(self.model, 'predict'):\n            next_state, reward = self.model.predict(state, action)\n            done = False\n        else:\n            if (state, action) in self.model.transitions:\n                next_state = self.model.transitions[(state, action)]\n                reward = self.model.rewards[(state, action)]\n                done = False\n            else:\n                next_state = state\n                reward = 0.0\n                done = True\n        return next_state, reward, done\n    def _simulate_rollout(self, state, max_depth=None):\n        if max_depth is None:\n            max_depth = self.max_depth\n        total_reward = 0.0\n        current_state = state\n        discount = 1.0\n        for depth in range(max_depth):\n            action = np.random.randint(self.num_actions)\n            next_state, reward, done = self._simulate_step(current_state, action)\n            total_reward += discount * reward\n            discount *= self.gamma\n            if done:\n                break\n            current_state = next_state\n        return total_reward\n    def _backpropagate(self, node, reward):\n        while node is not None:\n            node.update(reward)\n            node = node.parent\n            reward *= self.gamma\nclass MCTSAgent:\n    def __init__(self, model, num_states, num_actions, num_simulations=1000, \n                 exploration_weight=1.0):\n        self.model = model\n        self.num_states = num_states\n        self.num_actions = num_actions\n        self.mcts = MCTS(model, num_actions, exploration_weight)\n        self.num_simulations = num_simulations\n        self.search_times = []\n        self.tree_sizes = []\n        self.episode_rewards = []\n    def select_action(self, state, deterministic=False):\n        import time\n        start_time = time.time()\n        best_action, root = self.mcts.search(state, self.num_simulations)\n        search_time = time.time() - start_time\n        tree_size = self._count_nodes(root)\n        self.search_times.append(search_time)\n        self.tree_sizes.append(tree_size)\n        return best_action if best_action is not None else np.random.randint(self.num_actions)\n    def _count_nodes(self, node):\n        if not node.children:\n            return 1\n        return 1 + sum(self._count_nodes(child) for child in node.children.values())\n    def train_episode(self, env, max_steps=200):\n        state = env.reset()\n        total_reward = 0\n        steps = 0\n        for step in range(max_steps):\n            action = self.select_action(state)\n            next_state, reward, done = env.step(action)\n            total_reward += reward\n            steps += 1\n            if done:\n                break\n            state = next_state\n        self.episode_rewards.append(total_reward)\n        return total_reward, steps\n    def get_statistics(self):\n        return {\n            'avg_search_time': np.mean(self.search_times) if self.search_times else 0,\n            'avg_tree_size': np.mean(self.tree_sizes) if self.tree_sizes else 0,\n            'total_searches': len(self.search_times),\n            'avg_episode_reward': np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0\n        }\nprint(\"Monte Carlo Tree Search (MCTS) Demonstration\")\nprint(\"=\" * 50)\nprint(\"\\n1. Setting up environment and learned model...\")\nenv = SimpleGridWorld(size=6)\ntabular_model = TabularModel(env.num_states, env.num_actions)\nprint(\"Training tabular model...\")\nfor episode in range(100):\n    state = env.reset()\n    for step in range(50):\n        action = np.random.randint(env.num_actions)\n        next_state, reward, done = env.step(action)\n        tabular_model.update(state, action, reward, next_state)\n        if done:\n            break\n        state = next_state\nmcts_agent = MCTSAgent(\n    model=tabular_model,\n    num_states=env.num_states,\n    num_actions=env.num_actions,\n    num_simulations=200,\n    exploration_weight=1.4\n)\nprint(f\"Model trained with {len(tabular_model.transitions)} transitions\")\nprint(\"\\n2. Testing MCTS performance...\")\nn_test_episodes = 20\nepisode_rewards = []\nepisode_lengths = []\nfor episode in range(n_test_episodes):\n    reward, length = mcts_agent.train_episode(env, max_steps=100)\n    episode_rewards.append(reward)\n    episode_lengths.append(length)\n    if (episode + 1) % 5 == 0:\n        avg_reward = np.mean(episode_rewards[-5:])\n        avg_length = np.mean(episode_lengths[-5:])\n        stats = mcts_agent.get_statistics()\n        print(f\"Episodes {episode-4}-{episode+1}: Avg Reward = {avg_reward:.2f}, \"\n              f\"Avg Length = {avg_length:.1f}, Avg Search Time = {stats['avg_search_time']:.4f}s\")\nplt.figure(figsize=(15, 10))\nplt.subplot(2, 3, 1)\nplt.plot(episode_rewards, 'b-', linewidth=2, label='Episode Reward')\nplt.axhline(y=np.mean(episode_rewards), color='r', linestyle='--', alpha=0.7, label='Average')\nplt.title('MCTS Episode Performance')\nplt.xlabel('Episode')\nplt.ylabel('Reward')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 2)\nplt.plot(episode_lengths, 'g-', linewidth=2, label='Episode Length')\nplt.axhline(y=np.mean(episode_lengths), color='r', linestyle='--', alpha=0.7, label='Average')\nplt.title('Episode Lengths')\nplt.xlabel('Episode')\nplt.ylabel('Steps to Goal')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 3)\nsearch_times = mcts_agent.search_times\nplt.plot(search_times, 'purple', linewidth=2, label='Search Time')\nplt.axhline(y=np.mean(search_times), color='r', linestyle='--', alpha=0.7, label='Average')\nplt.title('MCTS Search Times')\nplt.xlabel('Search Number')\nplt.ylabel('Time (seconds)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 4)\ntree_sizes = mcts_agent.tree_sizes\nplt.plot(tree_sizes, 'orange', linewidth=2, label='Tree Size')\nplt.axhline(y=np.mean(tree_sizes), color='r', linestyle='--', alpha=0.7, label='Average')\nplt.title('MCTS Tree Sizes')\nplt.xlabel('Search Number')\nplt.ylabel('Number of Nodes')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 5)\nif len(search_times) > 0 and len(tree_sizes) > 0:\n    plt.scatter(tree_sizes, search_times, alpha=0.6, c='red', s=30)\n    if len(tree_sizes) > 1:\n        z = np.polyfit(tree_sizes, search_times, 1)\n        p = np.poly1d(z)\n        plt.plot(sorted(tree_sizes), p(sorted(tree_sizes)), \"r--\", alpha=0.8, linewidth=2)\nplt.title('Search Time vs Tree Size')\nplt.xlabel('Tree Size (nodes)')\nplt.ylabel('Search Time (seconds)')\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 6)\nrandom_rewards = []\nfor _ in range(n_test_episodes):\n    state = env.reset()\n    total_reward = 0\n    for step in range(100):\n        action = np.random.randint(env.num_actions)\n        next_state, reward, done = env.step(action)\n        total_reward += reward\n        if done:\n            break\n        state = next_state\n    random_rewards.append(total_reward)\ncomparison_data = [episode_rewards, random_rewards]\nlabels = ['MCTS', 'Random']\nplt.boxplot(comparison_data, labels=labels)\nplt.title('Performance Comparison')\nplt.ylabel('Episode Reward')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint(f\"\\n3. MCTS Performance Analysis:\")\nfinal_stats = mcts_agent.get_statistics()\nprint(f\"Average Episode Reward: {np.mean(episode_rewards):.3f} ¬± {np.std(episode_rewards):.3f}\")\nprint(f\"Average Episode Length: {np.mean(episode_lengths):.1f} ¬± {np.std(episode_lengths):.1f}\")\nprint(f\"Average Search Time: {final_stats['avg_search_time']:.4f} seconds\")\nprint(f\"Average Tree Size: {final_stats['avg_tree_size']:.1f} nodes\")\nprint(f\"Total MCTS Searches: {final_stats['total_searches']}\")\nprint(f\"\\nRandom Policy Baseline:\")\nprint(f\"Average Episode Reward: {np.mean(random_rewards):.3f} ¬± {np.std(random_rewards):.3f}\")\nimprovement = (np.mean(episode_rewards) - np.mean(random_rewards)) / np.mean(random_rewards) * 100\nprint(f\"\\nMCTS Improvement over Random: {improvement:.1f}%\")\nprint(f\"\\nüìä Key MCTS Insights:\")\nprint(\"‚Ä¢ MCTS provides sophisticated planning through tree search\")\nprint(\"‚Ä¢ UCB balances exploration and exploitation in tree nodes\")\nprint(\"‚Ä¢ Performance scales with number of simulations\")\nprint(\"‚Ä¢ Computational cost grows with search depth and simulations\")\nprint(\"‚Ä¢ Effective for discrete action spaces with learned models\")\nprint(f\"\\n‚úÖ MCTS demonstration complete!\")\nprint(\"üìä Next: Model Predictive Control (MPC)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8db3d2",
   "metadata": {},
   "source": [
    "# Section 6: Model Predictive Control (MPC)\n",
    "\n",
    "Model Predictive Control is a control strategy that uses a model to predict future behavior and optimizes a sequence of control actions over a finite horizon.\n",
    "\n",
    "## 6.1 Theoretical Foundation\n",
    "\n",
    "MPC operates on the principle of **receding horizon control**:\n",
    "\n",
    "1. **Prediction**: Use model to predict future states over horizon H\n",
    "2. **Optimization**: Solve optimal control problem over this horizon\n",
    "3. **Execution**: Apply only the first control action\n",
    "4. **Recede**: Shift horizon forward and repeat\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "- **Prediction Model**: $\\hat{s}_{t+1} = f(s_t, a_t)$\n",
    "- **Cost Function**: $J = \\sum_{k=0}^{H-1} c(s_{t+k}, a_{t+k}) + V_f(s_{t+H})$\n",
    "- **Constraints**: State and action constraints\n",
    "- **Terminal Cost**: $V_f(s_{t+H})$ (optional)\n",
    "\n",
    "### Advantages:\n",
    "- Handles constraints naturally\n",
    "- Provides explicit planning horizon\n",
    "- Can incorporate uncertainty\n",
    "- Works with nonlinear models\n",
    "\n",
    "### MPC in RL Context:\n",
    "- Use learned dynamics models\n",
    "- Optimize with gradient-based or sampling methods\n",
    "- Can incorporate learned value functions as terminal costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80395007",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MPCController:\n    def __init__(self, model, num_actions, horizon=10, num_samples=1000, \n                 temperature=1.0, elite_ratio=0.1):\n        self.model = model\n        self.num_actions = num_actions\n        self.horizon = horizon\n        self.num_samples = num_samples\n        self.temperature = temperature\n        self.elite_ratio = elite_ratio\n        self.elite_size = max(1, int(num_samples * elite_ratio))\n        self.optimization_costs = []\n        self.episode_rewards = []\n    def cross_entropy_optimization(self, initial_state):\n        action_means = np.zeros((self.horizon, self.num_actions))\n        action_stds = np.ones((self.horizon, self.num_actions))\n        best_cost = float('inf')\n        best_actions = None\n        for iteration in range(10):\n            action_sequences = []\n            costs = []\n            for _ in range(self.num_samples):\n                actions = []\n                for h in range(self.horizon):\n                    probs = np.exp(action_means[h] / self.temperature)\n                    probs = probs / np.sum(probs)\n                    action = np.random.choice(self.num_actions, p=probs)\n                    actions.append(action)\n                action_sequences.append(actions)\n                cost = self.evaluate_sequence(initial_state, actions)\n                costs.append(cost)\n            elite_indices = np.argsort(costs)[:self.elite_size]\n            elite_actions = [action_sequences[i] for i in elite_indices]\n            for h in range(self.horizon):\n                elite_actions_h = [seq[h] for seq in elite_actions]\n                for a in range(self.num_actions):\n                    count = elite_actions_h.count(a)\n                    action_means[h, a] = count / len(elite_actions_h)\n                action_means[h] = np.log(action_means[h] + 1e-8)\n            min_cost = min(costs)\n            if min_cost < best_cost:\n                best_cost = min_cost\n                best_actions = action_sequences[costs.index(min_cost)]\n        self.optimization_costs.append(best_cost)\n        return best_actions\n    def random_shooting(self, initial_state):\n        best_cost = float('inf')\n        best_actions = None\n        for _ in range(self.num_samples):\n            actions = [np.random.randint(self.num_actions) for _ in range(self.horizon)]\n            cost = self.evaluate_sequence(initial_state, actions)\n            if cost < best_cost:\n                best_cost = cost\n                best_actions = actions\n        self.optimization_costs.append(best_cost)\n        return best_actions\n    def evaluate_sequence(self, initial_state, actions):\n        state = initial_state\n        total_cost = 0.0\n        discount = 1.0\n        for action in actions:\n            if hasattr(self.model, 'predict'):\n                next_state, reward = self.model.predict(state, action)\n            else:\n                if (state, action) in self.model.transitions:\n                    next_state = self.model.transitions[(state, action)]\n                    reward = self.model.rewards[(state, action)]\n                else:\n                    next_state = state\n                    reward = 0.0\n            cost = -reward\n            total_cost += discount * cost\n            discount *= 0.95\n            state = next_state\n        return total_cost\n    def select_action(self, state, method='cross_entropy'):\n        if method == 'cross_entropy':\n            action_sequence = self.cross_entropy_optimization(state)\n        else:\n            action_sequence = self.random_shooting(state)\n        return action_sequence[0] if action_sequence else np.random.randint(self.num_actions)\nclass MPCAgent:\n    def __init__(self, model, num_states, num_actions, horizon=10, method='cross_entropy'):\n        self.model = model\n        self.num_states = num_states\n        self.num_actions = num_actions\n        self.controller = MPCController(model, num_actions, horizon=horizon)\n        self.method = method\n        self.episode_rewards = []\n        self.planning_costs = []\n    def train_episode(self, env, max_steps=200):\n        state = env.reset()\n        total_reward = 0\n        steps = 0\n        for step in range(max_steps):\n            action = self.controller.select_action(state, self.method)\n            next_state, reward, done = env.step(action)\n            total_reward += reward\n            steps += 1\n            if done:\n                break\n            state = next_state\n        self.episode_rewards.append(total_reward)\n        if self.controller.optimization_costs:\n            self.planning_costs.extend(self.controller.optimization_costs)\n        return total_reward, steps\n    def get_statistics(self):\n        return {\n            'avg_episode_reward': np.mean(self.episode_rewards[-10:]) if self.episode_rewards else 0,\n            'avg_planning_cost': np.mean(self.planning_costs[-10:]) if self.planning_costs else 0,\n            'total_episodes': len(self.episode_rewards)\n        }\nprint(\"Model Predictive Control (MPC) Demonstration\")\nprint(\"=\" * 50)\nprint(\"\\n1. Setting up MPC with learned model...\")\nenv = SimpleGridWorld(size=5)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nneural_model = NeuralModel(env.num_states, env.num_actions, hidden_size=64).to(device)\ntrainer = ModelTrainer(neural_model, env, device=device)\nprint(\"Training neural model for MPC...\")\ntrainer.train(num_episodes=200, verbose=False)\nagents = {\n    'MPC-CEM': MPCAgent(neural_model, env.num_states, env.num_actions, horizon=8, method='cross_entropy'),\n    'MPC-RS': MPCAgent(neural_model, env.num_states, env.num_actions, horizon=8, method='random_shooting')\n}\nprint(\"\\n2. Testing MPC performance...\")\nn_episodes = 15\nresults = {}\nfor name, agent in agents.items():\n    print(f\"\\nTesting {name}...\")\n    episode_rewards = []\n    episode_lengths = []\n    for episode in range(n_episodes):\n        reward, length = agent.train_episode(env, max_steps=100)\n        episode_rewards.append(reward)\n        episode_lengths.append(length)\n        if (episode + 1) % 5 == 0:\n            avg_reward = np.mean(episode_rewards[-5:])\n            print(f\"  Episodes {episode-4}-{episode+1}: Avg Reward = {avg_reward:.2f}\")\n    results[name] = {\n        'episode_rewards': episode_rewards,\n        'episode_lengths': episode_lengths,\n        'statistics': agent.get_statistics()\n    }\nplt.figure(figsize=(15, 10))\nplt.subplot(2, 3, 1)\nfor name, data in results.items():\n    rewards = data['episode_rewards']\n    plt.plot(rewards, linewidth=2, label=name, marker='o', markersize=4)\nplt.title('MPC Performance Comparison')\nplt.xlabel('Episode')\nplt.ylabel('Episode Reward')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 2)\nfor name, data in results.items():\n    lengths = data['episode_lengths']\n    plt.plot(lengths, linewidth=2, label=name, marker='s', markersize=4)\nplt.title('Episode Lengths')\nplt.xlabel('Episode')\nplt.ylabel('Steps to Goal')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 3)\nreward_data = [results[name]['episode_rewards'] for name in results.keys()]\nlabels = list(results.keys())\nplt.boxplot(reward_data, labels=labels)\nplt.title('Reward Distribution')\nplt.ylabel('Episode Reward')\nplt.xticks(rotation=45)\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 4)\nif 'MPC-CEM' in results:\n    agent = agents['MPC-CEM']\n    if agent.planning_costs:\n        plt.plot(agent.planning_costs, 'purple', linewidth=2, alpha=0.7)\n        plt.axhline(y=np.mean(agent.planning_costs), color='red', linestyle='--', \n                   alpha=0.7, label=f'Mean: {np.mean(agent.planning_costs):.2f}')\n        plt.title('MPC-CEM Planning Costs')\n        plt.xlabel('Planning Step')\n        plt.ylabel('Cost')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 5)\nprint(\"\\n3. Analyzing effect of planning horizon...\")\nhorizon_results = {}\nhorizons = [3, 5, 8, 12]\nfor h in horizons:\n    agent = MPCAgent(neural_model, env.num_states, env.num_actions, horizon=h, method='cross_entropy')\n    rewards = []\n    for episode in range(5):\n        reward, _ = agent.train_episode(env, max_steps=100)\n        rewards.append(reward)\n    horizon_results[h] = np.mean(rewards)\nhorizons_list = list(horizon_results.keys())\nperformance_list = list(horizon_results.values())\nplt.bar(horizons_list, performance_list, alpha=0.7, color='skyblue', edgecolor='black')\nplt.title('Performance vs Planning Horizon')\nplt.xlabel('Planning Horizon')\nplt.ylabel('Average Episode Reward')\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 6)\nmethod_names = list(results.keys())\navg_rewards = [np.mean(results[name]['episode_rewards']) for name in method_names]\navg_lengths = [np.mean(results[name]['episode_lengths']) for name in method_names]\nx = np.arange(len(method_names))\nwidth = 0.35\nplt.bar(x - width/2, avg_rewards, width, label='Avg Reward', alpha=0.7)\nplt.bar(x + width/2, [l/10 for l in avg_lengths], width, label='Avg Length/10', alpha=0.7)\nplt.title('MPC Method Comparison')\nplt.xlabel('Method')\nplt.ylabel('Performance')\nplt.xticks(x, method_names, rotation=45)\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint(f\"\\n4. MPC Analysis Summary:\")\nfor name, data in results.items():\n    stats = data['statistics']\n    print(f\"\\n{name}:\")\n    print(f\"  Average Episode Reward: {np.mean(data['episode_rewards']):.3f} ¬± {np.std(data['episode_rewards']):.3f}\")\n    print(f\"  Average Episode Length: {np.mean(data['episode_lengths']):.1f} ¬± {np.std(data['episode_lengths']):.1f}\")\n    if stats['avg_planning_cost'] > 0:\n        print(f\"  Average Planning Cost: {stats['avg_planning_cost']:.3f}\")\nprint(f\"\\nHorizon Analysis:\")\nfor h, perf in horizon_results.items():\n    print(f\"  Horizon {h}: {perf:.3f} average reward\")\nprint(f\"\\nüìä Key MPC Insights:\")\nprint(\"‚Ä¢ MPC provides principled planning with explicit horizons\")\nprint(\"‚Ä¢ Cross-Entropy Method often outperforms random shooting\")\nprint(\"‚Ä¢ Longer horizons generally improve performance but increase computation\")\nprint(\"‚Ä¢ MPC naturally handles constraints and can incorporate uncertainty\")\nprint(\"‚Ä¢ Effective for continuous control and discrete planning problems\")\nprint(f\"\\n‚úÖ MPC demonstration complete!\")\nprint(\"üéØ Final section: Comprehensive comparison and conclusions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec553db",
   "metadata": {},
   "source": [
    "# Section 7: Advanced Model-Based Methods and Modern Approaches\n",
    "\n",
    "## 7.1 Modern Neural Model-Based Methods\n",
    "\n",
    "### Model-Based Meta-Learning\n",
    "- **MAML for Model Learning**: Learning models that can quickly adapt to new environments\n",
    "- **Gradient-Based Meta-Learning**: Using gradients to update model parameters efficiently\n",
    "\n",
    "### Uncertainty-Aware Models\n",
    "- **Bayesian Neural Networks**: Capturing epistemic uncertainty in dynamics\n",
    "- **Ensemble Methods**: Multiple models for uncertainty quantification\n",
    "- **Dropout-Based Uncertainty**: Using Monte Carlo dropout for uncertainty estimation\n",
    "\n",
    "### Advanced Planning Methods\n",
    "- **Differentiable Planning**: End-to-end training of planning modules\n",
    "- **Learned Optimizers**: Using neural networks as optimizers for planning\n",
    "- **Hierarchical Planning**: Multi-level planning for complex tasks\n",
    "\n",
    "## 7.2 State-of-the-Art Methods\n",
    "\n",
    "### Model-Based Policy Optimization (MBPO)\n",
    "- Combines model-based and model-free learning\n",
    "- Uses learned models to generate synthetic data\n",
    "- Applies model-free algorithms to mixed real and synthetic data\n",
    "\n",
    "### Dreamer and DreamerV2\n",
    "- World models with latent state representations\n",
    "- Planning in latent space\n",
    "- Actor-critic learning within the world model\n",
    "\n",
    "### MuZero\n",
    "- Combines MCTS with learned models\n",
    "- No explicit environment model\n",
    "- Learns value, policy, and reward predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4256764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comprehensive Model-Based Reinforcement Learning Analysis\")\nprint(\"=\" * 60)\nclass ModelBasedComparisonFramework:\n    def __init__(self):\n        self.results = {}\n        self.environments = {}\n        self.methods = {}\n    def add_environment(self, name, env):\n        self.environments[name] = env\n    def add_method(self, name, method_class, **kwargs):\n        self.methods[name] = {'class': method_class, 'kwargs': kwargs}\n    def run_comparison(self, n_episodes=50, max_steps=200, n_runs=3):\n        print(f\"\\nRunning comprehensive comparison...\")\n        print(f\"Episodes per run: {n_episodes}, Runs per method: {n_runs}\")\n        for env_name, env in self.environments.items():\n            print(f\"\\nüåç Environment: {env_name}\")\n            self.results[env_name] = {}\n            if hasattr(env, 'num_states'):\n                tabular_model = TabularModel(env.num_states, env.num_actions)\n                neural_model = NeuralModel(env.num_states, env.num_actions, hidden_size=32)\n                self._train_models(env, tabular_model, neural_model)\n            for method_name, method_info in self.methods.items():\n                print(f\"  üìä Testing {method_name}...\")\n                method_results = []\n                for run in range(n_runs):\n                    kwargs = method_info['kwargs'].copy()\n                    if 'model' in kwargs:\n                        if kwargs['model'] == 'tabular':\n                            kwargs['model'] = tabular_model\n                        elif kwargs['model'] == 'neural':\n                            kwargs['model'] = neural_model\n                    try:\n                        agent = method_info['class'](**kwargs)\n                        episode_rewards = []\n                        episode_lengths = []\n                        for episode in range(n_episodes):\n                            reward, length = agent.train_episode(env, max_steps=max_steps)\n                            episode_rewards.append(reward)\n                            episode_lengths.append(length)\n                        method_results.append({\n                            'episode_rewards': episode_rewards,\n                            'episode_lengths': episode_lengths,\n                            'final_performance': np.mean(episode_rewards[-10:]),\n                            'learning_efficiency': self._calculate_learning_efficiency(episode_rewards),\n                            'statistics': agent.get_statistics() if hasattr(agent, 'get_statistics') else {}\n                        })\n                    except Exception as e:\n                        print(f\"    ‚ö†Ô∏è Error with {method_name}: {str(e)}\")\n                        continue\n                if method_results:\n                    self.results[env_name][method_name] = self._aggregate_results(method_results)\n                    avg_performance = self.results[env_name][method_name]['avg_final_performance']\n                    std_performance = self.results[env_name][method_name]['std_final_performance']\n                    print(f\"    ‚úÖ Final Performance: {avg_performance:.3f} ¬± {std_performance:.3f}\")\n    def _train_models(self, env, tabular_model, neural_model, episodes=100):\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        neural_model = neural_model.to(device)\n        trainer = ModelTrainer(neural_model, env, device=device)\n        trainer.train(num_episodes=episodes, verbose=False)\n        for episode in range(episodes):\n            state = env.reset()\n            for step in range(50):\n                action = np.random.randint(env.num_actions)\n                next_state, reward, done = env.step(action)\n                tabular_model.update(state, action, reward, next_state)\n                if done:\n                    break\n                state = next_state\n    def _calculate_learning_efficiency(self, rewards):\n        return np.sum(rewards) / len(rewards)\n    def _aggregate_results(self, method_results):\n        final_performances = [r['final_performance'] for r in method_results]\n        learning_efficiencies = [r['learning_efficiency'] for r in method_results]\n        return {\n            'avg_final_performance': np.mean(final_performances),\n            'std_final_performance': np.std(final_performances),\n            'avg_learning_efficiency': np.mean(learning_efficiencies),\n            'std_learning_efficiency': np.std(learning_efficiencies),\n            'all_results': method_results\n        }\n    def visualize_results(self):\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        fig.suptitle('Model-Based RL Comprehensive Comparison', fontsize=16)\n        ax1 = axes[0, 0]\n        for env_name, env_results in self.results.items():\n            methods = list(env_results.keys())\n            performances = [env_results[m]['avg_final_performance'] for m in methods]\n            errors = [env_results[m]['std_final_performance'] for m in methods]\n            x = np.arange(len(methods))\n            ax1.bar(x, performances, yerr=errors, alpha=0.7, \n                   label=env_name, capsize=5)\n            ax1.set_xticks(x)\n            ax1.set_xticklabels(methods, rotation=45, ha='right')\n        ax1.set_title('Final Performance Comparison')\n        ax1.set_ylabel('Average Episode Reward')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        ax2 = axes[0, 1]\n        for env_name, env_results in self.results.items():\n            methods = list(env_results.keys())\n            efficiencies = [env_results[m]['avg_learning_efficiency'] for m in methods]\n            errors = [env_results[m]['std_learning_efficiency'] for m in methods]\n            x = np.arange(len(methods))\n            ax2.bar(x, efficiencies, yerr=errors, alpha=0.7,\n                   label=env_name, capsize=5)\n            ax2.set_xticks(x)\n            ax2.set_xticklabels(methods, rotation=45, ha='right')\n        ax2.set_title('Learning Efficiency')\n        ax2.set_ylabel('Average Reward over Episodes')\n        ax2.legend()\n        ax2.grid(True, alpha=0.3)\n        ax3 = axes[1, 0]\n        if self.results:\n            env_name = list(self.results.keys())[0]\n            env_results = self.results[env_name]\n            for method_name, method_data in env_results.items():\n                if method_data['all_results']:\n                    all_rewards = [r['episode_rewards'] for r in method_data['all_results']]\n                    if all_rewards:\n                        avg_rewards = np.mean(all_rewards, axis=0)\n                        smoothed = pd.Series(avg_rewards).rolling(window=5).mean()\n                        ax3.plot(smoothed, label=method_name, linewidth=2)\n            ax3.set_title(f'Learning Curves - {env_name}')\n            ax3.set_xlabel('Episode')\n            ax3.set_ylabel('Episode Reward (Smoothed)')\n            ax3.legend()\n            ax3.grid(True, alpha=0.3)\n        ax4 = axes[1, 1]\n        ax4.text(0.5, 0.5, 'Method Characteristics:\\\\n\\\\n'\n                          '‚Ä¢ Sample Efficiency\\\\n'\n                          '‚Ä¢ Computational Cost\\\\n'\n                          '‚Ä¢ Adaptability\\\\n'\n                          '‚Ä¢ Theoretical Guarantees\\\\n'\n                          '‚Ä¢ Implementation Complexity',\n                ha='center', va='center', transform=ax4.transAxes,\n                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n        ax4.set_title('Key Method Properties')\n        ax4.axis('off')\n        plt.tight_layout()\n        plt.show()\n    def print_summary(self):\n        print(f\"\\nüìã COMPREHENSIVE ANALYSIS SUMMARY\")\n        print(\"=\" * 60)\n        for env_name, env_results in self.results.items():\n            print(f\"\\nüåç Environment: {env_name}\")\n            print(\"-\" * 40)\n            sorted_methods = sorted(env_results.items(), \n                                  key=lambda x: x[1]['avg_final_performance'], \n                                  reverse=True)\n            print(\"Performance Ranking:\")\n            for i, (method_name, data) in enumerate(sorted_methods, 1):\n                perf = data['avg_final_performance']\n                std = data['std_final_performance']\n                eff = data['avg_learning_efficiency']\n                print(f\"  {i}. {method_name}: {perf:.3f} ¬± {std:.3f} \"\n                      f\"(efficiency: {eff:.3f})\")\nframework = ModelBasedComparisonFramework()\nframework.add_environment(\"GridWorld-5x5\", SimpleGridWorld(size=5))\nframework.add_method(\"Q-Learning\", \n                    lambda **kwargs: DynaQAgent(25, 4, planning_steps=0),\n                    num_states=25, num_actions=4)\nframework.add_method(\"Dyna-Q(5)\", \n                    lambda **kwargs: DynaQAgent(25, 4, planning_steps=5),\n                    num_states=25, num_actions=4)\nframework.add_method(\"Dyna-Q(20)\", \n                    lambda **kwargs: DynaQAgent(25, 4, planning_steps=20),\n                    num_states=25, num_actions=4)\nframework.add_method(\"MCTS\", \n                    lambda **kwargs: MCTSAgent(kwargs['model'], 25, 4, num_simulations=100),\n                    model='tabular')\nframework.add_method(\"MPC-CEM\", \n                    lambda **kwargs: MPCAgent(kwargs['model'], 25, 4, horizon=5, method='cross_entropy'),\n                    model='neural')\nframework.run_comparison(n_episodes=30, n_runs=2)\nframework.visualize_results()\nframework.print_summary()\nprint(f\"\\nüéØ FINAL CONCLUSIONS: Model-Based Reinforcement Learning\")\nprint(\"=\" * 60)\nprint(f\"\\nüìä Key Findings:\")\nprint(\"1. Sample Efficiency: Model-based methods generally require fewer environment interactions\")\nprint(\"2. Planning Benefits: More planning steps typically improve performance\")  \nprint(\"3. Model Quality: Better models lead to better planning performance\")\nprint(\"4. Computational Trade-offs: Planning methods trade computation for sample efficiency\")\nprint(\"5. Adaptability: Some methods (Dyna-Q+) handle environment changes better\")\nprint(f\"\\nüî¨ Method Characteristics:\")\nprint(\"‚Ä¢ Tabular Models: Simple, exact, limited to discrete spaces\")\nprint(\"‚Ä¢ Neural Models: Flexible, scalable, but require careful training\")\nprint(\"‚Ä¢ Dyna-Q: Simple integration of learning and planning\")\nprint(\"‚Ä¢ MCTS: Sophisticated tree search, good for discrete actions\")\nprint(\"‚Ä¢ MPC: Principled control theory approach, handles constraints\")\nprint(f\"\\nüí° Practical Recommendations:\")\nprint(\"1. Use model-based methods when sample efficiency is critical\")\nprint(\"2. Choose tabular models for small discrete environments\")\nprint(\"3. Use neural models for high-dimensional or continuous spaces\")\nprint(\"4. Apply Dyna-Q for balanced learning and planning\")\nprint(\"5. Use MCTS for complex decision trees\")\nprint(\"6. Apply MPC when constraints are important\")\nprint(f\"\\nüöÄ Future Directions:\")\nprint(\"‚Ä¢ Uncertainty-aware planning\")\nprint(\"‚Ä¢ Hierarchical model-based RL\")\nprint(\"‚Ä¢ Meta-learning for quick model adaptation\")\nprint(\"‚Ä¢ Differentiable planning modules\")\nprint(\"‚Ä¢ Hybrid model-free and model-based methods\")\nprint(f\"\\n‚úÖ MODEL-BASED REINFORCEMENT LEARNING COMPLETE!\")\nprint(\"üéì You now have a comprehensive understanding of:\")\nprint(\"   ‚Ä¢ Theoretical foundations and mathematical formulations\")\nprint(\"   ‚Ä¢ Environment model learning (tabular and neural)\")\nprint(\"   ‚Ä¢ Classical planning with learned models\")\nprint(\"   ‚Ä¢ Dyna-Q algorithm for integrated learning and planning\")\nprint(\"   ‚Ä¢ Monte Carlo Tree Search (MCTS) for sophisticated planning\")\nprint(\"   ‚Ä¢ Model Predictive Control (MPC) for constrained optimization\")\nprint(\"   ‚Ä¢ Modern approaches and state-of-the-art methods\")\nprint(\"   ‚Ä¢ Comparative analysis and practical guidelines\")\nprint(f\"\\nüåü Congratulations on completing this comprehensive study!\")\nprint(\"üìö Continue exploring advanced topics in model-based RL!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}