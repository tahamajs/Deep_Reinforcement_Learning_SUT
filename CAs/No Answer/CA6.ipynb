{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df7d2a7b",
   "metadata": {},
   "source": [
    "# CA6: Policy Gradient Methods - Complete Implementation and Analysis\n",
    "\n",
    "## Deep Reinforcement Learning - Session 6\n",
    "**Author**: Deep RL Course  \n",
    "**Date**: 2024  \n",
    "**Topic**: From Value-Based to Policy-Based Reinforcement Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Course Overview\n",
    "\n",
    "Welcome to the comprehensive study of **Policy Gradient Methods** in Deep Reinforcement Learning. This session marks a fundamental shift from the value-based methods we explored in previous sessions (DQN, Double DQN, Dueling DQN) to policy-based approaches that directly optimize the policy itself.\n",
    "\n",
    "### Key Learning Objectives\n",
    "\n",
    "By completing this comprehensive exercise, you will master:\n",
    "\n",
    "1. **Theoretical Foundations**: Deep understanding of policy gradient theorem and mathematical derivations\n",
    "2. **REINFORCE Algorithm**: Complete implementation and analysis of Monte Carlo policy gradients\n",
    "3. **Actor-Critic Methods**: Advanced architectures combining policy and value learning\n",
    "4. **A2C/A3C Implementation**: State-of-the-art policy gradient algorithms with parallelization\n",
    "5. **Variance Reduction**: Sophisticated techniques to stabilize policy gradient learning\n",
    "6. **Continuous Control**: Extension to continuous action spaces and control problems\n",
    "7. **Performance Analysis**: Comprehensive evaluation and comparison methodologies\n",
    "\n",
    "### Session Structure\n",
    "\n",
    "- **Section 1**: Theoretical Foundations of Policy Gradient Methods\n",
    "- **Section 2**: REINFORCE Algorithm Implementation and Analysis  \n",
    "- **Section 3**: Actor-Critic Methods with Baseline\n",
    "- **Section 4**: Advanced A2C/A3C Implementation\n",
    "- **Section 5**: Variance Reduction Techniques\n",
    "- **Section 6**: Continuous Action Space Policy Gradients\n",
    "- **Section 7**: Performance Analysis and Comparisons\n",
    "- **Section 8**: Practical Applications and Case Studies\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites and Environment Setup\n",
    "\n",
    "Before diving into policy gradient methods, let's establish our computational environment and theoretical foundation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc7daec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Categorical, Normal\nimport gymnasium as gym\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import deque, namedtuple\nimport random\nimport multiprocessing as mp\nimport threading\nimport time\nfrom typing import List, Tuple, Dict, Optional\nimport warnings\nwarnings.filterwarnings('ignore')\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\nprint(\"Environment setup complete!\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"Gymnasium version: {gym.__version__}\")\nprint(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febd8448",
   "metadata": {},
   "source": [
    "# Section 1: Theoretical Foundations of Policy Gradient Methods\n",
    "\n",
    "## 1.1 From Value-Based to Policy-Based Learning\n",
    "\n",
    "In our journey through reinforcement learning, we have primarily focused on **value-based methods** such as Q-learning and DQN. These methods learn a value function and derive a policy from it. However, there are fundamental limitations to this approach:\n",
    "\n",
    "### Limitations of Value-Based Methods\n",
    "\n",
    "1. **Discrete Action Spaces**: Q-learning naturally handles discrete actions but struggles with continuous action spaces\n",
    "2. **Deterministic Policies**: Value-based methods typically produce deterministic policies (with exploration strategies)\n",
    "3. **Action Space Explosion**: As action space grows, storing Q-values becomes intractable\n",
    "4. **Approximation Errors**: Bootstrapping can lead to error propagation and instability\n",
    "\n",
    "### The Policy Gradient Paradigm\n",
    "\n",
    "Policy gradient methods take a fundamentally different approach:\n",
    "\n",
    "- **Direct Policy Parameterization**: We parameterize the policy π_θ(a|s) directly with parameters θ\n",
    "- **Optimization Objective**: We optimize the expected return J(θ) = E_τ∼π_θ[R(τ)]\n",
    "- **Gradient Ascent**: We update parameters using ∇_θ J(θ)\n",
    "\n",
    "## 1.2 Mathematical Foundations\n",
    "\n",
    "### Policy Parameterization\n",
    "\n",
    "For discrete actions, we typically use a softmax parameterization:\n",
    "\n",
    "π_θ(a|s) = exp(f_θ(s,a)) / Σ_a' exp(f_θ(s,a'))\n",
    "\n",
    "For continuous actions, we often use Gaussian policies:\n",
    "\n",
    "π_θ(a|s) = N(μ_θ(s), σ_θ(s))\n",
    "\n",
    "### The Objective Function\n",
    "\n",
    "The performance measure for a policy π_θ is the expected return:\n",
    "\n",
    "J(θ) = E_s₀∼ρ₀ E_τ∼π_θ [R(τ)]\n",
    "\n",
    "Where:\n",
    "- ρ₀ is the initial state distribution\n",
    "- τ = (s₀, a₀, r₁, s₁, a₁, ...) is a trajectory\n",
    "- R(τ) = Σᵢ γⁱ rᵢ is the discounted return\n",
    "\n",
    "## 1.3 The Policy Gradient Theorem\n",
    "\n",
    "The cornerstone of policy gradient methods is the **Policy Gradient Theorem**, which provides an analytical expression for ∇_θ J(θ).\n",
    "\n",
    "### Theorem Statement\n",
    "\n",
    "For any differentiable policy π_θ and any performance measure J(θ):\n",
    "\n",
    "∇_θ J(θ) = E_s∼d^π_θ E_a∼π_θ [Q^π_θ(s,a) ∇_θ ln π_θ(a|s)]\n",
    "\n",
    "Where d^π_θ(s) is the stationary distribution of states under policy π_θ.\n",
    "\n",
    "### Monte Carlo Formulation\n",
    "\n",
    "In the episodic case, this becomes:\n",
    "\n",
    "∇_θ J(θ) = E_τ∼π_θ [Σₜ G_t ∇_θ ln π_θ(aₜ|sₜ)]\n",
    "\n",
    "Where G_t = Σᵢ₌ₜ^T γⁱ⁻ᵗ rᵢ is the return from time t.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "1. **Model-Free**: No need to know transition probabilities P(s'|s,a)\n",
    "2. **Unbiased**: The gradient estimate is unbiased\n",
    "3. **High Variance**: Monte Carlo estimates can have high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab6d362",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradientVisualization:\n    def __init__(self):\n        self.fig_count = 0\n    def visualize_policy_space(self):\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        theta_values = np.linspace(-3, 3, 100)\n        for i, temp in enumerate([0.1, 0.5, 1.0, 2.0]):\n            ax = axes[i//2, i%2]\n            prob_action_0 = 1 / (1 + np.exp(-theta_values/temp))\n            prob_action_1 = 1 - prob_action_0\n            ax.plot(theta_values, prob_action_0, label='P(a=0|s)', linewidth=2)\n            ax.plot(theta_values, prob_action_1, label='P(a=1|s)', linewidth=2)\n            ax.set_title(f'Policy Probabilities (Temperature={temp})')\n            ax.set_xlabel('Policy Parameter θ')\n            ax.set_ylabel('Probability')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.suptitle('Effect of Policy Parameters on Action Probabilities', \n                     fontsize=16, y=1.02)\n        plt.show()\n    def visualize_gradient_direction(self):\n        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n        theta1 = np.linspace(-2, 2, 20)\n        theta2 = np.linspace(-2, 2, 20)\n        T1, T2 = np.meshgrid(theta1, theta2)\n        J = np.exp(-(T1**2 + T2**2)/2) + 0.5*np.exp(-((T1-1)**2 + (T2+0.5)**2)/0.5)\n        grad_T1, grad_T2 = np.gradient(J)\n        contour = ax1.contour(T1, T2, J, levels=15)\n        ax1.clabel(contour, inline=True, fontsize=8)\n        ax1.quiver(T1[::2,::2], T2[::2,::2], \n                   grad_T1[::2,::2], grad_T2[::2,::2], \n                   alpha=0.7, color='red')\n        ax1.set_title('Policy Gradient Directions')\n        ax1.set_xlabel('θ₁')\n        ax1.set_ylabel('θ₂')\n        path_theta1 = [-1.5, -1.2, -0.8, -0.3, 0.2, 0.7, 0.95]\n        path_theta2 = [1.0, 0.7, 0.3, -0.1, -0.3, -0.4, -0.5]\n        ax2.contour(T1, T2, J, levels=15, alpha=0.5)\n        ax2.plot(path_theta1, path_theta2, 'bo-', linewidth=2, markersize=8, label='Gradient Ascent Path')\n        ax2.plot(path_theta1[0], path_theta2[0], 'go', markersize=10, label='Start')\n        ax2.plot(path_theta1[-1], path_theta2[-1], 'ro', markersize=10, label='Converged')\n        ax2.set_title('Policy Gradient Convergence')\n        ax2.set_xlabel('θ₁')\n        ax2.set_ylabel('θ₂')\n        ax2.legend()\n        plt.tight_layout()\n        plt.show()\n    def compare_value_vs_policy_based(self):\n        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))\n        states = np.arange(1, 11)\n        q_values_a1 = [2.1, 3.2, 1.8, 4.5, 2.7, 3.9, 1.2, 4.8, 3.1, 2.9]\n        q_values_a2 = [1.9, 2.8, 2.1, 3.2, 3.1, 2.7, 2.8, 3.5, 2.9, 3.2]\n        ax1.bar(states - 0.2, q_values_a1, 0.4, label='Q(s,a₁)', alpha=0.7)\n        ax1.bar(states + 0.2, q_values_a2, 0.4, label='Q(s,a₂)', alpha=0.7)\n        ax1.set_title('Value-Based: Q-Values')\n        ax1.set_xlabel('State')\n        ax1.set_ylabel('Q-Value')\n        ax1.legend()\n        ax1.grid(True, alpha=0.3)\n        policy_deterministic = [1 if q1 > q2 else 2 for q1, q2 in zip(q_values_a1, q_values_a2)]\n        colors = ['blue' if a == 1 else 'orange' for a in policy_deterministic]\n        ax2.bar(states, [1]*len(states), color=colors, alpha=0.7)\n        ax2.set_title('Derived Deterministic Policy')\n        ax2.set_xlabel('State')\n        ax2.set_ylabel('Selected Action')\n        ax2.set_yticks([1, 2])\n        ax2.set_yticklabels(['Action 1', 'Action 2'])\n        prob_a1 = [0.7, 0.8, 0.4, 0.9, 0.5, 0.8, 0.3, 0.9, 0.7, 0.6]\n        prob_a2 = [1-p for p in prob_a1]\n        ax3.bar(states - 0.2, prob_a1, 0.4, label='π(a₁|s)', alpha=0.7)\n        ax3.bar(states + 0.2, prob_a2, 0.4, label='π(a₂|s)', alpha=0.7)\n        ax3.set_title('Policy-Based: Stochastic Policy')\n        ax3.set_xlabel('State')\n        ax3.set_ylabel('Probability')\n        ax3.legend()\n        ax3.grid(True, alpha=0.3)\n        action_probs = np.array([prob_a1, prob_a2]).T\n        bottom = np.zeros(len(states))\n        colors = ['blue', 'orange']\n        labels = ['Action 1', 'Action 2']\n        for i in range(2):\n            ax4.bar(states, action_probs[:, i], bottom=bottom, \n                   color=colors[i], alpha=0.7, label=labels[i])\n            bottom += action_probs[:, i]\n        ax4.set_title('Stochastic Action Selection')\n        ax4.set_xlabel('State')\n        ax4.set_ylabel('Probability')\n        ax4.legend()\n        plt.tight_layout()\n        plt.suptitle('Value-Based vs Policy-Based Methods Comparison', \n                     fontsize=16, y=1.02)\n        plt.show()\nviz = PolicyGradientVisualization()\nprint(\"=== Policy Gradient Theoretical Foundations ===\")\nprint(\"\\n1. Visualizing Policy Parameter Effects:\")\nviz.visualize_policy_space()\nprint(\"\\n2. Policy Gradient Directions and Convergence:\")\nviz.visualize_gradient_direction()\nprint(\"\\n3. Value-Based vs Policy-Based Comparison:\")\nviz.compare_value_vs_policy_based()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730ceed2",
   "metadata": {},
   "source": [
    "# Section 2: REINFORCE Algorithm Implementation and Analysis\n",
    "\n",
    "## 2.1 The REINFORCE Algorithm (Monte Carlo Policy Gradient)\n",
    "\n",
    "REINFORCE, proposed by Williams (1992), is the simplest policy gradient algorithm. It directly implements the policy gradient theorem using Monte Carlo estimates of the return.\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "**REINFORCE Algorithm:**\n",
    "\n",
    "1. Initialize policy parameters θ randomly\n",
    "2. For each episode:\n",
    "   a. Generate episode trajectory τ = (s₀,a₀,r₁,s₁,a₁,r₂,...,sₜ,aₜ,rₜ₊₁) using π_θ\n",
    "   b. For each time step t in the episode:\n",
    "      - Calculate return G_t = Σᵢ₌ₜ^T γⁱ⁻ᵗ rᵢ\n",
    "      - Update θ ← θ + α G_t ∇_θ ln π_θ(aₜ|sₜ)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The REINFORCE update is:\n",
    "\n",
    "**θ ← θ + α Σₜ G_t ∇_θ ln π_θ(aₜ|sₜ)**\n",
    "\n",
    "Where:\n",
    "- **G_t**: Return from time step t\n",
    "- **∇_θ ln π_θ(aₜ|sₜ)**: Score function (gradient of log-probability)\n",
    "- **α**: Learning rate\n",
    "\n",
    "### Key Characteristics\n",
    "\n",
    "- **Unbiased**: E[∇_θ J(θ)] is the true policy gradient\n",
    "- **High Variance**: Monte Carlo estimates can be very noisy\n",
    "- **Sample Inefficient**: Requires complete episodes for updates\n",
    "- **On-Policy**: Uses trajectories generated by current policy\n",
    "\n",
    "## 2.2 Understanding the Variance Problem\n",
    "\n",
    "The main challenge with REINFORCE is the high variance of gradient estimates. Let's analyze why this occurs and its impact on learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7143ce83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class REINFORCEAgent:\n    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=1e-3, gamma=0.99):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.policy_net = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        ).to(device)\n        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=lr)\n        self.episode_rewards = []\n        self.policy_losses = []\n        self.gradient_norms = []\n        self.entropy_history = []\n    def select_action(self, state, return_log_prob=False):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            logits = self.policy_net(state)\n            probs = F.softmax(logits, dim=1)\n        dist = Categorical(probs)\n        action = dist.sample()\n        if return_log_prob:\n            log_prob = dist.log_prob(action)\n            return action.item(), log_prob.item()\n        return action.item()\n    def get_policy_distribution(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            logits = self.policy_net(state)\n            probs = F.softmax(logits, dim=1)\n        return probs.cpu().numpy().flatten()\n    def compute_returns(self, rewards):\n        returns = []\n        G = 0\n        for r in reversed(rewards):\n            G = r + self.gamma * G\n            returns.insert(0, G)\n        return returns\n    def update_policy(self, states, actions, returns):\n        states = torch.FloatTensor(states).to(device)\n        actions = torch.LongTensor(actions).to(device)\n        returns = torch.FloatTensor(returns).to(device)\n        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n        logits = self.policy_net(states)\n        dist = Categorical(logits=logits)\n        log_probs = dist.log_prob(actions)\n        policy_loss = -(log_probs * returns).mean()\n        self.optimizer.zero_grad()\n        policy_loss.backward()\n        total_norm = 0\n        for p in self.policy_net.parameters():\n            if p.grad is not None:\n                param_norm = p.grad.data.norm(2)\n                total_norm += param_norm.item() ** 2\n        total_norm = total_norm ** (1. / 2)\n        self.gradient_norms.append(total_norm)\n        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        entropy = dist.entropy().mean()\n        self.entropy_history.append(entropy.item())\n        self.policy_losses.append(policy_loss.item())\n        return policy_loss.item()\n    def train_episode(self, env):\n        state, _ = env.reset()\n        states, actions, rewards = [], [], []\n        episode_reward = 0\n        while True:\n            action = self.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            states.append(state)\n            actions.append(action)\n            rewards.append(reward)\n            episode_reward += reward\n            state = next_state\n            if terminated or truncated:\n                break\n        returns = self.compute_returns(rewards)\n        loss = self.update_policy(states, actions, returns)\n        self.episode_rewards.append(episode_reward)\n        return episode_reward, loss\n    def analyze_variance(self, env, num_episodes=100):\n        gradient_estimates = []\n        for _ in range(num_episodes):\n            state, _ = env.reset()\n            states, actions, rewards = [], [], []\n            while True:\n                action = self.select_action(state)\n                next_state, reward, terminated, truncated, _ = env.step(action)\n                states.append(state)\n                actions.append(action)\n                rewards.append(reward)\n                state = next_state\n                if terminated or truncated:\n                    break\n            states_tensor = torch.FloatTensor(states).to(device)\n            actions_tensor = torch.LongTensor(actions).to(device)\n            returns = torch.FloatTensor(self.compute_returns(rewards)).to(device)\n            logits = self.policy_net(states_tensor)\n            dist = Categorical(logits=logits)\n            log_probs = dist.log_prob(actions_tensor)\n            grad_contributions = (log_probs * returns).detach().cpu().numpy()\n            gradient_estimates.extend(grad_contributions)\n        return np.array(gradient_estimates)\ndef test_reinforce():\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    agent = REINFORCEAgent(state_dim, action_dim, lr=1e-3, gamma=0.99)\n    print(\"=== REINFORCE Training ===\")\n    num_episodes = 300\n    log_interval = 50\n    for episode in range(num_episodes):\n        episode_reward, loss = agent.train_episode(env)\n        if (episode + 1) % log_interval == 0:\n            avg_reward = np.mean(agent.episode_rewards[-log_interval:])\n            avg_loss = np.mean(agent.policy_losses[-log_interval:])\n            avg_entropy = np.mean(agent.entropy_history[-log_interval:])\n            print(f\"Episode {episode+1:4d} | Avg Reward: {avg_reward:7.2f} | \"\n                  f\"Loss: {avg_loss:.4f} | Entropy: {avg_entropy:.4f}\")\n    print(\"\\n=== Variance Analysis ===\")\n    gradient_estimates = agent.analyze_variance(env, num_episodes=50)\n    print(f\"Gradient estimate statistics:\")\n    print(f\"Mean: {np.mean(gradient_estimates):.4f}\")\n    print(f\"Std:  {np.std(gradient_estimates):.4f}\")\n    print(f\"Variance: {np.var(gradient_estimates):.4f}\")\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    axes[0,0].plot(agent.episode_rewards)\n    axes[0,0].plot(pd.Series(agent.episode_rewards).rolling(window=20).mean(), \n                   color='red', label='Moving Average')\n    axes[0,0].set_title('REINFORCE Learning Curve')\n    axes[0,0].set_xlabel('Episode')\n    axes[0,0].set_ylabel('Episode Reward')\n    axes[0,0].legend()\n    axes[0,0].grid(True, alpha=0.3)\n    axes[0,1].plot(agent.policy_losses)\n    axes[0,1].plot(pd.Series(agent.policy_losses).rolling(window=20).mean(), \n                   color='red', label='Moving Average')\n    axes[0,1].set_title('Policy Loss Over Time')\n    axes[0,1].set_xlabel('Episode')\n    axes[0,1].set_ylabel('Policy Loss')\n    axes[0,1].legend()\n    axes[0,1].grid(True, alpha=0.3)\n    axes[1,0].plot(agent.gradient_norms)\n    axes[1,0].set_title('Gradient Norms')\n    axes[1,0].set_xlabel('Episode')\n    axes[1,0].set_ylabel('Gradient L2 Norm')\n    axes[1,0].grid(True, alpha=0.3)\n    axes[1,1].hist(gradient_estimates, bins=30, alpha=0.7, density=True)\n    axes[1,1].axvline(np.mean(gradient_estimates), color='red', \n                      linestyle='--', label=f'Mean: {np.mean(gradient_estimates):.3f}')\n    axes[1,1].set_title('Distribution of Gradient Estimates')\n    axes[1,1].set_xlabel('Gradient Value')\n    axes[1,1].set_ylabel('Density')\n    axes[1,1].legend()\n    axes[1,1].grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    env.close()\n    return agent\nreinforce_agent = test_reinforce()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd8468",
   "metadata": {},
   "source": [
    "# Section 3: Actor-Critic Methods with Baseline\n",
    "\n",
    "## 3.1 Addressing the Variance Problem\n",
    "\n",
    "The high variance in REINFORCE stems from using the full return G_t as the signal for policy updates. We can reduce variance by subtracting a **baseline** b(s_t) from the return:\n",
    "\n",
    "**∇_θ J(θ) = E_τ∼π_θ [Σₜ (G_t - b(s_t)) ∇_θ ln π_θ(aₜ|sₜ)]**\n",
    "\n",
    "The baseline doesn't change the expectation of the gradient (unbiased) but can significantly reduce variance.\n",
    "\n",
    "### Choosing the Baseline: Value Function\n",
    "\n",
    "The optimal baseline is the **state-value function** V^π(s):\n",
    "\n",
    "**b(s_t) = V^π(s_t)**\n",
    "\n",
    "This leads to the **advantage function**:\n",
    "**A^π(s_t, a_t) = G_t - V^π(s_t)**\n",
    "\n",
    "## 3.2 Actor-Critic Architecture\n",
    "\n",
    "Actor-Critic methods combine:\n",
    "\n",
    "1. **Actor**: Policy π_θ(a|s) that selects actions\n",
    "2. **Critic**: Value function V_φ(s) that evaluates states\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "**Actor-Critic Algorithm:**\n",
    "\n",
    "1. Initialize actor parameters θ and critic parameters φ\n",
    "2. For each time step:\n",
    "   a. Select action a_t ∼ π_θ(·|s_t)\n",
    "   b. Observe reward r_t and next state s_{t+1}\n",
    "   c. Compute TD error: δ_t = r_t + γV_φ(s_{t+1}) - V_φ(s_t)\n",
    "   d. Update critic: φ ← φ + α_c δ_t ∇_φ V_φ(s_t)\n",
    "   e. Update actor: θ ← θ + α_a δ_t ∇_θ ln π_θ(a_t|s_t)\n",
    "\n",
    "### Key Advantages\n",
    "\n",
    "1. **Lower Variance**: Using value function baseline reduces gradient variance\n",
    "2. **Online Learning**: Can update after each step (no need to wait for episode end)\n",
    "3. **Faster Learning**: More frequent updates lead to faster convergence\n",
    "4. **Bootstrapping**: Uses learned value estimates rather than full returns\n",
    "\n",
    "## 3.3 Temporal Difference vs Monte Carlo\n",
    "\n",
    "Actor-Critic can use different targets for the advantage estimation:\n",
    "\n",
    "- **Monte Carlo**: A(s_t,a_t) = G_t - V(s_t)\n",
    "- **TD(0)**: A(s_t,a_t) = r_t + γV(s_{t+1}) - V(s_t)\n",
    "- **TD(λ)**: A(s_t,a_t) = G_t^λ - V(s_t)\n",
    "\n",
    "Each provides different bias-variance tradeoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c9ed72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticAgent:\n    def __init__(self, state_dim, action_dim, hidden_dim=128, \n                 actor_lr=1e-3, critic_lr=5e-3, gamma=0.99):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim)\n        ).to(device)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        ).to(device)\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n        self.episode_rewards = []\n        self.actor_losses = []\n        self.critic_losses = []\n        self.td_errors = []\n        self.advantages = []\n        self.value_estimates = []\n    def select_action(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            logits = self.actor(state)\n            probs = F.softmax(logits, dim=1)\n        dist = Categorical(probs)\n        action = dist.sample()\n        log_prob = dist.log_prob(action)\n        return action.item(), log_prob\n    def get_value(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            value = self.critic(state)\n        return value.item()\n    def update(self, state, action, reward, next_state, done, log_prob):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n        current_value = self.critic(state)\n        if done:\n            target_value = reward\n        else:\n            with torch.no_grad():\n                next_value = self.critic(next_state)\n            target_value = reward + self.gamma * next_value\n        td_error = target_value - current_value\n        advantage = td_error.detach()\n        critic_loss = td_error.pow(2)\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n        self.critic_optimizer.step()\n        actor_loss = -log_prob * advantage\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n        self.actor_optimizer.step()\n        self.actor_losses.append(actor_loss.item())\n        self.critic_losses.append(critic_loss.item())\n        self.td_errors.append(td_error.item())\n        self.advantages.append(advantage.item())\n        self.value_estimates.append(current_value.item())\n        return actor_loss.item(), critic_loss.item(), td_error.item()\n    def train_episode(self, env):\n        state, _ = env.reset()\n        episode_reward = 0\n        step_count = 0\n        while True:\n            action, log_prob = self.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            actor_loss, critic_loss, td_error = self.update(\n                state, action, reward, next_state, done, log_prob)\n            episode_reward += reward\n            step_count += 1\n            state = next_state\n            if done:\n                break\n        self.episode_rewards.append(episode_reward)\n        return episode_reward, step_count\nclass BaselineComparison:\n    def __init__(self, state_dim, action_dim):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n    def no_baseline_reinforce(self, env, num_episodes=200):\n        agent = REINFORCEAgent(self.state_dim, self.action_dim, lr=1e-3)\n        rewards = []\n        for episode in range(num_episodes):\n            reward, _ = agent.train_episode(env)\n            rewards.append(reward)\n        return rewards, agent.gradient_norms\n    def constant_baseline_reinforce(self, env, baseline_value=100, num_episodes=200):\n        class ConstantBaselineREINFORCE(REINFORCEAgent):\n            def __init__(self, *args, baseline=0, **kwargs):\n                super().__init__(*args, **kwargs)\n                self.baseline = baseline\n            def update_policy(self, states, actions, returns):\n                states = torch.FloatTensor(states).to(device)\n                actions = torch.LongTensor(actions).to(device)\n                returns = torch.FloatTensor(returns).to(device)\n                advantages = returns - self.baseline\n                advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n                logits = self.policy_net(states)\n                dist = Categorical(logits=logits)\n                log_probs = dist.log_prob(actions)\n                policy_loss = -(log_probs * advantages).mean()\n                self.optimizer.zero_grad()\n                policy_loss.backward()\n                total_norm = 0\n                for p in self.policy_net.parameters():\n                    if p.grad is not None:\n                        param_norm = p.grad.data.norm(2)\n                        total_norm += param_norm.item() ** 2\n                total_norm = total_norm ** (1. / 2)\n                self.gradient_norms.append(total_norm)\n                torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n                self.optimizer.step()\n                entropy = dist.entropy().mean()\n                self.entropy_history.append(entropy.item())\n                self.policy_losses.append(policy_loss.item())\n                return policy_loss.item()\n        agent = ConstantBaselineREINFORCE(self.state_dim, self.action_dim, \n                                        baseline=baseline_value, lr=1e-3)\n        rewards = []\n        for episode in range(num_episodes):\n            reward, _ = agent.train_episode(env)\n            rewards.append(reward)\n        return rewards, agent.gradient_norms\n    def actor_critic_baseline(self, env, num_episodes=200):\n        agent = ActorCriticAgent(self.state_dim, self.action_dim)\n        rewards = []\n        for episode in range(num_episodes):\n            reward, _ = agent.train_episode(env)\n            rewards.append(reward)\n        return rewards, agent.advantages\ndef test_baseline_comparison():\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    comparison = BaselineComparison(state_dim, action_dim)\n    print(\"=== Baseline Comparison Analysis ===\")\n    print(\"1. Training REINFORCE without baseline...\")\n    no_baseline_rewards, no_baseline_grads = comparison.no_baseline_reinforce(env)\n    print(\"2. Training REINFORCE with constant baseline...\")\n    constant_baseline_rewards, constant_baseline_grads = comparison.constant_baseline_reinforce(env, baseline_value=100)\n    print(\"3. Training Actor-Critic with learned baseline...\")\n    ac_rewards, ac_advantages = comparison.actor_critic_baseline(env)\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    window = 20\n    axes[0,0].plot(pd.Series(no_baseline_rewards).rolling(window).mean(), \n                   label='No Baseline', linewidth=2)\n    axes[0,0].plot(pd.Series(constant_baseline_rewards).rolling(window).mean(), \n                   label='Constant Baseline', linewidth=2)\n    axes[0,0].plot(pd.Series(ac_rewards).rolling(window).mean(), \n                   label='Actor-Critic', linewidth=2)\n    axes[0,0].set_title('Learning Curves Comparison')\n    axes[0,0].set_xlabel('Episode')\n    axes[0,0].set_ylabel('Average Reward')\n    axes[0,0].legend()\n    axes[0,0].grid(True, alpha=0.3)\n    axes[0,1].plot(no_baseline_grads, alpha=0.7, label='No Baseline')\n    axes[0,1].plot(constant_baseline_grads, alpha=0.7, label='Constant Baseline')\n    axes[0,1].set_title('Gradient Norms Comparison')\n    axes[0,1].set_xlabel('Episode')\n    axes[0,1].set_ylabel('Gradient L2 Norm')\n    axes[0,1].legend()\n    axes[0,1].grid(True, alpha=0.3)\n    final_performances = [\n        np.mean(no_baseline_rewards[-50:]),\n        np.mean(constant_baseline_rewards[-50:]),\n        np.mean(ac_rewards[-50:])\n    ]\n    methods = ['No Baseline', 'Constant Baseline', 'Actor-Critic']\n    colors = ['red', 'orange', 'green']\n    bars = axes[0,2].bar(methods, final_performances, color=colors, alpha=0.7)\n    axes[0,2].set_title('Final Performance (Last 50 Episodes)')\n    axes[0,2].set_ylabel('Average Reward')\n    axes[0,2].grid(True, alpha=0.3)\n    for bar, value in zip(bars, final_performances):\n        axes[0,2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                       f'{value:.1f}', ha='center', va='bottom')\n    axes[1,0].hist(no_baseline_grads, bins=30, alpha=0.5, label='No Baseline', density=True)\n    axes[1,0].hist(constant_baseline_grads, bins=30, alpha=0.5, label='Constant Baseline', density=True)\n    axes[1,0].set_title('Gradient Norm Distributions')\n    axes[1,0].set_xlabel('Gradient Norm')\n    axes[1,0].set_ylabel('Density')\n    axes[1,0].legend()\n    axes[1,0].grid(True, alpha=0.3)\n    if len(ac_advantages) > 0:\n        axes[1,1].plot(ac_advantages[:1000])\n        axes[1,1].set_title('Advantage Evolution (Actor-Critic)')\n        axes[1,1].set_xlabel('Step')\n        axes[1,1].set_ylabel('Advantage')\n        axes[1,1].grid(True, alpha=0.3)\n    variance_stats = {\n        'No Baseline': np.var(no_baseline_grads),\n        'Constant Baseline': np.var(constant_baseline_grads),\n        'AC Advantages': np.var(ac_advantages) if len(ac_advantages) > 0 else 0\n    }\n    methods_var = list(variance_stats.keys())\n    variances = list(variance_stats.values())\n    axes[1,2].bar(methods_var, variances, color=['red', 'orange', 'green'], alpha=0.7)\n    axes[1,2].set_title('Variance Comparison')\n    axes[1,2].set_ylabel('Variance')\n    axes[1,2].grid(True, alpha=0.3)\n    axes[1,2].tick_params(axis='x', rotation=45)\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n=== Numerical Results ===\")\n    print(f\"Final Performance (last 50 episodes):\")\n    print(f\"  No Baseline:       {final_performances[0]:.2f}\")\n    print(f\"  Constant Baseline: {final_performances[1]:.2f}\")\n    print(f\"  Actor-Critic:      {final_performances[2]:.2f}\")\n    print(f\"\\nGradient Variance:\")\n    print(f\"  No Baseline:       {variance_stats['No Baseline']:.4f}\")\n    print(f\"  Constant Baseline: {variance_stats['Constant Baseline']:.4f}\")\n    print(f\"  AC Advantages:     {variance_stats['AC Advantages']:.4f}\")\n    env.close()\ntest_baseline_comparison()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79476e",
   "metadata": {},
   "source": [
    "# Section 4: Advanced A2C/A3C Implementation\n",
    "\n",
    "## 4.1 Advantage Actor-Critic (A2C)\n",
    "\n",
    "A2C is a synchronous, deterministic variant of A3C that addresses several limitations of basic Actor-Critic:\n",
    "\n",
    "### Key Improvements:\n",
    "\n",
    "1. **N-step Returns**: Use n-step bootstrapping instead of 1-step TD\n",
    "2. **Entropy Regularization**: Encourage exploration by penalizing deterministic policies  \n",
    "3. **Shared Networks**: Actor and critic share lower-layer representations\n",
    "4. **Batch Updates**: Collect multiple trajectories before updating\n",
    "\n",
    "### N-step Advantage Estimation\n",
    "\n",
    "Instead of 1-step TD error, A2C uses n-step returns:\n",
    "\n",
    "**A(s_t, a_t) = (Σᵢ₌₀ⁿ⁻¹ γⁱ r_{t+i}) + γⁿ V(s_{t+n}) - V(s_t)**\n",
    "\n",
    "This provides better bias-variance tradeoff.\n",
    "\n",
    "### Entropy Regularization\n",
    "\n",
    "The actor loss includes an entropy term:\n",
    "\n",
    "**L_actor = -E[A(s_t,a_t) log π(a_t|s_t)] - β H(π(·|s_t))**\n",
    "\n",
    "Where H(π) = -Σ_a π(a|s) log π(a|s) is the policy entropy.\n",
    "\n",
    "## 4.2 Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "A3C introduces asynchronous training with multiple workers:\n",
    "\n",
    "### Architecture:\n",
    "- **Global Network**: Shared actor-critic parameters\n",
    "- **Worker Threads**: Independent environments and local networks\n",
    "- **Asynchronous Updates**: Workers update global network asynchronously\n",
    "\n",
    "### Algorithm Overview:\n",
    "\n",
    "1. Initialize global shared parameters θ and φ\n",
    "2. For each worker thread:\n",
    "   a. Copy global parameters to local networks\n",
    "   b. Collect trajectory of length T\n",
    "   c. Compute advantages using n-step returns\n",
    "   d. Compute gradients and update global network\n",
    "   e. Repeat\n",
    "\n",
    "### Benefits:\n",
    "- **Decorrelated Experience**: Different workers explore different parts of state space\n",
    "- **Stability**: Removes need for experience replay\n",
    "- **Efficiency**: Parallelization speeds up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf6d182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharedActorCriticNetwork(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super(SharedActorCriticNetwork, self).__init__()\n        self.shared_layers = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.actor_head = nn.Linear(hidden_dim, action_dim)\n        self.critic_head = nn.Linear(hidden_dim, 1)\n    def forward(self, state):\n        shared_features = self.shared_layers(state)\n        logits = self.actor_head(shared_features)\n        value = self.critic_head(shared_features)\n        return logits, value\nclass A2CAgent:\n    def __init__(self, state_dim, action_dim, hidden_dim=128, lr=7e-4, \n                 gamma=0.99, n_steps=5, entropy_coef=0.01, value_coef=0.5):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.gamma = gamma\n        self.n_steps = n_steps\n        self.entropy_coef = entropy_coef\n        self.value_coef = value_coef\n        self.network = SharedActorCriticNetwork(state_dim, action_dim, hidden_dim).to(device)\n        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n        self.states = []\n        self.actions = []\n        self.rewards = []\n        self.log_probs = []\n        self.values = []\n        self.dones = []\n        self.episode_rewards = []\n        self.actor_losses = []\n        self.critic_losses = []\n        self.entropy_losses = []\n        self.total_losses = []\n    def select_action(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            logits, value = self.network(state)\n            probs = F.softmax(logits, dim=1)\n        dist = Categorical(probs)\n        action = dist.sample()\n        log_prob = dist.log_prob(action)\n        return action.item(), log_prob.item(), value.item()\n    def store_transition(self, state, action, reward, log_prob, value, done):\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.log_probs.append(log_prob)\n        self.values.append(value)\n        self.dones.append(done)\n    def compute_n_step_returns(self, next_value=0):\n        returns = []\n        advantages = []\n        R = next_value\n        for i in reversed(range(len(self.rewards))):\n            R = self.rewards[i] + self.gamma * R * (1 - self.dones[i])\n            returns.insert(0, R)\n        returns = torch.FloatTensor(returns).to(device)\n        values = torch.FloatTensor(self.values).to(device)\n        advantages = returns - values\n        return returns, advantages\n    def update(self, next_state=None):\n        if len(self.states) == 0:\n            return\n        if next_state is not None:\n            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n            with torch.no_grad():\n                _, next_value = self.network(next_state)\n                next_value = next_value.item()\n        else:\n            next_value = 0\n        returns, advantages = self.compute_n_step_returns(next_value)\n        states = torch.FloatTensor(self.states).to(device)\n        actions = torch.LongTensor(self.actions).to(device)\n        log_probs_old = torch.FloatTensor(self.log_probs).to(device)\n        logits, values = self.network(states)\n        dist = Categorical(logits=logits)\n        log_probs_new = dist.log_prob(actions)\n        entropy = dist.entropy().mean()\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        actor_loss = -(log_probs_new * advantages.detach()).mean()\n        critic_loss = F.mse_loss(values.squeeze(), returns)\n        entropy_loss = -entropy\n        total_loss = (actor_loss + \n                     self.value_coef * critic_loss + \n                     self.entropy_coef * entropy_loss)\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=0.5)\n        self.optimizer.step()\n        self.actor_losses.append(actor_loss.item())\n        self.critic_losses.append(critic_loss.item())\n        self.entropy_losses.append(entropy_loss.item())\n        self.total_losses.append(total_loss.item())\n        self.states = []\n        self.actions = []\n        self.rewards = []\n        self.log_probs = []\n        self.values = []\n        self.dones = []\n        return total_loss.item()\n    def train_episode(self, env):\n        state, _ = env.reset()\n        episode_reward = 0\n        step_count = 0\n        while True:\n            action, log_prob, value = self.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            self.store_transition(state, action, reward, log_prob, value, done)\n            episode_reward += reward\n            step_count += 1\n            if len(self.states) >= self.n_steps or done:\n                if done:\n                    self.update()\n                else:\n                    self.update(next_state)\n            state = next_state\n            if done:\n                break\n        self.episode_rewards.append(episode_reward)\n        return episode_reward, step_count\nclass A3CWorker:\n    def __init__(self, worker_id, global_network, optimizer, state_dim, action_dim,\n                 gamma=0.99, n_steps=5, entropy_coef=0.01, value_coef=0.5):\n        self.worker_id = worker_id\n        self.global_network = global_network\n        self.global_optimizer = optimizer\n        self.gamma = gamma\n        self.n_steps = n_steps\n        self.entropy_coef = entropy_coef\n        self.value_coef = value_coef\n        self.local_network = SharedActorCriticNetwork(state_dim, action_dim).to(device)\n        self.states = []\n        self.actions = []\n        self.rewards = []\n        self.log_probs = []\n        self.values = []\n        self.dones = []\n    def sync_with_global(self):\n        self.local_network.load_state_dict(self.global_network.state_dict())\n    def select_action(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            logits, value = self.local_network(state)\n            probs = F.softmax(logits, dim=1)\n        dist = Categorical(probs)\n        action = dist.sample()\n        log_prob = dist.log_prob(action)\n        return action.item(), log_prob.item(), value.item()\n    def compute_loss(self, next_state=None):\n        if len(self.states) == 0:\n            return torch.tensor(0.0)\n        if next_state is not None:\n            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n            with torch.no_grad():\n                _, next_value = self.local_network(next_state)\n                next_value = next_value.item()\n        else:\n            next_value = 0\n        returns = []\n        R = next_value\n        for i in reversed(range(len(self.rewards))):\n            R = self.rewards[i] + self.gamma * R * (1 - self.dones[i])\n            returns.insert(0, R)\n        returns = torch.FloatTensor(returns).to(device)\n        states = torch.FloatTensor(self.states).to(device)\n        actions = torch.LongTensor(self.actions).to(device)\n        values = torch.FloatTensor(self.values).to(device)\n        logits, new_values = self.local_network(states)\n        dist = Categorical(logits=logits)\n        log_probs = dist.log_prob(actions)\n        entropy = dist.entropy().mean()\n        advantages = returns - values\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        actor_loss = -(log_probs * advantages.detach()).mean()\n        critic_loss = F.mse_loss(new_values.squeeze(), returns)\n        entropy_loss = -entropy\n        total_loss = (actor_loss + \n                     self.value_coef * critic_loss + \n                     self.entropy_coef * entropy_loss)\n        return total_loss\n    def update_global(self, loss):\n        self.global_optimizer.zero_grad()\n        loss.backward()\n        for local_param, global_param in zip(self.local_network.parameters(),\n                                           self.global_network.parameters()):\n            if global_param.grad is None:\n                global_param.grad = local_param.grad.clone()\n            else:\n                global_param.grad += local_param.grad\n        torch.nn.utils.clip_grad_norm_(self.global_network.parameters(), max_norm=0.5)\n        self.global_optimizer.step()\n    def train_worker(self, env, max_episodes=100):\n        episode_rewards = []\n        for episode in range(max_episodes):\n            self.sync_with_global()\n            state, _ = env.reset()\n            episode_reward = 0\n            self.states = []\n            self.actions = []\n            self.rewards = []\n            self.log_probs = []\n            self.values = []\n            self.dones = []\n            while True:\n                action, log_prob, value = self.select_action(state)\n                next_state, reward, terminated, truncated, _ = env.step(action)\n                done = terminated or truncated\n                self.states.append(state)\n                self.actions.append(action)\n                self.rewards.append(reward)\n                self.log_probs.append(log_prob)\n                self.values.append(value)\n                self.dones.append(done)\n                episode_reward += reward\n                if len(self.states) >= self.n_steps or done:\n                    if done:\n                        loss = self.compute_loss()\n                    else:\n                        loss = self.compute_loss(next_state)\n                    self.update_global(loss)\n                    self.states = []\n                    self.actions = []\n                    self.rewards = []\n                    self.log_probs = []\n                    self.values = []\n                    self.dones = []\n                state = next_state\n                if done:\n                    break\n            episode_rewards.append(episode_reward)\n            if episode % 50 == 0:\n                print(f\"Worker {self.worker_id} | Episode {episode} | Reward: {episode_reward:.2f}\")\n        return episode_rewards\ndef test_a2c_vs_a3c():\n    env_name = 'CartPole-v1'\n    env = gym.make(env_name)\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    print(\"=== A2C vs A3C Comparison ===\")\n    print(\"\\n1. Training A2C...\")\n    a2c_agent = A2CAgent(state_dim, action_dim, lr=7e-4, n_steps=5)\n    a2c_rewards = []\n    for episode in range(300):\n        reward, _ = a2c_agent.train_episode(env)\n        a2c_rewards.append(reward)\n        if (episode + 1) % 50 == 0:\n            avg_reward = np.mean(a2c_rewards[-50:])\n            print(f\"A2C Episode {episode+1:3d} | Avg Reward: {avg_reward:.2f}\")\n    print(\"\\n2. Training A3C (simplified)...\")\n    global_network = SharedActorCriticNetwork(state_dim, action_dim).to(device)\n    global_optimizer = optim.Adam(global_network.parameters(), lr=7e-4)\n    worker = A3CWorker(0, global_network, global_optimizer, state_dim, action_dim)\n    a3c_rewards = worker.train_worker(env, max_episodes=300)\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    window = 20\n    axes[0,0].plot(pd.Series(a2c_rewards).rolling(window).mean(), \n                   label='A2C', linewidth=2, color='blue')\n    axes[0,0].plot(pd.Series(a3c_rewards).rolling(window).mean(), \n                   label='A3C', linewidth=2, color='red')\n    axes[0,0].set_title('Learning Curves: A2C vs A3C')\n    axes[0,0].set_xlabel('Episode')\n    axes[0,0].set_ylabel('Average Reward')\n    axes[0,0].legend()\n    axes[0,0].grid(True, alpha=0.3)\n    if len(a2c_agent.total_losses) > 0:\n        axes[0,1].plot(a2c_agent.actor_losses, label='Actor Loss', alpha=0.7)\n        axes[0,1].plot(a2c_agent.critic_losses, label='Critic Loss', alpha=0.7)\n        axes[0,1].plot(a2c_agent.entropy_losses, label='Entropy Loss', alpha=0.7)\n        axes[0,1].set_title('A2C Loss Components')\n        axes[0,1].set_xlabel('Update')\n        axes[0,1].set_ylabel('Loss')\n        axes[0,1].legend()\n        axes[0,1].grid(True, alpha=0.3)\n    a2c_final = np.mean(a2c_rewards[-50:])\n    a3c_final = np.mean(a3c_rewards[-50:])\n    methods = ['A2C', 'A3C']\n    performances = [a2c_final, a3c_final]\n    colors = ['blue', 'red']\n    bars = axes[1,0].bar(methods, performances, color=colors, alpha=0.7)\n    axes[1,0].set_title('Final Performance (Last 50 Episodes)')\n    axes[1,0].set_ylabel('Average Reward')\n    axes[1,0].grid(True, alpha=0.3)\n    for bar, value in zip(bars, performances):\n        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                       f'{value:.1f}', ha='center', va='bottom')\n    a2c_sample_efficiency = []\n    a3c_sample_efficiency = []\n    for i in range(0, len(a2c_rewards), 10):\n        a2c_sample_efficiency.append(np.mean(a2c_rewards[max(0, i-10):i+1]))\n    for i in range(0, len(a3c_rewards), 10):\n        a3c_sample_efficiency.append(np.mean(a3c_rewards[max(0, i-10):i+1]))\n    axes[1,1].plot(range(0, len(a2c_rewards), 10), a2c_sample_efficiency, \n                   label='A2C', linewidth=2, color='blue')\n    axes[1,1].plot(range(0, len(a3c_rewards), 10), a3c_sample_efficiency, \n                   label='A3C', linewidth=2, color='red')\n    axes[1,1].set_title('Sample Efficiency Comparison')\n    axes[1,1].set_xlabel('Episode')\n    axes[1,1].set_ylabel('Average Reward (10-episode window)')\n    axes[1,1].legend()\n    axes[1,1].grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    print(f\"\\n=== Results Summary ===\")\n    print(f\"A2C Final Performance: {a2c_final:.2f}\")\n    print(f\"A3C Final Performance: {a3c_final:.2f}\")\n    print(f\"Winner: {'A2C' if a2c_final > a3c_final else 'A3C'}\")\n    env.close()\n    return a2c_agent, a2c_rewards, a3c_rewards\na2c_agent, a2c_rewards, a3c_rewards = test_a2c_vs_a3c()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5657f516",
   "metadata": {},
   "source": [
    "# Section 5: Variance Reduction Techniques\n",
    "\n",
    "## 5.1 Advanced Variance Reduction Methods\n",
    "\n",
    "Beyond basic baselines, several sophisticated techniques can further reduce the variance of policy gradient estimates:\n",
    "\n",
    "### 1. Control Variates\n",
    "Control variates use correlated random variables to reduce variance:\n",
    "\n",
    "**∇_θ J(θ) ≈ (1/n) Σᵢ [f(τᵢ) - c(g(τᵢ) - E[g(τ)])]**\n",
    "\n",
    "Where g(τ) is a control variate and c is chosen to minimize variance.\n",
    "\n",
    "### 2. Importance Sampling\n",
    "Allows using trajectories from different policies:\n",
    "\n",
    "**∇_θ J(θ) = E_τ∼π_β [ρ(τ) Σₜ G_t ∇_θ ln π_θ(aₜ|sₜ)]**\n",
    "\n",
    "Where ρ(τ) = π_θ(τ)/π_β(τ) is the importance weight.\n",
    "\n",
    "### 3. Natural Policy Gradients\n",
    "Use the natural gradient instead of standard gradient:\n",
    "\n",
    "**∇_θ J(θ) = F⁻¹ ∇_θ J(θ)**\n",
    "\n",
    "Where F is the Fisher Information Matrix.\n",
    "\n",
    "### 4. Generalized Advantage Estimation (GAE)\n",
    "Combines n-step returns with exponential averaging:\n",
    "\n",
    "**Â_t^(GAE) = Σₗ₌₀^∞ (γλ)ₗ δₜ₊ₗ**\n",
    "\n",
    "Where δₜ = rₜ + γV(sₜ₊₁) - V(sₜ) and λ ∈ [0,1] controls bias-variance tradeoff.\n",
    "\n",
    "## 5.2 Comparative Analysis of Variance Reduction\n",
    "\n",
    "We'll implement and compare different variance reduction techniques to understand their effectiveness and computational trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneralizedAdvantageEstimation:\n    @staticmethod\n    def compute_gae(rewards, values, dones, gamma=0.99, lambda_=0.95):\n        advantages = []\n        gae = 0\n        for i in reversed(range(len(rewards))):\n            if i == len(rewards) - 1:\n                next_value = 0\n            else:\n                next_value = values[i + 1]\n            delta = rewards[i] + gamma * next_value * (1 - dones[i]) - values[i]\n            gae = delta + gamma * lambda_ * (1 - dones[i]) * gae\n            advantages.insert(0, gae)\n        returns = [adv + val for adv, val in zip(advantages, values)]\n        return np.array(advantages), np.array(returns)\nclass ControlVariateREINFORCE(REINFORCEAgent):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.control_variate = nn.Sequential(\n            nn.Linear(self.state_dim, 64),\n            nn.ReLU(),\n            nn.Linear(64, 1)\n        ).to(device)\n        self.cv_optimizer = optim.Adam(self.control_variate.parameters(), lr=1e-3)\n        self.cv_predictions = []\n        self.cv_targets = []\n        self.cv_losses = []\n    def update_policy(self, states, actions, returns):\n        states_tensor = torch.FloatTensor(states).to(device)\n        actions_tensor = torch.LongTensor(actions).to(device)\n        returns_tensor = torch.FloatTensor(returns).to(device)\n        cv_predictions = self.control_variate(states_tensor).squeeze()\n        cv_loss = F.mse_loss(cv_predictions, returns_tensor)\n        self.cv_optimizer.zero_grad()\n        cv_loss.backward()\n        self.cv_optimizer.step()\n        with torch.no_grad():\n            cv_pred_detached = cv_predictions.detach()\n            cov = torch.mean((returns_tensor - returns_tensor.mean()) * \n                           (cv_pred_detached - cv_pred_detached.mean()))\n            var_cv = torch.var(cv_pred_detached)\n            c_optimal = cov / (var_cv + 1e-8)\n            c_optimal = torch.clamp(c_optimal, -2.0, 2.0)\n            controlled_returns = returns_tensor - c_optimal * (cv_pred_detached - cv_pred_detached.mean())\n        controlled_returns = (controlled_returns - controlled_returns.mean()) / (controlled_returns.std() + 1e-8)\n        logits = self.policy_net(states_tensor)\n        dist = Categorical(logits=logits)\n        log_probs = dist.log_prob(actions_tensor)\n        policy_loss = -(log_probs * controlled_returns).mean()\n        self.optimizer.zero_grad()\n        policy_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n        self.optimizer.step()\n        self.cv_predictions.extend(cv_predictions.detach().cpu().numpy())\n        self.cv_targets.extend(returns_tensor.cpu().numpy())\n        self.cv_losses.append(cv_loss.item())\n        entropy = dist.entropy().mean()\n        self.entropy_history.append(entropy.item())\n        self.policy_losses.append(policy_loss.item())\n        return policy_loss.item()\nclass GAEActorCritic(ActorCriticAgent):\n    def __init__(self, *args, lambda_gae=0.95, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.lambda_gae = lambda_gae\n        self.trajectory_states = []\n        self.trajectory_rewards = []\n        self.trajectory_values = []\n        self.trajectory_dones = []\n        self.trajectory_actions = []\n        self.trajectory_log_probs = []\n    def store_step(self, state, action, reward, value, done, log_prob):\n        self.trajectory_states.append(state)\n        self.trajectory_actions.append(action)\n        self.trajectory_rewards.append(reward)\n        self.trajectory_values.append(value)\n        self.trajectory_dones.append(done)\n        self.trajectory_log_probs.append(log_prob)\n    def update_with_gae(self):\n        if len(self.trajectory_states) == 0:\n            return\n        advantages, returns = GeneralizedAdvantageEstimation.compute_gae(\n            self.trajectory_rewards, \n            self.trajectory_values,\n            self.trajectory_dones,\n            gamma=self.gamma,\n            lambda_=self.lambda_gae\n        )\n        states = torch.FloatTensor(self.trajectory_states).to(device)\n        actions = torch.LongTensor(self.trajectory_actions).to(device)\n        returns = torch.FloatTensor(returns).to(device)\n        advantages = torch.FloatTensor(advantages).to(device)\n        old_log_probs = torch.FloatTensor(self.trajectory_log_probs).to(device)\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        actor_logits = self.actor(states)\n        critic_values = self.critic(states).squeeze()\n        dist = Categorical(logits=actor_logits)\n        new_log_probs = dist.log_prob(actions)\n        actor_loss = -(new_log_probs * advantages).mean()\n        critic_loss = F.mse_loss(critic_values, returns)\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n        self.actor_optimizer.step()\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n        self.critic_optimizer.step()\n        self.actor_losses.append(actor_loss.item())\n        self.critic_losses.append(critic_loss.item())\n        self.advantages.extend(advantages.cpu().numpy())\n        self.trajectory_states = []\n        self.trajectory_rewards = []\n        self.trajectory_values = []\n        self.trajectory_dones = []\n        self.trajectory_actions = []\n        self.trajectory_log_probs = []\n        return actor_loss.item(), critic_loss.item()\n    def train_episode(self, env, update_freq=10):\n        state, _ = env.reset()\n        episode_reward = 0\n        step_count = 0\n        while True:\n            action, log_prob = self.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            value = self.get_value(state)\n            self.store_step(state, action, reward, value, done, log_prob)\n            episode_reward += reward\n            step_count += 1\n            if step_count % update_freq == 0 or done:\n                self.update_with_gae()\n            state = next_state\n            if done:\n                break\n        self.episode_rewards.append(episode_reward)\n        return episode_reward, step_count\ndef variance_reduction_comparison():\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    print(\"=== Variance Reduction Techniques Comparison ===\")\n    methods = {}\n    num_episodes = 200\n    print(\"\\n1. Training Standard REINFORCE...\")\n    reinforce_agent = REINFORCEAgent(state_dim, action_dim, lr=1e-3)\n    reinforce_rewards = []\n    reinforce_grad_norms = []\n    for episode in range(num_episodes):\n        reward, _ = reinforce_agent.train_episode(env)\n        reinforce_rewards.append(reward)\n        if len(reinforce_agent.gradient_norms) > 0:\n            reinforce_grad_norms.append(reinforce_agent.gradient_norms[-1])\n    methods['REINFORCE'] = {\n        'rewards': reinforce_rewards,\n        'grad_norms': reinforce_grad_norms\n    }\n    print(\"2. Training REINFORCE with Control Variates...\")\n    cv_agent = ControlVariateREINFORCE(state_dim, action_dim, lr=1e-3)\n    cv_rewards = []\n    cv_grad_norms = []\n    for episode in range(num_episodes):\n        reward, _ = cv_agent.train_episode(env)\n        cv_rewards.append(reward)\n        if len(cv_agent.gradient_norms) > 0:\n            cv_grad_norms.append(cv_agent.gradient_norms[-1])\n    methods['Control Variate'] = {\n        'rewards': cv_rewards,\n        'grad_norms': cv_grad_norms\n    }\n    print(\"3. Training Standard Actor-Critic...\")\n    ac_agent = ActorCriticAgent(state_dim, action_dim)\n    ac_rewards = []\n    for episode in range(num_episodes):\n        reward, _ = ac_agent.train_episode(env)\n        ac_rewards.append(reward)\n    methods['Actor-Critic'] = {\n        'rewards': ac_rewards,\n        'grad_norms': []\n    }\n    print(\"4. Training GAE Actor-Critic...\")\n    gae_agent = GAEActorCritic(state_dim, action_dim, lambda_gae=0.95)\n    gae_rewards = []\n    for episode in range(num_episodes):\n        reward, _ = gae_agent.train_episode(env, update_freq=5)\n        gae_rewards.append(reward)\n    methods['GAE Actor-Critic'] = {\n        'rewards': gae_rewards,\n        'grad_norms': []\n    }\n    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n    colors = ['red', 'blue', 'green', 'orange']\n    method_names = list(methods.keys())\n    for i, (method, data) in enumerate(methods.items()):\n        window = 20\n        smoothed = pd.Series(data['rewards']).rolling(window).mean()\n        axes[0,0].plot(smoothed, label=method, color=colors[i], linewidth=2)\n    axes[0,0].set_title('Learning Curves Comparison')\n    axes[0,0].set_xlabel('Episode')\n    axes[0,0].set_ylabel('Average Reward')\n    axes[0,0].legend()\n    axes[0,0].grid(True, alpha=0.3)\n    final_performances = [np.mean(data['rewards'][-50:]) for data in methods.values()]\n    bars = axes[0,1].bar(method_names, final_performances, color=colors, alpha=0.7)\n    axes[0,1].set_title('Final Performance (Last 50 Episodes)')\n    axes[0,1].set_ylabel('Average Reward')\n    axes[0,1].grid(True, alpha=0.3)\n    plt.setp(axes[0,1].xaxis.get_majorticklabels(), rotation=45, ha='right')\n    for bar, value in zip(bars, final_performances):\n        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                       f'{value:.1f}', ha='center', va='bottom')\n    threshold = 450\n    episodes_to_threshold = []\n    for method, data in methods.items():\n        smoothed = pd.Series(data['rewards']).rolling(20).mean()\n        threshold_idx = None\n        for i, reward in enumerate(smoothed):\n            if reward >= threshold:\n                threshold_idx = i\n                break\n        episodes_to_threshold.append(threshold_idx if threshold_idx else num_episodes)\n    axes[0,2].bar(method_names, episodes_to_threshold, color=colors, alpha=0.7)\n    axes[0,2].set_title(f'Sample Efficiency (Episodes to {threshold} reward)')\n    axes[0,2].set_ylabel('Episodes')\n    axes[0,2].grid(True, alpha=0.3)\n    plt.setp(axes[0,2].xaxis.get_majorticklabels(), rotation=45, ha='right')\n    grad_methods = [(name, data) for name, data in methods.items() if data['grad_norms']]\n    if grad_methods:\n        for i, (method, data) in enumerate(grad_methods):\n            if data['grad_norms']:\n                axes[1,0].hist(data['grad_norms'], bins=30, alpha=0.5, \n                             label=method, density=True, color=colors[i])\n        axes[1,0].set_title('Gradient Norm Distributions')\n        axes[1,0].set_xlabel('Gradient Norm')\n        axes[1,0].set_ylabel('Density')\n        axes[1,0].legend()\n        axes[1,0].grid(True, alpha=0.3)\n    if len(cv_agent.cv_predictions) > 0:\n        cv_corr = np.corrcoef(cv_agent.cv_predictions, cv_agent.cv_targets)[0,1]\n        axes[1,1].scatter(cv_agent.cv_predictions[:500], cv_agent.cv_targets[:500], \n                         alpha=0.6, s=10)\n        axes[1,1].plot([min(cv_agent.cv_predictions), max(cv_agent.cv_predictions)],\n                      [min(cv_agent.cv_predictions), max(cv_agent.cv_predictions)],\n                      'r--', linewidth=2)\n        axes[1,1].set_title(f'Control Variate Effectiveness (ρ = {cv_corr:.3f})')\n        axes[1,1].set_xlabel('CV Prediction')\n        axes[1,1].set_ylabel('True Return')\n        axes[1,1].grid(True, alpha=0.3)\n    variance_stats = {}\n    for method, data in methods.items():\n        rewards = data['rewards']\n        variance_stats[method] = {\n            'mean': np.mean(rewards),\n            'std': np.std(rewards),\n            'final_mean': np.mean(rewards[-50:]),\n            'final_std': np.std(rewards[-50:])\n        }\n    summary_data = []\n    for method, stats in variance_stats.items():\n        summary_data.append([\n            method,\n            f\"{stats['mean']:.1f}\",\n            f\"{stats['std']:.1f}\",\n            f\"{stats['final_mean']:.1f}\",\n            f\"{stats['final_std']:.1f}\"\n        ])\n    table = axes[1,2].table(cellText=summary_data,\n                           colLabels=['Method', 'Mean', 'Std', 'Final Mean', 'Final Std'],\n                           cellLoc='center',\n                           loc='center')\n    table.auto_set_font_size(False)\n    table.set_fontsize(9)\n    table.scale(1.2, 1.5)\n    axes[1,2].axis('off')\n    axes[1,2].set_title('Performance Statistics')\n    plt.tight_layout()\n    plt.show()\n    print(f\"\\n=== Detailed Analysis ===\")\n    for method, stats in variance_stats.items():\n        print(f\"\\n{method}:\")\n        print(f\"  Overall Performance: {stats['mean']:.2f} ± {stats['std']:.2f}\")\n        print(f\"  Final Performance:   {stats['final_mean']:.2f} ± {stats['final_std']:.2f}\")\n        print(f\"  Sample Efficiency:   {episodes_to_threshold[list(variance_stats.keys()).index(method)]} episodes\")\n    env.close()\n    return methods\nvariance_methods = variance_reduction_comparison()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3dd721",
   "metadata": {},
   "source": [
    "# Section 6: Continuous Action Space Policy Gradients\n",
    "\n",
    "## 6.1 Extension to Continuous Control\n",
    "\n",
    "One of the major advantages of policy gradient methods is their natural ability to handle continuous action spaces. Instead of outputting discrete action probabilities, we parameterize continuous probability distributions.\n",
    "\n",
    "### Gaussian Policy Parameterization\n",
    "\n",
    "For continuous actions, we typically use a Gaussian (Normal) policy:\n",
    "\n",
    "**π_θ(a|s) = N(μ_θ(s), σ_θ(s))**\n",
    "\n",
    "Where:\n",
    "- **μ_θ(s)**: Mean of the action distribution (neural network output)\n",
    "- **σ_θ(s)**: Standard deviation of the action distribution\n",
    "\n",
    "### Policy Gradient for Continuous Actions\n",
    "\n",
    "The policy gradient for Gaussian policies is:\n",
    "\n",
    "**∇_θ ln π_θ(a|s) = ∇_θ ln N(a|μ_θ(s), σ_θ(s))**\n",
    "\n",
    "For a Gaussian policy, this becomes:\n",
    "\n",
    "**∇_θ ln π_θ(a|s) = (a - μ_θ(s))/σ_θ(s)² · ∇_θ μ_θ(s) - ∇_θ ln σ_θ(s)**\n",
    "\n",
    "### Parameterization Strategies\n",
    "\n",
    "1. **Separate Networks**: Different networks for μ and σ\n",
    "2. **Shared Network**: Single network outputting both μ and σ\n",
    "3. **Fixed Variance**: Learn only μ, keep σ constant\n",
    "4. **State-Independent Variance**: Learn σ as a parameter, not function of state\n",
    "\n",
    "## 6.2 Continuous Control Challenges\n",
    "\n",
    "Continuous control introduces several challenges:\n",
    "\n",
    "1. **Action Scaling**: Actions often need to be scaled to environment bounds\n",
    "2. **Exploration**: Balancing exploration vs exploitation in continuous space\n",
    "3. **Stability**: Continuous policies can be more sensitive to hyperparameters\n",
    "4. **Sample Efficiency**: High-dimensional continuous spaces require more samples\n",
    "\n",
    "## 6.3 Popular Continuous Control Environments\n",
    "\n",
    "- **MuJoCo**: Physics-based continuous control (HalfCheetah, Walker2d, etc.)\n",
    "- **PyBullet**: Open-source physics simulation\n",
    "- **Custom Control**: Pendulum, CartPole continuous, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9de6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContinuousActorCritic:\n    def __init__(self, state_dim, action_dim, action_bound=1.0, hidden_dim=128, \n                 actor_lr=1e-4, critic_lr=1e-3, gamma=0.99):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.action_bound = action_bound\n        self.gamma = gamma\n        self.actor = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim * 2)\n        ).to(device)\n        self.critic = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        ).to(device)\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n        self.episode_rewards = []\n        self.actor_losses = []\n        self.critic_losses = []\n        self.action_means = []\n        self.action_stds = []\n    def select_action(self, state, deterministic=False):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            actor_output = self.actor(state)\n        mean = actor_output[:, :self.action_dim]\n        log_std = actor_output[:, self.action_dim:]\n        log_std = torch.clamp(log_std, min=-20, max=2)\n        std = torch.exp(log_std)\n        if deterministic:\n            action = mean\n        else:\n            dist = Normal(mean, std)\n            action = dist.sample()\n        action_scaled = torch.tanh(action) * self.action_bound\n        if not deterministic:\n            log_prob = dist.log_prob(action).sum(axis=-1)\n            log_prob -= (2 * (np.log(2) - action - F.softplus(-2 * action))).sum(axis=-1)\n        else:\n            log_prob = None\n        return action_scaled.cpu().numpy().flatten(), log_prob\n    def get_value(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            value = self.critic(state)\n        return value.item()\n    def update(self, state, action, reward, next_state, done, log_prob):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        next_state = torch.FloatTensor(next_state).unsqueeze(0).to(device)\n        action = torch.FloatTensor(action).unsqueeze(0).to(device)\n        current_value = self.critic(state)\n        if done:\n            target_value = reward\n        else:\n            with torch.no_grad():\n                next_value = self.critic(next_state)\n            target_value = reward + self.gamma * next_value\n        advantage = target_value - current_value\n        critic_loss = advantage.pow(2)\n        self.critic_optimizer.zero_grad()\n        critic_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n        self.critic_optimizer.step()\n        actor_loss = -log_prob * advantage.detach()\n        self.actor_optimizer.zero_grad()\n        actor_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n        self.actor_optimizer.step()\n        actor_output = self.actor(state)\n        mean = actor_output[:, :self.action_dim]\n        log_std = actor_output[:, self.action_dim:]\n        std = torch.exp(torch.clamp(log_std, min=-20, max=2))\n        self.action_means.append(mean.mean().item())\n        self.action_stds.append(std.mean().item())\n        self.actor_losses.append(actor_loss.item())\n        self.critic_losses.append(critic_loss.item())\n        return actor_loss.item(), critic_loss.item()\n    def train_episode(self, env):\n        state, _ = env.reset()\n        episode_reward = 0\n        step_count = 0\n        while True:\n            action, log_prob = self.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            if log_prob is not None:\n                actor_loss, critic_loss = self.update(\n                    state, action, reward, next_state, done, log_prob)\n            episode_reward += reward\n            step_count += 1\n            state = next_state\n            if done:\n                break\n        self.episode_rewards.append(episode_reward)\n        return episode_reward, step_count\nclass PPOContinuous:\n    def __init__(self, state_dim, action_dim, action_bound=1.0, hidden_dim=128,\n                 lr=3e-4, gamma=0.99, eps_clip=0.2, k_epochs=4, entropy_coef=0.01):\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.action_bound = action_bound\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.k_epochs = k_epochs\n        self.entropy_coef = entropy_coef\n        self.network = SharedActorCriticNetwork(state_dim, action_dim * 2).to(device)\n        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n        self.states = []\n        self.actions = []\n        self.rewards = []\n        self.log_probs = []\n        self.values = []\n        self.dones = []\n        self.episode_rewards = []\n        self.policy_losses = []\n        self.value_losses = []\n        self.entropy_losses = []\n    def select_action(self, state):\n        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n        with torch.no_grad():\n            actor_critic_output, value = self.network(state)\n        mean = actor_critic_output[:, :self.action_dim]\n        log_std = actor_critic_output[:, self.action_dim:]\n        log_std = torch.clamp(log_std, min=-20, max=2)\n        std = torch.exp(log_std)\n        dist = Normal(mean, std)\n        action = dist.sample()\n        action_scaled = torch.tanh(action) * self.action_bound\n        log_prob = dist.log_prob(action).sum(axis=-1)\n        log_prob -= (2 * (np.log(2) - action - F.softplus(-2 * action))).sum(axis=-1)\n        return action_scaled.cpu().numpy().flatten(), log_prob.item(), value.item()\n    def store_transition(self, state, action, reward, log_prob, value, done):\n        self.states.append(state)\n        self.actions.append(action)\n        self.rewards.append(reward)\n        self.log_probs.append(log_prob)\n        self.values.append(value)\n        self.dones.append(done)\n    def compute_advantages(self):\n        advantages = []\n        returns = []\n        gae = 0\n        for i in reversed(range(len(self.rewards))):\n            if i == len(self.rewards) - 1:\n                next_value = 0\n            else:\n                next_value = self.values[i + 1]\n            delta = self.rewards[i] + self.gamma * next_value * (1 - self.dones[i]) - self.values[i]\n            gae = delta + self.gamma * 0.95 * (1 - self.dones[i]) * gae\n            advantages.insert(0, gae)\n        for i in range(len(advantages)):\n            returns.append(advantages[i] + self.values[i])\n        return torch.FloatTensor(advantages).to(device), torch.FloatTensor(returns).to(device)\n    def update(self):\n        if len(self.states) == 0:\n            return\n        advantages, returns = self.compute_advantages()\n        old_states = torch.FloatTensor(self.states).to(device)\n        old_actions = torch.FloatTensor(self.actions).to(device)\n        old_log_probs = torch.FloatTensor(self.log_probs).to(device)\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        for _ in range(self.k_epochs):\n            actor_output, values = self.network(old_states)\n            mean = actor_output[:, :self.action_dim]\n            log_std = actor_output[:, self.action_dim:]\n            log_std = torch.clamp(log_std, min=-20, max=2)\n            std = torch.exp(log_std)\n            dist = Normal(mean, std)\n            actions_unscaled = torch.atanh(torch.clamp(old_actions / self.action_bound, -0.999, 0.999))\n            new_log_probs = dist.log_prob(actions_unscaled).sum(axis=-1)\n            new_log_probs -= (2 * (np.log(2) - actions_unscaled - F.softplus(-2 * actions_unscaled))).sum(axis=-1)\n            ratio = torch.exp(new_log_probs - old_log_probs)\n            surr1 = ratio * advantages\n            surr2 = torch.clamp(ratio, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n            policy_loss = -torch.min(surr1, surr2).mean()\n            value_loss = F.mse_loss(values.squeeze(), returns)\n            entropy = dist.entropy().mean()\n            entropy_loss = -entropy\n            total_loss = policy_loss + 0.5 * value_loss + self.entropy_coef * entropy_loss\n            self.optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=0.5)\n            self.optimizer.step()\n            if _ == self.k_epochs - 1:\n                self.policy_losses.append(policy_loss.item())\n                self.value_losses.append(value_loss.item())\n                self.entropy_losses.append(entropy_loss.item())\n        self.states = []\n        self.actions = []\n        self.rewards = []\n        self.log_probs = []\n        self.values = []\n        self.dones = []\n    def train_episode(self, env, update_freq=2048):\n        state, _ = env.reset()\n        episode_reward = 0\n        step_count = 0\n        while True:\n            action, log_prob, value = self.select_action(state)\n            next_state, reward, terminated, truncated, _ = env.step(action)\n            done = terminated or truncated\n            self.store_transition(state, action, reward, log_prob, value, done)\n            episode_reward += reward\n            step_count += 1\n            if len(self.states) >= update_freq or done:\n                self.update()\n            state = next_state\n            if done:\n                break\n        self.episode_rewards.append(episode_reward)\n        return episode_reward, step_count\ndef test_continuous_control():\n    env = gym.make('Pendulum-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.shape[0]\n    action_bound = float(env.action_space.high[0])\n    print(\"=== Continuous Control Policy Gradients ===\")\n    print(f\"Environment: Pendulum-v1\")\n    print(f\"State dim: {state_dim}, Action dim: {action_dim}, Action bound: {action_bound}\")\n    methods = {}\n    num_episodes = 200\n    print(\"\\n1. Training Continuous Actor-Critic...\")\n    ac_agent = ContinuousActorCritic(state_dim, action_dim, action_bound)\n    ac_rewards = []\n    for episode in range(num_episodes):\n        reward, _ = ac_agent.train_episode(env)\n        ac_rewards.append(reward)\n        if (episode + 1) % 50 == 0:\n            avg_reward = np.mean(ac_rewards[-50:])\n            avg_std = np.mean(ac_agent.action_stds[-50:]) if ac_agent.action_stds else 0\n            print(f\"AC Episode {episode+1:3d} | Avg Reward: {avg_reward:7.2f} | Action Std: {avg_std:.3f}\")\n    methods['Continuous AC'] = ac_rewards\n    print(\"\\n2. Training PPO Continuous...\")\n    ppo_agent = PPOContinuous(state_dim, action_dim, action_bound)\n    ppo_rewards = []\n    for episode in range(num_episodes):\n        reward, _ = ppo_agent.train_episode(env, update_freq=1024)\n        ppo_rewards.append(reward)\n        if (episode + 1) % 50 == 0:\n            avg_reward = np.mean(ppo_rewards[-50:])\n            print(f\"PPO Episode {episode+1:3d} | Avg Reward: {avg_reward:7.2f}\")\n    methods['PPO Continuous'] = ppo_rewards\n    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n    window = 20\n    colors = ['blue', 'red']\n    for i, (method, rewards) in enumerate(methods.items()):\n        smoothed = pd.Series(rewards).rolling(window).mean()\n        axes[0,0].plot(smoothed, label=method, color=colors[i], linewidth=2)\n    axes[0,0].set_title('Learning Curves: Continuous Control')\n    axes[0,0].set_xlabel('Episode')\n    axes[0,0].set_ylabel('Episode Reward')\n    axes[0,0].legend()\n    axes[0,0].grid(True, alpha=0.3)\n    final_performances = [np.mean(rewards[-50:]) for rewards in methods.values()]\n    method_names = list(methods.keys())\n    bars = axes[0,1].bar(method_names, final_performances, color=colors, alpha=0.7)\n    axes[0,1].set_title('Final Performance (Last 50 Episodes)')\n    axes[0,1].set_ylabel('Average Reward')\n    axes[0,1].grid(True, alpha=0.3)\n    for bar, value in zip(bars, final_performances):\n        axes[0,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n                       f'{value:.1f}', ha='center', va='bottom')\n    if ac_agent.action_means and ac_agent.action_stds:\n        axes[1,0].plot(ac_agent.action_means, label='Action Mean', alpha=0.7)\n        axes[1,0].plot(ac_agent.action_stds, label='Action Std', alpha=0.7)\n        axes[1,0].set_title('Action Statistics (Continuous AC)')\n        axes[1,0].set_xlabel('Update Step')\n        axes[1,0].set_ylabel('Value')\n        axes[1,0].legend()\n        axes[1,0].grid(True, alpha=0.3)\n    test_states = [\n        [1.0, 0.0, 0.0],\n        [0.0, 1.0, 0.0],\n        [-1.0, 0.0, 0.0],\n        [0.0, 0.0, 1.0],\n        [0.0, 0.0, -1.0]\n    ]\n    ac_actions = []\n    ppo_actions = []\n    for state in test_states:\n        ac_action, _ = ac_agent.select_action(state)\n        ppo_action, _, _ = ppo_agent.select_action(state)\n        ac_actions.append(ac_action[0])\n        ppo_actions.append(ppo_action[0])\n    x_pos = range(len(test_states))\n    width = 0.35\n    axes[1,1].bar([x - width/2 for x in x_pos], ac_actions, width, \n                  label='Continuous AC', alpha=0.7, color='blue')\n    axes[1,1].bar([x + width/2 for x in x_pos], ppo_actions, width,\n                  label='PPO Continuous', alpha=0.7, color='red')\n    axes[1,1].set_title('Action Selection for Test States')\n    axes[1,1].set_xlabel('Test State Index')\n    axes[1,1].set_ylabel('Selected Action')\n    axes[1,1].set_xticks(x_pos)\n    axes[1,1].legend()\n    axes[1,1].grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    print(f\"\\n=== Results Summary ===\")\n    for method, rewards in methods.items():\n        final_perf = np.mean(rewards[-50:])\n        final_std = np.std(rewards[-50:])\n        print(f\"{method}:\")\n        print(f\"  Final Performance: {final_perf:.2f} ± {final_std:.2f}\")\n        print(f\"  Best Episode: {max(rewards):.2f}\")\n    print(f\"\\n=== Policy Evaluation ===\")\n    for method, agent in [('Continuous AC', ac_agent), ('PPO Continuous', ppo_agent)]:\n        test_rewards = []\n        for _ in range(10):\n            state, _ = env.reset()\n            total_reward = 0\n            for _ in range(200):\n                if hasattr(agent, 'select_action'):\n                    if method == 'Continuous AC':\n                        action, _ = agent.select_action(state, deterministic=True)\n                    else:\n                        action, _, _ = agent.select_action(state)\n                else:\n                    action = env.action_space.sample()\n                state, reward, terminated, truncated, _ = env.step(action)\n                total_reward += reward\n                if terminated or truncated:\n                    break\n            test_rewards.append(total_reward)\n        print(f\"{method} Test Performance: {np.mean(test_rewards):.2f} ± {np.std(test_rewards):.2f}\")\n    env.close()\n    return methods, ac_agent, ppo_agent\ncontinuous_results = test_continuous_control()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380578d8",
   "metadata": {},
   "source": [
    "# Section 7: Comprehensive Performance Analysis and Comparison\n",
    "\n",
    "In this section, we conduct a thorough empirical analysis of the policy gradient methods implemented in previous sections. We compare different algorithms across multiple environments and analyze their strengths and weaknesses.\n",
    "\n",
    "## 7.1 Experimental Setup\n",
    "\n",
    "Our experimental design includes:\n",
    "\n",
    "1. **Multiple Environments**: We test on both discrete (CartPole, LunarLander) and continuous (Pendulum, MountainCarContinuous) control tasks\n",
    "2. **Multiple Algorithms**: REINFORCE, Actor-Critic, A2C, A3C, and PPO variants\n",
    "3. **Statistical Significance**: Each algorithm is run multiple times with different random seeds\n",
    "4. **Variance Analysis**: We analyze both sample efficiency and final performance\n",
    "\n",
    "## 7.2 Hyperparameter Sensitivity\n",
    "\n",
    "Policy gradient methods are known to be sensitive to hyperparameters. We analyze the impact of:\n",
    "- Learning rates (actor and critic)\n",
    "- Network architectures\n",
    "- Entropy coefficients\n",
    "- Discount factors\n",
    "- Variance reduction techniques\n",
    "\n",
    "## 7.3 Convergence Analysis\n",
    "\n",
    "We examine:\n",
    "- **Sample Efficiency**: How quickly do algorithms learn?\n",
    "- **Stability**: How consistent is the performance across runs?\n",
    "- **Final Performance**: What is the asymptotic performance?\n",
    "- **Robustness**: How do algorithms perform across different environments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefd95fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PerformanceAnalyzer:\n    def __init__(self):\n        self.results = {}\n        self.statistical_tests = {}\n    def run_multiple_seeds(self, agent_class, env_name, num_runs=5, num_episodes=200, **agent_kwargs):\n        all_rewards = []\n        all_learning_curves = []\n        for run in range(num_runs):\n            print(f\"  Run {run+1}/{num_runs}\")\n            torch.manual_seed(42 + run)\n            np.random.seed(42 + run)\n            env = gym.make(env_name)\n            if hasattr(env.action_space, 'n'):\n                state_dim = env.observation_space.shape[0]\n                action_dim = env.action_space.n\n                agent = agent_class(state_dim, action_dim, **agent_kwargs)\n            else:\n                state_dim = env.observation_space.shape[0]\n                action_dim = env.action_space.shape[0]\n                action_bound = float(env.action_space.high[0])\n                agent = agent_class(state_dim, action_dim, action_bound, **agent_kwargs)\n            episode_rewards = []\n            for episode in range(num_episodes):\n                if hasattr(agent, 'train_episode'):\n                    reward, _ = agent.train_episode(env)\n                else:\n                    state, _ = env.reset()\n                    reward = 0\n                    while True:\n                        action, log_prob = agent.select_action(state)\n                        next_state, r, terminated, truncated, _ = env.step(action)\n                        done = terminated or truncated\n                        agent.update(state, action, r, next_state, done, log_prob)\n                        reward += r\n                        state = next_state\n                        if done:\n                            break\n                episode_rewards.append(reward)\n            all_rewards.extend(episode_rewards)\n            all_learning_curves.append(episode_rewards)\n            env.close()\n        return all_learning_curves, all_rewards\n    def compute_metrics(self, learning_curves, window=50):\n        learning_curves = np.array(learning_curves)\n        mean_curve = np.mean(learning_curves, axis=0)\n        target_performance = np.mean(mean_curve[-window:])\n        threshold = target_performance * 0.8\n        sample_efficiency = None\n        for i in range(len(mean_curve)):\n            if np.mean(mean_curve[max(0, i-window):i+1]) >= threshold:\n                sample_efficiency = i\n                break\n        final_rewards = learning_curves[:, -window:]\n        final_performance_mean = np.mean(final_rewards)\n        final_performance_std = np.std(final_rewards)\n        learning_stability = []\n        for curve in learning_curves:\n            smoothed = pd.Series(curve).rolling(window).mean().dropna()\n            if len(smoothed) > 0:\n                stability = np.std(smoothed) / (np.mean(smoothed) + 1e-8)\n                learning_stability.append(stability)\n        avg_stability = np.mean(learning_stability)\n        peak_performance = np.max([np.max(curve) for curve in learning_curves])\n        mid_start = len(mean_curve) // 3\n        mid_end = 2 * len(mean_curve) // 3\n        if mid_end > mid_start:\n            x = np.arange(mid_start, mid_end)\n            y = mean_curve[mid_start:mid_end]\n            convergence_rate = np.polyfit(x, y, 1)[0] if len(x) > 1 else 0\n        else:\n            convergence_rate = 0\n        return {\n            'sample_efficiency': sample_efficiency,\n            'final_performance_mean': final_performance_mean,\n            'final_performance_std': final_performance_std,\n            'learning_stability': avg_stability,\n            'peak_performance': peak_performance,\n            'convergence_rate': convergence_rate,\n            'learning_curves': learning_curves\n        }\n    def run_comprehensive_comparison(self):\n        discrete_algorithms = [\n            ('REINFORCE', REINFORCEAgent, {'lr': 1e-3, 'gamma': 0.99}),\n            ('Actor-Critic', ActorCriticAgent, {'actor_lr': 1e-4, 'critic_lr': 1e-3, 'gamma': 0.99}),\n            ('A2C', A2CAgent, {'lr': 1e-4, 'gamma': 0.99, 'entropy_coef': 0.01}),\n        ]\n        continuous_algorithms = [\n            ('Continuous AC', ContinuousActorCritic, {'actor_lr': 1e-4, 'critic_lr': 1e-3, 'gamma': 0.99}),\n            ('PPO Continuous', PPOContinuous, {'lr': 3e-4, 'gamma': 0.99, 'eps_clip': 0.2}),\n        ]\n        discrete_envs = ['CartPole-v1', 'LunarLander-v2']\n        continuous_envs = ['Pendulum-v1']\n        all_results = {}\n        for env_name in discrete_envs:\n            print(f\"\\n=== Testing on {env_name} ===\")\n            all_results[env_name] = {}\n            for alg_name, alg_class, alg_kwargs in discrete_algorithms:\n                print(f\"\\nTesting {alg_name}...\")\n                try:\n                    learning_curves, all_rewards = self.run_multiple_seeds(\n                        alg_class, env_name, num_runs=3, num_episodes=150, **alg_kwargs\n                    )\n                    metrics = self.compute_metrics(learning_curves)\n                    all_results[env_name][alg_name] = metrics\n                    print(f\"  Final Performance: {metrics['final_performance_mean']:.2f} ± {metrics['final_performance_std']:.2f}\")\n                    print(f\"  Sample Efficiency: {metrics['sample_efficiency']} episodes\")\n                except Exception as e:\n                    print(f\"  Error with {alg_name}: {e}\")\n                    all_results[env_name][alg_name] = None\n        for env_name in continuous_envs:\n            print(f\"\\n=== Testing on {env_name} ===\")\n            all_results[env_name] = {}\n            for alg_name, alg_class, alg_kwargs in continuous_algorithms:\n                print(f\"\\nTesting {alg_name}...\")\n                try:\n                    learning_curves, all_rewards = self.run_multiple_seeds(\n                        alg_class, env_name, num_runs=3, num_episodes=150, **alg_kwargs\n                    )\n                    metrics = self.compute_metrics(learning_curves)\n                    all_results[env_name][alg_name] = metrics\n                    print(f\"  Final Performance: {metrics['final_performance_mean']:.2f} ± {metrics['final_performance_std']:.2f}\")\n                    print(f\"  Sample Efficiency: {metrics['sample_efficiency']} episodes\")\n                except Exception as e:\n                    print(f\"  Error with {alg_name}: {e}\")\n                    all_results[env_name][alg_name] = None\n        self.results = all_results\n        return all_results\n    def visualize_comprehensive_results(self):\n        if not self.results:\n            print(\"No results to visualize. Run comprehensive comparison first.\")\n            return\n        fig = plt.figure(figsize=(20, 16))\n        env_names = list(self.results.keys())\n        n_envs = len(env_names)\n        for i, env_name in enumerate(env_names):\n            ax = plt.subplot(3, n_envs, i + 1)\n            for alg_name, metrics in self.results[env_name].items():\n                if metrics is not None and 'learning_curves' in metrics:\n                    curves = metrics['learning_curves']\n                    mean_curve = np.mean(curves, axis=0)\n                    std_curve = np.std(curves, axis=0)\n                    x = range(len(mean_curve))\n                    ax.plot(x, mean_curve, label=alg_name, linewidth=2)\n                    ax.fill_between(x, mean_curve - std_curve, mean_curve + std_curve, alpha=0.2)\n            ax.set_title(f'{env_name} - Learning Curves')\n            ax.set_xlabel('Episodes')\n            ax.set_ylabel('Episode Reward')\n            ax.legend()\n            ax.grid(True, alpha=0.3)\n        for i, env_name in enumerate(env_names):\n            ax = plt.subplot(3, n_envs, n_envs + i + 1)\n            alg_names = []\n            final_means = []\n            final_stds = []\n            for alg_name, metrics in self.results[env_name].items():\n                if metrics is not None:\n                    alg_names.append(alg_name)\n                    final_means.append(metrics['final_performance_mean'])\n                    final_stds.append(metrics['final_performance_std'])\n            if alg_names:\n                bars = ax.bar(range(len(alg_names)), final_means, \n                             yerr=final_stds, capsize=5, alpha=0.7)\n                ax.set_title(f'{env_name} - Final Performance')\n                ax.set_xlabel('Algorithm')\n                ax.set_ylabel('Average Reward')\n                ax.set_xticks(range(len(alg_names)))\n                ax.set_xticklabels(alg_names, rotation=45)\n                ax.grid(True, alpha=0.3)\n                for bar, mean, std in zip(bars, final_means, final_stds):\n                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std,\n                           f'{mean:.1f}', ha='center', va='bottom', fontsize=9)\n        for i, env_name in enumerate(env_names):\n            ax = plt.subplot(3, n_envs, 2*n_envs + i + 1)\n            alg_names = []\n            sample_effs = []\n            for alg_name, metrics in self.results[env_name].items():\n                if metrics is not None and metrics['sample_efficiency'] is not None:\n                    alg_names.append(alg_name)\n                    sample_effs.append(metrics['sample_efficiency'])\n            if alg_names:\n                bars = ax.bar(range(len(alg_names)), sample_effs, alpha=0.7, color='green')\n                ax.set_title(f'{env_name} - Sample Efficiency')\n                ax.set_xlabel('Algorithm')\n                ax.set_ylabel('Episodes to Convergence')\n                ax.set_xticks(range(len(alg_names)))\n                ax.set_xticklabels(alg_names, rotation=45)\n                ax.grid(True, alpha=0.3)\n                for bar, eff in zip(bars, sample_effs):\n                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n                           f'{eff}', ha='center', va='bottom', fontsize=9)\n        plt.tight_layout()\n        plt.show()\n        self.create_summary_table()\n    def create_summary_table(self):\n        print(\"\\n\" + \"=\"*100)\n        print(\"COMPREHENSIVE PERFORMANCE SUMMARY\")\n        print(\"=\"*100)\n        for env_name, env_results in self.results.items():\n            print(f\"\\n{env_name}:\")\n            print(\"-\" * 80)\n            print(f\"{'Algorithm':<15} {'Final Perf':<12} {'Std':<8} {'Sample Eff':<12} {'Stability':<10} {'Peak':<10}\")\n            print(\"-\" * 80)\n            for alg_name, metrics in env_results.items():\n                if metrics is not None:\n                    final_perf = f\"{metrics['final_performance_mean']:.2f}\"\n                    std = f\"{metrics['final_performance_std']:.2f}\"\n                    sample_eff = f\"{metrics['sample_efficiency']}\" if metrics['sample_efficiency'] is not None else \"N/A\"\n                    stability = f\"{metrics['learning_stability']:.3f}\"\n                    peak = f\"{metrics['peak_performance']:.2f}\"\n                    print(f\"{alg_name:<15} {final_perf:<12} {std:<8} {sample_eff:<12} {stability:<10} {peak:<10}\")\n                else:\n                    print(f\"{alg_name:<15} {'ERROR':<12} {'-':<8} {'-':<12} {'-':<10} {'-':<10}\")\n    def hyperparameter_sensitivity_analysis(self):\n        print(\"\\n=== Hyperparameter Sensitivity Analysis ===\")\n        learning_rates = [1e-5, 1e-4, 1e-3, 1e-2]\n        env_name = 'CartPole-v1'\n        sensitivity_results = {}\n        for lr in learning_rates:\n            print(f\"\\nTesting Actor-Critic with lr={lr}\")\n            try:\n                learning_curves, _ = self.run_multiple_seeds(\n                    ActorCriticAgent, env_name, num_runs=3, num_episodes=100,\n                    actor_lr=lr, critic_lr=lr*10, gamma=0.99\n                )\n                metrics = self.compute_metrics(learning_curves)\n                sensitivity_results[lr] = metrics['final_performance_mean']\n                print(f\"  Final Performance: {metrics['final_performance_mean']:.2f}\")\n            except Exception as e:\n                print(f\"  Error: {e}\")\n                sensitivity_results[lr] = None\n        plt.figure(figsize=(10, 6))\n        valid_lrs = [lr for lr, perf in sensitivity_results.items() if perf is not None]\n        valid_perfs = [perf for lr, perf in sensitivity_results.items() if perf is not None]\n        if valid_lrs:\n            plt.semilogx(valid_lrs, valid_perfs, 'bo-', linewidth=2, markersize=8)\n            plt.xlabel('Learning Rate')\n            plt.ylabel('Final Performance')\n            plt.title('Hyperparameter Sensitivity: Actor-Critic Learning Rate')\n            plt.grid(True, alpha=0.3)\n            best_idx = np.argmax(valid_perfs)\n            best_lr = valid_lrs[best_idx]\n            best_perf = valid_perfs[best_idx]\n            plt.annotate(f'Best: {best_lr}, {best_perf:.2f}', \n                        xy=(best_lr, best_perf), xytext=(best_lr*10, best_perf+10),\n                        arrowprops=dict(arrowstyle='->', color='red'))\n            plt.tight_layout()\n            plt.show()\n            print(f\"\\nBest learning rate: {best_lr} with performance: {best_perf:.2f}\")\n        else:\n            print(\"No valid results for sensitivity analysis\")\nprint(\"Starting Comprehensive Performance Analysis...\")\nprint(\"This may take several minutes...\")\nanalyzer = PerformanceAnalyzer()\nresults = analyzer.run_comprehensive_comparison()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcab34e7",
   "metadata": {},
   "source": [
    "# Section 8: Practical Applications and Advanced Topics\n",
    "\n",
    "Policy gradient methods have found widespread application in numerous real-world domains. In this final section, we explore practical considerations, advanced techniques, and future directions in policy gradient research.\n",
    "\n",
    "## 8.1 Real-World Applications\n",
    "\n",
    "### 8.1.1 Robotics and Control\n",
    "- **Robotic Manipulation**: Policy gradients excel in high-dimensional continuous control tasks\n",
    "- **Locomotion**: Learning walking, running, and complex movement patterns\n",
    "- **Autonomous Vehicles**: Path planning and control in dynamic environments\n",
    "\n",
    "### 8.1.2 Game Playing and Strategy\n",
    "- **AlphaGo and AlphaZero**: Combining policy gradients with Monte Carlo Tree Search\n",
    "- **StarCraft II**: Managing complex strategy games with partial observability\n",
    "- **Multi-agent Systems**: Learning coordination and competition strategies\n",
    "\n",
    "### 8.1.3 Natural Language Processing\n",
    "- **Neural Machine Translation**: Learning translation policies with attention mechanisms\n",
    "- **Dialogue Systems**: Optimizing conversational agents for user satisfaction\n",
    "- **Text Generation**: Fine-tuning language models with human feedback (RLHF)\n",
    "\n",
    "### 8.1.4 Finance and Trading\n",
    "- **Portfolio Optimization**: Learning adaptive trading strategies\n",
    "- **Risk Management**: Dynamic hedging and exposure control\n",
    "- **Market Making**: Optimizing bid-ask spreads in electronic trading\n",
    "\n",
    "## 8.2 Advanced Techniques\n",
    "\n",
    "### 8.2.1 Trust Region Methods\n",
    "- **TRPO (Trust Region Policy Optimization)**: Ensuring stable policy updates\n",
    "- **Natural Policy Gradients**: Using the Fisher Information Matrix\n",
    "- **Conjugate Gradient Methods**: Efficient computation of natural gradients\n",
    "\n",
    "### 8.2.2 Off-Policy Learning\n",
    "- **Importance Sampling**: Learning from data collected by different policies\n",
    "- **Off-Policy Actor-Critic (Off-PAC)**: Combining off-policy learning with actor-critic\n",
    "- **Retrace and V-trace**: Advanced off-policy correction methods\n",
    "\n",
    "### 8.2.3 Meta-Learning and Transfer\n",
    "- **Model-Agnostic Meta-Learning (MAML)**: Learning to adapt quickly to new tasks\n",
    "- **Transfer Learning**: Leveraging learned policies across domains\n",
    "- **Multi-Task Learning**: Sharing representations across related tasks\n",
    "\n",
    "## 8.3 Current Challenges and Limitations\n",
    "\n",
    "### 8.3.1 Sample Efficiency\n",
    "- Policy gradients typically require many environment interactions\n",
    "- Exploration vs exploitation trade-offs in complex environments\n",
    "- Need for better variance reduction techniques\n",
    "\n",
    "### 8.3.2 Hyperparameter Sensitivity\n",
    "- Performance heavily dependent on learning rates and network architectures\n",
    "- Difficulty in transferring hyperparameters across environments\n",
    "- Need for adaptive and robust optimization methods\n",
    "\n",
    "### 8.3.3 Theoretical Understanding\n",
    "- Limited theoretical guarantees for convergence\n",
    "- Understanding of exploration properties\n",
    "- Generalization bounds and sample complexity analysis\n",
    "\n",
    "## 8.4 Future Directions\n",
    "\n",
    "### 8.4.1 Hierarchical Reinforcement Learning\n",
    "- Learning policies at multiple temporal scales\n",
    "- Options and temporal abstractions\n",
    "- Goal-conditioned reinforcement learning\n",
    "\n",
    "### 8.4.2 Safe and Constrained Policy Learning\n",
    "- Incorporating safety constraints into policy optimization\n",
    "- Risk-aware policy gradients\n",
    "- Robust policy learning under uncertainty\n",
    "\n",
    "### 8.4.3 Multi-Agent Policy Gradients\n",
    "- Learning in multi-agent environments\n",
    "- Cooperative and competitive scenarios\n",
    "- Communication and coordination mechanisms\n",
    "\n",
    "## 8.5 Implementation Best Practices\n",
    "\n",
    "### 8.5.1 Network Architecture Design\n",
    "- **Layer Normalization**: Improving training stability\n",
    "- **Residual Connections**: Facilitating gradient flow in deep networks\n",
    "- **Attention Mechanisms**: Handling variable-length sequences and partial observability\n",
    "\n",
    "### 8.5.2 Training Strategies\n",
    "- **Curriculum Learning**: Gradually increasing task difficulty\n",
    "- **Experience Replay**: Efficiently using collected experience\n",
    "- **Distributed Training**: Scaling to large-scale problems\n",
    "\n",
    "### 8.5.3 Debugging and Analysis\n",
    "- **Gradient Monitoring**: Detecting vanishing and exploding gradients\n",
    "- **Policy Visualization**: Understanding learned behaviors\n",
    "- **Ablation Studies**: Isolating the impact of different components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32c0406",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedPolicyGradientFramework:\n    def __init__(self):\n        self.algorithms = {}\n        self.performance_metrics = {}\n    def demonstrate_curriculum_learning(self):\n        print(\"=== Curriculum Learning Demonstration ===\")\n        environments = [\n            ('CartPole-v1', 'Easy: Basic cart-pole balancing'),\n            ('LunarLander-v2', 'Medium: Lunar lander with complex dynamics'),\n            ('BipedalWalker-v3', 'Hard: Bipedal walking (if available)')\n        ]\n        agent = None\n        cumulative_performance = []\n        for i, (env_name, description) in enumerate(environments[:2]):\n            print(f\"\\nStage {i+1}: {description}\")\n            try:\n                env = gym.make(env_name)\n                if hasattr(env.action_space, 'n'):\n                    state_dim = env.observation_space.shape[0]\n                    action_dim = env.action_space.n\n                    if agent is None:\n                        agent = A2CAgent(state_dim, action_dim, lr=1e-4, gamma=0.99)\n                    else:\n                        old_actor = agent.shared_network.actor\n                        old_critic = agent.shared_network.critic\n                        agent = A2CAgent(state_dim, action_dim, lr=1e-4, gamma=0.99)\n                        if hasattr(old_actor, 'state_dict'):\n                            new_state_dict = agent.shared_network.state_dict()\n                            old_state_dict = old_actor.state_dict()\n                            for name, param in old_state_dict.items():\n                                if name in new_state_dict and new_state_dict[name].shape == param.shape:\n                                    new_state_dict[name] = param\n                            agent.shared_network.load_state_dict(new_state_dict)\n                stage_rewards = []\n                num_episodes = 100 if i == 0 else 50\n                for episode in range(num_episodes):\n                    reward, _ = agent.train_episode(env)\n                    stage_rewards.append(reward)\n                    if (episode + 1) % 25 == 0:\n                        avg_reward = np.mean(stage_rewards[-25:])\n                        print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.2f}\")\n                cumulative_performance.extend(stage_rewards)\n                env.close()\n            except Exception as e:\n                print(f\"  Error with {env_name}: {e}\")\n        if cumulative_performance:\n            plt.figure(figsize=(12, 6))\n            stage_boundaries = [100]\n            colors = ['blue', 'red', 'green']\n            stage_names = ['CartPole', 'LunarLander']\n            current_pos = 0\n            for i, boundary in enumerate(stage_boundaries + [len(cumulative_performance)]):\n                stage_rewards = cumulative_performance[current_pos:boundary]\n                x_vals = range(current_pos, boundary)\n                if stage_rewards:\n                    smoothed = pd.Series(stage_rewards).rolling(10).mean()\n                    plt.plot(x_vals, stage_rewards, alpha=0.3, color=colors[i])\n                    plt.plot(x_vals, smoothed, label=stage_names[i], color=colors[i], linewidth=2)\n                if i < len(stage_boundaries):\n                    plt.axvline(boundary, color='black', linestyle='--', alpha=0.5)\n                    plt.text(boundary, plt.ylim()[1], f'Stage {i+2}', rotation=90, ha='right')\n                current_pos = boundary\n            plt.title('Curriculum Learning: Progressive Difficulty')\n            plt.xlabel('Episode')\n            plt.ylabel('Episode Reward')\n            plt.legend()\n            plt.grid(True, alpha=0.3)\n            plt.tight_layout()\n            plt.show()\n        return cumulative_performance\n    def demonstrate_ensemble_methods(self):\n        print(\"\\n=== Ensemble Methods Demonstration ===\")\n        env_name = 'CartPole-v1'\n        env = gym.make(env_name)\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        ensemble_configs = [\n            {'lr': 1e-4, 'entropy_coef': 0.01, 'name': 'Conservative'},\n            {'lr': 5e-4, 'entropy_coef': 0.05, 'name': 'Balanced'},\n            {'lr': 1e-3, 'entropy_coef': 0.1, 'name': 'Aggressive'},\n        ]\n        ensemble_agents = []\n        for config in ensemble_configs:\n            agent = A2CAgent(state_dim, action_dim, \n                           lr=config['lr'], \n                           entropy_coef=config['entropy_coef'],\n                           gamma=0.99)\n            agent.name = config['name']\n            ensemble_agents.append(agent)\n        num_episodes = 100\n        ensemble_rewards = {agent.name: [] for agent in ensemble_agents}\n        for episode in range(num_episodes):\n            for agent in ensemble_agents:\n                reward, _ = agent.train_episode(env)\n                ensemble_rewards[agent.name].append(reward)\n            if (episode + 1) % 25 == 0:\n                print(f\"Episode {episode+1}:\")\n                for agent in ensemble_agents:\n                    avg_reward = np.mean(ensemble_rewards[agent.name][-25:])\n                    print(f\"  {agent.name}: {avg_reward:.2f}\")\n        def ensemble_select_action(state):\n            votes = []\n            for agent in ensemble_agents:\n                action_probs = agent.get_action_probabilities(state)\n                votes.append(action_probs)\n            ensemble_probs = np.mean(votes, axis=0)\n            return np.argmax(ensemble_probs)\n        test_episodes = 10\n        test_results = {}\n        for agent in ensemble_agents:\n            test_rewards = []\n            for _ in range(test_episodes):\n                state, _ = env.reset()\n                total_reward = 0\n                for _ in range(500):\n                    action, _ = agent.select_action(state, deterministic=True)\n                    state, reward, terminated, truncated, _ = env.step(action)\n                    total_reward += reward\n                    if terminated or truncated:\n                        break\n                test_rewards.append(total_reward)\n            test_results[agent.name] = np.mean(test_rewards)\n        plt.figure(figsize=(15, 5))\n        plt.subplot(1, 3, 1)\n        for agent_name, rewards in ensemble_rewards.items():\n            smoothed = pd.Series(rewards).rolling(10).mean()\n            plt.plot(smoothed, label=agent_name, linewidth=2)\n        plt.title('Ensemble Learning Curves')\n        plt.xlabel('Episode')\n        plt.ylabel('Reward')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.subplot(1, 3, 2)\n        final_performances = [np.mean(rewards[-25:]) for rewards in ensemble_rewards.values()]\n        agent_names = list(ensemble_rewards.keys())\n        bars = plt.bar(agent_names, final_performances, alpha=0.7)\n        plt.title('Final Training Performance')\n        plt.ylabel('Average Reward')\n        plt.xticks(rotation=45)\n        plt.grid(True, alpha=0.3)\n        for bar, value in zip(bars, final_performances):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n                    f'{value:.1f}', ha='center', va='bottom')\n        plt.subplot(1, 3, 3)\n        test_performances = list(test_results.values())\n        bars = plt.bar(agent_names, test_performances, alpha=0.7, color='green')\n        plt.title('Test Performance')\n        plt.ylabel('Average Reward')\n        plt.xticks(rotation=45)\n        plt.grid(True, alpha=0.3)\n        for bar, value in zip(bars, test_performances):\n            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 5,\n                    f'{value:.1f}', ha='center', va='bottom')\n        plt.tight_layout()\n        plt.show()\n        env.close()\n        print(f\"\\nEnsemble Results:\")\n        for name, performance in test_results.items():\n            print(f\"  {name}: {performance:.2f}\")\n        return ensemble_agents, test_results\n    def analyze_policy_interpretability(self):\n        print(\"\\n=== Policy Interpretability Analysis ===\")\n        env = gym.make('CartPole-v1')\n        state_dim = env.observation_space.shape[0]\n        action_dim = env.action_space.n\n        agent = ActorCriticAgent(state_dim, action_dim, \n                               actor_lr=1e-3, critic_lr=1e-2, gamma=0.99)\n        for episode in range(100):\n            agent.train_episode(env)\n        print(\"Analyzing policy across state space...\")\n        test_states = []\n        cart_positions = np.linspace(-2.4, 2.4, 5)\n        cart_velocities = np.linspace(-3, 3, 3)\n        pole_angles = np.linspace(-0.2, 0.2, 5)\n        pole_velocities = np.linspace(-2, 2, 3)\n        for pos in cart_positions:\n            for vel in cart_velocities[:2]:\n                for angle in pole_angles:\n                    for ang_vel in pole_velocities[:2]:\n                        test_states.append([pos, vel, angle, ang_vel])\n        policy_actions = []\n        value_predictions = []\n        for state in test_states:\n            action_probs = agent.get_action_probabilities(state)\n            value = agent.get_value(state)\n            policy_actions.append(np.argmax(action_probs))\n            value_predictions.append(value)\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        positions = [state[0] for state in test_states]\n        angles = [state[2] for state in test_states]\n        actions = policy_actions\n        pos_unique = sorted(set(positions))\n        angle_unique = sorted(set(angles))\n        if len(pos_unique) > 1 and len(angle_unique) > 1:\n            policy_grid = np.zeros((len(angle_unique), len(pos_unique)))\n            value_grid = np.zeros((len(angle_unique), len(pos_unique)))\n            for i, state in enumerate(test_states):\n                if len(set([state[1], state[3]])) <= 2:\n                    try:\n                        pos_idx = pos_unique.index(state[0])\n                        angle_idx = angle_unique.index(state[2])\n                        policy_grid[angle_idx, pos_idx] = actions[i]\n                        value_grid[angle_idx, pos_idx] = value_predictions[i]\n                    except ValueError:\n                        continue\n            im1 = axes[0,0].imshow(policy_grid, cmap='coolwarm', aspect='auto')\n            axes[0,0].set_title('Policy Decisions\\n(0=Left, 1=Right)')\n            axes[0,0].set_xlabel('Cart Position')\n            axes[0,0].set_ylabel('Pole Angle')\n            axes[0,0].set_xticks(range(len(pos_unique)))\n            axes[0,0].set_xticklabels([f'{pos:.1f}' for pos in pos_unique])\n            axes[0,0].set_yticks(range(len(angle_unique)))\n            axes[0,0].set_yticklabels([f'{angle:.2f}' for angle in angle_unique])\n            plt.colorbar(im1, ax=axes[0,0])\n            im2 = axes[0,1].imshow(value_grid, cmap='viridis', aspect='auto')\n            axes[0,1].set_title('Value Function')\n            axes[0,1].set_xlabel('Cart Position')\n            axes[0,1].set_ylabel('Pole Angle')\n            axes[0,1].set_xticks(range(len(pos_unique)))\n            axes[0,1].set_xticklabels([f'{pos:.1f}' for pos in pos_unique])\n            axes[0,1].set_yticks(range(len(angle_unique)))\n            axes[0,1].set_yticklabels([f'{angle:.2f}' for angle in angle_unique])\n            plt.colorbar(im2, ax=axes[0,1])\n        sample_states = test_states[:20]\n        action_probs_list = []\n        for state in sample_states:\n            probs = agent.get_action_probabilities(state)\n            action_probs_list.append(probs)\n        action_probs_array = np.array(action_probs_list)\n        axes[1,0].boxplot([action_probs_array[:, 0], action_probs_array[:, 1]], \n                         labels=['Left (0)', 'Right (1)'])\n        axes[1,0].set_title('Action Probability Distributions')\n        axes[1,0].set_ylabel('Probability')\n        axes[1,0].grid(True, alpha=0.3)\n        axes[1,1].hist(value_predictions, bins=20, alpha=0.7, color='green')\n        axes[1,1].set_title('Value Function Distribution')\n        axes[1,1].set_xlabel('State Value')\n        axes[1,1].set_ylabel('Frequency')\n        axes[1,1].grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        print(f\"Policy Analysis:\")\n        print(f\"  Total states analyzed: {len(test_states)}\")\n        print(f\"  Action distribution: Left={policy_actions.count(0)}, Right={policy_actions.count(1)}\")\n        print(f\"  Value range: [{min(value_predictions):.2f}, {max(value_predictions):.2f}]\")\n        print(f\"  Average value: {np.mean(value_predictions):.2f}\")\n        env.close()\n        return agent, test_states, policy_actions, value_predictions\nframework = AdvancedPolicyGradientFramework()\nprint(\"Running Advanced Policy Gradient Demonstrations...\")\nprint(\"This showcases practical applications and advanced techniques.\")\ncurriculum_results = framework.demonstrate_curriculum_learning()\nensemble_agents, ensemble_results = framework.demonstrate_ensemble_methods()\npolicy_analysis = framework.analyze_policy_interpretability()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9e9428",
   "metadata": {},
   "source": [
    "# Conclusion: Mastering Policy Gradient Methods\n",
    "\n",
    "This comprehensive notebook has provided a thorough exploration of policy gradient methods in deep reinforcement learning. We have covered the theoretical foundations, implemented key algorithms, and analyzed their performance across various scenarios.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "### 1. Theoretical Understanding\n",
    "- **Policy Gradient Theorem**: The fundamental principle enabling direct policy optimization\n",
    "- **Variance-Bias Trade-offs**: Understanding the importance of baseline methods and control variates\n",
    "- **Continuous vs Discrete**: Different parameterizations and optimization challenges\n",
    "\n",
    "### 2. Algorithm Mastery\n",
    "- **REINFORCE**: The foundation of policy gradient methods with Monte Carlo sampling\n",
    "- **Actor-Critic**: Combining policy and value learning for reduced variance\n",
    "- **Advanced Methods**: A2C, A3C, PPO for improved stability and efficiency\n",
    "- **Continuous Control**: Gaussian policies and specialized techniques for continuous action spaces\n",
    "\n",
    "### 3. Practical Implementation\n",
    "- **Variance Reduction**: GAE, baselines, and control variates for stable learning\n",
    "- **Network Architectures**: Shared vs separate networks, normalization techniques\n",
    "- **Hyperparameter Sensitivity**: Understanding the impact of learning rates, entropy coefficients, and discount factors\n",
    "\n",
    "### 4. Performance Analysis\n",
    "- **Sample Efficiency**: Policy gradients typically require many environment interactions\n",
    "- **Stability**: Importance of gradient clipping, learning rate scheduling, and regularization\n",
    "- **Generalization**: Transfer learning and ensemble methods for robust performance\n",
    "\n",
    "## Best Practices Summary\n",
    "\n",
    "### Implementation Guidelines\n",
    "1. **Start Simple**: Begin with REINFORCE or basic Actor-Critic before moving to complex methods\n",
    "2. **Variance Reduction**: Always implement baseline methods and consider advanced techniques like GAE\n",
    "3. **Network Design**: Use appropriate architectures with normalization and regularization\n",
    "4. **Hyperparameter Tuning**: Start with conservative learning rates and gradually adjust\n",
    "5. **Monitoring**: Track both policy and value losses, along with entropy for exploration analysis\n",
    "\n",
    "### Debugging Strategies\n",
    "1. **Gradient Analysis**: Monitor gradient norms to detect vanishing/exploding gradients\n",
    "2. **Policy Visualization**: Analyze learned behaviors to ensure reasonable policies\n",
    "3. **Statistical Validation**: Run multiple seeds and analyze variance in performance\n",
    "4. **Ablation Studies**: Isolate the impact of different components\n",
    "\n",
    "### When to Use Policy Gradients\n",
    "- **High-dimensional action spaces**: Particularly effective for continuous control\n",
    "- **Stochastic policies**: When you need probabilistic action selection\n",
    "- **Direct policy optimization**: When model-free learning is preferred\n",
    "- **Partial observability**: Can handle complex observation spaces effectively\n",
    "\n",
    "## Future Learning Directions\n",
    "\n",
    "### Advanced Topics to Explore\n",
    "1. **Meta-Learning**: MAML and other techniques for rapid adaptation\n",
    "2. **Multi-Agent RL**: Policy gradients in competitive and cooperative settings\n",
    "3. **Hierarchical RL**: Learning policies at multiple temporal scales\n",
    "4. **Safe RL**: Incorporating constraints and safety considerations\n",
    "\n",
    "### Research Frontiers\n",
    "1. **Sample Efficiency**: Developing more efficient exploration strategies\n",
    "2. **Theoretical Guarantees**: Better understanding of convergence properties\n",
    "3. **Robustness**: Learning policies that generalize across environments\n",
    "4. **Human-AI Interaction**: Incorporating human feedback and preferences\n",
    "\n",
    "## Final Recommendations\n",
    "\n",
    "Policy gradient methods represent a powerful and flexible approach to reinforcement learning. While they can be challenging to tune and may require significant computational resources, they offer unique advantages in terms of handling complex action spaces and learning stochastic policies.\n",
    "\n",
    "The key to success with policy gradients lies in:\n",
    "1. **Understanding the theoretical foundations**\n",
    "2. **Implementing robust variance reduction techniques**\n",
    "3. **Careful hyperparameter tuning and monitoring**\n",
    "4. **Systematic evaluation and analysis**\n",
    "\n",
    "By mastering these concepts and techniques, you'll be well-equipped to tackle a wide range of reinforcement learning problems using policy gradient methods.\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook serves as a comprehensive guide to policy gradient methods in deep reinforcement learning. The implementations provided are educational in nature and can be extended for more complex applications. Always remember to validate your implementations thoroughly and consider the specific requirements of your problem domain.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df2c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_comprehensive_demo():\n    print(\"=\"*80)\n    print(\"COMPREHENSIVE POLICY GRADIENT METHODS DEMONSTRATION\")\n    print(\"=\"*80)\n    try:\n        print(\"\\n1. Policy Gradient Fundamentals...\")\n        visualizer = PolicyGradientVisualization()\n        visualizer.demonstrate_policy_gradient_theorem()\n        print(\"\\n2. REINFORCE Algorithm Demonstration...\")\n        reinforce_results = demonstrate_reinforce()\n        print(\"\\n3. Actor-Critic Methods...\")\n        ac_results = demonstrate_actor_critic()\n        print(\"\\n4. Advanced A2C/A3C Methods...\")\n        advanced_results = demonstrate_advanced_methods()\n        print(\"\\n5. Variance Reduction Analysis...\")\n        variance_results = demonstrate_variance_reduction()\n        print(\"\\n6. Continuous Control Methods...\")\n        continuous_results = test_continuous_control()\n        print(\"\\n7. Performance Analysis...\")\n        analyzer = PerformanceAnalyzer()\n        quick_results = {}\n        env_name = 'CartPole-v1'\n        algorithms = [\n            ('REINFORCE', REINFORCEAgent, {'lr': 1e-3, 'gamma': 0.99}),\n            ('Actor-Critic', ActorCriticAgent, {'actor_lr': 1e-4, 'critic_lr': 1e-3, 'gamma': 0.99}),\n        ]\n        for alg_name, alg_class, kwargs in algorithms:\n            print(f\"  Testing {alg_name}...\")\n            try:\n                curves, rewards = analyzer.run_multiple_seeds(\n                    alg_class, env_name, num_runs=2, num_episodes=50, **kwargs\n                )\n                metrics = analyzer.compute_metrics(curves)\n                quick_results[alg_name] = metrics\n                print(f\"    Final Performance: {metrics['final_performance_mean']:.2f}\")\n            except Exception as e:\n                print(f\"    Error: {e}\")\n                quick_results[alg_name] = None\n        print(\"\\n8. Creating Summary Visualization...\")\n        plt.figure(figsize=(20, 12))\n        plt.subplot(2, 3, 1)\n        if 'reinforce_rewards' in locals():\n            smoothed_reinforce = pd.Series(reinforce_results['rewards']).rolling(10).mean()\n            plt.plot(smoothed_reinforce, label='REINFORCE', linewidth=2)\n        if 'ac_rewards' in locals():\n            smoothed_ac = pd.Series(ac_results['rewards']).rolling(10).mean()\n            plt.plot(smoothed_ac, label='Actor-Critic', linewidth=2)\n        plt.title('Learning Curves Comparison')\n        plt.xlabel('Episode')\n        plt.ylabel('Reward')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        plt.subplot(2, 3, 2)\n        algorithms_complexity = {\n            'REINFORCE': 1,\n            'Actor-Critic': 2,\n            'A2C': 3,\n            'A3C': 4,\n            'PPO': 5\n        }\n        alg_names = list(algorithms_complexity.keys())\n        complexity_scores = list(algorithms_complexity.values())\n        bars = plt.bar(alg_names, complexity_scores, alpha=0.7, color='skyblue')\n        plt.title('Algorithm Complexity')\n        plt.ylabel('Relative Complexity')\n        plt.xticks(rotation=45)\n        plt.grid(True, alpha=0.3)\n        plt.subplot(2, 3, 3)\n        if quick_results:\n            perf_names = []\n            perf_values = []\n            for name, metrics in quick_results.items():\n                if metrics is not None:\n                    perf_names.append(name)\n                    perf_values.append(metrics['final_performance_mean'])\n            if perf_names:\n                plt.bar(perf_names, perf_values, alpha=0.7, color='green')\n                plt.title('Final Performance Comparison')\n                plt.ylabel('Average Reward')\n                plt.xticks(rotation=45)\n                plt.grid(True, alpha=0.3)\n        plt.subplot(2, 3, 4)\n        insights = {\n            'Sample\\nEfficiency': 3,\n            'Stability': 4,\n            'Complexity': 3,\n            'Continuous\\nControl': 5,\n            'Scalability': 4\n        }\n        plt.bar(insights.keys(), insights.values(), alpha=0.7, color='orange')\n        plt.title('Policy Gradient Strengths')\n        plt.ylabel('Rating (1-5)')\n        plt.xticks(rotation=45)\n        plt.grid(True, alpha=0.3)\n        plt.subplot(2, 3, 5)\n        domains = ['Robotics', 'Gaming', 'Finance', 'NLP', 'Control']\n        applicability = [5, 5, 4, 4, 5]\n        plt.bar(domains, applicability, alpha=0.7, color='purple')\n        plt.title('Application Domains')\n        plt.ylabel('Applicability (1-5)')\n        plt.xticks(rotation=45)\n        plt.grid(True, alpha=0.3)\n        plt.subplot(2, 3, 6)\n        directions = ['Meta-Learning', 'Multi-Agent', 'Safe RL', 'Hierarchical', 'Offline RL']\n        importance = [4, 5, 5, 4, 4]\n        plt.bar(directions, importance, alpha=0.7, color='red')\n        plt.title('Future Research Directions')\n        plt.ylabel('Importance (1-5)')\n        plt.xticks(rotation=45)\n        plt.grid(True, alpha=0.3)\n        plt.tight_layout()\n        plt.show()\n        print(\"\\n\" + \"=\"*80)\n        print(\"DEMONSTRATION COMPLETE!\")\n        print(\"=\"*80)\n        print(\"\\nKey Achievements:\")\n        print(\"✓ Implemented complete policy gradient framework\")\n        print(\"✓ Demonstrated REINFORCE, Actor-Critic, A2C/A3C, and PPO\")\n        print(\"✓ Analyzed variance reduction techniques\")\n        print(\"✓ Explored continuous action spaces\")\n        print(\"✓ Conducted performance comparisons\")\n        print(\"✓ Showcased practical applications\")\n        print(\"\\nNext Steps:\")\n        print(\"• Experiment with different environments\")\n        print(\"• Tune hyperparameters for your specific problems\")\n        print(\"• Explore advanced techniques like TRPO and SAC\")\n        print(\"• Apply to real-world problems in your domain\")\n        print(\"\\n\" + \"=\"*80)\n    except Exception as e:\n        print(f\"Error in comprehensive demo: {e}\")\n        print(\"Please run individual sections to identify specific issues.\")\nprint(\"Starting Comprehensive Policy Gradient Methods Demonstration...\")\nprint(\"This will run all major examples from the notebook.\")\nprint(\"Please be patient as this may take several minutes to complete.\")\nrun_comprehensive_demo()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}