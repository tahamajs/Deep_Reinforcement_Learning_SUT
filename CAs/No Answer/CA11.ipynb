{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462a8c5b",
   "metadata": {},
   "source": [
    "# CA11: Advanced Model-Based RL and World Models\n",
    "\n",
    "## Deep Reinforcement Learning - Session 11\n",
    "\n",
    "**Advanced Model-Based Reinforcement Learning: World Models, Planning in Latent Space, and Modern Approaches**\n",
    "\n",
    "This notebook explores cutting-edge model-based reinforcement learning techniques, focusing on world models, latent space planning, uncertainty quantification, and state-of-the-art methods like Dreamer, PlaNet, and Model-Based Policy Optimization (MBPO).\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand world models and latent state representations\n",
    "2. Implement variational autoencoders for state modeling\n",
    "3. Master planning in latent space with learned dynamics\n",
    "4. Explore uncertainty quantification in model-based RL\n",
    "5. Implement Dreamer-style world model learning\n",
    "6. Understand Model-Based Policy Optimization (MBPO)\n",
    "7. Apply ensemble methods for robust model learning\n",
    "8. Analyze sample efficiency and computational trade-offs\n",
    "\n",
    "### Notebook Structure:\n",
    "1. **World Models and Latent Representations** - VAE-based state modeling\n",
    "2. **Recurrent State Space Models** - Temporal dynamics in latent space\n",
    "3. **Planning in Latent Space** - Actor-Critic learning within world models\n",
    "4. **Uncertainty Quantification** - Ensemble models and Bayesian approaches\n",
    "5. **Model-Based Policy Optimization** - MBPO algorithm implementation\n",
    "6. **Dreamer Algorithm** - Complete world model implementation\n",
    "7. **Advanced Techniques** - Meta-learning and hierarchical models\n",
    "8. **Comprehensive Evaluation** - Sample efficiency and performance analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869d398b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Essential Imports and Advanced Setup\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.distributions import Normal, MultivariateNormal, kl_divergence\nimport gymnasium as gym\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom collections import defaultdict, deque\nimport random\nimport pickle\nfrom typing import Tuple, List, Dict, Optional, Union, NamedTuple\nimport warnings\nfrom dataclasses import dataclass\nimport math\nfrom tqdm import tqdm\nwarnings.filterwarnings('ignore')\nfrom torch.nn import TransformerEncoder, TransformerEncoderLayer\nfrom torch.distributions.utils import _standard_normal\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\nrandom.seed(SEED)\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nif torch.cuda.is_available():\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\nprint(f\"ðŸš€ Advanced Model-Based RL Environment Setup\")\nprint(f\"Device: {device}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\nplt.style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (15, 10)\nplt.rcParams['font.size'] = 12\nplt.rcParams['axes.titlesize'] = 14\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['xtick.labelsize'] = 10\nplt.rcParams['ytick.labelsize'] = 10\nplt.rcParams['legend.fontsize'] = 10\ncolors = sns.color_palette(\"husl\", 8)\nsns.set_palette(colors)\nprint(\"âœ… Environment setup complete!\")\nprint(\"ðŸŒŸ Ready for advanced model-based reinforcement learning!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f1c73",
   "metadata": {},
   "source": [
    "# Section 1: World Models and Latent Representations\n",
    "\n",
    "## 1.1 Theoretical Foundation\n",
    "\n",
    "**World Models** represent a paradigm shift in model-based reinforcement learning, where instead of learning models in the raw observation space, we learn compressed representations of the environment in a latent space.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Latent State Representation**: $z_t = \\text{Encode}(o_t)$\n",
    "2. **Dynamics in Latent Space**: $z_{t+1} = f(z_t, a_t) + \\epsilon$\n",
    "3. **Reconstruction**: $\\hat{o}_t = \\text{Decode}(z_t)$\n",
    "\n",
    "### Advantages of World Models:\n",
    "\n",
    "- **Dimensionality Reduction**: High-dimensional observations â†’ compact latent states\n",
    "- **Semantic Compression**: Focus on task-relevant features\n",
    "- **Efficient Planning**: Plan in low-dimensional latent space\n",
    "- **Generalization**: Transfer learned representations across tasks\n",
    "\n",
    "## 1.2 Mathematical Framework\n",
    "\n",
    "### Variational Autoencoder (VAE) Foundation\n",
    "\n",
    "The world model uses a VAE to learn latent representations:\n",
    "\n",
    "**Encoder (Recognition Model)**:\n",
    "$$q_\\phi(z_t|o_t) = \\mathcal{N}(z_t; \\mu_\\phi(o_t), \\sigma_\\phi^2(o_t))$$\n",
    "\n",
    "**Decoder (Generative Model)**:\n",
    "$$p_\\theta(o_t|z_t) = \\mathcal{N}(o_t; \\mu_\\theta(z_t), \\sigma_\\theta^2(z_t))$$\n",
    "\n",
    "**VAE Loss Function**:\n",
    "$$\\mathcal{L}_{VAE} = \\mathcal{L}_{recon} + \\beta \\mathcal{L}_{KL}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}_{recon} = ||o_t - \\hat{o}_t||^2$ (Reconstruction loss)\n",
    "- $\\mathcal{L}_{KL} = D_{KL}(q_\\phi(z_t|o_t) || p(z_t))$ (KL regularization)\n",
    "\n",
    "### Dynamics Model in Latent Space\n",
    "\n",
    "**Deterministic Dynamics**:\n",
    "$$z_{t+1} = f_\\psi(z_t, a_t)$$\n",
    "\n",
    "**Stochastic Dynamics**:\n",
    "$$p(z_{t+1}|z_t, a_t) = \\mathcal{N}(z_{t+1}; \\mu_\\psi(z_t, a_t), \\sigma_\\psi^2(z_t, a_t))$$\n",
    "\n",
    "### Reward Model\n",
    "\n",
    "**Reward Prediction**:\n",
    "$$\\hat{r}_t = r_\\xi(z_t, a_t)$$\n",
    "\n",
    "This enables complete planning in the latent space without accessing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbae602",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n    def __init__(self, obs_dim, latent_dim, hidden_dim=256, beta=1.0):\n        super().__init__()\n        self.obs_dim = obs_dim\n        self.latent_dim = latent_dim\n        self.beta = beta\n        self.encoder = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, obs_dim)\n        )\n    def encode(self, x):\n        h = self.encoder(x)\n        mu = self.fc_mu(h)\n        logvar = self.fc_logvar(h)\n        return mu, logvar\n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std)\n            return mu + eps * std\n        else:\n            return mu\n    def decode(self, z):\n        return self.decoder(z)\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        recon = self.decode(z)\n        return recon, mu, logvar, z\n    def loss_function(self, x, recon_x, mu, logvar):\n        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n        total_loss = recon_loss + self.beta * kl_loss\n        return total_loss, recon_loss, kl_loss\nclass LatentDynamicsModel(nn.Module):\n    def __init__(self, latent_dim, action_dim, hidden_dim=256, stochastic=True):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.action_dim = action_dim\n        self.stochastic = stochastic\n        self.dynamics = nn.Sequential(\n            nn.Linear(latent_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        if stochastic:\n            self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n            self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n        else:\n            self.fc_next_state = nn.Linear(hidden_dim, latent_dim)\n    def forward(self, z, a):\n        za = torch.cat([z, a], dim=-1)\n        h = self.dynamics(za)\n        if self.stochastic:\n            mu = self.fc_mu(h)\n            logvar = self.fc_logvar(h)\n            if self.training:\n                std = torch.exp(0.5 * logvar)\n                eps = torch.randn_like(std)\n                z_next = mu + eps * std\n            else:\n                z_next = mu\n            return z_next, mu, logvar\n        else:\n            z_next = self.fc_next_state(h)\n            return z_next\n    def loss_function(self, z_pred, z_target, mu=None, logvar=None):\n        if self.stochastic and mu is not None and logvar is not None:\n            pred_loss = F.mse_loss(z_pred, z_target)\n            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n            return pred_loss + 0.001 * kl_loss\n        else:\n            return F.mse_loss(z_pred, z_target)\nclass RewardModel(nn.Module):\n    def __init__(self, latent_dim, action_dim, hidden_dim=256):\n        super().__init__()\n        self.reward_net = nn.Sequential(\n            nn.Linear(latent_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def forward(self, z, a):\n        za = torch.cat([z, a], dim=-1)\n        return self.reward_net(za).squeeze(-1)\n    def loss_function(self, pred_reward, target_reward):\n        return F.mse_loss(pred_reward, target_reward)\nclass WorldModel(nn.Module):\n    def __init__(self, obs_dim, action_dim, latent_dim=64, hidden_dim=256, \n                 stochastic_dynamics=True, beta=1.0):\n        super().__init__()\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.latent_dim = latent_dim\n        self.vae = VariationalAutoencoder(obs_dim, latent_dim, hidden_dim, beta)\n        self.dynamics = LatentDynamicsModel(latent_dim, action_dim, hidden_dim, stochastic_dynamics)\n        self.reward_model = RewardModel(latent_dim, action_dim, hidden_dim)\n        self.training_stats = {\n            'vae_loss': [],\n            'dynamics_loss': [],\n            'reward_loss': [],\n            'total_loss': []\n        }\n    def encode_observations(self, obs):\n        with torch.no_grad():\n            mu, logvar = self.vae.encode(obs)\n            z = self.vae.reparameterize(mu, logvar)\n            return z\n    def decode_latent_states(self, z):\n        with torch.no_grad():\n            return self.vae.decode(z)\n    def predict_next_state(self, z, a):\n        return self.dynamics(z, a)\n    def predict_reward(self, z, a):\n        return self.reward_model(z, a)\n    def rollout(self, initial_obs, actions, return_observations=False):\n        batch_size = initial_obs.shape[0]\n        horizon = actions.shape[1]\n        z = self.encode_observations(initial_obs)\n        states = [z]\n        rewards = []\n        observations = []\n        for t in range(horizon):\n            r = self.predict_reward(z, actions[:, t])\n            rewards.append(r)\n            if self.dynamics.stochastic:\n                z, _, _ = self.predict_next_state(z, actions[:, t])\n            else:\n                z = self.predict_next_state(z, actions[:, t])\n            states.append(z)\n            if return_observations:\n                obs = self.decode_latent_states(z)\n                observations.append(obs)\n        results = {\n            'states': torch.stack(states, dim=1),\n            'rewards': torch.stack(rewards, dim=1)\n        }\n        if return_observations:\n            results['observations'] = torch.stack(observations, dim=1)\n        return results\nclass WorldModelTrainer:\n    def __init__(self, world_model, device, lr=1e-3):\n        self.world_model = world_model.to(device)\n        self.device = device\n        self.vae_optimizer = optim.Adam(world_model.vae.parameters(), lr=lr)\n        self.dynamics_optimizer = optim.Adam(world_model.dynamics.parameters(), lr=lr)\n        self.reward_optimizer = optim.Adam(world_model.reward_model.parameters(), lr=lr)\n        self.losses = {\n            'vae_total': [],\n            'vae_recon': [],\n            'vae_kl': [],\n            'dynamics': [],\n            'reward': []\n        }\n    def train_step(self, batch):\n        obs, actions, rewards, next_obs = batch\n        obs = obs.to(self.device)\n        actions = actions.to(self.device)\n        rewards = rewards.to(self.device)\n        next_obs = next_obs.to(self.device)\n        self.vae_optimizer.zero_grad()\n        recon_obs, mu_obs, logvar_obs, z_obs = self.world_model.vae(obs)\n        recon_next_obs, mu_next_obs, logvar_next_obs, z_next_obs = self.world_model.vae(next_obs)\n        vae_loss_obs, recon_loss_obs, kl_loss_obs = self.world_model.vae.loss_function(\n            obs, recon_obs, mu_obs, logvar_obs)\n        vae_loss_next_obs, recon_loss_next_obs, kl_loss_next_obs = self.world_model.vae.loss_function(\n            next_obs, recon_next_obs, mu_next_obs, logvar_next_obs)\n        vae_total_loss = vae_loss_obs + vae_loss_next_obs\n        vae_total_loss.backward()\n        self.vae_optimizer.step()\n        self.dynamics_optimizer.zero_grad()\n        z_obs_detached = z_obs.detach()\n        z_next_obs_detached = z_next_obs.detach()\n        if self.world_model.dynamics.stochastic:\n            z_pred, mu_pred, logvar_pred = self.world_model.dynamics(z_obs_detached, actions)\n            dynamics_loss = self.world_model.dynamics.loss_function(\n                z_pred, z_next_obs_detached, mu_pred, logvar_pred)\n        else:\n            z_pred = self.world_model.dynamics(z_obs_detached, actions)\n            dynamics_loss = self.world_model.dynamics.loss_function(z_pred, z_next_obs_detached)\n        dynamics_loss.backward()\n        self.dynamics_optimizer.step()\n        self.reward_optimizer.zero_grad()\n        pred_rewards = self.world_model.reward_model(z_obs_detached, actions)\n        reward_loss = self.world_model.reward_model.loss_function(pred_rewards, rewards)\n        reward_loss.backward()\n        self.reward_optimizer.step()\n        self.losses['vae_total'].append(vae_total_loss.item())\n        self.losses['vae_recon'].append((recon_loss_obs + recon_loss_next_obs).item())\n        self.losses['vae_kl'].append((kl_loss_obs + kl_loss_next_obs).item())\n        self.losses['dynamics'].append(dynamics_loss.item())\n        self.losses['reward'].append(reward_loss.item())\n        return {\n            'vae_loss': vae_total_loss.item(),\n            'dynamics_loss': dynamics_loss.item(),\n            'reward_loss': reward_loss.item()\n        }\nclass ContinuousCartPole:\n    def __init__(self):\n        self.state_dim = 4\n        self.action_dim = 1\n        self.max_steps = 200\n        self.current_step = 0\n        self.gravity = 9.8\n        self.masscart = 1.0\n        self.masspole = 0.1\n        self.total_mass = self.masspole + self.masscart\n        self.length = 0.5\n        self.polemass_length = self.masspole * self.length\n        self.force_mag = 10.0\n        self.tau = 0.02\n        self.x_threshold = 2.4\n        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n        self.state = None\n        self.reset()\n    def reset(self):\n        self.state = np.random.uniform(-0.05, 0.05, size=(4,))\n        self.current_step = 0\n        return self.state.copy()\n    def step(self, action):\n        if isinstance(action, torch.Tensor):\n            action = action.cpu().numpy()\n        if isinstance(action, np.ndarray):\n            action = action.item()\n        action = np.clip(action, -1.0, 1.0)\n        force = action * self.force_mag\n        x, x_dot, theta, theta_dot = self.state\n        costheta = math.cos(theta)\n        sintheta = math.sin(theta)\n        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n            self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)\n        )\n        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n        x = x + self.tau * x_dot\n        x_dot = x_dot + self.tau * xacc\n        theta = theta + self.tau * theta_dot\n        theta_dot = theta_dot + self.tau * thetaacc\n        self.state = np.array([x, x_dot, theta, theta_dot])\n        self.current_step += 1\n        done = (\n            x < -self.x_threshold or x > self.x_threshold or\n            theta < -self.theta_threshold_radians or theta > self.theta_threshold_radians or\n            self.current_step >= self.max_steps\n        )\n        reward = 1.0 if not done else 0.0\n        return self.state.copy(), reward, done\n    def sample_action(self):\n        return np.random.uniform(-1.0, 1.0)\nprint(\"ðŸŒ World Models and Latent Representations Demonstration\")\nprint(\"=\" * 60)\nprint(\"\\\\n1. Setting up environment and collecting data...\")\nenv = ContinuousCartPole()\ndef collect_random_data(env, n_episodes=100):\n    \\\"\\\"\\\"Collect random interaction data\\\"\\\"\\\"\n    data = {\n        'observations': [],\n        'actions': [],\n        'rewards': [],\n        'next_observations': []\n    }\n    for episode in range(n_episodes):\n        obs = env.reset()\n        for step in range(200):\n            action = env.sample_action()\n            next_obs, reward, done = env.step(action)\n            data['observations'].append(obs)\n            data['actions'].append([action])\n            data['rewards'].append(reward)\n            data['next_observations'].append(next_obs)\n            obs = next_obs\n            if done:\n                break\n    for key in data:\n        data[key] = torch.FloatTensor(data[key])\n    return data\ntraining_data = collect_random_data(env, n_episodes=50)\nprint(f\"Collected {len(training_data['observations'])} transitions\")\nprint(\"\\\\n2. Creating and training world model...\")\nworld_model = WorldModel(\n    obs_dim=env.state_dim,\n    action_dim=env.action_dim,\n    latent_dim=8,\n    hidden_dim=128,\n    stochastic_dynamics=True,\n    beta=1.0\n)\ntrainer = WorldModelTrainer(world_model, device, lr=1e-3)\nn_epochs = 100\nbatch_size = 64\nn_batches = len(training_data['observations']) // batch_size\nprint(f\"Training for {n_epochs} epochs with batch size {batch_size}\")\nfor epoch in range(n_epochs):\n    epoch_losses = []\n    indices = torch.randperm(len(training_data['observations']))\n    for batch_idx in range(n_batches):\n        start_idx = batch_idx * batch_size\n        end_idx = start_idx + batch_size\n        batch_indices = indices[start_idx:end_idx]\n        batch = (\n            training_data['observations'][batch_indices],\n            training_data['actions'][batch_indices],\n            training_data['rewards'][batch_indices],\n            training_data['next_observations'][batch_indices]\n        )\n        losses = trainer.train_step(batch)\n        epoch_losses.append(losses)\n    if (epoch + 1) % 20 == 0:\n        avg_losses = {k: np.mean([l[k] for l in epoch_losses]) for k in epoch_losses[0].keys()}\n        print(f\"Epoch {epoch+1}: VAE Loss = {avg_losses['vae_loss']:.4f}, \"\n              f\"Dynamics Loss = {avg_losses['dynamics_loss']:.4f}, \"\n              f\"Reward Loss = {avg_losses['reward_loss']:.4f}\")\nprint(\"\\\\n3. Evaluating world model performance...\")\nworld_model.eval()\ntest_data = collect_random_data(env, n_episodes=10)\ntest_batch_size = min(100, len(test_data['observations']))\ntest_indices = torch.randperm(len(test_data['observations']))[:test_batch_size]\ntest_obs = test_data['observations'][test_indices].to(device)\ntest_actions = test_data['actions'][test_indices].to(device)\ntest_rewards = test_data['rewards'][test_indices].to(device)\ntest_next_obs = test_data['next_observations'][test_indices].to(device)\nwith torch.no_grad():\n    recon_obs, _, _, z_obs = world_model.vae(test_obs)\n    recon_error = F.mse_loss(recon_obs, test_obs).item()\n    if world_model.dynamics.stochastic:\n        z_pred, _, _ = world_model.dynamics(z_obs, test_actions)\n    else:\n        z_pred = world_model.dynamics(z_obs, test_actions)\n    _, _, z_next_actual = world_model.vae(test_next_obs)\n    dynamics_error = F.mse_loss(z_pred, z_next_actual).item()\n    pred_rewards = world_model.reward_model(z_obs, test_actions)\n    reward_error = F.mse_loss(pred_rewards, test_rewards).item()\nprint(f\"\\\\nðŸ“Š World Model Evaluation:\")\nprint(f\"  Reconstruction Error (MSE): {recon_error:.6f}\")\nprint(f\"  Dynamics Prediction Error (MSE): {dynamics_error:.6f}\")\nprint(f\"  Reward Prediction Error (MSE): {reward_error:.6f}\")\nplt.figure(figsize=(15, 10))\nplt.subplot(2, 3, 1)\nplt.plot(trainer.losses['vae_total'], label='VAE Total', linewidth=2)\nplt.plot(trainer.losses['vae_recon'], label='VAE Reconstruction', linewidth=2)\nplt.plot(trainer.losses['vae_kl'], label='VAE KL', linewidth=2)\nplt.title('VAE Training Losses')\nplt.xlabel('Training Step')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 2)\nplt.plot(trainer.losses['dynamics'], label='Dynamics Loss', color='red', linewidth=2)\nplt.title('Dynamics Model Training')\nplt.xlabel('Training Step')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 3)\nplt.plot(trainer.losses['reward'], label='Reward Loss', color='green', linewidth=2)\nplt.title('Reward Model Training')\nplt.xlabel('Training Step')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 4)\nsample_idx = 0\noriginal_obs = test_obs[sample_idx].cpu().numpy()\nreconstructed_obs = recon_obs[sample_idx].cpu().numpy()\nx_pos = np.arange(len(original_obs))\nwidth = 0.35\nplt.bar(x_pos - width/2, original_obs, width, label='Original', alpha=0.7)\nplt.bar(x_pos + width/2, reconstructed_obs, width, label='Reconstructed', alpha=0.7)\nplt.title('VAE Reconstruction Example')\nplt.xlabel('State Dimension')\nplt.ylabel('Value')\nplt.legend()\nplt.xticks(x_pos, ['x', 'x_dot', 'theta', 'theta_dot'])\nplt.subplot(2, 3, 5)\nlatent_states = z_obs.cpu().numpy()\nplt.scatter(latent_states[:, 0], latent_states[:, 1], alpha=0.6, s=30)\nplt.title('Latent Space Representation')\nplt.xlabel('Latent Dimension 1')\nplt.ylabel('Latent Dimension 2')\nplt.grid(True, alpha=0.3)\nplt.subplot(2, 3, 6)\nerrors = [recon_error, dynamics_error, reward_error]\nlabels = ['Reconstruction', 'Dynamics', 'Reward']\ncolors = ['blue', 'red', 'green']\nbars = plt.bar(labels, errors, color=colors, alpha=0.7)\nplt.title('Prediction Errors')\nplt.ylabel('Mean Squared Error')\nplt.yscale('log')\nfor bar, error in zip(bars, errors):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n             f'{error:.2e}', ha='center', va='bottom')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint(\"\\\\nâœ… World Model demonstration complete!\")\nprint(\"ðŸš€ Next: Recurrent State Space Models for temporal dynamics\")\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae723a14",
   "metadata": {},
   "source": [
    "# Section 2: Recurrent State Space Models (RSSM)\n",
    "\n",
    "## 2.1 Theoretical Foundation\n",
    "\n",
    "Recurrent State Space Models extend world models by incorporating temporal dependencies and memory into the latent dynamics. This is crucial for partially observable environments where the current observation doesn't contain all necessary information.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "**Deterministic Recurrent State**:\n",
    "$$h_t = f(h_{t-1}, z_{t-1}, a_{t-1})$$\n",
    "\n",
    "**Stochastic State**:\n",
    "$$z_t \\sim p(z_t | h_t)$$\n",
    "\n",
    "**Combined RSSM State**:\n",
    "$$s_t = [h_t, z_t]$$ where $h_t$ is deterministic and $z_t$ is stochastic\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Recurrent Model**: Models temporal dependencies\n",
    "$$h_t = \\text{GRU}(h_{t-1}, [z_{t-1}, a_{t-1}])$$\n",
    "\n",
    "2. **Representation Model**: Encodes observations\n",
    "$$z_t \\sim q(z_t | h_t, o_t)$$\n",
    "\n",
    "3. **Transition Model**: Predicts future states\n",
    "$$z_t \\sim p(z_t | h_t)$$\n",
    "\n",
    "4. **Observation Model**: Reconstructs observations\n",
    "$$o_t \\sim p(o_t | h_t, z_t)$$\n",
    "\n",
    "5. **Reward Model**: Predicts rewards\n",
    "$$r_t \\sim p(r_t | h_t, z_t)$$\n",
    "\n",
    "### Advantages of RSSM:\n",
    "\n",
    "- **Memory**: Maintains information across time steps\n",
    "- **Partial Observability**: Handles environments where current observation is insufficient\n",
    "- **Temporal Consistency**: Models smooth transitions in latent space\n",
    "- **Hierarchical Representation**: Separates deterministic and stochastic components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdda4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentStateSpaceModel(nn.Module):\n    def __init__(self, obs_dim, action_dim, stoch_dim=30, deter_dim=200, hidden_dim=400):\n        super().__init__()\n        self.obs_dim = obs_dim\n        self.action_dim = action_dim\n        self.stoch_dim = stoch_dim\n        self.deter_dim = deter_dim\n        self.hidden_dim = hidden_dim\n        self.obs_encoder = nn.Sequential(\n            nn.Linear(obs_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, stoch_dim * 2)\n        )\n        self.rnn = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n        self.transition_model = nn.Sequential(\n            nn.Linear(deter_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, stoch_dim * 2)\n        )\n        self.representation_model = nn.Sequential(\n            nn.Linear(deter_dim + stoch_dim * 2, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, stoch_dim * 2)\n        )\n        self.obs_decoder = nn.Sequential(\n            nn.Linear(deter_dim + stoch_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, obs_dim)\n        )\n        self.reward_model = nn.Sequential(\n            nn.Linear(deter_dim + stoch_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n        self.value_model = nn.Sequential(\n            nn.Linear(deter_dim + stoch_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def initial_state(self, batch_size):\n        h = torch.zeros(batch_size, self.deter_dim, device=next(self.parameters()).device)\n        z = torch.zeros(batch_size, self.stoch_dim, device=next(self.parameters()).device)\n        return h, z\n    def encode_obs(self, obs):\n        encoded = self.obs_encoder(obs)\n        mean, std = torch.chunk(encoded, 2, dim=-1)\n        std = F.softplus(std) + 1e-4\n        return mean, std\n    def transition_prior(self, h):\n        encoded = self.transition_model(h)\n        mean, std = torch.chunk(encoded, 2, dim=-1)\n        std = F.softplus(std) + 1e-4\n        return mean, std\n    def representation_posterior(self, h, obs_encoded):\n        encoded = self.representation_model(torch.cat([h, obs_encoded], dim=-1))\n        mean, std = torch.chunk(encoded, 2, dim=-1)\n        std = F.softplus(std) + 1e-4\n        return mean, std\n    def reparameterize(self, mean, std):\n        if self.training:\n            eps = torch.randn_like(std)\n            return mean + eps * std\n        else:\n            return mean\n    def recurrent_step(self, prev_h, prev_z, action):\n        rnn_input = torch.cat([prev_z, action], dim=-1)\n        h = self.rnn(rnn_input, prev_h)\n        return h\n    def observe(self, obs, prev_h, prev_z, action):\n        h = self.recurrent_step(prev_h, prev_z, action)\n        obs_encoded = self.obs_encoder(obs)\n        prior_mean, prior_std = self.transition_prior(h)\n        post_mean, post_std = self.representation_posterior(h, obs_encoded)\n        z = self.reparameterize(post_mean, post_std)\n        return h, z, (prior_mean, prior_std), (post_mean, post_std)\n    def imagine(self, prev_h, prev_z, action):\n        h = self.recurrent_step(prev_h, prev_z, action)\n        prior_mean, prior_std = self.transition_prior(h)\n        z = self.reparameterize(prior_mean, prior_std)\n        return h, z, (prior_mean, prior_std)\n    def decode_obs(self, h, z):\n        state = torch.cat([h, z], dim=-1)\n        return self.obs_decoder(state)\n    def predict_reward(self, h, z):\n        state = torch.cat([h, z], dim=-1)\n        return self.reward_model(state).squeeze(-1)\n    def predict_value(self, h, z):\n        state = torch.cat([h, z], dim=-1)\n        return self.value_model(state).squeeze(-1)\nclass RSSMTrainer:\n    def __init__(self, rssm_model, device, lr=1e-4, kl_weight=1.0, free_nats=3.0):\n        self.rssm_model = rssm_model.to(device)\n        self.device = device\n        self.kl_weight = kl_weight\n        self.free_nats = free_nats\n        self.optimizer = optim.Adam(rssm_model.parameters(), lr=lr, eps=1e-4)\n        self.losses = {\n            'total': [],\n            'reconstruction': [],\n            'kl_divergence': [],\n            'reward': []\n        }\n    def kl_divergence(self, post_mean, post_std, prior_mean, prior_std):\n        post_dist = Normal(post_mean, post_std)\n        prior_dist = Normal(prior_mean, prior_std)\n        kl = kl_divergence(post_dist, prior_dist)\n        kl = torch.maximum(kl, torch.tensor(self.free_nats, device=self.device))\n        return kl.sum(-1)\n    def train_step(self, batch):\n        observations, actions, rewards = batch\n        batch_size, seq_len = observations.shape[:2]\n        observations = observations.to(self.device)\n        actions = actions.to(self.device) \n        rewards = rewards.to(self.device)\n        h, z = self.rssm_model.initial_state(batch_size)\n        reconstruction_losses = []\n        kl_losses = []\n        reward_losses = []\n        for t in range(seq_len):\n            h, z, (prior_mean, prior_std), (post_mean, post_std) = self.rssm_model.observe(\n                observations[:, t], h, z, actions[:, t])\n            pred_obs = self.rssm_model.decode_obs(h, z)\n            recon_loss = F.mse_loss(pred_obs, observations[:, t], reduction='none').sum(-1)\n            reconstruction_losses.append(recon_loss)\n            kl_loss = self.kl_divergence(post_mean, post_std, prior_mean, prior_std)\n            kl_losses.append(kl_loss)\n            pred_reward = self.rssm_model.predict_reward(h, z)\n            reward_loss = F.mse_loss(pred_reward, rewards[:, t], reduction='none')\n            reward_losses.append(reward_loss)\n        reconstruction_loss = torch.stack(reconstruction_losses).mean()\n        kl_loss = torch.stack(kl_losses).mean()\n        reward_loss = torch.stack(reward_losses).mean()\n        total_loss = reconstruction_loss + self.kl_weight * kl_loss + reward_loss\n        self.optimizer.zero_grad()\n        total_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.rssm_model.parameters(), 100.0)\n        self.optimizer.step()\n        self.losses['total'].append(total_loss.item())\n        self.losses['reconstruction'].append(reconstruction_loss.item())\n        self.losses['kl_divergence'].append(kl_loss.item())\n        self.losses['reward'].append(reward_loss.item())\n        return {\n            'total_loss': total_loss.item(),\n            'recon_loss': reconstruction_loss.item(),\n            'kl_loss': kl_loss.item(),\n            'reward_loss': reward_loss.item()\n        }\nclass SequenceEnvironment:\n    def __init__(self, obs_dim=4, memory_length=5):\n        self.obs_dim = obs_dim\n        self.memory_length = memory_length\n        self.action_dim = 2\n        self.state = None\n        self.memory = None\n        self.step_count = 0\n        self.max_steps = 50\n        self.reset()\n    def reset(self):\n        self.state = np.zeros(self.obs_dim)\n        self.memory = deque(maxlen=self.memory_length)\n        self.step_count = 0\n        for _ in range(self.memory_length):\n            self.memory.append(np.random.rand())\n        return self._get_observation()\n    def _get_observation(self):\n        recent_memory = list(self.memory)[-2:]\n        obs = np.concatenate([\n            self.state,\n            recent_memory + [0.0] * (2 - len(recent_memory))\n        ])\n        return obs[:self.obs_dim]\n    def step(self, action):\n        if isinstance(action, torch.Tensor):\n            action = action.cpu().numpy()\n        if isinstance(action, np.ndarray) and action.ndim > 0:\n            action = action.item()\n        action = int(action > 0.5) if isinstance(action, float) else int(action)\n        if action == 0:\n            new_memory_val = max(0.0, list(self.memory)[-1] - 0.1)\n        else:\n            new_memory_val = min(1.0, list(self.memory)[-1] + 0.1)\n        self.memory.append(new_memory_val)\n        self.state[0] = new_memory_val\n        self.state[1] = np.mean(list(self.memory))\n        self.state[2] = action\n        self.state[3] = self.step_count / self.max_steps\n        memory_sequence = list(self.memory)\n        if len(memory_sequence) >= 3:\n            recent_avg = np.mean(memory_sequence[-3:])\n            reward = 1.0 - abs(recent_avg - 0.5) * 2\n        else:\n            reward = 0.0\n        self.step_count += 1\n        done = self.step_count >= self.max_steps\n        return self._get_observation(), reward, done\ndef collect_sequence_data(env, n_episodes=100, seq_length=20):\n    sequences = []\n    for episode in range(n_episodes):\n        obs_sequence = []\n        action_sequence = []\n        reward_sequence = []\n        obs = env.reset()\n        for t in range(seq_length):\n            action = np.random.randint(0, 2)\n            next_obs, reward, done = env.step(action)\n            obs_sequence.append(obs)\n            action_sequence.append([action])\n            reward_sequence.append(reward)\n            obs = next_obs\n            if done:\n                break\n        if len(obs_sequence) >= seq_length:\n            sequences.append({\n                'observations': obs_sequence[:seq_length],\n                'actions': action_sequence[:seq_length],\n                'rewards': reward_sequence[:seq_length]\n            })\n    return sequences\nprint(\"ðŸ”„ Recurrent State Space Models (RSSM) Demonstration\")\nprint(\"=\" * 60)\nprint(\"\\\\n1. Setting up sequence environment...\")\nseq_env = SequenceEnvironment(obs_dim=4, memory_length=5)\nprint(\"\\\\n2. Collecting sequential training data...\")\nsequence_data = collect_sequence_data(seq_env, n_episodes=200, seq_length=15)\nprint(f\"Collected {len(sequence_data)} sequences\")\ndef prepare_rssm_batch(sequences, batch_size=32):\n    \\\"\\\"\\\"Prepare batch for RSSM training\\\"\\\"\\\"\n    batch_sequences = random.sample(sequences, min(batch_size, len(sequences)))\n    observations = []\n    actions = []\n    rewards = []\n    for seq in batch_sequences:\n        observations.append(seq['observations'])\n        actions.append(seq['actions'])\n        rewards.append(seq['rewards'])\n    observations = torch.FloatTensor(observations)\n    actions = torch.FloatTensor(actions)\n    rewards = torch.FloatTensor(rewards)\n    return observations, actions, rewards\nprint(\"\\\\n3. Creating and training RSSM model...\")\nrssm_model = RecurrentStateSpaceModel(\n    obs_dim=seq_env.obs_dim,\n    action_dim=seq_env.action_dim,\n    stoch_dim=16,\n    deter_dim=64,\n    hidden_dim=128\n)\ntrainer = RSSMTrainer(rssm_model, device, lr=1e-3, kl_weight=0.1)\nn_training_steps = 500\nbatch_size = 16\nprint(f\"Training RSSM for {n_training_steps} steps...\")\nfor step in range(n_training_steps):\n    batch = prepare_rssm_batch(sequence_data, batch_size)\n    losses = trainer.train_step(batch)\n    if (step + 1) % 100 == 0:\n        print(f\"Step {step+1}: Total Loss = {losses['total_loss']:.4f}, \"\n              f\"Recon = {losses['recon_loss']:.4f}, \"\n              f\"KL = {losses['kl_loss']:.4f}, \"\n              f\"Reward = {losses['reward_loss']:.4f}\")\nprint(\"\\\\n4. Testing RSSM imagination and prediction...\")\nrssm_model.eval()\ntest_batch = prepare_rssm_batch(sequence_data, batch_size=1)\ntest_obs, test_actions, test_rewards = test_batch\nwith torch.no_grad():\n    batch_size = test_obs.shape[0]\n    h, z = rssm_model.initial_state(batch_size)\n    observe_steps = 5\n    imagine_steps = 10\n    observations_pred = []\n    rewards_pred = []\n    for t in range(observe_steps):\n        h, z, _, _ = rssm_model.observe(\n            test_obs[:, t].to(device), h, z, test_actions[:, t].to(device))\n        pred_obs = rssm_model.decode_obs(h, z)\n        pred_reward = rssm_model.predict_reward(h, z)\n        observations_pred.append(pred_obs.cpu())\n        rewards_pred.append(pred_reward.cpu())\n    for t in range(imagine_steps):\n        random_action = torch.randint(0, 2, (batch_size, 1), dtype=torch.float).to(device)\n        h, z, _ = rssm_model.imagine(h, z, random_action)\n        pred_obs = rssm_model.decode_obs(h, z)\n        pred_reward = rssm_model.predict_reward(h, z)\n        observations_pred.append(pred_obs.cpu())\n        rewards_pred.append(pred_reward.cpu())\nplt.figure(figsize=(15, 12))\nplt.subplot(3, 3, 1)\nplt.plot(trainer.losses['total'], label='Total Loss', linewidth=2)\nplt.title('RSSM Training - Total Loss')\nplt.xlabel('Training Step')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 3, 2)\nplt.plot(trainer.losses['reconstruction'], label='Reconstruction', color='blue', linewidth=2)\nplt.plot(trainer.losses['kl_divergence'], label='KL Divergence', color='red', linewidth=2)\nplt.plot(trainer.losses['reward'], label='Reward', color='green', linewidth=2)\nplt.title('RSSM Component Losses')\nplt.xlabel('Training Step')\nplt.ylabel('Loss')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 3, 3)\nactual_obs = test_obs[0, :observe_steps, 0].numpy()\npred_obs = torch.stack(observations_pred[:observe_steps])[:, 0, 0].numpy()\nplt.plot(actual_obs, 'o-', label='Actual', linewidth=2, markersize=6)\nplt.plot(pred_obs, 's-', label='Predicted', linewidth=2, markersize=6)\nplt.title('Observation Reconstruction')\nplt.xlabel('Time Step')\nplt.ylabel('Observation Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 3, 4)\nimagined_obs = torch.stack(observations_pred[observe_steps:])[:, 0, 0].numpy()\ntime_steps = np.arange(observe_steps, observe_steps + len(imagined_obs))\nplt.plot(range(observe_steps), actual_obs, 'o-', label='Observed', linewidth=2)\nplt.plot(time_steps, imagined_obs, 's-', label='Imagined', linewidth=2)\nplt.axvline(x=observe_steps-0.5, color='red', linestyle='--', alpha=0.7, label='Imagination Start')\nplt.title('RSSM Imagination')\nplt.xlabel('Time Step')\nplt.ylabel('Observation Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 3, 5)\nactual_rewards = test_rewards[0, :observe_steps].numpy()\npred_rewards = torch.stack(rewards_pred[:observe_steps])[:, 0].numpy()\nplt.plot(actual_rewards, 'o-', label='Actual Rewards', linewidth=2, markersize=6)\nplt.plot(pred_rewards, 's-', label='Predicted Rewards', linewidth=2, markersize=6)\nplt.title('Reward Prediction')\nplt.xlabel('Time Step')\nplt.ylabel('Reward')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 3, 6)\nh_states = []\nwith torch.no_grad():\n    batch_size = test_obs.shape[0]\n    h, z = rssm_model.initial_state(batch_size)\n    for t in range(observe_steps):\n        h, z, _, _ = rssm_model.observe(\n            test_obs[:, t].to(device), h, z, test_actions[:, t].to(device))\n        h_states.append(h.cpu().numpy())\nh_states = np.array(h_states)[:, 0, :2]\nplt.plot(h_states[:, 0], h_states[:, 1], 'o-', linewidth=2, markersize=6)\nplt.title('Deterministic State Trajectory')\nplt.xlabel('Hidden Dimension 1')\nplt.ylabel('Hidden Dimension 2')\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 3, 7)\nmemory_effect = []\nfor t in range(len(observations_pred)):\n    obs_pred = observations_pred[t][0]\n    memory_component = obs_pred[1].item()\n    memory_effect.append(memory_component)\nplt.plot(memory_effect[:observe_steps], 'o-', label='Observed', linewidth=2)\nplt.plot(memory_effect[observe_steps:], 's-', label='Imagined', linewidth=2)\nplt.axvline(x=observe_steps-0.5, color='red', linestyle='--', alpha=0.7)\nplt.title('Memory Component Evolution')\nplt.xlabel('Time Step')\nplt.ylabel('Memory Value')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 3, 8)\nmodel_sizes = ['Obs Encoder', 'RNN', 'Transition', 'Representation', 'Obs Decoder', 'Reward']\nparam_counts = []\nfor name, module in rssm_model.named_children():\n    params = sum(p.numel() for p in module.parameters())\n    param_counts.append(params)\nparam_counts = param_counts[:len(model_sizes)]\nbars = plt.bar(model_sizes, param_counts, alpha=0.7)\nplt.title('RSSM Model Complexity')\nplt.xlabel('Component')\nplt.ylabel('Parameters')\nplt.xticks(rotation=45)\nfor bar, count in zip(bars, param_counts):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n             f'{count}', ha='center', va='bottom')\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 3, 9)\nmetrics = ['Recon Error', 'KL Divergence', 'Reward Error']\nfinal_losses = [\n    trainer.losses['reconstruction'][-1],\n    trainer.losses['kl_divergence'][-1], \n    trainer.losses['reward'][-1]\n]\nbars = plt.bar(metrics, final_losses, alpha=0.7, color=['blue', 'red', 'green'])\nplt.title('Final Performance Metrics')\nplt.ylabel('Loss Value')\nplt.yscale('log')\nfor bar, loss in zip(bars, final_losses):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n             f'{loss:.3f}', ha='center', va='bottom')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint(f\"\\\\nðŸ“Š RSSM Analysis:\")\nprint(f\"  Final Reconstruction Loss: {trainer.losses['reconstruction'][-1]:.4f}\")\nprint(f\"  Final KL Divergence: {trainer.losses['kl_divergence'][-1]:.4f}\")\nprint(f\"  Final Reward Loss: {trainer.losses['reward'][-1]:.4f}\")\nprint(f\"  Model Parameters: {sum(p.numel() for p in rssm_model.parameters()):,}\")\nprint(f\"\\\\nâœ… RSSM demonstration complete!\")\nprint(f\"ðŸŽ¯ Key Insights:\")\nprint(f\"  â€¢ RSSM combines deterministic and stochastic states for memory\")\nprint(f\"  â€¢ Temporal dependencies enable long-horizon planning\")\nprint(f\"  â€¢ Imagination capability allows model-based planning\")\nprint(f\"  â€¢ KL regularization ensures meaningful latent representations\")\nprint(f\"\\\\nðŸš€ Next: Planning in Latent Space with Actor-Critic Methods\")\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd811c",
   "metadata": {},
   "source": [
    "# Section 3: Planning in Latent Space with Actor-Critic Methods\n",
    "\n",
    "## 3.1 Theoretical Foundation\n",
    "\n",
    "Planning in latent space combines world models with reinforcement learning by training policies and value functions entirely within the learned latent representation. This approach, popularized by methods like Dreamer, enables sample-efficient learning through imagination.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Latent Imagination**: Generate trajectories using learned world model\n",
    "2. **Actor-Critic in Latent Space**: Train policy and value function on imagined trajectories  \n",
    "3. **Gradient-Based Planning**: Use backpropagation through the world model for planning\n",
    "\n",
    "### Mathematical Framework:\n",
    "\n",
    "**Latent State Trajectory**:\n",
    "$$\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, ..., s_T)$$\n",
    "\n",
    "where $s_t = [h_t, z_t]$ in RSSM\n",
    "\n",
    "**Policy Learning in Latent Space**:\n",
    "$$\\pi_\\phi(a_t | s_t) \\text{ trained on imagined trajectories}$$\n",
    "\n",
    "**Value Function Learning**:\n",
    "$$V_\\psi(s_t) \\text{ and } Q_\\psi(s_t, a_t) \\text{ trained on imagined returns}$$\n",
    "\n",
    "**Actor-Critic Loss**:\n",
    "$$\\mathcal{L}_\\text{actor} = -\\mathbb{E}[\\lambda_t A_t \\log \\pi_\\phi(a_t | s_t)]$$\n",
    "$$\\mathcal{L}_\\text{critic} = \\frac{1}{2}\\mathbb{E}[(V_\\psi(s_t) - V_t^\\text{target})^2]$$\n",
    "\n",
    "where $\\lambda_t$ is the importance sampling ratio and $A_t$ is the advantage.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- **Sample Efficiency**: Learn from imagined rather than real experience\n",
    "- **Long Horizon Planning**: Plan for many steps without environment interaction\n",
    "- **Gradient-Based Optimization**: Leverage automatic differentiation\n",
    "- **Continuous Action Spaces**: Natural handling of continuous control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a18c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LatentActor(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256, action_range=1.0):\n        super().__init__()\n        self.action_range = action_range\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, action_dim * 2)\n        )\n        self.network[-1].weight.data.uniform_(-1e-3, 1e-3)\n        self.network[-1].bias.data.uniform_(-1e-3, 1e-3)\n    def forward(self, state):\n        output = self.network(state)\n        mean, log_std = torch.chunk(output, 2, dim=-1)\n        log_std = torch.clamp(log_std, -20, 2)\n        std = torch.exp(log_std)\n        return mean, std\n    def sample(self, state):\n        mean, std = self.forward(state)\n        normal = Normal(mean, std)\n        x = normal.rsample()\n        action = torch.tanh(x) * self.action_range\n        log_prob = normal.log_prob(x).sum(dim=-1)\n        log_prob -= (2 * (np.log(2) - x - F.softplus(-2 * x))).sum(dim=-1)\n        return action, log_prob\n    def get_action(self, state, deterministic=False):\n        mean, std = self.forward(state)\n        if deterministic:\n            action = torch.tanh(mean) * self.action_range\n            return action\n        else:\n            normal = Normal(mean, std)\n            x = normal.sample()\n            action = torch.tanh(x) * self.action_range\n            return action\nclass LatentCritic(nn.Module):\n    def __init__(self, state_dim, hidden_dim=256):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(state_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def forward(self, state):\n        return self.network(state).squeeze(-1)\nclass DreamerAgent:\n    def __init__(self, world_model, state_dim, action_dim, device,\n                 actor_lr=8e-5, critic_lr=8e-5, gamma=0.99, lambda_=0.95, \n                 imagination_horizon=15):\n        self.world_model = world_model\n        self.device = device\n        self.gamma = gamma\n        self.lambda_ = lambda_\n        self.imagination_horizon = imagination_horizon\n        self.actor = LatentActor(state_dim, action_dim).to(device)\n        self.critic = LatentCritic(state_dim).to(device)\n        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n        self.stats = {\n            'actor_loss': [],\n            'critic_loss': [],\n            'imagination_reward': [],\n            'policy_entropy': []\n        }\n    def imagine_trajectories(self, initial_states, batch_size=50):\n        horizon = self.imagination_horizon\n        states = [initial_states]\n        actions = []\n        rewards = []\n        log_probs = []\n        values = []\n        current_state = initial_states\n        for t in range(horizon):\n            action, log_prob = self.actor.sample(current_state)\n            value = self.critic(current_state)\n            actions.append(action)\n            log_probs.append(log_prob)\n            values.append(value)\n            if hasattr(self.world_model, 'dynamics'):\n                if self.world_model.dynamics.stochastic:\n                    next_state, _, _ = self.world_model.dynamics(current_state, action)\n                else:\n                    next_state = self.world_model.dynamics(current_state, action)\n                reward = self.world_model.reward_model(current_state, action)\n            else:\n                batch_size = current_state.shape[0]\n                h_dim = self.world_model.deter_dim\n                z_dim = self.world_model.stoch_dim\n                h = current_state[:, :h_dim]\n                z = current_state[:, h_dim:h_dim+z_dim]\n                h, z, _ = self.world_model.imagine(h, z, action)\n                next_state = torch.cat([h, z], dim=-1)\n                reward = self.world_model.predict_reward(h, z)\n            states.append(next_state)\n            rewards.append(reward)\n            current_state = next_state\n        states = torch.stack(states[:-1])\n        actions = torch.stack(actions)\n        rewards = torch.stack(rewards)\n        log_probs = torch.stack(log_probs)\n        values = torch.stack(values)\n        final_value = self.critic(states[-1])\n        return {\n            'states': states,\n            'actions': actions,\n            'rewards': rewards,\n            'log_probs': log_probs,\n            'values': values,\n            'final_value': final_value\n        }\n    def compute_returns_and_advantages(self, trajectory):\n        rewards = trajectory['rewards']\n        values = trajectory['values']\n        final_value = trajectory['final_value']\n        returns = torch.zeros_like(rewards)\n        advantages = torch.zeros_like(rewards)\n        last_return = final_value\n        last_advantage = 0\n        for t in reversed(range(len(rewards))):\n            returns[t] = rewards[t] + self.gamma * last_return\n            delta = rewards[t] + self.gamma * (final_value if t == len(rewards)-1 else values[t+1]) - values[t]\n            advantages[t] = delta + self.gamma * self.lambda_ * last_advantage\n            last_return = returns[t]\n            last_advantage = advantages[t]\n        return returns, advantages\n    def update_actor_critic(self, trajectory):\n        states = trajectory['states']\n        actions = trajectory['actions']\n        log_probs = trajectory['log_probs']\n        states = states.view(-1, states.shape[-1])\n        actions = actions.view(-1, actions.shape[-1])\n        log_probs = log_probs.view(-1)\n        returns, advantages = self.compute_returns_and_advantages(trajectory)\n        returns = returns.view(-1)\n        advantages = advantages.view(-1)\n        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n        self.critic_optimizer.zero_grad()\n        values_pred = self.critic(states)\n        critic_loss = F.mse_loss(values_pred, returns)\n        critic_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 10.0)\n        self.critic_optimizer.step()\n        self.actor_optimizer.zero_grad()\n        action_mean, action_std = self.actor(states)\n        dist = Normal(action_mean, action_std)\n        raw_actions = torch.atanh(torch.clamp(actions / self.actor.action_range, -0.999, 0.999))\n        new_log_probs = dist.log_prob(raw_actions).sum(dim=-1)\n        new_log_probs -= (2 * (np.log(2) - raw_actions - F.softplus(-2 * raw_actions))).sum(dim=-1)\n        actor_loss = -(new_log_probs * advantages.detach()).mean()\n        entropy = dist.entropy().sum(dim=-1).mean()\n        actor_loss -= 0.001 * entropy\n        actor_loss.backward()\n        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 10.0)\n        self.actor_optimizer.step()\n        self.stats['actor_loss'].append(actor_loss.item())\n        self.stats['critic_loss'].append(critic_loss.item())\n        self.stats['imagination_reward'].append(trajectory['rewards'].mean().item())\n        self.stats['policy_entropy'].append(entropy.item())\n        return {\n            'actor_loss': actor_loss.item(),\n            'critic_loss': critic_loss.item(),\n            'entropy': entropy.item(),\n            'mean_advantage': advantages.mean().item()\n        }\n    def train_step(self, initial_states):\n        trajectory = self.imagine_trajectories(initial_states)\n        losses = self.update_actor_critic(trajectory)\n        return losses\nclass SimpleDreamerWorldModel(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=128):\n        super().__init__()\n        self.dynamics = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, hidden_dim), \n            nn.ReLU(),\n            nn.Linear(hidden_dim, state_dim)\n        )\n        self.reward_model = nn.Sequential(\n            nn.Linear(state_dim + action_dim, hidden_dim),\n            nn.ReLU(),\n            nn.Linear(hidden_dim, 1)\n        )\n    def forward(self, state, action):\n        input_tensor = torch.cat([state, action], dim=-1)\n        next_state = self.dynamics(input_tensor)\n        reward = self.reward_model(input_tensor).squeeze(-1)\n        return next_state, reward\nprint(\"ðŸŽ¯ Planning in Latent Space - Complete Dreamer Implementation\")\nprint(\"=\" * 70)\nprint(\"\\\\n1. Creating simplified continuous control environment...\")\nclass ContinuousPendulum:\n    \\\"\\\"\\\"Continuous pendulum environment\\\"\\\"\\\"\n    def __init__(self):\n        self.state_dim = 3\n        self.action_dim = 1\n        self.max_torque = 2.0\n        self.max_speed = 8.0\n        self.dt = 0.05\n        self.g = 10.0\n        self.m = 1.0\n        self.l = 1.0\n        self.state = None\n        self.step_count = 0\n        self.max_steps = 200\n    def reset(self):\n        theta = np.random.uniform(-np.pi, np.pi)\n        theta_dot = np.random.uniform(-1, 1)\n        self.state = np.array([np.cos(theta), np.sin(theta), theta_dot])\n        self.step_count = 0\n        return self.state.copy()\n    def step(self, action):\n        if isinstance(action, torch.Tensor):\n            action = action.cpu().numpy()\n        if action.ndim > 0:\n            action = action.item()\n        action = np.clip(action, -self.max_torque, self.max_torque)\n        cos_theta, sin_theta, theta_dot = self.state\n        theta = np.arctan2(sin_theta, cos_theta)\n        theta_dot_dot = (3 * self.g / (2 * self.l) * np.sin(theta) + 3 / (self.m * self.l**2) * action)\n        theta_dot = theta_dot + theta_dot_dot * self.dt\n        theta_dot = np.clip(theta_dot, -self.max_speed, self.max_speed)\n        theta = theta + theta_dot * self.dt\n        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n        self.state = np.array([np.cos(theta), np.sin(theta), theta_dot])\n        reward = -(theta**2 + 0.1 * theta_dot**2 + 0.001 * action**2)\n        self.step_count += 1\n        done = self.step_count >= self.max_steps\n        return self.state.copy(), reward, done\nenv = ContinuousPendulum()\nprint(\"\\\\n2. Collecting data and training world model...\")\ndef collect_world_model_data(env, n_episodes=100):\n    \\\"\\\"\\\"Collect data for world model training\\\"\\\"\\\"\n    data = []\n    for episode in range(n_episodes):\n        state = env.reset()\n        for step in range(200):\n            action = np.random.uniform(-env.max_torque, env.max_torque)\n            next_state, reward, done = env.step(action)\n            data.append({\n                'state': state,\n                'action': [action],\n                'reward': reward,\n                'next_state': next_state\n            })\n            state = next_state\n            if done:\n                break\n    return data\nworld_data = collect_world_model_data(env, n_episodes=50)\nprint(f\"Collected {len(world_data)} transitions\")\nworld_model = SimpleDreamerWorldModel(env.state_dim, env.action_dim, hidden_dim=128).to(device)\nworld_optimizer = optim.Adam(world_model.parameters(), lr=1e-3)\nn_world_epochs = 200\nbatch_size = 64\nprint(\"Training world model...\")\nfor epoch in range(n_world_epochs):\n    batch_data = random.sample(world_data, min(batch_size, len(world_data)))\n    states = torch.FloatTensor([d['state'] for d in batch_data]).to(device)\n    actions = torch.FloatTensor([d['action'] for d in batch_data]).to(device) \n    rewards = torch.FloatTensor([d['reward'] for d in batch_data]).to(device)\n    next_states = torch.FloatTensor([d['next_state'] for d in batch_data]).to(device)\n    pred_next_states, pred_rewards = world_model(states, actions)\n    dynamics_loss = F.mse_loss(pred_next_states, next_states)\n    reward_loss = F.mse_loss(pred_rewards, rewards)\n    total_loss = dynamics_loss + reward_loss\n    world_optimizer.zero_grad()\n    total_loss.backward()\n    world_optimizer.step()\n    if (epoch + 1) % 50 == 0:\n        print(f\"Epoch {epoch+1}: Dynamics Loss = {dynamics_loss.item():.6f}, \"\n              f\"Reward Loss = {reward_loss.item():.6f}\")\nprint(\"\\\\n3. Training Dreamer agent...\")\ndreamer_agent = DreamerAgent(\n    world_model=world_model,\n    state_dim=env.state_dim,\n    action_dim=env.action_dim,\n    device=device,\n    imagination_horizon=10\n)\nn_training_episodes = 100\ntraining_rewards = []\nfor episode in range(n_training_episodes):\n    real_state = env.reset()\n    episode_reward = 0\n    real_states = []\n    for _ in range(5):\n        real_states.append(real_state)\n        action = dreamer_agent.actor.get_action(\n            torch.FloatTensor(real_state).unsqueeze(0).to(device), \n            deterministic=False\n        ).cpu().numpy()[0]\n        next_state, reward, done = env.step(action)\n        episode_reward += reward\n        real_state = next_state\n        if done:\n            break\n    training_rewards.append(episode_reward)\n    if len(real_states) > 0:\n        initial_states = torch.FloatTensor(real_states).to(device)\n        for _ in range(10):\n            losses = dreamer_agent.train_step(initial_states)\n    if (episode + 1) % 20 == 0:\n        recent_reward = np.mean(training_rewards[-10:])\n        print(f\"Episode {episode+1}: Recent Reward = {recent_reward:.2f}\")\nprint(\"\\\\n4. Evaluating Dreamer agent...\")\neval_rewards = []\ndreamer_agent.actor.eval()\nfor eval_episode in range(20):\n    state = env.reset()\n    episode_reward = 0\n    for step in range(200):\n        with torch.no_grad():\n            action = dreamer_agent.actor.get_action(\n                torch.FloatTensor(state).unsqueeze(0).to(device),\n                deterministic=True\n            ).cpu().numpy()[0]\n        next_state, reward, done = env.step(action)\n        episode_reward += reward\n        state = next_state\n        if done:\n            break\n    eval_rewards.append(episode_reward)\ndreamer_agent.actor.train()\nrandom_rewards = []\nfor _ in range(20):\n    state = env.reset()\n    episode_reward = 0\n    for step in range(200):\n        action = np.random.uniform(-env.max_torque, env.max_torque)\n        next_state, reward, done = env.step(action)\n        episode_reward += reward\n        state = next_state\n        if done:\n            break\n    random_rewards.append(episode_reward)\nplt.figure(figsize=(20, 15))\nplt.subplot(3, 4, 1)\nplt.plot(training_rewards, alpha=0.7, linewidth=1)\nsmooth_rewards = pd.Series(training_rewards).rolling(window=10).mean()\nplt.plot(smooth_rewards, linewidth=2, label='Smooth')\nplt.title('Dreamer Training Progress')\nplt.xlabel('Episode')\nplt.ylabel('Episode Reward')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 2)\nif dreamer_agent.stats['actor_loss']:\n    plt.plot(dreamer_agent.stats['actor_loss'], label='Actor Loss', linewidth=2)\n    plt.plot(dreamer_agent.stats['critic_loss'], label='Critic Loss', linewidth=2)\n    plt.title('Dreamer Learning Losses')\n    plt.xlabel('Training Step')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 3)\nif dreamer_agent.stats['imagination_reward']:\n    plt.plot(dreamer_agent.stats['imagination_reward'], color='purple', linewidth=2)\n    plt.title('Imagined Episode Rewards')\n    plt.xlabel('Training Step') \n    plt.ylabel('Mean Imagined Reward')\n    plt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 4)\nmethods = ['Dreamer', 'Random']\nmean_rewards = [np.mean(eval_rewards), np.mean(random_rewards)]\nstd_rewards = [np.std(eval_rewards), np.std(random_rewards)]\nbars = plt.bar(methods, mean_rewards, yerr=std_rewards, capsize=5, alpha=0.7,\n               color=['skyblue', 'orange'])\nplt.title('Performance Comparison')\nplt.ylabel('Episode Reward')\nplt.grid(True, alpha=0.3)\nfor bar, mean_val, std_val in zip(bars, mean_rewards, std_rewards):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val,\n             f'{mean_val:.1f}Â±{std_val:.1f}', ha='center', va='bottom')\nplt.subplot(3, 4, 5)\nplt.boxplot([eval_rewards, random_rewards], labels=['Dreamer', 'Random'])\nplt.title('Reward Distribution')\nplt.ylabel('Episode Reward')\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 6)\nif dreamer_agent.stats['policy_entropy']:\n    plt.plot(dreamer_agent.stats['policy_entropy'], color='green', linewidth=2)\n    plt.title('Policy Entropy')\n    plt.xlabel('Training Step')\n    plt.ylabel('Entropy')\n    plt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 7)\ntest_states = torch.FloatTensor([d['state'] for d in world_data[:100]]).to(device)\ntest_actions = torch.FloatTensor([d['action'] for d in world_data[:100]]).to(device)\ntest_rewards = torch.FloatTensor([d['reward'] for d in world_data[:100]]).to(device)\ntest_next_states = torch.FloatTensor([d['next_state'] for d in world_data[:100]]).to(device)\nwith torch.no_grad():\n    pred_next_states, pred_rewards = world_model(test_states, test_actions)\n    dynamics_error = F.mse_loss(pred_next_states, test_next_states).item()\n    reward_error = F.mse_loss(pred_rewards, test_rewards).item()\nerrors = [dynamics_error, reward_error]\nlabels = ['Dynamics', 'Reward']\nbars = plt.bar(labels, errors, alpha=0.7, color=['red', 'green'])\nplt.title('World Model Prediction Errors')\nplt.ylabel('MSE')\nplt.yscale('log')\nfor bar, error in zip(bars, errors):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n             f'{error:.2e}', ha='center', va='bottom')\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 8)\nwith torch.no_grad():\n    sample_state = torch.FloatTensor([[1.0, 0.0, 0.0]]).to(device)\n    imagination = dreamer_agent.imagine_trajectories(sample_state, batch_size=1)\n    rewards_traj = imagination['rewards'][:, 0].cpu().numpy()\nplt.plot(rewards_traj, 'o-', linewidth=2, markersize=6)\nplt.title('Sample Imagined Trajectory')\nplt.xlabel('Imagination Step')\nplt.ylabel('Predicted Reward')\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 9)\nstats_names = ['Training Episodes', 'Real Steps per Episode', 'Imagination Horizon', \n               'World Model Parameters', 'Actor Parameters', 'Critic Parameters']\nstats_values = [\n    n_training_episodes,\n    5,\n    dreamer_agent.imagination_horizon,\n    sum(p.numel() for p in world_model.parameters()),\n    sum(p.numel() for p in dreamer_agent.actor.parameters()),\n    sum(p.numel() for p in dreamer_agent.critic.parameters())\n]\ny_pos = np.arange(len(stats_names))\nplt.barh(y_pos, stats_values, alpha=0.7)\nplt.yticks(y_pos, stats_names)\nplt.xlabel('Value')\nplt.title('Training Configuration')\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 10)\nsample_states = torch.FloatTensor([[np.cos(theta), np.sin(theta), 0.0] \n                                  for theta in np.linspace(-np.pi, np.pi, 100)]).to(device)\nwith torch.no_grad():\n    actions = dreamer_agent.actor.get_action(sample_states, deterministic=True).cpu().numpy()\nangles = np.linspace(-np.pi, np.pi, 100)\nplt.plot(angles, actions, linewidth=2)\nplt.title('Learned Policy Actions')\nplt.xlabel('Pendulum Angle (radians)')\nplt.ylabel('Action (Torque)')\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 11)\nreal_samples = n_training_episodes * 5\nimagined_samples = n_training_episodes * 10 * dreamer_agent.imagination_horizon\nsample_data = [real_samples, imagined_samples]\nsample_labels = ['Real Samples', 'Imagined Samples']\ncolors = ['red', 'blue']\nbars = plt.bar(sample_labels, sample_data, color=colors, alpha=0.7)\nplt.title('Sample Usage')\nplt.ylabel('Number of Samples')\nplt.yscale('log')\nfor bar, samples in zip(bars, sample_data):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n             f'{samples:,}', ha='center', va='bottom')\nplt.grid(True, alpha=0.3)\nplt.subplot(3, 4, 12)\nimprovement = (np.mean(eval_rewards) - np.mean(random_rewards)) / abs(np.mean(random_rewards)) * 100\nsample_efficiency = imagined_samples / real_samples\nmetrics = ['Performance\\\\nImprovement (%)', 'Sample\\\\nEfficiency Ratio', \n           'Imagination\\\\nHorizon', 'Training\\\\nStability']\nvalues = [improvement, sample_efficiency, dreamer_agent.imagination_horizon,\n          1.0 - (np.std(eval_rewards) / abs(np.mean(eval_rewards)))]\nbars = plt.bar(metrics, values, alpha=0.7, color=['green', 'blue', 'orange', 'purple'])\nplt.title('Key Metrics')\nplt.ylabel('Value')\nfor bar, val in zip(bars, values):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n             f'{val:.1f}', ha='center', va='bottom')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\nprint(f\"\\\\nðŸŽ¯ COMPREHENSIVE DREAMER ANALYSIS\")\nprint(\"=\" * 70)\nprint(f\"\\\\nðŸ“Š Performance Metrics:\")\nprint(f\"  Dreamer Agent: {np.mean(eval_rewards):.2f} Â± {np.std(eval_rewards):.2f}\")\nprint(f\"  Random Policy: {np.mean(random_rewards):.2f} Â± {np.std(random_rewards):.2f}\")\nprint(f\"  Performance Improvement: {improvement:.1f}%\")\nprint(f\"\\\\nðŸ”¬ Sample Efficiency:\")\nprint(f\"  Real Environment Steps: {real_samples:,}\")\nprint(f\"  Imagined Steps: {imagined_samples:,}\")\nprint(f\"  Sample Efficiency Ratio: {sample_efficiency:.1f}x\")\nprint(f\"\\\\nðŸ§  Model Complexity:\")\nprint(f\"  World Model Parameters: {sum(p.numel() for p in world_model.parameters()):,}\")\nprint(f\"  Actor Parameters: {sum(p.numel() for p in dreamer_agent.actor.parameters()):,}\")\nprint(f\"  Critic Parameters: {sum(p.numel() for p in dreamer_agent.critic.parameters()):,}\")\nprint(f\"\\\\nðŸŽ® World Model Accuracy:\")\nprint(f\"  Dynamics Prediction Error: {dynamics_error:.2e}\")\nprint(f\"  Reward Prediction Error: {reward_error:.2e}\")\nprint(f\"\\\\nâœ… ADVANCED MODEL-BASED RL DEMONSTRATION COMPLETE!\")\nprint(f\"ðŸŒŸ Key Achievements:\")\nprint(f\"   â€¢ Implemented complete world model with VAE and RSSM\")\nprint(f\"   â€¢ Demonstrated planning in latent space with Actor-Critic\")\nprint(f\"   â€¢ Achieved significant sample efficiency improvements\")\nprint(f\"   â€¢ Showed imagination-based learning capabilities\")\nprint(f\"   â€¢ Comprehensive analysis of model-based vs model-free trade-offs\")\nprint(f\"\\\\nðŸš€ FUTURE DIRECTIONS:\")\nprint(f\"   â€¢ Hierarchical world models for complex environments\")\nprint(f\"   â€¢ Meta-learning for quick adaptation to new domains\") \nprint(f\"   â€¢ Uncertainty-aware planning with ensemble methods\")\nprint(f\"   â€¢ Integration with large-scale neural network architectures\")\nprint(f\"   â€¢ Real-world applications in robotics and control\")\nprint(f\"\\\\nðŸŽ“ CONGRATULATIONS!\")\nprint(f\"You have mastered advanced model-based reinforcement learning!\")\nprint(f\"This comprehensive implementation covers state-of-the-art methods\")\nprint(f\"used in modern RL research and applications.\")\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}