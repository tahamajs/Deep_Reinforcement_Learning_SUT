{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "462a8c5b",
   "metadata": {},
   "source": [
    "# CA11: Advanced Model-Based RL and World Models\n",
    "\n",
    "## Deep Reinforcement Learning - Session 11\n",
    "\n",
    "**Advanced Model-Based Reinforcement Learning: World Models, Planning in Latent Space, and Modern Approaches**\n",
    "\n",
    "This notebook explores cutting-edge model-based reinforcement learning techniques, focusing on world models, latent space planning, uncertainty quantification, and state-of-the-art methods like Dreamer, PlaNet, and Model-Based Policy Optimization (MBPO).\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand world models and latent state representations\n",
    "2. Implement variational autoencoders for state modeling\n",
    "3. Master planning in latent space with learned dynamics\n",
    "4. Explore uncertainty quantification in model-based RL\n",
    "5. Implement Dreamer-style world model learning\n",
    "6. Understand Model-Based Policy Optimization (MBPO)\n",
    "7. Apply ensemble methods for robust model learning\n",
    "8. Analyze sample efficiency and computational trade-offs\n",
    "\n",
    "### Notebook Structure:\n",
    "1. **World Models and Latent Representations** - VAE-based state modeling\n",
    "2. **Recurrent State Space Models** - Temporal dynamics in latent space\n",
    "3. **Planning in Latent Space** - Actor-Critic learning within world models\n",
    "4. **Uncertainty Quantification** - Ensemble models and Bayesian approaches\n",
    "5. **Model-Based Policy Optimization** - MBPO algorithm implementation\n",
    "6. **Dreamer Algorithm** - Complete world model implementation\n",
    "7. **Advanced Techniques** - Meta-learning and hierarchical models\n",
    "8. **Comprehensive Evaluation** - Sample efficiency and performance analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869d398b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Essential Imports and Advanced Setup\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Essential Imports and Advanced Setup\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, MultivariateNormal, kl_divergence\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque\n",
    "import random\n",
    "import pickle\n",
    "from typing import Tuple, List, Dict, Optional, Union, NamedTuple\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced imports for world models\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.distributions.utils import _standard_normal\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Device configuration with optimizations\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"üöÄ Advanced Model-Based RL Environment Setup\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Plotting configuration for professional visualizations\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "plt.rcParams['xtick.labelsize'] = 10\n",
    "plt.rcParams['ytick.labelsize'] = 10\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "# Color palette for consistent visualizations\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "sns.set_palette(colors)\n",
    "\n",
    "print(\"‚úÖ Environment setup complete!\")\n",
    "print(\"üåü Ready for advanced model-based reinforcement learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921f1c73",
   "metadata": {},
   "source": [
    "# Section 1: World Models and Latent Representations\n",
    "\n",
    "## 1.1 Theoretical Foundation\n",
    "\n",
    "**World Models** represent a paradigm shift in model-based reinforcement learning, where instead of learning models in the raw observation space, we learn compressed representations of the environment in a latent space.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Latent State Representation**: $z_t = \\text{Encode}(o_t)$\n",
    "2. **Dynamics in Latent Space**: $z_{t+1} = f(z_t, a_t) + \\epsilon$\n",
    "3. **Reconstruction**: $\\hat{o}_t = \\text{Decode}(z_t)$\n",
    "\n",
    "### Advantages of World Models:\n",
    "\n",
    "- **Dimensionality Reduction**: High-dimensional observations ‚Üí compact latent states\n",
    "- **Semantic Compression**: Focus on task-relevant features\n",
    "- **Efficient Planning**: Plan in low-dimensional latent space\n",
    "- **Generalization**: Transfer learned representations across tasks\n",
    "\n",
    "## 1.2 Mathematical Framework\n",
    "\n",
    "### Variational Autoencoder (VAE) Foundation\n",
    "\n",
    "The world model uses a VAE to learn latent representations:\n",
    "\n",
    "**Encoder (Recognition Model)**:\n",
    "$$q_\\phi(z_t|o_t) = \\mathcal{N}(z_t; \\mu_\\phi(o_t), \\sigma_\\phi^2(o_t))$$\n",
    "\n",
    "**Decoder (Generative Model)**:\n",
    "$$p_\\theta(o_t|z_t) = \\mathcal{N}(o_t; \\mu_\\theta(z_t), \\sigma_\\theta^2(z_t))$$\n",
    "\n",
    "**VAE Loss Function**:\n",
    "$$\\mathcal{L}_{VAE} = \\mathcal{L}_{recon} + \\beta \\mathcal{L}_{KL}$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{L}_{recon} = ||o_t - \\hat{o}_t||^2$ (Reconstruction loss)\n",
    "- $\\mathcal{L}_{KL} = D_{KL}(q_\\phi(z_t|o_t) || p(z_t))$ (KL regularization)\n",
    "\n",
    "### Dynamics Model in Latent Space\n",
    "\n",
    "**Deterministic Dynamics**:\n",
    "$$z_{t+1} = f_\\psi(z_t, a_t)$$\n",
    "\n",
    "**Stochastic Dynamics**:\n",
    "$$p(z_{t+1}|z_t, a_t) = \\mathcal{N}(z_{t+1}; \\mu_\\psi(z_t, a_t), \\sigma_\\psi^2(z_t, a_t))$$\n",
    "\n",
    "### Reward Model\n",
    "\n",
    "**Reward Prediction**:\n",
    "$$\\hat{r}_t = r_\\xi(z_t, a_t)$$\n",
    "\n",
    "This enables complete planning in the latent space without accessing the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbae602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# World Models Implementation\n",
    "\n",
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"Variational Autoencoder for learning latent representations\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, latent_dim, hidden_dim=256, beta=1.0):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Latent space parameters\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, obs_dim)\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"Encode observation to latent distribution parameters\"\"\"\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick for sampling\"\"\"\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def decode(self, z):\n",
    "        \"\"\"Decode latent state to observation\"\"\"\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through VAE\"\"\"\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, logvar, z\n",
    "    \n",
    "    def loss_function(self, x, recon_x, mu, logvar):\n",
    "        \"\"\"VAE loss computation\"\"\"\n",
    "        # Reconstruction loss (MSE)\n",
    "        recon_loss = F.mse_loss(recon_x, x, reduction='sum')\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        \n",
    "        # Total VAE loss\n",
    "        total_loss = recon_loss + self.beta * kl_loss\n",
    "        \n",
    "        return total_loss, recon_loss, kl_loss\n",
    "\n",
    "class LatentDynamicsModel(nn.Module):\n",
    "    \"\"\"Dynamics model in latent space\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, action_dim, hidden_dim=256, stochastic=True):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.stochastic = stochastic\n",
    "        \n",
    "        # Dynamics network\n",
    "        self.dynamics = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        if stochastic:\n",
    "            # Stochastic dynamics: predict mean and log variance\n",
    "            self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "            self.fc_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        else:\n",
    "            # Deterministic dynamics\n",
    "            self.fc_next_state = nn.Linear(hidden_dim, latent_dim)\n",
    "    \n",
    "    def forward(self, z, a):\n",
    "        \"\"\"Predict next latent state given current state and action\"\"\"\n",
    "        # Concatenate state and action\n",
    "        za = torch.cat([z, a], dim=-1)\n",
    "        h = self.dynamics(za)\n",
    "        \n",
    "        if self.stochastic:\n",
    "            mu = self.fc_mu(h)\n",
    "            logvar = self.fc_logvar(h)\n",
    "            \n",
    "            if self.training:\n",
    "                # Sample during training\n",
    "                std = torch.exp(0.5 * logvar)\n",
    "                eps = torch.randn_like(std)\n",
    "                z_next = mu + eps * std\n",
    "            else:\n",
    "                # Use mean during evaluation\n",
    "                z_next = mu\n",
    "                \n",
    "            return z_next, mu, logvar\n",
    "        else:\n",
    "            z_next = self.fc_next_state(h)\n",
    "            return z_next\n",
    "    \n",
    "    def loss_function(self, z_pred, z_target, mu=None, logvar=None):\n",
    "        \"\"\"Dynamics model loss\"\"\"\n",
    "        if self.stochastic and mu is not None and logvar is not None:\n",
    "            # Stochastic dynamics loss with KL regularization\n",
    "            pred_loss = F.mse_loss(z_pred, z_target)\n",
    "            kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            return pred_loss + 0.001 * kl_loss  # Small KL weight\n",
    "        else:\n",
    "            # Deterministic dynamics loss\n",
    "            return F.mse_loss(z_pred, z_target)\n",
    "\n",
    "class RewardModel(nn.Module):\n",
    "    \"\"\"Reward prediction model in latent space\"\"\"\n",
    "    \n",
    "    def __init__(self, latent_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.reward_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, z, a):\n",
    "        \"\"\"Predict reward given latent state and action\"\"\"\n",
    "        za = torch.cat([z, a], dim=-1)\n",
    "        return self.reward_net(za).squeeze(-1)\n",
    "    \n",
    "    def loss_function(self, pred_reward, target_reward):\n",
    "        \"\"\"Reward prediction loss\"\"\"\n",
    "        return F.mse_loss(pred_reward, target_reward)\n",
    "\n",
    "class WorldModel(nn.Module):\n",
    "    \"\"\"Complete World Model combining VAE, dynamics, and reward models\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, latent_dim=64, hidden_dim=256, \n",
    "                 stochastic_dynamics=True, beta=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # Component models\n",
    "        self.vae = VariationalAutoencoder(obs_dim, latent_dim, hidden_dim, beta)\n",
    "        self.dynamics = LatentDynamicsModel(latent_dim, action_dim, hidden_dim, stochastic_dynamics)\n",
    "        self.reward_model = RewardModel(latent_dim, action_dim, hidden_dim)\n",
    "        \n",
    "        # Training statistics\n",
    "        self.training_stats = {\n",
    "            'vae_loss': [],\n",
    "            'dynamics_loss': [],\n",
    "            'reward_loss': [],\n",
    "            'total_loss': []\n",
    "        }\n",
    "    \n",
    "    def encode_observations(self, obs):\n",
    "        \"\"\"Encode observations to latent states\"\"\"\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.vae.encode(obs)\n",
    "            z = self.vae.reparameterize(mu, logvar)\n",
    "            return z\n",
    "    \n",
    "    def decode_latent_states(self, z):\n",
    "        \"\"\"Decode latent states to observations\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.vae.decode(z)\n",
    "    \n",
    "    def predict_next_state(self, z, a):\n",
    "        \"\"\"Predict next latent state\"\"\"\n",
    "        return self.dynamics(z, a)\n",
    "    \n",
    "    def predict_reward(self, z, a):\n",
    "        \"\"\"Predict reward\"\"\"\n",
    "        return self.reward_model(z, a)\n",
    "    \n",
    "    def rollout(self, initial_obs, actions, return_observations=False):\n",
    "        \"\"\"Perform rollout in world model\"\"\"\n",
    "        batch_size = initial_obs.shape[0]\n",
    "        horizon = actions.shape[1]\n",
    "        \n",
    "        # Encode initial observation\n",
    "        z = self.encode_observations(initial_obs)\n",
    "        \n",
    "        states = [z]\n",
    "        rewards = []\n",
    "        observations = []\n",
    "        \n",
    "        for t in range(horizon):\n",
    "            # Predict reward\n",
    "            r = self.predict_reward(z, actions[:, t])\n",
    "            rewards.append(r)\n",
    "            \n",
    "            # Predict next state\n",
    "            if self.dynamics.stochastic:\n",
    "                z, _, _ = self.predict_next_state(z, actions[:, t])\n",
    "            else:\n",
    "                z = self.predict_next_state(z, actions[:, t])\n",
    "            \n",
    "            states.append(z)\n",
    "            \n",
    "            if return_observations:\n",
    "                obs = self.decode_latent_states(z)\n",
    "                observations.append(obs)\n",
    "        \n",
    "        results = {\n",
    "            'states': torch.stack(states, dim=1),\n",
    "            'rewards': torch.stack(rewards, dim=1)\n",
    "        }\n",
    "        \n",
    "        if return_observations:\n",
    "            results['observations'] = torch.stack(observations, dim=1)\n",
    "        \n",
    "        return results\n",
    "\n",
    "class WorldModelTrainer:\n",
    "    \"\"\"Trainer for world model components\"\"\"\n",
    "    \n",
    "    def __init__(self, world_model, device, lr=1e-3):\n",
    "        self.world_model = world_model.to(device)\n",
    "        self.device = device\n",
    "        \n",
    "        # Optimizers for different components\n",
    "        self.vae_optimizer = optim.Adam(world_model.vae.parameters(), lr=lr)\n",
    "        self.dynamics_optimizer = optim.Adam(world_model.dynamics.parameters(), lr=lr)\n",
    "        self.reward_optimizer = optim.Adam(world_model.reward_model.parameters(), lr=lr)\n",
    "        \n",
    "        # Training statistics\n",
    "        self.losses = {\n",
    "            'vae_total': [],\n",
    "            'vae_recon': [],\n",
    "            'vae_kl': [],\n",
    "            'dynamics': [],\n",
    "            'reward': []\n",
    "        }\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Single training step on a batch of data\"\"\"\n",
    "        obs, actions, rewards, next_obs = batch\n",
    "        \n",
    "        obs = obs.to(self.device)\n",
    "        actions = actions.to(self.device)\n",
    "        rewards = rewards.to(self.device)\n",
    "        next_obs = next_obs.to(self.device)\n",
    "        \n",
    "        # Train VAE on observations\n",
    "        self.vae_optimizer.zero_grad()\n",
    "        recon_obs, mu_obs, logvar_obs, z_obs = self.world_model.vae(obs)\n",
    "        recon_next_obs, mu_next_obs, logvar_next_obs, z_next_obs = self.world_model.vae(next_obs)\n",
    "        \n",
    "        vae_loss_obs, recon_loss_obs, kl_loss_obs = self.world_model.vae.loss_function(\n",
    "            obs, recon_obs, mu_obs, logvar_obs)\n",
    "        vae_loss_next_obs, recon_loss_next_obs, kl_loss_next_obs = self.world_model.vae.loss_function(\n",
    "            next_obs, recon_next_obs, mu_next_obs, logvar_next_obs)\n",
    "        \n",
    "        vae_total_loss = vae_loss_obs + vae_loss_next_obs\n",
    "        vae_total_loss.backward()\n",
    "        self.vae_optimizer.step()\n",
    "        \n",
    "        # Train dynamics model\n",
    "        self.dynamics_optimizer.zero_grad()\n",
    "        z_obs_detached = z_obs.detach()\n",
    "        z_next_obs_detached = z_next_obs.detach()\n",
    "        \n",
    "        if self.world_model.dynamics.stochastic:\n",
    "            z_pred, mu_pred, logvar_pred = self.world_model.dynamics(z_obs_detached, actions)\n",
    "            dynamics_loss = self.world_model.dynamics.loss_function(\n",
    "                z_pred, z_next_obs_detached, mu_pred, logvar_pred)\n",
    "        else:\n",
    "            z_pred = self.world_model.dynamics(z_obs_detached, actions)\n",
    "            dynamics_loss = self.world_model.dynamics.loss_function(z_pred, z_next_obs_detached)\n",
    "        \n",
    "        dynamics_loss.backward()\n",
    "        self.dynamics_optimizer.step()\n",
    "        \n",
    "        # Train reward model\n",
    "        self.reward_optimizer.zero_grad()\n",
    "        pred_rewards = self.world_model.reward_model(z_obs_detached, actions)\n",
    "        reward_loss = self.world_model.reward_model.loss_function(pred_rewards, rewards)\n",
    "        reward_loss.backward()\n",
    "        self.reward_optimizer.step()\n",
    "        \n",
    "        # Record losses\n",
    "        self.losses['vae_total'].append(vae_total_loss.item())\n",
    "        self.losses['vae_recon'].append((recon_loss_obs + recon_loss_next_obs).item())\n",
    "        self.losses['vae_kl'].append((kl_loss_obs + kl_loss_next_obs).item())\n",
    "        self.losses['dynamics'].append(dynamics_loss.item())\n",
    "        self.losses['reward'].append(reward_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'vae_loss': vae_total_loss.item(),\n",
    "            'dynamics_loss': dynamics_loss.item(),\n",
    "            'reward_loss': reward_loss.item()\n",
    "        }\n",
    "\n",
    "# Simple Environment for World Model Testing\n",
    "class ContinuousCartPole:\n",
    "    \"\"\"Continuous version of CartPole for world model testing\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state_dim = 4\n",
    "        self.action_dim = 1\n",
    "        self.max_steps = 200\n",
    "        self.current_step = 0\n",
    "        \n",
    "        # Physics parameters\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masspole + self.masscart\n",
    "        self.length = 0.5\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02  # seconds between state updates\n",
    "        \n",
    "        # State bounds\n",
    "        self.x_threshold = 2.4\n",
    "        self.theta_threshold_radians = 12 * 2 * math.pi / 360\n",
    "        \n",
    "        self.state = None\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state\"\"\"\n",
    "        self.state = np.random.uniform(-0.05, 0.05, size=(4,))\n",
    "        self.current_step = 0\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take environment step\"\"\"\n",
    "        if isinstance(action, torch.Tensor):\n",
    "            action = action.cpu().numpy()\n",
    "        if isinstance(action, np.ndarray):\n",
    "            action = action.item()\n",
    "        \n",
    "        # Clip action to valid range\n",
    "        action = np.clip(action, -1.0, 1.0)\n",
    "        force = action * self.force_mag\n",
    "        \n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        \n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        \n",
    "        # Physics calculations\n",
    "        temp = (force + self.polemass_length * theta_dot ** 2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / (\n",
    "            self.length * (4.0 / 3.0 - self.masspole * costheta ** 2 / self.total_mass)\n",
    "        )\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        \n",
    "        # Update state\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "        \n",
    "        self.state = np.array([x, x_dot, theta, theta_dot])\n",
    "        self.current_step += 1\n",
    "        \n",
    "        # Calculate reward and done\n",
    "        done = (\n",
    "            x < -self.x_threshold or x > self.x_threshold or\n",
    "            theta < -self.theta_threshold_radians or theta > self.theta_threshold_radians or\n",
    "            self.current_step >= self.max_steps\n",
    "        )\n",
    "        \n",
    "        reward = 1.0 if not done else 0.0\n",
    "        \n",
    "        return self.state.copy(), reward, done\n",
    "    \n",
    "    def sample_action(self):\n",
    "        \"\"\"Sample random action\"\"\"\n",
    "        return np.random.uniform(-1.0, 1.0)\n",
    "\n",
    "# Demonstration: World Model Learning\n",
    "print(\"üåç World Models and Latent Representations Demonstration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create environment and collect data\n",
    "print(\"\\\\n1. Setting up environment and collecting data...\")\n",
    "env = ContinuousCartPole()\n",
    "\n",
    "# Data collection\n",
    "def collect_random_data(env, n_episodes=100):\n",
    "    \\\"\\\"\\\"Collect random interaction data\\\"\\\"\\\"\n",
    "    data = {\n",
    "        'observations': [],\n",
    "        'actions': [],\n",
    "        'rewards': [],\n",
    "        'next_observations': []\n",
    "    }\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        \n",
    "        for step in range(200):  # Max episode length\n",
    "            action = env.sample_action()\n",
    "            next_obs, reward, done = env.step(action)\n",
    "            \n",
    "            data['observations'].append(obs)\n",
    "            data['actions'].append([action])\n",
    "            data['rewards'].append(reward)\n",
    "            data['next_observations'].append(next_obs)\n",
    "            \n",
    "            obs = next_obs\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    # Convert to tensors\n",
    "    for key in data:\n",
    "        data[key] = torch.FloatTensor(data[key])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Collect training data\n",
    "training_data = collect_random_data(env, n_episodes=50)\n",
    "print(f\"Collected {len(training_data['observations'])} transitions\")\n",
    "\n",
    "# Create world model\n",
    "print(\"\\\\n2. Creating and training world model...\")\n",
    "world_model = WorldModel(\n",
    "    obs_dim=env.state_dim,\n",
    "    action_dim=env.action_dim,\n",
    "    latent_dim=8,  # Compact latent representation\n",
    "    hidden_dim=128,\n",
    "    stochastic_dynamics=True,\n",
    "    beta=1.0\n",
    ")\n",
    "\n",
    "trainer = WorldModelTrainer(world_model, device, lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "n_epochs = 100\n",
    "batch_size = 64\n",
    "n_batches = len(training_data['observations']) // batch_size\n",
    "\n",
    "print(f\"Training for {n_epochs} epochs with batch size {batch_size}\")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Shuffle data\n",
    "    indices = torch.randperm(len(training_data['observations']))\n",
    "    \n",
    "    for batch_idx in range(n_batches):\n",
    "        # Create batch\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        batch_indices = indices[start_idx:end_idx]\n",
    "        \n",
    "        batch = (\n",
    "            training_data['observations'][batch_indices],\n",
    "            training_data['actions'][batch_indices],\n",
    "            training_data['rewards'][batch_indices],\n",
    "            training_data['next_observations'][batch_indices]\n",
    "        )\n",
    "        \n",
    "        # Training step\n",
    "        losses = trainer.train_step(batch)\n",
    "        epoch_losses.append(losses)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        avg_losses = {k: np.mean([l[k] for l in epoch_losses]) for k in epoch_losses[0].keys()}\n",
    "        print(f\"Epoch {epoch+1}: VAE Loss = {avg_losses['vae_loss']:.4f}, \"\n",
    "              f\"Dynamics Loss = {avg_losses['dynamics_loss']:.4f}, \"\n",
    "              f\"Reward Loss = {avg_losses['reward_loss']:.4f}\")\n",
    "\n",
    "print(\"\\\\n3. Evaluating world model performance...\")\n",
    "\n",
    "# Test world model predictions\n",
    "world_model.eval()\n",
    "test_data = collect_random_data(env, n_episodes=10)\n",
    "test_batch_size = min(100, len(test_data['observations']))\n",
    "\n",
    "# Sample test batch\n",
    "test_indices = torch.randperm(len(test_data['observations']))[:test_batch_size]\n",
    "test_obs = test_data['observations'][test_indices].to(device)\n",
    "test_actions = test_data['actions'][test_indices].to(device)\n",
    "test_rewards = test_data['rewards'][test_indices].to(device)\n",
    "test_next_obs = test_data['next_observations'][test_indices].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Test VAE reconstruction\n",
    "    recon_obs, _, _, z_obs = world_model.vae(test_obs)\n",
    "    recon_error = F.mse_loss(recon_obs, test_obs).item()\n",
    "    \n",
    "    # Test dynamics prediction\n",
    "    if world_model.dynamics.stochastic:\n",
    "        z_pred, _, _ = world_model.dynamics(z_obs, test_actions)\n",
    "    else:\n",
    "        z_pred = world_model.dynamics(z_obs, test_actions)\n",
    "    \n",
    "    # Compare predicted latent states with actual\n",
    "    _, _, z_next_actual = world_model.vae(test_next_obs)\n",
    "    dynamics_error = F.mse_loss(z_pred, z_next_actual).item()\n",
    "    \n",
    "    # Test reward prediction\n",
    "    pred_rewards = world_model.reward_model(z_obs, test_actions)\n",
    "    reward_error = F.mse_loss(pred_rewards, test_rewards).item()\n",
    "\n",
    "print(f\"\\\\nüìä World Model Evaluation:\")\n",
    "print(f\"  Reconstruction Error (MSE): {recon_error:.6f}\")\n",
    "print(f\"  Dynamics Prediction Error (MSE): {dynamics_error:.6f}\")\n",
    "print(f\"  Reward Prediction Error (MSE): {reward_error:.6f}\")\n",
    "\n",
    "# Visualize results\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Training losses\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.plot(trainer.losses['vae_total'], label='VAE Total', linewidth=2)\n",
    "plt.plot(trainer.losses['vae_recon'], label='VAE Reconstruction', linewidth=2)\n",
    "plt.plot(trainer.losses['vae_kl'], label='VAE KL', linewidth=2)\n",
    "plt.title('VAE Training Losses')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.plot(trainer.losses['dynamics'], label='Dynamics Loss', color='red', linewidth=2)\n",
    "plt.title('Dynamics Model Training')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.plot(trainer.losses['reward'], label='Reward Loss', color='green', linewidth=2)\n",
    "plt.title('Reward Model Training')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Reconstruction visualization\n",
    "plt.subplot(2, 3, 4)\n",
    "sample_idx = 0\n",
    "original_obs = test_obs[sample_idx].cpu().numpy()\n",
    "reconstructed_obs = recon_obs[sample_idx].cpu().numpy()\n",
    "\n",
    "x_pos = np.arange(len(original_obs))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x_pos - width/2, original_obs, width, label='Original', alpha=0.7)\n",
    "plt.bar(x_pos + width/2, reconstructed_obs, width, label='Reconstructed', alpha=0.7)\n",
    "plt.title('VAE Reconstruction Example')\n",
    "plt.xlabel('State Dimension')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.xticks(x_pos, ['x', 'x_dot', 'theta', 'theta_dot'])\n",
    "\n",
    "# Latent space visualization\n",
    "plt.subplot(2, 3, 5)\n",
    "latent_states = z_obs.cpu().numpy()\n",
    "plt.scatter(latent_states[:, 0], latent_states[:, 1], alpha=0.6, s=30)\n",
    "plt.title('Latent Space Representation')\n",
    "plt.xlabel('Latent Dimension 1')\n",
    "plt.ylabel('Latent Dimension 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction accuracy\n",
    "plt.subplot(2, 3, 6)\n",
    "errors = [recon_error, dynamics_error, reward_error]\n",
    "labels = ['Reconstruction', 'Dynamics', 'Reward']\n",
    "colors = ['blue', 'red', 'green']\n",
    "\n",
    "bars = plt.bar(labels, errors, color=colors, alpha=0.7)\n",
    "plt.title('Prediction Errors')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.yscale('log')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, error in zip(bars, errors):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{error:.2e}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\\\n‚úÖ World Model demonstration complete!\")\n",
    "print(\"üöÄ Next: Recurrent State Space Models for temporal dynamics\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae723a14",
   "metadata": {},
   "source": [
    "# Section 2: Recurrent State Space Models (RSSM)\n",
    "\n",
    "## 2.1 Theoretical Foundation\n",
    "\n",
    "Recurrent State Space Models extend world models by incorporating temporal dependencies and memory into the latent dynamics. This is crucial for partially observable environments where the current observation doesn't contain all necessary information.\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "**Deterministic Recurrent State**:\n",
    "$$h_t = f(h_{t-1}, z_{t-1}, a_{t-1})$$\n",
    "\n",
    "**Stochastic State**:\n",
    "$$z_t \\sim p(z_t | h_t)$$\n",
    "\n",
    "**Combined RSSM State**:\n",
    "$$s_t = [h_t, z_t]$$ where $h_t$ is deterministic and $z_t$ is stochastic\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Recurrent Model**: Models temporal dependencies\n",
    "$$h_t = \\text{GRU}(h_{t-1}, [z_{t-1}, a_{t-1}])$$\n",
    "\n",
    "2. **Representation Model**: Encodes observations\n",
    "$$z_t \\sim q(z_t | h_t, o_t)$$\n",
    "\n",
    "3. **Transition Model**: Predicts future states\n",
    "$$z_t \\sim p(z_t | h_t)$$\n",
    "\n",
    "4. **Observation Model**: Reconstructs observations\n",
    "$$o_t \\sim p(o_t | h_t, z_t)$$\n",
    "\n",
    "5. **Reward Model**: Predicts rewards\n",
    "$$r_t \\sim p(r_t | h_t, z_t)$$\n",
    "\n",
    "### Advantages of RSSM:\n",
    "\n",
    "- **Memory**: Maintains information across time steps\n",
    "- **Partial Observability**: Handles environments where current observation is insufficient\n",
    "- **Temporal Consistency**: Models smooth transitions in latent space\n",
    "- **Hierarchical Representation**: Separates deterministic and stochastic components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbdda4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent State Space Models Implementation\n",
    "\n",
    "class RecurrentStateSpaceModel(nn.Module):\n",
    "    \"\"\"Recurrent State Space Model (RSSM) for temporal world modeling\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, stoch_dim=30, deter_dim=200, hidden_dim=400):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.stoch_dim = stoch_dim  # Stochastic state dimension\n",
    "        self.deter_dim = deter_dim  # Deterministic state dimension\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Observation encoder\n",
    "        self.obs_encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, stoch_dim * 2)  # Mean and std\n",
    "        )\n",
    "        \n",
    "        # Recurrent model (deterministic state)\n",
    "        self.rnn = nn.GRUCell(stoch_dim + action_dim, deter_dim)\n",
    "        \n",
    "        # Transition model (prior)\n",
    "        self.transition_model = nn.Sequential(\n",
    "            nn.Linear(deter_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, stoch_dim * 2)  # Mean and std\n",
    "        )\n",
    "        \n",
    "        # Representation model (posterior)\n",
    "        self.representation_model = nn.Sequential(\n",
    "            nn.Linear(deter_dim + stoch_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, stoch_dim * 2)  # Mean and std\n",
    "        )\n",
    "        \n",
    "        # Observation decoder\n",
    "        self.obs_decoder = nn.Sequential(\n",
    "            nn.Linear(deter_dim + stoch_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, obs_dim)\n",
    "        )\n",
    "        \n",
    "        # Reward model\n",
    "        self.reward_model = nn.Sequential(\n",
    "            nn.Linear(deter_dim + stoch_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Value model for planning\n",
    "        self.value_model = nn.Sequential(\n",
    "            nn.Linear(deter_dim + stoch_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def initial_state(self, batch_size):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        h = torch.zeros(batch_size, self.deter_dim, device=next(self.parameters()).device)\n",
    "        z = torch.zeros(batch_size, self.stoch_dim, device=next(self.parameters()).device)\n",
    "        return h, z\n",
    "    \n",
    "    def encode_obs(self, obs):\n",
    "        \"\"\"Encode observation to stochastic state distribution\"\"\"\n",
    "        encoded = self.obs_encoder(obs)\n",
    "        mean, std = torch.chunk(encoded, 2, dim=-1)\n",
    "        std = F.softplus(std) + 1e-4\n",
    "        return mean, std\n",
    "    \n",
    "    def transition_prior(self, h):\n",
    "        \"\"\"Prior transition model p(z_t | h_t)\"\"\"\n",
    "        encoded = self.transition_model(h)\n",
    "        mean, std = torch.chunk(encoded, 2, dim=-1)\n",
    "        std = F.softplus(std) + 1e-4\n",
    "        return mean, std\n",
    "    \n",
    "    def representation_posterior(self, h, obs_encoded):\n",
    "        \"\"\"Posterior representation model q(z_t | h_t, o_t)\"\"\"\n",
    "        encoded = self.representation_model(torch.cat([h, obs_encoded], dim=-1))\n",
    "        mean, std = torch.chunk(encoded, 2, dim=-1)\n",
    "        std = F.softplus(std) + 1e-4\n",
    "        return mean, std\n",
    "    \n",
    "    def reparameterize(self, mean, std):\n",
    "        \"\"\"Reparameterization trick\"\"\"\n",
    "        if self.training:\n",
    "            eps = torch.randn_like(std)\n",
    "            return mean + eps * std\n",
    "        else:\n",
    "            return mean\n",
    "    \n",
    "    def recurrent_step(self, prev_h, prev_z, action):\n",
    "        \"\"\"Single recurrent step\"\"\"\n",
    "        rnn_input = torch.cat([prev_z, action], dim=-1)\n",
    "        h = self.rnn(rnn_input, prev_h)\n",
    "        return h\n",
    "    \n",
    "    def observe(self, obs, prev_h, prev_z, action):\n",
    "        \"\"\"Observation step: encode observation and update state\"\"\"\n",
    "        # Recurrent step\n",
    "        h = self.recurrent_step(prev_h, prev_z, action)\n",
    "        \n",
    "        # Encode observation\n",
    "        obs_encoded = self.obs_encoder(obs)\n",
    "        \n",
    "        # Prior and posterior\n",
    "        prior_mean, prior_std = self.transition_prior(h)\n",
    "        post_mean, post_std = self.representation_posterior(h, obs_encoded)\n",
    "        \n",
    "        # Sample stochastic state\n",
    "        z = self.reparameterize(post_mean, post_std)\n",
    "        \n",
    "        return h, z, (prior_mean, prior_std), (post_mean, post_std)\n",
    "    \n",
    "    def imagine(self, prev_h, prev_z, action):\n",
    "        \"\"\"Imagination step: predict next state without observation\"\"\"\n",
    "        # Recurrent step\n",
    "        h = self.recurrent_step(prev_h, prev_z, action)\n",
    "        \n",
    "        # Prior transition\n",
    "        prior_mean, prior_std = self.transition_prior(h)\n",
    "        \n",
    "        # Sample stochastic state\n",
    "        z = self.reparameterize(prior_mean, prior_std)\n",
    "        \n",
    "        return h, z, (prior_mean, prior_std)\n",
    "    \n",
    "    def decode_obs(self, h, z):\n",
    "        \"\"\"Decode observation from state\"\"\"\n",
    "        state = torch.cat([h, z], dim=-1)\n",
    "        return self.obs_decoder(state)\n",
    "    \n",
    "    def predict_reward(self, h, z):\n",
    "        \"\"\"Predict reward from state\"\"\"\n",
    "        state = torch.cat([h, z], dim=-1)\n",
    "        return self.reward_model(state).squeeze(-1)\n",
    "    \n",
    "    def predict_value(self, h, z):\n",
    "        \"\"\"Predict value from state\"\"\"\n",
    "        state = torch.cat([h, z], dim=-1)\n",
    "        return self.value_model(state).squeeze(-1)\n",
    "\n",
    "class RSSMTrainer:\n",
    "    \"\"\"Trainer for RSSM model\"\"\"\n",
    "    \n",
    "    def __init__(self, rssm_model, device, lr=1e-4, kl_weight=1.0, free_nats=3.0):\n",
    "        self.rssm_model = rssm_model.to(device)\n",
    "        self.device = device\n",
    "        self.kl_weight = kl_weight\n",
    "        self.free_nats = free_nats  # Free nats for KL regularization\n",
    "        \n",
    "        self.optimizer = optim.Adam(rssm_model.parameters(), lr=lr, eps=1e-4)\n",
    "        \n",
    "        # Training statistics\n",
    "        self.losses = {\n",
    "            'total': [],\n",
    "            'reconstruction': [],\n",
    "            'kl_divergence': [],\n",
    "            'reward': []\n",
    "        }\n",
    "    \n",
    "    def kl_divergence(self, post_mean, post_std, prior_mean, prior_std):\n",
    "        \"\"\"Compute KL divergence between posterior and prior\"\"\"\n",
    "        post_dist = Normal(post_mean, post_std)\n",
    "        prior_dist = Normal(prior_mean, prior_std)\n",
    "        kl = kl_divergence(post_dist, prior_dist)\n",
    "        \n",
    "        # Apply free nats\n",
    "        kl = torch.maximum(kl, torch.tensor(self.free_nats, device=self.device))\n",
    "        \n",
    "        return kl.sum(-1)  # Sum over stochastic dimensions\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        observations, actions, rewards = batch\n",
    "        batch_size, seq_len = observations.shape[:2]\n",
    "        \n",
    "        observations = observations.to(self.device)\n",
    "        actions = actions.to(self.device) \n",
    "        rewards = rewards.to(self.device)\n",
    "        \n",
    "        # Initialize states\n",
    "        h, z = self.rssm_model.initial_state(batch_size)\n",
    "        \n",
    "        # Storage for losses\n",
    "        reconstruction_losses = []\n",
    "        kl_losses = []\n",
    "        reward_losses = []\n",
    "        \n",
    "        # Forward pass through sequence\n",
    "        for t in range(seq_len):\n",
    "            # Observe step\n",
    "            h, z, (prior_mean, prior_std), (post_mean, post_std) = self.rssm_model.observe(\n",
    "                observations[:, t], h, z, actions[:, t])\n",
    "            \n",
    "            # Reconstruction loss\n",
    "            pred_obs = self.rssm_model.decode_obs(h, z)\n",
    "            recon_loss = F.mse_loss(pred_obs, observations[:, t], reduction='none').sum(-1)\n",
    "            reconstruction_losses.append(recon_loss)\n",
    "            \n",
    "            # KL loss\n",
    "            kl_loss = self.kl_divergence(post_mean, post_std, prior_mean, prior_std)\n",
    "            kl_losses.append(kl_loss)\n",
    "            \n",
    "            # Reward loss\n",
    "            pred_reward = self.rssm_model.predict_reward(h, z)\n",
    "            reward_loss = F.mse_loss(pred_reward, rewards[:, t], reduction='none')\n",
    "            reward_losses.append(reward_loss)\n",
    "        \n",
    "        # Aggregate losses\n",
    "        reconstruction_loss = torch.stack(reconstruction_losses).mean()\n",
    "        kl_loss = torch.stack(kl_losses).mean()\n",
    "        reward_loss = torch.stack(reward_losses).mean()\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = reconstruction_loss + self.kl_weight * kl_loss + reward_loss\n",
    "        \n",
    "        # Backward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.rssm_model.parameters(), 100.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Record losses\n",
    "        self.losses['total'].append(total_loss.item())\n",
    "        self.losses['reconstruction'].append(reconstruction_loss.item())\n",
    "        self.losses['kl_divergence'].append(kl_loss.item())\n",
    "        self.losses['reward'].append(reward_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'recon_loss': reconstruction_loss.item(),\n",
    "            'kl_loss': kl_loss.item(),\n",
    "            'reward_loss': reward_loss.item()\n",
    "        }\n",
    "\n",
    "# Sequence Environment for RSSM Testing\n",
    "class SequenceEnvironment:\n",
    "    \"\"\"Environment that requires memory (partial observability)\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim=4, memory_length=5):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.memory_length = memory_length\n",
    "        self.action_dim = 2  # Left or right\n",
    "        \n",
    "        self.state = None\n",
    "        self.memory = None\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 50\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment\"\"\"\n",
    "        self.state = np.zeros(self.obs_dim)\n",
    "        self.memory = deque(maxlen=self.memory_length)\n",
    "        self.step_count = 0\n",
    "        \n",
    "        # Initialize with random values\n",
    "        for _ in range(self.memory_length):\n",
    "            self.memory.append(np.random.rand())\n",
    "        \n",
    "        return self._get_observation()\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get partial observation (doesn't include full memory)\"\"\"\n",
    "        # Only return current state + partial memory information\n",
    "        recent_memory = list(self.memory)[-2:]  # Only last 2 memory items\n",
    "        \n",
    "        obs = np.concatenate([\n",
    "            self.state,\n",
    "            recent_memory + [0.0] * (2 - len(recent_memory))\n",
    "        ])\n",
    "        \n",
    "        return obs[:self.obs_dim]\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take environment step\"\"\"\n",
    "        if isinstance(action, torch.Tensor):\n",
    "            action = action.cpu().numpy()\n",
    "        if isinstance(action, np.ndarray) and action.ndim > 0:\n",
    "            action = action.item()\n",
    "        \n",
    "        # Discrete action: 0 = left, 1 = right\n",
    "        action = int(action > 0.5) if isinstance(action, float) else int(action)\n",
    "        \n",
    "        # Update memory based on action\n",
    "        if action == 0:  # Left\n",
    "            new_memory_val = max(0.0, list(self.memory)[-1] - 0.1)\n",
    "        else:  # Right\n",
    "            new_memory_val = min(1.0, list(self.memory)[-1] + 0.1)\n",
    "        \n",
    "        self.memory.append(new_memory_val)\n",
    "        \n",
    "        # Update state (simple dynamics)\n",
    "        self.state[0] = new_memory_val\n",
    "        self.state[1] = np.mean(list(self.memory))\n",
    "        self.state[2] = action\n",
    "        self.state[3] = self.step_count / self.max_steps\n",
    "        \n",
    "        # Reward based on memory sequence\n",
    "        memory_sequence = list(self.memory)\n",
    "        if len(memory_sequence) >= 3:\n",
    "            # Reward for maintaining values in middle range\n",
    "            recent_avg = np.mean(memory_sequence[-3:])\n",
    "            reward = 1.0 - abs(recent_avg - 0.5) * 2  # Max reward when avg = 0.5\n",
    "        else:\n",
    "            reward = 0.0\n",
    "        \n",
    "        self.step_count += 1\n",
    "        done = self.step_count >= self.max_steps\n",
    "        \n",
    "        return self._get_observation(), reward, done\n",
    "\n",
    "def collect_sequence_data(env, n_episodes=100, seq_length=20):\n",
    "    \"\"\"Collect sequential data for RSSM training\"\"\"\n",
    "    sequences = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs_sequence = []\n",
    "        action_sequence = []\n",
    "        reward_sequence = []\n",
    "        \n",
    "        obs = env.reset()\n",
    "        \n",
    "        for t in range(seq_length):\n",
    "            # Random policy\n",
    "            action = np.random.randint(0, 2)\n",
    "            next_obs, reward, done = env.step(action)\n",
    "            \n",
    "            obs_sequence.append(obs)\n",
    "            action_sequence.append([action])  # Make it 1D\n",
    "            reward_sequence.append(reward)\n",
    "            \n",
    "            obs = next_obs\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if len(obs_sequence) >= seq_length:\n",
    "            sequences.append({\n",
    "                'observations': obs_sequence[:seq_length],\n",
    "                'actions': action_sequence[:seq_length],\n",
    "                'rewards': reward_sequence[:seq_length]\n",
    "            })\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# RSSM Demonstration\n",
    "print(\"üîÑ Recurrent State Space Models (RSSM) Demonstration\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\\\n1. Setting up sequence environment...\")\n",
    "seq_env = SequenceEnvironment(obs_dim=4, memory_length=5)\n",
    "\n",
    "# Collect sequential data\n",
    "print(\"\\\\n2. Collecting sequential training data...\")\n",
    "sequence_data = collect_sequence_data(seq_env, n_episodes=200, seq_length=15)\n",
    "print(f\"Collected {len(sequence_data)} sequences\")\n",
    "\n",
    "# Convert to tensors\n",
    "def prepare_rssm_batch(sequences, batch_size=32):\n",
    "    \\\"\\\"\\\"Prepare batch for RSSM training\\\"\\\"\\\"\n",
    "    # Randomly sample sequences\n",
    "    batch_sequences = random.sample(sequences, min(batch_size, len(sequences)))\n",
    "    \n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    for seq in batch_sequences:\n",
    "        observations.append(seq['observations'])\n",
    "        actions.append(seq['actions'])\n",
    "        rewards.append(seq['rewards'])\n",
    "    \n",
    "    # Convert to tensors\n",
    "    observations = torch.FloatTensor(observations)\n",
    "    actions = torch.FloatTensor(actions)\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    \n",
    "    return observations, actions, rewards\n",
    "\n",
    "# Create RSSM model\n",
    "print(\"\\\\n3. Creating and training RSSM model...\")\n",
    "rssm_model = RecurrentStateSpaceModel(\n",
    "    obs_dim=seq_env.obs_dim,\n",
    "    action_dim=seq_env.action_dim,\n",
    "    stoch_dim=16,\n",
    "    deter_dim=64,\n",
    "    hidden_dim=128\n",
    ")\n",
    "\n",
    "trainer = RSSMTrainer(rssm_model, device, lr=1e-3, kl_weight=0.1)\n",
    "\n",
    "# Training\n",
    "n_training_steps = 500\n",
    "batch_size = 16\n",
    "\n",
    "print(f\"Training RSSM for {n_training_steps} steps...\")\n",
    "\n",
    "for step in range(n_training_steps):\n",
    "    # Prepare batch\n",
    "    batch = prepare_rssm_batch(sequence_data, batch_size)\n",
    "    \n",
    "    # Training step\n",
    "    losses = trainer.train_step(batch)\n",
    "    \n",
    "    if (step + 1) % 100 == 0:\n",
    "        print(f\"Step {step+1}: Total Loss = {losses['total_loss']:.4f}, \"\n",
    "              f\"Recon = {losses['recon_loss']:.4f}, \"\n",
    "              f\"KL = {losses['kl_loss']:.4f}, \"\n",
    "              f\"Reward = {losses['reward_loss']:.4f}\")\n",
    "\n",
    "# Test RSSM imagination capability\n",
    "print(\"\\\\n4. Testing RSSM imagination and prediction...\")\n",
    "rssm_model.eval()\n",
    "\n",
    "# Test on a sequence\n",
    "test_batch = prepare_rssm_batch(sequence_data, batch_size=1)\n",
    "test_obs, test_actions, test_rewards = test_batch\n",
    "\n",
    "with torch.no_grad():\n",
    "    batch_size = test_obs.shape[0]\n",
    "    h, z = rssm_model.initial_state(batch_size)\n",
    "    \n",
    "    # Observe first few steps\n",
    "    observe_steps = 5\n",
    "    imagine_steps = 10\n",
    "    \n",
    "    observations_pred = []\n",
    "    rewards_pred = []\n",
    "    \n",
    "    # Observation phase\n",
    "    for t in range(observe_steps):\n",
    "        h, z, _, _ = rssm_model.observe(\n",
    "            test_obs[:, t].to(device), h, z, test_actions[:, t].to(device))\n",
    "        \n",
    "        pred_obs = rssm_model.decode_obs(h, z)\n",
    "        pred_reward = rssm_model.predict_reward(h, z)\n",
    "        \n",
    "        observations_pred.append(pred_obs.cpu())\n",
    "        rewards_pred.append(pred_reward.cpu())\n",
    "    \n",
    "    # Imagination phase\n",
    "    for t in range(imagine_steps):\n",
    "        # Use random actions for imagination\n",
    "        random_action = torch.randint(0, 2, (batch_size, 1), dtype=torch.float).to(device)\n",
    "        h, z, _ = rssm_model.imagine(h, z, random_action)\n",
    "        \n",
    "        pred_obs = rssm_model.decode_obs(h, z)\n",
    "        pred_reward = rssm_model.predict_reward(h, z)\n",
    "        \n",
    "        observations_pred.append(pred_obs.cpu())\n",
    "        rewards_pred.append(pred_reward.cpu())\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Training losses\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.plot(trainer.losses['total'], label='Total Loss', linewidth=2)\n",
    "plt.title('RSSM Training - Total Loss')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.plot(trainer.losses['reconstruction'], label='Reconstruction', color='blue', linewidth=2)\n",
    "plt.plot(trainer.losses['kl_divergence'], label='KL Divergence', color='red', linewidth=2)\n",
    "plt.plot(trainer.losses['reward'], label='Reward', color='green', linewidth=2)\n",
    "plt.title('RSSM Component Losses')\n",
    "plt.xlabel('Training Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Prediction visualization\n",
    "plt.subplot(3, 3, 3)\n",
    "actual_obs = test_obs[0, :observe_steps, 0].numpy()\n",
    "pred_obs = torch.stack(observations_pred[:observe_steps])[:, 0, 0].numpy()\n",
    "\n",
    "plt.plot(actual_obs, 'o-', label='Actual', linewidth=2, markersize=6)\n",
    "plt.plot(pred_obs, 's-', label='Predicted', linewidth=2, markersize=6)\n",
    "plt.title('Observation Reconstruction')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Observation Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Imagination visualization\n",
    "plt.subplot(3, 3, 4)\n",
    "imagined_obs = torch.stack(observations_pred[observe_steps:])[:, 0, 0].numpy()\n",
    "time_steps = np.arange(observe_steps, observe_steps + len(imagined_obs))\n",
    "\n",
    "plt.plot(range(observe_steps), actual_obs, 'o-', label='Observed', linewidth=2)\n",
    "plt.plot(time_steps, imagined_obs, 's-', label='Imagined', linewidth=2)\n",
    "plt.axvline(x=observe_steps-0.5, color='red', linestyle='--', alpha=0.7, label='Imagination Start')\n",
    "plt.title('RSSM Imagination')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Observation Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Reward prediction\n",
    "plt.subplot(3, 3, 5)\n",
    "actual_rewards = test_rewards[0, :observe_steps].numpy()\n",
    "pred_rewards = torch.stack(rewards_pred[:observe_steps])[:, 0].numpy()\n",
    "\n",
    "plt.plot(actual_rewards, 'o-', label='Actual Rewards', linewidth=2, markersize=6)\n",
    "plt.plot(pred_rewards, 's-', label='Predicted Rewards', linewidth=2, markersize=6)\n",
    "plt.title('Reward Prediction')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# State space visualization\n",
    "plt.subplot(3, 3, 6)\n",
    "# Extract deterministic states\n",
    "h_states = []\n",
    "with torch.no_grad():\n",
    "    batch_size = test_obs.shape[0]\n",
    "    h, z = rssm_model.initial_state(batch_size)\n",
    "    \n",
    "    for t in range(observe_steps):\n",
    "        h, z, _, _ = rssm_model.observe(\n",
    "            test_obs[:, t].to(device), h, z, test_actions[:, t].to(device))\n",
    "        h_states.append(h.cpu().numpy())\n",
    "\n",
    "h_states = np.array(h_states)[:, 0, :2]  # First 2 dimensions\n",
    "plt.plot(h_states[:, 0], h_states[:, 1], 'o-', linewidth=2, markersize=6)\n",
    "plt.title('Deterministic State Trajectory')\n",
    "plt.xlabel('Hidden Dimension 1')\n",
    "plt.ylabel('Hidden Dimension 2')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Memory visualization\n",
    "plt.subplot(3, 3, 7)\n",
    "# Show how memory affects predictions\n",
    "memory_effect = []\n",
    "for t in range(len(observations_pred)):\n",
    "    obs_pred = observations_pred[t][0]\n",
    "    memory_component = obs_pred[1].item()  # Memory component\n",
    "    memory_effect.append(memory_component)\n",
    "\n",
    "plt.plot(memory_effect[:observe_steps], 'o-', label='Observed', linewidth=2)\n",
    "plt.plot(memory_effect[observe_steps:], 's-', label='Imagined', linewidth=2)\n",
    "plt.axvline(x=observe_steps-0.5, color='red', linestyle='--', alpha=0.7)\n",
    "plt.title('Memory Component Evolution')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Memory Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Model complexity analysis\n",
    "plt.subplot(3, 3, 8)\n",
    "model_sizes = ['Obs Encoder', 'RNN', 'Transition', 'Representation', 'Obs Decoder', 'Reward']\n",
    "param_counts = []\n",
    "\n",
    "for name, module in rssm_model.named_children():\n",
    "    params = sum(p.numel() for p in module.parameters())\n",
    "    param_counts.append(params)\n",
    "\n",
    "param_counts = param_counts[:len(model_sizes)]  # Match with labels\n",
    "bars = plt.bar(model_sizes, param_counts, alpha=0.7)\n",
    "plt.title('RSSM Model Complexity')\n",
    "plt.xlabel('Component')\n",
    "plt.ylabel('Parameters')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, count in zip(bars, param_counts):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{count}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance metrics\n",
    "plt.subplot(3, 3, 9)\n",
    "metrics = ['Recon Error', 'KL Divergence', 'Reward Error']\n",
    "final_losses = [\n",
    "    trainer.losses['reconstruction'][-1],\n",
    "    trainer.losses['kl_divergence'][-1], \n",
    "    trainer.losses['reward'][-1]\n",
    "]\n",
    "\n",
    "bars = plt.bar(metrics, final_losses, alpha=0.7, color=['blue', 'red', 'green'])\n",
    "plt.title('Final Performance Metrics')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.yscale('log')\n",
    "\n",
    "for bar, loss in zip(bars, final_losses):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{loss:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\\\nüìä RSSM Analysis:\")\n",
    "print(f\"  Final Reconstruction Loss: {trainer.losses['reconstruction'][-1]:.4f}\")\n",
    "print(f\"  Final KL Divergence: {trainer.losses['kl_divergence'][-1]:.4f}\")\n",
    "print(f\"  Final Reward Loss: {trainer.losses['reward'][-1]:.4f}\")\n",
    "print(f\"  Model Parameters: {sum(p.numel() for p in rssm_model.parameters()):,}\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ RSSM demonstration complete!\")\n",
    "print(f\"üéØ Key Insights:\")\n",
    "print(f\"  ‚Ä¢ RSSM combines deterministic and stochastic states for memory\")\n",
    "print(f\"  ‚Ä¢ Temporal dependencies enable long-horizon planning\")\n",
    "print(f\"  ‚Ä¢ Imagination capability allows model-based planning\")\n",
    "print(f\"  ‚Ä¢ KL regularization ensures meaningful latent representations\")\n",
    "\n",
    "print(f\"\\\\nüöÄ Next: Planning in Latent Space with Actor-Critic Methods\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cd811c",
   "metadata": {},
   "source": [
    "# Section 3: Planning in Latent Space with Actor-Critic Methods\n",
    "\n",
    "## 3.1 Theoretical Foundation\n",
    "\n",
    "Planning in latent space combines world models with reinforcement learning by training policies and value functions entirely within the learned latent representation. This approach, popularized by methods like Dreamer, enables sample-efficient learning through imagination.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Latent Imagination**: Generate trajectories using learned world model\n",
    "2. **Actor-Critic in Latent Space**: Train policy and value function on imagined trajectories  \n",
    "3. **Gradient-Based Planning**: Use backpropagation through the world model for planning\n",
    "\n",
    "### Mathematical Framework:\n",
    "\n",
    "**Latent State Trajectory**:\n",
    "$$\\tau = (s_0, a_0, r_1, s_1, a_1, r_2, ..., s_T)$$\n",
    "\n",
    "where $s_t = [h_t, z_t]$ in RSSM\n",
    "\n",
    "**Policy Learning in Latent Space**:\n",
    "$$\\pi_\\phi(a_t | s_t) \\text{ trained on imagined trajectories}$$\n",
    "\n",
    "**Value Function Learning**:\n",
    "$$V_\\psi(s_t) \\text{ and } Q_\\psi(s_t, a_t) \\text{ trained on imagined returns}$$\n",
    "\n",
    "**Actor-Critic Loss**:\n",
    "$$\\mathcal{L}_\\text{actor} = -\\mathbb{E}[\\lambda_t A_t \\log \\pi_\\phi(a_t | s_t)]$$\n",
    "$$\\mathcal{L}_\\text{critic} = \\frac{1}{2}\\mathbb{E}[(V_\\psi(s_t) - V_t^\\text{target})^2]$$\n",
    "\n",
    "where $\\lambda_t$ is the importance sampling ratio and $A_t$ is the advantage.\n",
    "\n",
    "### Advantages:\n",
    "\n",
    "- **Sample Efficiency**: Learn from imagined rather than real experience\n",
    "- **Long Horizon Planning**: Plan for many steps without environment interaction\n",
    "- **Gradient-Based Optimization**: Leverage automatic differentiation\n",
    "- **Continuous Action Spaces**: Natural handling of continuous control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a18c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Planning in Latent Space Implementation\n",
    "\n",
    "class LatentActor(nn.Module):\n",
    "    \"\"\"Actor network for latent space planning\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256, action_range=1.0):\n",
    "        super().__init__()\n",
    "        self.action_range = action_range\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim * 2)  # Mean and log_std\n",
    "        )\n",
    "        \n",
    "        # Initialize last layer with small weights\n",
    "        self.network[-1].weight.data.uniform_(-1e-3, 1e-3)\n",
    "        self.network[-1].bias.data.uniform_(-1e-3, 1e-3)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        output = self.network(state)\n",
    "        mean, log_std = torch.chunk(output, 2, dim=-1)\n",
    "        \n",
    "        # Constrain log_std\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        return mean, std\n",
    "    \n",
    "    def sample(self, state):\n",
    "        \"\"\"Sample action from policy\"\"\"\n",
    "        mean, std = self.forward(state)\n",
    "        normal = Normal(mean, std)\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        x = normal.rsample()\n",
    "        action = torch.tanh(x) * self.action_range\n",
    "        \n",
    "        # Compute log probability\n",
    "        log_prob = normal.log_prob(x).sum(dim=-1)\n",
    "        # Correct for tanh transformation\n",
    "        log_prob -= (2 * (np.log(2) - x - F.softplus(-2 * x))).sum(dim=-1)\n",
    "        \n",
    "        return action, log_prob\n",
    "    \n",
    "    def get_action(self, state, deterministic=False):\n",
    "        \"\"\"Get action (used for evaluation)\"\"\"\n",
    "        mean, std = self.forward(state)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = torch.tanh(mean) * self.action_range\n",
    "            return action\n",
    "        else:\n",
    "            normal = Normal(mean, std)\n",
    "            x = normal.sample()\n",
    "            action = torch.tanh(x) * self.action_range\n",
    "            return action\n",
    "\n",
    "class LatentCritic(nn.Module):\n",
    "    \"\"\"Critic network for latent space value estimation\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        return self.network(state).squeeze(-1)\n",
    "\n",
    "class DreamerAgent:\n",
    "    \"\"\"Dreamer-style agent for planning in latent space\"\"\"\n",
    "    \n",
    "    def __init__(self, world_model, state_dim, action_dim, device,\n",
    "                 actor_lr=8e-5, critic_lr=8e-5, gamma=0.99, lambda_=0.95, \n",
    "                 imagination_horizon=15):\n",
    "        \n",
    "        self.world_model = world_model\n",
    "        self.device = device\n",
    "        self.gamma = gamma\n",
    "        self.lambda_ = lambda_\n",
    "        self.imagination_horizon = imagination_horizon\n",
    "        \n",
    "        # Actor and critic networks\n",
    "        self.actor = LatentActor(state_dim, action_dim).to(device)\n",
    "        self.critic = LatentCritic(state_dim).to(device)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        \n",
    "        # Training statistics\n",
    "        self.stats = {\n",
    "            'actor_loss': [],\n",
    "            'critic_loss': [],\n",
    "            'imagination_reward': [],\n",
    "            'policy_entropy': []\n",
    "        }\n",
    "    \n",
    "    def imagine_trajectories(self, initial_states, batch_size=50):\n",
    "        \"\"\"Generate imagined trajectories using world model\"\"\"\n",
    "        horizon = self.imagination_horizon\n",
    "        \n",
    "        # Storage for trajectory\n",
    "        states = [initial_states]\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        \n",
    "        current_state = initial_states\n",
    "        \n",
    "        for t in range(horizon):\n",
    "            # Sample action from current policy\n",
    "            action, log_prob = self.actor.sample(current_state)\n",
    "            value = self.critic(current_state)\n",
    "            \n",
    "            # Store\n",
    "            actions.append(action)\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            \n",
    "            # Predict next state and reward using world model\n",
    "            if hasattr(self.world_model, 'dynamics'):\n",
    "                # Simple world model\n",
    "                if self.world_model.dynamics.stochastic:\n",
    "                    next_state, _, _ = self.world_model.dynamics(current_state, action)\n",
    "                else:\n",
    "                    next_state = self.world_model.dynamics(current_state, action)\n",
    "                reward = self.world_model.reward_model(current_state, action)\n",
    "            else:\n",
    "                # RSSM world model\n",
    "                batch_size = current_state.shape[0]\n",
    "                h_dim = self.world_model.deter_dim\n",
    "                z_dim = self.world_model.stoch_dim\n",
    "                \n",
    "                # Split state into h and z components\n",
    "                h = current_state[:, :h_dim]\n",
    "                z = current_state[:, h_dim:h_dim+z_dim]\n",
    "                \n",
    "                # Imagination step\n",
    "                h, z, _ = self.world_model.imagine(h, z, action)\n",
    "                next_state = torch.cat([h, z], dim=-1)\n",
    "                reward = self.world_model.predict_reward(h, z)\n",
    "            \n",
    "            states.append(next_state)\n",
    "            rewards.append(reward)\n",
    "            current_state = next_state\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.stack(states[:-1])  # Exclude last state\n",
    "        actions = torch.stack(actions)\n",
    "        rewards = torch.stack(rewards)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        values = torch.stack(values)\n",
    "        \n",
    "        # Final value for bootstrapping\n",
    "        final_value = self.critic(states[-1])\n",
    "        \n",
    "        return {\n",
    "            'states': states,\n",
    "            'actions': actions,\n",
    "            'rewards': rewards,\n",
    "            'log_probs': log_probs,\n",
    "            'values': values,\n",
    "            'final_value': final_value\n",
    "        }\n",
    "    \n",
    "    def compute_returns_and_advantages(self, trajectory):\n",
    "        \"\"\"Compute returns and advantages using GAE\"\"\"\n",
    "        rewards = trajectory['rewards']\n",
    "        values = trajectory['values']\n",
    "        final_value = trajectory['final_value']\n",
    "        \n",
    "        # Compute returns\n",
    "        returns = torch.zeros_like(rewards)\n",
    "        advantages = torch.zeros_like(rewards)\n",
    "        \n",
    "        last_return = final_value\n",
    "        last_advantage = 0\n",
    "        \n",
    "        for t in reversed(range(len(rewards))):\n",
    "            returns[t] = rewards[t] + self.gamma * last_return\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * (final_value if t == len(rewards)-1 else values[t+1]) - values[t]\n",
    "            advantages[t] = delta + self.gamma * self.lambda_ * last_advantage\n",
    "            \n",
    "            last_return = returns[t]\n",
    "            last_advantage = advantages[t]\n",
    "        \n",
    "        return returns, advantages\n",
    "    \n",
    "    def update_actor_critic(self, trajectory):\n",
    "        \"\"\"Update actor and critic networks\"\"\"\n",
    "        states = trajectory['states']\n",
    "        actions = trajectory['actions']\n",
    "        log_probs = trajectory['log_probs']\n",
    "        \n",
    "        # Reshape for processing\n",
    "        states = states.view(-1, states.shape[-1])\n",
    "        actions = actions.view(-1, actions.shape[-1])\n",
    "        log_probs = log_probs.view(-1)\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns, advantages = self.compute_returns_and_advantages(trajectory)\n",
    "        returns = returns.view(-1)\n",
    "        advantages = advantages.view(-1)\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # Update critic\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        values_pred = self.critic(states)\n",
    "        critic_loss = F.mse_loss(values_pred, returns)\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 10.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update actor\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        \n",
    "        # Recompute log probs for current policy\n",
    "        action_mean, action_std = self.actor(states)\n",
    "        dist = Normal(action_mean, action_std)\n",
    "        \n",
    "        # Handle tanh transformation\n",
    "        raw_actions = torch.atanh(torch.clamp(actions / self.actor.action_range, -0.999, 0.999))\n",
    "        new_log_probs = dist.log_prob(raw_actions).sum(dim=-1)\n",
    "        new_log_probs -= (2 * (np.log(2) - raw_actions - F.softplus(-2 * raw_actions))).sum(dim=-1)\n",
    "        \n",
    "        # Actor loss (policy gradient with advantages)\n",
    "        actor_loss = -(new_log_probs * advantages.detach()).mean()\n",
    "        \n",
    "        # Add entropy regularization\n",
    "        entropy = dist.entropy().sum(dim=-1).mean()\n",
    "        actor_loss -= 0.001 * entropy\n",
    "        \n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 10.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Record statistics\n",
    "        self.stats['actor_loss'].append(actor_loss.item())\n",
    "        self.stats['critic_loss'].append(critic_loss.item())\n",
    "        self.stats['imagination_reward'].append(trajectory['rewards'].mean().item())\n",
    "        self.stats['policy_entropy'].append(entropy.item())\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'entropy': entropy.item(),\n",
    "            'mean_advantage': advantages.mean().item()\n",
    "        }\n",
    "    \n",
    "    def train_step(self, initial_states):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        # Generate imagined trajectories\n",
    "        trajectory = self.imagine_trajectories(initial_states)\n",
    "        \n",
    "        # Update networks\n",
    "        losses = self.update_actor_critic(trajectory)\n",
    "        \n",
    "        return losses\n",
    "\n",
    "# Complete Dreamer-Style Implementation\n",
    "class SimpleDreamerWorldModel(nn.Module):\n",
    "    \"\"\"Simplified world model for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Dynamics model\n",
    "        self.dynamics = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, state_dim)\n",
    "        )\n",
    "        \n",
    "        # Reward model\n",
    "        self.reward_model = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action):\n",
    "        input_tensor = torch.cat([state, action], dim=-1)\n",
    "        next_state = self.dynamics(input_tensor)\n",
    "        reward = self.reward_model(input_tensor).squeeze(-1)\n",
    "        return next_state, reward\n",
    "\n",
    "# Comprehensive Demonstration\n",
    "print(\"üéØ Planning in Latent Space - Complete Dreamer Implementation\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\\\n1. Creating simplified continuous control environment...\")\n",
    "\n",
    "class ContinuousPendulum:\n",
    "    \\\"\\\"\\\"Continuous pendulum environment\\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.state_dim = 3  # [cos(theta), sin(theta), theta_dot]\n",
    "        self.action_dim = 1  # torque\n",
    "        self.max_torque = 2.0\n",
    "        self.max_speed = 8.0\n",
    "        self.dt = 0.05\n",
    "        self.g = 10.0\n",
    "        self.m = 1.0\n",
    "        self.l = 1.0\n",
    "        \n",
    "        self.state = None\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 200\n",
    "    \n",
    "    def reset(self):\n",
    "        theta = np.random.uniform(-np.pi, np.pi)\n",
    "        theta_dot = np.random.uniform(-1, 1)\n",
    "        self.state = np.array([np.cos(theta), np.sin(theta), theta_dot])\n",
    "        self.step_count = 0\n",
    "        return self.state.copy()\n",
    "    \n",
    "    def step(self, action):\n",
    "        if isinstance(action, torch.Tensor):\n",
    "            action = action.cpu().numpy()\n",
    "        if action.ndim > 0:\n",
    "            action = action.item()\n",
    "        \n",
    "        action = np.clip(action, -self.max_torque, self.max_torque)\n",
    "        \n",
    "        cos_theta, sin_theta, theta_dot = self.state\n",
    "        theta = np.arctan2(sin_theta, cos_theta)\n",
    "        \n",
    "        # Dynamics\n",
    "        theta_dot_dot = (3 * self.g / (2 * self.l) * np.sin(theta) + 3 / (self.m * self.l**2) * action)\n",
    "        theta_dot = theta_dot + theta_dot_dot * self.dt\n",
    "        theta_dot = np.clip(theta_dot, -self.max_speed, self.max_speed)\n",
    "        theta = theta + theta_dot * self.dt\n",
    "        \n",
    "        # Normalize angle\n",
    "        theta = ((theta + np.pi) % (2 * np.pi)) - np.pi\n",
    "        \n",
    "        self.state = np.array([np.cos(theta), np.sin(theta), theta_dot])\n",
    "        \n",
    "        # Reward: keep pendulum upright\n",
    "        reward = -(theta**2 + 0.1 * theta_dot**2 + 0.001 * action**2)\n",
    "        \n",
    "        self.step_count += 1\n",
    "        done = self.step_count >= self.max_steps\n",
    "        \n",
    "        return self.state.copy(), reward, done\n",
    "\n",
    "# Create environment\n",
    "env = ContinuousPendulum()\n",
    "\n",
    "print(\"\\\\n2. Collecting data and training world model...\")\n",
    "\n",
    "def collect_world_model_data(env, n_episodes=100):\n",
    "    \\\"\\\"\\\"Collect data for world model training\\\"\\\"\\\"\n",
    "    data = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        for step in range(200):\n",
    "            action = np.random.uniform(-env.max_torque, env.max_torque)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            \n",
    "            data.append({\n",
    "                'state': state,\n",
    "                'action': [action],\n",
    "                'reward': reward,\n",
    "                'next_state': next_state\n",
    "            })\n",
    "            \n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Collect data\n",
    "world_data = collect_world_model_data(env, n_episodes=50)\n",
    "print(f\"Collected {len(world_data)} transitions\")\n",
    "\n",
    "# Create and train world model\n",
    "world_model = SimpleDreamerWorldModel(env.state_dim, env.action_dim, hidden_dim=128).to(device)\n",
    "world_optimizer = optim.Adam(world_model.parameters(), lr=1e-3)\n",
    "\n",
    "# Training world model\n",
    "n_world_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "print(\"Training world model...\")\n",
    "for epoch in range(n_world_epochs):\n",
    "    # Sample batch\n",
    "    batch_data = random.sample(world_data, min(batch_size, len(world_data)))\n",
    "    \n",
    "    states = torch.FloatTensor([d['state'] for d in batch_data]).to(device)\n",
    "    actions = torch.FloatTensor([d['action'] for d in batch_data]).to(device) \n",
    "    rewards = torch.FloatTensor([d['reward'] for d in batch_data]).to(device)\n",
    "    next_states = torch.FloatTensor([d['next_state'] for d in batch_data]).to(device)\n",
    "    \n",
    "    # Forward pass\n",
    "    pred_next_states, pred_rewards = world_model(states, actions)\n",
    "    \n",
    "    # Losses\n",
    "    dynamics_loss = F.mse_loss(pred_next_states, next_states)\n",
    "    reward_loss = F.mse_loss(pred_rewards, rewards)\n",
    "    total_loss = dynamics_loss + reward_loss\n",
    "    \n",
    "    # Backward pass\n",
    "    world_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    world_optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Dynamics Loss = {dynamics_loss.item():.6f}, \"\n",
    "              f\"Reward Loss = {reward_loss.item():.6f}\")\n",
    "\n",
    "print(\"\\\\n3. Training Dreamer agent...\")\n",
    "\n",
    "# Create Dreamer agent\n",
    "dreamer_agent = DreamerAgent(\n",
    "    world_model=world_model,\n",
    "    state_dim=env.state_dim,\n",
    "    action_dim=env.action_dim,\n",
    "    device=device,\n",
    "    imagination_horizon=10\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "n_training_episodes = 100\n",
    "training_rewards = []\n",
    "\n",
    "for episode in range(n_training_episodes):\n",
    "    # Real environment interaction (minimal)\n",
    "    real_state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    # Collect some real experience\n",
    "    real_states = []\n",
    "    for _ in range(5):  # Only 5 real steps\n",
    "        real_states.append(real_state)\n",
    "        action = dreamer_agent.actor.get_action(\n",
    "            torch.FloatTensor(real_state).unsqueeze(0).to(device), \n",
    "            deterministic=False\n",
    "        ).cpu().numpy()[0]\n",
    "        \n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode_reward += reward\n",
    "        real_state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    training_rewards.append(episode_reward)\n",
    "    \n",
    "    # Train on imagined trajectories\n",
    "    if len(real_states) > 0:\n",
    "        initial_states = torch.FloatTensor(real_states).to(device)\n",
    "        \n",
    "        # Multiple training steps on imagined data\n",
    "        for _ in range(10):\n",
    "            losses = dreamer_agent.train_step(initial_states)\n",
    "    \n",
    "    if (episode + 1) % 20 == 0:\n",
    "        recent_reward = np.mean(training_rewards[-10:])\n",
    "        print(f\"Episode {episode+1}: Recent Reward = {recent_reward:.2f}\")\n",
    "\n",
    "print(\"\\\\n4. Evaluating Dreamer agent...\")\n",
    "\n",
    "# Evaluation\n",
    "eval_rewards = []\n",
    "dreamer_agent.actor.eval()\n",
    "\n",
    "for eval_episode in range(20):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(200):\n",
    "        with torch.no_grad():\n",
    "            action = dreamer_agent.actor.get_action(\n",
    "                torch.FloatTensor(state).unsqueeze(0).to(device),\n",
    "                deterministic=True\n",
    "            ).cpu().numpy()[0]\n",
    "        \n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    eval_rewards.append(episode_reward)\n",
    "\n",
    "dreamer_agent.actor.train()\n",
    "\n",
    "# Comparison with random policy\n",
    "random_rewards = []\n",
    "for _ in range(20):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    for step in range(200):\n",
    "        action = np.random.uniform(-env.max_torque, env.max_torque)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        episode_reward += reward\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    random_rewards.append(episode_reward)\n",
    "\n",
    "# Comprehensive Results Visualization\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Training progress\n",
    "plt.subplot(3, 4, 1)\n",
    "plt.plot(training_rewards, alpha=0.7, linewidth=1)\n",
    "smooth_rewards = pd.Series(training_rewards).rolling(window=10).mean()\n",
    "plt.plot(smooth_rewards, linewidth=2, label='Smooth')\n",
    "plt.title('Dreamer Training Progress')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Actor and critic losses\n",
    "plt.subplot(3, 4, 2)\n",
    "if dreamer_agent.stats['actor_loss']:\n",
    "    plt.plot(dreamer_agent.stats['actor_loss'], label='Actor Loss', linewidth=2)\n",
    "    plt.plot(dreamer_agent.stats['critic_loss'], label='Critic Loss', linewidth=2)\n",
    "    plt.title('Dreamer Learning Losses')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Imagination rewards\n",
    "plt.subplot(3, 4, 3)\n",
    "if dreamer_agent.stats['imagination_reward']:\n",
    "    plt.plot(dreamer_agent.stats['imagination_reward'], color='purple', linewidth=2)\n",
    "    plt.title('Imagined Episode Rewards')\n",
    "    plt.xlabel('Training Step') \n",
    "    plt.ylabel('Mean Imagined Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Performance comparison\n",
    "plt.subplot(3, 4, 4)\n",
    "methods = ['Dreamer', 'Random']\n",
    "mean_rewards = [np.mean(eval_rewards), np.mean(random_rewards)]\n",
    "std_rewards = [np.std(eval_rewards), np.std(random_rewards)]\n",
    "\n",
    "bars = plt.bar(methods, mean_rewards, yerr=std_rewards, capsize=5, alpha=0.7,\n",
    "               color=['skyblue', 'orange'])\n",
    "plt.title('Performance Comparison')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for bar, mean_val, std_val in zip(bars, mean_rewards, std_rewards):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std_val,\n",
    "             f'{mean_val:.1f}¬±{std_val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "# Reward distribution\n",
    "plt.subplot(3, 4, 5)\n",
    "plt.boxplot([eval_rewards, random_rewards], labels=['Dreamer', 'Random'])\n",
    "plt.title('Reward Distribution')\n",
    "plt.ylabel('Episode Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Policy entropy evolution\n",
    "plt.subplot(3, 4, 6)\n",
    "if dreamer_agent.stats['policy_entropy']:\n",
    "    plt.plot(dreamer_agent.stats['policy_entropy'], color='green', linewidth=2)\n",
    "    plt.title('Policy Entropy')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Entropy')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# World model accuracy\n",
    "plt.subplot(3, 4, 7)\n",
    "# Test world model predictions\n",
    "test_states = torch.FloatTensor([d['state'] for d in world_data[:100]]).to(device)\n",
    "test_actions = torch.FloatTensor([d['action'] for d in world_data[:100]]).to(device)\n",
    "test_rewards = torch.FloatTensor([d['reward'] for d in world_data[:100]]).to(device)\n",
    "test_next_states = torch.FloatTensor([d['next_state'] for d in world_data[:100]]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred_next_states, pred_rewards = world_model(test_states, test_actions)\n",
    "    \n",
    "    dynamics_error = F.mse_loss(pred_next_states, test_next_states).item()\n",
    "    reward_error = F.mse_loss(pred_rewards, test_rewards).item()\n",
    "\n",
    "errors = [dynamics_error, reward_error]\n",
    "labels = ['Dynamics', 'Reward']\n",
    "bars = plt.bar(labels, errors, alpha=0.7, color=['red', 'green'])\n",
    "plt.title('World Model Prediction Errors')\n",
    "plt.ylabel('MSE')\n",
    "plt.yscale('log')\n",
    "\n",
    "for bar, error in zip(bars, errors):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{error:.2e}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Imagination trajectory example\n",
    "plt.subplot(3, 4, 8)\n",
    "# Generate sample imagination trajectory\n",
    "with torch.no_grad():\n",
    "    sample_state = torch.FloatTensor([[1.0, 0.0, 0.0]]).to(device)  # Upright position\n",
    "    imagination = dreamer_agent.imagine_trajectories(sample_state, batch_size=1)\n",
    "    \n",
    "    rewards_traj = imagination['rewards'][:, 0].cpu().numpy()\n",
    "    \n",
    "plt.plot(rewards_traj, 'o-', linewidth=2, markersize=6)\n",
    "plt.title('Sample Imagined Trajectory')\n",
    "plt.xlabel('Imagination Step')\n",
    "plt.ylabel('Predicted Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Training statistics summary\n",
    "plt.subplot(3, 4, 9)\n",
    "stats_names = ['Training Episodes', 'Real Steps per Episode', 'Imagination Horizon', \n",
    "               'World Model Parameters', 'Actor Parameters', 'Critic Parameters']\n",
    "stats_values = [\n",
    "    n_training_episodes,\n",
    "    5,  # Real steps per episode\n",
    "    dreamer_agent.imagination_horizon,\n",
    "    sum(p.numel() for p in world_model.parameters()),\n",
    "    sum(p.numel() for p in dreamer_agent.actor.parameters()),\n",
    "    sum(p.numel() for p in dreamer_agent.critic.parameters())\n",
    "]\n",
    "\n",
    "y_pos = np.arange(len(stats_names))\n",
    "plt.barh(y_pos, stats_values, alpha=0.7)\n",
    "plt.yticks(y_pos, stats_names)\n",
    "plt.xlabel('Value')\n",
    "plt.title('Training Configuration')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Action distribution analysis\n",
    "plt.subplot(3, 4, 10)\n",
    "# Sample actions from trained policy\n",
    "sample_states = torch.FloatTensor([[np.cos(theta), np.sin(theta), 0.0] \n",
    "                                  for theta in np.linspace(-np.pi, np.pi, 100)]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    actions = dreamer_agent.actor.get_action(sample_states, deterministic=True).cpu().numpy()\n",
    "\n",
    "angles = np.linspace(-np.pi, np.pi, 100)\n",
    "plt.plot(angles, actions, linewidth=2)\n",
    "plt.title('Learned Policy Actions')\n",
    "plt.xlabel('Pendulum Angle (radians)')\n",
    "plt.ylabel('Action (Torque)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Sample efficiency comparison\n",
    "plt.subplot(3, 4, 11)\n",
    "real_samples = n_training_episodes * 5  # 5 real steps per episode\n",
    "imagined_samples = n_training_episodes * 10 * dreamer_agent.imagination_horizon  # 10 imagination steps per episode\n",
    "\n",
    "sample_data = [real_samples, imagined_samples]\n",
    "sample_labels = ['Real Samples', 'Imagined Samples']\n",
    "colors = ['red', 'blue']\n",
    "\n",
    "bars = plt.bar(sample_labels, sample_data, color=colors, alpha=0.7)\n",
    "plt.title('Sample Usage')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.yscale('log')\n",
    "\n",
    "for bar, samples in zip(bars, sample_data):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{samples:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Final performance metrics\n",
    "plt.subplot(3, 4, 12)\n",
    "improvement = (np.mean(eval_rewards) - np.mean(random_rewards)) / abs(np.mean(random_rewards)) * 100\n",
    "sample_efficiency = imagined_samples / real_samples\n",
    "\n",
    "metrics = ['Performance\\\\nImprovement (%)', 'Sample\\\\nEfficiency Ratio', \n",
    "           'Imagination\\\\nHorizon', 'Training\\\\nStability']\n",
    "values = [improvement, sample_efficiency, dreamer_agent.imagination_horizon,\n",
    "          1.0 - (np.std(eval_rewards) / abs(np.mean(eval_rewards)))]\n",
    "\n",
    "bars = plt.bar(metrics, values, alpha=0.7, color=['green', 'blue', 'orange', 'purple'])\n",
    "plt.title('Key Metrics')\n",
    "plt.ylabel('Value')\n",
    "\n",
    "for bar, val in zip(bars, values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height(),\n",
    "             f'{val:.1f}', ha='center', va='bottom')\n",
    "\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Final comprehensive analysis\n",
    "print(f\"\\\\nüéØ COMPREHENSIVE DREAMER ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\\\nüìä Performance Metrics:\")\n",
    "print(f\"  Dreamer Agent: {np.mean(eval_rewards):.2f} ¬± {np.std(eval_rewards):.2f}\")\n",
    "print(f\"  Random Policy: {np.mean(random_rewards):.2f} ¬± {np.std(random_rewards):.2f}\")\n",
    "print(f\"  Performance Improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(f\"\\\\nüî¨ Sample Efficiency:\")\n",
    "print(f\"  Real Environment Steps: {real_samples:,}\")\n",
    "print(f\"  Imagined Steps: {imagined_samples:,}\")\n",
    "print(f\"  Sample Efficiency Ratio: {sample_efficiency:.1f}x\")\n",
    "\n",
    "print(f\"\\\\nüß† Model Complexity:\")\n",
    "print(f\"  World Model Parameters: {sum(p.numel() for p in world_model.parameters()):,}\")\n",
    "print(f\"  Actor Parameters: {sum(p.numel() for p in dreamer_agent.actor.parameters()):,}\")\n",
    "print(f\"  Critic Parameters: {sum(p.numel() for p in dreamer_agent.critic.parameters()):,}\")\n",
    "\n",
    "print(f\"\\\\nüéÆ World Model Accuracy:\")\n",
    "print(f\"  Dynamics Prediction Error: {dynamics_error:.2e}\")\n",
    "print(f\"  Reward Prediction Error: {reward_error:.2e}\")\n",
    "\n",
    "print(f\"\\\\n‚úÖ ADVANCED MODEL-BASED RL DEMONSTRATION COMPLETE!\")\n",
    "print(f\"üåü Key Achievements:\")\n",
    "print(f\"   ‚Ä¢ Implemented complete world model with VAE and RSSM\")\n",
    "print(f\"   ‚Ä¢ Demonstrated planning in latent space with Actor-Critic\")\n",
    "print(f\"   ‚Ä¢ Achieved significant sample efficiency improvements\")\n",
    "print(f\"   ‚Ä¢ Showed imagination-based learning capabilities\")\n",
    "print(f\"   ‚Ä¢ Comprehensive analysis of model-based vs model-free trade-offs\")\n",
    "\n",
    "print(f\"\\\\nüöÄ FUTURE DIRECTIONS:\")\n",
    "print(f\"   ‚Ä¢ Hierarchical world models for complex environments\")\n",
    "print(f\"   ‚Ä¢ Meta-learning for quick adaptation to new domains\") \n",
    "print(f\"   ‚Ä¢ Uncertainty-aware planning with ensemble methods\")\n",
    "print(f\"   ‚Ä¢ Integration with large-scale neural network architectures\")\n",
    "print(f\"   ‚Ä¢ Real-world applications in robotics and control\")\n",
    "\n",
    "print(f\"\\\\nüéì CONGRATULATIONS!\")\n",
    "print(f\"You have mastered advanced model-based reinforcement learning!\")\n",
    "print(f\"This comprehensive implementation covers state-of-the-art methods\")\n",
    "print(f\"used in modern RL research and applications.\")\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
