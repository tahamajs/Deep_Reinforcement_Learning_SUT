# Ca10: Model-based Reinforcement Learning and Planning Methods
# Table of Contents

- [CA10: Model-Based Reinforcement Learning and Planning Methods](#ca10-model-based-reinforcement-learning-and-planning-methods)
- [Deep Reinforcement Learning - Session 10](#deep-reinforcement-learning---session-10)
- [Course Information](#course-information)
- [Learning Objectives](#learning-objectives)
- [Prerequisites](#prerequisites)
- [Roadmap](#roadmap)
- [Project Structure](#project-structure)
- [Contents Overview](#contents-overview)
- [Table of Contents](#table-of-contents)
- [Essential Imports and Environment Setup](#essential-imports-and-environment-setup)
- [Set random seeds for reproducibility](#set-random-seeds-for-reproducibility)
- [Device configuration](#device-configuration)
- [Plotting configuration](#plotting-configuration)
- [Import local CA10 modules (use .py implementations)](#import-local-ca10-modules-use-py-implementations)
- [Ensure repo root / notebook dir is on path](#ensure-repo-root--notebook-dir-is-on-path)
- [Reload to pick up local edits](#reload-to-pick-up-local-edits)
- [Expose key classes/functions into notebook namespace for convenience](#expose-key-classesfunctions-into-notebook-namespace-for-convenience)
- [Keep device in sync (notebook-level variable)](#keep-device-in-sync-notebook-level-variable)
- [Section 1: Theoretical Foundations of Model-Based Reinforcement Learning](#section-1-theoretical-foundations-of-model-based-reinforcement-learning)
- [1.1 From Model-Free to Model-Based Learning](#11-from-model-free-to-model-based-learning)
- [Model-Free vs Model-Based Comparison](#model-free-vs-model-based-comparison)
- [1.2 The Model-Based RL Framework](#12-the-model-based-rl-framework)
- [1.3 Advantages of Model-Based Methods](#13-advantages-of-model-based-methods)
- [1.4 Challenges in Model-Based RL](#14-challenges-in-model-based-rl)
- [Theoretical Foundation Visualization](#theoretical-foundation-visualization)
- [Import visualization functions from modular files](#import-visualization-functions-from-modular-files)
- [Run the demonstration](#run-the-demonstration)
- [Section 2: Environment Models and Model Learning](#section-2-environment-models-and-model-learning)
- [2.1 Types of Environment Models](#21-types-of-environment-models)
- [By Representation Type:](#by-representation-type)
- [By Uncertainty Representation:](#by-uncertainty-representation)
- [2.2 Model Learning Approaches](#22-model-learning-approaches)
- [Maximum Likelihood Estimation (MLE)](#maximum-likelihood-estimation-mle)
- [Neural Network Models](#neural-network-models)
- [Training Objectives](#training-objectives)
- [2.3 Model Validation and Selection](#23-model-validation-and-selection)
- [Validation Strategies](#validation-strategies)
- [Model Selection Criteria](#model-selection-criteria)
- [Environment Models Implementation](#environment-models-implementation)
- [Import from modular files](#import-from-modular-files)
- [Demonstrate model learning](#demonstrate-model-learning)
- [Create environment and collect data](#create-environment-and-collect-data)
- [Collect experience](#collect-experience)
- [Prepare data for neural model](#prepare-data-for-neural-model)
- [Convert states to one-hot for neural model](#convert-states-to-one-hot-for-neural-model)
- [Train neural model](#train-neural-model)
- [Test model accuracy](#test-model-accuracy)
- [Visualize training progress](#visualize-training-progress)
- [Section 3: Classical Planning with Learned Models](#section-3-classical-planning-with-learned-models)
- [3.1 Dynamic Programming with Learned Models](#31-dynamic-programming-with-learned-models)
- [Value Iteration with Learned Models](#value-iteration-with-learned-models)
- [Policy Iteration with Learned Models](#policy-iteration-with-learned-models)
- [3.2 Handling Model Uncertainty](#32-handling-model-uncertainty)
- [Pessimistic Planning](#pessimistic-planning)
- [Optimistic Planning  ](#optimistic-planning--)
- [Robust Planning](#robust-planning)
- [3.3 Model-Based Policy Search](#33-model-based-policy-search)
- [Gradient-Based Policy Search](#gradient-based-policy-search)
- [Evolutionary Policy Search](#evolutionary-policy-search)
- [Random Shooting](#random-shooting)
- [Classical Planning Implementation](#classical-planning-implementation)
- [Section 5: Monte Carlo Tree Search (MCTS)](#section-5-monte-carlo-tree-search-mcts)
- [5.1 Theoretical Foundation](#51-theoretical-foundation)
- [Key Components:](#key-components)
- [UCB1 Formula for Node Selection:](#ucb1-formula-for-node-selection)
- [MCTS in Model-Based RL:](#mcts-in-model-based-rl)
- [Section 6: Model Predictive Control (MPC)](#section-6-model-predictive-control-mpc)
- [6.1 Theoretical Foundation](#61-theoretical-foundation)
- [Key Components:](#key-components)
- [Advantages:](#advantages)
- [MPC in RL Context:](#mpc-in-rl-context)
- [Section 7: Advanced Model-Based Methods and Modern Approaches](#section-7-advanced-model-based-methods-and-modern-approaches)
- [7.1 Modern Neural Model-Based Methods](#71-modern-neural-model-based-methods)
- [Model-Based Meta-Learning](#model-based-meta-learning)
- [Uncertainty-Aware Models](#uncertainty-aware-models)
- [Advanced Planning Methods](#advanced-planning-methods)
- [7.2 State-of-the-Art Methods](#72-state-of-the-art-methods)
- [Model-Based Policy Optimization (MBPO)](#model-based-policy-optimization-mbpo)
- [Dreamer and DreamerV2](#dreamer-and-dreamerv2)
- [MuZero](#muzero)
- [Comprehensive Model-Based RL Comparison and Analysis](#comprehensive-model-based-rl-comparison-and-analysis)
- [Create visualizations directory if it doesn't exist](#create-visualizations-directory-if-it-doesnt-exist)
- [Run individual demonstrations](#run-individual-demonstrations)


## Deep Reinforcement Learning - Session 10

### Course Information
- **Course**: Deep Reinforcement Learning
- **Session**: 10
- **Topic**: Model-Based Reinforcement Learning and Planning Methods
- **Focus**: Learning environment models and using them for planning and control

### Learning Objectives

By the end of this notebook, you will understand:

1. **Model-Based RL Foundations**:
- Differences between model-free and model-based approaches
- Advantages and challenges of model-based methods
- Model learning from experience and uncertainty quantification
- Sample efficiency and planning capabilities

2. **Environment Modeling**:
- Tabular vs neural network models for dynamics
- Maximum likelihood estimation for model learning
- Ensemble methods for uncertainty quantification
- Model validation and selection criteria

3. **Classical Planning Algorithms**:
- Value iteration and policy iteration with learned models
- Handling model uncertainty in planning
- Pessimistic and optimistic planning approaches
- Model-based policy search methods

4. **Integrated Planning and Learning**:
- Dyna-Q algorithm and its variants
- Balancing real experience and planning
- Prioritized sweeping and focused planning
- Performance analysis and convergence properties

5. **Advanced Planning Methods**:
- Monte Carlo Tree Search (MCTS) fundamentals
- Upper Confidence Bound (UCB) exploration
- MCTS applications in RL and games
- AlphaGo and modern MCTS variants

6. **Model Predictive Control**:
- Receding horizon control principles
- MPC formulation and optimization
- Continuous control with MPC
- Integration with learned models

7. **Modern Neural Approaches**:
- World models and latent state learning
- Differentiable planning methods
- Model-based meta-learning
- State-of-the-art methods (MBPO, Dreamer, MuZero)

### Prerequisites

Before starting this notebook, ensure you have:

- **Mathematical Background**:
- Markov Decision Processes (MDPs) and Bellman equations
- Dynamic programming (value/policy iteration)
- Probability theory and expectation
- Optimization and gradient methods

- **Programming Skills**:
- Advanced Python programming and debugging
- PyTorch proficiency (neural networks, optimization)
- NumPy for numerical computations
- Matplotlib/Seaborn for advanced visualization

- **Reinforcement Learning Fundamentals**:
- Q-learning and SARSA algorithms
- Policy gradient methods
- Experience replay and stability techniques
- Value function approximation

- **Previous Course Knowledge**:
- CA1-CA3: Basic RL concepts and tabular methods
- CA4-CA7: Deep RL and value-based methods
- CA8-CA9: Advanced policy methods and multi-modal learning
- Strong foundation in PyTorch and neural network implementation

### Roadmap

This notebook follows a structured progression from model learning to advanced planning:

1. **Section 1: Theoretical Foundations** (45 min)
- Model-free vs model-based RL comparison
- Advantages and challenges of model-based approaches
- Sample efficiency and planning capabilities
- Mathematical framework and key concepts

2. **Section 2: Environment Models** (60 min)
- Tabular and neural network model architectures
- Model learning from experience data
- Uncertainty quantification with ensembles
- Model validation and selection strategies

3. **Section 3: Classical Planning** (60 min)
- Value iteration with learned models
- Policy iteration and evaluation
- Handling model uncertainty (pessimistic/optimistic)
- Model-based policy search methods

4. **Section 4: Dyna-Q Algorithm** (45 min)
- Integrating planning and learning
- Dyna-Q implementation and variants
- Prioritized sweeping and focused planning
- Performance analysis and convergence

5. **Section 5: Monte Carlo Tree Search** (60 min)
- MCTS fundamentals and UCB exploration
- Selection, expansion, simulation, backpropagation
- MCTS in RL and game applications
- AlphaGo and modern variants

6. **Section 6: Model Predictive Control** (45 min)
- Receding horizon control principles
- MPC formulation and optimization
- Continuous control applications
- Integration with learned dynamics

7. **Section 7: Advanced Methods** (45 min)
- Modern neural model-based approaches
- World models and latent planning
- State-of-the-art methods overview
- Current research directions

8. **Section 8: Comparative Analysis** (60 min)
- Model-based vs model-free comparison
- Performance benchmarking across environments
- Ablation studies and sensitivity analysis
- Practical recommendations and best practices

### Project Structure

This notebook uses a modular implementation organized as follows:

```
CA10/
‚îú‚îÄ‚îÄ models/                    # Environment model implementations
‚îÇ   ‚îú‚îÄ‚îÄ tabular_model.py      # Tabular transition/reward models
‚îÇ   ‚îú‚îÄ‚îÄ neural_model.py       # Neural network dynamics models
‚îÇ   ‚îú‚îÄ‚îÄ ensemble_model.py     # Ensemble methods for uncertainty
‚îÇ   ‚îî‚îÄ‚îÄ model_trainer.py      # Model training utilities
‚îú‚îÄ‚îÄ planning/                 # Classical planning algorithms
‚îÇ   ‚îú‚îÄ‚îÄ value_iteration.py    # Value iteration with models
‚îÇ   ‚îú‚îÄ‚îÄ policy_iteration.py   # Policy iteration with models
‚îÇ   ‚îú‚îÄ‚îÄ uncertainty_planning.py # Robust planning methods
‚îÇ   ‚îî‚îÄ‚îÄ policy_search.py      # Model-based policy search
‚îú‚îÄ‚îÄ dyna_q/                   # Dyna-Q implementations
‚îÇ   ‚îú‚îÄ‚îÄ dyna_q.py            # Basic Dyna-Q algorithm
‚îÇ   ‚îú‚îÄ‚îÄ dyna*q*plus.py       # Dyna-Q+ with exploration bonuses
‚îÇ   ‚îú‚îÄ‚îÄ prioritized_sweeping.py # Prioritized sweeping
‚îÇ   ‚îî‚îÄ‚îÄ utils.py              # Dyna-Q utilities
‚îú‚îÄ‚îÄ mcts/                     # Monte Carlo Tree Search
‚îÇ   ‚îú‚îÄ‚îÄ mcts.py              # Core MCTS implementation
‚îÇ   ‚îú‚îÄ‚îÄ uct.py               # Upper Confidence Bound
‚îÇ   ‚îú‚îÄ‚îÄ mcts_rl.py           # MCTS for RL applications
‚îÇ   ‚îî‚îÄ‚îÄ utils.py              # MCTS utilities
‚îú‚îÄ‚îÄ mpc/                      # Model Predictive Control
‚îÇ   ‚îú‚îÄ‚îÄ mpc_controller.py    # MPC implementation
‚îÇ   ‚îú‚îÄ‚îÄ continuous_mpc.py    # Continuous control MPC
‚îÇ   ‚îú‚îÄ‚îÄ optimization.py      # MPC optimization methods
‚îÇ   ‚îî‚îÄ‚îÄ utils.py              # MPC utilities
‚îú‚îÄ‚îÄ environments/             # Custom environments
‚îÇ   ‚îú‚îÄ‚îÄ grid_world.py        # Simple grid world
‚îÇ   ‚îú‚îÄ‚îÄ blocking_maze.py     # Maze with changing blocks
‚îÇ   ‚îú‚îÄ‚îÄ continuous_control.py # Continuous control tasks
‚îÇ   ‚îî‚îÄ‚îÄ wrappers.py           # Environment wrappers
‚îú‚îÄ‚îÄ experiments/              # Experiment scripts
‚îÇ   ‚îú‚îÄ‚îÄ model_comparison.py  # Model accuracy experiments
‚îÇ   ‚îú‚îÄ‚îÄ planning_comparison.py # Planning method comparison
‚îÇ   ‚îú‚îÄ‚îÄ dyna_experiments.py  # Dyna-Q experiments
‚îÇ   ‚îú‚îÄ‚îÄ mcts_experiments.py  # MCTS experiments
‚îÇ   ‚îú‚îÄ‚îÄ mpc_experiments.py   # MPC experiments
‚îÇ   ‚îî‚îÄ‚îÄ comprehensive_analysis.py # Full comparative analysis
‚îú‚îÄ‚îÄ utils/                    # General utilities
‚îÇ   ‚îú‚îÄ‚îÄ visualization.py     # Plotting and analysis tools
‚îÇ   ‚îú‚îÄ‚îÄ analysis.py          # Performance analysis utilities
‚îÇ   ‚îú‚îÄ‚îÄ data_collection.py   # Experience collection tools
‚îÇ   ‚îî‚îÄ‚îÄ evaluation.py        # Evaluation metrics
‚îú‚îÄ‚îÄ requirements.txt          # Python dependencies
‚îî‚îÄ‚îÄ CA10.ipynb               # This educational notebook
```

### Contents Overview

1. **Section 1**: Theoretical Foundations of Model-Based Reinforcement Learning
2. **Section 2**: Environment Models and Model Learning
3. **Section 3**: Classical Planning (with Learned Models)
4. **Section 4**: Dyna-Q Algorithm
5. **Section 5**: Monte Carlo Tree Search (MCTS)
6. **Section 6**: Model Predictive Control (MPC)
7. **Section 7**: Advanced Model-Based Methods & Modern Approaches
8. **Section 8**: Comparative Analysis & Demonstrations

<!-- vscode-jupyter-toc-config {"maxdepth":2, "orderedList": false} -->

<a id="toc-start"></a>

## Table of Contents

- [Section 1: Theoretical Foundations](#section-1-theoretical-foundations)
- [Section 2: Environment Models](#section-2-environment-models)
- [Section 3: Classical Planning (with Learned Models)](#section-3-classical-planning-with-learned-models)
- [Section 4: Dyna-Q Algorithm](#section-4-dyna-q)
- [Section 5: Monte Carlo Tree Search (MCTS)](#section-5-monte-carlo-tree-search-mcts)
- [Section 6: Model Predictive Control (MPC)](#section-6-model-predictive-control-mpc)
- [Section 7: Advanced Model-Based Methods & Modern Approaches](#section-7-advanced-model-based-methods-and-modern-approaches)
- [Section 8: Comparative Analysis & Demonstrations](#comprehensive-model-based-rl-comparison-and-analysis)

<!-- Explicit HTML anchors (helps VS Code/Jupyter TOC and direct linking) -->
<a id="section-1-theoretical-foundations"></a>
<a id="section-2-environment-models"></a>
<a id="section-3-classical-planning-with-learned-models"></a>
<a id="section-4-dyna-q"></a>
<a id="section-5-monte-carlo-tree-search-mcts"></a>
<a id="section-6-model-predictive-control-mpc"></a>
<a id="section-7-advanced-model-based-methods-and-modern-approaches"></a>
<a id="comprehensive-model-based-rl-comparison-and-analysis"></a>

<!-- End of TOC cell -->


```python
# Essential Imports and Environment Setup
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Categorical, Normal
import gymnasium as gym
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from collections import defaultdict, deque
import random
import pickle
from typing import Tuple, List, Dict, Optional, Union
import warnings
warnings.filterwarnings('ignore')

# Set Random Seeds for Reproducibility
np.random.seed(42)
torch.manual_seed(42)
random.seed(42)

# Device Configuration
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Plotting Configuration
plt.style.use('default')
plt.rcParams['figure.figsize'] = (12, 8)
plt.rcParams['font.size'] = 12

print("Environment setup complete!")
print(f"PyTorch version: {torch.**version**}")
print(f"Gymnasium version: {gym.**version**}")
print(f"NumPy version: {np.**version**}")

# Import Local Ca10 Modules (use .py Implementations)
import os, sys, importlib
# Ensure Repo Root / Notebook Dir Is on Path
sys.path.append(os.path.dirname(os.path.abspath('**file**')))

from agents.classical_planning import *
from experiments.comparison import *
from agents.dyna_q import *
from agents.mcts import *
from agents.mpc import *
from models.models import *
from environments.environments import *
# Reload to Pick Up Local Edits
importlib.reload(classical_planning)
importlib.reload(comparison)
importlib.reload(dyna_q)
importlib.reload(mcts)
importlib.reload(mpc)
importlib.reload(models)
importlib.reload(environments)

# Expose Key Classes/functions into Notebook Namespace for Convenience
from classical_planning import (
    ModelBasedPlanner,
    UncertaintyAwarePlanner,
    ModelBasedPolicySearch,
    demonstrate*classical*planning,
)
from dyna*q import DynaQAgent, DynaQPlusAgent, demonstrate*dyna_q
from mcts import MCTSAgent, demonstrate_mcts
from mpc import MPCAgent, MPCController, demonstrate_mpc
from models import TabularModel, NeuralModel, ModelTrainer, device as models_device
from environments import SimpleGridWorld, BlockingMaze
from comparison import demonstrate_comparison

# Keep Device in Sync (notebook-level Variable)
device = models_device

print("Imported CA10 modules from .py files")
```

    Using device: cpu
    Environment setup complete!
    PyTorch version: 2.0.1
    Gymnasium version: 1.2.1
    NumPy version: 1.26.4
    Using device: cpu
    Environment setup complete!
    PyTorch version: 2.0.1
    Gymnasium version: 1.2.1
    NumPy version: 1.26.4
    Imported CA10 modules from .py files
    Using device: cpu
    Environment setup complete!
    PyTorch version: 2.0.1
    Gymnasium version: 1.2.1
    NumPy version: 1.26.4
    Imported CA10 modules from .py files


# Section 1: Theoretical Foundations of Model-based Reinforcement Learning

## 1.1 from Model-free to Model-based Learning

In our journey through reinforcement learning, we have primarily focused on **model-free methods** such as Q-learning, SARSA, and policy gradient methods. These methods learn directly from experience without explicitly modeling the environment. However, there are fundamental advantages to learning and using environment models:

### Model-free Vs Model-based Comparison

| Aspect | Model-Free Methods | Model-Based Methods |
|--------|-------------------|--------------------|
| **Learning** | Learn value functions or policies directly | Learn environment model first |
| **Sample Efficiency** | Generally less sample efficient | Generally more sample efficient |
| **Computational Cost** | Lower per-step computation | Higher per-step computation |
| **Planning** | No explicit planning | Can plan with learned model |
| **Robustness** | More robust to model errors | Sensitive to model inaccuracies |
| **Interpretability** | Less interpretable | More interpretable (explicit model) |

## 1.2 the Model-based Rl Framework

The general model-based RL framework consists of three main components:

1. **Model Learning**: Learn a model of the environment from experience
   $$\hat{P}(s'|s,a) \approx P(s'|s,a)$$
   $$\hat{R}(s,a) \approx E[R|s,a]$$

2. **Planning**: Use the learned model to compute optimal policies
- Value iteration with learned model
- Policy iteration with learned model  
- Monte Carlo Tree Search
- Model Predictive Control

3. **Acting**: Execute actions in the real environment
- Collect new experience
- Update the model
- Replan with improved model

## 1.3 Advantages of Model-based Methods

**Sample Efficiency**: 
- Can generate synthetic experience using the learned model
- Each real experience can be used multiple times for planning
- Particularly important in expensive real-world applications

**Transfer Learning**:
- Models can transfer across different tasks in the same environment
- Learned dynamics are often more general than policies

**Interpretability and Safety**:
- Explicit models provide insight into system behavior
- Can simulate outcomes before taking actions
- Enable safety verification and constraint checking

**Planning Capabilities**:
- Can look ahead and plan optimal sequences of actions
- Adapt quickly to changes in rewards or goals
- Enable hierarchical and long-term planning

## 1.4 Challenges in Model-based Rl

**Model Bias and Compounding Errors**:
- Errors in the learned model can compound over time
- Model bias can lead to suboptimal policies
- Challenge: Learning accurate models in complex environments

**Computational Complexity**:
- Planning with models can be computationally expensive
- Trade-off between planning depth and computational cost

**Partial Observability**:
- Real environments often have hidden state
- Challenge: Learning models from partial observations

**Stochastic Environments**:
- Need to model uncertainty in transitions and rewards
- Balance between model complexity and accuracy


```python
# Theoretical Foundation Visualization

# Import Visualization Functions from Modular Files
from classical*planning import demonstrate*classical_planning

# Run the Demonstration
demonstrate*classical*planning()
```

    Model-Based RL Framework Visualizations
    ==================================================
    
    1. Framework Comparison:



    
![png](CA10*files/CA10*4_1.png)
    


    
    2. Sample Efficiency Analysis:



    
![png](CA10*files/CA10*4_3.png)
    


    
    3. Comprehensive Comparison Table:



    
![png](CA10*files/CA10*4_5.png)
    


    
    ‚úÖ Theoretical foundations established!
    üìä Next: Environment model learning and representation


# Section 2: Environment Models and Model Learning

## 2.1 Types of Environment Models

Environment models can be categorized along several dimensions:

### By Representation Type:

**Tabular Models**:
- Store explicit transition probabilities: $P(s'|s,a)$
- Store explicit rewards: $R(s,a)$
- Suitable for small, discrete state-action spaces
- Example: Storing counts and computing maximum likelihood estimates

**Function Approximation Models**:
- Use neural networks to approximate dynamics
- $s' = f_\theta(s,a) + \epsilon$ (deterministic + noise)
- $P(s'|s,a) = \pi_\theta(s'|s,a)$ (stochastic)
- Suitable for large, continuous state-action spaces

### By Uncertainty Representation:

**Deterministic Models**:
- Predict single next state: $s' = f(s,a)$
- Simple but ignores environment stochasticity
- Can add noise independently

**Stochastic Models**:
- Predict distribution over next states: $P(s'|s,a)$
- More accurate for stochastic environments
- Can be parametric (Gaussian) or non-parametric

**Ensemble Models**:
- Multiple models trained on different data subsets
- Uncertainty estimated from ensemble disagreement
- More robust and better uncertainty quantification

## 2.2 Model Learning Approaches

### Maximum Likelihood Estimation (mle)

For tabular environments, we can use simple counting:
$$\hat{P}(s'|s,a) = \frac{N(s,a,s')}{N(s,a)}$$
$$\hat{R}(s,a) = \frac{1}{N(s,a)} \sum*{i} R*i(s,a)$$

### Neural Network Models

For complex environments, use neural networks:
- **Forward Model**: $(s,a) \rightarrow (s', r)$
- **Inverse Model**: $(s,s') \rightarrow a$
- **Combined**: Learn both forward and inverse models jointly

### Training Objectives

**Deterministic Dynamics**:
$$L = \mathbb{E}*{(s,a,s',r) \sim D}[||s' - f*\theta(s,a)||^2 + ||r - g_\theta(s,a)||^2]$$

**Stochastic Dynamics**:
$$L = -\mathbb{E}*{(s,a,s',r) \sim D}[\log P*\theta(s'|s,a) + \log P_\theta(r|s,a)]$$

## 2.3 Model Validation and Selection

### Validation Strategies

**Hold-out Validation**:
- Split data into training and validation sets
- Evaluate model accuracy on unseen transitions
- Risk: May not reflect planning performance

**Cross-Validation**:
- Multiple train/validation splits
- More robust estimate of model quality
- Higher computational cost

**Policy-Aware Validation**:
- Evaluate model on states visited by current policy
- More relevant for planning performance
- Adapts as policy changes

### Model Selection Criteria

**Prediction Accuracy**:
- Mean squared error for continuous states
- Cross-entropy for discrete states
- May not correlate with planning performance

**Planning Performance**:
- Evaluate policies learned with the model
- More relevant but computationally expensive
- Gold standard when feasible

**Uncertainty Calibration**:
- Ensure predicted uncertainty matches actual errors
- Important for robust planning
- Use reliability diagrams and calibration error


```python
# Environment Models Implementation

# Import from Modular Files
from models import TabularModel, NeuralModel, ModelTrainer
from environments import SimpleGridWorld

# Demonstrate Model Learning
print("Environment Model Learning Demonstration")
print("=" * 50)

# Create Environment and Collect Data
env = SimpleGridWorld(size=4)
tabular*model = TabularModel(env.num*states, env.num_actions)

# Collect Experience
n_episodes = 1000
experience_data = []

print("\n1. Collecting experience...")
for episode in range(n_episodes):
    state = env.reset()
    done = False
    
    while not done:
        action = np.random.randint(env.num_actions)  # Random policy
        next_state, reward, done = env.step(action)
        
        # Update tabular model
        tabular*model.update(state, action, next*state, reward)
        
        # Store for neural model
        experience*data.append((state, action, next*state, reward))
        
        state = next_state

print(f"Collected {len(experience_data)} transitions")

# Prepare Data for Neural Model
states = np.array([exp[0] for exp in experience_data])
actions = np.array([exp[1] for exp in experience_data])
next*states = np.array([exp[2] for exp in experience*data])
rewards = np.array([exp[3] for exp in experience_data])

# Convert States to One-hot for Neural Model
states*onehot = np.eye(env.num*states)[states]
next*states*onehot = np.eye(env.num*states)[next*states]

# Train Neural Model
print("\n2. Training neural model...")
neural*model = NeuralModel(env.num*states, env.num*actions, hidden*dim=64, ensemble_size=3).to(device)
trainer = ModelTrainer(neural_model, lr=1e-3)

trainer.train*batch((states*onehot, actions, next*states*onehot, rewards), epochs=50, batch_size=64)

print("\n3. Model accuracy comparison:")

# Test Model Accuracy
test*states = np.random.randint(0, env.num*states, 100)
test*actions = np.random.randint(0, env.num*actions, 100)

tabular_errors = []
neural_errors = []

for s, a in zip(test*states, test*actions):
    # Get true transition (using many samples)
    true*next*states = []
    true_rewards = []
    
    for _ in range(50):
        env.state = s
        next*state, reward, * = env.step(a)
        true*next*states.append(next_state)
        true_rewards.append(reward)
    
    true*reward = np.mean(true*rewards)
    
    # Tabular model prediction
    pred*reward*tabular = tabular*model.get*expected_reward(s, a)
    tabular*errors.append(abs(true*reward - pred*reward*tabular))
    
    # Neural model prediction
    state*tensor = torch.FloatTensor(np.eye(env.num*states)[s]).to(device)
    action_tensor = torch.LongTensor([a]).to(device)
    *, pred*reward*neural = neural*model(state*tensor, action*tensor)
    pred*reward*neural = pred*reward*neural.cpu().item()
    neural*errors.append(abs(true*reward - pred*reward*neural))

print(f"Tabular model MAE: {np.mean(tabular_errors):.4f}")
print(f"Neural model MAE: {np.mean(neural_errors):.4f}")

# Visualize Training Progress
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(trainer.loss_history)
plt.title('Neural Model Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True, alpha=0.3)

plt.subplot(1, 2, 2)
plt.bar(['Tabular Model', 'Neural Model'], [np.mean(tabular*errors), np.mean(neural*errors)], 
        color=['blue', 'red'], alpha=0.7)
plt.title('Model Accuracy Comparison')
plt.ylabel('Mean Absolute Error')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print("\n‚úÖ Environment model learning complete!")
print("üìä Next: Classical planning with learned models")
```

    Using device: cpu
    Environment setup complete!
    PyTorch version: 2.0.1
    Gymnasium version: 1.2.1
    NumPy version: 1.26.4
    Environment Model Learning Demonstration
    ==================================================
    
    1. Collecting experience...



    ---------------------------------------------------------------------------

    NameError                                 Traceback (most recent call last)

    Cell In[1], line 25
         22 done = False
         24 while not done:
    ---> 25     action = np.random.randint(env.num_actions)  # Random policy
         26     next_state, reward, done = env.step(action)
         28     # Update tabular model


    NameError: name 'np' is not defined


# Section 3: Classical Planning with Learned Models

## 3.1 Dynamic Programming with Learned Models

Once we have learned an environment model, we can use classical dynamic programming algorithms to compute optimal policies. This is one of the most straightforward applications of model-based RL.

### Value Iteration with Learned Models

The Value Iteration algorithm can be applied directly using our learned transition probabilities and rewards:

$$V*{k+1}(s) = \max*a \sum*{s'} \hat{P}(s'|s,a)[\hat{R}(s,a,s') + \gamma V*k(s')]$$

**Key Advantages:**
- Guaranteed convergence to optimal policy (if model is accurate)
- Can compute policy for all states simultaneously
- No need for exploration during planning phase

**Potential Issues:**
- Model errors compound over planning horizon
- Assumes learned model is accurate
- May overfit to limited experience

### Policy Iteration with Learned Models

Policy Iteration alternates between policy evaluation and policy improvement using the learned model:

**Policy Evaluation:**
$$V^\pi(s) = \sum_{s'} \hat{P}(s'|s,\pi(s))[\hat{R}(s,\pi(s),s') + \gamma V^\pi(s')]$$

**Policy Improvement:**
$$\pi'(s) = \arg\max*a \sum*{s'} \hat{P}(s'|s,a)[\hat{R}(s,a,s') + \gamma V^\pi(s')]$$

## 3.2 Handling Model Uncertainty

Real learned models have uncertainty. Several approaches address this:

### Pessimistic Planning
- Use lower confidence bounds for model predictions
- $\hat{P}_{pessimistic}(s'|s,a) = \hat{P}(s'|s,a) - \beta \sigma(s'|s,a)$
- Leads to more robust but potentially conservative policies

### Optimistic Planning
- Use upper confidence bounds for model predictions
- Encourages exploration of uncertain regions
- Can lead to more aggressive exploration policies

### Robust Planning
- Optimize for worst-case model within confidence region
- $\max*\pi \min*{M \in \mathcal{U}} V^\pi_M$
- Very conservative but safe approach

## 3.3 Model-based Policy Search

Instead of computing value functions, we can directly search in policy space using the learned model:

### Gradient-based Policy Search
- Use model to compute policy gradients
- $\nabla*\theta J(\theta) = \mathbb{E}*{\tau \sim \pi*\theta, M}[\sum*t \nabla*\theta \log \pi*\theta(a*t|s*t) R(\tau)]$
- Generate synthetic rollouts with learned model

### Evolutionary Policy Search
- Maintain population of policy parameters
- Evaluate policies using learned model
- Select and mutate best policies

### Random Shooting
- Sample random action sequences
- Evaluate using learned model
- Select best sequence and execute first action


```python
# Classical Planning Implementation

class ModelBasedPlanner:
    """Classical planning algorithms using learned models"""
    
    def **init**(self, model, num*states, num*actions, gamma=0.99):
        self.model = model
        self.num*states = num*states
        self.num*actions = num*actions
        self.gamma = gamma
        
        # Initialize value function and policy
        self.V = np.zeros(num_states)
        self.policy = np.zeros(num_states, dtype=int)
        
        # Planning history for analysis
        self.value_history = []
        self.policy_history = []
    
    def value*iteration(self, max*iterations=100, tolerance=1e-6):
        """Value Iteration using learned model"""
        
        print(f"Running Value Iteration (max*iter={max*iterations}, tol={tolerance})")
        
        for iteration in range(max_iterations):
            old_V = self.V.copy()
            
            for state in range(self.num_states):
                # Compute Q-values for all actions
                q*values = np.zeros(self.num*actions)
                
                for action in range(self.num_actions):
                    # Compute expected value using model
                    expected_value = 0
                    
                    for next*state in range(self.num*states):
                        transition*prob = self.model.get*transition*prob(state, action, next*state)
                        reward = self.model.get*expected*reward(state, action)
                        expected*value += transition*prob * (reward + self.gamma * old*V[next*state])
                    
                    q*values[action] = expected*value
                
                # Update value and policy
                self.V[state] = np.max(q_values)
                self.policy[state] = np.argmax(q_values)
            
            # Store history
            self.value_history.append(self.V.copy())
            self.policy_history.append(self.policy.copy())
            
            # Check convergence
            if np.max(np.abs(self.V - old_V)) < tolerance:
                print(f"Converged after {iteration + 1} iterations")
                break
        
        return self.V, self.policy
    
    def policy*iteration(self, max*iterations=50, eval*max*iterations=100):
        """Policy Iteration using learned model"""
        
        print(f"Running Policy Iteration (max*iter={max*iterations})")
        
        # Initialize random policy
        self.policy = np.random.randint(0, self.num*actions, self.num*states)
        
        for iteration in range(max_iterations):
            old_policy = self.policy.copy()
            
            # Policy Evaluation
            self.V = self.policy*evaluation(self.policy, max*iterations=eval*max*iterations)
            
            # Policy Improvement
            for state in range(self.num_states):
                q*values = np.zeros(self.num*actions)
                
                for action in range(self.num_actions):
                    expected_value = 0
                    
                    for next*state in range(self.num*states):
                        transition*prob = self.model.get*transition*prob(state, action, next*state)
                        reward = self.model.get*expected*reward(state, action)
                        expected*value += transition*prob * (reward + self.gamma * self.V[next_state])
                    
                    q*values[action] = expected*value
                
                self.policy[state] = np.argmax(q_values)
            
            # Store history
            self.value_history.append(self.V.copy())
            self.policy_history.append(self.policy.copy())
            
            # Check convergence
            if np.array*equal(self.policy, old*policy):
                print(f"Converged after {iteration + 1} iterations")
                break
        
        return self.V, self.policy
    
    def policy*evaluation(self, policy, max*iterations=100, tolerance=1e-6):
        """Evaluate a given policy using learned model"""
        
        V = np.zeros(self.num_states)
        
        for iteration in range(max_iterations):
            old_V = V.copy()
            
            for state in range(self.num_states):
                action = policy[state]
                expected_value = 0
                
                for next*state in range(self.num*states):
                    transition*prob = self.model.get*transition*prob(state, action, next*state)
                    reward = self.model.get*expected*reward(state, action)
                    expected*value += transition*prob * (reward + self.gamma * old*V[next*state])
                
                V[state] = expected_value
            
            if np.max(np.abs(V - old_V)) < tolerance:
                break
        
        return V
    
    def compute*q*function(self):
        """Compute Q-function from current value function"""
        
        Q = np.zeros((self.num*states, self.num*actions))
        
        for state in range(self.num_states):
            for action in range(self.num_actions):
                expected_value = 0
                
                for next*state in range(self.num*states):
                    transition*prob = self.model.get*transition*prob(state, action, next*state)
                    reward = self.model.get*expected*reward(state, action)
                    expected*value += transition*prob * (reward + self.gamma * self.V[next_state])
                
                Q[state, action] = expected_value
        
        return Q

class UncertaintyAwarePlanner:
    """Planning with model uncertainty"""
    
    def **init**(self, ensemble*model, num*states, num_actions, gamma=0.99):
        self.ensemble*model = ensemble*model
        self.num*states = num*states
        self.num*actions = num*actions
        self.gamma = gamma
        
    def pessimistic*value*iteration(self, beta=1.0, max_iterations=100):
        """Value iteration with pessimistic model estimates"""
        
        V = np.zeros(self.num_states)
        policy = np.zeros(self.num_states, dtype=int)
        
        print(f"Running Pessimistic Value Iteration (beta={beta})")
        
        for iteration in range(max_iterations):
            old_V = V.copy()
            
            for state in range(self.num_states):
                q*values = np.zeros(self.num*actions)
                
                for action in range(self.num_actions):
                    # Get predictions from ensemble
                    state*onehot = np.eye(self.num*states)[state:state+1]
                    action_tensor = np.array([action])
                    
                    # Convert to tensors
                    state*tensor = torch.FloatTensor(state*onehot).to(device)
                    action*tensor = torch.LongTensor(action*tensor).to(device)
                    
                    # Get ensemble predictions
                    next*state*mean, reward*mean, next*state*std, reward*std = \
                        self.ensemble*model.predict*with*uncertainty(state*tensor, action_tensor)\n",
                    \n",
                    # Use pessimistic estimates\n",
                    pessimistic*reward = reward*mean.cpu().item() - beta * reward_std.cpu().item()\n",
                    \n",
                    # For simplicity, assume deterministic next state (can be extended)\n",
                    next*state*pred = next*state*mean.cpu().numpy()[0]\n",
                    next*state*idx = np.argmax(next*state*pred)  # Most likely next state\n",
                    \n",
                    q*values[action] = pessimistic*reward + self.gamma * old*V[next*state_idx]\n",
                \n",
                V[state] = np.max(q_values)\n",
                policy[state] = np.argmax(q_values)\n",
            \n",
            if np.max(np.abs(V - old_V)) < 1e-6:\n",
                print(f\"Converged after {iteration + 1} iterations\")\n",
                break\n",
        \n",
        return V, policy\n",
    \n",
    def optimistic*value*iteration(self, beta=1.0, max_iterations=100):\n",
        \"\"\"Value iteration with optimistic model estimates\"\"\"\n",
        \n",
        V = np.zeros(self.num_states)\n",
        policy = np.zeros(self.num_states, dtype=int)\n",
        \n",
        print(f\"Running Optimistic Value Iteration (beta={beta})\")\n",
        \n",
        for iteration in range(max_iterations):\n",
            old_V = V.copy()\n",
            \n",
            for state in range(self.num_states):\n",
                q*values = np.zeros(self.num*actions)\n",
                \n",
                for action in range(self.num_actions):\n",
                    # Get predictions from ensemble\n",
                    state*onehot = np.eye(self.num*states)[state:state+1]\n",
                    action_tensor = np.array([action])\n",
                    \n",
                    # Convert to tensors\n",
                    state*tensor = torch.FloatTensor(state*onehot).to(device)\n",
                    action*tensor = torch.LongTensor(action*tensor).to(device)\n",
                    \n",
                    # Get ensemble predictions\n",
                    next*state*mean, reward*mean, next*state*std, reward*std = \\\n",
                        self.ensemble*model.predict*with*uncertainty(state*tensor, action_tensor)\n",
                    \n",
                    # Use optimistic estimates\n",
                    optimistic*reward = reward*mean.cpu().item() + beta * reward_std.cpu().item()\n",
                    \n",
                    # For simplicity, assume deterministic next state\n",
                    next*state*pred = next*state*mean.cpu().numpy()[0]\n",
                    next*state*idx = np.argmax(next*state*pred)\n",
                    \n",
                    q*values[action] = optimistic*reward + self.gamma * old*V[next*state_idx]\n",
                \n",
                V[state] = np.max(q_values)\n",
                policy[state] = np.argmax(q_values)\n",
            \n",
            if np.max(np.abs(V - old_V)) < 1e-6:\n",
                print(f\"Converged after {iteration + 1} iterations\")\n",
                break\n",
        \n",
        return V, policy\n",
"\n",
"class ModelBasedPolicySearch:\n",
"    \"\"\"Policy search using learned models\"\"\"\n",
"    \n",
"    def **init**(self, model, state*dim, action*dim, gamma=0.99):\n",
"        self.model = model\n",
"        self.state*dim = state*dim\n",
"        self.action*dim = action*dim\n",
"        self.gamma = gamma\n",
"    \n",
"    def random*shooting(self, initial*state, horizon=10, num_sequences=1000):\n",
"        \"\"\"Random shooting with learned model\"\"\"\n",
"        \n",
"        best_sequence = None\n",
"        best_value = -np.inf\n",
"        \n",
"        for * in range(num*sequences):\n",
"            # Sample random action sequence\n",
"            action*sequence = np.random.randint(0, self.action*dim, horizon)\n",
"            \n",
"            # Evaluate sequence using model\n",
"            total_reward = 0\n",
"            current*state = initial*state\n",
"            discount = 1.0\n",
"            \n",
"            for action in action_sequence:\n",
"                next*state, reward = self.model.sample*transition(current_state, action)\n",
"                total_reward += discount * reward\n",
"                discount *= self.gamma\n",
"                current*state = next*state\n",
"            \n",
"            if total*reward > best*value:\n",
"                best*value = total*reward\n",
"                best*sequence = action*sequence\n",
"        \n",
"        return best*sequence, best*value\n",
"    \n",
"    def cross*entropy*method(self, initial*state, horizon=10, num*sequences=1000, \n",
"                           num*elite=100, num*iterations=10):\n",
"        \"\"\"Cross-entropy method for policy search\"\"\"\n",
"        \n",
"        # Initialize action probabilities (uniform)\n",
"        action*probs = np.ones((horizon, self.action*dim)) / self.action_dim\n",
"        \n",
"        for iteration in range(num_iterations):\n",
"            # Sample action sequences\n",
"            sequences = []\n",
"            values = []\n",
"            \n",
"            for * in range(num*sequences):\n",
"                sequence = []\n",
"                for t in range(horizon):\n",
"                    action = np.random.choice(self.action*dim, p=action*probs[t])\n",
"                    sequence.append(action)\n",
"                \n",
"                # Evaluate sequence\n",
"                total_reward = 0\n",
"                current*state = initial*state\n",
"                discount = 1.0\n",
"                \n",
"                for action in sequence:\n",
"                    next*state, reward = self.model.sample*transition(current_state, action)\n",
"                    total_reward += discount * reward\n",
"                    discount *= self.gamma\n",
"                    current*state = next*state\n",
"                \n",
"                sequences.append(sequence)\n",
"                values.append(total_reward)\n",
"            \n",
"            # Select elite sequences\n",
"            elite*indices = np.argsort(values)[-num*elite:]\n",
"            elite*sequences = [sequences[i] for i in elite*indices]\n",
"            \n",
"            # Update action probabilities\n",
"            action*counts = np.zeros((horizon, self.action*dim))\n",
"            \n",
"            for sequence in elite_sequences:\n",
"                for t, action in enumerate(sequence):\n",
"                    action_counts[t, action] += 1\n",
"            \n",
"            # Smooth update\n",
"            alpha = 0.7\n",
"            new*probs = action*counts / num_elite\n",
"            action*probs = alpha * new*probs + (1 - alpha) * action_probs\n",
"            \n",
"            # Add small amount of noise for exploration\n",
"            action*probs += 0.01 / self.action*dim\n",
"            action*probs /= np.sum(action*probs, axis=1, keepdims=True)\n",
"        \n",
"        # Return best sequence\n",
"        best*sequence = elite*sequences[np.argmax([values[i] for i in elite_indices])]\n",
"        best*value = max([values[i] for i in elite*indices])\n",
"        \n",
"        return best*sequence, best*value\n",
"\n",
"# Demonstration of classical planning\n",
"print(\"Classical Planning with Learned Models\")\n",
"print(\"=\" * 50)\n",
"\n",
"# Use the tabular model we learned earlier\n",
"planner = ModelBasedPlanner(tabular*model, env.num*states, env.num_actions, gamma=0.95)\n",
"\n",
"print(\"\\n1. Value Iteration with Learned Model:\")\n",
"vi*values, vi*policy = planner.value*iteration(max*iterations=50)\n",
"\n",
"print(\"\\n2. Policy Iteration with Learned Model:\")\n",
"planner*pi = ModelBasedPlanner(tabular*model, env.num*states, env.num*actions, gamma=0.95)\n",
"pi*values, pi*policy = planner*pi.policy*iteration(max_iterations=20)\n",
"\n",
"# Compare with uncertainty-aware planning\n",
"print(\"\\n3. Uncertainty-Aware Planning:\")\n",
"uncertainty*planner = UncertaintyAwarePlanner(neural*model, env.num*states, env.num*actions)\n",
"pessimistic*V, pessimistic*policy = uncertainty*planner.pessimistic*value_iteration(beta=0.5)\n",
"optimistic*V, optimistic*policy = uncertainty*planner.optimistic*value_iteration(beta=0.5)\n",
"\n",
"# Visualize results\n",
"fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
"\n",
"# Reshape values and policies for visualization\n",
"grid*size = int(np.sqrt(env.num*states))\n",
"\n",
"def plot*value*function(ax, values, title):\n",
"    value*grid = values.reshape(grid*size, grid_size)\n",
"    im = ax.imshow(value_grid, cmap='viridis')\n",
"    ax.set_title(title)\n",
"    plt.colorbar(im, ax=ax)\n",
"    \n",
"def plot_policy(ax, policy, title):\n",
"    policy*grid = policy.reshape(grid*size, grid_size)\n",
"    # Map actions to arrows: 0=‚Üë, 1=‚Üì, 2=‚Üê, 3=‚Üí\n",
"    arrow_map = {0: '‚Üë', 1: '‚Üì', 2: '‚Üê', 3: '‚Üí'}\n",
"    \n",
"    ax.imshow(np.zeros((grid*size, grid*size)), cmap='gray', alpha=0.3)\n",
"    \n",
"    for i in range(grid_size):\n",
"        for j in range(grid_size):\n",
"            action = policy_grid[i, j]\n",
"            ax.text(j, i, arrow_map[action], ha='center', va='center', \n",
"                   fontsize=20, fontweight='bold', color='blue')\n",
"    \n",
"    ax.set_title(title)\n",
"    ax.set_xticks([])\n",
"    ax.set_yticks([])\n",
"\n",
"# Plot value functions\n",
"plot*value*function(axes[0, 0], vi_values, 'Value Iteration - Values')\n",
"plot*value*function(axes[0, 1], pi_values, 'Policy Iteration - Values')\n",
"plot*value*function(axes[0, 2], pessimistic_V, 'Pessimistic Planning - Values')\n",
"\n",
"# Plot policies\n",
"plot*policy(axes[1, 0], vi*policy, 'Value Iteration - Policy')\n",
"plot*policy(axes[1, 1], pi*policy, 'Policy Iteration - Policy')\n",
"plot*policy(axes[1, 2], pessimistic*policy, 'Pessimistic Planning - Policy')\n",
"\n",
"plt.tight_layout()\n",
"plt.show()\n",
"\n",
"# Compare planning methods\n",
"print(\"\\n4. Planning Method Comparison:\")\n",
"print(f\"Value Iteration - Max Value: {np.max(vi*values):.3f}, Policy Changes: {len(planner.value*history)}\")\n",
"print(f\"Policy Iteration - Max Value: {np.max(pi*values):.3f}, Policy Changes: {len(planner*pi.value_history)}\")\n",
"print(f\"Pessimistic Planning - Max Value: {np.max(pessimistic_V):.3f}\")\n",
"print(f\"Optimistic Planning - Max Value: {np.max(optimistic_V):.3f}\")\n",
"\n",
"# Test policy search methods\n",
"print(\"\\n5. Model-Based Policy Search:\")\n",
"policy*searcher = ModelBasedPolicySearch(tabular*model, env.num*states, env.num*actions)\n",
"\n",
"# Random shooting\n",
"initial_state = 0\n",
"best*sequence*rs, best*value*rs = policy*searcher.random*shooting(initial*state, horizon=5, num*sequences=500)\n",
"print(f\"Random Shooting - Best Value: {best*value*rs:.3f}, Best Sequence: {best*sequence*rs}\")\n",
"\n",
"# Cross-entropy method\n",
"best*sequence*cem, best*value*cem = policy*searcher.cross*entropy*method(initial*state, horizon=5, \n",
"                                                                        num*sequences=200, num*elite=20)\n",
"print(f\"Cross-Entropy Method - Best Value: {best*value*cem:.3f}, Best Sequence: {best*sequence*cem}\")\n",
"\n",
"print(\"\\n‚úÖ Classical planning with learned models complete!\")\n",
"print(\"üìä Next: Dyna-Q algorithm - integrating planning and learning\")"
```


      Cell In[8], line 175
        self.ensemble*model.predict*with*uncertainty(state*tensor, action_tensor)\n",
                                                                                     
    ^
    SyntaxError: unexpected character after line continuation character




```python
demonstrate*classical*planning()
```

    Classical Planning with Learned Models
    ==================================================
    
    1. Collecting experience for model learning...


    Collected 57251 transitions
    
    2. Training neural model...
    Epoch 5/50, Loss: 0.0000
    Epoch 10/50, Loss: 0.0000
    Epoch 5/50, Loss: 0.0000
    Epoch 10/50, Loss: 0.0000
    Unexpected exception formatting exception. Falling back to standard exception
    Unexpected exception formatting exception. Falling back to standard exception


    Traceback (most recent call last):
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py", line 3508, in run_code
        exec(code*obj, self.user*global*ns, self.user*ns)
      File "/var/folders/cg/l2rdx46d6lv3b5xc17b420yc0000gn/T/ipykernel_76508/2283393919.py", line 1, in <module>
        demonstrate*classical*planning()
      File "/Users/tahamajs/Documents/uni/DRL/CAs/Solutions/CA10/classical*planning.py", line 434, in demonstrate*classical_planning
        print("\n2. Training neural model...")
      File "/Users/tahamajs/Documents/uni/DRL/CAs/Solutions/CA10/models.py", line 270, in train_batch
      File "/Users/tahamajs/Documents/uni/DRL/CAs/Solutions/CA10/models.py", line 233, in train_step
        for i in range(self.model.ensemble_size):
    KeyboardInterrupt
    
    During handling of the above exception, another exception occurred:
    
    Traceback (most recent call last):
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/interactiveshell.py", line 2105, in showtraceback
        stb = self.InteractiveTB.structured_traceback(
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py", line 1428, in structured_traceback
        return FormattedTB.structured_traceback(
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py", line 1319, in structured_traceback
        return VerboseTB.structured_traceback(
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py", line 1172, in structured_traceback
        formatted*exception = self.format*exception*as*a*whole(etype, evalue, etb, number*of*lines*of_context,
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py", line 1087, in format*exception*as*a*whole
        frames.append(self.format_record(record))
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py", line 969, in format_record
        frame*info.lines, Colors, self.has*colors, lvals
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/IPython/core/ultratb.py", line 792, in lines
        return self._sd.lines
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stack*data/utils.py", line 144, in cached*property_wrapper
        value = obj.**dict**[self.func.**name**] = self.func(obj)
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stack_data/core.py", line 734, in lines
        pieces = self.included_pieces
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stack*data/utils.py", line 144, in cached*property_wrapper
        value = obj.**dict**[self.func.**name**] = self.func(obj)
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stack*data/core.py", line 681, in included*pieces
        pos = scope*pieces.index(self.executing*piece)
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stack*data/utils.py", line 144, in cached*property_wrapper
        value = obj.**dict**[self.func.**name**] = self.func(obj)
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/stack*data/core.py", line 660, in executing*piece
        return only(
      File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/executing/executing.py", line 190, in only
        raise NotOneValueFound('Expected one value, found 0')
    executing.executing.NotOneValueFound: Expected one value, found 0



```python
demonstrate*dyna*q()
```

# Section 5: Monte Carlo Tree Search (mcts)

Monte Carlo Tree Search is a powerful planning algorithm that has achieved remarkable success in games like Go and has been extended to general reinforcement learning problems.

## 5.1 Theoretical Foundation

MCTS combines:
- **Tree Search**: Systematic exploration of possible future states
- **Monte Carlo Simulation**: Random rollouts to estimate value
- **Multi-Armed Bandit**: UCB for action selection in tree nodes

### Key Components:

1. **Selection**: Navigate from root to leaf using bandit strategy
2. **Expansion**: Add one or more child nodes
3. **Simulation**: Random rollout from new node
4. **Backpropagation**: Update all nodes on path with result

### Ucb1 Formula for Node Selection:

$$UCB1(i) = \overline{X*i} + C\sqrt{\frac{\ln n}{n*i}}$$

Where:
- $\overline{X_i}$ = average reward of action i
- $n_i$ = number of times action i was selected
- $n$ = total number of selections
- $C$ = exploration parameter

### Mcts in Model-based Rl:

MCTS can be used with learned models to perform sophisticated planning by building search trees that explore promising action sequences.


```python
demonstrate_mcts()
```

    Monte Carlo Tree Search (MCTS) Demonstration
    ==================================================
    
    1. Setting up environment and learned model...
    Training tabular model...



    ---------------------------------------------------------------------------

    IndexError                                Traceback (most recent call last)

    Cell In[6], line 233
        231 action = np.random.randint(env.num_actions)
        232 next_state, reward, done = env.step(action)
    --> 233 tabular*model.update(state, action, reward, next*state)
        234 if done:
        235     break


    Cell In[4], line 22, in TabularModel.update(self, state, action, next_state, reward)
         20 def update(self, state, action, next_state, reward):
         21     """Update model with new transition"""
    ---> 22     self.transition*counts[state, action, next*state] += 1
         23     self.sa_counts[state, action] += 1
         25     self.reward_sums[state, action] += reward


    IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices


# Section 6: Model Predictive Control (mpc)

Model Predictive Control is a control strategy that uses a model to predict future behavior and optimizes a sequence of control actions over a finite horizon.

## 6.1 Theoretical Foundation

MPC operates on the principle of **receding horizon control**:

1. **Prediction**: Use model to predict future states over horizon H
2. **Optimization**: Solve optimal control problem over this horizon
3. **Execution**: Apply only the first control action
4. **Recede**: Shift horizon forward and repeat

### Key Components:

- **Prediction Model**: $\hat{s}*{t+1} = f(s*t, a_t)$
- **Cost Function**: $J = \sum*{k=0}^{H-1} c(s*{t+k}, a*{t+k}) + V*f(s_{t+H})$
- **Constraints**: State and action constraints
- **Terminal Cost**: $V*f(s*{t+H})$ (optional)

### Advantages:
- Handles constraints naturally
- Provides explicit planning horizon
- Can incorporate uncertainty
- Works with nonlinear models

### Mpc in Rl Context:
- Use learned dynamics models
- Optimize with gradient-based or sampling methods
- Can incorporate learned value functions as terminal costs


```python
demonstrate_mpc()
```

# Section 7: Advanced Model-based Methods and Modern Approaches

## 7.1 Modern Neural Model-based Methods

### Model-based Meta-learning
- **MAML for Model Learning**: Learning models that can quickly adapt to new environments
- **Gradient-Based Meta-Learning**: Using gradients to update model parameters efficiently

### Uncertainty-aware Models
- **Bayesian Neural Networks**: Capturing epistemic uncertainty in dynamics
- **Ensemble Methods**: Multiple models for uncertainty quantification
- **Dropout-Based Uncertainty**: Using Monte Carlo dropout for uncertainty estimation

### Advanced Planning Methods
- **Differentiable Planning**: End-to-end training of planning modules
- **Learned Optimizers**: Using neural networks as optimizers for planning
- **Hierarchical Planning**: Multi-level planning for complex tasks

## 7.2 State-of-the-art Methods

### Model-based Policy Optimization (mbpo)
- Combines model-based and model-free learning
- Uses learned models to generate synthetic data
- Applies model-free algorithms to mixed real and synthetic data

### Dreamer and Dreamerv2
- World models with latent state representations
- Planning in latent space
- Actor-critic learning within the world model

### Muzero
- Combines MCTS with learned models
- No explicit environment model
- Learns value, policy, and reward predictions


```python
demonstrate_comparison()
```

    Comprehensive Model-Based Reinforcement Learning Analysis
    ============================================================
    
    Running comprehensive comparison...
    Episodes per run: 30, Runs per method: 2
    
    üåç Environment: GridWorld-5x5



    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    Cell In[7], line 255
        250 framework.add_method("MPC-CEM", 
        251                     lambda **kwargs: MPCAgent(kwargs['model'], 25, 4, horizon=5, method='cross_entropy'),
        252                     model='neural')
        254 # Run comprehensive comparison
    --> 255 framework.run*comparison(n*episodes=30, n_runs=2)
        257 # Visualize and analyze results
        258 framework.visualize_results()


    Cell In[7], line 37, in ModelBasedComparisonFramework.run*comparison(self, n*episodes, max*steps, n*runs)
         34 if hasattr(env, 'num_states'):
         35     # Create and train models
         36     tabular*model = TabularModel(env.num*states, env.num_actions)
    ---> 37     neural*model = NeuralModel(env.num*states, env.num*actions, hidden*size=32)
         39     # Quick model training
         40     self.*train*models(env, tabular*model, neural*model)


    TypeError: NeuralModel.**init**() got an unexpected keyword argument 'hidden*size'. Did you mean 'hidden*dim'?



```python
# Comprehensive Model-based Rl Comparison and Analysis
import os
from dyna*q import demonstrate*dyna_q
from mcts import demonstrate_mcts
from mpc import demonstrate_mpc

# Create Visualizations Directory If It Doesn't Exist
os.makedirs('visualizations', exist_ok=True)

print("Running individual model-based RL demonstrations...")
print("All visualizations will be saved to the 'visualizations' folder")

# Run Individual Demonstrations
print("\n1. Running Dyna-Q demonstration...")
demonstrate*dyna*q()

print("\n2. Running MCTS demonstration...")
demonstrate_mcts()

print("\n3. Running MPC demonstration...")
demonstrate_mpc()

print(f"\n‚úÖ All demonstrations complete! Check the 'visualizations' folder for saved plots.")
```

    Running individual model-based RL demonstrations...
    All visualizations will be saved to the 'visualizations' folder
    
    1. Running Dyna-Q demonstration...
    Dyna-Q Algorithm Demonstration
    ==================================================
    
    1. Training on Simple GridWorld:
    
    Training Q-Learning...
      Episode 50: Avg Reward = 0.916, Direct Updates = 846, Planning Updates = 0
      Episode 100: Avg Reward = 0.924, Direct Updates = 1287, Planning Updates = 0
      Episode 150: Avg Reward = 0.925, Direct Updates = 1736, Planning Updates = 0
      Episode 200: Avg Reward = 0.922, Direct Updates = 2178, Planning Updates = 0
    
    Training Dyna-Q (n=5)...
      Episode 50: Avg Reward = 0.917, Direct Updates = 580, Planning Updates = 2900
      Episode 100: Avg Reward = 0.928, Direct Updates = 1025, Planning Updates = 5125
      Episode 150: Avg Reward = 0.916, Direct Updates = 1468, Planning Updates = 7340
      Episode 200: Avg Reward = 0.927, Direct Updates = 1916, Planning Updates = 9580
    
    Training Dyna-Q (n=50)...
      Episode 50: Avg Reward = 0.922, Direct Updates = 552, Planning Updates = 27600
      Episode 100: Avg Reward = 0.924, Direct Updates = 1007, Planning Updates = 50350
      Episode 150: Avg Reward = 0.923, Direct Updates = 1467, Planning Updates = 73350
      Episode 200: Avg Reward = 0.924, Direct Updates = 1904, Planning Updates = 95200
    
    Training Dyna-Q+ (n=5)...
      Episode 50: Avg Reward = 0.922, Direct Updates = 623, Planning Updates = 3115
      Episode 100: Avg Reward = 0.917, Direct Updates = 1069, Planning Updates = 5345
      Episode 150: Avg Reward = 0.919, Direct Updates = 1502, Planning Updates = 7510
      Episode 200: Avg Reward = 0.918, Direct Updates = 1950, Planning Updates = 9750
    
    2. Testing on Blocking Maze (Environment Change):
    
    Training Dyna-Q on Blocking Maze...
      Episode 51: Reward = 1.0, Steps = 11
    
    ** Environment changed at episode 100 **
      Episode 100: Reward = 1.0, Steps = 12
      Episode 151: Reward = 1.0, Steps = 11
      Episode 201: Reward = 1.0, Steps = 11
      Episode 251: Reward = 1.0, Steps = 15
    
    Training Dyna-Q+ on Blocking Maze...
      Episode 50: Avg Reward = 0.922, Direct Updates = 623, Planning Updates = 3115
      Episode 100: Avg Reward = 0.917, Direct Updates = 1069, Planning Updates = 5345
      Episode 150: Avg Reward = 0.919, Direct Updates = 1502, Planning Updates = 7510
      Episode 200: Avg Reward = 0.918, Direct Updates = 1950, Planning Updates = 9750
    
    2. Testing on Blocking Maze (Environment Change):
    
    Training Dyna-Q on Blocking Maze...
      Episode 51: Reward = 1.0, Steps = 11
    
    ** Environment changed at episode 100 **
      Episode 100: Reward = 1.0, Steps = 12
      Episode 151: Reward = 1.0, Steps = 11
      Episode 201: Reward = 1.0, Steps = 11
      Episode 251: Reward = 1.0, Steps = 15
    
    Training Dyna-Q+ on Blocking Maze...
      Episode 51: Reward = 1.0, Steps = 1634
      Episode 51: Reward = 1.0, Steps = 1634
    
    ** Environment changed at episode 100 **
      Episode 100: Reward = 1.0, Steps = 1431
    
    ** Environment changed at episode 100 **
      Episode 100: Reward = 1.0, Steps = 1431
      Episode 151: Reward = 1.0, Steps = 686
      Episode 201: Reward = 1.0, Steps = 286
      Episode 151: Reward = 1.0, Steps = 686
      Episode 201: Reward = 1.0, Steps = 286
      Episode 251: Reward = 1.0, Steps = 418
      Episode 251: Reward = 1.0, Steps = 418



    
![png](CA10*files/CA10*17_1.png)
    


    
    3. Key Insights from Dyna-Q Experiments:
    
    Simple GridWorld Results:
      Q-Learning: Final Performance = 0.923, Planning Efficiency = 0.0x
      Dyna-Q (n=5): Final Performance = 0.920, Planning Efficiency = 5.0x
      Dyna-Q (n=50): Final Performance = 0.918, Planning Efficiency = 50.0x
      Dyna-Q+ (n=5): Final Performance = 0.920, Planning Efficiency = 5.0x
    
    Blocking Maze Results (Adaptability):
      Dyna-Q: Performance before change = 1.000, after change = 1.000, adaptation = 0.000
      Dyna-Q+: Performance before change = 0.950, after change = 1.000, adaptation = 1.000
    
    üìä Key Takeaways:
    ‚Ä¢ Dyna-Q achieves better sample efficiency through planning
    ‚Ä¢ More planning steps generally improve performance
    ‚Ä¢ Dyna-Q+ adapts better to environment changes
    ‚Ä¢ Model-based methods excel when environment is stable
    
    2. Running MCTS demonstration...
    Monte Carlo Tree Search (MCTS) Demonstration
    ==================================================
    
    1. Setting up environment and learned model...
    Training tabular model...
    Model trained with 4763.0 transitions
    
    2. Testing MCTS performance...
    Episodes 0-5: Avg Reward = -0.66, Avg Length = 86.0, Avg Search Time = 0.1113s
    Episodes 0-5: Avg Reward = -0.66, Avg Length = 86.0, Avg Search Time = 0.1113s
    Episodes 5-10: Avg Reward = 0.20, Avg Length = 60.8, Avg Search Time = 0.1056s
    Episodes 5-10: Avg Reward = 0.20, Avg Length = 60.8, Avg Search Time = 0.1056s
    Episodes 10-15: Avg Reward = -0.78, Avg Length = 98.0, Avg Search Time = 0.1041s
    Episodes 10-15: Avg Reward = -0.78, Avg Length = 98.0, Avg Search Time = 0.1041s
    Episodes 15-20: Avg Reward = -0.20, Avg Length = 81.0, Avg Search Time = 0.1040s
    Episodes 15-20: Avg Reward = -0.20, Avg Length = 81.0, Avg Search Time = 0.1040s



    
![png](CA10*files/CA10*17_3.png)
    


    
    3. MCTS Performance Analysis:
    Average Episode Reward: -0.360 ¬± 0.735
    Average Episode Length: 81.5 ¬± 28.5
    Average Search Time: 0.1040 seconds
    Average Tree Size: 201.0 nodes
    Total MCTS Searches: 1629
    
    Random Policy Baseline:
    Average Episode Reward: -0.460 ¬± 0.743
    
    MCTS Improvement over Random: -21.7%
    
    üìä Key MCTS Insights:
    ‚Ä¢ MCTS provides sophisticated planning through tree search
    ‚Ä¢ UCB balances exploration and exploitation in tree nodes
    ‚Ä¢ Performance scales with number of simulations
    ‚Ä¢ Computational cost grows with search depth and simulations
    ‚Ä¢ Effective for discrete action spaces with learned models
    
    3. Running MPC demonstration...
    Model Predictive Control (MPC) Demonstration
    ==================================================
    
    1. Setting up MPC with learned model...
    Collected 104475 transitions



    ---------------------------------------------------------------------------

    TypeError                                 Traceback (most recent call last)

    Cell In[10], line 21
         18 demonstrate_mcts()
         20 print("\n3. Running MPC demonstration...")
    ---> 21 demonstrate_mpc()
         23 print(f"\n‚úÖ All demonstrations complete! Check the 'visualizations' folder for saved plots.")


    File ~/Documents/uni/DRL/CAs/Solutions/CA10/mpc.py:266, in demonstrate_mpc()
        263 next*states*onehot = np.eye(env.num*states)[next*states]
        265 # Use previously trained neural model
    --> 266 neural*model = NeuralModel(env.num*states, env.num*actions, hidden*dim=64).to(
        267     device
        268 )
        270 # Quick training for demonstration
        271 trainer = ModelTrainer(neural_model, lr=1e-3)


    File ~/Documents/uni/DRL/CAs/Solutions/CA10/models.py:123, in NeuralModel.**init**(self, state*dim, action*dim, hidden*dim, ensemble*size)
        122 def **init**(self, state*dim, action*dim, hidden*dim=256, ensemble*size=1):
    --> 123     super(NeuralModel, self).**init**()
        125     self.state*dim = state*dim
        126     self.action*dim = action*dim


    TypeError: super(type, obj): obj must be an instance or subtype of type

