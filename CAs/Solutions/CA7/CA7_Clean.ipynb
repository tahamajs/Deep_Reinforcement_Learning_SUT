{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# CA7: Deep Q-Networks and Value-Based Methods\n",
        "\n",
        "## Abstract\n",
        "\n",
        "This notebook presents a comprehensive study of Deep Q-Networks (DQN) and advanced value-based reinforcement learning methods. We examine the theoretical foundations, implementation details, and performance characteristics of various DQN variants including basic DQN, Double DQN, and Dueling DQN. Through modular implementations and comprehensive experiments, we demonstrate the effectiveness of these methods on classic control environments and provide insights into their comparative performance, convergence properties, and practical applications.\n",
        "\n",
        "**Keywords:** Deep reinforcement learning, Q-learning, Deep Q-networks, value-based methods, experience replay, target networks, Double DQN, Dueling DQN\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Setup and Configuration](#setup-and-configuration)\n",
        "2. [Section 1: Theoretical Foundations](#section-1-theoretical-foundations)\n",
        "3. [Section 2: Basic DQN Implementation](#section-2-basic-dqn-implementation)\n",
        "4. [Section 3: Experience Replay and Target Networks](#section-3-experience-replay-and-target-networks)\n",
        "5. [Section 4: Double DQN](#section-4-double-dqn)\n",
        "6. [Section 5: Dueling DQN](#section-5-dueling-dqn)\n",
        "7. [Section 6: Comprehensive Analysis](#section-6-comprehensive-analysis)\n",
        "8. [Section 7: Advanced Experiments](#section-7-advanced-experiments)\n",
        "9. [Conclusion](#conclusion)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n",
        "\n",
        "Let's begin by setting up the environment and importing all necessary libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Initial Configuration\n",
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Add project path\n",
        "sys.path.append(os.path.dirname(os.path.abspath('__file__')))\n",
        "\n",
        "# Core scientific libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Deep learning libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "def set_seed(seed=42):\n",
        "    \"\"\"Set seed for reproducible results\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "# Configure matplotlib\n",
        "plt.style.use('default')\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 12\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ Initial setup completed!\")\n",
        "print(f\"üîß Using device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(f\"üêç PyTorch version: {torch.__version__}\")\n",
        "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
        "print(f\"üéÆ Gymnasium version: {gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import DQN Agent Modules\n",
        "try:\n",
        "    # Import core DQN modules\n",
        "    from agents.core import DQNAgent, ReplayBuffer, QNetwork\n",
        "    from agents.double_dqn import DoubleDQNAgent\n",
        "    from agents.dueling_dqn import DuelingDQNAgent\n",
        "    \n",
        "    # Import utility modules\n",
        "    from agents.utils import QNetworkVisualization, PerformanceAnalyzer\n",
        "    \n",
        "    print(\"‚úÖ Core DQN modules imported successfully!\")\n",
        "    print(\"ü§ñ Available agents: DQNAgent, DoubleDQNAgent, DuelingDQNAgent\")\n",
        "    print(\"üìä Available tools: QNetworkVisualization, PerformanceAnalyzer\")\n",
        "    \n",
        "except ImportError as e:\n",
        "    print(f\"‚ùå Error importing DQN modules: {e}\")\n",
        "    print(\"üîß Please ensure all agent modules are available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create Environment for Testing\n",
        "print(\"üóÇÔ∏è Setting up test environment...\")\n",
        "env = gym.make('CartPole-v1')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "\n",
        "print(f\"Environment: CartPole-v1\")\n",
        "print(f\"State dimension: {state_dim}\")\n",
        "print(f\"Action dimension: {action_dim}\")\n",
        "print(f\"Goal: Balance pole for maximum steps (up to 500)\")\n",
        "print(f\"Target Performance: Average reward ‚â• 195\")\n",
        "\n",
        "# Test environment reset\n",
        "state, info = env.reset()\n",
        "print(f\"‚úÖ Environment ready! Initial state shape: {state.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 1: Theoretical Foundations\n",
        "\n",
        "### 1.1 Introduction to Deep Q-Learning\n",
        "\n",
        "Deep Q-Networks (DQN) represent a breakthrough in combining neural networks with Q-learning to handle high-dimensional state spaces. Traditional Q-learning maintains a lookup table for Q-values, but this quickly becomes impractical for complex environments.\n",
        "\n",
        "### 1.2 Mathematical Foundation\n",
        "\n",
        "The Q-learning algorithm learns the action-value function Q(s,a) which represents the expected cumulative reward:\n",
        "\n",
        "$$Q^\\pi(s,a) = \\mathbb{E}_{\\pi}\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0=s, a_0=a\\right]$$\n",
        "\n",
        "The optimal Q-function satisfies the Bellman optimality equation:\n",
        "\n",
        "$$Q^*(s,a) = \\mathbb{E}_{s'}\\left[r + \\gamma \\max_{a'} Q^*(s',a')\\right]$$\n",
        "\n",
        "DQN approximates this function using a neural network Q(s,a;Œ∏) trained to minimize:\n",
        "\n",
        "$$\\mathcal{L}(\\theta) = \\mathbb{E}\\,[(r + \\gamma \\max_{a'} Q(s',a';\\theta^-) - Q(s,a;\\theta))^2]$$\n",
