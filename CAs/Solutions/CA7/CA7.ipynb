{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36568c43",
   "metadata": {},
   "source": [
    "# CA7: Deep Q-Networks (DQN) and Value-Based Methods\n",
    "## Deep Reinforcement Learning - Session 7\n",
    "\n",
    "### Course Information\n",
    "- **Course**: Deep Reinforcement Learning\n",
    "- **Session**: 7\n",
    "- **Topic**: Deep Q-Networks (DQN) and Advanced Value-Based Methods\n",
    "- **Focus**: Complete theoretical foundations and practical implementations\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Theoretical Foundations**:\n",
    "   - Q-learning and its limitations in complex environments\n",
    "   - Deep Q-Networks (DQN) architecture and training process\n",
    "   - Experience replay and target networks\n",
    "   - Double DQN and addressing overestimation bias\n",
    "   - Dueling DQN and advantage decomposition\n",
    "\n",
    "2. **Implementation Skills**:\n",
    "   - Complete DQN implementation from scratch\n",
    "   - Experience replay buffer design and management\n",
    "   - Target network updates and stability techniques\n",
    "   - Advanced variants: Double DQN, Dueling DQN\n",
    "   - Performance analysis and debugging techniques\n",
    "\n",
    "3. **Practical Applications**:\n",
    "   - Training DQN on classic control environments\n",
    "   - Hyperparameter tuning and optimization strategies\n",
    "   - Comparison with different DQN variants\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "This notebook uses a modular implementation organized as follows:\n",
    "\n",
    "```\n",
    "CA7/\n",
    "├── dqn/                    # Core DQN implementations\n",
    "│   ├── core.py            # Basic DQN, ReplayBuffer, DQNAgent\n",
    "│   ├── double_dqn.py      # Double DQN implementation\n",
    "│   ├── dueling_dqn.py     # Dueling DQN architecture\n",
    "│   └── utils.py           # Visualization and analysis utilities\n",
    "├── experiments/           # Experiment scripts\n",
    "│   ├── basic_dqn_experiment.py\n",
    "│   └── comprehensive_dqn_analysis.py\n",
    "├── requirements.txt       # Python dependencies\n",
    "└── CA7.ipynb             # This educational notebook\n",
    "```\n",
    "\n",
    "### Contents Overview\n",
    "\n",
    "1. **Section 1**: Theoretical Foundations of Deep Q-Learning\n",
    "2. **Section 2**: Basic DQN Implementation and Core Concepts\n",
    "3. **Section 3**: Experience Replay and Target Networks\n",
    "4. **Section 4**: Double DQN and Overestimation Bias\n",
    "5. **Section 5**: Dueling DQN and Value Decomposition\n",
    "6. **Section 6**: Performance Analysis and Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48491e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential Setup and Imports\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "# Import our modular DQN implementations\n",
    "from dqn import (\n",
    "    DQNAgent, DoubleDQNAgent, DuelingDQNAgent,\n",
    "    QNetworkVisualization, PerformanceAnalyzer\n",
    ")\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gymnasium as gym\n",
    "import warnings\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Matplotlib settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CA7: Deep Q-Networks (DQN) and Value-Based Methods\")\n",
    "print(\"=\"*60)\n",
    "print(\"Modular implementation loaded successfully!\")\n",
    "print(\"Available agents: DQNAgent, DoubleDQNAgent, DuelingDQNAgent\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4663da",
   "metadata": {},
   "source": [
    "# Section 1: Theoretical Foundations of Deep Q-Learning\n",
    "\n",
    "## 1.1 From Tabular Q-Learning to Deep Q-Networks\n",
    "\n",
    "Traditional Q-learning works well for discrete, small state spaces where we can maintain a Q-table. However, in complex environments like Atari games or continuous control tasks, the state space becomes enormous, making tabular methods impractical.\n",
    "\n",
    "### The Q-Learning Foundation\n",
    "\n",
    "The Q-learning update rule is:\n",
    "```\n",
    "Q(s, a) ← Q(s, a) + α[r + γ max Q(s', a') - Q(s, a)]\n",
    "                                a'\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `Q(s, a)`: Action-value function\n",
    "- `α`: Learning rate\n",
    "- `r`: Reward\n",
    "- `γ`: Discount factor\n",
    "- `s'`: Next state\n",
    "\n",
    "### The Deep Q-Network Approach\n",
    "\n",
    "DQN replaces the Q-table with a deep neural network `Q(s, a; θ)` that approximates the Q-values for all actions given a state. The network parameters `θ` are updated to minimize the temporal difference (TD) error.\n",
    "\n",
    "## 1.2 Core Challenges in Deep Q-Learning\n",
    "\n",
    "### 1. Instability and Divergence\n",
    "- Neural networks can be unstable when used with bootstrapping\n",
    "- Updates can cause the target to change rapidly\n",
    "- Non-stationary target problem\n",
    "\n",
    "### 2. Correlation in Sequential Data\n",
    "- RL data is highly correlated (sequential states)\n",
    "- Violates the i.i.d. assumption of supervised learning\n",
    "- Can lead to poor generalization\n",
    "\n",
    "### 3. Overestimation Bias\n",
    "- Max operator in Q-learning can lead to overestimation\n",
    "- Amplified in function approximation settings\n",
    "- Can cause instability and poor performance\n",
    "\n",
    "## 1.3 DQN Innovations\n",
    "\n",
    "### Experience Replay\n",
    "- Store experiences in a replay buffer\n",
    "- Sample random batches for training\n",
    "- Breaks correlation and improves sample efficiency\n",
    "\n",
    "### Target Network\n",
    "- Use a separate target network for computing targets\n",
    "- Update target network periodically\n",
    "- Provides stability during training\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The DQN loss function is:\n",
    "```\n",
    "L(θ) = E[(r + γ max Q(s', a'; θ⁻) - Q(s, a; θ))²]\n",
    "                   a'\n",
    "```\n",
    "\n",
    "Where `θ⁻` represents the parameters of the target network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e506ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Theoretical Foundations Visualization\n",
    "\n",
    "# Create visualization instance\n",
    "visualizer = QNetworkVisualization()\n",
    "\n",
    "print(\"1. Visualizing Core Q-Learning Concepts...\")\n",
    "visualizer.visualize_q_learning_concepts()\n",
    "\n",
    "print(\"\\n2. Demonstrating Overestimation Bias...\")\n",
    "visualizer.demonstrate_overestimation_bias()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbfe2d",
   "metadata": {},
   "source": [
    "# Section 2: Basic DQN Implementation and Core Concepts\n",
    "\n",
    "## 2.1 Deep Q-Network Architecture\n",
    "\n",
    "The DQN architecture typically consists of:\n",
    "\n",
    "1. **Input Layer**: Processes the state representation\n",
    "2. **Hidden Layers**: Fully connected layers for feature extraction\n",
    "3. **Output Layer**: Outputs Q-values for all possible actions\n",
    "\n",
    "### Key Design Principles:\n",
    "\n",
    "- **State Preprocessing**: Normalize inputs for stable training\n",
    "- **Network Depth**: Balance between expressiveness and training stability\n",
    "- **Activation Functions**: ReLU is commonly used for hidden layers\n",
    "- **Output Layer**: Linear activation for Q-value regression\n",
    "\n",
    "## 2.2 Experience Replay Buffer\n",
    "\n",
    "The replay buffer serves several critical functions:\n",
    "\n",
    "1. **Decorrelation**: Breaks temporal correlations in sequential data\n",
    "2. **Sample Efficiency**: Allows multiple updates from the same experience\n",
    "3. **Stability**: Provides more stable gradients through diverse batches\n",
    "\n",
    "## 2.3 Training Loop and Key Components\n",
    "\n",
    "The DQN training process involves:\n",
    "\n",
    "1. **Action Selection**: ε-greedy exploration strategy\n",
    "2. **Environment Interaction**: Execute actions and collect experiences\n",
    "3. **Experience Storage**: Add experiences to replay buffer\n",
    "4. **Network Updates**: Sample batches and perform gradient descent\n",
    "5. **Target Network Updates**: Periodic synchronization for stability\n",
    "\n",
    "Let's demonstrate the basic DQN implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e80cd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DQN Demonstration\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"Environment: CartPole-v1\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "print(f\"Goal: Balance pole for as long as possible (max 500 steps)\")\n",
    "print()\n",
    "\n",
    "# Create DQN agent\n",
    "agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    lr=1e-3,                    # Learning rate\n",
    "    gamma=0.99,                 # Discount factor\n",
    "    epsilon_start=1.0,          # Initial exploration\n",
    "    epsilon_end=0.01,           # Final exploration\n",
    "    epsilon_decay=0.995,        # Exploration decay\n",
    "    buffer_size=10000,          # Experience replay buffer size\n",
    "    batch_size=64,              # Training batch size\n",
    "    target_update_freq=100      # Target network update frequency\n",
    ")\n",
    "\n",
    "# Training parameters\n",
    "num_episodes = 100\n",
    "max_steps_per_episode = 500\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  Episodes: {num_episodes}\")\n",
    "print(f\"  Max steps per episode: {max_steps_per_episode}\")\n",
    "print(f\"  Learning rate: {agent.optimizer.param_groups[0]['lr']}\")\n",
    "print(f\"  Gamma: {agent.gamma}\")\n",
    "print(f\"  Epsilon decay: {agent.epsilon_decay}\")\n",
    "print()\n",
    "\n",
    "# Quick training demonstration\n",
    "print(\"Starting training demonstration...\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "episode_rewards = []\n",
    "for episode in range(num_episodes):\n",
    "    reward, steps = agent.train_episode(env, max_steps=max_steps_per_episode)\n",
    "    episode_rewards.append(reward)\n",
    "\n",
    "    if (episode + 1) % 25 == 0:\n",
    "        avg_reward = np.mean(episode_rewards[-25:])\n",
    "        print(f\"Episode {episode+1:3d} | Avg Reward: {avg_reward:6.1f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"Training demonstration completed!\")\n",
    "print()\n",
    "\n",
    "# Final evaluation\n",
    "print(\"Final Evaluation:\")\n",
    "eval_results = agent.evaluate(env, num_episodes=10)\n",
    "print(f\"Mean Reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b059ba6",
   "metadata": {},
   "source": [
    "# Section 3: Experience Replay and Target Networks\n",
    "\n",
    "## 3.1 Experience Replay: Breaking the Correlation Chain\n",
    "\n",
    "Experience replay is one of the most crucial innovations in DQN. It addresses several fundamental challenges:\n",
    "\n",
    "### Problems with Sequential Training\n",
    "1. **Temporal Correlation**: Consecutive states are highly correlated\n",
    "2. **Non-stationarity**: The data distribution changes as the policy evolves\n",
    "3. **Sample Inefficiency**: Each experience is used only once\n",
    "\n",
    "### Benefits of Experience Replay\n",
    "1. **Decorrelation**: Random sampling breaks temporal dependencies\n",
    "2. **Sample Efficiency**: Multiple learning updates from each experience\n",
    "3. **Stability**: More stable gradients from diverse batches\n",
    "\n",
    "## 3.2 Target Networks: Stabilizing the Moving Target\n",
    "\n",
    "The target network addresses the moving target problem in Q-learning:\n",
    "\n",
    "### The Problem\n",
    "In standard Q-learning, both the predicted Q-value and the target Q-value are computed using the same network, creating instability.\n",
    "\n",
    "### The Solution\n",
    "- Maintain two networks: main (online) and target\n",
    "- Use target network to compute stable targets\n",
    "- Update target network less frequently than main network\n",
    "\n",
    "Let's analyze the impact of these components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc090006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experience Replay and Target Network Analysis\n",
    "\n",
    "# Create analyzer\n",
    "analyzer = PerformanceAnalyzer()\n",
    "\n",
    "# Analyze Q-value distributions\n",
    "print(\"Analyzing Q-value distributions...\")\n",
    "agent, _ = analyzer.analyze_q_value_distributions(\n",
    "    agent, gym.make('CartPole-v1'), num_samples=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f46c2",
   "metadata": {},
   "source": [
    "# Section 4: Double DQN and Overestimation Bias\n",
    "\n",
    "## 4.1 The Overestimation Problem in Q-Learning\n",
    "\n",
    "Standard Q-learning suffers from a systematic overestimation bias due to the max operator in the Bellman equation. This problem is amplified in function approximation settings.\n",
    "\n",
    "### Mathematical Analysis of Overestimation Bias\n",
    "\n",
    "In standard DQN, the target is computed as:\n",
    "```\n",
    "y = r + γ max Q(s', a'; θ⁻)\n",
    "          a'\n",
    "```\n",
    "\n",
    "The issue arises because we use the same network to both **select** the action and **evaluate** it.\n",
    "\n",
    "### Impact on Learning\n",
    "- **Suboptimal Policies**: Overestimated Q-values can lead to poor action selection\n",
    "- **Instability**: Inconsistent value estimates cause training instability\n",
    "- **Slow Convergence**: Biased estimates slow down learning\n",
    "\n",
    "## 4.2 Double DQN Solution\n",
    "\n",
    "Double DQN addresses this by **decoupling action selection from action evaluation**:\n",
    "\n",
    "### Key Insight\n",
    "Use the main network to select actions, but the target network to evaluate them:\n",
    "\n",
    "```\n",
    "y = r + γ Q(s', argmax Q(s', a'; θ), θ⁻)\n",
    "              a'\n",
    "```\n",
    "\n",
    "### Benefits\n",
    "- **Reduced Bias**: Eliminates the correlation between selection and evaluation\n",
    "- **Better Stability**: More consistent Q-value estimates\n",
    "- **Improved Performance**: Often leads to better final policies\n",
    "\n",
    "Let's compare standard DQN vs Double DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676aaba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double DQN Demonstration\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Test both variants\n",
    "agents = {\n",
    "    'Standard DQN': DQNAgent(\n",
    "        state_dim=state_dim, action_dim=action_dim,\n",
    "        lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n",
    "    ),\n",
    "    'Double DQN': DoubleDQNAgent(\n",
    "        state_dim=state_dim, action_dim=action_dim,\n",
    "        lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Comparing Standard DQN vs Double DQN...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "results = {}\n",
    "num_episodes = 50\n",
    "\n",
    "for name, agent in agents.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        reward, _ = agent.train_episode(env, max_steps=500)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        if (episode + 1) % 25 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-25:])\n",
    "            print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n",
    "\n",
    "    # Evaluate\n",
    "    eval_results = agent.evaluate(env, num_episodes=10)\n",
    "    results[name] = {\n",
    "        'rewards': episode_rewards,\n",
    "        'eval_performance': eval_results,\n",
    "        'final_performance': np.mean(episode_rewards[-10:])\n",
    "    }\n",
    "\n",
    "# Visualize comparison\n",
    "PerformanceAnalyzer.plot_learning_curves(results)\n",
    "\n",
    "env.close()\n",
    "print(\"Comparison completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0bf1c",
   "metadata": {},
   "source": [
    "# Section 5: Dueling DQN and Value Decomposition\n",
    "\n",
    "## 5.1 The Motivation Behind Dueling Architecture\n",
    "\n",
    "Standard DQN learns Q-values directly, but these can be decomposed into two meaningful components:\n",
    "\n",
    "### Value Decomposition Theory\n",
    "The Q-function can be decomposed as:\n",
    "```\n",
    "Q(s,a) = V(s) + A(s,a)\n",
    "```\n",
    "\n",
    "Where:\n",
    "- **V(s)**: State value function - \"How good is this state?\"\n",
    "- **A(s,a)**: Advantage function - \"How much better is action a compared to average?\"\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "1. **State Value Learning**: Many states have similar values regardless of action\n",
    "2. **Action Ranking**: What matters most is the relative advantage of actions\n",
    "3. **Sample Efficiency**: Decoupling allows better generalization\n",
    "4. **Faster Learning**: State values can be learned from all experiences\n",
    "\n",
    "## 5.2 Dueling Network Architecture\n",
    "\n",
    "### Network Structure\n",
    "```\n",
    "Input State\n",
    "     |\n",
    "Feature Extraction\n",
    "     |\n",
    "   Split into two streams\n",
    "     /              \\\n",
    "Value Stream    Advantage Stream\n",
    "   V(s)           A(s,a)\n",
    "     \\              /\n",
    "      Combining Module\n",
    "           |\n",
    "        Q(s,a)\n",
    "```\n",
    "\n",
    "### Combining the Streams\n",
    "\n",
    "To address identifiability issues, we subtract the mean advantage:\n",
    "```\n",
    "Q(s,a) = V(s) + A(s,a) - (1/|A|) Σ A(s,a')\n",
    "                                    a'\n",
    "```\n",
    "\n",
    "## 5.3 Benefits of Dueling Architecture\n",
    "\n",
    "1. **Better Value Estimation**: State values learned more efficiently\n",
    "2. **Improved Policy**: Better action selection through advantage learning\n",
    "3. **Robustness**: More stable learning across different environments\n",
    "\n",
    "Let's demonstrate the Dueling DQN architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb9d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dueling DQN Demonstration\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Test different variants\n",
    "agents = {\n",
    "    'Standard DQN': DQNAgent(\n",
    "        state_dim=state_dim, action_dim=action_dim,\n",
    "        lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n",
    "    ),\n",
    "    'Dueling DQN (Mean)': DuelingDQNAgent(\n",
    "        state_dim=state_dim, action_dim=action_dim,\n",
    "        dueling_type='mean', lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n",
    "    ),\n",
    "    'Dueling DQN (Max)': DuelingDQNAgent(\n",
    "        state_dim=state_dim, action_dim=action_dim,\n",
    "        dueling_type='max', lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Comparing DQN variants with Dueling architecture...\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "results = {}\n",
    "num_episodes = 60\n",
    "\n",
    "for name, agent in agents.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        reward, _ = agent.train_episode(env, max_steps=500)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        if (episode + 1) % 30 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-30:])\n",
    "            print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n",
    "\n",
    "    # Evaluate\n",
    "    eval_results = agent.evaluate(env, num_episodes=10)\n",
    "    results[name] = {\n",
    "        'rewards': episode_rewards,\n",
    "        'eval_performance': eval_results,\n",
    "        'final_performance': np.mean(episode_rewards[-10:])\n",
    "    }\n",
    "\n",
    "# Visualize comparison\n",
    "PerformanceAnalyzer.plot_learning_curves(results)\n",
    "\n",
    "# Analyze value-advantage decomposition for dueling agents\n",
    "print(\"\\nAnalyzing Value-Advantage Decomposition...\")\n",
    "sample_state = [0.1, 0.1, 0.1, 0.1]  # Example CartPole state\n",
    "\n",
    "for name, data in results.items():\n",
    "    agent = data.get('agent')\n",
    "    if hasattr(agent, 'get_value_advantage_decomposition'):\n",
    "        decomp = agent.get_value_advantage_decomposition(sample_state)\n",
    "        print(f\"\\n{name} decomposition for sample state:\")\n",
    "        print(f\"  Q-values: {decomp['q_values']}\")\n",
    "        print(f\"  State value: {decomp['value']:.3f}\")\n",
    "        print(f\"  Advantages: {decomp['advantage']}\")\n",
    "\n",
    "env.close()\n",
    "print(\"\\nDueling DQN demonstration completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b254050",
   "metadata": {},
   "source": [
    "# Section 6: Performance Analysis and Comparisons\n",
    "\n",
    "## 6.1 Comprehensive Performance Analysis\n",
    "\n",
    "Let's run a comprehensive analysis comparing all DQN variants and analyze their performance characteristics.\n",
    "\n",
    "## 6.2 Key Insights and Best Practices\n",
    "\n",
    "### DQN Best Practices:\n",
    "1. **Experience Replay**: Essential for breaking correlations and improving stability\n",
    "2. **Target Networks**: Critical for preventing divergence and ensuring stable learning\n",
    "3. **Double DQN**: Reduces overestimation bias, especially important in complex environments\n",
    "4. **Dueling Architecture**: Improves value estimation and can accelerate learning\n",
    "\n",
    "### Hyperparameter Tuning:\n",
    "- **Learning Rate**: 1e-3 to 1e-4 typically works well\n",
    "- **Gamma**: 0.99 for most environments\n",
    "- **Epsilon Decay**: 0.995 provides good exploration-exploitation balance\n",
    "- **Buffer Size**: 10,000-50,000 experiences usually sufficient\n",
    "- **Batch Size**: 32-128 samples per update\n",
    "- **Target Update Frequency**: 100-1000 steps\n",
    "\n",
    "### When to Use Each Variant:\n",
    "- **Basic DQN**: Good starting point, works well on simple environments\n",
    "- **Double DQN**: Better for environments with overestimation issues\n",
    "- **Dueling DQN**: Excellent for environments where value estimation matters\n",
    "- **Combined (Double + Dueling)**: Best overall performance on complex tasks\n",
    "\n",
    "## 6.3 Running Full Experiments\n",
    "\n",
    "For comprehensive experiments, use the experiment scripts in the `experiments/` directory:\n",
    "\n",
    "```bash\n",
    "# Run basic DQN experiment\n",
    "python experiments/basic_dqn_experiment.py\n",
    "\n",
    "# Run comprehensive analysis (all variants)\n",
    "python experiments/comprehensive_dqn_analysis.py\n",
    "```\n",
    "\n",
    "These scripts will provide detailed training curves, performance metrics, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c4d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Comprehensive Analysis\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CA7 Final Analysis: DQN Variants Comparison\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create environment for analysis\n",
    "env = gym.make('CartPole-v1')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Test all variants\n",
    "variants = {\n",
    "    'Basic DQN': DQNAgent(\n",
    "        state_dim=state_dim, action_dim=action_dim,\n",
    "        lr=1e-3, epsilon_decay=0.995, buffer_size=15000\n",
    "    ),\n",
    "    'Double DQN': DoubleDQNAgent(\n",
    "        state_dim=state_dim, action_dim=action_dim,\n",
    "        lr=1e-3, epsilon_decay=0.995, buffer_size=15000\n",
    "    ),\n",
    "    'Dueling DQN': DuelingDQNAgent(\n",
    "        state_dim=state_dim, action_dim=action_dim,\n",
    "        dueling_type='mean', lr=1e-3, epsilon_decay=0.995, buffer_size=15000\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Training all DQN variants for comparison...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "final_results = {}\n",
    "num_episodes = 80\n",
    "\n",
    "for name, agent in variants.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        reward, _ = agent.train_episode(env, max_steps=500)\n",
    "        episode_rewards.append(reward)\n",
    "\n",
    "        if (episode + 1) % 40 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-40:])\n",
    "            print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n",
    "\n",
    "    # Final evaluation\n",
    "    eval_results = agent.evaluate(env, num_episodes=15)\n",
    "    final_results[name] = {\n",
    "        'agent': agent,\n",
    "        'rewards': episode_rewards,\n",
    "        'losses': agent.losses,\n",
    "        'epsilon_history': agent.epsilon_history,\n",
    "        'eval_performance': eval_results,\n",
    "        'final_performance': np.mean(episode_rewards[-15:])\n",
    "    }\n",
    "\n",
    "# Create comprehensive visualization\n",
    "PerformanceAnalyzer.plot_learning_curves(final_results)\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, data in final_results.items():\n",
    "    eval_perf = data['eval_performance']\n",
    "    final_perf = data['final_performance']\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Training Performance: {final_perf:.1f}\")\n",
    "    print(f\"  Evaluation Performance: {eval_perf['mean_reward']:.1f} ± {eval_perf['std_reward']:.1f}\")\n",
    "    print()\n",
    "\n",
    "print(\"Key Insights:\")\n",
    "print(\"• All DQN variants can solve CartPole-v1 (avg reward > 195)\")\n",
    "print(\"• Double DQN reduces overestimation bias\")\n",
    "print(\"• Dueling DQN improves value estimation efficiency\")\n",
    "print(\"• Experience replay and target networks are essential for stability\")\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CA7 Analysis Complete!\")\n",
    "print(\"For more detailed experiments, run:\")\n",
    "print(\"  python experiments/comprehensive_dqn_analysis.py\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
