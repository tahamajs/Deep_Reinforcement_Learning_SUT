{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Abstract](#abstract)\n",
    "2. [1. Introduction](#1-introduction)\n",
    "   - [1.1 Motivation](#11-motivation)\n",
    "   - [1.2 Key Contributions](#12-key-contributions)\n",
    "   - [1.3 Organization](#13-organization)\n",
    "3. [2. Theoretical Foundations](#2-theoretical-foundations)\n",
    "   - [2.1 Q-Learning and Function Approximation](#21-q-learning-and-function-approximation)\n",
    "   - [2.2 Deep Neural Networks in RL](#22-deep-neural-networks-in-rl)\n",
    "   - [2.3 Experience Replay and Target Networks](#23-experience-replay-and-target-networks)\n",
    "4. [3. Basic DQN Implementation and Core Concepts](#3-basic-dqn-implementation-and-core-concepts)\n",
    "   - [3.1 DQN Algorithm](#31-dqn-algorithm)\n",
    "   - [3.2 Network Architecture](#32-network-architecture)\n",
    "   - [3.3 Training Procedure](#33-training-procedure)\n",
    "5. [4. Experience Replay and Target Networks](#4-experience-replay-and-target-networks)\n",
    "   - [4.1 Experience Replay Mechanism](#41-experience-replay-mechanism)\n",
    "   - [4.2 Target Network Implementation](#42-target-network-implementation)\n",
    "   - [4.3 Stability Improvements](#43-stability-improvements)\n",
    "6. [5. Double DQN and Overestimation Bias](#5-double-dqn-and-overestimation-bias)\n",
    "   - [5.1 Overestimation Problem](#51-overestimation-problem)\n",
    "   - [5.2 Double DQN Solution](#52-double-dqn-solution)\n",
    "   - [5.3 Implementation and Analysis](#53-implementation-and-analysis)\n",
    "7. [6. Dueling DQN and Value Decomposition](#6-dueling-dqn-and-value-decomposition)\n",
    "   - [6.1 Value-Advantage Decomposition](#61-value-advantage-decomposition)\n",
    "   - [6.2 Dueling Architecture](#62-dueling-architecture)\n",
    "   - [6.3 Implementation Details](#63-implementation-details)\n",
    "8. [7. Experimental Results and Comparisons](#7-experimental-results-and-comparisons)\n",
    "   - [7.1 Environment Setup](#71-environment-setup)\n",
    "   - [7.2 Performance Comparison](#72-performance-comparison)\n",
    "   - [7.3 Ablation Studies](#73-ablation-studies)\n",
    "9. [8. Results and Discussion](#8-results-and-discussion)\n",
    "   - [8.1 Summary of Findings](#81-summary-of-findings)\n",
    "   - [8.2 Theoretical Contributions](#82-theoretical-contributions)\n",
    "   - [8.3 Practical Implications](#83-practical-implications)\n",
    "   - [8.4 Limitations and Future Work](#84-limitations-and-future-work)\n",
    "   - [8.5 Conclusions](#85-conclusions)\n",
    "10. [References](#references)\n",
    "11. [Appendix A: Implementation Details](#appendix-a-implementation-details)\n",
    "    - [A.1 Modular Architecture](#a1-modular-architecture)\n",
    "    - [A.2 Code Quality Features](#a2-code-quality-features)\n",
    "    - [A.3 Performance Considerations](#a3-performance-considerations)\n",
    "\n",
    "---\n",
    "\n",
    "# Computer Assignment 7: Deep Q-Networks and Value-Based Methods\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This assignment presents a comprehensive study of Deep Q-Networks (DQN) and advanced value-based reinforcement learning methods. We examine the theoretical foundations, implementation details, and performance characteristics of various DQN variants including basic DQN, Double DQN, and Dueling DQN. The analysis demonstrates the effectiveness of these methods on classic control environments and provides insights into their comparative performance. Our modular implementation achieves state-of-the-art results on CartPole-v1, with Double DQN and Dueling DQN showing significant improvements over the baseline approach. Through systematic experimentation, we demonstrate the convergence properties, sample efficiency, and practical trade-offs of different DQN variants, providing insights into algorithm selection for various reinforcement learning scenarios.\n",
    "\n",
    "**Keywords:** Deep reinforcement learning, Q-learning, Deep Q-networks, value-based methods, experience replay, target networks, Double DQN, Dueling DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. INTRODUCTION\n",
    "\n",
    "### A. Motivation\n",
    "\n",
    "Deep Q-Networks (DQN) represent a significant advancement in reinforcement learning, successfully combining deep neural networks with Q-learning to solve complex decision-making problems. \n",
    "Traditional Q-learning methods face significant limitations when dealing with high-dimensional state spaces. \n",
    "The exponential growth of state-action pairs makes tabular methods impractical for complex environments such as Atari games or continuous control tasks.\n",
    "\n",
    "### B. Key Contributions\n",
    "\n",
    "The main contributions of this work include:\n",
    "\n",
    "1. **Theoretical Analysis**: Comprehensive examination of DQN variants and their mathematical foundations\n",
    "2. **Implementation**: Modular implementation of DQN, Double DQN, and Dueling DQN algorithms\n",
    "3. **Performance Evaluation**: Comparative analysis of different DQN variants on standard benchmarks\n",
    "4. **Practical Insights**: Guidelines for hyperparameter tuning and best practices\n",
    "\n",
    "### C. Organization\n",
    "\n",
    "This paper is organized as follows:\n",
    "- **Section II**: Theoretical foundations and mathematical formulation\n",
    "- **Section III**: Basic DQN implementation and core concepts\n",
    "- **Section IV**: Experience replay and target networks\n",
    "- **Section V**: Double DQN and overestimation bias\n",
    "- **Section VI**: Dueling DQN and value decomposition\n",
    "- **Section VII**: Experimental results and comparisons\n",
    "- **Section VIII**: Conclusions and future work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. THEORETICAL FOUNDATIONS\n",
    "\n",
    "### A. Problem Formulation\n",
    "\n",
    "We consider a Markov Decision Process (MDP) defined by the tuple \\\\((\\\\mathcal{S}, \\\\mathcal{A}, \\\\mathcal{P}, \\\\mathcal{R}, \\\\gamma)\\\\), where:\n",
    "- \\\\(\\\\mathcal{S}\\\\) is the state space\n",
    "- \\\\(\\\\mathcal{A}\\\\) is the action space\n",
    "- \\\\(\\\\mathcal{P}: \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\times \\\\mathcal{S} \\\\rightarrow [0,1]\\\\) is the transition probability function\n",
    "- \\\\(\\\\mathcal{R}: \\\\mathcal{S} \\\\times \\\\mathcal{A} \\\\rightarrow \\\\mathbb{R}\\\\) is the reward function\n",
    "- \\\\(\\\\gamma \\\\in [0,1)\\\\) is the discount factor\n",
    "\n",
    "The objective is to learn an optimal policy \\\\(\\\\pi^*\\\\) that maximizes the expected cumulative reward:\n",
    "\\\\[\n",
    "\\\\pi^* = \\\\arg\\\\max_{\\\\pi} \\\\mathbb{E}\\\\left[\\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r_t \\\\mid \\\\pi\\\\right]\n",
    "\\\\]\n",
    "\n",
    "### B. Q-Learning Foundation\n",
    "\n",
    "The Q-learning algorithm learns the action-value function \\\\(Q^\\\\pi(s,a)\\\\) defined as:\n",
    "\\\\[\n",
    "Q^\\\\pi(s,a) = \\\\mathbb{E}_{\\\\pi}\\\\left[\\\\sum_{t=0}^{\\\\infty} \\\\gamma^t r_t \\\\mid s_0=s, a_0=a\\\\right]\n",
    "\\\\]\n",
    "\n",
    "The optimal Q-function satisfies the Bellman optimality equation:\n",
    "\\\\[\n",
    "Q^*(s,a) = \\\\mathbb{E}_{s'}\\\\left[r + \\\\gamma \\\\max_{a'} Q^*(s',a')\\\\right]\n",
    "\\\\]\n",
    "\n",
    "### C. Deep Q-Network Architecture\n",
    "\n",
    "DQN approximates the Q-function using a deep neural network \\\\(Q(s,a;\\\\theta)\\\\) with parameters \\\\(\\\\theta\\\\). The network is trained to minimize the temporal difference (TD) error:\n",
    "\\\\[\n",
    "\\\\mathcal{L}(\\\\theta) = \\\\mathbb{E}_{(s,a,r,s') \\\\sim \\\\mathcal{D}}\\\\left[\\\\left(y - Q(s,a;\\\\theta)\\\\right)^2\\\\right]\n",
    "\\\\]\n",
    "\n",
    "where \\\\(y = r + \\\\gamma \\\\max_{a'} Q(s',a';\\\\theta^-)\\\\) and \\\\(\\\\theta^-\\\\) represents the parameters of the target network.\n",
    "\n",
    "### D. Key Innovations\n",
    "\n",
    "**Experience Replay**: Store experiences \\\\((s,a,r,s')\\\\) in a replay buffer \\\\(\\\\mathcal{D}\\\\) and sample random minibatches for training. This breaks temporal correlations and improves sample efficiency.\n",
    "\n",
    "**Target Networks**: Maintain a separate target network with parameters \\\\(\\\\theta^-\\\\) that are periodically updated from the main network. This provides stability during training by preventing the target from changing too rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. SETUP AND IMPORTS\n",
    "\n",
    "We import the necessary modules from our modular implementation. All DQN algorithms are implemented in separate Python files for better code organization and reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gymnasium as gym\nimport warnings\nsys.path.append(os.path.dirname(os.path.abspath('.')))\nfrom agents.core import DQNAgent\nfrom agents.double_dqn import DoubleDQNAgent\nfrom agents.dueling_dqn import DuelingDQNAgent\nfrom agents.utils import QNetworkVisualization, PerformanceAnalyzer\nnp.random.seed(42)\nplt.style.use('seaborn-v0_8')\nplt.rcParams['figure.figsize'] = (12, 8)\nplt.rcParams['font.size'] = 12\nwarnings.filterwarnings('ignore')\nprint(\"=\" * 60)\nprint(\"Deep Q-Networks (DQN) - Comprehensive Analysis\")\nprint(\"=\" * 60)\nprint(\"Modules loaded successfully!\")\nprint(\"Available agents: DQNAgent, DoubleDQNAgent, DuelingDQNAgent\")\nprint(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. THEORETICAL CONCEPTS VISUALIZATION\n",
    "\n",
    "### A. Q-Learning Fundamentals\n",
    "\n",
    "We visualize the core concepts of Q-learning including Q-value updates, experience replay benefits, target network updates, and exploration strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = QNetworkVisualization()\nprint(\"Visualizing Core Q-Learning Concepts...\")\nvisualizer.visualize_q_learning_concepts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Overestimation Bias in Q-Learning\n",
    "\n",
    "One of the key challenges in Q-learning is the overestimation bias introduced by the max operator in the Bellman equation. This bias occurs because we use the same network to both select and evaluate actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Network Architecture Visualization\n",
    "\n",
    "Understanding the structure and information flow in DQN networks is crucial for debugging and optimization. We visualize network architectures and their learned representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_network_architecture():\n    print(\"=\" * 60)\n    print(\"Network Architecture Analysis\")\n    print(\"=\" * 60)\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    agents = {\n        'Basic DQN': DQNAgent(state_dim=state_dim, action_dim=action_dim, lr=1e-3),\n        'Double DQN': DoubleDQNAgent(state_dim=state_dim, action_dim=action_dim, lr=1e-3),\n        'Dueling DQN': DuelingDQNAgent(state_dim=state_dim, action_dim=action_dim, lr=1e-3)\n    }\n    fig = plt.figure(figsize=(18, 12))\n    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n    fig.suptitle('DQN Network Architecture Analysis', fontsize=16, fontweight='bold')\n    ax = fig.add_subplot(gs[0, 0])\n    param_counts = {}\n    for name, agent in agents.items():\n        total_params = sum(p.numel() for p in agent.q_network.parameters())\n        trainable_params = sum(p.numel() for p in agent.q_network.parameters() if p.requires_grad)\n        param_counts[name] = {'total': total_params, 'trainable': trainable_params}\n    names = list(param_counts.keys())\n    total_counts = [param_counts[n]['total'] for n in names]\n    trainable_counts = [param_counts[n]['trainable'] for n in names]\n    x = np.arange(len(names))\n    width = 0.35\n    ax.bar(x - width/2, total_counts, width, label='Total', alpha=0.8, color='lightblue')\n    ax.bar(x + width/2, trainable_counts, width, label='Trainable', alpha=0.8, color='darkblue')\n    ax.set_ylabel('Number of Parameters')\n    ax.set_title('Model Parameter Count')\n    ax.set_xticks(x)\n    ax.set_xticklabels(names, rotation=15, ha='right')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[0, 1])\n    agent = agents['Basic DQN']\n    layer_params = {}\n    for name, param in agent.q_network.named_parameters():\n        layer_name = name.split('.')[0]\n        if layer_name not in layer_params:\n            layer_params[layer_name] = 0\n        layer_params[layer_name] += param.numel()\n    ax.pie(layer_params.values(), labels=layer_params.keys(), autopct='%1.1f%%', startangle=90)\n    ax.set_title('Parameter Distribution (Basic DQN)')\n    ax = fig.add_subplot(gs[0, 2])\n    all_weights = []\n    for param in agents['Basic DQN'].q_network.parameters():\n        if param.requires_grad and len(param.shape) > 1:\n            all_weights.extend(param.detach().cpu().numpy().flatten())\n    ax.hist(all_weights, bins=50, alpha=0.7, color='purple', edgecolor='black', density=True)\n    ax.axvline(0, color='red', linestyle='--', alpha=0.7, label='Zero')\n    ax.set_title('Initial Weight Distribution (Basic DQN)')\n    ax.set_xlabel('Weight Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, :])\n    random_states = [env.reset()[0] for _ in range(100)]\n    random_states = np.array(random_states)\n    for name, agent in agents.items():\n        q_values = []\n        for state in random_states:\n            q_vals = agent.get_q_values(state)\n            q_values.append(q_vals)\n        q_values = np.array(q_values)\n        for action in range(action_dim):\n            ax.hist(q_values[:, action], bins=20, alpha=0.3, \n                   label=f'{name} - Action {action}', density=True)\n    ax.set_title('Q-Value Distributions on Random States (Before Training)')\n    ax.set_xlabel('Q-Value')\n    ax.set_ylabel('Density')\n    ax.legend(loc='upper right', fontsize=8)\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, 0])\n    sample_state = env.reset()[0]\n    agent = agents['Basic DQN']\n    import torch\n    state_tensor = torch.FloatTensor(sample_state).unsqueeze(0).to(agent.device)\n    activations = []\n    def hook_fn(module, input, output):\n        activations.append(output.detach().cpu().numpy())\n    hooks = []\n    for layer in agent.q_network.children():\n        if isinstance(layer, torch.nn.Linear):\n            hooks.append(layer.register_forward_hook(hook_fn))\n    with torch.no_grad():\n        _ = agent.q_network(state_tensor)\n    for hook in hooks:\n        hook.remove()\n    if activations:\n        act = activations[0].flatten()\n        ax.bar(range(len(act)), act, alpha=0.7, color='green')\n        ax.set_title('First Layer Activations (Sample State)')\n        ax.set_xlabel('Neuron Index')\n        ax.set_ylabel('Activation Value')\n        ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, 1])\n    base_state = env.reset()[0]\n    perturbations = np.linspace(-0.5, 0.5, 20)\n    q_changes = {f'Action {i}': [] for i in range(action_dim)}\n    for pert in perturbations:\n        perturbed_state = base_state.copy()\n        perturbed_state[0] += pert\n        q_vals = agents['Basic DQN'].get_q_values(perturbed_state)\n        for i in range(action_dim):\n            q_changes[f'Action {i}'].append(q_vals[i])\n    for action_name, q_vals in q_changes.items():\n        ax.plot(perturbations, q_vals, marker='o', label=action_name, linewidth=2)\n    ax.axvline(0, color='red', linestyle='--', alpha=0.5, label='Original')\n    ax.set_title('Q-Value Sensitivity to State Perturbation')\n    ax.set_xlabel('Perturbation (First State Dimension)')\n    ax.set_ylabel('Q-Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, 2])\n    complexity_metrics = {}\n    for name, agent in agents.items():\n        total_params = sum(p.numel() for p in agent.q_network.parameters())\n        memory_mb = (total_params * 4) / (1024 * 1024)\n        import time\n        state_tensor = torch.FloatTensor(base_state).unsqueeze(0).to(agent.device)\n        start_time = time.time()\n        for _ in range(100):\n            with torch.no_grad():\n                _ = agent.q_network(state_tensor)\n        elapsed = (time.time() - start_time) * 1000\n        complexity_metrics[name] = {\n            'Memory (MB)': memory_mb,\n            'Time (ms/100)': elapsed\n        }\n    metrics = ['Memory (MB)', 'Time (ms/100)']\n    x = np.arange(len(agents))\n    width = 0.35\n    for i, metric in enumerate(metrics):\n        values = [complexity_metrics[name][metric] for name in agents.keys()]\n        normalized_values = np.array(values) / max(values)\n        ax.bar(x + i * width, normalized_values, width, label=metric, alpha=0.8)\n    ax.set_ylabel('Normalized Value')\n    ax.set_title('Network Complexity Comparison')\n    ax.set_xticks(x + width / 2)\n    ax.set_xticklabels(agents.keys(), rotation=15, ha='right')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.show()\n    env.close()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Network architecture analysis completed!\")\n    print(\"=\" * 60)\n    return complexity_metrics\nnetwork_analysis = visualize_network_architecture()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Demonstrating Overestimation Bias...\")\nvisualizer.demonstrate_overestimation_bias()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. BASIC DQN IMPLEMENTATION\n",
    "\n",
    "### A. Algorithm Description\n",
    "\n",
    "The basic DQN algorithm consists of the following steps:\n",
    "\n",
    "1. **Initialize** replay buffer \\\\(\\\\mathcal{D}\\\\), Q-network \\\\(Q(s,a;\\\\theta)\\\\), and target network \\\\(Q(s,a;\\\\theta^-)\\\\)\n",
    "2. **For each episode**:\n",
    "   - Observe initial state \\\\(s\\\\)\n",
    "   - **For each timestep**:\n",
    "     - Select action \\\\(a\\\\) using \\\\(\\\\epsilon\\\\)-greedy policy\n",
    "     - Execute action, observe reward \\\\(r\\\\) and next state \\\\(s'\\\\)\n",
    "     - Store transition \\\\((s,a,r,s')\\\\) in \\\\(\\\\mathcal{D}\\\\)\n",
    "     - Sample minibatch from \\\\(\\\\mathcal{D}\\\\)\n",
    "     - Update Q-network by minimizing \\\\(\\\\mathcal{L}(\\\\theta)\\\\)\n",
    "     - Periodically update target network: \\\\(\\\\theta^- \\\\leftarrow \\\\theta\\\\)\n",
    "\n",
    "### B. Training Demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nprint(f\"Environment: CartPole-v1\")\nprint(f\"State dimension: {state_dim}\")\nprint(f\"Action dimension: {action_dim}\")\nprint()\nagent = DQNAgent(\n    state_dim=state_dim,\n    action_dim=action_dim,\n    lr=1e-3,\n    gamma=0.99,\n    epsilon_start=1.0,\n    epsilon_end=0.01,\n    epsilon_decay=0.995,\n    buffer_size=10000,\n    batch_size=64,\n    target_update_freq=100,\n)\nnum_episodes = 100\nmax_steps_per_episode = 500\nprint(\"Training Basic DQN...\")\nprint(\"-\" * 40)\nepisode_rewards = []\nfor episode in range(num_episodes):\n    reward, steps = agent.train_episode(env, max_steps=max_steps_per_episode)\n    episode_rewards.append(reward)\n    if (episode + 1) % 25 == 0:\n        avg_reward = np.mean(episode_rewards[-25:])\n        print(f\"Episode {episode+1:3d} | Avg Reward: {avg_reward:6.1f} | Epsilon: {agent.epsilon:.3f}\")\nprint(\"-\" * 40)\nprint(\"\\nEvaluating trained agent...\")\neval_results = agent.evaluate(env, num_episodes=10)\nprint(f\"Mean Reward: {eval_results['mean_reward']:.2f} ± {eval_results['std_reward']:.2f}\")\nenv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Q-Value Analysis\n",
    "\n",
    "We analyze the learned Q-value distributions to understand the agent's learned value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = PerformanceAnalyzer()\nprint(\"Analyzing Q-value distributions...\")\nagent, analysis_results = analyzer.analyze_q_value_distributions(\n    agent, gym.make(\"CartPole-v1\"), num_samples=500\n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. DOUBLE DQN\n",
    "\n",
    "### A. Motivation and Theory\n",
    "\n",
    "Standard DQN suffers from a systematic overestimation bias due to the max operator in the Bellman equation. In standard DQN, the target is:\n",
    "\\\\[\n",
    "y = r + \\\\gamma \\\\max_{a'} Q(s',a';\\\\theta^-)\n",
    "\\\\]\n",
    "\n",
    "The issue arises because we use the same network to both select the action (argmax) and evaluate it (max).\n",
    "\n",
    "### B. Double DQN Solution\n",
    "\n",
    "Double DQN addresses this by decoupling action selection from action evaluation:\n",
    "\\\\[\n",
    "y = r + \\\\gamma Q(s', \\\\arg\\\\max_{a'} Q(s',a';\\\\theta); \\\\theta^-)\n",
    "\\\\]\n",
    "\n",
    "We use the main network \\\\(\\\\theta\\\\) to select the action and the target network \\\\(\\\\theta^-\\\\) to evaluate it.\n",
    "\n",
    "### C. Comparative Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nagents = {\n    'Standard DQN': DQNAgent(\n        state_dim=state_dim, action_dim=action_dim,\n        lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n    ),\n    'Double DQN': DoubleDQNAgent(\n        state_dim=state_dim, action_dim=action_dim,\n        lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n    )\n}\nprint(\"Comparing Standard DQN vs Double DQN...\")\nprint(\"=\" * 50)\nresults = {}\nnum_episodes = 50\nfor name, agent in agents.items():\n    print(f\"\\nTraining {name}...\")\n    episode_rewards = []\n    for episode in range(num_episodes):\n        reward, _ = agent.train_episode(env, max_steps=500)\n        episode_rewards.append(reward)\n        if (episode + 1) % 25 == 0:\n            avg_reward = np.mean(episode_rewards[-25:])\n            print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n    eval_results = agent.evaluate(env, num_episodes=10)\n    results[name] = {\n        'rewards': episode_rewards,\n        'losses': agent.losses,\n        'epsilon_history': agent.epsilon_history,\n        'eval_performance': eval_results,\n        'final_performance': np.mean(episode_rewards[-10:])\n    }\nPerformanceAnalyzer.plot_learning_curves(results)\nenv.close()\nprint(\"\\nComparison completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. DUELING DQN\n",
    "\n",
    "### A. Architecture and Theory\n",
    "\n",
    "Dueling DQN decomposes the Q-function into two components:\n",
    "\\\\[\n",
    "Q(s,a) = V(s) + A(s,a)\n",
    "\\\\]\n",
    "\n",
    "where:\n",
    "- \\\\(V(s)\\\\) is the state value function - \"How good is this state?\"\n",
    "- \\\\(A(s,a)\\\\) is the advantage function - \"How much better is action a?\"\n",
    "\n",
    "To address identifiability issues, we use the aggregation formula:\n",
    "\\\\[\n",
    "Q(s,a;\\\\theta,\\\\alpha,\\\\beta) = V(s;\\\\theta,\\\\beta) + \\\\left(A(s,a;\\\\theta,\\\\alpha) - \\\\frac{1}{|\\\\mathcal{A}|}\\\\sum_{a'}A(s,a';\\\\theta,\\\\alpha)\\\\right)\n",
    "\\\\]\n",
    "\n",
    "### B. Benefits\n",
    "\n",
    "1. **Better value estimation**: State values can be learned from all experiences\n",
    "2. **Improved generalization**: Decoupling allows better learning of state values\n",
    "3. **Faster convergence**: More efficient use of training data\n",
    "\n",
    "### C. Experimental Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nagents = {\n    'Standard DQN': DQNAgent(\n        state_dim=state_dim, action_dim=action_dim,\n        lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n    ),\n    'Dueling DQN (Mean)': DuelingDQNAgent(\n        state_dim=state_dim, action_dim=action_dim,\n        dueling_type='mean', lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n    )\n}\nprint(\"Comparing DQN variants with Dueling architecture...\")\nresults = {}\nnum_episodes = 60\nfor name, agent in agents.items():\n    print(f\"\\nTraining {name}...\")\n    episode_rewards = []\n    for episode in range(num_episodes):\n        reward, _ = agent.train_episode(env, max_steps=500)\n        episode_rewards.append(reward)\n        if (episode + 1) % 30 == 0:\n            avg_reward = np.mean(episode_rewards[-30:])\n            print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n    eval_results = agent.evaluate(env, num_episodes=10)\n    results[name] = {'rewards': episode_rewards, 'losses': agent.losses, 'epsilon_history': agent.epsilon_history, 'eval_performance': eval_results}\nPerformanceAnalyzer.plot_learning_curves(results)\nenv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. COMPREHENSIVE COMPARISON\n",
    "\n",
    "### A. Experimental Setup\n",
    "\n",
    "We compare all DQN variants on the CartPole-v1 environment with consistent hyperparameters.\n",
    "\n",
    "### B. Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"Final Comprehensive Analysis\")\nprint(\"=\" * 60)\nenv = gym.make('CartPole-v1')\nvariants = {\n    'Basic DQN': DQNAgent(state_dim=4, action_dim=2, lr=1e-3, epsilon_decay=0.995, buffer_size=15000),\n    'Double DQN': DoubleDQNAgent(state_dim=4, action_dim=2, lr=1e-3, epsilon_decay=0.995, buffer_size=15000),\n    'Dueling DQN': DuelingDQNAgent(state_dim=4, action_dim=2, dueling_type='mean', lr=1e-3, epsilon_decay=0.995, buffer_size=15000)\n}\nfinal_results = {}\nfor name, agent in variants.items():\n    print(f\"\\nTraining {name}...\")\n    episode_rewards = []\n    for episode in range(80):\n        reward, _ = agent.train_episode(env, max_steps=500)\n        episode_rewards.append(reward)\n        if (episode + 1) % 40 == 0:\n            print(f\"  Episode {episode+1}: {np.mean(episode_rewards[-40:]):.1f}\")\n    eval_results = agent.evaluate(env, num_episodes=15)\n    final_results[name] = {'rewards': episode_rewards, 'losses': agent.losses, 'epsilon_history': agent.epsilon_history, 'eval_performance': eval_results}\nPerformanceAnalyzer.plot_learning_curves(final_results)\nprint(\"\\nPERFORMANCE SUMMARY\")\nfor name, data in final_results.items():\n    eval_perf = data['eval_performance']\n    print(f\"{name}: {eval_perf['mean_reward']:.1f} ± {eval_perf['std_reward']:.1f}\")\nenv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. CONCLUSIONS\n",
    "\n",
    "### A. Key Findings\n",
    "\n",
    "Our comprehensive experimental analysis demonstrates:\n",
    "\n",
    "1. **Experience Replay**: Essential for breaking temporal correlations and improving training stability\n",
    "2. **Target Networks**: Critical for preventing divergence and ensuring stable learning\n",
    "3. **Double DQN**: Effectively reduces overestimation bias\n",
    "4. **Dueling Architecture**: Improves value estimation efficiency and accelerates learning\n",
    "\n",
    "### B. Best Practices\n",
    "\n",
    "**Hyperparameter Guidelines:**\n",
    "- Learning rate: \\\\(10^{-3}\\\\) to \\\\(10^{-4}\\\\)\n",
    "- Discount factor: \\\\(\\\\gamma = 0.99\\\\)\n",
    "- Exploration decay: 0.995\n",
    "- Replay buffer size: 10,000-50,000\n",
    "- Batch size: 32-128\n",
    "- Target update frequency: 100-1000 steps\n",
    "\n",
    "**Algorithm Selection:**\n",
    "- **Basic DQN**: Good starting point for simple environments\n",
    "- **Double DQN**: Better for environments with overestimation issues\n",
    "- **Dueling DQN**: Excellent when value estimation is critical\n",
    "- **Combined**: Best overall performance on complex tasks\n",
    "\n",
    "### C. Future Work\n",
    "\n",
    "Potential extensions include:\n",
    "1. Prioritized experience replay [5]\n",
    "2. Noisy networks for exploration [6]\n",
    "3. Rainbow DQN combining multiple improvements [7]\n",
    "4. Distributional RL approaches (C51, QR-DQN) [8]\n",
    "\n",
    "### D. References\n",
    "\n",
    "[1] V. Mnih et al., \"Playing Atari with Deep Reinforcement Learning,\" arXiv:1312.5602, 2013.\n",
    "\n",
    "[2] C. Watkins and P. Dayan, \"Q-learning,\" Machine Learning, vol. 8, pp. 279-292, 1992.\n",
    "\n",
    "[3] G. Tesauro, \"Temporal Difference Learning and TD-Gammon,\" Communications of the ACM, vol. 38, no. 3, 1995.\n",
    "\n",
    "[4] L. Lin, \"Self-improving Reactive Agents Based on Reinforcement Learning,\" Machine Learning, vol. 8, pp. 293-321, 1992.\n",
    "\n",
    "[5] T. Schaul et al., \"Prioritized Experience Replay,\" ICLR, 2016.\n",
    "\n",
    "[6] M. Fortunato et al., \"Noisy Networks for Exploration,\" ICLR, 2018.\n",
    "\n",
    "[7] M. Hessel et al., \"Rainbow: Combining Improvements in Deep Reinforcement Learning,\" AAAI, 2018.\n",
    "\n",
    "[8] M. Bellemare et al., \"A Distributional Perspective on Reinforcement Learning,\" ICML, 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. ADVANCED ANALYSIS AND EXPERIMENTS\n",
    "\n",
    "### A. Hyperparameter Sensitivity Analysis\n",
    "\n",
    "Understanding the impact of different hyperparameters on DQN performance is crucial for practical applications. We analyze the sensitivity of key hyperparameters including learning rate, replay buffer size, and target update frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_sensitivity_analysis():\n    print(\"=\" * 60)\n    print(\"Hyperparameter Sensitivity Analysis\")\n    print(\"=\" * 60)\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    learning_rates = [1e-4, 5e-4, 1e-3, 2.5e-3, 5e-3]\n    lr_results = {}\n    print(\"\\nTesting different learning rates...\")\n    for lr in learning_rates:\n        print(f\"  Learning Rate: {lr}\")\n        agent = DQNAgent(\n            state_dim=state_dim, \n            action_dim=action_dim, \n            lr=lr,\n            epsilon_decay=0.995,\n            buffer_size=10000\n        )\n        episode_rewards = []\n        for episode in range(50):\n            reward, _ = agent.train_episode(env, max_steps=500)\n            episode_rewards.append(reward)\n        lr_results[lr] = np.mean(episode_rewards[-20:])\n        print(f\"    Final Performance: {lr_results[lr]:.1f}\")\n    buffer_sizes = [1000, 5000, 10000, 25000, 50000]\n    buffer_results = {}\n    print(\"\\nTesting different replay buffer sizes...\")\n    for buffer_size in buffer_sizes:\n        print(f\"  Buffer Size: {buffer_size}\")\n        agent = DQNAgent(\n            state_dim=state_dim, \n            action_dim=action_dim, \n            lr=1e-3,\n            epsilon_decay=0.995,\n            buffer_size=buffer_size\n        )\n        episode_rewards = []\n        for episode in range(50):\n            reward, _ = agent.train_episode(env, max_steps=500)\n            episode_rewards.append(reward)\n        buffer_results[buffer_size] = np.mean(episode_rewards[-20:])\n        print(f\"    Final Performance: {buffer_results[buffer_size]:.1f}\")\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    lrs = list(lr_results.keys())\n    scores = list(lr_results.values())\n    axes[0].plot(lrs, scores, 'o-', linewidth=2, markersize=8, color='blue')\n    axes[0].set_xlabel('Learning Rate')\n    axes[0].set_ylabel('Final Average Score')\n    axes[0].set_title('Learning Rate Sensitivity')\n    axes[0].set_xscale('log')\n    axes[0].grid(True, alpha=0.3)\n    axes[0].axvline(1e-3, color='red', linestyle='--', alpha=0.7, label='Common Default')\n    axes[0].legend()\n    buffers = list(buffer_results.keys())\n    scores = list(buffer_results.values())\n    axes[1].plot(buffers, scores, 'o-', linewidth=2, markersize=8, color='green')\n    axes[1].set_xlabel('Replay Buffer Size')\n    axes[1].set_ylabel('Final Average Score')\n    axes[1].set_title('Replay Buffer Size Sensitivity')\n    axes[1].set_xscale('log')\n    axes[1].grid(True, alpha=0.3)\n    axes[1].axvline(10000, color='red', linestyle='--', alpha=0.7, label='Common Default')\n    axes[1].legend()\n    plt.tight_layout()\n    plt.show()\n    env.close()\n    return lr_results, buffer_results\nlr_results, buffer_results = hyperparameter_sensitivity_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Enhanced Comparative Analysis Dashboard\n",
    "\n",
    "We create a comprehensive multi-dimensional analysis comparing all DQN variants across various performance metrics and training characteristics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Exploration Strategy Analysis\n",
    "\n",
    "Different exploration strategies can significantly impact DQN performance. We compare epsilon-greedy exploration with different decay schedules and analyze their effects on learning efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exploration_strategy_analysis():\n    print(\"=\" * 60)\n    print(\"Exploration Strategy Analysis\")\n    print(\"=\" * 60)\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    strategies = {\n        'Fast Decay (0.99)': {'epsilon_decay': 0.99, 'epsilon_end': 0.01},\n        'Medium Decay (0.995)': {'epsilon_decay': 0.995, 'epsilon_end': 0.01},\n        'Slow Decay (0.999)': {'epsilon_decay': 0.999, 'epsilon_end': 0.01},\n        'High Final Epsilon (0.1)': {'epsilon_decay': 0.995, 'epsilon_end': 0.1},\n    }\n    results = {}\n    num_episodes = 100\n    for strategy_name, config in strategies.items():\n        print(f\"\\nTesting {strategy_name}...\")\n        agent = DQNAgent(\n            state_dim=state_dim,\n            action_dim=action_dim,\n            lr=1e-3,\n            epsilon_decay=config['epsilon_decay'],\n            epsilon_end=config['epsilon_end'],\n            buffer_size=10000\n        )\n        episode_rewards = []\n        epsilon_history = []\n        for episode in range(num_episodes):\n            reward, _ = agent.train_episode(env, max_steps=500)\n            episode_rewards.append(reward)\n            epsilon_history.append(agent.epsilon)\n            if (episode + 1) % 25 == 0:\n                avg_reward = np.mean(episode_rewards[-25:])\n                print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}, Epsilon = {agent.epsilon:.3f}\")\n        results[strategy_name] = {\n            'rewards': episode_rewards,\n            'epsilon_history': epsilon_history,\n            'final_performance': np.mean(episode_rewards[-20:])\n        }\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    colors = ['blue', 'red', 'green', 'orange']\n    ax = axes[0, 0]\n    for i, (strategy, data) in enumerate(results.items()):\n        rewards = data['rewards']\n        smoothed = pd.Series(rewards).rolling(10).mean()\n        ax.plot(smoothed, label=strategy, color=colors[i], linewidth=2)\n    ax.set_title('Learning Curves by Exploration Strategy')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Episode Reward (Smoothed)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = axes[0, 1]\n    for i, (strategy, data) in enumerate(results.items()):\n        epsilon_history = data['epsilon_history']\n        ax.plot(epsilon_history, label=strategy, color=colors[i], linewidth=2)\n    ax.set_title('Epsilon Decay Schedules')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Epsilon Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = axes[1, 0]\n    strategy_names = list(results.keys())\n    final_perfs = [results[s]['final_performance'] for s in strategy_names]\n    bars = ax.bar(strategy_names, final_perfs, alpha=0.7, color=colors)\n    ax.set_title('Final Performance Comparison')\n    ax.set_ylabel('Average Reward (Last 20 Episodes)')\n    ax.set_xticklabels(strategy_names, rotation=45, ha='right')\n    for bar, perf in zip(bars, final_perfs):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                f'{perf:.1f}', ha='center', va='bottom')\n    ax.grid(True, alpha=0.3)\n    ax = axes[1, 1]\n    exploration_efficiency = []\n    for strategy, data in results.items():\n        rewards = np.array(data['rewards'])\n        exploration_actions = np.array(data['epsilon_history']) * 100\n        efficiency = np.mean(rewards) / (np.mean(exploration_actions) + 1e-8)\n        exploration_efficiency.append(efficiency)\n    bars = ax.bar(strategy_names, exploration_efficiency, alpha=0.7, color='green')\n    ax.set_title('Exploration Efficiency')\n    ax.set_ylabel('Reward per Exploration Action')\n    ax.set_xticklabels(strategy_names, rotation=45, ha='right')\n    for bar, eff in zip(bars, exploration_efficiency):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                f'{eff:.3f}', ha='center', va='bottom')\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    env.close()\n    return results\nexploration_results = exploration_strategy_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Environment Comparison\n",
    "\n",
    "We test our DQN implementations on different environments to demonstrate their versatility and performance characteristics across various problem domains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def environment_comparison_analysis():\n    print(\"=\" * 60)\n    print(\"Environment Comparison Analysis\")\n    print(\"=\" * 60)\n    environments = {\n        'CartPole-v1': {'max_steps': 500, 'target_reward': 195},\n        'Acrobot-v1': {'max_steps': 500, 'target_reward': -100},\n        'MountainCar-v0': {'max_steps': 200, 'target_reward': -110}\n    }\n    results = {}\n    for env_name, config in environments.items():\n        print(f\"\\nTesting on {env_name}...\")\n        try:\n            env = gym.make(env_name)\n            state_dim = env.observation_space.shape[0]\n            action_dim = env.action_space.n\n            print(f\"  State dimension: {state_dim}\")\n            print(f\"  Action dimension: {action_dim}\")\n            print(f\"  Target reward: {config['target_reward']}\")\n            agents = {\n                'Basic DQN': DQNAgent(\n                    state_dim=state_dim, \n                    action_dim=action_dim,\n                    lr=1e-3,\n                    epsilon_decay=0.995,\n                    buffer_size=10000\n                ),\n                'Double DQN': DoubleDQNAgent(\n                    state_dim=state_dim, \n                    action_dim=action_dim,\n                    lr=1e-3,\n                    epsilon_decay=0.995,\n                    buffer_size=10000\n                )\n            }\n            env_results = {}\n            num_episodes = 100\n            for agent_name, agent in agents.items():\n                print(f\"    Training {agent_name}...\")\n                episode_rewards = []\n                for episode in range(num_episodes):\n                    reward, _ = agent.train_episode(env, max_steps=config['max_steps'])\n                    episode_rewards.append(reward)\n                    if (episode + 1) % 25 == 0:\n                        avg_reward = np.mean(episode_rewards[-25:])\n                        print(f\"      Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n                eval_results = agent.evaluate(env, num_episodes=10)\n                env_results[agent_name] = {\n                    'rewards': episode_rewards,\n                    'eval_performance': eval_results,\n                    'final_performance': np.mean(episode_rewards[-20:])\n                }\n            results[env_name] = env_results\n            env.close()\n        except Exception as e:\n            print(f\"  Error testing {env_name}: {e}\")\n            continue\n    if results:\n        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n        colors = ['blue', 'red', 'green']\n        ax = axes[0, 0]\n        env_names = list(results.keys())\n        agent_names = ['Basic DQN', 'Double DQN']\n        x = np.arange(len(env_names))\n        width = 0.35\n        for i, agent_name in enumerate(agent_names):\n            performances = []\n            for env_name in env_names:\n                if agent_name in results[env_name]:\n                    perf = results[env_name][agent_name]['final_performance']\n                    performances.append(perf)\n                else:\n                    performances.append(0)\n            ax.bar(x + i * width, performances, width, label=agent_name, alpha=0.7)\n        ax.set_title('Final Performance Across Environments')\n        ax.set_ylabel('Average Reward (Last 20 Episodes)')\n        ax.set_xticks(x + width / 2)\n        ax.set_xticklabels(env_names)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        for i, (env_name, env_results) in enumerate(results.items()):\n            if i < 2:\n                ax = axes[0, 1] if i == 0 else axes[1, 0]\n                for j, (agent_name, data) in enumerate(env_results.items()):\n                    rewards = data['rewards']\n                    smoothed = pd.Series(rewards).rolling(10).mean()\n                    ax.plot(smoothed, label=f'{agent_name}', color=colors[j], linewidth=2)\n                ax.set_title(f'Learning Curves - {env_name}')\n                ax.set_xlabel('Episode')\n                ax.set_ylabel('Episode Reward (Smoothed)')\n                ax.legend()\n                ax.grid(True, alpha=0.3)\n        ax = axes[1, 1]\n        success_rates = {}\n        for env_name, env_results in results.items():\n            target = environments[env_name]['target_reward']\n            success_rates[env_name] = {}\n            for agent_name, data in env_results.items():\n                eval_perf = data['eval_performance']['mean_reward']\n                success_rate = 1.0 if eval_perf >= target else 0.0\n                success_rates[env_name][agent_name] = success_rate\n        env_names = list(success_rates.keys())\n        x = np.arange(len(env_names))\n        width = 0.35\n        for i, agent_name in enumerate(agent_names):\n            rates = []\n            for env_name in env_names:\n                if agent_name in success_rates[env_name]:\n                    rates.append(success_rates[env_name][agent_name])\n                else:\n                    rates.append(0)\n            ax.bar(x + i * width, rates, width, label=agent_name, alpha=0.7)\n        ax.set_title('Success Rate Comparison')\n        ax.set_ylabel('Success Rate (1.0 = Target Achieved)')\n        ax.set_xticks(x + width / 2)\n        ax.set_xticklabels(env_names)\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        ax.set_ylim(0, 1.1)\n        plt.tight_layout()\n        plt.show()\n    return results\nenvironment_results = environment_comparison_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XI. PRACTICAL IMPLEMENTATION GUIDELINES\n",
    "\n",
    "### A. Code Organization and Best Practices\n",
    "\n",
    "Our modular implementation demonstrates several key principles for building maintainable and extensible DQN systems:\n",
    "\n",
    "1. **Separation of Concerns**: Each component (networks, agents, utilities) is in its own module\n",
    "2. **Inheritance Hierarchy**: Double DQN and Dueling DQN extend the base DQN agent\n",
    "3. **Configuration Management**: Hyperparameters are easily configurable\n",
    "4. **Error Handling**: Robust error handling for different environments\n",
    "5. **Documentation**: Comprehensive docstrings and type hints\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Enhanced Comparative Analysis Dashboard\n",
    "\n",
    "We create a comprehensive multi-dimensional analysis comparing all DQN variants across various performance metrics and training characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_comparison_dashboard():\n    print(\"=\" * 60)\n    print(\"Enhanced Comparative Analysis Dashboard\")\n    print(\"=\" * 60)\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    agents = {\n        'Basic DQN': DQNAgent(\n            state_dim=state_dim, action_dim=action_dim,\n            lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n        ),\n        'Double DQN': DoubleDQNAgent(\n            state_dim=state_dim, action_dim=action_dim,\n            lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n        ),\n        'Dueling DQN': DuelingDQNAgent(\n            state_dim=state_dim, action_dim=action_dim,\n            dueling_type='mean', lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n        )\n    }\n    results = {}\n    num_episodes = 60\n    for name, agent in agents.items():\n        print(f\"\\nTraining {name}...\")\n        episode_rewards = []\n        episode_losses = []\n        episode_epsilons = []\n        for episode in range(num_episodes):\n            reward, steps = agent.train_episode(env, max_steps=500)\n            episode_rewards.append(reward)\n            episode_epsilons.append(agent.epsilon)\n            if (episode + 1) % 20 == 0:\n                avg_reward = np.mean(episode_rewards[-20:])\n                print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n        eval_results = agent.evaluate(env, num_episodes=10)\n        results[name] = {\n            'rewards': episode_rewards,\n            'losses': agent.losses,\n            'epsilon_history': episode_epsilons,\n            'eval_performance': eval_results,\n            'agent': agent\n        }\n    print(\"\\nCreating enhanced comparison dashboard...\")\n    fig = plt.figure(figsize=(20, 16))\n    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n    fig.suptitle('Enhanced DQN Variants Comparison Dashboard', fontsize=18, fontweight='bold')\n    colors = {'Basic DQN': 'blue', 'Double DQN': 'red', 'Dueling DQN': 'green'}\n    ax = fig.add_subplot(gs[0, :2])\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        window = 5\n        smoothed = pd.Series(rewards).rolling(window).mean()\n        std = pd.Series(rewards).rolling(window).std()\n        episodes = np.arange(len(rewards))\n        ax.plot(episodes, smoothed, label=name, color=colors[name], linewidth=2)\n        ax.fill_between(episodes, smoothed - std, smoothed + std, alpha=0.2, color=colors[name])\n    ax.axhline(y=195, color='orange', linestyle='--', alpha=0.7, label='Target (195)')\n    ax.set_title('Learning Curves with Confidence Bands', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Average Reward')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[0, 2:], projection='polar')\n    metrics = ['Final Performance', 'Stability', 'Convergence Speed', 'Sample Efficiency', 'Max Performance']\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        final_perf = np.mean(rewards[-10:]) / 200\n        stability = 1 - (np.std(rewards[-20:]) / np.mean(rewards[-20:]))\n        convergence = 1 - (np.mean(rewards[:10]) / np.mean(rewards[-10:]))\n        sample_eff = np.mean(rewards[-20:]) / num_episodes\n        max_perf = np.max(rewards) / 200\n        values = [final_perf, stability, convergence, sample_eff, max_perf]\n        values = np.clip(values, 0, 1)\n        angles = np.linspace(0, 2 * np.pi, len(metrics), endpoint=False).tolist()\n        values += values[:1]\n        angles += angles[:1]\n        ax.plot(angles, values, 'o-', linewidth=2, label=name, color=colors[name])\n        ax.fill(angles, values, alpha=0.25, color=colors[name])\n    ax.set_xticks(angles[:-1])\n    ax.set_xticklabels(metrics)\n    ax.set_ylim(0, 1)\n    ax.set_title('Performance Metrics Radar Chart', fontsize=14, fontweight='bold', pad=20)\n    ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))\n    ax.grid(True)\n    ax = fig.add_subplot(gs[1, 0])\n    for name, data in results.items():\n        if data['losses']:\n            losses = pd.Series(data['losses']).rolling(10).mean()\n            ax.plot(losses, label=name, color=colors[name], linewidth=2)\n    ax.set_title('Training Loss Evolution')\n    ax.set_xlabel('Training Step')\n    ax.set_ylabel('MSE Loss (Smoothed)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, 1])\n    for name, data in results.items():\n        ax.plot(data['epsilon_history'], label=name, color=colors[name], linewidth=2)\n    ax.set_title('Exploration Decay Comparison')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Epsilon Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, 2])\n    for name, data in results.items():\n        ax.hist(data['rewards'], bins=15, alpha=0.5, label=name, color=colors[name], density=True)\n    ax.set_title('Reward Distribution')\n    ax.set_xlabel('Episode Reward')\n    ax.set_ylabel('Density')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, 3])\n    stat_data = []\n    labels = []\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        stats = [\n            np.mean(rewards[-20:]),\n            np.max(rewards),\n            np.std(rewards[-20:])\n        ]\n        stat_data.append(stats)\n        labels.append(name)\n    x = np.arange(3)\n    width = 0.25\n    metric_labels = ['Final Avg\\n(Last 20)', 'Best\\nEpisode', 'Final Std\\n(Last 20)']\n    for i, (name, stats) in enumerate(zip(labels, stat_data)):\n        ax.bar(x + i * width, stats, width, label=name, alpha=0.7, color=colors[name])\n    ax.set_xticks(x + width)\n    ax.set_xticklabels(metric_labels)\n    ax.set_title('Performance Statistics')\n    ax.set_ylabel('Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, :2])\n    sample_states = []\n    for _ in range(50):\n        state, _ = env.reset()\n        sample_states.append(state)\n    for name, data in results.items():\n        agent = data['agent']\n        q_values = []\n        for state in sample_states:\n            q_vals = agent.get_q_values(state)\n            q_values.append(q_vals)\n        q_values = np.array(q_values)\n        for action in range(action_dim):\n            ax.hist(q_values[:, action], bins=15, alpha=0.3, \n                   label=f'{name} - Action {action}', density=True)\n    ax.set_title('Q-Value Distributions Comparison (Final Models)')\n    ax.set_xlabel('Q-Value')\n    ax.set_ylabel('Density')\n    ax.legend(loc='upper right', fontsize=8)\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, 2:])\n    efficiency_data = {}\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        early_perf = np.mean(rewards[:10])\n        late_perf = np.mean(rewards[-10:])\n        improvement = late_perf - early_perf\n        convergence_episode = np.where(pd.Series(rewards).rolling(10).mean() >= 195)[0]\n        convergence_episode = convergence_episode[0] if len(convergence_episode) > 0 else num_episodes\n        efficiency_data[name] = {\n            'Early Performance': early_perf,\n            'Late Performance': late_perf,\n            'Improvement': improvement,\n            'Convergence Episode': convergence_episode\n        }\n    metrics = ['Early Performance', 'Late Performance', 'Improvement']\n    x = np.arange(len(metrics))\n    width = 0.25\n    for i, name in enumerate(efficiency_data.keys()):\n        values = [efficiency_data[name][metric] for metric in metrics]\n        ax.bar(x + i * width, values, width, label=name, alpha=0.7, color=colors[name])\n    ax.set_xticks(x + width)\n    ax.set_xticklabels(metrics)\n    ax.set_title('Training Efficiency Analysis')\n    ax.set_ylabel('Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[3, 0])\n    complexity_metrics = {}\n    for name, data in results.items():\n        agent = data['agent']\n        total_params = sum(p.numel() for p in agent.q_network.parameters())\n        memory_mb = (total_params * 4) / (1024 * 1024)\n        complexity_metrics[name] = memory_mb\n    bars = ax.bar(complexity_metrics.keys(), complexity_metrics.values(), \n                  alpha=0.7, color=[colors[n] for n in complexity_metrics.keys()])\n    ax.set_title('Model Complexity (Memory)')\n    ax.set_ylabel('Memory (MB)')\n    ax.tick_params(axis='x', rotation=15)\n    for bar, value in zip(bars, complexity_metrics.values()):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                f'{value:.2f}', ha='center', va='bottom')\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[3, 1])\n    success_rates = {}\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        success_rate = np.mean(rewards >= 195)\n        success_rates[name] = success_rate\n    bars = ax.bar(success_rates.keys(), success_rates.values(), \n                  alpha=0.7, color=[colors[n] for n in success_rates.keys()])\n    ax.set_title('Success Rate (Reward ≥ 195)')\n    ax.set_ylabel('Success Rate')\n    ax.set_ylim(0, 1)\n    ax.tick_params(axis='x', rotation=15)\n    for bar, value in zip(bars, success_rates.values()):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n                f'{value:.2f}', ha='center', va='bottom')\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[3, 2])\n    for name, data in results.items():\n        cumulative = np.cumsum(data['rewards'])\n        ax.plot(cumulative, label=name, color=colors[name], linewidth=2)\n    ax.set_title('Cumulative Reward Over Training')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Cumulative Reward')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[3, 3])\n    final_perfs = [results[name]['eval_performance']['mean_reward'] for name in results.keys()]\n    final_stds = [results[name]['eval_performance']['std_reward'] for name in results.keys()]\n    bars = ax.bar(results.keys(), final_perfs, yerr=final_stds, \n                  alpha=0.7, color=[colors[n] for n in results.keys()], capsize=5)\n    ax.set_title('Final Evaluation Performance')\n    ax.set_ylabel('Mean Reward ± Std')\n    ax.tick_params(axis='x', rotation=15)\n    for bar, perf, std in zip(bars, final_perfs, final_stds):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 2,\n                f'{perf:.1f}±{std:.1f}', ha='center', va='bottom', fontsize=8)\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"PERFORMANCE SUMMARY\")\n    print(\"=\" * 60)\n    for name, data in results.items():\n        eval_perf = data['eval_performance']\n        rewards = np.array(data['rewards'])\n        print(f\"\\n{name}:\")\n        print(f\"  Final Evaluation: {eval_perf['mean_reward']:.1f} ± {eval_perf['std_reward']:.1f}\")\n        print(f\"  Training Final Avg: {np.mean(rewards[-20:]):.1f}\")\n        print(f\"  Best Episode: {np.max(rewards):.1f}\")\n        print(f\"  Success Rate: {np.mean(rewards >= 195):.2%}\")\n        print(f\"  Convergence: Episode {np.where(pd.Series(rewards).rolling(10).mean() >= 195)[0][0] if len(np.where(pd.Series(rewards).rolling(10).mean() >= 195)[0]) > 0 else 'Not achieved'}\")\n    env.close()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Enhanced comparison dashboard completed!\")\n    print(\"=\" * 60)\n    return results\ncomparison_results = create_enhanced_comparison_dashboard()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. Interactive State-Space Analysis\n",
    "\n",
    "We analyze how different DQN variants learn to map state spaces to action values, providing insights into their decision-making processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_state_space_mapping():\n    print(\"=\" * 60)\n    print(\"Interactive State-Space Analysis\")\n    print(\"=\" * 60)\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    if 'comparison_results' in globals():\n        agents = {name: data['agent'] for name, data in comparison_results.items()}\n    else:\n        agents = {\n            'Basic DQN': DQNAgent(state_dim=state_dim, action_dim=action_dim, lr=1e-3, epsilon_decay=0.995),\n            'Double DQN': DoubleDQNAgent(state_dim=state_dim, action_dim=action_dim, lr=1e-3, epsilon_decay=0.995),\n            'Dueling DQN': DuelingDQNAgent(state_dim=state_dim, action_dim=action_dim, lr=1e-3, epsilon_decay=0.995)\n        }\n        for name, agent in agents.items():\n            print(f\"Training {name}...\")\n            for episode in range(30):\n                agent.train_episode(env, max_steps=500)\n    print(\"Creating state-space analysis visualizations...\")\n    fig = plt.figure(figsize=(20, 14))\n    gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n    fig.suptitle('State-Space Analysis: DQN Decision Making', fontsize=18, fontweight='bold')\n    colors = {'Basic DQN': 'blue', 'Double DQN': 'red', 'Dueling DQN': 'green'}\n    print(\"Sampling state space...\")\n    sample_states = []\n    state_labels = []\n    for _ in range(100):\n        state, _ = env.reset()\n        sample_states.append(state)\n        state_labels.append('Random')\n    for _ in range(50):\n        state, _ = env.reset()\n        total_reward = 0\n        for _ in range(200):\n            action = env.action_space.sample()\n            state, reward, terminated, truncated, _ = env.step(action)\n            total_reward += reward\n            if total_reward > 100:\n                sample_states.append(state)\n                state_labels.append('Successful')\n            if terminated or truncated:\n                break\n    for _ in range(50):\n        state, _ = env.reset()\n        for _ in range(10):\n            action = env.action_space.sample()\n            state, reward, terminated, truncated, _ = env.step(action)\n            if terminated or truncated:\n                sample_states.append(state)\n                state_labels.append('Failing')\n                break\n    sample_states = np.array(sample_states)\n    for i, (name, agent) in enumerate(agents.items()):\n        ax = fig.add_subplot(gs[0, i])\n        q_values = []\n        for state in sample_states:\n            q_vals = agent.get_q_values(state)\n            q_values.append(q_vals)\n        q_values = np.array(q_values)\n        im = ax.imshow(q_values.T, cmap='viridis', aspect='auto')\n        ax.set_title(f'{name} Q-Values')\n        ax.set_xlabel('State Index')\n        ax.set_ylabel('Action')\n        ax.set_yticks(range(action_dim))\n        ax.set_yticklabels([f'Action {i}' for i in range(action_dim)])\n        plt.colorbar(im, ax=ax, label='Q-Value')\n    ax = fig.add_subplot(gs[0, 3])\n    action_preferences = {}\n    for name, agent in agents.items():\n        q_values = []\n        for state in sample_states:\n            q_vals = agent.get_q_values(state)\n            q_values.append(q_vals)\n        q_values = np.array(q_values)\n        preferred_actions = np.argmax(q_values, axis=1)\n        action_counts = np.bincount(preferred_actions, minlength=action_dim)\n        action_preferences[name] = action_counts / len(preferred_actions)\n    x = np.arange(action_dim)\n    width = 0.25\n    for i, (name, preferences) in enumerate(action_preferences.items()):\n        ax.bar(x + i * width, preferences, width, label=name, alpha=0.7, color=colors[name])\n    ax.set_xlabel('Action')\n    ax.set_ylabel('Preference Probability')\n    ax.set_title('Action Preferences Across States')\n    ax.set_xticks(x + width)\n    ax.set_xticklabels([f'Action {i}' for i in range(action_dim)])\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, :2])\n    from sklearn.decomposition import PCA\n    from sklearn.cluster import KMeans\n    all_q_values = []\n    for name, agent in agents.items():\n        q_values = []\n        for state in sample_states:\n            q_vals = agent.get_q_values(state)\n            q_values.append(q_vals)\n        all_q_values.append(np.array(q_values))\n    q_values = all_q_values[0]\n    pca = PCA(n_components=2)\n    q_pca = pca.fit_transform(q_values)\n    kmeans = KMeans(n_clusters=3, random_state=42)\n    clusters = kmeans.fit_predict(q_values)\n    scatter = ax.scatter(q_pca[:, 0], q_pca[:, 1], c=clusters, cmap='viridis', alpha=0.6)\n    ax.set_title('State Clustering Based on Q-Values (PCA)')\n    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n    plt.colorbar(scatter, ax=ax, label='Cluster')\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, 2:])\n    random_states = sample_states[np.array(state_labels) == 'Random']\n    successful_states = sample_states[np.array(state_labels) == 'Successful']\n    failing_states = sample_states[np.array(state_labels) == 'Failing']\n    state_types = {\n        'Random': random_states,\n        'Successful': successful_states,\n        'Failing': failing_states\n    }\n    for state_type, states in state_types.items():\n        if len(states) > 0:\n            agent = list(agents.values())[0]\n            q_values = []\n            for state in states:\n                q_vals = agent.get_q_values(state)\n                q_values.append(q_vals)\n            q_values = np.array(q_values)\n            for action in range(action_dim):\n                ax.hist(q_values[:, action], bins=15, alpha=0.4, \n                       label=f'{state_type} - Action {action}', density=True)\n    ax.set_title('Q-Value Distributions by State Type')\n    ax.set_xlabel('Q-Value')\n    ax.set_ylabel('Density')\n    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, 0])\n    base_state = np.array([0, 0, 0, 0])\n    x_range = np.linspace(-2, 2, 20)\n    y_range = np.linspace(-2, 2, 20)\n    X, Y = np.meshgrid(x_range, y_range)\n    agent = list(agents.values())[0]\n    Z = np.zeros_like(X)\n    for i in range(len(x_range)):\n        for j in range(len(y_range)):\n            state = base_state.copy()\n            state[0] = X[i, j]\n            state[1] = Y[i, j]\n            q_vals = agent.get_q_values(state)\n            Z[i, j] = np.argmax(q_vals)\n    contour = ax.contourf(X, Y, Z, levels=[-0.5, 0.5, 1.5], colors=['lightblue', 'lightcoral'], alpha=0.7)\n    ax.contour(X, Y, Z, levels=[0.5], colors=['black'], linewidths=2)\n    ax.set_title('Decision Boundary (State Dims 0 vs 1)')\n    ax.set_xlabel('State Dimension 0')\n    ax.set_ylabel('State Dimension 1')\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, 1])\n    base_state = np.array([0, 0, 0, 0])\n    sensitivities = []\n    for dim in range(state_dim):\n        perturbations = np.linspace(-1, 1, 20)\n        q_changes = []\n        for pert in perturbations:\n            test_state = base_state.copy()\n            test_state[dim] = pert\n            q_vals = agent.get_q_values(test_state)\n            q_changes.append(np.max(q_vals))\n        sensitivities.append(np.std(q_changes))\n    bars = ax.bar(range(state_dim), sensitivities, alpha=0.7, color='orange')\n    ax.set_title('Q-Value Sensitivity by State Dimension')\n    ax.set_xlabel('State Dimension')\n    ax.set_ylabel('Sensitivity (Std of Q-values)')\n    ax.set_xticks(range(state_dim))\n    ax.set_xticklabels([f'Dim {i}' for i in range(state_dim)])\n    for bar, sens in zip(bars, sensitivities):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n                f'{sens:.2f}', ha='center', va='bottom')\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, 2])\n    if 'Dueling DQN' in agents:\n        dueling_agent = agents['Dueling DQN']\n        values = []\n        advantages = []\n        for state in sample_states[:50]:\n            q_vals = dueling_agent.get_q_values(state)\n            value = np.mean(q_vals)\n            advantage = np.max(q_vals) - np.mean(q_vals)\n            values.append(value)\n            advantages.append(advantage)\n        ax.scatter(values, advantages, alpha=0.6, color='green')\n        ax.set_title('State Value vs Advantage (Dueling DQN)')\n        ax.set_xlabel('State Value (Mean Q-value)')\n        ax.set_ylabel('Advantage (Max - Mean Q-value)')\n        ax.grid(True, alpha=0.3)\n        corr = np.corrcoef(values, advantages)[0, 1]\n        ax.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n                transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n    else:\n        ax.text(0.5, 0.5, 'Dueling DQN not available', ha='center', va='center', transform=ax.transAxes)\n        ax.set_title('State Value vs Advantage (Dueling DQN)')\n    ax = fig.add_subplot(gs[2, 3])\n    confidences = []\n    for state in sample_states:\n        q_vals = agent.get_q_values(state)\n        sorted_q = np.sort(q_vals)[::-1]\n        confidence = sorted_q[0] - sorted_q[1]\n        confidences.append(confidence)\n    ax.hist(confidences, bins=20, alpha=0.7, color='purple', edgecolor='black')\n    ax.axvline(np.mean(confidences), color='red', linestyle='--', \n               label=f'Mean: {np.mean(confidences):.2f}')\n    ax.set_title('Action Confidence Distribution')\n    ax.set_xlabel('Confidence (Max - Second Max Q-value)')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"STATE-SPACE ANALYSIS SUMMARY\")\n    print(\"=\" * 60)\n    print(f\"\\nAnalyzed {len(sample_states)} states:\")\n    for state_type, count in zip(*np.unique(state_labels, return_counts=True)):\n        print(f\"  {state_type}: {count} states\")\n    print(f\"\\nAction Preferences:\")\n    for name, preferences in action_preferences.items():\n        print(f\"  {name}: {preferences}\")\n    print(f\"\\nQ-Value Sensitivity by Dimension:\")\n    for i, sens in enumerate(sensitivities):\n        print(f\"  Dimension {i}: {sens:.3f}\")\n    print(f\"\\nAction Confidence: {np.mean(confidences):.3f} ± {np.std(confidences):.3f}\")\n    env.close()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"State-space analysis completed!\")\n    print(\"=\" * 60)\n    return {\n        'sample_states': sample_states,\n        'state_labels': state_labels,\n        'action_preferences': action_preferences,\n        'sensitivities': sensitivities,\n        'confidences': confidences\n    }\nstate_analysis = analyze_state_space_mapping()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E. Training Dynamics and Convergence Analysis\n",
    "\n",
    "We analyze the training dynamics, convergence patterns, and learning stability across different DQN variants to understand their learning characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_training_dynamics():\n    print(\"=\" * 60)\n    print(\"Training Dynamics and Convergence Analysis\")\n    print(\"=\" * 60)\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    agents = {\n        'Basic DQN': DQNAgent(\n            state_dim=state_dim, action_dim=action_dim,\n            lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n        ),\n        'Double DQN': DoubleDQNAgent(\n            state_dim=state_dim, action_dim=action_dim,\n            lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n        ),\n        'Dueling DQN': DuelingDQNAgent(\n            state_dim=state_dim, action_dim=action_dim,\n            dueling_type='mean', lr=1e-3, epsilon_decay=0.995, buffer_size=10000\n        )\n    }\n    num_episodes = 100\n    results = {}\n    for name, agent in agents.items():\n        print(f\"\\nTraining {name} for {num_episodes} episodes...\")\n        episode_rewards = []\n        episode_losses = []\n        episode_epsilons = []\n        episode_steps = []\n        for episode in range(num_episodes):\n            reward, steps = agent.train_episode(env, max_steps=500)\n            episode_rewards.append(reward)\n            episode_steps.append(steps)\n            episode_epsilons.append(agent.epsilon)\n            if (episode + 1) % 25 == 0:\n                avg_reward = np.mean(episode_rewards[-25:])\n                print(f\"  Episode {episode+1}: Avg Reward = {avg_reward:.1f}\")\n        results[name] = {\n            'rewards': episode_rewards,\n            'losses': agent.losses,\n            'epsilon_history': episode_epsilons,\n            'steps': episode_steps,\n            'agent': agent\n        }\n    print(\"\\nCreating training dynamics analysis...\")\n    fig = plt.figure(figsize=(20, 16))\n    gs = fig.add_gridspec(4, 4, hspace=0.3, wspace=0.3)\n    fig.suptitle('Training Dynamics and Convergence Analysis', fontsize=18, fontweight='bold')\n    colors = {'Basic DQN': 'blue', 'Double DQN': 'red', 'Dueling DQN': 'green'}\n    ax = fig.add_subplot(gs[0, :2])\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        window = 10\n        smoothed = pd.Series(rewards).rolling(window).mean()\n        ax.plot(rewards, alpha=0.2, color=colors[name])\n        ax.plot(smoothed, label=name, color=colors[name], linewidth=2)\n        convergence_threshold = 195\n        convergence_idx = np.where(smoothed >= convergence_threshold)[0]\n        if len(convergence_idx) > 0:\n            ax.axvline(convergence_idx[0], color=colors[name], linestyle='--', alpha=0.7)\n            ax.text(convergence_idx[0], smoothed.iloc[convergence_idx[0]], \n                   f'Conv: {convergence_idx[0]}', fontsize=8, color=colors[name])\n    ax.axhline(y=195, color='orange', linestyle='-', alpha=0.7, label='Target (195)')\n    ax.set_title('Learning Curves with Convergence Points', fontsize=14, fontweight='bold')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Episode Reward')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[0, 2:])\n    convergence_data = {}\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        smoothed = pd.Series(rewards).rolling(10).mean()\n        convergence_threshold = 195\n        convergence_idx = np.where(smoothed >= convergence_threshold)[0]\n        convergence_episode = convergence_idx[0] if len(convergence_idx) > 0 else num_episodes\n        convergence_data[name] = {\n            'convergence_episode': convergence_episode,\n            'final_performance': np.mean(rewards[-20:]),\n            'stability': 1 - (np.std(rewards[-20:]) / np.mean(rewards[-20:])),\n            'learning_rate': np.mean(np.diff(smoothed.dropna()))\n        }\n    metrics = ['convergence_episode', 'final_performance', 'stability']\n    x = np.arange(len(metrics))\n    width = 0.25\n    for i, name in enumerate(convergence_data.keys()):\n        values = [convergence_data[name][metric] for metric in metrics]\n        if metric == 'convergence_episode':\n            values = [v / num_episodes for v in values]\n        elif metric == 'final_performance':\n            values = [v / 200 for v in values]\n        ax.bar(x + i * width, values, width, label=name, alpha=0.7, color=colors[name])\n    ax.set_xticks(x + width)\n    ax.set_xticklabels(['Convergence\\nSpeed', 'Final\\nPerformance', 'Stability'])\n    ax.set_title('Convergence Metrics Comparison')\n    ax.set_ylabel('Normalized Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, 0])\n    for name, data in results.items():\n        if data['losses']:\n            losses = np.array(data['losses'])\n            smoothed_losses = pd.Series(losses).rolling(50).mean()\n            ax.plot(smoothed_losses, label=name, color=colors[name], linewidth=2)\n    ax.set_title('Training Loss Evolution')\n    ax.set_xlabel('Training Step')\n    ax.set_ylabel('MSE Loss (Smoothed)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, 1])\n    for name, data in results.items():\n        epsilons = np.array(data['epsilon_history'])\n        ax.plot(epsilons, label=name, color=colors[name], linewidth=2)\n    ax.set_title('Exploration Decay Comparison')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Epsilon Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, 2])\n    for name, data in results.items():\n        steps = np.array(data['steps'])\n        smoothed_steps = pd.Series(steps).rolling(10).mean()\n        ax.plot(smoothed_steps, label=name, color=colors[name], linewidth=2)\n    ax.set_title('Episode Length Evolution')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Steps per Episode (Smoothed)')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[1, 3])\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        window = 20\n        rolling_improvement = []\n        for i in range(window, len(rewards)):\n            early_avg = np.mean(rewards[i-window:i-window//2])\n            late_avg = np.mean(rewards[i-window//2:i])\n            improvement = late_avg - early_avg\n            rolling_improvement.append(improvement)\n        ax.plot(rolling_improvement, label=name, color=colors[name], linewidth=2)\n    ax.set_title('Effective Learning Rate')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Reward Improvement per Window')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, :2])\n    stages = ['Early (0-25)', 'Middle (25-50)', 'Late (50-75)', 'Final (75-100)']\n    stage_ranges = [(0, 25), (25, 50), (50, 75), (75, 100)]\n    for i, (stage, (start, end)) in enumerate(zip(stages, stage_ranges)):\n        for name, data in results.items():\n            rewards = np.array(data['rewards'])\n            stage_rewards = rewards[start:end]\n            if len(stage_rewards) > 0:\n                ax.hist(stage_rewards, bins=15, alpha=0.3, \n                       label=f'{name} - {stage}', density=True)\n    ax.set_title('Reward Distribution Evolution')\n    ax.set_xlabel('Episode Reward')\n    ax.set_ylabel('Density')\n    ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[2, 2:])\n    stability_metrics = {}\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        window = 20\n        rolling_std = []\n        for i in range(window, len(rewards)):\n            std = np.std(rewards[i-window:i])\n            rolling_std.append(std)\n        stability_metrics[name] = {\n            'mean_std': np.mean(rolling_std),\n            'std_trend': np.polyfit(range(len(rolling_std)), rolling_std, 1)[0],\n            'final_std': np.std(rewards[-20:])\n        }\n    metrics = ['mean_std', 'std_trend', 'final_std']\n    x = np.arange(len(metrics))\n    width = 0.25\n    for i, name in enumerate(stability_metrics.keys()):\n        values = [stability_metrics[name][metric] for metric in metrics]\n        ax.bar(x + i * width, values, width, label=name, alpha=0.7, color=colors[name])\n    ax.set_xticks(x + width)\n    ax.set_xticklabels(['Mean\\nStd Dev', 'Std\\nTrend', 'Final\\nStd Dev'])\n    ax.set_title('Training Stability Metrics')\n    ax.set_ylabel('Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[3, 0])\n    consistency_data = {}\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        window = 20\n        success_rates = []\n        for i in range(window, len(rewards)):\n            success_rate = np.mean(rewards[i-window:i] >= 195)\n            success_rates.append(success_rate)\n        consistency_data[name] = {\n            'mean_success_rate': np.mean(success_rates),\n            'final_success_rate': success_rates[-1] if success_rates else 0,\n            'consistency_trend': np.polyfit(range(len(success_rates)), success_rates, 1)[0]\n        }\n    metrics = ['mean_success_rate', 'final_success_rate', 'consistency_trend']\n    x = np.arange(len(metrics))\n    width = 0.25\n    for i, name in enumerate(consistency_data.keys()):\n        values = [consistency_data[name][metric] for metric in metrics]\n        ax.bar(x + i * width, values, width, label=name, alpha=0.7, color=colors[name])\n    ax.set_xticks(x + width)\n    ax.set_xticklabels(['Mean\\nSuccess Rate', 'Final\\nSuccess Rate', 'Consistency\\nTrend'])\n    ax.set_title('Performance Consistency')\n    ax.set_ylabel('Value')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[3, 1])\n    efficiency_data = {}\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        steps = np.array(data['steps'])\n        total_reward = np.sum(rewards)\n        total_steps = np.sum(steps)\n        efficiency = total_reward / total_steps if total_steps > 0 else 0\n        cumulative_rewards = np.cumsum(rewards)\n        cumulative_steps = np.cumsum(steps)\n        efficiency_over_time = cumulative_rewards / (cumulative_steps + 1e-8)\n        efficiency_data[name] = {\n            'overall_efficiency': efficiency,\n            'final_efficiency': efficiency_over_time[-1],\n            'efficiency_trend': np.polyfit(range(len(efficiency_over_time)), efficiency_over_time, 1)[0]\n        }\n    metrics = ['overall_efficiency', 'final_efficiency', 'efficiency_trend']\n    x = np.arange(len(metrics))\n    width = 0.25\n    for i, name in enumerate(efficiency_data.keys()):\n        values = [efficiency_data[name][metric] for metric in metrics]\n        ax.bar(x + i * width, values, width, label=name, alpha=0.7, color=colors[name])\n    ax.set_xticks(x + width)\n    ax.set_xticklabels(['Overall\\nEfficiency', 'Final\\nEfficiency', 'Efficiency\\nTrend'])\n    ax.set_title('Learning Efficiency')\n    ax.set_ylabel('Reward per Step')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[3, 2])\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        smoothed = pd.Series(rewards).rolling(10).mean()\n        convergence_threshold = 195\n        convergence_idx = np.where(smoothed >= convergence_threshold)[0]\n        if len(convergence_idx) > 0:\n            convergence_episode = convergence_idx[0]\n            ax.axvline(convergence_episode, color=colors[name], linestyle='--', alpha=0.7)\n            ax.text(convergence_episode, 195, f'{name}: {convergence_episode}', \n                   fontsize=8, color=colors[name], rotation=90)\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        smoothed = pd.Series(rewards).rolling(10).mean()\n        ax.plot(smoothed, label=name, color=colors[name], linewidth=2)\n    ax.axhline(y=195, color='orange', linestyle='-', alpha=0.7, label='Target')\n    ax.set_title('Convergence Pattern Analysis')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Smoothed Reward')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = fig.add_subplot(gs[3, 3])\n    summary_data = []\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        steps = np.array(data['steps'])\n        summary = {\n            'Algorithm': name,\n            'Final Avg': f\"{np.mean(rewards[-20:]):.1f}\",\n            'Best Episode': f\"{np.max(rewards):.1f}\",\n            'Success Rate': f\"{np.mean(rewards >= 195):.1%}\",\n            'Avg Steps': f\"{np.mean(steps):.1f}\"\n        }\n        summary_data.append(summary)\n    ax.axis('tight')\n    ax.axis('off')\n    table_data = []\n    headers = ['Algorithm', 'Final Avg', 'Best', 'Success Rate', 'Avg Steps']\n    for summary in summary_data:\n        row = [summary[header] for header in headers]\n        table_data.append(row)\n    table = ax.table(cellText=table_data, colLabels=headers, \n                    cellLoc='center', loc='center')\n    table.auto_set_font_size(False)\n    table.set_fontsize(8)\n    table.scale(1, 1.5)\n    for i in range(len(table_data)):\n        for j in range(len(headers)):\n            if j == 0:\n                table[(i+1, j)].set_facecolor(colors[table_data[i][0]])\n                table[(i+1, j)].set_text_props(weight='bold', color='white')\n    ax.set_title('Training Summary Statistics', fontsize=12, fontweight='bold')\n    plt.tight_layout()\n    plt.show()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"TRAINING DYNAMICS ANALYSIS SUMMARY\")\n    print(\"=\" * 60)\n    for name, data in results.items():\n        rewards = np.array(data['rewards'])\n        steps = np.array(data['steps'])\n        print(f\"\\n{name}:\")\n        print(f\"  Final Performance: {np.mean(rewards[-20:]):.1f} ± {np.std(rewards[-20:]):.1f}\")\n        print(f\"  Best Episode: {np.max(rewards):.1f}\")\n        print(f\"  Success Rate: {np.mean(rewards >= 195):.1%}\")\n        print(f\"  Average Steps: {np.mean(steps):.1f}\")\n        print(f\"  Convergence Episode: {convergence_data[name]['convergence_episode']}\")\n        print(f\"  Training Stability: {stability_metrics[name]['mean_std']:.2f}\")\n        print(f\"  Learning Efficiency: {efficiency_data[name]['overall_efficiency']:.3f}\")\n    env.close()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Training dynamics analysis completed!\")\n    print(\"=\" * 60)\n    return {\n        'results': results,\n        'convergence_data': convergence_data,\n        'stability_metrics': stability_metrics,\n        'efficiency_data': efficiency_data,\n        'consistency_data': consistency_data\n    }\ndynamics_analysis = analyze_training_dynamics()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XII. SUMMARY AND CONCLUSIONS\n",
    "\n",
    "### A. Key Findings and Insights\n",
    "\n",
    "Our comprehensive analysis of Deep Q-Networks and their variants has revealed several important insights:\n",
    "\n",
    "1. **Algorithm Performance**: Dueling DQN consistently outperforms both Basic DQN and Double DQN in terms of final performance and convergence speed\n",
    "2. **Training Stability**: Double DQN shows improved stability compared to Basic DQN, reducing overestimation bias\n",
    "3. **Sample Efficiency**: All variants demonstrate good sample efficiency, with Dueling DQN being the most efficient\n",
    "4. **Convergence Patterns**: Different algorithms exhibit distinct convergence patterns and learning dynamics\n",
    "\n",
    "### B. Practical Recommendations\n",
    "\n",
    "Based on our analysis, we recommend:\n",
    "\n",
    "- **For Simple Environments**: Basic DQN is sufficient and computationally efficient\n",
    "- **For Complex Environments**: Dueling DQN provides the best balance of performance and stability\n",
    "- **For Research**: Double DQN is valuable for understanding and mitigating overestimation bias\n",
    "- **For Production**: Consider the trade-offs between performance, computational cost, and implementation complexity\n",
    "\n",
    "### C. Future Research Directions\n",
    "\n",
    "Potential areas for future investigation include:\n",
    "\n",
    "1. **Advanced Architectures**: Rainbow DQN, Noisy Networks, and Distributional RL\n",
    "2. **Multi-Agent Systems**: Extending DQN to multi-agent environments\n",
    "3. **Continuous Control**: Adapting DQN for continuous action spaces\n",
    "4. **Transfer Learning**: Applying pre-trained DQN models to new environments\n",
    "\n",
    "### D. Educational Value\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "- **Theoretical Understanding**: Mathematical foundations of Q-learning and DQN\n",
    "- **Practical Implementation**: Modular, well-documented code structure\n",
    "- **Experimental Analysis**: Comprehensive evaluation and comparison methodologies\n",
    "- **Visualization Techniques**: Advanced plotting and analysis tools\n",
    "\n",
    "The modular design and comprehensive analysis make this an excellent resource for learning about Deep Q-Networks and value-based reinforcement learning methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\nprint(\"Project Structure and Module Overview\")\nprint(\"=\" * 60)\nprint(\"\\nDirectory Structure:\")\nprint(\"\"\"\nCA7/\n├── agents/\n│   ├── __init__.py\n│   ├── core.py\n│   ├── double_dqn.py\n│   ├── dueling_dqn.py\n│   └── utils.py\n├── experiments/\n│   ├── __init__.py\n│   ├── basic_dqn_experiment.py\n│   └── comprehensive_dqn_analysis.py\n├── training_examples.py\n├── requirements.txt\n├── CA7.ipynb\n└── README.md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Advanced Visualization Suite\n",
    "\n",
    "We provide comprehensive visualization tools to understand DQN behavior, training dynamics, and performance characteristics across different scenarios.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comprehensive_visualizations():\n    print(\"=\" * 60)\n    print(\"Advanced Visualization Suite\")\n    print(\"=\" * 60)\n    env = gym.make('CartPole-v1')\n    state_dim = env.observation_space.shape[0]\n    action_dim = env.action_space.n\n    print(\"Training DQN agent for visualization...\")\n    agent = DQNAgent(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        lr=1e-3,\n        epsilon_decay=0.995,\n        buffer_size=10000\n    )\n    episode_rewards = []\n    for episode in range(50):\n        reward, _ = agent.train_episode(env, max_steps=500)\n        episode_rewards.append(reward)\n    print(\"Creating comprehensive visualizations...\")\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    fig.suptitle('DQN Training Progress Dashboard', fontsize=16, fontweight='bold')\n    ax = axes[0, 0]\n    window = 5\n    smoothed_rewards = pd.Series(episode_rewards).rolling(window).mean()\n    ax.plot(episode_rewards, alpha=0.3, color='lightblue', label='Raw Rewards')\n    ax.plot(smoothed_rewards, color='blue', linewidth=2, label=f'Smoothed ({window})')\n    ax.axhline(y=195, color='red', linestyle='--', alpha=0.7, label='Target (195)')\n    ax.set_title('Learning Curve')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Episode Reward')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = axes[0, 1]\n    if agent.losses:\n        loss_window = 10\n        smoothed_losses = pd.Series(agent.losses).rolling(loss_window).mean()\n        ax.plot(agent.losses, alpha=0.3, color='lightcoral', label='Raw Loss')\n        ax.plot(smoothed_losses, color='red', linewidth=2, label=f'Smoothed ({loss_window})')\n        ax.set_title('Training Loss Evolution')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('MSE Loss')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    ax = axes[0, 2]\n    ax.plot(agent.epsilon_history, color='green', linewidth=2)\n    ax.set_title('Exploration Decay')\n    ax.set_xlabel('Training Step')\n    ax.set_ylabel('Epsilon Value')\n    ax.grid(True, alpha=0.3)\n    ax = axes[1, 0]\n    if agent.q_values_history:\n        q_window = 10\n        smoothed_q = pd.Series(agent.q_values_history).rolling(q_window).mean()\n        ax.plot(agent.q_values_history, alpha=0.3, color='lightgreen', label='Raw Q-values')\n        ax.plot(smoothed_q, color='green', linewidth=2, label=f'Smoothed ({q_window})')\n        ax.set_title('Q-Value Evolution')\n        ax.set_xlabel('Training Step')\n        ax.set_ylabel('Average Q-Value')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n    ax = axes[1, 1]\n    ax.hist(episode_rewards, bins=15, alpha=0.7, color='skyblue', edgecolor='black')\n    ax.axvline(np.mean(episode_rewards), color='red', linestyle='--', \n               label=f'Mean: {np.mean(episode_rewards):.1f}')\n    ax.axvline(np.median(episode_rewards), color='orange', linestyle='--', \n               label=f'Median: {np.median(episode_rewards):.1f}')\n    ax.set_title('Reward Distribution')\n    ax.set_xlabel('Episode Reward')\n    ax.set_ylabel('Frequency')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = axes[1, 2]\n    metrics = {\n        'Mean Reward': np.mean(episode_rewards),\n        'Std Reward': np.std(episode_rewards),\n        'Max Reward': np.max(episode_rewards),\n        'Min Reward': np.min(episode_rewards),\n        'Success Rate': np.mean(np.array(episode_rewards) >= 195)\n    }\n    bars = ax.bar(metrics.keys(), metrics.values(), alpha=0.7, color=['blue', 'green', 'red', 'orange', 'purple'])\n    ax.set_title('Performance Metrics')\n    ax.set_ylabel('Value')\n    ax.tick_params(axis='x', rotation=45)\n    for bar, value in zip(bars, metrics.values()):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n                f'{value:.2f}', ha='center', va='bottom')\n    plt.tight_layout()\n    plt.show()\n    print(\"\\nCreating Q-Value Analysis Dashboard...\")\n    sample_states = []\n    for _ in range(100):\n        state, _ = env.reset()\n        sample_states.append(state)\n        for _ in range(np.random.randint(1, 10)):\n            action = env.action_space.sample()\n            state, _, terminated, truncated, _ = env.step(action)\n            if terminated or truncated:\n                break\n            sample_states.append(state)\n    sample_states = np.array(sample_states[:100])\n    q_values_all = []\n    for state in sample_states:\n        q_vals = agent.get_q_values(state)\n        q_values_all.append(q_vals)\n    q_values_all = np.array(q_values_all)\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    fig.suptitle('Q-Value Analysis Dashboard', fontsize=16, fontweight='bold')\n    ax = axes[0, 0]\n    for i in range(action_dim):\n        ax.hist(q_values_all[:, i], bins=20, alpha=0.6, label=f'Action {i}', density=True)\n    ax.set_title('Q-Value Distributions by Action')\n    ax.set_xlabel('Q-Value')\n    ax.set_ylabel('Density')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = axes[0, 1]\n    im = ax.imshow(q_values_all.T, cmap='viridis', aspect='auto')\n    ax.set_title('Q-Value Heatmap (States vs Actions)')\n    ax.set_xlabel('State Index')\n    ax.set_ylabel('Action')\n    ax.set_yticks(range(action_dim))\n    ax.set_yticklabels([f'Action {i}' for i in range(action_dim)])\n    plt.colorbar(im, ax=ax, label='Q-Value')\n    ax = axes[0, 2]\n    q_stats = {\n        'Mean': np.mean(q_values_all),\n        'Std': np.std(q_values_all),\n        'Max': np.max(q_values_all),\n        'Min': np.min(q_values_all),\n        'Range': np.ptp(q_values_all)\n    }\n    bars = ax.bar(q_stats.keys(), q_stats.values(), alpha=0.7, color='lightcoral')\n    ax.set_title('Q-Value Statistics')\n    ax.set_ylabel('Value')\n    for bar, value in zip(bars, q_stats.values()):\n        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n                f'{value:.2f}', ha='center', va='bottom')\n    ax = axes[1, 0]\n    action_preferences = np.argmax(q_values_all, axis=1)\n    action_counts = np.bincount(action_preferences, minlength=action_dim)\n    ax.pie(action_counts, labels=[f'Action {i}' for i in range(action_dim)], \n           autopct='%1.1f%%', startangle=90)\n    ax.set_title('Action Preference Distribution')\n    ax = axes[1, 1]\n    if action_dim >= 2:\n        ax.scatter(q_values_all[:, 0], q_values_all[:, 1], alpha=0.6, s=20)\n        ax.set_xlabel('Q-Value Action 0')\n        ax.set_ylabel('Q-Value Action 1')\n        ax.set_title('Q-Value Correlation (Action 0 vs 1)')\n        corr = np.corrcoef(q_values_all[:, 0], q_values_all[:, 1])[0, 1]\n        ax.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n                transform=ax.transAxes, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n        ax.grid(True, alpha=0.3)\n    ax = axes[1, 2]\n    max_q_per_state = np.max(q_values_all, axis=1)\n    mean_q_per_state = np.mean(q_values_all, axis=1)\n    ax.scatter(mean_q_per_state, max_q_per_state, alpha=0.6, s=20)\n    ax.set_xlabel('Mean Q-Value Across Actions')\n    ax.set_ylabel('Max Q-Value')\n    ax.set_title('State-wise Q-Value Analysis')\n    min_val = min(ax.get_xlim()[0], ax.get_ylim()[0])\n    max_val = max(ax.get_xlim()[1], ax.get_ylim()[1])\n    ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5, label='y=x')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    print(\"\\nCreating Training Dynamics Analysis...\")\n    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n    fig.suptitle('Training Dynamics Analysis', fontsize=16, fontweight='bold')\n    ax = axes[0, 0]\n    episodes = np.arange(len(episode_rewards))\n    window = 10\n    rolling_mean = pd.Series(episode_rewards).rolling(window).mean()\n    rolling_std = pd.Series(episode_rewards).rolling(window).std()\n    ax.plot(episodes, episode_rewards, alpha=0.3, color='lightblue', label='Raw Rewards')\n    ax.plot(episodes, rolling_mean, color='blue', linewidth=2, label=f'Rolling Mean ({window})')\n    ax.fill_between(episodes, \n                    rolling_mean - rolling_std, \n                    rolling_mean + rolling_std, \n                    alpha=0.3, color='blue', label=f'±1 Std')\n    ax.axhline(y=195, color='red', linestyle='--', alpha=0.7, label='Target')\n    ax.set_title('Reward Progression with Confidence Intervals')\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Episode Reward')\n    ax.legend()\n    ax.grid(True, alpha=0.3)\n    ax = axes[0, 1]\n    if hasattr(agent, 'optimizer'):\n        lr = agent.optimizer.param_groups[0]['lr']\n        ax.bar(['Learning Rate'], [lr], alpha=0.7, color='green')\n        ax.set_title('Current Learning Rate')\n        ax.set_ylabel('Learning Rate')\n        ax.text(0, lr + lr*0.1, f'{lr:.2e}', ha='center', va='bottom')\n    ax = axes[1, 0]\n    buffer_utilization = len(agent.replay_buffer) / agent.replay_buffer.capacity\n    ax.pie([buffer_utilization, 1-buffer_utilization], \n           labels=['Used', 'Available'], \n           autopct='%1.1f%%', \n           colors=['lightblue', 'lightgray'])\n    ax.set_title(f'Replay Buffer Utilization\\n({len(agent.replay_buffer)}/{agent.replay_buffer.capacity})')\n    ax = axes[1, 1]\n    if len(episode_rewards) > 10:\n        early_performance = np.mean(episode_rewards[:10])\n        late_performance = np.mean(episode_rewards[-10:])\n        improvement = late_performance - early_performance\n        categories = ['Early (1-10)', 'Late (Last 10)', 'Improvement']\n        values = [early_performance, late_performance, improvement]\n        colors = ['lightcoral', 'lightgreen', 'gold']\n        bars = ax.bar(categories, values, alpha=0.7, color=colors)\n        ax.set_title('Training Efficiency Analysis')\n        ax.set_ylabel('Average Reward')\n        for bar, value in zip(bars, values):\n            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                    f'{value:.1f}', ha='center', va='bottom')\n        ax.grid(True, alpha=0.3)\n    plt.tight_layout()\n    plt.show()\n    env.close()\n    print(\"\\n\" + \"=\" * 60)\n    print(\"Comprehensive visualizations completed!\")\n    print(\"=\" * 60)\n    return {\n        'episode_rewards': episode_rewards,\n        'q_values_all': q_values_all,\n        'sample_states': sample_states,\n        'agent': agent\n    }\nviz_results = create_comprehensive_visualizations()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}