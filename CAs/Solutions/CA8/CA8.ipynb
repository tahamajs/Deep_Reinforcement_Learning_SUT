{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5044d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA8: Causal Reasoning and Multi-Modal Reinforcement Learning\n",
    "\n",
    "This notebook explores advanced topics in deep reinforcement learning, focusing on:\n",
    "1. Causal discovery and reasoning\n",
    "2. Multi-modal environments\n",
    "3. Causal RL agents\n",
    "4. Counterfactual reasoning\n",
    "\n",
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34c52a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Import our custom modules\n",
    "from causal_rl_utils import device\n",
    "from causal_discovery import CausalGraph, CausalDiscovery\n",
    "from causal_rl_agent import CausalRLAgent, CounterfactualRLAgent, CausalReasoningNetwork\n",
    "from multi_modal_env import MultiModalGridWorld, MultiModalWrapper\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99890099",
   "metadata": {},
   "source": [
    "## Section 1: Causal Discovery\n",
    "\n",
    "In this section, we explore methods for learning causal structure from observational data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e070b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Causal Graph functionality\n",
    "\n",
    "def demonstrate_causal_graph():\n",
    "    \"\"\"Demonstrate basic causal graph operations\"\"\"\n",
    "    print(\"=== Causal Graph Demonstration ===\")\n",
    "    \n",
    "    # Create a simple causal graph\n",
    "    variables = ['A', 'B', 'C', 'D']\n",
    "    graph = CausalGraph(variables)\n",
    "    \n",
    "    # Add some causal relationships\n",
    "    graph.add_edge('A', 'B')\n",
    "    graph.add_edge('A', 'C')\n",
    "    graph.add_edge('B', 'D')\n",
    "    graph.add_edge('C', 'D')\n",
    "    \n",
    "    print(f\"Variables: {graph.variables}\")\n",
    "    print(f\"Graph structure: {graph}\")\n",
    "    \n",
    "    # Test graph properties\n",
    "    print(f\"Is DAG: {graph.is_dag()}\")\n",
    "    print(f\"Topological order: {graph.get_topological_order()}\")\n",
    "    \n",
    "    # Test parent/child relationships\n",
    "    print(f\"Parents of D: {graph.get_parents('D')}\")\n",
    "    print(f\"Children of A: {graph.get_children('A')}\")\n",
    "    print(f\"Ancestors of D: {graph.get_ancestors('D')}\")\n",
    "    print(f\"Descendants of A: {graph.get_descendants('A')}\")\n",
    "    \n",
    "    # Visualize the graph\n",
    "    try:\n",
    "        import networkx as nx\n",
    "        G = graph.to_networkx()\n",
    "        pos = nx.spring_layout(G)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        nx.draw(G, pos, with_labels=True, node_color='lightblue', \n",
    "                node_size=2000, font_size=16, arrows=True, arrowsize=20)\n",
    "        plt.title(\"Causal Graph Visualization\")\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"NetworkX not available for visualization\")\n",
    "    \n",
    "    return graph\n",
    "\n",
    "# Run demonstration\n",
    "causal_graph = demonstrate_causal_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702726d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate causal discovery algorithms\n",
    "\n",
    "def demonstrate_causal_discovery():\n",
    "    \"\"\"Demonstrate causal discovery from data\"\"\"\n",
    "    print(\"=== Causal Discovery Demonstration ===\")\n",
    "    \n",
    "    # Generate synthetic data with known causal structure\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    n_vars = 4\n",
    "    \n",
    "    # True causal structure: A -> B -> D <- C <- A\n",
    "    A = np.random.normal(0, 1, n_samples)\n",
    "    C = A + np.random.normal(0, 0.5, n_samples)\n",
    "    B = A + np.random.normal(0, 0.5, n_samples)\n",
    "    D = B + C + np.random.normal(0, 0.5, n_samples)\n",
    "    \n",
    "    data = np.column_stack([A, B, C, D])\n",
    "    var_names = ['A', 'B', 'C', 'D']\n",
    "    \n",
    "    print(\"Generated data with true causal structure: A -> B, A -> C, B -> D, C -> D\")\n",
    "    \n",
    "    # Apply different discovery algorithms\n",
    "    algorithms = {\n",
    "        'PC Algorithm': CausalDiscovery.pc_algorithm,\n",
    "        'GES Algorithm': CausalDiscovery.ges_algorithm,\n",
    "        'LiNGAM': CausalDiscovery.lingam_algorithm\n",
    "    }\n",
    "    \n",
    "    discovered_graphs = {}\n",
    "    \n",
    "    for name, algorithm in algorithms.items():\n",
    "        try:\n",
    "            graph = algorithm(data, var_names)\n",
    "            discovered_graphs[name] = graph\n",
    "            print(f\"\\n{name} discovered structure:\")\n",
    "            print(graph)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n{name} failed: {e}\")\n",
    "    \n",
    "    return discovered_graphs\n",
    "\n",
    "# Run causal discovery\n",
    "discovered_graphs = demonstrate_causal_discovery()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46bc676",
   "metadata": {},
   "source": [
    "## Section 2: Causal Reinforcement Learning\n",
    "\n",
    "Now we implement RL agents that leverage causal structure for improved learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26af5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Causal RL Agent\n",
    "\n",
    "def demonstrate_causal_rl():\n",
    "    \"\"\"Demonstrate causal RL agent on a simple environment\"\"\"\n",
    "    print(\"=== Causal RL Agent Demonstration ===\")\n",
    "    \n",
    "    # Create a simple grid world environment\n",
    "    class SimpleGridWorld:\n",
    "        \"\"\"Simple grid world for testing\"\"\"\n",
    "        def __init__(self, size=5):\n",
    "            self.size = size\n",
    "            self.state_dim = 2  # position only\n",
    "            self.action_dim = 4  # up, down, left, right\n",
    "            \n",
    "        def reset(self):\n",
    "            self.pos = np.random.randint(0, self.size, 2)\n",
    "            return self.pos.astype(float), {}\n",
    "            \n",
    "        def step(self, action):\n",
    "            # Action mapping\n",
    "            moves = [(-1, 0), (1, 0), (0, -1), (0, 1)]  # up, down, left, right\n",
    "            new_pos = self.pos + np.array(moves[action])\n",
    "            \n",
    "            # Check bounds\n",
    "            new_pos = np.clip(new_pos, 0, self.size - 1)\n",
    "            self.pos = new_pos\n",
    "            \n",
    "            # Simple reward: distance to center\n",
    "            center = np.array([self.size//2, self.size//2])\n",
    "            dist = np.linalg.norm(self.pos - center)\n",
    "            reward = -dist / (self.size * np.sqrt(2))\n",
    "            \n",
    "            return self.pos.astype(float), reward, False, False, {}\n",
    "    \n",
    "    env = SimpleGridWorld()\n",
    "    \n",
    "    # Create causal graph for the environment\n",
    "    # Assume position affects reward through distance to center\n",
    "    variables = ['pos_x', 'pos_y', 'distance', 'reward']\n",
    "    causal_graph = CausalGraph(variables)\n",
    "    causal_graph.add_edge('pos_x', 'distance')\n",
    "    causal_graph.add_edge('pos_y', 'distance')\n",
    "    causal_graph.add_edge('distance', 'reward')\n",
    "    \n",
    "    print(f\"Environment causal graph: {causal_graph}\")\n",
    "    \n",
    "    # Create causal RL agent\n",
    "    agent = CausalRLAgent(\n",
    "        state_dim=env.state_dim,\n",
    "        action_dim=env.action_dim,\n",
    "        causal_graph=causal_graph,\n",
    "        lr=1e-3\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    print(\"\\nTraining Causal RL Agent...\")\n",
    "    rewards = []\n",
    "    \n",
    "    for episode in range(100):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        for step in range(20):\n",
    "            action, _ = agent.select_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            \n",
    "            # Simple training data\n",
    "            agent.update([state], [action], [reward], [next_state], [done])\n",
    "            \n",
    "            episode_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        rewards.append(episode_reward)\n",
    "        \n",
    "        if (episode + 1) % 20 == 0:\n",
    "            avg_reward = np.mean(rewards[-20:])\n",
    "            print(f\"Episode {episode+1:3d} | Avg Reward: {avg_reward:.3f}\")\n",
    "    \n",
    "    # Test causal interventions\n",
    "    print(\"\\nTesting causal interventions...\")\n",
    "    test_state = np.array([2.0, 2.0])  # Center position\n",
    "    \n",
    "    # Original prediction\n",
    "    original_action, _ = agent.select_action(test_state, deterministic=True)\n",
    "    print(f\"Original state {test_state}: Action {original_action}\")\n",
    "    \n",
    "    # Intervene on position\n",
    "    intervention = {'pos_x': 0.0, 'pos_y': 0.0}  # Move to corner\n",
    "    intervened_state = agent.perform_intervention(test_state, intervention)\n",
    "    intervened_action, _ = agent.select_action(intervened_state, deterministic=True)\n",
    "    print(f\"After intervention {intervention}: Action {intervened_action}\")\n",
    "    \n",
    "    return {\n",
    "        'agent': agent,\n",
    "        'environment': env,\n",
    "        'rewards': rewards,\n",
    "        'causal_graph': causal_graph\n",
    "    }\n",
    "\n",
    "# Run demonstration\n",
    "causal_rl_results = demonstrate_causal_rl()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a40cce",
   "metadata": {},
   "source": [
    "## Section 3: Multi-Modal Environments\n",
    "\n",
    "This section explores environments that provide multiple modalities of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaadf1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Multi-Modal Environment\n",
    "\n",
    "def demonstrate_multi_modal_env():\n",
    "    \"\"\"Demonstrate multi-modal grid world environment\"\"\"\n",
    "    print(\"=== Multi-Modal Environment Demonstration ===\")\n",
    "    \n",
    "    # Create multi-modal environment\n",
    "    env = MultiModalGridWorld(size=6, render_size=84)\n",
    "    \n",
    "    # Reset and get observation\n",
    "    obs, _ = env.reset()\n",
    "    \n",
    "    print(\"Observation modalities:\")\n",
    "    print(f\"- Visual: {obs['visual'].shape} (RGB image)\")\n",
    "    print(f\"- Text: {obs['text']['text']}\")\n",
    "    print(f\"- State: {obs['state']} (agent position)\")\n",
    "    \n",
    "    # Take some random actions and show observations\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    for i in range(6):\n",
    "        action = np.random.randint(0, 4)\n",
    "        next_obs, reward, done, _, _ = env.step(action)\n",
    "        \n",
    "        ax = axes[i // 3, i % 3]\n",
    "        ax.imshow(next_obs['visual'])\n",
    "        ax.set_title(f\"Step {i+1}: {next_obs['text']['text'][:30]}...\")\n",
    "        ax.axis('off')\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Demonstrate multi-modal wrapper\n",
    "    wrapper = MultiModalWrapper(env)\n",
    "    processed_obs = wrapper.process_observation(obs)\n",
    "    \n",
    "    print(f\"\\nProcessed observation shape: {processed_obs.shape}\")\n",
    "    print(f\"Feature breakdown:\")\n",
    "    print(f\"- Visual features: {wrapper.visual_dim}\")\n",
    "    print(f\"- Text features: {wrapper.text_dim}\")\n",
    "    print(f\"- State features: {wrapper.state_dim}\")\n",
    "    \n",
    "    return env, wrapper\n",
    "\n",
    "# Run demonstration\n",
    "mm_env, mm_wrapper = demonstrate_multi_modal_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30b4a9a",
   "metadata": {},
   "source": [
    "## Section 4: Integrated Causal Multi-Modal RL\n",
    "\n",
    "Combining causal reasoning with multi-modal perception for advanced RL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37de8975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated demonstration combining all components\n",
    "\n",
    "def demonstrate_integrated_system():\n",
    "    \"\"\"Demonstrate integrated causal multi-modal RL system\"\"\"\n",
    "    print(\"=== Integrated Causal Multi-Modal RL Demonstration ===\")\n",
    "    \n",
    "    # Create multi-modal environment\n",
    "    env = MultiModalGridWorld(size=4, render_size=64)  # Smaller for faster training\n",
    "    wrapper = MultiModalWrapper(env)\n",
    "    \n",
    "    # Create causal graph for multi-modal setting\n",
    "    variables = ['agent_x', 'agent_y', 'goal_x', 'goal_y', 'visual_features', 'text_features', 'reward']\n",
    "    causal_graph = CausalGraph(variables)\n",
    "    \n",
    "    # Define causal relationships\n",
    "    causal_graph.add_edge('agent_x', 'visual_features')\n",
    "    causal_graph.add_edge('agent_y', 'visual_features')\n",
    "    causal_graph.add_edge('goal_x', 'visual_features')\n",
    "    causal_graph.add_edge('goal_y', 'visual_features')\n",
    "    causal_graph.add_edge('agent_x', 'text_features')\n",
    "    causal_graph.add_edge('agent_y', 'text_features')\n",
    "    causal_graph.add_edge('goal_x', 'text_features')\n",
    "    causal_graph.add_edge('goal_y', 'text_features')\n",
    "    causal_graph.add_edge('visual_features', 'reward')\n",
    "    causal_graph.add_edge('text_features', 'reward')\n",
    "    \n",
    "    print(f\"Causal graph for multi-modal RL: {causal_graph}\")\n",
    "    \n",
    "    # Create causal RL agent (adapted for multi-modal)\n",
    "    class MultiModalCausalRLAgent(CausalRLAgent):\n",
    "        \"\"\"Causal RL agent adapted for multi-modal observations\"\"\"\n",
    "        \n",
    "        def __init__(self, wrapper, causal_graph, lr=1e-3):\n",
    "            self.wrapper = wrapper\n",
    "            state_dim = wrapper.total_dim\n",
    "            action_dim = 4  # grid world actions\n",
    "            super().__init__(state_dim, action_dim, causal_graph, lr)\n",
    "        \n",
    "        def select_action(self, obs, deterministic=False):\n",
    "            \"\"\"Select action from multi-modal observation\"\"\"\n",
    "            # Process observation\n",
    "            state = self.wrapper.process_observation(obs)\n",
    "            return super().select_action(state, deterministic)\n",
    "        \n",
    "        def train_episode(self, env):\n",
    "            \"\"\"Train for one episode with multi-modal observations\"\"\"\n",
    "            obs, _ = env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            states, actions, rewards, next_obss, dones = [], [], [], [], []\n",
    "            \n",
    "            while steps < env.max_steps:\n",
    "                action, _ = self.select_action(obs)\n",
    "                next_obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "                \n",
    "                # Store processed states\n",
    "                states.append(self.wrapper.process_observation(obs))\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                next_obss.append(self.wrapper.process_observation(next_obs))\n",
    "                dones.append(done)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                steps += 1\n",
    "                obs = next_obs\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            # Update agent\n",
    "            if len(states) > 0:\n",
    "                self.update(states, actions, rewards, next_obss, dones)\n",
    "            \n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            return episode_reward, steps\n",
    "    \n",
    "    # Create and train agent\n",
    "    agent = MultiModalCausalRLAgent(wrapper, causal_graph, lr=1e-3)\n",
    "    \n",
    "    print(\"\\nTraining Multi-Modal Causal RL Agent...\")\n",
    "    training_rewards = []\n",
    "    \n",
    "    for episode in range(50):  # Shorter training for demo\n",
    "        reward, steps = agent.train_episode(env)\n",
    "        training_rewards.append(reward)\n",
    "        \n",
    "        if (episode + 1) % 10 == 0:\n",
    "            avg_reward = np.mean(training_rewards[-10:])\n",
    "            print(f\"Episode {episode+1:2d} | Avg Reward: {avg_reward:.3f} | Steps: {steps}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # Training curve\n",
    "    axes[0].plot(training_rewards)\n",
    "    axes[0].plot(pd.Series(training_rewards).rolling(5).mean(), \n",
    "                 color='red', label='Moving Average')\n",
    "    axes[0].set_title('Multi-Modal Causal RL Training')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Episode Reward')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sample environment render\n",
    "    obs, _ = env.reset()\n",
    "    axes[1].imshow(obs['visual'])\n",
    "    axes[1].set_title(f'Environment Render\\n{obs[\"text\"][\"text\"]}')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'agent': agent,\n",
    "        'environment': env,\n",
    "        'wrapper': wrapper,\n",
    "        'training_rewards': training_rewards,\n",
    "        'causal_graph': causal_graph\n",
    "    }\n",
    "\n",
    "# Run integrated demonstration\n",
    "integrated_results = demonstrate_integrated_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025938ad",
   "metadata": {},
   "source": [
    "## Section 5: Comprehensive Experiments\n",
    "\n",
    "Running comprehensive experiments to compare different approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbb64d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive experiments comparing different approaches\n",
    "\n",
    "def run_comprehensive_experiments():\n",
    "    \"\"\"Run comprehensive experiments comparing different RL approaches\"\"\"\n",
    "    print(\"=== Comprehensive RL Experiments ===\")\n",
    "    \n",
    "    # Create environments\n",
    "    simple_env = MultiModalGridWorld(size=5, render_size=64)\n",
    "    wrapper = MultiModalWrapper(simple_env)\n",
    "    \n",
    "    # Setup causal graph\n",
    "    variables = ['agent_x', 'agent_y', 'goal_x', 'goal_y', 'visual', 'text', 'reward']\n",
    "    causal_graph = CausalGraph(variables)\n",
    "    causal_graph.add_edge('agent_x', 'visual')\n",
    "    causal_graph.add_edge('agent_y', 'visual')\n",
    "    causal_graph.add_edge('goal_x', 'visual')\n",
    "    causal_graph.add_edge('goal_y', 'visual')\n",
    "    causal_graph.add_edge('agent_x', 'text')\n",
    "    causal_graph.add_edge('agent_y', 'text')\n",
    "    causal_graph.add_edge('goal_x', 'text')\n",
    "    causal_graph.add_edge('goal_y', 'text')\n",
    "    causal_graph.add_edge('visual', 'reward')\n",
    "    causal_graph.add_edge('text', 'reward')\n",
    "    \n",
    "    # Experiment configurations\n",
    "    experiments = {\n",
    "        'Standard RL': {'use_causal': False, 'use_multi_modal': False},\n",
    "        'Multi-Modal RL': {'use_causal': False, 'use_multi_modal': True},\n",
    "        'Causal RL': {'use_causal': True, 'use_multi_modal': False},\n",
    "        'Causal Multi-Modal RL': {'use_causal': True, 'use_multi_modal': True}\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for exp_name, config in experiments.items():\n",
    "        print(f\"\\n--- Running {exp_name} ---\")\n",
    "        \n",
    "        if config['use_causal']:\n",
    "            if config['use_multi_modal']:\n",
    "                # Causal Multi-Modal Agent\n",
    "                class ExpAgent(MultiModalCausalRLAgent):\n",
    "                    pass\n",
    "                agent = ExpAgent(wrapper, causal_graph)\n",
    "            else:\n",
    "                # Causal Agent (simplified state)\n",
    "                agent = CausalRLAgent(\n",
    "                    state_dim=2, action_dim=4, causal_graph=causal_graph\n",
    "                )\n",
    "        else:\n",
    "            if config['use_multi_modal']:\n",
    "                # Multi-Modal Agent (no causal reasoning)\n",
    "                class ExpAgent(CausalRLAgent):\n",
    "                    def __init__(self, wrapper):\n",
    "                        self.wrapper = wrapper\n",
    "                        super().__init__(wrapper.total_dim, 4, causal_graph)\n",
    "                        \n",
    "                    def select_action(self, obs, deterministic=False):\n",
    "                        state = self.wrapper.process_observation(obs)\n",
    "                        return super().select_action(state, deterministic)\n",
    "                agent = ExpAgent(wrapper)\n",
    "            else:\n",
    "                # Standard Agent\n",
    "                agent = CausalRLAgent(2, 4, causal_graph)\n",
    "        \n",
    "        # Train agent\n",
    "        rewards = []\n",
    "        for episode in range(30):  # Short training for demo\n",
    "            if config['use_multi_modal']:\n",
    "                reward, _ = agent.train_episode(simple_env)\n",
    "            else:\n",
    "                # Simple training for non-multi-modal\n",
    "                state, _ = simple_env.reset()\n",
    "                episode_reward = 0\n",
    "                for step in range(10):\n",
    "                    action, _ = agent.select_action(state.astype(float))\n",
    "                    next_state, reward, done, _, _ = simple_env.step(action)\n",
    "                    agent.update([state.astype(float)], [action], [reward], \n",
    "                               [next_state.astype(float)], [done])\n",
    "                    episode_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "                reward = episode_reward\n",
    "            \n",
    "            rewards.append(reward)\n",
    "        \n",
    "        results[exp_name] = {\n",
    "            'rewards': rewards,\n",
    "            'final_avg': np.mean(rewards[-10:]),\n",
    "            'config': config\n",
    "        }\n",
    "        \n",
    "        print(f\"{exp_name}: Final Avg Reward = {results[exp_name]['final_avg']:.3f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Training curves\n",
    "    for exp_name, result in results.items():\n",
    "        axes[0].plot(result['rewards'], label=exp_name, linewidth=2)\n",
    "    \n",
    "    axes[0].set_title('Training Performance Comparison')\n",
    "    axes[0].set_xlabel('Episode')\n",
    "    axes[0].set_ylabel('Episode Reward')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Final performance bar chart\n",
    "    exp_names = list(results.keys())\n",
    "    final_scores = [results[name]['final_avg'] for name in exp_names]\n",
    "    \n",
    "    bars = axes[1].bar(exp_names, final_scores, color=['blue', 'green', 'red', 'purple'], alpha=0.7)\n",
    "    axes[1].set_title('Final Performance Comparison')\n",
    "    axes[1].set_ylabel('Average Reward (Last 10 Episodes)')\n",
    "    axes[1].tick_params(axis='x', rotation=45)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, score in zip(bars, final_scores):\n",
    "        axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                    f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n=== Experiment Summary ===\")\n",
    "    for exp_name, result in results.items():\n",
    "        config = result['config']\n",
    "        causal_status = \"✓\" if config['use_causal'] else \"✗\"\n",
    "        modal_status = \"✓\" if config['use_multi_modal'] else \"✗\"\n",
    "        print(f\"{exp_name:20s}: Causal={causal_status} Multi-Modal={modal_status} \"\n",
    "              f\"Final Score={result['final_avg']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive experiments\n",
    "experiment_results = run_comprehensive_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3876a63a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Causal Discovery**: Learning causal structure from data using PC, GES, and LiNGAM algorithms\n",
    "2. **Causal RL Agents**: Agents that leverage causal reasoning for improved decision making\n",
    "3. **Multi-Modal Environments**: Environments providing visual, textual, and state information\n",
    "4. **Integrated Systems**: Combining causal reasoning with multi-modal perception\n",
    "\n",
    "Key insights:\n",
    "- Causal reasoning can improve sample efficiency and interpretability\n",
    "- Multi-modal information provides richer representations for learning\n",
    "- Combining both approaches leads to more robust and capable RL systems\n",
    "\n",
    "The modular design allows for easy extension and experimentation with different causal discovery methods, RL algorithms, and multi-modal architectures."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
