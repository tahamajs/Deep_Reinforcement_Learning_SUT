{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c444804b",
   "metadata": {},
   "source": [
    "# CA12: Multi-Agent Reinforcement Learning and Advanced Policy Methods\n",
    "\n",
    "## Deep Reinforcement Learning - Session 12\n",
    "\n",
    "**Multi-Agent Reinforcement Learning (MARL), Advanced Policy Gradient Methods, and Distributed Training**\n",
    "\n",
    "This notebook explores advanced reinforcement learning topics including multi-agent systems, sophisticated policy gradient methods, distributed training techniques, and modern approaches to collaborative and competitive learning environments.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand multi-agent reinforcement learning fundamentals\n",
    "2. Implement cooperative and competitive MARL algorithms\n",
    "3. Master advanced policy gradient methods (PPO, TRPO, SAC variants)\n",
    "4. Explore distributed training and asynchronous methods\n",
    "5. Implement communication and coordination mechanisms\n",
    "6. Understand game-theoretic foundations of MARL\n",
    "7. Apply meta-learning and few-shot adaptation\n",
    "8. Analyze emergent behaviors in multi-agent systems\n",
    "\n",
    "### Notebook Structure:\n",
    "1. **Multi-Agent Foundations** - Game theory and MARL basics\n",
    "2. **Cooperative Multi-Agent Learning** - Centralized training, decentralized execution\n",
    "3. **Competitive and Mixed-Motive Systems** - Self-play and adversarial training\n",
    "4. **Advanced Policy Methods** - PPO variants, SAC improvements, TRPO\n",
    "5. **Distributed Reinforcement Learning** - A3C, IMPALA, and modern distributed methods\n",
    "6. **Communication and Coordination** - Message passing and emergent communication\n",
    "7. **Meta-Learning in RL** - Few-shot adaptation and transfer learning\n",
    "8. **Comprehensive Applications** - Real-world multi-agent scenarios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0788ebcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cvxpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m minimize, linprog\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m softmax\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcvxpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcp\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Set random seeds for reproducibility\u001b[39;00m\n\u001b[32m     47\u001b[39m SEED = \u001b[32m42\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cvxpy'"
     ]
    }
   ],
   "source": [
    "# Essential Imports and Advanced Setup for Multi-Agent RL\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Categorical, MultivariateNormal, kl_divergence\n",
    "import torch.multiprocessing as mp\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict, deque, namedtuple\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import copy\n",
    "import time\n",
    "import threading\n",
    "from typing import Tuple, List, Dict, Optional, Union, NamedTuple, Any\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "import itertools\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Advanced imports for multi-agent systems\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "import torch.distributed as dist\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import networkx as nx\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Game theory and optimization\n",
    "from scipy.optimize import minimize, linprog\n",
    "from scipy.special import softmax\n",
    "import cvxpy as cp\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Device configuration with multi-GPU support\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "n_gpus = torch.cuda.device_count()\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(f\"ðŸ¤– Multi-Agent Reinforcement Learning Environment Setup\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Available GPUs: {n_gpus}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Advanced plotting configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 11\n",
    "plt.rcParams['ytick.labelsize'] = 11\n",
    "plt.rcParams['legend.fontsize'] = 11\n",
    "\n",
    "# Color schemes for multi-agent visualizations\n",
    "agent_colors = sns.color_palette(\"Set2\", 8)\n",
    "performance_colors = sns.color_palette(\"viridis\", 6)\n",
    "sns.set_palette(agent_colors)\n",
    "\n",
    "# Configuration classes for advanced RL\n",
    "@dataclass\n",
    "class MultiAgentConfig:\n",
    "    \"\"\"Configuration for multi-agent systems.\"\"\"\n",
    "    n_agents: int = 2\n",
    "    state_dim: int = 10\n",
    "    action_dim: int = 4\n",
    "    hidden_dim: int = 128\n",
    "    lr: float = 3e-4\n",
    "    gamma: float = 0.99\n",
    "    tau: float = 0.005\n",
    "    batch_size: int = 256\n",
    "    buffer_size: int = 100000\n",
    "    update_freq: int = 10\n",
    "    communication: bool = False\n",
    "    message_dim: int = 32\n",
    "    coordination_mechanism: str = \"centralized\"  # centralized, decentralized, mixed\n",
    "\n",
    "@dataclass \n",
    "class PolicyConfig:\n",
    "    \"\"\"Configuration for advanced policy methods.\"\"\"\n",
    "    algorithm: str = \"PPO\"  # PPO, TRPO, SAC, DDPG, TD3\n",
    "    clip_ratio: float = 0.2\n",
    "    target_kl: float = 0.01\n",
    "    entropy_coef: float = 0.01\n",
    "    value_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    n_epochs: int = 10\n",
    "    minibatch_size: int = 64\n",
    "    use_gae: bool = True\n",
    "    gae_lambda: float = 0.95\n",
    "\n",
    "# Global configurations\n",
    "ma_config = MultiAgentConfig()\n",
    "policy_config = PolicyConfig()\n",
    "\n",
    "print(\"âœ… Multi-Agent RL environment setup complete!\")\n",
    "print(f\"ðŸŽ¯ Configuration: {ma_config.n_agents} agents, {ma_config.coordination_mechanism} coordination\")\n",
    "print(\"ðŸš€ Ready for advanced multi-agent reinforcement learning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e214d5e",
   "metadata": {},
   "source": [
    "# Section 1: Multi-Agent Foundations and Game Theory\n",
    "\n",
    "## 1.1 Theoretical Foundation\n",
    "\n",
    "### Multi-Agent Reinforcement Learning (MARL)\n",
    "\n",
    "Multi-Agent Reinforcement Learning extends single-agent RL to environments with multiple learning agents. Key challenges include:\n",
    "\n",
    "1. **Non-stationarity**: The environment appears non-stationary from each agent's perspective as other agents learn\n",
    "2. **Partial observability**: Agents may have limited information about others' actions and observations\n",
    "3. **Credit assignment**: Determining individual contributions to team rewards\n",
    "4. **Scalability**: Computational complexity grows exponentially with number of agents\n",
    "5. **Equilibrium concepts**: Finding stable solutions in multi-agent settings\n",
    "\n",
    "### Game-Theoretic Foundations\n",
    "\n",
    "**Nash Equilibrium**: A strategy profile where no agent can improve by unilaterally changing strategy.\n",
    "\n",
    "For agents $i = 1, ..., n$ with strategy spaces $S_i$ and utility functions $u_i(s_1, ..., s_n)$:\n",
    "$$s^* = (s_1^*, ..., s_n^*) \\text{ is a Nash equilibrium if } \\forall i, s_i: u_i(s_i^*, s_{-i}^*) \\geq u_i(s_i, s_{-i}^*)$$\n",
    "\n",
    "**Pareto Optimality**: A strategy profile is Pareto optimal if no other profile improves at least one agent's utility without decreasing another's.\n",
    "\n",
    "**Stackelberg Equilibrium**: Leader-follower game structure where one agent commits to a strategy first.\n",
    "\n",
    "### MARL Paradigms\n",
    "\n",
    "1. **Independent Learning**: Each agent treats others as part of the environment\n",
    "2. **Joint Action Learning**: Agents learn about others' actions and adapt accordingly  \n",
    "3. **Multi-Agent Actor-Critic (MAAC)**: Centralized training with decentralized execution\n",
    "4. **Communication-Based Learning**: Agents exchange information to coordinate\n",
    "\n",
    "### Cooperation vs Competition Spectrum\n",
    "\n",
    "- **Fully Cooperative**: Shared reward, common goal (e.g., team sports)\n",
    "- **Fully Competitive**: Zero-sum game (e.g., adversarial settings)\n",
    "- **Mixed-Motive**: Partially cooperative and competitive (e.g., resource sharing)\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "**Multi-Agent MDP (MMDP)**:\n",
    "- State space: $\\mathcal{S}$\n",
    "- Joint action space: $\\mathcal{A} = \\mathcal{A}_1 \\times ... \\times \\mathcal{A}_n$\n",
    "- Transition dynamics: $P(s'|s, a_1, ..., a_n)$\n",
    "- Reward functions: $R_i(s, a_1, ..., a_n, s')$ for each agent $i$\n",
    "- Discount factor: $\\gamma \\in [0, 1)$\n",
    "\n",
    "**Policy Gradient in MARL**:\n",
    "$$\\nabla_{\\theta_i} J_i(\\theta_i) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta}}[\\sum_{t=0}^T \\nabla_{\\theta_i} \\log \\pi_{\\theta_i}(a_{i,t}|o_{i,t}) A_i^t]$$\n",
    "\n",
    "Where $A_i^t$ is agent $i$'s advantage at time $t$, which can be computed using various methods including multi-agent value functions.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4ac09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Game Theory Utilities and Basic Multi-Agent Framework\n",
    "\n",
    "class GameTheoryUtils:\n",
    "    \"\"\"Utility class for game-theoretic analysis.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def find_nash_equilibria(payoff_matrices):\n",
    "        \"\"\"\n",
    "        Find pure strategy Nash equilibria for n-player games.\n",
    "        \n",
    "        Args:\n",
    "            payoff_matrices: List of payoff matrices, one per player\n",
    "        Returns:\n",
    "            List of Nash equilibrium strategy profiles\n",
    "        \"\"\"\n",
    "        n_players = len(payoff_matrices)\n",
    "        if n_players != 2:\n",
    "            raise NotImplementedError(\"Only 2-player games supported\")\n",
    "            \n",
    "        matrix_a, matrix_b = payoff_matrices[0], payoff_matrices[1]\n",
    "        nash_equilibria = []\n",
    "        \n",
    "        rows, cols = matrix_a.shape\n",
    "        \n",
    "        for i in range(rows):\n",
    "            for j in range(cols):\n",
    "                # Check if (i,j) is a Nash equilibrium\n",
    "                is_nash = True\n",
    "                \n",
    "                # Check if player 1 wants to deviate\n",
    "                for i_prime in range(rows):\n",
    "                    if matrix_a[i_prime, j] > matrix_a[i, j]:\n",
    "                        is_nash = False\n",
    "                        break\n",
    "                \n",
    "                # Check if player 2 wants to deviate\n",
    "                if is_nash:\n",
    "                    for j_prime in range(cols):\n",
    "                        if matrix_b[i, j_prime] > matrix_b[i, j]:\n",
    "                            is_nash = False\n",
    "                            break\n",
    "                \n",
    "                if is_nash:\n",
    "                    nash_equilibria.append((i, j))\n",
    "        \n",
    "        return nash_equilibria\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_pareto_optimal(payoff_matrices, strategy_profile):\n",
    "        \"\"\"Check if a strategy profile is Pareto optimal.\"\"\"\n",
    "        current_payoffs = [matrix[strategy_profile] for matrix in payoff_matrices]\n",
    "        \n",
    "        # Check all other strategy profiles\n",
    "        for profile in itertools.product(*[range(matrix.shape[i]) for i, matrix in enumerate(payoff_matrices)]):\n",
    "            if profile == strategy_profile:\n",
    "                continue\n",
    "                \n",
    "            candidate_payoffs = [matrix[profile] for matrix in payoff_matrices]\n",
    "            \n",
    "            # Check if candidate dominates current\n",
    "            dominates = True\n",
    "            strictly_better = False\n",
    "            \n",
    "            for i in range(len(current_payoffs)):\n",
    "                if candidate_payoffs[i] < current_payoffs[i]:\n",
    "                    dominates = False\n",
    "                    break\n",
    "                elif candidate_payoffs[i] > current_payoffs[i]:\n",
    "                    strictly_better = True\n",
    "            \n",
    "            if dominates and strictly_better:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_best_response(payoff_matrix, opponent_strategy):\n",
    "        \"\"\"Compute best response to opponent's mixed strategy.\"\"\"\n",
    "        expected_payoffs = payoff_matrix @ opponent_strategy\n",
    "        return np.zeros_like(expected_payoffs).at[np.argmax(expected_payoffs)].set(1.0)\n",
    "\n",
    "class MultiAgentEnvironment:\n",
    "    \"\"\"Base class for multi-agent environments.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents, state_dim, action_dim, cooperative=True):\n",
    "        self.n_agents = n_agents\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.cooperative = cooperative\n",
    "        self.state = None\n",
    "        self.step_count = 0\n",
    "        self.max_steps = 200\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        self.state = np.random.randn(self.state_dim)\n",
    "        self.step_count = 0\n",
    "        return [self.state.copy() for _ in range(self.n_agents)]\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute joint action and return next states, rewards, dones.\"\"\"\n",
    "        self.step_count += 1\n",
    "        \n",
    "        # Simple dynamics: state evolves based on joint action\n",
    "        joint_action = np.mean(actions, axis=0)\n",
    "        noise = np.random.randn(self.state_dim) * 0.1\n",
    "        self.state = 0.9 * self.state + 0.1 * joint_action[:self.state_dim] + noise\n",
    "        \n",
    "        # Compute rewards\n",
    "        if self.cooperative:\n",
    "            # Cooperative: shared reward based on coordination\n",
    "            coordination_bonus = -np.mean([np.linalg.norm(actions[i] - joint_action) for i in range(self.n_agents)])\n",
    "            base_reward = -np.linalg.norm(self.state)  # Drive state to origin\n",
    "            rewards = [base_reward + coordination_bonus] * self.n_agents\n",
    "        else:\n",
    "            # Competitive: individual rewards with competition\n",
    "            rewards = []\n",
    "            for i in range(self.n_agents):\n",
    "                individual_reward = -np.linalg.norm(self.state - actions[i][:self.state_dim])\n",
    "                competition_penalty = sum([np.linalg.norm(actions[i] - actions[j]) \n",
    "                                         for j in range(self.n_agents) if j != i]) * 0.1\n",
    "                rewards.append(individual_reward - competition_penalty)\n",
    "        \n",
    "        done = self.step_count >= self.max_steps\n",
    "        next_states = [self.state.copy() for _ in range(self.n_agents)]\n",
    "        \n",
    "        return next_states, rewards, done\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Visualize current environment state.\"\"\"\n",
    "        pass\n",
    "\n",
    "# Demonstration of game theory concepts\n",
    "def demonstrate_game_theory():\n",
    "    \"\"\"Demonstrate basic game theory concepts.\"\"\"\n",
    "    print(\"ðŸŽ¯ Game Theory Analysis Demo\")\n",
    "    \n",
    "    # Prisoner's Dilemma\n",
    "    print(\"\\n1. Prisoner's Dilemma:\")\n",
    "    # Player 1's payoff matrix (rows: cooperate, defect)\n",
    "    prisoner_a = np.array([[-1, -3], [0, -2]])  # (cooperate, defect) vs (cooperate, defect)\n",
    "    # Player 2's payoff matrix \n",
    "    prisoner_b = np.array([[-1, 0], [-3, -2]])\n",
    "    \n",
    "    print(\"Player 1 payoff matrix:\")\n",
    "    print(prisoner_a)\n",
    "    print(\"Player 2 payoff matrix:\")\n",
    "    print(prisoner_b)\n",
    "    \n",
    "    nash_eq = GameTheoryUtils.find_nash_equilibria([prisoner_a, prisoner_b])\n",
    "    print(f\"Nash equilibria: {nash_eq}\")\n",
    "    \n",
    "    for eq in nash_eq:\n",
    "        is_pareto = GameTheoryUtils.is_pareto_optimal([prisoner_a, prisoner_b], eq)\n",
    "        print(f\"Strategy {eq}: Pareto optimal = {is_pareto}\")\n",
    "    \n",
    "    # Coordination Game\n",
    "    print(\"\\n2. Coordination Game:\")\n",
    "    coord_a = np.array([[2, 0], [0, 1]])\n",
    "    coord_b = np.array([[2, 0], [0, 1]])\n",
    "    \n",
    "    print(\"Coordination game (both players have same payoffs):\")\n",
    "    print(coord_a)\n",
    "    \n",
    "    nash_eq = GameTheoryUtils.find_nash_equilibria([coord_a, coord_b])\n",
    "    print(f\"Nash equilibria: {nash_eq}\")\n",
    "    \n",
    "    return prisoner_a, prisoner_b, coord_a, coord_b\n",
    "\n",
    "# Test multi-agent environment\n",
    "def test_multi_agent_env():\n",
    "    \"\"\"Test the basic multi-agent environment.\"\"\"\n",
    "    print(\"\\nðŸ¤– Multi-Agent Environment Test\")\n",
    "    \n",
    "    # Cooperative environment\n",
    "    print(\"Testing cooperative environment:\")\n",
    "    coop_env = MultiAgentEnvironment(n_agents=3, state_dim=4, action_dim=4, cooperative=True)\n",
    "    states = coop_env.reset()\n",
    "    print(f\"Initial states shape: {[s.shape for s in states]}\")\n",
    "    \n",
    "    # Random actions\n",
    "    actions = [np.random.randn(coop_env.action_dim) for _ in range(coop_env.n_agents)]\n",
    "    next_states, rewards, done = coop_env.step(actions)\n",
    "    \n",
    "    print(f\"Rewards (cooperative): {rewards}\")\n",
    "    print(f\"All agents get same reward: {len(set(rewards)) == 1}\")\n",
    "    \n",
    "    # Competitive environment  \n",
    "    print(\"\\nTesting competitive environment:\")\n",
    "    comp_env = MultiAgentEnvironment(n_agents=3, state_dim=4, action_dim=4, cooperative=False)\n",
    "    states = comp_env.reset()\n",
    "    next_states, rewards, done = comp_env.step(actions)\n",
    "    \n",
    "    print(f\"Rewards (competitive): {rewards}\")\n",
    "    print(f\"Agents get different rewards: {len(set(rewards)) > 1}\")\n",
    "    \n",
    "    return coop_env, comp_env\n",
    "\n",
    "# Run demonstrations\n",
    "game_matrices = demonstrate_game_theory()\n",
    "environments = test_multi_agent_env()\n",
    "\n",
    "print(\"\\nâœ… Game theory and multi-agent foundations implemented successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9507ce",
   "metadata": {},
   "source": [
    "# Section 2: Cooperative Multi-Agent Learning\n",
    "\n",
    "## 2.1 Centralized Training, Decentralized Execution (CTDE)\n",
    "\n",
    "The CTDE paradigm is fundamental to modern cooperative MARL:\n",
    "\n",
    "**Training Phase**: \n",
    "- Central coordinator has access to global information\n",
    "- Can compute joint value functions and coordinate policy updates\n",
    "- Addresses non-stationarity through centralized critic\n",
    "\n",
    "**Execution Phase**:\n",
    "- Each agent acts based on local observations only\n",
    "- No communication required during deployment\n",
    "- Maintains scalability and robustness\n",
    "\n",
    "### Multi-Agent Actor-Critic (MAAC)\n",
    "\n",
    "**Centralized Critic**: Estimates joint action-value function $Q(s, a_1, ..., a_n)$\n",
    "\n",
    "**Actor Update**: Each agent $i$ updates policy using centralized critic:\n",
    "$$\\nabla_{\\theta_i} J_i = \\mathbb{E}[\\nabla_{\\theta_i} \\log \\pi_{\\theta_i}(a_i|o_i) \\cdot Q^{\\pi}(s, a_1, ..., a_n)]$$\n",
    "\n",
    "**Critic Update**: Minimize joint TD error:\n",
    "$$L(\\phi) = \\mathbb{E}[(Q_{\\phi}(s, a_1, ..., a_n) - y)^2]$$\n",
    "$$y = r + \\gamma Q_{\\phi'}(s', \\pi_{\\theta_1'}(o_1'), ..., \\pi_{\\theta_n'}(o_n'))$$\n",
    "\n",
    "### Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "\n",
    "Extension of DDPG to multi-agent settings:\n",
    "\n",
    "1. **Centralized Critics**: Each agent maintains its own critic that uses global information\n",
    "2. **Experience Replay**: Shared replay buffer with transitions $(s, a_1, ..., a_n, r_1, ..., r_n, s')$\n",
    "3. **Target Networks**: Slow-updating target networks for stability\n",
    "\n",
    "**Critic Loss for Agent $i$**:\n",
    "$$L_i(\\phi_i) = \\mathbb{E}[(Q_{\\phi_i}(s, a_1, ..., a_n) - y_i)^2]$$\n",
    "$$y_i = r_i + \\gamma Q_{\\phi_i'}(s', \\mu_{\\theta_1'}(o_1'), ..., \\mu_{\\theta_n'}(o_n'))$$\n",
    "\n",
    "**Actor Loss for Agent $i$**:\n",
    "$$L_i(\\theta_i) = -\\mathbb{E}[Q_{\\phi_i}(s, a_1|_{a_i=\\mu_{\\theta_i}(o_i)}, ..., a_n)]$$\n",
    "\n",
    "### Counterfactual Multi-Agent Policy Gradients (COMA)\n",
    "\n",
    "Uses counterfactual reasoning for credit assignment:\n",
    "\n",
    "**Counterfactual Baseline**:\n",
    "$$A_i(s, a) = Q(s, a) - \\sum_{a_i'} \\pi_i(a_i'|o_i) Q(s, a_{-i}, a_i')$$\n",
    "\n",
    "This baseline removes the effect of agent $i$'s action, isolating its contribution to the team reward.\n",
    "\n",
    "### Value Decomposition Networks (VDN)\n",
    "\n",
    "Decomposes team value function into individual components:\n",
    "$$Q_{tot}(s, a) = \\sum_{i=1}^n Q_i(o_i, a_i)$$\n",
    "\n",
    "**Advantages**:\n",
    "- Individual value functions can be learned independently\n",
    "- Naturally handles partial observability\n",
    "- Maintains convergence guarantees under certain conditions\n",
    "\n",
    "**Limitations**:\n",
    "- Additivity assumption may be too restrictive\n",
    "- Cannot represent complex coordination patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa73cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Agent Deep Deterministic Policy Gradient (MADDPG) Implementation\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    \"\"\"Actor network for MADDPG.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=128, max_action=1.0):\n",
    "        super(Actor, self).__init__()\n",
    "        self.max_action = max_action\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.max_action * self.net(obs)\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"Centralized critic for MADDPG.\"\"\"\n",
    "    \n",
    "    def __init__(self, total_obs_dim, total_action_dim, hidden_dim=128):\n",
    "        super(Critic, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(total_obs_dim + total_action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs, actions):\n",
    "        return self.net(torch.cat([obs, actions], dim=-1))\n",
    "\n",
    "class MADDPGAgent:\n",
    "    \"\"\"Single agent in MADDPG framework.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_id, obs_dim, action_dim, total_obs_dim, total_action_dim,\n",
    "                 lr_actor=1e-4, lr_critic=1e-3, gamma=0.99, tau=0.005):\n",
    "        self.agent_id = agent_id\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        \n",
    "        # Networks\n",
    "        self.actor = Actor(obs_dim, action_dim).to(device)\n",
    "        self.critic = Critic(total_obs_dim, total_action_dim).to(device)\n",
    "        self.target_actor = Actor(obs_dim, action_dim).to(device)\n",
    "        self.target_critic = Critic(total_obs_dim, total_action_dim).to(device)\n",
    "        \n",
    "        # Copy parameters to target networks\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr_actor)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr_critic)\n",
    "        \n",
    "        # Noise for exploration\n",
    "        self.noise_scale = 0.1\n",
    "        self.noise_decay = 0.9999\n",
    "    \n",
    "    def act(self, obs, add_noise=True):\n",
    "        \"\"\"Select action given observation.\"\"\"\n",
    "        obs = torch.FloatTensor(obs).to(device)\n",
    "        action = self.actor(obs).cpu().data.numpy()\n",
    "        \n",
    "        if add_noise:\n",
    "            noise = np.random.normal(0, self.noise_scale, size=action.shape)\n",
    "            action += noise\n",
    "            self.noise_scale *= self.noise_decay\n",
    "        \n",
    "        return np.clip(action, -1, 1)\n",
    "    \n",
    "    def update_critic(self, obs, actions, rewards, next_obs, next_actions, dones):\n",
    "        \"\"\"Update critic network.\"\"\"\n",
    "        obs = torch.FloatTensor(obs).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_obs = torch.FloatTensor(next_obs).to(device)\n",
    "        next_actions = torch.FloatTensor(next_actions).to(device)\n",
    "        dones = torch.BoolTensor(dones).to(device)\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q = self.critic(obs, actions).squeeze()\n",
    "        \n",
    "        # Target Q-values\n",
    "        with torch.no_grad():\n",
    "            target_q = self.target_critic(next_obs, next_actions).squeeze()\n",
    "            target_q = rewards + self.gamma * target_q * ~dones\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(current_q, target_q)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        return critic_loss.item()\n",
    "    \n",
    "    def update_actor(self, obs, actions):\n",
    "        \"\"\"Update actor network.\"\"\"\n",
    "        obs = torch.FloatTensor(obs).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        \n",
    "        # Replace this agent's action with current policy\n",
    "        actions_pred = actions.clone()\n",
    "        agent_obs = obs[:, self.agent_id]  # This agent's observations\n",
    "        actions_pred[:, self.agent_id] = self.actor(agent_obs)\n",
    "        \n",
    "        # Actor loss: maximize Q-value\n",
    "        actor_loss = -self.critic(obs.view(obs.size(0), -1), \n",
    "                                 actions_pred.view(actions_pred.size(0), -1)).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        return actor_loss.item()\n",
    "    \n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update of target networks.\"\"\"\n",
    "        for target, source in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target.data.copy_(self.tau * source.data + (1.0 - self.tau) * target.data)\n",
    "        \n",
    "        for target, source in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target.data.copy_(self.tau * source.data + (1.0 - self.tau) * target.data)\n",
    "\n",
    "class MADDPG:\n",
    "    \"\"\"Multi-Agent Deep Deterministic Policy Gradient.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents, obs_dim, action_dim, buffer_size=100000):\n",
    "        self.n_agents = n_agents\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        total_obs_dim = n_agents * obs_dim\n",
    "        total_action_dim = n_agents * action_dim\n",
    "        \n",
    "        # Create agents\n",
    "        self.agents = [\n",
    "            MADDPGAgent(i, obs_dim, action_dim, total_obs_dim, total_action_dim)\n",
    "            for i in range(n_agents)\n",
    "        ]\n",
    "        \n",
    "        # Replay buffer\n",
    "        self.replay_buffer = ReplayBuffer(buffer_size)\n",
    "        \n",
    "    def act(self, observations, add_noise=True):\n",
    "        \"\"\"Get actions from all agents.\"\"\"\n",
    "        actions = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            action = agent.act(observations[i], add_noise)\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Store experience and update agents.\"\"\"\n",
    "        # Store experience\n",
    "        self.replay_buffer.push(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        # Update agents if enough samples\n",
    "        if len(self.replay_buffer) > ma_config.batch_size:\n",
    "            self.update()\n",
    "    \n",
    "    def update(self):\n",
    "        \"\"\"Update all agents.\"\"\"\n",
    "        batch = self.replay_buffer.sample(ma_config.batch_size)\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        # Prepare data for centralized training\n",
    "        states_flat = np.array(states).reshape(len(states), -1)\n",
    "        actions_flat = np.array(actions).reshape(len(actions), -1)\n",
    "        next_states_flat = np.array(next_states).reshape(len(next_states), -1)\n",
    "        \n",
    "        # Get next actions from target actors\n",
    "        next_actions = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            next_obs = torch.FloatTensor(next_states).to(device)[:, i]\n",
    "            next_action = agent.target_actor(next_obs)\n",
    "            next_actions.append(next_action)\n",
    "        \n",
    "        next_actions_flat = torch.cat(next_actions, dim=-1).cpu().data.numpy()\n",
    "        \n",
    "        # Update each agent\n",
    "        losses = {'actor': [], 'critic': []}\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            agent_rewards = np.array(rewards)[:, i]\n",
    "            agent_dones = np.array(dones)\n",
    "            \n",
    "            # Update critic\n",
    "            critic_loss = agent.update_critic(\n",
    "                states_flat, actions_flat, agent_rewards,\n",
    "                next_states_flat, next_actions_flat, agent_dones\n",
    "            )\n",
    "            losses['critic'].append(critic_loss)\n",
    "            \n",
    "            # Update actor\n",
    "            actor_loss = agent.update_actor(states, actions)\n",
    "            losses['actor'].append(actor_loss)\n",
    "            \n",
    "            # Soft update target networks\n",
    "            agent.soft_update()\n",
    "        \n",
    "        return losses\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Replay buffer for multi-agent experiences.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Store a transition.\"\"\"\n",
    "        self.buffer.append((states, actions, rewards, next_states, dones))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of transitions.\"\"\"\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Value Decomposition Network (VDN) Implementation\n",
    "class VDNAgent(nn.Module):\n",
    "    \"\"\"Individual agent network for VDN.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=64):\n",
    "        super(VDNAgent, self).__init__()\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        return self.net(obs)\n",
    "\n",
    "class VDN:\n",
    "    \"\"\"Value Decomposition Network for cooperative MARL.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents, obs_dim, action_dim, lr=1e-3):\n",
    "        self.n_agents = n_agents\n",
    "        self.agents = [VDNAgent(obs_dim, action_dim).to(device) for _ in range(n_agents)]\n",
    "        self.target_agents = [VDNAgent(obs_dim, action_dim).to(device) for _ in range(n_agents)]\n",
    "        \n",
    "        # Copy parameters\n",
    "        for agent, target in zip(self.agents, self.target_agents):\n",
    "            target.load_state_dict(agent.state_dict())\n",
    "        \n",
    "        self.optimizers = [optim.Adam(agent.parameters(), lr=lr) for agent in self.agents]\n",
    "        self.replay_buffer = ReplayBuffer(10000)\n",
    "        \n",
    "    def act(self, observations, epsilon=0.1):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        actions = []\n",
    "        for i, agent in enumerate(self.agents):\n",
    "            if np.random.random() < epsilon:\n",
    "                action = np.random.randint(agent.net[-1].out_features)\n",
    "            else:\n",
    "                obs = torch.FloatTensor(observations[i]).to(device)\n",
    "                q_values = agent(obs)\n",
    "                action = q_values.argmax().item()\n",
    "            actions.append(action)\n",
    "        return actions\n",
    "    \n",
    "    def update(self, batch_size=32):\n",
    "        \"\"\"Update VDN agents.\"\"\"\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = self.replay_buffer.sample(batch_size)\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        total_loss = 0\n",
    "        \n",
    "        # Convert to tensors\n",
    "        team_rewards = torch.FloatTensor([sum(r) for r in rewards]).to(device)\n",
    "        team_dones = torch.BoolTensor([any(d) for d in dones]).to(device)\n",
    "        \n",
    "        for i, (agent, target_agent, optimizer) in enumerate(zip(self.agents, self.target_agents, self.optimizers)):\n",
    "            agent_states = torch.FloatTensor([s[i] for s in states]).to(device)\n",
    "            agent_actions = torch.LongTensor([a[i] for a in actions]).to(device)\n",
    "            agent_next_states = torch.FloatTensor([s[i] for s in next_states]).to(device)\n",
    "            \n",
    "            # Current Q-values\n",
    "            q_values = agent(agent_states)\n",
    "            q_values = q_values.gather(1, agent_actions.unsqueeze(1)).squeeze()\n",
    "            \n",
    "            # Target Q-values\n",
    "            with torch.no_grad():\n",
    "                next_q_values = target_agent(agent_next_states).max(1)[0]\n",
    "                target_q = team_rewards + 0.99 * next_q_values * ~team_dones\n",
    "            \n",
    "            loss = F.mse_loss(q_values, target_q)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        tau = 0.005\n",
    "        for agent, target_agent in zip(self.agents, self.target_agents):\n",
    "            for param, target_param in zip(agent.parameters(), target_agent.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "        \n",
    "        return total_loss / self.n_agents\n",
    "\n",
    "print(\"ðŸ¤– Cooperative multi-agent algorithms implemented successfully!\")\n",
    "print(\"âœ… MADDPG, VDN, and supporting utilities ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdf213f",
   "metadata": {},
   "source": [
    "# Section 3: Advanced Policy Gradient Methods\n",
    "\n",
    "## 3.1 Proximal Policy Optimization (PPO)\n",
    "\n",
    "PPO addresses the challenge of step size in policy gradient methods through clipped objective functions.\n",
    "\n",
    "### PPO-Clip Objective\n",
    "\n",
    "**Probability Ratio**:\n",
    "$$r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$$\n",
    "\n",
    "**Clipped Objective**:\n",
    "$$L^{CLIP}(\\theta) = \\hat{\\mathbb{E}}_t[\\min(r_t(\\theta)A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)A_t)]$$\n",
    "\n",
    "Where $\\epsilon$ is the clipping parameter (typically 0.1-0.3) and $A_t$ is the advantage estimate.\n",
    "\n",
    "### Trust Region Policy Optimization (TRPO)\n",
    "\n",
    "TRPO constrains policy updates to stay within a trust region:\n",
    "\n",
    "**Objective**:\n",
    "$$\\max_\\theta \\hat{\\mathbb{E}}_t[\\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}A_t]$$\n",
    "\n",
    "**Subject to**:\n",
    "$$\\hat{\\mathbb{E}}_t[KL[\\pi_{\\theta_{old}}(\\cdot|s_t), \\pi_\\theta(\\cdot|s_t)]] \\leq \\delta$$\n",
    "\n",
    "**Conjugate Gradient Solution**:\n",
    "TRPO uses conjugate gradient to solve the constrained optimization problem:\n",
    "$$g = \\nabla_\\theta L(\\theta_{old})$$\n",
    "$$H = \\nabla_\\theta^2 KL[\\pi_{\\theta_{old}}, \\pi_\\theta]$$\n",
    "$$\\theta_{new} = \\theta_{old} + \\sqrt{\\frac{2\\delta}{g^T H^{-1} g}} H^{-1} g$$\n",
    "\n",
    "### Soft Actor-Critic (SAC)\n",
    "\n",
    "SAC maximizes both expected return and entropy for better exploration:\n",
    "\n",
    "**Objective**:\n",
    "$$J(\\pi) = \\sum_{t=0}^T \\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi}[r(s_t, a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t))]$$\n",
    "\n",
    "Where $\\alpha$ is the temperature parameter controlling exploration-exploitation trade-off.\n",
    "\n",
    "**Soft Q-Function Updates**:\n",
    "$$J_Q(\\phi) = \\mathbb{E}_{(s_t, a_t, r_t, s_{t+1}) \\sim \\mathcal{D}}[\\frac{1}{2}(Q_\\phi(s_t, a_t) - y_t)^2]$$\n",
    "$$y_t = r_t + \\gamma \\mathbb{E}_{a_{t+1} \\sim \\pi}[Q_{\\phi'}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi(a_{t+1}|s_{t+1})]$$\n",
    "\n",
    "**Policy Updates**:\n",
    "$$J_\\pi(\\theta) = \\mathbb{E}_{s_t \\sim \\mathcal{D}, a_t \\sim \\pi_\\theta}[\\alpha \\log \\pi_\\theta(a_t|s_t) - Q_\\phi(s_t, a_t)]$$\n",
    "\n",
    "### Advanced Advantage Estimation\n",
    "\n",
    "**Generalized Advantage Estimation (GAE)**:\n",
    "$$A_t^{GAE(\\gamma, \\lambda)} = \\sum_{l=0}^\\infty (\\gamma\\lambda)^l \\delta_{t+l}^V$$\n",
    "\n",
    "Where $\\delta_t^V = r_t + \\gamma V(s_{t+1}) - V(s_t)$ is the TD error.\n",
    "\n",
    "GAE balances bias and variance:\n",
    "- $\\lambda = 0$: Low variance, high bias (TD error)\n",
    "- $\\lambda = 1$: High variance, low bias (Monte Carlo)\n",
    "\n",
    "### Multi-Agent Policy Gradient Extensions\n",
    "\n",
    "**Multi-Agent PPO (MAPPO)**:\n",
    "- Centralized value function: $V(s_1, ..., s_n)$\n",
    "- Individual actor updates with shared value baseline\n",
    "- Addresses non-stationarity through centralized training\n",
    "\n",
    "**Multi-Agent SAC (MASAC)**:\n",
    "- Individual entropy regularization per agent\n",
    "- Shared experience replay buffer\n",
    "- Independent policy and Q-function updates\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82aa7498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Policy Gradient Methods Implementation\n",
    "\n",
    "class PPONetwork(nn.Module):\n",
    "    \"\"\"Combined actor-critic network for PPO.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=64, discrete=True):\n",
    "        super(PPONetwork, self).__init__()\n",
    "        self.discrete = discrete\n",
    "        \n",
    "        # Shared layers\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Actor head\n",
    "        if discrete:\n",
    "            self.actor = nn.Linear(hidden_dim, action_dim)\n",
    "        else:\n",
    "            self.actor_mean = nn.Linear(hidden_dim, action_dim)\n",
    "            self.actor_logstd = nn.Parameter(torch.zeros(1, action_dim))\n",
    "        \n",
    "        # Critic head\n",
    "        self.critic = nn.Linear(hidden_dim, 1)\n",
    "    \n",
    "    def forward(self, obs):\n",
    "        shared_features = self.shared(obs)\n",
    "        value = self.critic(shared_features)\n",
    "        \n",
    "        if self.discrete:\n",
    "            action_logits = self.actor(shared_features)\n",
    "            return action_logits, value\n",
    "        else:\n",
    "            action_mean = self.actor_mean(shared_features)\n",
    "            action_std = torch.exp(self.actor_logstd.expand_as(action_mean))\n",
    "            return (action_mean, action_std), value\n",
    "    \n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        if self.discrete:\n",
    "            logits, value = self.forward(obs)\n",
    "            probs = Categorical(logits=logits)\n",
    "            if action is None:\n",
    "                action = probs.sample()\n",
    "            return action, probs.log_prob(action), probs.entropy(), value\n",
    "        else:\n",
    "            (mean, std), value = self.forward(obs)\n",
    "            probs = Normal(mean, std)\n",
    "            if action is None:\n",
    "                action = probs.sample()\n",
    "            return action, probs.log_prob(action).sum(-1), probs.entropy().sum(-1), value\n",
    "\n",
    "class PPOAgent:\n",
    "    \"\"\"Proximal Policy Optimization agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, lr=3e-4, discrete=True):\n",
    "        self.network = PPONetwork(obs_dim, action_dim, discrete=discrete).to(device)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr, eps=1e-5)\n",
    "        self.discrete = discrete\n",
    "        \n",
    "        # PPO hyperparameters\n",
    "        self.clip_coef = 0.2\n",
    "        self.ent_coef = 0.01\n",
    "        self.vf_coef = 0.5\n",
    "        self.max_grad_norm = 0.5\n",
    "        self.target_kl = 0.01\n",
    "        \n",
    "    def get_action_and_value(self, obs, action=None):\n",
    "        return self.network.get_action_and_value(obs, action)\n",
    "    \n",
    "    def update(self, rollouts, n_epochs=10, minibatch_size=64):\n",
    "        \"\"\"Update PPO using clipped objective.\"\"\"\n",
    "        obs, actions, logprobs, returns, values, advantages = rollouts\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        clipfracs = []\n",
    "        total_losses = []\n",
    "        \n",
    "        for epoch in range(n_epochs):\n",
    "            # Random minibatches\n",
    "            indices = torch.randperm(len(obs))\n",
    "            \n",
    "            for start in range(0, len(obs), minibatch_size):\n",
    "                end = start + minibatch_size\n",
    "                mb_indices = indices[start:end]\n",
    "                \n",
    "                mb_obs = obs[mb_indices]\n",
    "                mb_actions = actions[mb_indices]\n",
    "                mb_logprobs = logprobs[mb_indices]\n",
    "                mb_returns = returns[mb_indices]\n",
    "                mb_values = values[mb_indices]\n",
    "                mb_advantages = advantages[mb_indices]\n",
    "                \n",
    "                # Forward pass\n",
    "                _, newlogprob, entropy, newvalue = self.get_action_and_value(mb_obs, mb_actions)\n",
    "                \n",
    "                # Policy loss\n",
    "                logratio = newlogprob - mb_logprobs\n",
    "                ratio = logratio.exp()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    # Calculate approximate KL divergence\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs.append(((ratio - 1.0).abs() > self.clip_coef).float().mean().item())\n",
    "                \n",
    "                # Clipped surrogate objective\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                v_loss = F.mse_loss(newvalue.squeeze(), mb_returns)\n",
    "                \n",
    "                # Entropy loss\n",
    "                entropy_loss = entropy.mean()\n",
    "                \n",
    "                # Total loss\n",
    "                loss = pg_loss - self.ent_coef * entropy_loss + v_loss * self.vf_coef\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                total_losses.append(loss.item())\n",
    "            \n",
    "            # Early stopping based on KL divergence\n",
    "            if approx_kl > self.target_kl:\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'total_loss': np.mean(total_losses),\n",
    "            'policy_loss': pg_loss.item(),\n",
    "            'value_loss': v_loss.item(),\n",
    "            'entropy_loss': entropy_loss.item(),\n",
    "            'approx_kl': approx_kl.item(),\n",
    "            'clipfrac': np.mean(clipfracs)\n",
    "        }\n",
    "\n",
    "class SACAgent:\n",
    "    \"\"\"Soft Actor-Critic agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, lr=3e-4, alpha=0.2, tau=0.005):\n",
    "        # Actor network\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.actor_mean = nn.Linear(256, action_dim).to(device)\n",
    "        self.actor_logstd = nn.Linear(256, action_dim).to(device)\n",
    "        \n",
    "        # Q networks\n",
    "        self.q1 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.q2 = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        # Target Q networks\n",
    "        self.target_q1 = copy.deepcopy(self.q1)\n",
    "        self.target_q2 = copy.deepcopy(self.q2)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.actor_optimizer = optim.Adam(list(self.actor.parameters()) + \n",
    "                                        list(self.actor_mean.parameters()) + \n",
    "                                        list(self.actor_logstd.parameters()), lr=lr)\n",
    "        self.q1_optimizer = optim.Adam(self.q1.parameters(), lr=lr)\n",
    "        self.q2_optimizer = optim.Adam(self.q2.parameters(), lr=lr)\n",
    "        \n",
    "        # Hyperparameters\n",
    "        self.alpha = alpha\n",
    "        self.tau = tau\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        # Automatic entropy tuning\n",
    "        self.target_entropy = -action_dim\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True, device=device)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=lr)\n",
    "    \n",
    "    def get_action(self, obs, deterministic=False):\n",
    "        \"\"\"Sample action from policy.\"\"\"\n",
    "        obs = torch.FloatTensor(obs).to(device)\n",
    "        \n",
    "        # Forward pass through actor\n",
    "        features = self.actor(obs)\n",
    "        mean = self.actor_mean(features)\n",
    "        log_std = self.actor_logstd(features)\n",
    "        log_std = torch.clamp(log_std, -20, 2)\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        if deterministic:\n",
    "            action = torch.tanh(mean)\n",
    "        else:\n",
    "            # Sample from Normal distribution\n",
    "            normal = Normal(mean, std)\n",
    "            x = normal.rsample()  # Reparameterization trick\n",
    "            action = torch.tanh(x)\n",
    "            \n",
    "            # Compute log probability\n",
    "            log_prob = normal.log_prob(x)\n",
    "            # Enforcing action bounds\n",
    "            log_prob -= torch.log(1 - action.pow(2) + 1e-6)\n",
    "            log_prob = log_prob.sum(1, keepdim=True)\n",
    "        \n",
    "        return action.cpu().data.numpy(), log_prob if not deterministic else None\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update SAC networks.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.FloatTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.BoolTensor(dones).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get next actions and log probabilities\n",
    "            next_actions, next_log_probs = self.get_action(next_states)\n",
    "            next_actions = torch.FloatTensor(next_actions).to(device)\n",
    "            \n",
    "            # Target Q-values\n",
    "            target_q1 = self.target_q1(torch.cat([next_states, next_actions], dim=1))\n",
    "            target_q2 = self.target_q2(torch.cat([next_states, next_actions], dim=1))\n",
    "            target_q = torch.min(target_q1, target_q2) - self.alpha * next_log_probs\n",
    "            target_q = rewards + self.gamma * (1 - dones.float()) * target_q\n",
    "        \n",
    "        # Current Q-values\n",
    "        current_q1 = self.q1(torch.cat([states, actions], dim=1))\n",
    "        current_q2 = self.q2(torch.cat([states, actions], dim=1))\n",
    "        \n",
    "        # Q-function losses\n",
    "        q1_loss = F.mse_loss(current_q1, target_q)\n",
    "        q2_loss = F.mse_loss(current_q2, target_q)\n",
    "        \n",
    "        # Update Q-functions\n",
    "        self.q1_optimizer.zero_grad()\n",
    "        q1_loss.backward()\n",
    "        self.q1_optimizer.step()\n",
    "        \n",
    "        self.q2_optimizer.zero_grad()\n",
    "        q2_loss.backward()\n",
    "        self.q2_optimizer.step()\n",
    "        \n",
    "        # Update policy\n",
    "        new_actions, log_probs = self.get_action(states)\n",
    "        new_actions = torch.FloatTensor(new_actions).to(device)\n",
    "        \n",
    "        q1_new = self.q1(torch.cat([states, new_actions], dim=1))\n",
    "        q2_new = self.q2(torch.cat([states, new_actions], dim=1))\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "        \n",
    "        actor_loss = (self.alpha * log_probs - q_new).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        # Update alpha (temperature parameter)\n",
    "        alpha_loss = (-self.log_alpha * (log_probs + self.target_entropy).detach()).mean()\n",
    "        \n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "        \n",
    "        self.alpha = self.log_alpha.exp().item()\n",
    "        \n",
    "        # Soft update target networks\n",
    "        self.soft_update()\n",
    "        \n",
    "        return {\n",
    "            'q1_loss': q1_loss.item(),\n",
    "            'q2_loss': q2_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'alpha_loss': alpha_loss.item(),\n",
    "            'alpha': self.alpha\n",
    "        }\n",
    "    \n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update target networks.\"\"\"\n",
    "        for target_param, param in zip(self.target_q1.parameters(), self.q1.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_q2.parameters(), self.q2.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "class GAEBuffer:\n",
    "    \"\"\"Buffer for collecting trajectories and computing GAE.\"\"\"\n",
    "    \n",
    "    def __init__(self, size, obs_dim, action_dim, gamma=0.99, gae_lambda=0.95):\n",
    "        self.size = size\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        \n",
    "        self.obs = np.zeros((size, obs_dim), dtype=np.float32)\n",
    "        self.actions = np.zeros((size, action_dim), dtype=np.float32)\n",
    "        self.rewards = np.zeros(size, dtype=np.float32)\n",
    "        self.values = np.zeros(size, dtype=np.float32)\n",
    "        self.logprobs = np.zeros(size, dtype=np.float32)\n",
    "        self.dones = np.zeros(size, dtype=np.float32)\n",
    "        \n",
    "        self.ptr = 0\n",
    "        self.max_size = size\n",
    "    \n",
    "    def store(self, obs, action, reward, value, logprob, done):\n",
    "        \"\"\"Store a single transition.\"\"\"\n",
    "        self.obs[self.ptr] = obs\n",
    "        self.actions[self.ptr] = action\n",
    "        self.rewards[self.ptr] = reward\n",
    "        self.values[self.ptr] = value\n",
    "        self.logprobs[self.ptr] = logprob\n",
    "        self.dones[self.ptr] = done\n",
    "        \n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "    \n",
    "    def compute_gae(self, last_value=0):\n",
    "        \"\"\"Compute GAE advantages and returns.\"\"\"\n",
    "        advantages = np.zeros_like(self.rewards)\n",
    "        returns = np.zeros_like(self.rewards)\n",
    "        \n",
    "        last_gae = 0\n",
    "        for t in reversed(range(self.size)):\n",
    "            if t == self.size - 1:\n",
    "                next_nonterminal = 1.0 - self.dones[t]\n",
    "                next_value = last_value\n",
    "            else:\n",
    "                next_nonterminal = 1.0 - self.dones[t+1]\n",
    "                next_value = self.values[t+1]\n",
    "            \n",
    "            delta = self.rewards[t] + self.gamma * next_value * next_nonterminal - self.values[t]\n",
    "            advantages[t] = last_gae = delta + self.gamma * self.gae_lambda * next_nonterminal * last_gae\n",
    "        \n",
    "        returns = advantages + self.values\n",
    "        return advantages, returns\n",
    "    \n",
    "    def get_batch(self):\n",
    "        \"\"\"Get all stored data as tensors.\"\"\"\n",
    "        return {\n",
    "            'obs': torch.FloatTensor(self.obs).to(device),\n",
    "            'actions': torch.FloatTensor(self.actions).to(device),\n",
    "            'rewards': torch.FloatTensor(self.rewards).to(device),\n",
    "            'values': torch.FloatTensor(self.values).to(device),\n",
    "            'logprobs': torch.FloatTensor(self.logprobs).to(device),\n",
    "            'dones': torch.FloatTensor(self.dones).to(device)\n",
    "        }\n",
    "\n",
    "# Demonstration function\n",
    "def demonstrate_advanced_policies():\n",
    "    \"\"\"Demonstrate advanced policy methods on a simple environment.\"\"\"\n",
    "    print(\"ðŸŽ¯ Advanced Policy Methods Demo\")\n",
    "    \n",
    "    # Create simple continuous control environment\n",
    "    obs_dim, action_dim = 4, 2\n",
    "    \n",
    "    # PPO demonstration\n",
    "    print(\"\\n1. PPO Agent:\")\n",
    "    ppo_agent = PPOAgent(obs_dim, action_dim, discrete=False)\n",
    "    obs = torch.randn(1, obs_dim)\n",
    "    action, logprob, entropy, value = ppo_agent.get_action_and_value(obs)\n",
    "    print(f\"PPO Action shape: {action.shape}, Value: {value.item():.3f}\")\n",
    "    \n",
    "    # SAC demonstration\n",
    "    print(\"\\n2. SAC Agent:\")\n",
    "    sac_agent = SACAgent(obs_dim, action_dim)\n",
    "    action, log_prob = sac_agent.get_action(obs.numpy()[0])\n",
    "    print(f\"SAC Action: {action}, Log Prob: {log_prob.item():.3f}\")\n",
    "    \n",
    "    print(\"\\nâœ… Advanced policy methods demonstrated successfully!\")\n",
    "\n",
    "# Run demonstration\n",
    "demonstrate_advanced_policies()\n",
    "\n",
    "print(\"ðŸš€ Advanced policy gradient methods implemented successfully!\")\n",
    "print(\"âœ… PPO, SAC, and GAE utilities ready for multi-agent training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2c2345",
   "metadata": {},
   "source": [
    "# Section 4: Distributed Reinforcement Learning\n",
    "\n",
    "## 4.1 Asynchronous Methods\n",
    "\n",
    "Distributed RL enables parallel learning across multiple environments and workers, significantly improving sample efficiency and wall-clock training time.\n",
    "\n",
    "### Asynchronous Advantage Actor-Critic (A3C)\n",
    "\n",
    "A3C runs multiple actor-learners in parallel, each interacting with a separate environment instance:\n",
    "\n",
    "**Global Network Update**:\n",
    "$$\\theta_{global} \\leftarrow \\theta_{global} + \\alpha \\sum_{i=1}^{n_{workers}} \\nabla \\theta_i$$\n",
    "\n",
    "**Local Gradient Accumulation**:\n",
    "Each worker $i$ accumulates gradients over $t_{max}$ steps:\n",
    "$$\\nabla \\theta_i = \\sum_{t=1}^{t_{max}} \\nabla \\log \\pi_{\\theta_i}(a_t|s_t) A_t + \\beta \\nabla H(\\pi_{\\theta_i}(s_t))$$\n",
    "\n",
    "Where $A_t$ is computed using n-step returns or GAE.\n",
    "\n",
    "### IMPALA (Importance Weighted Actor-Learner Architecture)\n",
    "\n",
    "IMPALA addresses the off-policy nature of distributed learning through importance sampling:\n",
    "\n",
    "**V-trace Target**:\n",
    "$$v_s = V(s_t) + \\sum_{i=0}^{n-1} \\gamma^i \\prod_{j=0}^{i} c_{t+j} [r_{t+i} + \\gamma V(s_{t+i+1}) - V(s_{t+i})]$$\n",
    "\n",
    "**Importance Weights**:\n",
    "$$\\rho_t = \\min(\\bar{\\rho}, \\frac{\\pi(a_t|s_t)}{\\mu(a_t|s_t)})$$\n",
    "$$c_t = \\min(\\bar{c}, \\frac{\\pi(a_t|s_t)}{\\mu(a_t|s_t)})$$\n",
    "\n",
    "Where $\\mu$ is the behavior policy and $\\pi$ is the target policy.\n",
    "\n",
    "### Distributed PPO (D-PPO)\n",
    "\n",
    "Scales PPO to distributed settings while maintaining policy gradient guarantees:\n",
    "\n",
    "1. **Rollout Collection**: Workers collect experience in parallel\n",
    "2. **Gradient Aggregation**: Central server aggregates gradients\n",
    "3. **Synchronized Updates**: Global policy update after each epoch\n",
    "\n",
    "**Gradient Synchronization**:\n",
    "$$g_{global} = \\frac{1}{N} \\sum_{i=1}^{N} g_i$$\n",
    "\n",
    "Where $g_i$ is the gradient from worker $i$.\n",
    "\n",
    "## 4.2 Evolutionary Strategies (ES) in RL\n",
    "\n",
    "ES provides gradient-free optimization for RL policies:\n",
    "\n",
    "**Population-Based Update**:\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\frac{1}{\\sigma \\lambda} \\sum_{i=1}^{\\lambda} R_i \\epsilon_i$$\n",
    "\n",
    "Where:\n",
    "- $\\epsilon_i \\sim \\mathcal{N}(0, I)$ are random perturbations\n",
    "- $R_i$ is the return achieved by perturbed policy $\\theta_t + \\sigma \\epsilon_i$\n",
    "- $\\lambda$ is the population size\n",
    "\n",
    "### Advantages of ES:\n",
    "1. **Parallelizable**: Each worker evaluates different policy perturbation\n",
    "2. **Gradient-free**: Works with non-differentiable rewards\n",
    "3. **Robust**: Less sensitive to hyperparameters\n",
    "4. **Communication efficient**: Only needs to share scalars (returns)\n",
    "\n",
    "## 4.3 Multi-Agent Distributed Learning\n",
    "\n",
    "### Centralized Training Distributed Execution (CTDE) at Scale\n",
    "\n",
    "**Hierarchical Coordination**:\n",
    "- **Global Coordinator**: Manages high-level strategy\n",
    "- **Local Coordinators**: Handle subgroup coordination\n",
    "- **Individual Agents**: Execute local policies\n",
    "\n",
    "**Communication Patterns**:\n",
    "1. **Broadcast**: Central coordinator broadcasts information to all agents\n",
    "2. **Reduce**: Agents send information to central coordinator\n",
    "3. **All-reduce**: All agents receive aggregated information from all others\n",
    "4. **Ring**: Information flows in a circular pattern\n",
    "\n",
    "### Parameter Server Architecture\n",
    "\n",
    "**Parameter Server**: Maintains global model parameters\n",
    "**Workers**: Pull parameters, compute gradients, push updates\n",
    "\n",
    "**Asynchronous Updates**:\n",
    "$$\\theta_{t+1} = \\theta_t - \\alpha \\sum_{i \\in \\text{available}} \\nabla_i$$\n",
    "\n",
    "**Advantages**:\n",
    "- Fault tolerance through redundancy\n",
    "- Scalable to thousands of workers\n",
    "- Flexible resource allocation\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d45dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Reinforcement Learning Implementation\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Process, Queue, Value, Array\n",
    "import queue\n",
    "import threading\n",
    "from threading import Lock\n",
    "import time\n",
    "\n",
    "class ParameterServer:\n",
    "    \"\"\"Parameter server for distributed RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_state_dict):\n",
    "        self.params = {k: v.clone().share_memory_() for k, v in model_state_dict.items()}\n",
    "        self.lock = Lock()\n",
    "        self.version = Value('i', 0)\n",
    "        self.update_count = Value('i', 0)\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"Get current parameters.\"\"\"\n",
    "        with self.lock:\n",
    "            return {k: v.clone() for k, v in self.params.items()}, self.version.value\n",
    "    \n",
    "    def update_parameters(self, gradients, lr=1e-4):\n",
    "        \"\"\"Update parameters with gradients.\"\"\"\n",
    "        with self.lock:\n",
    "            for key, grad in gradients.items():\n",
    "                if key in self.params:\n",
    "                    self.params[key] -= lr * grad\n",
    "            \n",
    "            self.version.value += 1\n",
    "            self.update_count.value += 1\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Get server statistics.\"\"\"\n",
    "        return {\n",
    "            'version': self.version.value,\n",
    "            'updates': self.update_count.value\n",
    "        }\n",
    "\n",
    "class A3CWorker:\n",
    "    \"\"\"A3C worker for distributed training.\"\"\"\n",
    "    \n",
    "    def __init__(self, worker_id, global_model, local_model, env_fn, gamma=0.99, n_steps=5):\n",
    "        self.worker_id = worker_id\n",
    "        self.global_model = global_model\n",
    "        self.local_model = local_model\n",
    "        self.env = env_fn()\n",
    "        self.gamma = gamma\n",
    "        self.n_steps = n_steps\n",
    "        self.optimizer = optim.Adam(global_model.parameters(), lr=1e-4)\n",
    "        \n",
    "    def compute_n_step_returns(self, rewards, values, next_value, dones):\n",
    "        \"\"\"Compute n-step returns.\"\"\"\n",
    "        returns = []\n",
    "        R = next_value\n",
    "        \n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + self.gamma * R * (1 - dones[i])\n",
    "            returns.insert(0, R)\n",
    "        \n",
    "        return returns\n",
    "    \n",
    "    def train_step(self):\n",
    "        \"\"\"Single training step for A3C worker.\"\"\"\n",
    "        # Sync local model with global model\n",
    "        self.local_model.load_state_dict(self.global_model.state_dict())\n",
    "        \n",
    "        states, actions, rewards, values, log_probs, dones = [], [], [], [], [], []\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        for _ in range(self.n_steps):\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, value = self.local_model(state_tensor)\n",
    "                probs = F.softmax(logits, dim=-1)\n",
    "                dist = Categorical(probs)\n",
    "                action = dist.sample()\n",
    "                log_prob = dist.log_prob(action)\n",
    "            \n",
    "            next_state, reward, done, _ = self.env.step(action.item())\n",
    "            \n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            values.append(value.item())\n",
    "            log_probs.append(log_prob)\n",
    "            dones.append(done)\n",
    "            \n",
    "            state = next_state if not done else self.env.reset()\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Compute returns\n",
    "        with torch.no_grad():\n",
    "            if done:\n",
    "                next_value = 0\n",
    "            else:\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                _, next_value = self.local_model(state_tensor)\n",
    "                next_value = next_value.item()\n",
    "        \n",
    "        returns = self.compute_n_step_returns(rewards, values, next_value, dones)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        returns = torch.FloatTensor(returns)\n",
    "        values = torch.FloatTensor(values)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        \n",
    "        # Compute losses\n",
    "        advantages = returns - values\n",
    "        \n",
    "        # Actor loss\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        \n",
    "        # Critic loss\n",
    "        critic_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        # Entropy bonus\n",
    "        logits, _ = self.local_model(states)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        entropy = -(probs * torch.log(probs + 1e-8)).sum(-1).mean()\n",
    "        \n",
    "        total_loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "        \n",
    "        # Compute gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(self.local_model.parameters(), 40)\n",
    "        \n",
    "        # Update global model\n",
    "        for global_param, local_param in zip(self.global_model.parameters(), \n",
    "                                           self.local_model.parameters()):\n",
    "            if global_param.grad is not None:\n",
    "                global_param.grad = local_param.grad\n",
    "            else:\n",
    "                global_param.grad = local_param.grad.clone()\n",
    "        \n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': critic_loss.item(),\n",
    "            'entropy': entropy.item()\n",
    "        }\n",
    "\n",
    "class IMPALALearner:\n",
    "    \"\"\"IMPALA learner with V-trace correction.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, lr=1e-4, rho_bar=1.0, c_bar=1.0):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.rho_bar = rho_bar  # Importance sampling clipping for policy gradient\n",
    "        self.c_bar = c_bar      # Importance sampling clipping for value function\n",
    "        \n",
    "    def vtrace(self, rewards, values, behavior_log_probs, target_log_probs, bootstrap_value, gamma=0.99):\n",
    "        \"\"\"Compute V-trace targets.\"\"\"\n",
    "        # Importance sampling ratios\n",
    "        rhos = torch.exp(target_log_probs - behavior_log_probs)\n",
    "        clipped_rhos = torch.clamp(rhos, max=self.rho_bar)\n",
    "        clipped_cs = torch.clamp(rhos, max=self.c_bar)\n",
    "        \n",
    "        # V-trace computation\n",
    "        values_t_plus_1 = torch.cat([values[1:], bootstrap_value.unsqueeze(0)])\n",
    "        deltas = clipped_rhos * (rewards + gamma * values_t_plus_1 - values)\n",
    "        \n",
    "        # Compute V-trace targets\n",
    "        vs = []\n",
    "        v_s = values[-1] + deltas[-1]\n",
    "        vs.append(v_s)\n",
    "        \n",
    "        for i in reversed(range(len(deltas) - 1)):\n",
    "            v_s = values[i] + deltas[i] + gamma * clipped_cs[i] * (v_s - values_t_plus_1[i])\n",
    "            vs.append(v_s)\n",
    "        \n",
    "        vs.reverse()\n",
    "        return torch.stack(vs)\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update IMPALA learner.\"\"\"\n",
    "        states, actions, rewards, behavior_log_probs, bootstrap_value = batch\n",
    "        \n",
    "        # Forward pass\n",
    "        logits, values = self.model(states)\n",
    "        \n",
    "        # Current policy log probabilities\n",
    "        target_log_probs = F.log_softmax(logits, dim=-1).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        \n",
    "        # V-trace targets\n",
    "        vtrace_targets = self.vtrace(rewards, values.squeeze(), behavior_log_probs, \n",
    "                                   target_log_probs, bootstrap_value)\n",
    "        \n",
    "        # Advantages for policy gradient\n",
    "        advantages = vtrace_targets - values.squeeze()\n",
    "        \n",
    "        # Losses\n",
    "        policy_loss = -(target_log_probs * advantages.detach()).mean()\n",
    "        value_loss = F.mse_loss(values.squeeze(), vtrace_targets.detach())\n",
    "        \n",
    "        # Entropy regularization\n",
    "        entropy = -(F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1)).sum(-1).mean()\n",
    "        \n",
    "        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy\n",
    "        \n",
    "        # Update\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 40)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': entropy.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "\n",
    "class DistributedPPOCoordinator:\n",
    "    \"\"\"Coordinator for distributed PPO training.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_workers, obs_dim, action_dim, lr=3e-4):\n",
    "        self.n_workers = n_workers\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        # Global model\n",
    "        self.global_model = PPONetwork(obs_dim, action_dim, discrete=True)\n",
    "        self.optimizer = optim.Adam(self.global_model.parameters(), lr=lr)\n",
    "        \n",
    "        # Communication queues\n",
    "        self.task_queues = [Queue() for _ in range(n_workers)]\n",
    "        self.result_queue = Queue()\n",
    "        \n",
    "        # Training statistics\n",
    "        self.episode_rewards = []\n",
    "        self.losses = []\n",
    "    \n",
    "    def collect_rollouts(self, n_steps=128):\n",
    "        \"\"\"Coordinate rollout collection across workers.\"\"\"\n",
    "        # Send collection tasks to workers\n",
    "        for i in range(self.n_workers):\n",
    "            self.task_queues[i].put(('collect', n_steps))\n",
    "        \n",
    "        # Collect results\n",
    "        all_rollouts = []\n",
    "        for _ in range(self.n_workers):\n",
    "            rollouts = self.result_queue.get()\n",
    "            all_rollouts.append(rollouts)\n",
    "        \n",
    "        return all_rollouts\n",
    "    \n",
    "    def aggregate_rollouts(self, rollouts_list):\n",
    "        \"\"\"Aggregate rollouts from all workers.\"\"\"\n",
    "        aggregated = {\n",
    "            'obs': [],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'values': [],\n",
    "            'log_probs': [],\n",
    "            'advantages': [],\n",
    "            'returns': []\n",
    "        }\n",
    "        \n",
    "        for rollouts in rollouts_list:\n",
    "            for key in aggregated:\n",
    "                aggregated[key].extend(rollouts[key])\n",
    "        \n",
    "        # Convert to tensors\n",
    "        for key in aggregated:\n",
    "            aggregated[key] = torch.FloatTensor(aggregated[key])\n",
    "        \n",
    "        return aggregated\n",
    "    \n",
    "    def update_global_model(self, rollouts):\n",
    "        \"\"\"Update global model using aggregated rollouts.\"\"\"\n",
    "        ppo_agent = PPOAgent(self.obs_dim, self.action_dim)\n",
    "        ppo_agent.network = self.global_model\n",
    "        ppo_agent.optimizer = self.optimizer\n",
    "        \n",
    "        # Prepare rollouts for PPO update\n",
    "        obs = rollouts['obs']\n",
    "        actions = rollouts['actions']\n",
    "        log_probs = rollouts['log_probs']\n",
    "        returns = rollouts['returns']\n",
    "        values = rollouts['values']\n",
    "        advantages = rollouts['advantages']\n",
    "        \n",
    "        ppo_rollouts = (obs, actions, log_probs, returns, values, advantages)\n",
    "        losses = ppo_agent.update(ppo_rollouts)\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def broadcast_parameters(self):\n",
    "        \"\"\"Send updated parameters to all workers.\"\"\"\n",
    "        state_dict = self.global_model.state_dict()\n",
    "        for i in range(self.n_workers):\n",
    "            self.task_queues[i].put(('update_params', state_dict))\n",
    "\n",
    "class EvolutionaryStrategy:\n",
    "    \"\"\"Simple evolutionary strategy for RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, population_size=50, sigma=0.1, lr=0.01):\n",
    "        self.model = model\n",
    "        self.population_size = population_size\n",
    "        self.sigma = sigma\n",
    "        self.lr = lr\n",
    "        \n",
    "        # Get parameter shapes\n",
    "        self.param_shapes = []\n",
    "        self.param_sizes = []\n",
    "        for param in model.parameters():\n",
    "            self.param_shapes.append(param.shape)\n",
    "            self.param_sizes.append(param.numel())\n",
    "        \n",
    "        self.total_params = sum(self.param_sizes)\n",
    "    \n",
    "    def generate_population(self):\n",
    "        \"\"\"Generate population of parameter perturbations.\"\"\"\n",
    "        return [np.random.randn(self.total_params) for _ in range(self.population_size)]\n",
    "    \n",
    "    def set_parameters(self, flat_params):\n",
    "        \"\"\"Set model parameters from flattened array.\"\"\"\n",
    "        idx = 0\n",
    "        with torch.no_grad():\n",
    "            for param, size, shape in zip(self.model.parameters(), self.param_sizes, self.param_shapes):\n",
    "                param_values = flat_params[idx:idx+size].reshape(shape)\n",
    "                param.copy_(torch.FloatTensor(param_values))\n",
    "                idx += size\n",
    "    \n",
    "    def get_parameters(self):\n",
    "        \"\"\"Get flattened model parameters.\"\"\"\n",
    "        params = []\n",
    "        for param in self.model.parameters():\n",
    "            params.append(param.detach().cpu().numpy().flatten())\n",
    "        return np.concatenate(params)\n",
    "    \n",
    "    def update(self, rewards, perturbations):\n",
    "        \"\"\"Update parameters using ES.\"\"\"\n",
    "        # Normalize rewards\n",
    "        rewards = np.array(rewards)\n",
    "        rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-8)\n",
    "        \n",
    "        # Compute parameter update\n",
    "        current_params = self.get_parameters()\n",
    "        param_update = np.zeros_like(current_params)\n",
    "        \n",
    "        for reward, perturbation in zip(rewards, perturbations):\n",
    "            param_update += reward * perturbation\n",
    "        \n",
    "        param_update = self.lr * param_update / (self.population_size * self.sigma)\n",
    "        \n",
    "        # Update parameters\n",
    "        new_params = current_params + param_update\n",
    "        self.set_parameters(new_params)\n",
    "        \n",
    "        return param_update\n",
    "\n",
    "# Demonstration functions\n",
    "def demonstrate_parameter_server():\n",
    "    \"\"\"Demonstrate parameter server functionality.\"\"\"\n",
    "    print(\"ðŸ–¥ï¸  Parameter Server Demo\")\n",
    "    \n",
    "    # Create dummy model\n",
    "    model = nn.Sequential(nn.Linear(4, 32), nn.ReLU(), nn.Linear(32, 2))\n",
    "    \n",
    "    # Initialize parameter server\n",
    "    param_server = ParameterServer(model.state_dict())\n",
    "    \n",
    "    print(f\"Initial version: {param_server.get_stats()['version']}\")\n",
    "    \n",
    "    # Simulate gradient update\n",
    "    dummy_gradients = {name: torch.randn_like(param) for name, param in model.named_parameters()}\n",
    "    param_server.update_parameters(dummy_gradients)\n",
    "    \n",
    "    print(f\"After update: {param_server.get_stats()}\")\n",
    "    \n",
    "    return param_server\n",
    "\n",
    "def demonstrate_evolutionary_strategy():\n",
    "    \"\"\"Demonstrate evolutionary strategy.\"\"\"\n",
    "    print(\"\\nðŸ§¬ Evolutionary Strategy Demo\")\n",
    "    \n",
    "    # Create simple model\n",
    "    model = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 2))\n",
    "    es = EvolutionaryStrategy(model, population_size=10, sigma=0.1)\n",
    "    \n",
    "    # Generate population\n",
    "    population = es.generate_population()\n",
    "    print(f\"Generated population of size: {len(population)}\")\n",
    "    print(f\"Parameter dimensionality: {es.total_params}\")\n",
    "    \n",
    "    # Simulate fitness evaluation\n",
    "    rewards = np.random.randn(len(population))\n",
    "    es.update(rewards, population)\n",
    "    \n",
    "    print(\"âœ… ES update completed\")\n",
    "    \n",
    "    return es\n",
    "\n",
    "# Run demonstrations\n",
    "print(\"ðŸŒ Distributed Reinforcement Learning Systems\")\n",
    "param_server_demo = demonstrate_parameter_server()\n",
    "es_demo = demonstrate_evolutionary_strategy()\n",
    "\n",
    "print(\"\\nðŸš€ Distributed RL implementations ready!\")\n",
    "print(\"âœ… Parameter server, A3C, IMPALA, and ES components implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5baa8904",
   "metadata": {},
   "source": [
    "# Section 5: Communication and Coordination in Multi-Agent Systems\n",
    "\n",
    "## 5.1 Communication Protocols\n",
    "\n",
    "Multi-agent systems often require sophisticated communication mechanisms to achieve coordination and share information effectively. This section explores various communication paradigms and their implementation in reinforcement learning contexts.\n",
    "\n",
    "### Communication Types:\n",
    "1. **Direct Communication**: Explicit message passing between agents\n",
    "2. **Emergent Communication**: Learned communication protocols through RL\n",
    "3. **Indirect Communication**: Environment-mediated information sharing\n",
    "4. **Broadcast vs. Targeted**: Communication scope and recipients\n",
    "\n",
    "### Mathematical Framework:\n",
    "For agent $i$ sending message $m_i^t$ at time $t$:\n",
    "$$m_i^t = \\text{CommPolicy}_i(s_i^t, h_i^t)$$\n",
    "\n",
    "Where $h_i^t$ is the communication history and the message influences other agents:\n",
    "$$\\pi_j(a_j^t | s_j^t, \\{m_k^t\\}_{k \\neq j})$$\n",
    "\n",
    "### Key Challenges:\n",
    "- **Communication Overhead**: Balancing information sharing with computational cost\n",
    "- **Partial Observability**: Deciding what information to communicate\n",
    "- **Communication Noise**: Handling unreliable communication channels\n",
    "- **Scalability**: Maintaining efficiency as the number of agents increases\n",
    "\n",
    "## 5.2 Coordination Mechanisms\n",
    "\n",
    "### Centralized Coordination:\n",
    "- Global coordinator makes joint decisions\n",
    "- Optimal but not scalable\n",
    "- Single point of failure\n",
    "\n",
    "### Decentralized Coordination:\n",
    "- Agents coordinate through local interactions\n",
    "- Scalable and robust\n",
    "- May lead to suboptimal solutions\n",
    "\n",
    "### Hierarchical Coordination:\n",
    "- Multi-level coordination structure\n",
    "- Combines benefits of centralized and decentralized approaches\n",
    "- Natural for many real-world scenarios\n",
    "\n",
    "### Market-Based Coordination:\n",
    "- Agents bid for tasks or resources\n",
    "- Economically motivated coordination\n",
    "- Natural load balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa136c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Communication and Coordination Implementation\n",
    "\n",
    "class CommunicationChannel:\n",
    "    \"\"\"Communication channel for multi-agent systems.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents, message_dim=16, noise_std=0.1):\n",
    "        self.n_agents = n_agents\n",
    "        self.message_dim = message_dim\n",
    "        self.noise_std = noise_std\n",
    "        self.message_history = []\n",
    "        \n",
    "    def send_message(self, sender_id, message, recipients=None):\n",
    "        \"\"\"Send message from one agent to others.\"\"\"\n",
    "        if recipients is None:\n",
    "            recipients = list(range(self.n_agents))\n",
    "            recipients.remove(sender_id)\n",
    "        \n",
    "        # Add noise to simulate real-world communication\n",
    "        noisy_message = message + torch.randn_like(message) * self.noise_std\n",
    "        \n",
    "        comm_event = {\n",
    "            'sender': sender_id,\n",
    "            'recipients': recipients,\n",
    "            'message': noisy_message,\n",
    "            'timestamp': len(self.message_history)\n",
    "        }\n",
    "        \n",
    "        self.message_history.append(comm_event)\n",
    "        return comm_event\n",
    "    \n",
    "    def get_messages_for_agent(self, agent_id, last_n=5):\n",
    "        \"\"\"Get recent messages for a specific agent.\"\"\"\n",
    "        relevant_messages = []\n",
    "        for event in self.message_history[-last_n:]:\n",
    "            if agent_id in event['recipients']:\n",
    "                relevant_messages.append({\n",
    "                    'sender': event['sender'],\n",
    "                    'message': event['message'],\n",
    "                    'timestamp': event['timestamp']\n",
    "                })\n",
    "        return relevant_messages\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear communication history.\"\"\"\n",
    "        self.message_history = []\n",
    "\n",
    "class AttentionCommunication(nn.Module):\n",
    "    \"\"\"Attention-based communication mechanism.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, message_dim=16, n_heads=4):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.message_dim = message_dim\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # Message encoding\n",
    "        self.message_encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, message_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(message_dim, message_dim)\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(message_dim, n_heads, batch_first=True)\n",
    "        \n",
    "        # Message processing\n",
    "        self.message_processor = nn.Sequential(\n",
    "            nn.Linear(message_dim, message_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(message_dim, message_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, observations, messages=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            observations: [batch_size, n_agents, obs_dim]\n",
    "            messages: [batch_size, n_agents, message_dim] or None\n",
    "        \"\"\"\n",
    "        batch_size, n_agents, _ = observations.shape\n",
    "        \n",
    "        # Encode observations into messages\n",
    "        encoded_messages = self.message_encoder(observations)  # [batch, n_agents, message_dim]\n",
    "        \n",
    "        if messages is not None:\n",
    "            # Combine with previous messages\n",
    "            combined_messages = encoded_messages + messages\n",
    "        else:\n",
    "            combined_messages = encoded_messages\n",
    "        \n",
    "        # Apply attention across agents\n",
    "        attended_messages, attention_weights = self.attention(\n",
    "            combined_messages, combined_messages, combined_messages\n",
    "        )\n",
    "        \n",
    "        # Process attended messages\n",
    "        processed_messages = self.message_processor(attended_messages)\n",
    "        \n",
    "        return processed_messages, attention_weights\n",
    "\n",
    "class CoordinationMechanism:\n",
    "    \"\"\"Base class for coordination mechanisms.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents):\n",
    "        self.n_agents = n_agents\n",
    "        self.coordination_history = []\n",
    "    \n",
    "    def coordinate(self, agent_states, task_requirements):\n",
    "        \"\"\"Coordinate agents based on states and task requirements.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def evaluate_coordination(self, joint_actions, outcomes):\n",
    "        \"\"\"Evaluate the quality of coordination.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class MarketBasedCoordination(CoordinationMechanism):\n",
    "    \"\"\"Market-based coordination using auction mechanisms.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents, n_tasks=5):\n",
    "        super().__init__(n_agents)\n",
    "        self.n_tasks = n_tasks\n",
    "        self.task_values = torch.rand(n_tasks) * 10  # Task values\n",
    "        \n",
    "    def conduct_auction(self, agent_bids):\n",
    "        \"\"\"\n",
    "        Conduct first-price sealed-bid auction.\n",
    "        \n",
    "        Args:\n",
    "            agent_bids: [n_agents, n_tasks] - bid matrix\n",
    "        \n",
    "        Returns:\n",
    "            task_assignments: [n_tasks] - winning agent for each task\n",
    "            winning_bids: [n_tasks] - winning bid amounts\n",
    "        \"\"\"\n",
    "        winning_agents = torch.argmax(agent_bids, dim=0)\n",
    "        winning_bids = torch.max(agent_bids, dim=0).values\n",
    "        \n",
    "        return winning_agents, winning_bids\n",
    "    \n",
    "    def coordinate(self, agent_capabilities, task_requirements):\n",
    "        \"\"\"Coordinate using market mechanism.\"\"\"\n",
    "        # Generate bids based on capabilities and task requirements\n",
    "        agent_bids = torch.zeros(self.n_agents, self.n_tasks)\n",
    "        \n",
    "        for i in range(self.n_agents):\n",
    "            for j in range(self.n_tasks):\n",
    "                # Simple bidding strategy: capability match * task value - cost\n",
    "                capability_match = torch.dot(agent_capabilities[i], task_requirements[j])\n",
    "                cost = torch.norm(agent_capabilities[i] - task_requirements[j])\n",
    "                agent_bids[i, j] = capability_match * self.task_values[j] - cost\n",
    "        \n",
    "        # Conduct auction\n",
    "        assignments, winning_bids = self.conduct_auction(agent_bids)\n",
    "        \n",
    "        coordination_result = {\n",
    "            'assignments': assignments,\n",
    "            'bids': agent_bids,\n",
    "            'winning_bids': winning_bids,\n",
    "            'total_value': torch.sum(winning_bids)\n",
    "        }\n",
    "        \n",
    "        self.coordination_history.append(coordination_result)\n",
    "        return coordination_result\n",
    "\n",
    "class HierarchicalCoordination(CoordinationMechanism):\n",
    "    \"\"\"Hierarchical coordination with multiple levels.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents, hierarchy_levels=2):\n",
    "        super().__init__(n_agents)\n",
    "        self.hierarchy_levels = hierarchy_levels\n",
    "        self.create_hierarchy()\n",
    "    \n",
    "    def create_hierarchy(self):\n",
    "        \"\"\"Create hierarchical structure.\"\"\"\n",
    "        self.hierarchy = {}\n",
    "        agents_per_level = [self.n_agents]\n",
    "        \n",
    "        for level in range(self.hierarchy_levels):\n",
    "            agents_at_level = max(1, agents_per_level[-1] // 2)\n",
    "            agents_per_level.append(agents_at_level)\n",
    "            \n",
    "            self.hierarchy[level] = {\n",
    "                'coordinators': list(range(agents_at_level)),\n",
    "                'subordinates': list(range(agents_per_level[level]))\n",
    "            }\n",
    "    \n",
    "    def coordinate_level(self, level, agent_states):\n",
    "        \"\"\"Coordinate agents at specific hierarchy level.\"\"\"\n",
    "        if level >= self.hierarchy_levels:\n",
    "            return agent_states\n",
    "        \n",
    "        coordinators = self.hierarchy[level]['coordinators']\n",
    "        subordinates = self.hierarchy[level]['subordinates']\n",
    "        \n",
    "        # High-level coordination decisions\n",
    "        coordination_decisions = []\n",
    "        for coordinator_id in coordinators:\n",
    "            # Simple coordination: average subordinate states\n",
    "            subordinate_indices = subordinates[coordinator_id::len(coordinators)]\n",
    "            if subordinate_indices:\n",
    "                avg_state = torch.mean(agent_states[subordinate_indices], dim=0)\n",
    "                coordination_decisions.append(avg_state)\n",
    "            else:\n",
    "                coordination_decisions.append(torch.zeros_like(agent_states[0]))\n",
    "        \n",
    "        return torch.stack(coordination_decisions)\n",
    "    \n",
    "    def coordinate(self, agent_states, global_objective):\n",
    "        \"\"\"Hierarchical coordination process.\"\"\"\n",
    "        current_states = agent_states\n",
    "        coordination_trace = []\n",
    "        \n",
    "        for level in range(self.hierarchy_levels):\n",
    "            level_decisions = self.coordinate_level(level, current_states)\n",
    "            coordination_trace.append(level_decisions)\n",
    "            current_states = level_decisions\n",
    "        \n",
    "        # Final global decision\n",
    "        global_decision = torch.mean(current_states, dim=0)\n",
    "        \n",
    "        return {\n",
    "            'global_decision': global_decision,\n",
    "            'level_decisions': coordination_trace,\n",
    "            'hierarchy': self.hierarchy\n",
    "        }\n",
    "\n",
    "class EmergentCommunicationAgent(nn.Module):\n",
    "    \"\"\"Agent that learns to communicate through RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, message_dim=8, vocab_size=16):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.message_dim = message_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Observation encoding\n",
    "        self.obs_encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32)\n",
    "        )\n",
    "        \n",
    "        # Message generation\n",
    "        self.message_generator = nn.Sequential(\n",
    "            nn.Linear(32, message_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(message_dim, vocab_size)\n",
    "        )\n",
    "        \n",
    "        # Message interpretation\n",
    "        self.message_interpreter = nn.Sequential(\n",
    "            nn.Linear(vocab_size, message_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(message_dim, 16)\n",
    "        )\n",
    "        \n",
    "        # Action policy (considering messages)\n",
    "        self.action_policy = nn.Sequential(\n",
    "            nn.Linear(32 + 16, 64),  # obs_encoding + message_interpretation\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Value function\n",
    "        self.value_function = nn.Sequential(\n",
    "            nn.Linear(32 + 16, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def generate_message(self, obs):\n",
    "        \"\"\"Generate message based on observation.\"\"\"\n",
    "        obs_encoding = self.obs_encoder(obs)\n",
    "        message_logits = self.message_generator(obs_encoding)\n",
    "        \n",
    "        # Sample message from categorical distribution\n",
    "        message_dist = Categorical(logits=message_logits)\n",
    "        message = message_dist.sample()\n",
    "        message_log_prob = message_dist.log_prob(message)\n",
    "        \n",
    "        return message, message_log_prob\n",
    "    \n",
    "    def interpret_messages(self, messages):\n",
    "        \"\"\"Interpret received messages.\"\"\"\n",
    "        # Convert discrete messages to one-hot\n",
    "        one_hot_messages = F.one_hot(messages, self.vocab_size).float()\n",
    "        \n",
    "        # Average messages from multiple agents\n",
    "        if len(one_hot_messages.shape) > 1:\n",
    "            avg_message = torch.mean(one_hot_messages, dim=0)\n",
    "        else:\n",
    "            avg_message = one_hot_messages\n",
    "        \n",
    "        return self.message_interpreter(avg_message)\n",
    "    \n",
    "    def forward(self, obs, received_messages=None):\n",
    "        \"\"\"Forward pass considering observations and messages.\"\"\"\n",
    "        obs_encoding = self.obs_encoder(obs)\n",
    "        \n",
    "        if received_messages is not None:\n",
    "            message_info = self.interpret_messages(received_messages)\n",
    "            combined_input = torch.cat([obs_encoding, message_info], dim=-1)\n",
    "        else:\n",
    "            message_info = torch.zeros(16)\n",
    "            combined_input = torch.cat([obs_encoding, message_info], dim=-1)\n",
    "        \n",
    "        # Generate action probabilities\n",
    "        action_logits = self.action_policy(combined_input)\n",
    "        action_probs = F.softmax(action_logits, dim=-1)\n",
    "        \n",
    "        # Generate value estimate\n",
    "        value = self.value_function(combined_input)\n",
    "        \n",
    "        return action_probs, value\n",
    "\n",
    "# Demonstration functions\n",
    "def demonstrate_communication():\n",
    "    \"\"\"Demonstrate communication mechanisms.\"\"\"\n",
    "    print(\"ðŸ“¡ Communication Mechanisms Demo\")\n",
    "    \n",
    "    # Initialize communication channel\n",
    "    comm_channel = CommunicationChannel(n_agents=4, message_dim=8)\n",
    "    \n",
    "    # Simulate message exchange\n",
    "    message = torch.randn(8)\n",
    "    comm_event = comm_channel.send_message(sender_id=0, message=message, recipients=[1, 2, 3])\n",
    "    \n",
    "    print(f\"Message sent from agent 0 to agents {comm_event['recipients']}\")\n",
    "    print(f\"Message shape: {comm_event['message'].shape}\")\n",
    "    \n",
    "    # Get messages for specific agent\n",
    "    messages = comm_channel.get_messages_for_agent(agent_id=1)\n",
    "    print(f\"Agent 1 received {len(messages)} messages\")\n",
    "    \n",
    "    return comm_channel\n",
    "\n",
    "def demonstrate_coordination():\n",
    "    \"\"\"Demonstrate coordination mechanisms.\"\"\"\n",
    "    print(\"\\nðŸ¤ Coordination Mechanisms Demo\")\n",
    "    \n",
    "    # Market-based coordination\n",
    "    market_coord = MarketBasedCoordination(n_agents=4, n_tasks=3)\n",
    "    \n",
    "    # Generate random agent capabilities and task requirements\n",
    "    agent_capabilities = torch.randn(4, 5)\n",
    "    task_requirements = torch.randn(3, 5)\n",
    "    \n",
    "    coordination_result = market_coord.coordinate(agent_capabilities, task_requirements)\n",
    "    \n",
    "    print(\"Market-based coordination result:\")\n",
    "    print(f\"Task assignments: {coordination_result['assignments']}\")\n",
    "    print(f\"Total value: {coordination_result['total_value']:.2f}\")\n",
    "    \n",
    "    # Hierarchical coordination\n",
    "    hierarchical_coord = HierarchicalCoordination(n_agents=8, hierarchy_levels=2)\n",
    "    agent_states = torch.randn(8, 6)\n",
    "    \n",
    "    hierarchy_result = hierarchical_coord.coordinate(agent_states, global_objective=None)\n",
    "    print(f\"\\nHierarchical coordination levels: {len(hierarchy_result['level_decisions'])}\")\n",
    "    print(f\"Global decision shape: {hierarchy_result['global_decision'].shape}\")\n",
    "    \n",
    "    return market_coord, hierarchical_coord\n",
    "\n",
    "def demonstrate_emergent_communication():\n",
    "    \"\"\"Demonstrate emergent communication.\"\"\"\n",
    "    print(\"\\nðŸ—£ï¸  Emergent Communication Demo\")\n",
    "    \n",
    "    # Create emergent communication agent\n",
    "    agent = EmergentCommunicationAgent(obs_dim=10, action_dim=4, message_dim=8, vocab_size=16)\n",
    "    \n",
    "    # Generate observation\n",
    "    obs = torch.randn(10)\n",
    "    \n",
    "    # Generate message\n",
    "    message, message_log_prob = agent.generate_message(obs)\n",
    "    print(f\"Generated message: {message.item()}, log prob: {message_log_prob.item():.3f}\")\n",
    "    \n",
    "    # Forward pass with message\n",
    "    action_probs, value = agent(obs, received_messages=torch.tensor([message]))\n",
    "    print(f\"Action probabilities shape: {action_probs.shape}\")\n",
    "    print(f\"Value estimate: {value.item():.3f}\")\n",
    "    \n",
    "    return agent\n",
    "\n",
    "# Run demonstrations\n",
    "print(\"ðŸŒ Communication and Coordination Systems\")\n",
    "comm_demo = demonstrate_communication()\n",
    "coord_demo = demonstrate_coordination()\n",
    "emergent_demo = demonstrate_emergent_communication()\n",
    "\n",
    "print(\"\\nðŸš€ Communication and coordination implementations ready!\")\n",
    "print(\"âœ… Multi-agent communication, coordination, and emergent protocols implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ac323",
   "metadata": {},
   "source": [
    "# Section 6: Meta-Learning and Adaptation in Multi-Agent Systems\n",
    "\n",
    "## 6.1 Meta-Learning Foundations\n",
    "\n",
    "Meta-learning, or \"learning to learn,\" is particularly important in multi-agent systems where agents must quickly adapt to:\n",
    "- New opponent strategies\n",
    "- Changing team compositions  \n",
    "- Novel task distributions\n",
    "- Dynamic environment conditions\n",
    "\n",
    "### Mathematical Framework:\n",
    "Given a distribution of tasks $\\mathcal{T}$, meta-learning aims to find parameters $\\theta$ such that:\n",
    "$$\\theta^* = \\arg\\min_\\theta \\mathbb{E}_{\\tau \\sim \\mathcal{T}} \\left[ \\mathcal{L}_\\tau(\\theta - \\alpha \\nabla_\\theta \\mathcal{L}_\\tau(\\theta)) \\right]$$\n",
    "\n",
    "Where $\\alpha$ is the inner learning rate and $\\mathcal{L}_\\tau$ is the loss on task $\\tau$.\n",
    "\n",
    "## 6.2 Model-Agnostic Meta-Learning (MAML) for Multi-Agent Systems\n",
    "\n",
    "MAML can be extended to multi-agent settings where agents must quickly adapt their policies to new scenarios:\n",
    "\n",
    "### Multi-Agent MAML Objective:\n",
    "$$\\min_{\\theta_1, ..., \\theta_n} \\sum_{i=1}^n \\mathbb{E}_{\\tau \\sim \\mathcal{T}} \\left[ \\mathcal{L}_{\\tau,i}(\\phi_{i,\\tau}) \\right]$$\n",
    "\n",
    "Where $\\phi_{i,\\tau} = \\theta_i - \\alpha_i \\nabla_{\\theta_i} \\mathcal{L}_{\\tau,i}(\\theta_i)$\n",
    "\n",
    "## 6.3 Few-Shot Learning in Multi-Agent Contexts\n",
    "\n",
    "### Key Challenges:\n",
    "1. **Opponent Modeling**: Quickly learning opponent behavior patterns\n",
    "2. **Team Formation**: Adapting to new team compositions\n",
    "3. **Strategy Transfer**: Applying learned strategies to new scenarios\n",
    "4. **Communication Adaptation**: Adjusting communication protocols\n",
    "\n",
    "### Applications:\n",
    "- **Multi-Agent Navigation**: Adapting to new environments with different agents\n",
    "- **Competitive Games**: Quickly learning counter-strategies\n",
    "- **Cooperative Tasks**: Forming effective teams with unknown agents\n",
    "\n",
    "## 6.4 Continual Learning in Dynamic Multi-Agent Environments\n",
    "\n",
    "### Catastrophic Forgetting Problem:\n",
    "In multi-agent systems, agents may forget how to handle previously encountered opponents or scenarios when learning new ones.\n",
    "\n",
    "### Solutions:\n",
    "1. **Elastic Weight Consolidation (EWC)**: Protect important parameters\n",
    "2. **Progressive Networks**: Expand capacity for new tasks\n",
    "3. **Memory-Augmented Networks**: Store and replay important experiences\n",
    "4. **Meta-Learning**: Learn how to quickly adapt without forgetting\n",
    "\n",
    "## 6.5 Self-Play and Population-Based Training\n",
    "\n",
    "### Self-Play Evolution:\n",
    "Agents improve by playing against previous versions of themselves or a diverse population of strategies.\n",
    "\n",
    "### Population Diversity:\n",
    "$$\\text{Diversity} = \\mathbb{E}_{\\pi_i, \\pi_j \\sim P} [D(\\pi_i, \\pi_j)]$$\n",
    "\n",
    "Where $P$ is the population and $D$ measures strategic distance between policies.\n",
    "\n",
    "### Benefits:\n",
    "- Robust strategy development\n",
    "- Automatic curriculum generation\n",
    "- Exploration of diverse play styles\n",
    "- Prevention of exploitation vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b8ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Meta-Learning and Adaptation Implementation\n",
    "\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "\n",
    "class MAMLAgent(nn.Module):\n",
    "    \"\"\"Multi-Agent Model-Agnostic Meta-Learning Agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, hidden_dim=128, meta_lr=1e-3, inner_lr=1e-2):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.meta_lr = meta_lr\n",
    "        self.inner_lr = inner_lr\n",
    "        \n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        # Meta-optimizer\n",
    "        self.meta_optimizer = optim.Adam(self.parameters(), lr=meta_lr)\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        \"\"\"Forward pass.\"\"\"\n",
    "        policy_logits = self.policy_net(obs)\n",
    "        value = self.value_net(obs)\n",
    "        return F.softmax(policy_logits, dim=-1), value\n",
    "    \n",
    "    def inner_update(self, support_batch, num_steps=5):\n",
    "        \"\"\"Perform inner loop adaptation.\"\"\"\n",
    "        # Create temporary model copy for adaptation\n",
    "        adapted_model = copy.deepcopy(self)\n",
    "        inner_optimizer = optim.SGD(adapted_model.parameters(), lr=self.inner_lr)\n",
    "        \n",
    "        for _ in range(num_steps):\n",
    "            obs, actions, rewards, next_obs, dones = support_batch\n",
    "            \n",
    "            # Compute policy and value predictions\n",
    "            action_probs, values = adapted_model(obs)\n",
    "            next_values = adapted_model(next_obs)[1]\n",
    "            \n",
    "            # Compute targets\n",
    "            targets = rewards + 0.99 * next_values * (1 - dones)\n",
    "            \n",
    "            # Compute losses\n",
    "            policy_loss = -torch.log(action_probs.gather(1, actions.unsqueeze(1))).squeeze() * (targets - values).detach()\n",
    "            value_loss = F.mse_loss(values.squeeze(), targets.detach())\n",
    "            \n",
    "            total_loss = policy_loss.mean() + 0.5 * value_loss\n",
    "            \n",
    "            # Inner update\n",
    "            inner_optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            inner_optimizer.step()\n",
    "        \n",
    "        return adapted_model\n",
    "    \n",
    "    def meta_update(self, tasks_batch):\n",
    "        \"\"\"Perform meta-update using multiple tasks.\"\"\"\n",
    "        meta_losses = []\n",
    "        \n",
    "        for task_data in tasks_batch:\n",
    "            support_batch, query_batch = task_data\n",
    "            \n",
    "            # Inner adaptation\n",
    "            adapted_model = self.inner_update(support_batch)\n",
    "            \n",
    "            # Evaluate on query set\n",
    "            obs, actions, rewards, next_obs, dones = query_batch\n",
    "            action_probs, values = adapted_model(obs)\n",
    "            next_values = adapted_model(next_obs)[1]\n",
    "            \n",
    "            targets = rewards + 0.99 * next_values * (1 - dones)\n",
    "            \n",
    "            # Meta-loss\n",
    "            policy_loss = -torch.log(action_probs.gather(1, actions.unsqueeze(1))).squeeze() * (targets - values).detach()\n",
    "            value_loss = F.mse_loss(values.squeeze(), targets.detach())\n",
    "            meta_loss = policy_loss.mean() + 0.5 * value_loss\n",
    "            \n",
    "            meta_losses.append(meta_loss)\n",
    "        \n",
    "        # Meta-gradient update\n",
    "        total_meta_loss = torch.stack(meta_losses).mean()\n",
    "        \n",
    "        self.meta_optimizer.zero_grad()\n",
    "        total_meta_loss.backward()\n",
    "        self.meta_optimizer.step()\n",
    "        \n",
    "        return total_meta_loss.item()\n",
    "\n",
    "class OpponentModel(nn.Module):\n",
    "    \"\"\"Model for predicting opponent behavior.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, opponent_action_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.opponent_action_dim = opponent_action_dim\n",
    "        \n",
    "        # Opponent policy predictor\n",
    "        self.opponent_predictor = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, opponent_action_dim)\n",
    "        )\n",
    "        \n",
    "        # Confidence estimator\n",
    "        self.confidence_net = nn.Sequential(\n",
    "            nn.Linear(obs_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        self.history = []\n",
    "    \n",
    "    def predict_opponent_action(self, obs, my_action):\n",
    "        \"\"\"Predict opponent action given observation and my action.\"\"\"\n",
    "        input_tensor = torch.cat([obs, F.one_hot(my_action, self.action_dim).float()], dim=-1)\n",
    "        opponent_logits = self.opponent_predictor(input_tensor)\n",
    "        confidence = self.confidence_net(input_tensor)\n",
    "        \n",
    "        return F.softmax(opponent_logits, dim=-1), confidence\n",
    "    \n",
    "    def update_model(self, obs, my_action, opponent_action):\n",
    "        \"\"\"Update opponent model with observed behavior.\"\"\"\n",
    "        input_tensor = torch.cat([obs, F.one_hot(my_action, self.action_dim).float()], dim=-1)\n",
    "        predicted_logits = self.opponent_predictor(input_tensor)\n",
    "        \n",
    "        # Cross-entropy loss for opponent action prediction\n",
    "        loss = F.cross_entropy(predicted_logits, opponent_action)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Store in history\n",
    "        self.history.append({\n",
    "            'obs': obs.detach(),\n",
    "            'my_action': my_action,\n",
    "            'opponent_action': opponent_action,\n",
    "            'loss': loss.item()\n",
    "        })\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def get_adaptation_speed(self):\n",
    "        \"\"\"Compute how quickly the model is adapting.\"\"\"\n",
    "        if len(self.history) < 10:\n",
    "            return 0.0\n",
    "        \n",
    "        recent_losses = [h['loss'] for h in self.history[-10:]]\n",
    "        early_losses = [h['loss'] for h in self.history[-20:-10]] if len(self.history) >= 20 else recent_losses\n",
    "        \n",
    "        return max(0, np.mean(early_losses) - np.mean(recent_losses))\n",
    "\n",
    "class PopulationBasedTraining:\n",
    "    \"\"\"Population-based training for multi-agent systems.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent_class, population_size=8, mutation_rate=0.1):\n",
    "        self.agent_class = agent_class\n",
    "        self.population_size = population_size\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.generation = 0\n",
    "        \n",
    "        # Initialize population\n",
    "        self.population = []\n",
    "        self.fitness_scores = []\n",
    "        self.diversity_scores = []\n",
    "        \n",
    "        for i in range(population_size):\n",
    "            agent = agent_class()\n",
    "            self.population.append(agent)\n",
    "            self.fitness_scores.append(0.0)\n",
    "            self.diversity_scores.append(0.0)\n",
    "    \n",
    "    def evaluate_fitness(self, agent_idx, opponents, n_games=10):\n",
    "        \"\"\"Evaluate agent fitness against opponents.\"\"\"\n",
    "        agent = self.population[agent_idx]\n",
    "        total_reward = 0\n",
    "        \n",
    "        for _ in range(n_games):\n",
    "            # Simple evaluation: random game outcome influenced by agent capability\n",
    "            # In practice, this would be actual game playing\n",
    "            game_reward = torch.randn(1).item() + agent_idx * 0.1  # Placeholder\n",
    "            total_reward += game_reward\n",
    "        \n",
    "        avg_fitness = total_reward / n_games\n",
    "        self.fitness_scores[agent_idx] = avg_fitness\n",
    "        \n",
    "        return avg_fitness\n",
    "    \n",
    "    def compute_diversity(self, agent_idx):\n",
    "        \"\"\"Compute diversity of agent compared to population.\"\"\"\n",
    "        agent = self.population[agent_idx]\n",
    "        diversity_sum = 0\n",
    "        \n",
    "        for other_idx, other_agent in enumerate(self.population):\n",
    "            if other_idx != agent_idx:\n",
    "                # Simple diversity metric: parameter distance\n",
    "                param_distance = 0\n",
    "                for p1, p2 in zip(agent.parameters(), other_agent.parameters()):\n",
    "                    param_distance += torch.norm(p1 - p2).item()\n",
    "                diversity_sum += param_distance\n",
    "        \n",
    "        avg_diversity = diversity_sum / (self.population_size - 1)\n",
    "        self.diversity_scores[agent_idx] = avg_diversity\n",
    "        \n",
    "        return avg_diversity\n",
    "    \n",
    "    def select_parents(self, selection_pressure=0.7):\n",
    "        \"\"\"Select parents for next generation.\"\"\"\n",
    "        # Combine fitness and diversity scores\n",
    "        combined_scores = []\n",
    "        for i in range(self.population_size):\n",
    "            score = selection_pressure * self.fitness_scores[i] + (1 - selection_pressure) * self.diversity_scores[i]\n",
    "            combined_scores.append(score)\n",
    "        \n",
    "        # Tournament selection\n",
    "        parents = []\n",
    "        for _ in range(self.population_size // 2):\n",
    "            tournament_size = 3\n",
    "            tournament_indices = np.random.choice(self.population_size, tournament_size, replace=False)\n",
    "            winner = tournament_indices[np.argmax([combined_scores[i] for i in tournament_indices])]\n",
    "            parents.append(winner)\n",
    "        \n",
    "        return parents\n",
    "    \n",
    "    def mutate_agent(self, agent):\n",
    "        \"\"\"Mutate agent parameters.\"\"\"\n",
    "        mutated_agent = copy.deepcopy(agent)\n",
    "        \n",
    "        for param in mutated_agent.parameters():\n",
    "            if torch.rand(1).item() < self.mutation_rate:\n",
    "                noise = torch.randn_like(param) * 0.1\n",
    "                param.data += noise\n",
    "        \n",
    "        return mutated_agent\n",
    "    \n",
    "    def evolve_generation(self):\n",
    "        \"\"\"Evolve population for one generation.\"\"\"\n",
    "        # Evaluate all agents\n",
    "        for i in range(self.population_size):\n",
    "            self.evaluate_fitness(i, opponents=list(range(self.population_size)))\n",
    "            self.compute_diversity(i)\n",
    "        \n",
    "        # Select parents\n",
    "        parent_indices = self.select_parents()\n",
    "        \n",
    "        # Create next generation\n",
    "        new_population = []\n",
    "        \n",
    "        # Keep top performers\n",
    "        top_performers = sorted(range(self.population_size), \n",
    "                              key=lambda x: self.fitness_scores[x], reverse=True)[:2]\n",
    "        \n",
    "        for idx in top_performers:\n",
    "            new_population.append(copy.deepcopy(self.population[idx]))\n",
    "        \n",
    "        # Generate offspring\n",
    "        while len(new_population) < self.population_size:\n",
    "            parent_idx = np.random.choice(parent_indices)\n",
    "            parent = self.population[parent_idx]\n",
    "            offspring = self.mutate_agent(parent)\n",
    "            new_population.append(offspring)\n",
    "        \n",
    "        # Update population\n",
    "        self.population = new_population\n",
    "        self.generation += 1\n",
    "        \n",
    "        return {\n",
    "            'generation': self.generation,\n",
    "            'avg_fitness': np.mean(self.fitness_scores),\n",
    "            'max_fitness': np.max(self.fitness_scores),\n",
    "            'avg_diversity': np.mean(self.diversity_scores)\n",
    "        }\n",
    "\n",
    "class SelfPlayTraining:\n",
    "    \"\"\"Self-play training system.\"\"\"\n",
    "    \n",
    "    def __init__(self, agent, env, save_frequency=10):\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        self.save_frequency = save_frequency\n",
    "        \n",
    "        # Historical opponents (checkpoints)\n",
    "        self.historical_opponents = []\n",
    "        self.training_iteration = 0\n",
    "        \n",
    "    def add_checkpoint(self):\n",
    "        \"\"\"Add current agent as historical opponent.\"\"\"\n",
    "        checkpoint = copy.deepcopy(self.agent)\n",
    "        self.historical_opponents.append({\n",
    "            'agent': checkpoint,\n",
    "            'iteration': self.training_iteration,\n",
    "            'performance': 0.0\n",
    "        })\n",
    "        \n",
    "        # Limit number of historical opponents\n",
    "        if len(self.historical_opponents) > 20:\n",
    "            self.historical_opponents.pop(0)\n",
    "    \n",
    "    def select_opponent(self, strategy='diverse'):\n",
    "        \"\"\"Select opponent for training.\"\"\"\n",
    "        if not self.historical_opponents:\n",
    "            return copy.deepcopy(self.agent)  # Self-play\n",
    "        \n",
    "        if strategy == 'diverse':\n",
    "            # Select diverse set of opponents\n",
    "            return np.random.choice(self.historical_opponents)['agent']\n",
    "        \n",
    "        elif strategy == 'recent':\n",
    "            # Focus on recent opponents\n",
    "            recent_opponents = self.historical_opponents[-5:]\n",
    "            return np.random.choice(recent_opponents)['agent']\n",
    "        \n",
    "        elif strategy == 'strongest':\n",
    "            # Play against strongest opponents\n",
    "            strongest = max(self.historical_opponents, key=lambda x: x['performance'])\n",
    "            return strongest['agent']\n",
    "        \n",
    "        else:\n",
    "            return np.random.choice(self.historical_opponents)['agent']\n",
    "    \n",
    "    def train_step(self, opponent_strategy='diverse'):\n",
    "        \"\"\"Single self-play training step.\"\"\"\n",
    "        opponent = self.select_opponent(opponent_strategy)\n",
    "        \n",
    "        # Play game against opponent (simplified)\n",
    "        state = self.env.reset()\n",
    "        total_reward = 0\n",
    "        \n",
    "        for step in range(100):  # Max episode length\n",
    "            # Agent action\n",
    "            with torch.no_grad():\n",
    "                action_probs, _ = self.agent(torch.FloatTensor(state))\n",
    "                action = Categorical(action_probs).sample().item()\n",
    "            \n",
    "            # Opponent action (simplified)\n",
    "            with torch.no_grad():\n",
    "                opp_action_probs, _ = opponent(torch.FloatTensor(state))\n",
    "                opp_action = Categorical(opp_action_probs).sample().item()\n",
    "            \n",
    "            # Environment step (placeholder)\n",
    "            next_state, reward, done, _ = self.env.step([action, opp_action])\n",
    "            total_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        self.training_iteration += 1\n",
    "        \n",
    "        # Periodically save checkpoint\n",
    "        if self.training_iteration % self.save_frequency == 0:\n",
    "            self.add_checkpoint()\n",
    "        \n",
    "        return total_reward\n",
    "\n",
    "# Demonstration functions\n",
    "def demonstrate_maml():\n",
    "    \"\"\"Demonstrate MAML for multi-agent learning.\"\"\"\n",
    "    print(\"ðŸ§  Meta-Learning (MAML) Demo\")\n",
    "    \n",
    "    # Create MAML agent\n",
    "    maml_agent = MAMLAgent(obs_dim=8, action_dim=4, hidden_dim=64)\n",
    "    \n",
    "    # Create dummy task batch\n",
    "    tasks_batch = []\n",
    "    for _ in range(3):  # 3 tasks\n",
    "        # Support set\n",
    "        support_obs = torch.randn(10, 8)\n",
    "        support_actions = torch.randint(0, 4, (10,))\n",
    "        support_rewards = torch.randn(10)\n",
    "        support_next_obs = torch.randn(10, 8)\n",
    "        support_dones = torch.zeros(10)\n",
    "        \n",
    "        support_batch = (support_obs, support_actions, support_rewards, support_next_obs, support_dones)\n",
    "        \n",
    "        # Query set\n",
    "        query_obs = torch.randn(5, 8)\n",
    "        query_actions = torch.randint(0, 4, (5,))\n",
    "        query_rewards = torch.randn(5)\n",
    "        query_next_obs = torch.randn(5, 8)\n",
    "        query_dones = torch.zeros(5)\n",
    "        \n",
    "        query_batch = (query_obs, query_actions, query_rewards, query_next_obs, query_dones)\n",
    "        \n",
    "        tasks_batch.append((support_batch, query_batch))\n",
    "    \n",
    "    # Perform meta-update\n",
    "    meta_loss = maml_agent.meta_update(tasks_batch)\n",
    "    print(f\"Meta-loss: {meta_loss:.4f}\")\n",
    "    \n",
    "    return maml_agent\n",
    "\n",
    "def demonstrate_opponent_modeling():\n",
    "    \"\"\"Demonstrate opponent modeling.\"\"\"\n",
    "    print(\"\\nðŸŽ¯ Opponent Modeling Demo\")\n",
    "    \n",
    "    opponent_model = OpponentModel(obs_dim=8, action_dim=4, opponent_action_dim=4)\n",
    "    \n",
    "    # Simulate opponent interactions\n",
    "    for _ in range(20):\n",
    "        obs = torch.randn(8)\n",
    "        my_action = torch.randint(0, 4, (1,)).item()\n",
    "        opponent_action = torch.randint(0, 4, (1,))\n",
    "        \n",
    "        loss = opponent_model.update_model(obs, my_action, opponent_action)\n",
    "    \n",
    "    adaptation_speed = opponent_model.get_adaptation_speed()\n",
    "    print(f\"Adaptation speed: {adaptation_speed:.4f}\")\n",
    "    \n",
    "    # Test prediction\n",
    "    test_obs = torch.randn(8)\n",
    "    test_action = 0\n",
    "    pred_action_probs, confidence = opponent_model.predict_opponent_action(test_obs, test_action)\n",
    "    \n",
    "    print(f\"Predicted opponent action probabilities: {pred_action_probs}\")\n",
    "    print(f\"Prediction confidence: {confidence.item():.3f}\")\n",
    "    \n",
    "    return opponent_model\n",
    "\n",
    "def demonstrate_population_training():\n",
    "    \"\"\"Demonstrate population-based training.\"\"\"\n",
    "    print(\"\\nðŸ§¬ Population-Based Training Demo\")\n",
    "    \n",
    "    # Define simple agent class for demo\n",
    "    class SimpleAgent(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.policy = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 2))\n",
    "    \n",
    "    pbt = PopulationBasedTraining(SimpleAgent, population_size=6)\n",
    "    \n",
    "    # Evolve for a few generations\n",
    "    for generation in range(3):\n",
    "        stats = pbt.evolve_generation()\n",
    "        print(f\"Generation {stats['generation']}: \"\n",
    "              f\"Avg Fitness: {stats['avg_fitness']:.3f}, \"\n",
    "              f\"Max Fitness: {stats['max_fitness']:.3f}\")\n",
    "    \n",
    "    return pbt\n",
    "\n",
    "# Run demonstrations\n",
    "print(\"ðŸŽ“ Meta-Learning and Adaptation Systems\")\n",
    "maml_demo = demonstrate_maml()\n",
    "opponent_demo = demonstrate_opponent_modeling()\n",
    "population_demo = demonstrate_population_training()\n",
    "\n",
    "print(\"\\nðŸš€ Meta-learning and adaptation implementations ready!\")\n",
    "print(\"âœ… MAML, opponent modeling, and population-based training implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba34c93",
   "metadata": {},
   "source": [
    "# Section 7: Comprehensive Applications and Case Studies\n",
    "\n",
    "## 7.1 Multi-Agent Resource Allocation\n",
    "\n",
    "Resource allocation is a fundamental problem in multi-agent systems where agents must efficiently distribute limited resources while considering individual objectives and system-wide constraints.\n",
    "\n",
    "### Problem Formulation:\n",
    "- **Agents**: $\\mathcal{A} = \\{1, 2, ..., n\\}$\n",
    "- **Resources**: $\\mathcal{R} = \\{r_1, r_2, ..., r_m\\}$ with quantities $\\{q_1, q_2, ..., q_m\\}$\n",
    "- **Allocations**: $x_{i,j}$ = amount of resource $j$ allocated to agent $i$\n",
    "- **Constraints**: $\\sum_{i=1}^n x_{i,j} \\leq q_j$ for all $j$\n",
    "\n",
    "### Objective Functions:\n",
    "1. **Utilitarian**: $\\max \\sum_{i=1}^n U_i(x_i)$\n",
    "2. **Egalitarian**: $\\max \\min_i U_i(x_i)$\n",
    "3. **Nash Social Welfare**: $\\max \\prod_{i=1}^n U_i(x_i)$\n",
    "\n",
    "## 7.2 Autonomous Vehicle Coordination\n",
    "\n",
    "Multi-agent reinforcement learning applications in autonomous vehicle systems present unique challenges in safety, efficiency, and scalability.\n",
    "\n",
    "### Key Components:\n",
    "- **Vehicle Agents**: Each vehicle as an independent learning agent\n",
    "- **Communication**: V2V (Vehicle-to-Vehicle) and V2I (Vehicle-to-Infrastructure)\n",
    "- **Objectives**: Safety, traffic flow optimization, fuel efficiency\n",
    "- **Constraints**: Traffic rules, physical limitations, safety margins\n",
    "\n",
    "### Coordination Challenges:\n",
    "1. **Intersection Management**: Distributed traffic light control\n",
    "2. **Highway Merging**: Cooperative lane changing and merging\n",
    "3. **Platooning**: Formation and maintenance of vehicle platoons\n",
    "4. **Emergency Response**: Coordinated response to accidents or hazards\n",
    "\n",
    "## 7.3 Smart Grid Management\n",
    "\n",
    "The smart grid represents a complex multi-agent system where various entities must coordinate for efficient energy distribution and consumption.\n",
    "\n",
    "### Agent Types:\n",
    "- **Producers**: Power plants, renewable energy sources\n",
    "- **Consumers**: Residential, commercial, industrial users\n",
    "- **Storage**: Battery systems, pumped hydro storage\n",
    "- **Grid Operators**: Transmission and distribution system operators\n",
    "\n",
    "### Challenges:\n",
    "- **Demand Response**: Dynamic pricing and consumption adjustment\n",
    "- **Load Balancing**: Real-time supply-demand matching\n",
    "- **Renewable Integration**: Managing intermittent energy sources\n",
    "- **Market Mechanisms**: Automated bidding and trading\n",
    "\n",
    "## 7.4 Robotics Swarm Coordination\n",
    "\n",
    "Swarm robotics involves coordinating large numbers of simple robots to achieve complex collective behaviors.\n",
    "\n",
    "### Applications:\n",
    "- **Search and Rescue**: Coordinated search patterns\n",
    "- **Environmental Monitoring**: Distributed sensor networks\n",
    "- **Construction**: Collaborative building and assembly\n",
    "- **Military/Defense**: Autonomous drone swarms\n",
    "\n",
    "### Technical Challenges:\n",
    "- **Scalability**: Algorithms that work with hundreds or thousands of agents\n",
    "- **Fault Tolerance**: Graceful degradation when agents fail\n",
    "- **Communication Limits**: Bandwidth and range constraints\n",
    "- **Real-time Coordination**: Fast decision making in dynamic environments\n",
    "\n",
    "## 7.5 Financial Trading Systems\n",
    "\n",
    "Multi-agent systems in financial markets involve multiple trading agents with different strategies and objectives.\n",
    "\n",
    "### Agent Categories:\n",
    "- **Market Makers**: Provide liquidity\n",
    "- **Arbitrageurs**: Exploit price differences\n",
    "- **Trend Followers**: Follow market momentum\n",
    "- **Mean Reversion**: Bet on price corrections\n",
    "\n",
    "### Market Dynamics:\n",
    "- **Price Discovery**: Collective determination of asset values\n",
    "- **Liquidity Provision**: Ensuring tradeable markets\n",
    "- **Risk Management**: Controlling exposure and volatility\n",
    "- **Regulatory Compliance**: Following trading rules and regulations\n",
    "\n",
    "## 7.6 Game-Theoretic Analysis Framework\n",
    "\n",
    "### Nash Equilibrium in Multi-Agent RL:\n",
    "For policies $\\pi = (\\pi_1, ..., \\pi_n)$, a Nash equilibrium satisfies:\n",
    "$$J_i(\\pi_i^*, \\pi_{-i}^*) \\geq J_i(\\pi_i, \\pi_{-i}^*) \\quad \\forall \\pi_i, \\forall i$$\n",
    "\n",
    "### Stackelberg Games:\n",
    "Leader-follower dynamics where one agent commits to a strategy first:\n",
    "$$\\max_{\\pi_L} J_L(\\pi_L, \\pi_F^*(\\pi_L))$$\n",
    "$$\\text{s.t. } \\pi_F^*(\\pi_L) = \\arg\\max_{\\pi_F} J_F(\\pi_L, \\pi_F)$$\n",
    "\n",
    "### Cooperative Game Theory:\n",
    "- **Shapley Value**: Fair allocation of cooperative gains\n",
    "- **Core**: Stable coalition structures\n",
    "- **Nucleolus**: Solution concept for transferable utility games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Applications and Case Studies Implementation\n",
    "\n",
    "class ResourceAllocationEnvironment:\n",
    "    \"\"\"Multi-agent resource allocation environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents=4, n_resources=3, resource_capacities=None):\n",
    "        self.n_agents = n_agents\n",
    "        self.n_resources = n_resources\n",
    "        \n",
    "        if resource_capacities is None:\n",
    "            self.resource_capacities = torch.ones(n_resources) * 10.0\n",
    "        else:\n",
    "            self.resource_capacities = torch.tensor(resource_capacities)\n",
    "        \n",
    "        # Agent utility functions (random for demo)\n",
    "        self.agent_utilities = []\n",
    "        for _ in range(n_agents):\n",
    "            utility_weights = torch.rand(n_resources) * 2  # Random utility weights\n",
    "            self.agent_utilities.append(utility_weights)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment.\"\"\"\n",
    "        self.current_allocations = torch.zeros(self.n_agents, self.n_resources)\n",
    "        self.remaining_resources = self.resource_capacities.clone()\n",
    "        self.time_step = 0\n",
    "        \n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Get current state for all agents.\"\"\"\n",
    "        states = []\n",
    "        for i in range(self.n_agents):\n",
    "            # State includes current allocation and remaining resources\n",
    "            agent_state = torch.cat([\n",
    "                self.current_allocations[i],  # Own allocation\n",
    "                self.remaining_resources,     # Remaining resources\n",
    "                self.current_allocations.sum(0)  # Total allocated\n",
    "            ])\n",
    "            states.append(agent_state)\n",
    "        \n",
    "        return torch.stack(states)\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Execute actions for all agents.\n",
    "        Actions: [n_agents, n_resources] - requested allocation amounts\n",
    "        \"\"\"\n",
    "        actions = torch.tensor(actions).float()\n",
    "        \n",
    "        # Ensure actions are non-negative and within limits\n",
    "        actions = torch.clamp(actions, 0, 1)  # Normalized requests\n",
    "        \n",
    "        # Scale actions based on remaining resources\n",
    "        scaled_actions = actions * self.remaining_resources.unsqueeze(0)\n",
    "        \n",
    "        # Resolve conflicts using proportional allocation\n",
    "        total_requests = scaled_actions.sum(0)\n",
    "        allocation_ratios = torch.ones_like(total_requests)\n",
    "        \n",
    "        # Apply capacity constraints\n",
    "        over_capacity = total_requests > self.remaining_resources\n",
    "        allocation_ratios[over_capacity] = (self.remaining_resources[over_capacity] / \n",
    "                                          total_requests[over_capacity])\n",
    "        \n",
    "        # Compute actual allocations\n",
    "        actual_allocations = scaled_actions * allocation_ratios.unsqueeze(0)\n",
    "        \n",
    "        # Update state\n",
    "        self.current_allocations += actual_allocations\n",
    "        self.remaining_resources -= actual_allocations.sum(0)\n",
    "        \n",
    "        # Compute rewards (utility gained)\n",
    "        rewards = []\n",
    "        for i in range(self.n_agents):\n",
    "            utility = torch.dot(actual_allocations[i], self.agent_utilities[i])\n",
    "            rewards.append(utility.item())\n",
    "        \n",
    "        self.time_step += 1\n",
    "        done = self.time_step >= 20 or torch.all(self.remaining_resources <= 0.1)\n",
    "        \n",
    "        return self.get_state(), rewards, done, {}\n",
    "    \n",
    "    def compute_social_welfare(self):\n",
    "        \"\"\"Compute total social welfare.\"\"\"\n",
    "        total_welfare = 0\n",
    "        for i in range(self.n_agents):\n",
    "            agent_welfare = torch.dot(self.current_allocations[i], self.agent_utilities[i])\n",
    "            total_welfare += agent_welfare.item()\n",
    "        return total_welfare\n",
    "\n",
    "class AutonomousVehicleEnvironment:\n",
    "    \"\"\"Simplified autonomous vehicle coordination environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_vehicles=4, road_length=100):\n",
    "        self.n_vehicles = n_vehicles\n",
    "        self.road_length = road_length\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment.\"\"\"\n",
    "        # Vehicle positions (random start)\n",
    "        self.positions = torch.rand(self.n_vehicles) * self.road_length * 0.3\n",
    "        \n",
    "        # Vehicle velocities (start slow)\n",
    "        self.velocities = torch.ones(self.n_vehicles) * 5.0\n",
    "        \n",
    "        # Target velocities (desired speed)\n",
    "        self.target_velocities = torch.rand(self.n_vehicles) * 10 + 10  # 10-20 m/s\n",
    "        \n",
    "        self.time_step = 0\n",
    "        \n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Get state for all vehicles.\"\"\"\n",
    "        states = []\n",
    "        for i in range(self.n_vehicles):\n",
    "            # Find nearest neighbors\n",
    "            distances = torch.abs(self.positions - self.positions[i])\n",
    "            distances[i] = float('inf')  # Exclude self\n",
    "            \n",
    "            # Get nearest vehicle info\n",
    "            nearest_idx = torch.argmin(distances)\n",
    "            relative_pos = self.positions[nearest_idx] - self.positions[i]\n",
    "            relative_vel = self.velocities[nearest_idx] - self.velocities[i]\n",
    "            \n",
    "            vehicle_state = torch.tensor([\n",
    "                self.positions[i] / self.road_length,  # Normalized position\n",
    "                self.velocities[i] / 20.0,             # Normalized velocity\n",
    "                self.target_velocities[i] / 20.0,      # Normalized target velocity\n",
    "                relative_pos / self.road_length,       # Relative position to nearest\n",
    "                relative_vel / 20.0,                   # Relative velocity to nearest\n",
    "                distances.min() / 20.0                 # Distance to nearest vehicle\n",
    "            ])\n",
    "            \n",
    "            states.append(vehicle_state)\n",
    "        \n",
    "        return torch.stack(states)\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Execute actions (acceleration commands).\n",
    "        Actions: [n_vehicles] - acceleration values (-1 to 1)\n",
    "        \"\"\"\n",
    "        actions = torch.tensor(actions).float()\n",
    "        actions = torch.clamp(actions, -1, 1)\n",
    "        \n",
    "        dt = 0.1  # Time step\n",
    "        max_accel = 3.0  # m/s^2\n",
    "        \n",
    "        # Update velocities\n",
    "        accelerations = actions * max_accel\n",
    "        self.velocities += accelerations * dt\n",
    "        self.velocities = torch.clamp(self.velocities, 0, 25)  # Speed limits\n",
    "        \n",
    "        # Update positions\n",
    "        self.positions += self.velocities * dt\n",
    "        \n",
    "        # Compute rewards\n",
    "        rewards = []\n",
    "        for i in range(self.n_vehicles):\n",
    "            # Reward components\n",
    "            speed_reward = -torch.abs(self.velocities[i] - self.target_velocities[i]) * 0.1\n",
    "            \n",
    "            # Safety reward (maintain distance)\n",
    "            distances = torch.abs(self.positions - self.positions[i])\n",
    "            distances[i] = float('inf')\n",
    "            min_distance = distances.min()\n",
    "            safety_reward = -10.0 if min_distance < 2.0 else 0.0\n",
    "            \n",
    "            # Efficiency reward (progress)\n",
    "            progress_reward = self.velocities[i] * 0.05\n",
    "            \n",
    "            total_reward = speed_reward + safety_reward + progress_reward\n",
    "            rewards.append(total_reward.item())\n",
    "        \n",
    "        self.time_step += 1\n",
    "        done = self.time_step >= 100 or torch.any(self.positions >= self.road_length)\n",
    "        \n",
    "        return self.get_state(), rewards, done, {}\n",
    "\n",
    "class SmartGridEnvironment:\n",
    "    \"\"\"Smart grid multi-agent environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_producers=2, n_consumers=3, n_storage=1):\n",
    "        self.n_producers = n_producers\n",
    "        self.n_consumers = n_consumers\n",
    "        self.n_storage = n_storage\n",
    "        self.n_agents = n_producers + n_consumers + n_storage\n",
    "        \n",
    "        # Production capacities and costs\n",
    "        self.production_capacities = torch.rand(n_producers) * 50 + 20  # 20-70 MW\n",
    "        self.production_costs = torch.rand(n_producers) * 0.1 + 0.05   # $0.05-0.15/MWh\n",
    "        \n",
    "        # Consumer demands\n",
    "        self.base_demands = torch.rand(n_consumers) * 30 + 10  # 10-40 MW\n",
    "        \n",
    "        # Storage capacities\n",
    "        self.storage_capacities = torch.ones(n_storage) * 100  # 100 MWh\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment.\"\"\"\n",
    "        self.current_storage = self.storage_capacities * 0.5  # Start half-full\n",
    "        self.time_step = 0\n",
    "        \n",
    "        # Random demand fluctuation\n",
    "        self.current_demands = self.base_demands * (0.8 + 0.4 * torch.rand(self.n_consumers))\n",
    "        \n",
    "        # Random renewable production (solar/wind variability)\n",
    "        self.renewable_factor = torch.rand(1).item() * 0.5 + 0.5  # 0.5-1.0\n",
    "        \n",
    "        return self.get_state()\n",
    "    \n",
    "    def get_state(self):\n",
    "        \"\"\"Get state for all agents.\"\"\"\n",
    "        states = []\n",
    "        \n",
    "        # Producer states\n",
    "        for i in range(self.n_producers):\n",
    "            producer_state = torch.tensor([\n",
    "                self.production_capacities[i] / 100,  # Normalized capacity\n",
    "                self.production_costs[i] * 10,        # Scaled cost\n",
    "                self.renewable_factor,                # Renewable availability\n",
    "                self.current_demands.sum() / 100,     # Total demand\n",
    "                self.time_step / 24.0                 # Time of day (normalized)\n",
    "            ])\n",
    "            states.append(producer_state)\n",
    "        \n",
    "        # Consumer states  \n",
    "        for i in range(self.n_consumers):\n",
    "            consumer_state = torch.tensor([\n",
    "                self.current_demands[i] / 50,         # Normalized demand\n",
    "                self.base_demands[i] / 50,            # Base demand\n",
    "                torch.sin(self.time_step * 2 * np.pi / 24),  # Time of day cycle\n",
    "                (self.current_demands.sum() - self.current_demands[i]) / 100,  # Other demand\n",
    "                self.renewable_factor                 # Renewable availability\n",
    "            ])\n",
    "            states.append(consumer_state)\n",
    "        \n",
    "        # Storage states\n",
    "        for i in range(self.n_storage):\n",
    "            storage_state = torch.tensor([\n",
    "                self.current_storage[i] / self.storage_capacities[i],  # Charge level\n",
    "                self.storage_capacities[i] / 100,     # Capacity\n",
    "                self.current_demands.sum() / 100,     # Total demand\n",
    "                self.renewable_factor,                # Renewable availability\n",
    "                self.time_step / 24.0                 # Time of day\n",
    "            ])\n",
    "            states.append(storage_state)\n",
    "        \n",
    "        return torch.stack(states)\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"\n",
    "        Execute actions for all agents.\n",
    "        Actions: [n_agents] - normalized action values\n",
    "        \"\"\"\n",
    "        actions = torch.tensor(actions).float()\n",
    "        actions = torch.clamp(actions, -1, 1)\n",
    "        \n",
    "        # Parse actions\n",
    "        producer_actions = actions[:self.n_producers]  # Production levels\n",
    "        consumer_actions = actions[self.n_producers:self.n_producers + self.n_consumers]  # Demand response\n",
    "        storage_actions = actions[self.n_producers + self.n_consumers:]  # Charge/discharge\n",
    "        \n",
    "        # Compute actual production\n",
    "        production = producer_actions * self.production_capacities * self.renewable_factor\n",
    "        production = torch.clamp(production, 0, self.production_capacities)\n",
    "        \n",
    "        # Compute adjusted demand (demand response)\n",
    "        adjusted_demands = self.current_demands * (1 + consumer_actions * 0.3)\n",
    "        adjusted_demands = torch.clamp(adjusted_demands, self.current_demands * 0.7, \n",
    "                                     self.current_demands * 1.3)\n",
    "        \n",
    "        # Storage actions (positive = discharge, negative = charge)\n",
    "        storage_power = storage_actions * 20  # Max 20 MW charge/discharge rate\n",
    "        \n",
    "        # Update storage levels\n",
    "        self.current_storage -= storage_power * 0.1  # 0.1 hour time step\n",
    "        self.current_storage = torch.clamp(self.current_storage, 0, self.storage_capacities)\n",
    "        \n",
    "        # Balance supply and demand\n",
    "        total_supply = production.sum() + storage_power.sum()\n",
    "        total_demand = adjusted_demands.sum()\n",
    "        imbalance = total_supply - total_demand\n",
    "        \n",
    "        # Compute rewards\n",
    "        rewards = []\n",
    "        \n",
    "        # Producer rewards (profit - penalty for imbalance)\n",
    "        for i in range(self.n_producers):\n",
    "            revenue = production[i] * 0.1  # $0.1/MWh base price\n",
    "            cost = production[i] * self.production_costs[i]\n",
    "            imbalance_penalty = abs(imbalance) * 0.01  # Penalty for grid imbalance\n",
    "            producer_reward = revenue - cost - imbalance_penalty\n",
    "            rewards.append(producer_reward.item())\n",
    "        \n",
    "        # Consumer rewards (savings from demand response - inconvenience)\n",
    "        for i in range(self.n_consumers):\n",
    "            base_cost = self.current_demands[i] * 0.1\n",
    "            actual_cost = adjusted_demands[i] * 0.1\n",
    "            inconvenience = abs(consumer_actions[i]) * 2.0  # Cost of changing demand\n",
    "            consumer_reward = base_cost - actual_cost - inconvenience\n",
    "            rewards.append(consumer_reward.item())\n",
    "        \n",
    "        # Storage rewards (arbitrage opportunities - degradation)\n",
    "        for i in range(self.n_storage):\n",
    "            arbitrage_reward = storage_power[i] * 0.02  # Profit from price differences\n",
    "            degradation_cost = abs(storage_power[i]) * 0.001  # Battery wear\n",
    "            storage_reward = arbitrage_reward - degradation_cost\n",
    "            rewards.append(storage_reward.item())\n",
    "        \n",
    "        # Update time and demand\n",
    "        self.time_step += 1\n",
    "        if self.time_step % 6 == 0:  # Update demand every 6 hours\n",
    "            self.current_demands = self.base_demands * (0.8 + 0.4 * torch.rand(self.n_consumers))\n",
    "        \n",
    "        done = self.time_step >= 24  # One day\n",
    "        \n",
    "        info = {\n",
    "            'total_supply': total_supply.item(),\n",
    "            'total_demand': total_demand.item(),\n",
    "            'imbalance': imbalance.item(),\n",
    "            'renewable_factor': self.renewable_factor\n",
    "        }\n",
    "        \n",
    "        return self.get_state(), rewards, done, info\n",
    "\n",
    "class MultiAgentGameTheoryAnalyzer:\n",
    "    \"\"\"Analyzer for game-theoretic properties of multi-agent systems.\"\"\"\n",
    "    \n",
    "    def __init__(self, n_agents, n_actions):\n",
    "        self.n_agents = n_agents\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "    def compute_payoff_matrix(self, agents, env, n_episodes=100):\n",
    "        \"\"\"Compute payoff matrix for all agent strategy combinations.\"\"\"\n",
    "        payoffs = np.zeros([self.n_actions] * self.n_agents + [self.n_agents])\n",
    "        \n",
    "        for action_profile in itertools.product(range(self.n_actions), repeat=self.n_agents):\n",
    "            total_rewards = np.zeros(self.n_agents)\n",
    "            \n",
    "            for episode in range(n_episodes):\n",
    "                state = env.reset()\n",
    "                episode_rewards = np.zeros(self.n_agents)\n",
    "                \n",
    "                for step in range(100):  # Max episode length\n",
    "                    actions = list(action_profile)\n",
    "                    next_state, rewards, done, _ = env.step(actions)\n",
    "                    \n",
    "                    episode_rewards += np.array(rewards)\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                    \n",
    "                    state = next_state\n",
    "                \n",
    "                total_rewards += episode_rewards\n",
    "            \n",
    "            avg_rewards = total_rewards / n_episodes\n",
    "            payoffs[action_profile] = avg_rewards\n",
    "        \n",
    "        return payoffs\n",
    "    \n",
    "    def find_nash_equilibria(self, payoff_matrix):\n",
    "        \"\"\"Find pure strategy Nash equilibria.\"\"\"\n",
    "        nash_equilibria = []\n",
    "        \n",
    "        for action_profile in itertools.product(range(self.n_actions), repeat=self.n_agents):\n",
    "            is_nash = True\n",
    "            \n",
    "            for agent in range(self.n_agents):\n",
    "                current_payoff = payoff_matrix[action_profile][agent]\n",
    "                \n",
    "                # Check if agent can improve by changing strategy\n",
    "                for alt_action in range(self.n_actions):\n",
    "                    if alt_action == action_profile[agent]:\n",
    "                        continue\n",
    "                    \n",
    "                    alt_profile = list(action_profile)\n",
    "                    alt_profile[agent] = alt_action\n",
    "                    alt_payoff = payoff_matrix[tuple(alt_profile)][agent]\n",
    "                    \n",
    "                    if alt_payoff > current_payoff:\n",
    "                        is_nash = False\n",
    "                        break\n",
    "                \n",
    "                if not is_nash:\n",
    "                    break\n",
    "            \n",
    "            if is_nash:\n",
    "                nash_equilibria.append(action_profile)\n",
    "        \n",
    "        return nash_equilibria\n",
    "    \n",
    "    def compute_social_welfare(self, payoff_matrix, action_profile):\n",
    "        \"\"\"Compute social welfare for given action profile.\"\"\"\n",
    "        return np.sum(payoff_matrix[action_profile])\n",
    "    \n",
    "    def find_social_optimum(self, payoff_matrix):\n",
    "        \"\"\"Find action profile that maximizes social welfare.\"\"\"\n",
    "        best_welfare = float('-inf')\n",
    "        best_profile = None\n",
    "        \n",
    "        for action_profile in itertools.product(range(self.n_actions), repeat=self.n_agents):\n",
    "            welfare = self.compute_social_welfare(payoff_matrix, action_profile)\n",
    "            if welfare > best_welfare:\n",
    "                best_welfare = welfare\n",
    "                best_profile = action_profile\n",
    "        \n",
    "        return best_profile, best_welfare\n",
    "\n",
    "# Demonstration functions\n",
    "def demonstrate_resource_allocation():\n",
    "    \"\"\"Demonstrate resource allocation environment.\"\"\"\n",
    "    print(\"ðŸ­ Resource Allocation Demo\")\n",
    "    \n",
    "    env = ResourceAllocationEnvironment(n_agents=3, n_resources=2, \n",
    "                                      resource_capacities=[20.0, 15.0])\n",
    "    \n",
    "    # Random policy simulation\n",
    "    state = env.reset()\n",
    "    total_rewards = np.zeros(3)\n",
    "    \n",
    "    for step in range(10):\n",
    "        actions = torch.rand(3, 2) * 0.3  # Random allocation requests\n",
    "        next_state, rewards, done, _ = env.step(actions)\n",
    "        \n",
    "        total_rewards += np.array(rewards)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    social_welfare = env.compute_social_welfare()\n",
    "    print(f\"Final allocations: {env.current_allocations}\")\n",
    "    print(f\"Social welfare: {social_welfare:.2f}\")\n",
    "    print(f\"Individual rewards: {total_rewards}\")\n",
    "    \n",
    "    return env\n",
    "\n",
    "def demonstrate_autonomous_vehicles():\n",
    "    \"\"\"Demonstrate autonomous vehicle coordination.\"\"\"\n",
    "    print(\"\\nðŸš— Autonomous Vehicle Coordination Demo\")\n",
    "    \n",
    "    env = AutonomousVehicleEnvironment(n_vehicles=4, road_length=100)\n",
    "    \n",
    "    state = env.reset()\n",
    "    print(f\"Initial positions: {env.positions}\")\n",
    "    print(f\"Target velocities: {env.target_velocities}\")\n",
    "    \n",
    "    # Simple coordination: maintain spacing\n",
    "    for step in range(20):\n",
    "        actions = []\n",
    "        for i in range(env.n_vehicles):\n",
    "            # Simple controller: match target speed, avoid collisions\n",
    "            speed_error = env.target_velocities[i] - env.velocities[i]\n",
    "            action = speed_error * 0.1\n",
    "            \n",
    "            # Collision avoidance\n",
    "            distances = torch.abs(env.positions - env.positions[i])\n",
    "            distances[i] = float('inf')\n",
    "            min_distance = distances.min()\n",
    "            \n",
    "            if min_distance < 5.0:  # Too close\n",
    "                action = -0.5  # Brake\n",
    "            \n",
    "            actions.append(action)\n",
    "        \n",
    "        next_state, rewards, done, _ = env.step(actions)\n",
    "        \n",
    "        if step % 5 == 0:\n",
    "            print(f\"Step {step}: Positions: {env.positions.round(1).tolist()}\")\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    return env\n",
    "\n",
    "def demonstrate_smart_grid():\n",
    "    \"\"\"Demonstrate smart grid coordination.\"\"\"\n",
    "    print(\"\\nâš¡ Smart Grid Management Demo\")\n",
    "    \n",
    "    env = SmartGridEnvironment(n_producers=2, n_consumers=2, n_storage=1)\n",
    "    \n",
    "    state = env.reset()\n",
    "    print(f\"Production capacities: {env.production_capacities.round(1)}\")\n",
    "    print(f\"Base demands: {env.base_demands.round(1)}\")\n",
    "    \n",
    "    total_rewards = np.zeros(5)  # 2 producers + 2 consumers + 1 storage\n",
    "    \n",
    "    for step in range(12):  # Half day simulation\n",
    "        # Simple coordination strategies\n",
    "        actions = []\n",
    "        \n",
    "        # Producers: produce based on demand\n",
    "        total_demand = env.current_demands.sum()\n",
    "        for i in range(env.n_producers):\n",
    "            production_ratio = min(1.0, total_demand / env.production_capacities.sum())\n",
    "            actions.append(production_ratio)\n",
    "        \n",
    "        # Consumers: slight demand response\n",
    "        for i in range(env.n_consumers):\n",
    "            demand_response = 0.1 * (torch.randn(1).item())\n",
    "            actions.append(demand_response)\n",
    "        \n",
    "        # Storage: charge during low demand, discharge during high demand\n",
    "        if total_demand > env.base_demands.sum():\n",
    "            actions.append(0.5)  # Discharge\n",
    "        else:\n",
    "            actions.append(-0.3)  # Charge\n",
    "        \n",
    "        next_state, rewards, done, info = env.step(actions)\n",
    "        total_rewards += np.array(rewards)\n",
    "        \n",
    "        if step % 3 == 0:\n",
    "            print(f\"Hour {step*2}: Supply={info['total_supply']:.1f}, \"\n",
    "                  f\"Demand={info['total_demand']:.1f}, \"\n",
    "                  f\"Imbalance={info['imbalance']:.1f}\")\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        state = next_state\n",
    "    \n",
    "    print(f\"Total rewards: {total_rewards.round(2)}\")\n",
    "    \n",
    "    return env\n",
    "\n",
    "# Run comprehensive demonstrations\n",
    "print(\"ðŸŒŸ Comprehensive Multi-Agent Applications\")\n",
    "resource_env = demonstrate_resource_allocation()\n",
    "vehicle_env = demonstrate_autonomous_vehicles()\n",
    "grid_env = demonstrate_smart_grid()\n",
    "\n",
    "print(\"\\nðŸš€ All comprehensive applications implemented!\")\n",
    "print(\"âœ… Resource allocation, autonomous vehicles, and smart grid systems ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b35fa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Evaluation and Training Framework\n",
    "\n",
    "class MultiAgentTrainingOrchestrator:\n",
    "    \"\"\"Orchestrator for comprehensive multi-agent training and evaluation.\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.training_history = []\n",
    "        self.evaluation_results = []\n",
    "        \n",
    "        # Initialize components based on config\n",
    "        self.setup_environment()\n",
    "        self.setup_agents()\n",
    "        self.setup_evaluation_metrics()\n",
    "    \n",
    "    def setup_environment(self):\n",
    "        \"\"\"Setup environment based on configuration.\"\"\"\n",
    "        env_type = self.config.get('environment', 'resource_allocation')\n",
    "        \n",
    "        if env_type == 'resource_allocation':\n",
    "            self.env = ResourceAllocationEnvironment(\n",
    "                n_agents=self.config.get('n_agents', 4),\n",
    "                n_resources=self.config.get('n_resources', 3)\n",
    "            )\n",
    "        elif env_type == 'autonomous_vehicles':\n",
    "            self.env = AutonomousVehicleEnvironment(\n",
    "                n_vehicles=self.config.get('n_agents', 4),\n",
    "                road_length=self.config.get('road_length', 100)\n",
    "            )\n",
    "        elif env_type == 'smart_grid':\n",
    "            self.env = SmartGridEnvironment(\n",
    "                n_producers=self.config.get('n_producers', 2),\n",
    "                n_consumers=self.config.get('n_consumers', 3),\n",
    "                n_storage=self.config.get('n_storage', 1)\n",
    "            )\n",
    "        else:\n",
    "            self.env = MultiAgentEnvironment(\n",
    "                n_agents=self.config.get('n_agents', 4),\n",
    "                state_dim=self.config.get('state_dim', 10),\n",
    "                action_dim=self.config.get('action_dim', 4)\n",
    "            )\n",
    "    \n",
    "    def setup_agents(self):\n",
    "        \"\"\"Setup agents based on configuration.\"\"\"\n",
    "        algorithm = self.config.get('algorithm', 'MADDPG')\n",
    "        n_agents = self.config.get('n_agents', 4)\n",
    "        \n",
    "        self.agents = []\n",
    "        \n",
    "        if algorithm == 'MADDPG':\n",
    "            obs_dim = self.config.get('obs_dim', 8)\n",
    "            action_dim = self.config.get('action_dim', 4)\n",
    "            \n",
    "            for i in range(n_agents):\n",
    "                agent = MADDPGAgent(\n",
    "                    agent_id=i,\n",
    "                    obs_dim=obs_dim,\n",
    "                    action_dim=action_dim,\n",
    "                    n_agents=n_agents,\n",
    "                    lr_actor=self.config.get('lr_actor', 1e-3),\n",
    "                    lr_critic=self.config.get('lr_critic', 1e-3)\n",
    "                )\n",
    "                self.agents.append(agent)\n",
    "        \n",
    "        elif algorithm == 'VDN':\n",
    "            for i in range(n_agents):\n",
    "                agent = VDNAgent(\n",
    "                    agent_id=i,\n",
    "                    obs_dim=self.config.get('obs_dim', 8),\n",
    "                    action_dim=self.config.get('action_dim', 4),\n",
    "                    lr=self.config.get('lr', 1e-3)\n",
    "                )\n",
    "                self.agents.append(agent)\n",
    "        \n",
    "        elif algorithm == 'PPO':\n",
    "            for i in range(n_agents):\n",
    "                agent = PPOAgent(\n",
    "                    obs_dim=self.config.get('obs_dim', 8),\n",
    "                    action_dim=self.config.get('action_dim', 4),\n",
    "                    lr=self.config.get('lr', 3e-4)\n",
    "                )\n",
    "                self.agents.append(agent)\n",
    "        \n",
    "        # Initialize communication if enabled\n",
    "        if self.config.get('enable_communication', False):\n",
    "            self.comm_channel = CommunicationChannel(\n",
    "                n_agents=n_agents,\n",
    "                message_dim=self.config.get('message_dim', 16)\n",
    "            )\n",
    "        else:\n",
    "            self.comm_channel = None\n",
    "    \n",
    "    def setup_evaluation_metrics(self):\n",
    "        \"\"\"Setup evaluation metrics.\"\"\"\n",
    "        self.metrics = {\n",
    "            'individual_rewards': [],\n",
    "            'social_welfare': [],\n",
    "            'cooperation_score': [],\n",
    "            'communication_efficiency': [],\n",
    "            'convergence_rate': [],\n",
    "            'nash_equilibrium_distance': []\n",
    "        }\n",
    "    \n",
    "    def train_episode(self, episode_idx):\n",
    "        \"\"\"Train agents for one episode.\"\"\"\n",
    "        state = self.env.reset()\n",
    "        episode_rewards = np.zeros(len(self.agents))\n",
    "        episode_length = 0\n",
    "        \n",
    "        # Episode-specific metrics\n",
    "        cooperation_events = 0\n",
    "        communication_events = 0\n",
    "        \n",
    "        while episode_length < self.config.get('max_episode_length', 100):\n",
    "            actions = []\n",
    "            \n",
    "            # Get actions from all agents\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                if hasattr(agent, 'get_action'):\n",
    "                    if self.comm_channel:\n",
    "                        # Include communication\n",
    "                        messages = self.comm_channel.get_messages_for_agent(i)\n",
    "                        action = agent.get_action(state[i], messages)\n",
    "                    else:\n",
    "                        action = agent.get_action(state[i])\n",
    "                else:\n",
    "                    # Simple policy for baseline\n",
    "                    action = torch.randint(0, self.config.get('action_dim', 4), (1,)).item()\n",
    "                \n",
    "                actions.append(action)\n",
    "            \n",
    "            # Execute actions\n",
    "            next_state, rewards, done, info = self.env.step(actions)\n",
    "            \n",
    "            # Store experiences and update agents\n",
    "            for i, agent in enumerate(self.agents):\n",
    "                if hasattr(agent, 'store_experience'):\n",
    "                    agent.store_experience(state[i], actions[i], rewards[i], next_state[i], done)\n",
    "                \n",
    "                if hasattr(agent, 'update') and episode_idx % self.config.get('update_freq', 1) == 0:\n",
    "                    agent.update()\n",
    "            \n",
    "            # Handle communication\n",
    "            if self.comm_channel:\n",
    "                for i, agent in enumerate(self.agents):\n",
    "                    if hasattr(agent, 'generate_message') and np.random.rand() < 0.1:\n",
    "                        message = agent.generate_message(state[i])\n",
    "                        self.comm_channel.send_message(i, message)\n",
    "                        communication_events += 1\n",
    "            \n",
    "            episode_rewards += np.array(rewards)\n",
    "            episode_length += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # Compute cooperation score (placeholder)\n",
    "        cooperation_score = np.std(episode_rewards) / (np.mean(episode_rewards) + 1e-8)\n",
    "        cooperation_score = 1.0 / (1.0 + cooperation_score)  # Higher is more cooperative\n",
    "        \n",
    "        # Store episode results\n",
    "        episode_result = {\n",
    "            'episode': episode_idx,\n",
    "            'individual_rewards': episode_rewards,\n",
    "            'social_welfare': np.sum(episode_rewards),\n",
    "            'cooperation_score': cooperation_score,\n",
    "            'communication_events': communication_events,\n",
    "            'episode_length': episode_length\n",
    "        }\n",
    "        \n",
    "        self.training_history.append(episode_result)\n",
    "        \n",
    "        return episode_result\n",
    "    \n",
    "    def evaluate_agents(self, n_episodes=10):\n",
    "        \"\"\"Comprehensive evaluation of trained agents.\"\"\"\n",
    "        print(f\"ðŸ” Evaluating agents over {n_episodes} episodes...\")\n",
    "        \n",
    "        evaluation_rewards = []\n",
    "        social_welfares = []\n",
    "        cooperation_scores = []\n",
    "        \n",
    "        for eval_episode in range(n_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_rewards = np.zeros(len(self.agents))\n",
    "            episode_length = 0\n",
    "            \n",
    "            while episode_length < self.config.get('max_episode_length', 100):\n",
    "                actions = []\n",
    "                \n",
    "                # Use deterministic policies for evaluation\n",
    "                for i, agent in enumerate(self.agents):\n",
    "                    with torch.no_grad():\n",
    "                        if hasattr(agent, 'get_action'):\n",
    "                            if self.comm_channel:\n",
    "                                messages = self.comm_channel.get_messages_for_agent(i)\n",
    "                                action = agent.get_action(state[i], messages, deterministic=True)\n",
    "                            else:\n",
    "                                action = agent.get_action(state[i], deterministic=True)\n",
    "                        else:\n",
    "                            action = torch.randint(0, self.config.get('action_dim', 4), (1,)).item()\n",
    "                    \n",
    "                    actions.append(action)\n",
    "                \n",
    "                next_state, rewards, done, info = self.env.step(actions)\n",
    "                episode_rewards += np.array(rewards)\n",
    "                episode_length += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            evaluation_rewards.append(episode_rewards)\n",
    "            social_welfares.append(np.sum(episode_rewards))\n",
    "            \n",
    "            # Compute cooperation score\n",
    "            cooperation_score = np.std(episode_rewards) / (np.mean(episode_rewards) + 1e-8)\n",
    "            cooperation_score = 1.0 / (1.0 + cooperation_score)\n",
    "            cooperation_scores.append(cooperation_score)\n",
    "        \n",
    "        # Aggregate results\n",
    "        evaluation_result = {\n",
    "            'mean_individual_rewards': np.mean(evaluation_rewards, axis=0),\n",
    "            'std_individual_rewards': np.std(evaluation_rewards, axis=0),\n",
    "            'mean_social_welfare': np.mean(social_welfares),\n",
    "            'std_social_welfare': np.std(social_welfares),\n",
    "            'mean_cooperation_score': np.mean(cooperation_scores),\n",
    "            'std_cooperation_score': np.std(cooperation_scores)\n",
    "        }\n",
    "        \n",
    "        self.evaluation_results.append(evaluation_result)\n",
    "        \n",
    "        return evaluation_result\n",
    "    \n",
    "    def run_training(self):\n",
    "        \"\"\"Run complete training procedure.\"\"\"\n",
    "        n_episodes = self.config.get('n_episodes', 1000)\n",
    "        eval_freq = self.config.get('eval_freq', 100)\n",
    "        \n",
    "        print(f\"ðŸš€ Starting training for {n_episodes} episodes...\")\n",
    "        print(f\"ðŸ“Š Algorithm: {self.config.get('algorithm', 'MADDPG')}\")\n",
    "        print(f\"ðŸ¤– Number of agents: {len(self.agents)}\")\n",
    "        print(f\"ðŸŒ Environment: {self.config.get('environment', 'multi_agent')}\")\n",
    "        \n",
    "        for episode in range(n_episodes):\n",
    "            # Training episode\n",
    "            episode_result = self.train_episode(episode)\n",
    "            \n",
    "            # Periodic evaluation\n",
    "            if episode % eval_freq == 0:\n",
    "                eval_result = self.evaluate_agents()\n",
    "                \n",
    "                print(f\"\\nðŸ“ˆ Episode {episode} Results:\")\n",
    "                print(f\"   Training Social Welfare: {episode_result['social_welfare']:.2f}\")\n",
    "                print(f\"   Evaluation Social Welfare: {eval_result['mean_social_welfare']:.2f} Â± {eval_result['std_social_welfare']:.2f}\")\n",
    "                print(f\"   Cooperation Score: {eval_result['mean_cooperation_score']:.3f}\")\n",
    "                \n",
    "                # Early stopping check\n",
    "                if len(self.evaluation_results) > 3:\n",
    "                    recent_performance = [r['mean_social_welfare'] for r in self.evaluation_results[-3:]]\n",
    "                    if np.std(recent_performance) < 0.1:  # Converged\n",
    "                        print(f\"ðŸŽ¯ Training converged at episode {episode}\")\n",
    "                        break\n",
    "        \n",
    "        print(\"âœ… Training completed!\")\n",
    "        \n",
    "        # Final comprehensive evaluation\n",
    "        final_evaluation = self.evaluate_agents(n_episodes=50)\n",
    "        \n",
    "        return {\n",
    "            'training_history': self.training_history,\n",
    "            'evaluation_results': self.evaluation_results,\n",
    "            'final_evaluation': final_evaluation\n",
    "        }\n",
    "    \n",
    "    def visualize_results(self):\n",
    "        \"\"\"Visualize training and evaluation results.\"\"\"\n",
    "        if not self.training_history:\n",
    "            print(\"âŒ No training history to visualize\")\n",
    "            return\n",
    "        \n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Social welfare over training\n",
    "        plt.subplot(2, 3, 1)\n",
    "        social_welfares = [result['social_welfare'] for result in self.training_history]\n",
    "        plt.plot(social_welfares)\n",
    "        plt.title('Social Welfare During Training')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Social Welfare')\n",
    "        \n",
    "        # Individual rewards over training\n",
    "        plt.subplot(2, 3, 2)\n",
    "        if len(self.training_history) > 0:\n",
    "            n_agents = len(self.training_history[0]['individual_rewards'])\n",
    "            for agent_id in range(n_agents):\n",
    "                agent_rewards = [result['individual_rewards'][agent_id] for result in self.training_history]\n",
    "                plt.plot(agent_rewards, label=f'Agent {agent_id}')\n",
    "        plt.title('Individual Rewards During Training')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Reward')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Cooperation scores\n",
    "        plt.subplot(2, 3, 3)\n",
    "        cooperation_scores = [result['cooperation_score'] for result in self.training_history]\n",
    "        plt.plot(cooperation_scores)\n",
    "        plt.title('Cooperation Score During Training')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Cooperation Score')\n",
    "        \n",
    "        # Evaluation results\n",
    "        if self.evaluation_results:\n",
    "            plt.subplot(2, 3, 4)\n",
    "            eval_welfare_means = [result['mean_social_welfare'] for result in self.evaluation_results]\n",
    "            eval_welfare_stds = [result['std_social_welfare'] for result in self.evaluation_results]\n",
    "            episodes = range(0, len(self.evaluation_results) * self.config.get('eval_freq', 100), \n",
    "                           self.config.get('eval_freq', 100))\n",
    "            \n",
    "            plt.errorbar(episodes, eval_welfare_means, yerr=eval_welfare_stds, capsize=5)\n",
    "            plt.title('Evaluation Social Welfare')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Social Welfare')\n",
    "            \n",
    "            # Final individual performance comparison\n",
    "            plt.subplot(2, 3, 5)\n",
    "            if self.evaluation_results:\n",
    "                final_result = self.evaluation_results[-1]\n",
    "                agent_means = final_result['mean_individual_rewards']\n",
    "                agent_stds = final_result['std_individual_rewards']\n",
    "                agents = range(len(agent_means))\n",
    "                \n",
    "                plt.bar(agents, agent_means, yerr=agent_stds, capsize=5)\n",
    "                plt.title('Final Individual Agent Performance')\n",
    "                plt.xlabel('Agent ID')\n",
    "                plt.ylabel('Mean Reward')\n",
    "        \n",
    "        # Algorithm comparison (if multiple runs)\n",
    "        plt.subplot(2, 3, 6)\n",
    "        plt.text(0.5, 0.5, f\"Algorithm: {self.config.get('algorithm', 'Unknown')}\\n\"\n",
    "                            f\"Environment: {self.config.get('environment', 'Unknown')}\\n\"\n",
    "                            f\"Agents: {len(self.agents)}\\n\"\n",
    "                            f\"Episodes: {len(self.training_history)}\",\n",
    "                 horizontalalignment='center', verticalalignment='center',\n",
    "                 transform=plt.gca().transAxes, fontsize=12,\n",
    "                 bbox=dict(boxstyle='round', facecolor='lightblue'))\n",
    "        plt.title('Configuration Summary')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Demonstration of comprehensive training\n",
    "def run_comprehensive_demo():\n",
    "    \"\"\"Run comprehensive multi-agent RL demonstration.\"\"\"\n",
    "    print(\"ðŸŒŸ Comprehensive Multi-Agent RL Training Demo\")\n",
    "    \n",
    "    # Configuration for different scenarios\n",
    "    configs = [\n",
    "        {\n",
    "            'name': 'MADDPG Resource Allocation',\n",
    "            'algorithm': 'MADDPG',\n",
    "            'environment': 'resource_allocation',\n",
    "            'n_agents': 3,\n",
    "            'n_resources': 2,\n",
    "            'obs_dim': 7,  # own_allocation + remaining + total_allocated\n",
    "            'action_dim': 2,  # allocation for each resource\n",
    "            'n_episodes': 200,\n",
    "            'eval_freq': 50,\n",
    "            'lr_actor': 1e-3,\n",
    "            'lr_critic': 1e-3\n",
    "        },\n",
    "        {\n",
    "            'name': 'PPO Autonomous Vehicles',\n",
    "            'algorithm': 'PPO',\n",
    "            'environment': 'autonomous_vehicles',\n",
    "            'n_agents': 3,\n",
    "            'road_length': 100,\n",
    "            'obs_dim': 6,  # position, velocity, target_velocity, relative_pos, relative_vel, distance\n",
    "            'action_dim': 3,  # discrete acceleration actions\n",
    "            'n_episodes': 300,\n",
    "            'eval_freq': 75,\n",
    "            'lr': 3e-4,\n",
    "            'enable_communication': True,\n",
    "            'message_dim': 8\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configs:\n",
    "        print(f\"\\nðŸŽ¯ Running: {config['name']}\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Create and run orchestrator\n",
    "        orchestrator = MultiAgentTrainingOrchestrator(config)\n",
    "        training_results = orchestrator.run_training()\n",
    "        \n",
    "        # Store results\n",
    "        results[config['name']] = {\n",
    "            'config': config,\n",
    "            'results': training_results,\n",
    "            'orchestrator': orchestrator\n",
    "        }\n",
    "        \n",
    "        # Visualize results\n",
    "        orchestrator.visualize_results()\n",
    "        \n",
    "        print(f\"âœ… Completed: {config['name']}\")\n",
    "        \n",
    "        # Print final performance summary\n",
    "        if training_results['evaluation_results']:\n",
    "            final_eval = training_results['final_evaluation']\n",
    "            print(f\"ðŸ“Š Final Performance Summary:\")\n",
    "            print(f\"   Social Welfare: {final_eval['mean_social_welfare']:.2f} Â± {final_eval['std_social_welfare']:.2f}\")\n",
    "            print(f\"   Individual Rewards: {final_eval['mean_individual_rewards'].round(2)}\")\n",
    "            print(f\"   Cooperation Score: {final_eval['mean_cooperation_score']:.3f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comprehensive demonstration\n",
    "print(\"ðŸš€ Starting Comprehensive Multi-Agent RL Demonstration\")\n",
    "print(\"This will train and evaluate multiple algorithms on different environments...\")\n",
    "\n",
    "# Note: This would be a full training run - for demo purposes, we'll show the structure\n",
    "print(\"ðŸ“‹ Demo Structure:\")\n",
    "print(\"1. MADDPG on Resource Allocation\")\n",
    "print(\"2. PPO on Autonomous Vehicle Coordination\")  \n",
    "print(\"3. Comprehensive evaluation and visualization\")\n",
    "print(\"\\nâš ï¸  Full training would take significant time - structure demonstrated above\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Comprehensive Multi-Agent RL Framework Complete!\")\n",
    "print(\"âœ… Training orchestrator, evaluation framework, and visualization ready!\")\n",
    "print(\"âœ… All advanced multi-agent RL concepts implemented!\")\n",
    "print(\"\\nðŸ“š Notebook Summary:\")\n",
    "print(\"â€¢ Multi-Agent Foundations & Game Theory\")\n",
    "print(\"â€¢ Cooperative Learning (MADDPG, VDN)\")\n",
    "print(\"â€¢ Advanced Policy Methods (PPO, SAC)\")\n",
    "print(\"â€¢ Distributed RL (A3C, IMPALA)\")\n",
    "print(\"â€¢ Communication & Coordination\")\n",
    "print(\"â€¢ Meta-Learning & Adaptation\")  \n",
    "print(\"â€¢ Comprehensive Applications & Case Studies\")\n",
    "print(\"â€¢ Complete Training & Evaluation Framework\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93ec12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CA12: Multi-Agent Reinforcement Learning and Advanced Policy Methods\n",
    "\n",
    "## Deep Reinforcement Learning - Session 12\n",
    "\n",
    "**Multi-Agent Reinforcement Learning (MARL), Advanced Policy Gradient Methods, and Distributed Training**\n",
    "\n",
    "This notebook explores advanced reinforcement learning topics including multi-agent systems, sophisticated policy gradient methods, distributed training techniques, and modern approaches to collaborative and competitive learning environments.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand multi-agent reinforcement learning fundamentals\n",
    "2. Implement cooperative and competitive MARL algorithms\n",
    "3. Master advanced policy gradient methods (PPO, TRPO, SAC variants)\n",
    "4. Explore distributed training and asynchronous methods\n",
    "5. Implement communication and coordination mechanisms\n",
    "6. Understand game-theoretic foundations of MARL\n",
    "7. Apply meta-learning and few-shot adaptation\n",
    "8. Analyze emergent behaviors in multi-agent systems\n",
    "\n",
    "### Notebook Structure:\n",
    "1. **Multi-Agent Foundations** - Game theory and MARL basics\n",
    "2. **Cooperative Multi-Agent Learning** - Centralized training, decentralized execution\n",
    "3. **Competitive and Mixed-Motive Systems** - Self-play and adversarial training\n",
    "4. **Advanced Policy Methods** - PPO variants, SAC improvements, TRPO\n",
    "5. **Distributed Reinforcement Learning** - A3C, IMPALA, and modern distributed methods\n",
    "6. **Communication and Coordination** - Message passing and emergent communication\n",
    "7. **Meta-Learning in RL** - Few-shot adaptation and transfer learning\n",
    "8. **Comprehensive Applications** - Real-world multi-agent scenarios\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d266fa9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c54aa38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dee118",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b6d009",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
