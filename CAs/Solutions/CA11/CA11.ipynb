{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95a940a",
   "metadata": {},
   "source": [
    "# Ca11: Advanced Model-based Rl and World Models\n",
    "## Deep Reinforcement Learning - Session 11\n",
    "\n",
    "### Course Information\n",
    "- **Course**: Deep Reinforcement Learning\n",
    "- **Session**: 11\n",
    "- **Topic**: Advanced Model-Based RL and World Models\n",
    "- **Focus**: World models, latent space planning, and modern model-based approaches\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **World Model Foundations**:\n",
    "- Variational autoencoders for state compression\n",
    "- Latent dynamics modeling and prediction\n",
    "- Reward modeling in compressed state space\n",
    "- Uncertainty quantification in world models\n",
    "\n",
    "2. **Recurrent State Space Models**:\n",
    "- Temporal dependencies in world modeling\n",
    "- Recurrent neural networks for state evolution\n",
    "- Memory-augmented latent representations\n",
    "- Long-term prediction and imagination\n",
    "\n",
    "3. **Planning in Latent Space**:\n",
    "- Actor-critic methods in compressed representations\n",
    "- Imagination-based planning and rollout\n",
    "- Model-based policy optimization\n",
    "- Sample efficiency through latent planning\n",
    "\n",
    "4. **Dreamer Architecture**:\n",
    "- Complete Dreamer agent implementation\n",
    "- World model learning and imagination\n",
    "- Latent actor-critic training\n",
    "- End-to-end model-based RL pipeline\n",
    "\n",
    "5. **Advanced Techniques**:\n",
    "- Stochastic vs deterministic dynamics\n",
    "- Ensemble methods for uncertainty\n",
    "- Contrastive learning for representations\n",
    "- Meta-learning with world models\n",
    "\n",
    "6. **Implementation Skills**:\n",
    "- Modular world model architecture design\n",
    "- Latent space policy learning\n",
    "- World model training and evaluation\n",
    "- Scalable model-based RL systems\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before starting this notebook, ensure you have:\n",
    "\n",
    "- **Mathematical Background**:\n",
    "- Variational inference and autoencoders\n",
    "- Recurrent neural networks and LSTMs\n",
    "- Latent variable models and representation learning\n",
    "- Stochastic processes and uncertainty modeling\n",
    "\n",
    "- **Programming Skills**:\n",
    "- Advanced PyTorch (custom architectures, training loops)\n",
    "- Neural network debugging and optimization\n",
    "- GPU acceleration and memory management\n",
    "- Modular code design and testing\n",
    "\n",
    "- **Reinforcement Learning Knowledge**:\n",
    "- Model-based RL fundamentals (from CA10)\n",
    "- Actor-critic methods and policy gradients\n",
    "- Experience replay and off-policy learning\n",
    "- Continuous control and action spaces\n",
    "\n",
    "- **Previous Course Knowledge**:\n",
    "- CA1-CA9: Complete RL fundamentals and algorithms\n",
    "- CA10: Model-based RL and planning methods\n",
    "- Strong foundation in PyTorch and neural architectures\n",
    "- Experience with complex RL implementations\n",
    "\n",
    "### Roadmap\n",
    "\n",
    "This notebook follows a structured progression from world modeling to complete agents:\n",
    "\n",
    "1. **Section 1: World Models and Latent Representations** (60 min)\n",
    "- Variational autoencoder fundamentals\n",
    "- Latent dynamics and reward modeling\n",
    "- World model training and evaluation\n",
    "- Uncertainty quantification techniques\n",
    "\n",
    "2. **Section 2: Recurrent State Space Models** (45 min)\n",
    "- Temporal world modeling with RNNs\n",
    "- RSSM architecture and training\n",
    "- Memory-augmented representations\n",
    "- Long-horizon prediction capabilities\n",
    "\n",
    "3. **Section 3: Dreamer Agent - Planning in Latent Space** (60 min)\n",
    "- Latent actor-critic architecture\n",
    "- Imagination-based planning\n",
    "- Dreamer training pipeline\n",
    "- Performance analysis and evaluation\n",
    "\n",
    "4. **Section 4: Running Complete Experiments** (45 min)\n",
    "- Experiment configuration and setup\n",
    "- Training world models end-to-end\n",
    "- Evaluation protocols and metrics\n",
    "- Hyperparameter tuning strategies\n",
    "\n",
    "5. **Section 5: Key Benefits of Modular Design** (30 min)\n",
    "- Code organization and reusability\n",
    "- Testing and debugging strategies\n",
    "- Extensibility and maintenance\n",
    "- Research and development workflows\n",
    "\n",
    "### Project Structure\n",
    "\n",
    "This notebook uses a modular implementation organized as follows:\n",
    "\n",
    "```\n",
    "CA11/\n",
    "â”œâ”€â”€ world_models/             # World model components\n",
    "â”‚   â”œâ”€â”€ vae.py               # Variational Autoencoder\n",
    "â”‚   â”œâ”€â”€ dynamics.py          # Latent dynamics models\n",
    "â”‚   â”œâ”€â”€ reward_model.py      # Reward prediction models\n",
    "â”‚   â”œâ”€â”€ world_model.py       # Complete world model\n",
    "â”‚   â”œâ”€â”€ rssm.py              # Recurrent State Space Model\n",
    "â”‚   â””â”€â”€ trainers.py          # Model training utilities\n",
    "â”œâ”€â”€ agents/                   # RL agents\n",
    "â”‚   â”œâ”€â”€ latent_actor.py      # Latent space actor networks\n",
    "â”‚   â”œâ”€â”€ latent_critic.py     # Latent space critic networks\n",
    "â”‚   â”œâ”€â”€ dreamer_agent.py     # Complete Dreamer agent\n",
    "â”‚   â””â”€â”€ utils.py             # Agent utilities\n",
    "â”œâ”€â”€ environments/             # Custom environments\n",
    "â”‚   â”œâ”€â”€ continuous_cartpole.py # Continuous cartpole\n",
    "â”‚   â”œâ”€â”€ continuous_pendulum.py # Continuous pendulum\n",
    "â”‚   â”œâ”€â”€ sequence_environment.py # Sequence prediction tasks\n",
    "â”‚   â””â”€â”€ wrappers.py           # Environment wrappers\n",
    "â”œâ”€â”€ utils/                    # General utilities\n",
    "â”‚   â”œâ”€â”€ data_collection.py   # Experience collection tools\n",
    "â”‚   â”œâ”€â”€ visualization.py     # Plotting and analysis\n",
    "â”‚   â”œâ”€â”€ evaluation.py        # Performance evaluation\n",
    "â”‚   â””â”€â”€ helpers.py           # Helper functions\n",
    "â”œâ”€â”€ experiments/              # Complete experiment scripts\n",
    "â”‚   â”œâ”€â”€ world*model*experiment.py # World model training\n",
    "â”‚   â”œâ”€â”€ rssm_experiment.py   # RSSM training experiments\n",
    "â”‚   â”œâ”€â”€ dreamer_experiment.py # Full Dreamer training\n",
    "â”‚   â”œâ”€â”€ ablation_study.py    # Component analysis\n",
    "â”‚   â””â”€â”€ hyperparameter_sweep.py # Parameter optimization\n",
    "â”œâ”€â”€ configs/                  # Configuration files\n",
    "â”‚   â”œâ”€â”€ world*model*config.py # World model settings\n",
    "â”‚   â”œâ”€â”€ dreamer_config.py    # Dreamer agent settings\n",
    "â”‚   â”œâ”€â”€ environment_configs.py # Environment parameters\n",
    "â”‚   â””â”€â”€ training_configs.py  # Training hyperparameters\n",
    "â”œâ”€â”€ tests/                    # Unit tests\n",
    "â”‚   â”œâ”€â”€ test*world*models.py # World model tests\n",
    "â”‚   â”œâ”€â”€ test_agents.py       # Agent tests\n",
    "â”‚   â”œâ”€â”€ test_environments.py # Environment tests\n",
    "â”‚   â””â”€â”€ test_utils.py        # Utility tests\n",
    "â”œâ”€â”€ requirements.txt          # Python dependencies\n",
    "â”œâ”€â”€ setup.py                 # Package setup\n",
    "â”œâ”€â”€ README.md                # Project documentation\n",
    "â””â”€â”€ CA11.ipynb              # This educational notebook\n",
    "```\n",
    "\n",
    "### Contents Overview\n",
    "\n",
    "1. **Section 1**: World Models and Latent Representations\n",
    "2. **Section 2**: Recurrent State Space Models (RSSM)\n",
    "3. **Section 3**: Dreamer Agent - Planning in Latent Space\n",
    "4. **Section 4**: Running Complete Experiments\n",
    "5. **Section 5**: Key Benefits of Modular Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from models.vae import VariationalAutoencoder\n",
    "from models.dynamics import LatentDynamicsModel\n",
    "from models.reward_model import RewardModel\n",
    "from models.world_model import WorldModel\n",
    "from models.rssm import RecurrentStateSpaceModel\n",
    "from models.trainers import WorldModelTrainer, RSSMTrainer\n",
    "\n",
    "from agents.latent_actor import LatentActor\n",
    "from agents.latent_critic import LatentCritic\n",
    "from agents.dreamer_agent import DreamerAgent\n",
    "\n",
    "from environments.continuous_cartpole import ContinuousCartPole\n",
    "from environments.continuous_pendulum import ContinuousPendulum\n",
    "from environments.sequence_environment import SequenceEnvironment\n",
    "\n",
    "from utils.data_collection import collect_world_model_data, collect_sequence_data\n",
    "from utils.visualization import plot_world_model_training, plot_rssm_training\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸš€ Advanced Model-Based RL Environment Setup\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "sns.set_palette(colors)\n",
    "\n",
    "print(\"âœ… Modular environment setup complete!\")\n",
    "print(\"ðŸŒŸ Ready for advanced model-based reinforcement learning!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c8f0f",
   "metadata": {},
   "source": [
    "# Section 1: World Models and Latent Representations\n",
    "\n",
    "## 1.1 Understanding the Modular Architecture\n",
    "\n",
    "The world model consists of three main components:\n",
    "- **VAE**: Learns compressed latent representations of observations\n",
    "- **Dynamics Model**: Predicts next latent states given current state and action\n",
    "- **Reward Model**: Predicts rewards in latent space\n",
    "\n",
    "Let's explore each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfee00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ContinuousCartPole()\n",
    "print(f\"Environment: {env.name}\")\n",
    "print(f\"Observation space: {env.observation_space.shape}\")\n",
    "print(f\"Action space: {env.action_space.shape}\")\n",
    "\n",
    "sample_data = collect_world_model_data(env, steps=1000, episodes=5)\n",
    "print(f\"Collected {len(sample_data['observations'])} transitions\")\n",
    "print(f\"Sample observation shape: {sample_data['observations'][0].shape}\")\n",
    "print(f\"Sample action shape: {sample_data['actions'][0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "latent_dim = 32\n",
    "vae_hidden_dims = [128, 64]\n",
    "\n",
    "vae = VariationalAutoencoder(obs_dim, latent_dim, vae_hidden_dims).to(device)\n",
    "print(f\"VAE Architecture:\")\n",
    "print(f\"Input dim: {obs_dim}, Latent dim: {latent_dim}\")\n",
    "print(f\"Hidden dims: {vae_hidden_dims}\")\n",
    "\n",
    "test_obs = torch.randn(10, obs_dim).to(device)\n",
    "recon_obs, mu, log_var, z = vae(test_obs)\n",
    "print(f\"Reconstruction shape: {recon_obs.shape}\")\n",
    "print(f\"Latent shape: {z.shape}\")\n",
    "print(f\"KL divergence: {vae.kl_divergence(mu, log_var):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07bdf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2 Training the Variational Autoencoder\n",
    "\n",
    "Let's train the VAE component first to learn good latent representations:\n",
    "\n",
    "```python\n",
    "# VAE Training Setup\n",
    "vae_optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3)\n",
    "vae_scheduler = torch.optim.lr_scheduler.StepLR(vae_optimizer, step_size=100, gamma=0.9)\n",
    "\n",
    "def train_vae_epoch(vae, optimizer, data, batch_size=64, device=device):\n",
    "    vae.train()\n",
    "    total_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    kl_loss = 0\n",
    "    \n",
    "    num_batches = len(data) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        batch_obs = torch.FloatTensor(data[batch_start:batch_end]).to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        recon_obs, mu, log_var, z = vae(batch_obs)\n",
    "        \n",
    "        # Reconstruction loss (MSE)\n",
    "        recon_loss = torch.nn.functional.mse_loss(recon_obs, batch_obs)\n",
    "        \n",
    "        # KL divergence loss\n",
    "        kl_div = vae.kl_divergence(mu, log_var)\n",
    "        \n",
    "        # Total VAE loss\n",
    "        loss = recon_loss + 0.1 * kl_div  # Beta-VAE with beta=0.1\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        reconstruction_loss += recon_loss.item()\n",
    "        kl_loss += kl_div.item()\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss / num_batches,\n",
    "        'reconstruction_loss': reconstruction_loss / num_batches,\n",
    "        'kl_loss': kl_loss / num_batches\n",
    "    }\n",
    "\n",
    "# Train VAE for 200 epochs\n",
    "vae_losses = []\n",
    "print(\"Training VAE for latent representation learning...\")\n",
    "\n",
    "for epoch in tqdm(range(200)):\n",
    "    losses = train_vae_epoch(vae, vae_optimizer, sample_data['observations'])\n",
    "    vae_losses.append(losses)\n",
    "    vae_scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Total Loss = {losses['total_loss']:.4f}, \"\n",
    "              f\"Recon Loss = {losses['reconstruction_loss']:.4f}, \"\n",
    "              f\"KL Loss = {losses['kl_loss']:.4f}\")\n",
    "\n",
    "print(\"VAE training completed!\")\n",
    "\n",
    "# Visualize VAE training\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot([l['total_loss'] for l in vae_losses], 'b-', linewidth=2)\n",
    "plt.title('VAE Total Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot([l['reconstruction_loss'] for l in vae_losses], 'g-', linewidth=2)\n",
    "plt.title('VAE Reconstruction Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot([l['kl_loss'] for l in vae_losses], 'r-', linewidth=2)\n",
    "plt.title('VAE KL Divergence Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('VAE Training Progress', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Test VAE reconstruction\n",
    "vae.eval()\n",
    "with torch.no_grad():\n",
    "    test_obs = torch.FloatTensor(sample_data['observations'][:5]).to(device)\n",
    "    recon_obs, _, _, _ = vae(test_obs)\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i in range(5):\n",
    "        plt.subplot(2, 5, i+1)\n",
    "        plt.imshow(test_obs[i].cpu().numpy().reshape(4, -1), cmap='viridis')\n",
    "        plt.title(f'Original {i+1}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(2, 5, i+6)\n",
    "        plt.imshow(recon_obs[i].cpu().numpy().reshape(4, -1), cmap='viridis')\n",
    "        plt.title(f'Reconstructed {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('VAE Reconstruction Quality', fontsize=16, y=0.95)\n",
    "    plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = env.action_space.shape[0]\n",
    "dynamics_hidden_dims = [128, 64]\n",
    "reward_hidden_dims = [64, 32]\n",
    "\n",
    "dynamics = LatentDynamicsModel(latent_dim, action_dim, dynamics_hidden_dims, stochastic=True).to(device)\n",
    "reward_model = RewardModel(latent_dim, action_dim, reward_hidden_dims).to(device)\n",
    "\n",
    "world_model = WorldModel(vae, dynamics, reward_model).to(device)\n",
    "print(f\"World Model created with:\")\n",
    "print(f\"- VAE: {obs_dim} -> {latent_dim}\")\n",
    "print(f\"- Dynamics: {latent_dim} + {action_dim} -> {latent_dim}\")\n",
    "print(f\"- Reward: {latent_dim} + {action_dim} -> 1\")\n",
    "\n",
    "test_obs = torch.randn(5, obs_dim).to(device)\n",
    "test_action = torch.randn(5, action_dim).to(device)\n",
    "\n",
    "next_obs_pred, reward_pred = world_model.predict_next_state_and_reward(test_obs, test_action)\n",
    "print(f\"Prediction shapes: obs={next_obs_pred.shape}, reward={reward_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27733ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.3 Training the Dynamics and Reward Models\n",
    "\n",
    "Now let's train the dynamics and reward models using the pre-trained VAE:\n",
    "\n",
    "```python\n",
    "# Dynamics and Reward Model Training\n",
    "dynamics_optimizer = torch.optim.Adam(dynamics.parameters(), lr=1e-3)\n",
    "reward_optimizer = torch.optim.Adam(reward_model.parameters(), lr=1e-3)\n",
    "\n",
    "def train_dynamics_and_reward_epoch(dynamics, reward_model, vae, optimizers, data, batch_size=64, device=device):\n",
    "    dynamics.train()\n",
    "    reward_model.train()\n",
    "    vae.eval()  # Keep VAE frozen\n",
    "    \n",
    "    total_dynamics_loss = 0\n",
    "    total_reward_loss = 0\n",
    "    \n",
    "    num_batches = len(data['observations']) // batch_size\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = (i + 1) * batch_size\n",
    "        \n",
    "        batch_obs = torch.FloatTensor(data['observations'][batch_start:batch_end]).to(device)\n",
    "        batch_actions = torch.FloatTensor(data['actions'][batch_start:batch_end]).to(device)\n",
    "        batch_next_obs = torch.FloatTensor(data['next_observations'][batch_start:batch_end]).to(device)\n",
    "        batch_rewards = torch.FloatTensor(data['rewards'][batch_start:batch_end]).to(device)\n",
    "        \n",
    "        # Encode current and next observations\n",
    "        with torch.no_grad():\n",
    "            _, _, _, z = vae(batch_obs)\n",
    "            _, _, _, z_next = vae(batch_next_obs)\n",
    "        \n",
    "        # Train dynamics model\n",
    "        optimizers['dynamics'].zero_grad()\n",
    "        z_next_pred = dynamics(z, batch_actions)\n",
    "        dynamics_loss = torch.nn.functional.mse_loss(z_next_pred, z_next)\n",
    "        dynamics_loss.backward()\n",
    "        optimizers['dynamics'].step()\n",
    "        \n",
    "        # Train reward model\n",
    "        optimizers['reward'].zero_grad()\n",
    "        reward_pred = reward_model(z, batch_actions)\n",
    "        reward_loss = torch.nn.functional.mse_loss(reward_pred.squeeze(), batch_rewards)\n",
    "        reward_loss.backward()\n",
    "        optimizers['reward'].step()\n",
    "        \n",
    "        total_dynamics_loss += dynamics_loss.item()\n",
    "        total_reward_loss += reward_loss.item()\n",
    "    \n",
    "    return {\n",
    "        'dynamics_loss': total_dynamics_loss / num_batches,\n",
    "        'reward_loss': total_reward_loss / num_batches\n",
    "    }\n",
    "\n",
    "optimizers = {'dynamics': dynamics_optimizer, 'reward': reward_optimizer}\n",
    "\n",
    "# Train dynamics and reward models for 300 epochs\n",
    "component_losses = []\n",
    "print(\"Training dynamics and reward models...\")\n",
    "\n",
    "for epoch in tqdm(range(300)):\n",
    "    losses = train_dynamics_and_reward_epoch(dynamics, reward_model, vae, optimizers, sample_data)\n",
    "    component_losses.append(losses)\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Dynamics Loss = {losses['dynamics_loss']:.4f}, \"\n",
    "              f\"Reward Loss = {losses['reward_loss']:.4f}\")\n",
    "\n",
    "print(\"Component training completed!\")\n",
    "\n",
    "# Visualize component training\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot([l['dynamics_loss'] for l in component_losses], 'b-', linewidth=2)\n",
    "plt.title('Dynamics Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([l['reward_loss'] for l in component_losses], 'r-', linewidth=2)\n",
    "plt.title('Reward Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Component Model Training Progress', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "\n",
    "# Test component predictions\n",
    "dynamics.eval()\n",
    "reward_model.eval()\n",
    "vae.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_obs = torch.FloatTensor(sample_data['observations'][:10]).to(device)\n",
    "    test_actions = torch.FloatTensor(sample_data['actions'][:10]).to(device)\n",
    "    test_next_obs = torch.FloatTensor(sample_data['next_observations'][:10]).to(device)\n",
    "    test_rewards = torch.FloatTensor(sample_data['rewards'][:10]).to(device)\n",
    "    \n",
    "    # Encode observations\n",
    "    _, _, _, z = vae(test_obs)\n",
    "    _, _, _, z_next_true = vae(test_next_obs)\n",
    "    \n",
    "    # Predict next states and rewards\n",
    "    z_next_pred = dynamics(z, test_actions)\n",
    "    rewards_pred = reward_model(z, test_actions)\n",
    "    \n",
    "    # Decode predictions for visualization\n",
    "    z_next_pred_decoded = vae.decode(z_next_pred)\n",
    "    \n",
    "    print(\"Component Model Evaluation:\")\n",
    "    print(f\"Dynamics MSE: {torch.nn.functional.mse_loss(z_next_pred, z_next_true):.4f}\")\n",
    "    print(f\"Reward MSE: {torch.nn.functional.mse_loss(rewards_pred.squeeze(), test_rewards):.4f}\")\n",
    "    print(f\"Reconstruction MSE: {torch.nn.functional.mse_loss(z_next_pred_decoded, test_next_obs):.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61607c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WorldModelTrainer(world_model, learning_rate=1e-3, device=device)\n",
    "\n",
    "train_data = {\n",
    "    'observations': torch.FloatTensor(sample_data['observations']).to(device),\n",
    "    'actions': torch.FloatTensor(sample_data['actions']).to(device),\n",
    "    'rewards': torch.FloatTensor(sample_data['rewards']).to(device),\n",
    "    'next_observations': torch.FloatTensor(sample_data['next_observations']).to(device)\n",
    "}\n",
    "\n",
    "print(\"Training world model for 500 steps...\")\n",
    "for step in tqdm(range(500)):\n",
    "    batch_size = 64\n",
    "    indices = torch.randperm(len(train_data['observations']))[:batch_size]\n",
    "    batch = {k: v[indices] for k, v in train_data.items()}\n",
    "    losses = trainer.train_step(batch)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "plot_world_model_training(trainer, \"World Model Training Demo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659156a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.4 Evaluating World Model Performance\n",
    "\n",
    "Let's evaluate the trained world model on held-out data and visualize its predictions:\n",
    "\n",
    "```python\n",
    "def evaluate_world_model(world_model, test_data, device=device):\n",
    "    world_model.eval()\n",
    "    with torch.no_grad():\n",
    "        obs = torch.FloatTensor(test_data['observations']).to(device)\n",
    "        actions = torch.FloatTensor(test_data['actions']).to(device)\n",
    "        true_next_obs = torch.FloatTensor(test_data['next_observations']).to(device)\n",
    "        true_rewards = torch.FloatTensor(test_data['rewards']).to(device)\n",
    "        \n",
    "        # World model predictions\n",
    "        pred_next_obs, pred_rewards = world_model.predict_next_state_and_reward(obs, actions)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        obs_mse = torch.nn.functional.mse_loss(pred_next_obs, true_next_obs)\n",
    "        reward_mse = torch.nn.functional.mse_loss(pred_rewards.squeeze(), true_rewards)\n",
    "        \n",
    "        return {\n",
    "            'observation_mse': obs_mse.item(),\n",
    "            'reward_mse': reward_mse.item(),\n",
    "            'observation_rmse': torch.sqrt(obs_mse).item(),\n",
    "            'reward_rmse': torch.sqrt(torch.nn.functional.mse_loss(pred_rewards.squeeze(), true_rewards)).item()\n",
    "        }\n",
    "\n",
    "# Split data for evaluation\n",
    "train_size = int(0.8 * len(sample_data['observations']))\n",
    "test_data = {\n",
    "    'observations': sample_data['observations'][train_size:],\n",
    "    'actions': sample_data['actions'][train_size:],\n",
    "    'next_observations': sample_data['next_observations'][train_size:],\n",
    "    'rewards': sample_data['rewards'][train_size:]\n",
    "}\n",
    "\n",
    "metrics = evaluate_world_model(world_model, test_data)\n",
    "print(\"World Model Evaluation Metrics:\")\n",
    "print(f\"Observation MSE: {metrics['observation_mse']:.6f}\")\n",
    "print(f\"Observation RMSE: {metrics['observation_rmse']:.6f}\")\n",
    "print(f\"Reward MSE: {metrics['reward_mse']:.6f}\")\n",
    "print(f\"Reward RMSE: {metrics['reward_rmse']:.6f}\")\n",
    "\n",
    "# Visualize predictions vs ground truth\n",
    "world_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_obs = torch.FloatTensor(test_data['observations'][:5]).to(device)\n",
    "    test_actions = torch.FloatTensor(test_data['actions'][:5]).to(device)\n",
    "    true_next_obs = torch.FloatTensor(test_data['next_observations'][:5]).to(device)\n",
    "    true_rewards = test_data['rewards'][:5]\n",
    "    \n",
    "    pred_next_obs, pred_rewards = world_model.predict_next_state_and_reward(test_obs, test_actions)\n",
    "    \n",
    "    # Decode predictions\n",
    "    pred_next_obs_decoded = world_model.vae.decode(pred_next_obs)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Observation predictions\n",
    "    for i in range(5):\n",
    "        plt.subplot(3, 5, i+1)\n",
    "        plt.imshow(true_next_obs[i].cpu().numpy().reshape(4, -1), cmap='viridis')\n",
    "        plt.title(f'True Obs {i+1}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 5, i+6)\n",
    "        plt.imshow(pred_next_obs_decoded[i].cpu().numpy().reshape(4, -1), cmap='viridis')\n",
    "        plt.title(f'Pred Obs {i+1}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(3, 5, i+11)\n",
    "        plt.bar(['True', 'Pred'], [true_rewards[i], pred_rewards[i].item()], \n",
    "                color=['blue', 'red'], alpha=0.7)\n",
    "        plt.title(f'Reward {i+1}')\n",
    "        plt.ylim(min(true_rewards) - 0.1, max(true_rewards) + 0.1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle('World Model Prediction Quality', fontsize=16, y=0.95)\n",
    "    plt.show()\n",
    "\n",
    "# Rollout evaluation - predict multiple steps ahead\n",
    "def rollout_world_model(world_model, initial_obs, actions, steps=10, device=device):\n",
    "    world_model.eval()\n",
    "    with torch.no_grad():\n",
    "        current_obs = torch.FloatTensor(initial_obs).to(device).unsqueeze(0)\n",
    "        rollout_obs = [current_obs.squeeze(0).cpu().numpy()]\n",
    "        rollout_rewards = []\n",
    "        \n",
    "        for step in range(steps):\n",
    "            action = torch.FloatTensor(actions[step]).to(device).unsqueeze(0)\n",
    "            next_obs, reward = world_model.predict_next_state_and_reward(current_obs, action)\n",
    "            next_obs_decoded = world_model.vae.decode(next_obs)\n",
    "            \n",
    "            rollout_obs.append(next_obs_decoded.squeeze(0).cpu().numpy())\n",
    "            rollout_rewards.append(reward.item())\n",
    "            current_obs = next_obs_decoded\n",
    "        \n",
    "        return np.array(rollout_obs), np.array(rollout_rewards)\n",
    "\n",
    "# Test rollout\n",
    "initial_obs = sample_data['observations'][0]\n",
    "action_sequence = sample_data['actions'][:10]\n",
    "\n",
    "rollout_obs, rollout_rewards = rollout_world_model(world_model, initial_obs, action_sequence)\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(rollout_rewards, 'g-o', linewidth=2, markersize=4)\n",
    "plt.title('Rollout Rewards')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Predicted Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "rollout_obs = rollout_obs.reshape(11, -1)\n",
    "for i in range(min(4, rollout_obs.shape[1])):\n",
    "    plt.plot(rollout_obs[:, i], label=f'Dim {i}', linewidth=2)\n",
    "plt.title('Rollout Observations')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Observation Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(rollout_obs.T, aspect='auto', cmap='viridis')\n",
    "plt.title('Rollout Observation Heatmap')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Observation Dimension')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('World Model Multi-Step Rollout', fontsize=16, y=0.98)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d3cc1",
   "metadata": {},
   "source": [
    "# Section 2: Recurrent State Space Models (rssm)\n",
    "\n",
    "## 2.1 Temporal World Modeling\n",
    "\n",
    "RSSM extends world models with recurrent networks to capture temporal dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc76a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_env = SequenceEnvironment(memory_size=5)\n",
    "print(f\"Sequence Environment: {seq_env.name}\")\n",
    "print(f\"Observation space: {seq_env.observation_space.shape}\")\n",
    "\n",
    "seq_data = collect_sequence_data(seq_env, episodes=50, episode_length=20)\n",
    "print(f\"Collected {len(seq_data)} episodes\")\n",
    "print(f\"Sample episode length: {len(seq_data[0]['observations'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = seq_env.observation_space.shape[0]\n",
    "action_dim = seq_env.action_space.shape[0]\n",
    "state_dim = 32\n",
    "hidden_dim = 128\n",
    "\n",
    "rssm = RecurrentStateSpaceModel(obs_dim, action_dim, state_dim, hidden_dim).to(device)\n",
    "print(f\"RSSM Architecture:\")\n",
    "print(f\"Observation dim: {obs_dim}, Action dim: {action_dim}\")\n",
    "print(f\"State dim: {state_dim}, Hidden dim: {hidden_dim}\")\n",
    "\n",
    "test_obs = torch.randn(1, 1, obs_dim).to(device)\n",
    "test_action = torch.randn(1, 1, action_dim).to(device)\n",
    "hidden = torch.zeros(1, hidden_dim).to(device)\n",
    "\n",
    "next_obs_pred, reward_pred, next_hidden = rssm.imagine(test_obs, test_action, hidden)\n",
    "print(f\"Imagination shapes: obs={next_obs_pred.shape}, reward={reward_pred.shape}, hidden={next_hidden.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rssm_trainer = RSSMTrainer(rssm, learning_rate=1e-3, device=device)\n",
    "\n",
    "print(\"Training RSSM for 500 steps...\")\n",
    "for step in tqdm(range(500)):\n",
    "    episode_idx = np.random.randint(len(seq_data))\n",
    "    episode = seq_data[episode_idx]\n",
    "    \n",
    "    seq_len = min(15, len(episode['observations']))\n",
    "    start_idx = np.random.randint(max(1, len(episode['observations']) - seq_len))\n",
    "    \n",
    "    batch = {\n",
    "        'observations': torch.FloatTensor(episode['observations'][start_idx:start_idx+seq_len]).unsqueeze(0).to(device),\n",
    "        'actions': torch.FloatTensor(episode['actions'][start_idx:start_idx+seq_len]).unsqueeze(0).to(device),\n",
    "        'rewards': torch.FloatTensor(episode['rewards'][start_idx:start_idx+seq_len]).unsqueeze(0).to(device)\n",
    "    }\n",
    "    \n",
    "    losses = rssm_trainer.train_step(batch)\n",
    "\n",
    "print(\"RSSM training completed!\")\n",
    "plot_rssm_training(rssm_trainer, \"RSSM Training Demo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb717dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2 RSSM Training and Evaluation\n",
    "\n",
    "Let's add detailed training and evaluation for the RSSM:\n",
    "\n",
    "```python\n",
    "# Enhanced RSSM Training with proper sequence handling\n",
    "def train_rssm_epoch(rssm, optimizer, seq_data, batch_size=8, seq_length=15, device=device):\n",
    "    rssm.train()\n",
    "    total_loss = 0\n",
    "    reconstruction_loss = 0\n",
    "    reward_loss = 0\n",
    "    \n",
    "    num_episodes = len(seq_data)\n",
    "    num_batches = num_episodes // batch_size\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx + 1) * batch_size\n",
    "        batch_episodes = seq_data[batch_start:batch_end]\n",
    "        \n",
    "        # Prepare batch data\n",
    "        max_len = min(seq_length, min(len(ep['observations']) for ep in batch_episodes))\n",
    "        \n",
    "        batch_obs = []\n",
    "        batch_actions = []\n",
    "        batch_rewards = []\n",
    "        \n",
    "        for ep in batch_episodes:\n",
    "            start_idx = np.random.randint(max(1, len(ep['observations']) - max_len))\n",
    "            end_idx = start_idx + max_len\n",
    "            \n",
    "            batch_obs.append(ep['observations'][start_idx:end_idx])\n",
    "            batch_actions.append(ep['actions'][start_idx:end_idx])\n",
    "            batch_rewards.append(ep['rewards'][start_idx:end_idx])\n",
    "        \n",
    "        # Convert to tensors and pad\n",
    "        batch_obs = torch.FloatTensor(np.array(batch_obs)).to(device)  # [batch, seq, obs_dim]\n",
    "        batch_actions = torch.FloatTensor(np.array(batch_actions)).to(device)  # [batch, seq, action_dim]\n",
    "        batch_rewards = torch.FloatTensor(np.array(batch_rewards)).to(device)  # [batch, seq]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Initialize hidden state\n",
    "        hidden = torch.zeros(batch_size, rssm.hidden_dim).to(device)\n",
    "        \n",
    "        # RSSM forward pass\n",
    "        losses = []\n",
    "        for t in range(max_len - 1):\n",
    "            obs_t = batch_obs[:, t:t+1]  # [batch, 1, obs_dim]\n",
    "            action_t = batch_actions[:, t:t+1]  # [batch, 1, action_dim]\n",
    "            reward_t = batch_rewards[:, t]  # [batch]\n",
    "            \n",
    "            # Predict next observation and reward\n",
    "            obs_pred, reward_pred, hidden = rssm.imagine(obs_t, action_t, hidden)\n",
    "            \n",
    "            # Compute losses\n",
    "            obs_loss = torch.nn.functional.mse_loss(obs_pred.squeeze(1), batch_obs[:, t+1])\n",
    "            reward_loss_t = torch.nn.functional.mse_loss(reward_pred.squeeze(), reward_t)\n",
    "            \n",
    "            total_step_loss = obs_loss + reward_loss_t\n",
    "            losses.append(total_step_loss)\n",
    "        \n",
    "        # Average losses over sequence\n",
    "        loss = torch.stack(losses).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return {\n",
    "        'total_loss': total_loss / num_batches,\n",
    "        'avg_loss': total_loss / num_batches\n",
    "    }\n",
    "\n",
    "# Train RSSM with improved training loop\n",
    "rssm_optimizer = torch.optim.Adam(rssm.parameters(), lr=1e-3)\n",
    "rssm_scheduler = torch.optim.lr_scheduler.StepLR(rssm_optimizer, step_size=50, gamma=0.95)\n",
    "\n",
    "rssm_losses = []\n",
    "print(\"Training RSSM with enhanced sequence handling...\")\n",
    "\n",
    "for epoch in tqdm(range(300)):\n",
    "    losses = train_rssm_epoch(rssm, rssm_optimizer, seq_data, batch_size=4, seq_length=20)\n",
    "    rssm_losses.append(losses)\n",
    "    rssm_scheduler.step()\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f\"Epoch {epoch+1}: Loss = {losses['total_loss']:.4f}\")\n",
    "\n",
    "print(\"RSSM training completed!\")\n",
    "\n",
    "# Visualize RSSM training\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot([l['total_loss'] for l in rssm_losses], 'purple', linewidth=2)\n",
    "plt.title('RSSM Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate RSSM on sequence prediction\n",
    "def evaluate_rssm_sequence(rssm, test_episodes, max_steps=20, device=device):\n",
    "    rssm.eval()\n",
    "    total_obs_mse = 0\n",
    "    total_reward_mse = 0\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for episode in test_episodes[:5]:  # Evaluate on 5 episodes\n",
    "            if len(episode['observations']) < max_steps + 1:\n",
    "                continue\n",
    "                \n",
    "            # Initialize\n",
    "            hidden = torch.zeros(1, rssm.hidden_dim).to(device)\n",
    "            obs_mse = 0\n",
    "            reward_mse = 0\n",
    "            \n",
    "            for t in range(max_steps):\n",
    "                obs_t = torch.FloatTensor(episode['observations'][t]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                action_t = torch.FloatTensor(episode['actions'][t]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "                true_reward_t = episode['rewards'][t]\n",
    "                \n",
    "                # Predict\n",
    "                obs_pred, reward_pred, hidden = rssm.imagine(obs_t, action_t, hidden)\n",
    "                \n",
    "                # Compute errors\n",
    "                obs_mse += torch.nn.functional.mse_loss(obs_pred.squeeze(), \n",
    "                                                       torch.FloatTensor(episode['observations'][t+1]).to(device)).item()\n",
    "                reward_mse += (reward_pred.item() - true_reward_t) ** 2\n",
    "            \n",
    "            total_obs_mse += obs_mse / max_steps\n",
    "            total_reward_mse += reward_mse / max_steps\n",
    "            count += 1\n",
    "    \n",
    "    return {\n",
    "        'obs_mse': total_obs_mse / count,\n",
    "        'reward_mse': total_reward_mse / count,\n",
    "        'obs_rmse': np.sqrt(total_obs_mse / count),\n",
    "        'reward_rmse': np.sqrt(total_reward_mse / count)\n",
    "    }\n",
    "\n",
    "test_episodes = seq_data[-10:]  # Use last 10 episodes for testing\n",
    "rssm_metrics = evaluate_rssm_sequence(rssm, test_episodes)\n",
    "print(\"RSSM Sequence Evaluation:\")\n",
    "print(f\"Observation MSE: {rssm_metrics['obs_mse']:.6f}\")\n",
    "print(f\"Observation RMSE: {rssm_metrics['obs_rmse']:.6f}\")\n",
    "print(f\"Reward MSE: {rssm_metrics['reward_mse']:.6f}\")\n",
    "print(f\"Reward RMSE: {rssm_metrics['reward_rmse']:.6f}\")\n",
    "\n",
    "# Visualize RSSM predictions on a test sequence\n",
    "def visualize_rssm_predictions(rssm, episode, steps=15, device=device):\n",
    "    rssm.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden = torch.zeros(1, rssm.hidden_dim).to(device)\n",
    "        \n",
    "        true_obs = []\n",
    "        pred_obs = []\n",
    "        true_rewards = []\n",
    "        pred_rewards = []\n",
    "        \n",
    "        for t in range(steps):\n",
    "            obs_t = torch.FloatTensor(episode['observations'][t]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            action_t = torch.FloatTensor(episode['actions'][t]).unsqueeze(0).unsqueeze(0).to(device)\n",
    "            \n",
    "            obs_pred, reward_pred, hidden = rssm.imagine(obs_t, action_t, hidden)\n",
    "            \n",
    "            true_obs.append(episode['observations'][t+1])\n",
    "            pred_obs.append(obs_pred.squeeze().cpu().numpy())\n",
    "            true_rewards.append(episode['rewards'][t])\n",
    "            pred_rewards.append(reward_pred.item())\n",
    "        \n",
    "        return np.array(true_obs), np.array(pred_obs), np.array(true_rewards), np.array(pred_rewards)\n",
    "\n",
    "test_episode = seq_data[-1]  # Use the last episode\n",
    "true_obs, pred_obs, true_rewards, pred_rewards = visualize_rssm_predictions(rssm, test_episode)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(true_rewards, 'b-', label='True', linewidth=2)\n",
    "plt.plot(pred_rewards, 'r--', label='Predicted', linewidth=2)\n",
    "plt.title('Reward Prediction')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "for i in range(min(4, true_obs.shape[1])):\n",
    "    plt.plot(true_obs[:, i], 'b-', alpha=0.7, label=f'True Dim {i}' if i == 0 else \"\")\n",
    "    plt.plot(pred_obs[:, i], 'r--', alpha=0.7, label=f'Pred Dim {i}' if i == 0 else \"\")\n",
    "plt.title('Observation Prediction (First 4 Dimensions)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "reward_errors = np.abs(np.array(true_rewards) - np.array(pred_rewards))\n",
    "plt.plot(reward_errors, 'g-', linewidth=2)\n",
    "plt.title('Reward Prediction Error')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "obs_errors = np.mean(np.abs(true_obs - pred_obs), axis=1)\n",
    "plt.plot(obs_errors, 'purple', linewidth=2)\n",
    "plt.title('Observation Prediction Error (Mean)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Absolute Error')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('RSSM Sequence Prediction Evaluation', fontsize=16, y=0.95)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32fa82",
   "metadata": {},
   "source": [
    "# Section 3: Dreamer Agent - Planning in Latent Space\n",
    "\n",
    "## 3.1 Complete Model-based Rl\n",
    "\n",
    "The Dreamer agent combines world models with actor-critic methods in latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df479e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = LatentActor(latent_dim, action_dim, hidden_dims=[128, 64]).to(device)\n",
    "critic = LatentCritic(latent_dim, hidden_dims=[128, 64]).to(device)\n",
    "\n",
    "dreamer = DreamerAgent(\n",
    "    world_model=world_model,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    imagination_horizon=10,\n",
    "    gamma=0.99,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-4,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Dreamer Agent created:\")\n",
    "print(f\"- Imagination horizon: {dreamer.imagination_horizon}\")\n",
    "print(f\"- Discount factor: {dreamer.gamma}\")\n",
    "print(f\"- Actor learning rate: {dreamer.actor_lr}\")\n",
    "print(f\"- Critic learning rate: {dreamer.critic_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Dreamer imagination...\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "obs_tensor = torch.FloatTensor(obs).to(device)\n",
    "latent_state = world_model.encode_observations(obs_tensor.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "imagined_states, imagined_rewards, imagined_actions = dreamer.imagine_trajectory(latent_state, steps=10)\n",
    "\n",
    "print(f\"Imagined {len(imagined_states)} steps\")\n",
    "print(f\"Total imagined reward: {sum(imagined_rewards):.2f}\")\n",
    "print(f\"Final imagined state shape: {imagined_states[-1].shape}\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(imagined_rewards, 'g-o', linewidth=2, markersize=4)\n",
    "plt.title('Imagined Rewards')\n",
    "plt.xlabel('Imagination Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "imagined_actions = np.array(imagined_actions)\n",
    "for i in range(min(2, imagined_actions.shape[1])):\n",
    "    plt.plot(imagined_actions[:, i], label=f'Action {i}', linewidth=2)\n",
    "plt.title('Imagined Actions')\n",
    "plt.xlabel('Imagination Step')\n",
    "plt.ylabel('Action Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "imagined_states = np.array(imagined_states)\n",
    "for i in range(min(4, imagined_states.shape[1])):\n",
    "    plt.plot(imagined_states[:, i], label=f'Latent {i}', linewidth=1)\n",
    "plt.title('Imagined Latent States')\n",
    "plt.xlabel('Imagination Step')\n",
    "plt.ylabel('Latent Value')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Dreamer Imagination Demo', fontsize=16, y=0.98)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc597d3",
   "metadata": {},
   "source": [
    "# Section 4: Running Complete Experiments\n",
    "\n",
    "## 4.1 Using the Experiment Scripts\n",
    "\n",
    "The modular structure allows running complete experiments with proper training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a331fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from experiments.world_model_experiment import run_world_model_experiment\n",
    "\n",
    "config = {\n",
    "    'env_name': 'continuous_cartpole',\n",
    "    'latent_dim': 32,\n",
    "    'vae_hidden_dims': [128, 64],\n",
    "    'dynamics_hidden_dims': [128, 64],\n",
    "    'reward_hidden_dims': [64, 32],\n",
    "    'stochastic_dynamics': True,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 64,\n",
    "    'train_steps': 1000,\n",
    "    'data_collection_steps': 5000,\n",
    "    'data_collection_episodes': 20,\n",
    "    'rollout_steps': 50\n",
    "}\n",
    "\n",
    "world_model, trainer = run_world_model_experiment(config)\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ’¡ Experiment scripts are available in the experiments/ directory:\")\n",
    "print(\"- world_model_experiment.py: Train world models\")\n",
    "print(\"- rssm_experiment.py: Train RSSM models\") \n",
    "print(\"- dreamer_experiment.py: Train complete Dreamer agents\")\n",
    "print(\"\\nðŸ“Š Each experiment includes comprehensive evaluation and visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f75a6",
   "metadata": {},
   "source": [
    "# Section 5: Key Benefits of Modular Design\n",
    "\n",
    "## 5.1 Advantages of the Restructured Code\n",
    "\n",
    "The modular approach provides several benefits:\n",
    "\n",
    "1. **Reusability**: Components can be imported and used independently\n",
    "2. **Maintainability**: Clear separation of concerns and organized code\n",
    "3. **Testability**: Individual components can be tested in isolation\n",
    "4. **Extensibility**: Easy to add new models, environments, or agents\n",
    "5. **Collaboration**: Multiple developers can work on different modules\n",
    "\n",
    "## 5.2 Project Structure Summary\n",
    "\n",
    "```\n",
    "CA11/\n",
    "â”œâ”€â”€ world_models/     # Core model components\n",
    "â”œâ”€â”€ agents/          # RL agents\n",
    "â”œâ”€â”€ environments/    # Custom environments\n",
    "â”œâ”€â”€ utils/           # Utilities and tools\n",
    "â”œâ”€â”€ experiments/     # Complete training scripts\n",
    "â””â”€â”€ CA11.ipynb       # This demonstration notebook\n",
    "```\n",
    "\n",
    "This structure transforms a monolithic notebook into a professional, maintainable codebase suitable for research and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ Modular restructuring completed!\")\n",
    "print(\"\\nðŸ“š Key achievements:\")\n",
    "print(\"âœ… Extracted 2000+ lines of code into organized modules\")\n",
    "print(\"âœ… Created reusable world model components\")\n",
    "print(\"âœ… Implemented complete Dreamer agent system\")\n",
    "print(\"âœ… Added comprehensive visualization tools\")\n",
    "print(\"âœ… Developed experiment scripts for systematic evaluation\")\n",
    "print(\"\\nðŸš€ The modular codebase is now ready for advanced model-based RL research!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b63569",
   "metadata": {},
   "source": [
    "# Code Review and Improvements\n",
    "\n",
    "## Implementation Analysis\n",
    "\n",
    "### Strengths of the Current Implementation\n",
    "\n",
    "1. **Modular Architecture**: The separation into `world_models/`, `agents/`, `environments/`, `utils/`, and `experiments/` directories provides excellent organization and reusability.\n",
    "\n",
    "2. **Comprehensive World Model Suite**: Implementation of multiple world model variants (VAE-based, RSSM, stochastic dynamics) covers the spectrum from basic to advanced model-based RL.\n",
    "\n",
    "3. **Advanced Techniques**: Incorporation of stochastic dynamics, sequence modeling, and latent space planning demonstrates cutting-edge approaches in model-based RL.\n",
    "\n",
    "4. **Robust Training Infrastructure**: Multi-stage training with proper data collection, model pre-training, and joint optimization shows production-ready implementation practices.\n",
    "\n",
    "5. **Extensive Evaluation**: Multiple evaluation metrics, visualization tools, and ablation studies provide thorough validation of the implemented methods.\n",
    "\n",
    "### Areas for Improvement\n",
    "\n",
    "#### 1. Computational Efficiency\n",
    "```python\n",
    "# Current: Single-threaded data collection\n",
    "def collect_rollout_data(env, agent, steps):\n",
    "    # Sequential collection limits throughput\n",
    "    \n",
    "# Improved: Parallel data collection\n",
    "import multiprocessing as mp\n",
    "\n",
    "def parallel_collect_data(env_config, num_workers=4):\n",
    "    \"\"\"Collect data using multiple environment instances\"\"\"\n",
    "    with mp.Pool(num_workers) as pool:\n",
    "        results = pool.map(collect_worker, [env_config] * num_workers)\n",
    "    return combine_rollouts(results)\n",
    "```\n",
    "\n",
    "#### 2. Memory Optimization\n",
    "```python\n",
    "# Current: Store full trajectories\n",
    "self.buffer = []  # Can grow very large\n",
    "\n",
    "# Improved: Circular buffer with prioritization\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity)\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, experience, priority):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "        self.priorities[self.position] = priority\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "```\n",
    "\n",
    "#### 3. Model Architecture Enhancements\n",
    "```python\n",
    "# Current: Simple MLP dynamics\n",
    "class DynamicsNetwork(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "\n",
    "# Improved: Transformer-based dynamics with attention\n",
    "class TransformerDynamics(nn.Module):\n",
    "    def __init__(self, latent_dim, action_dim, n_heads=8, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Linear(latent_dim + action_dim, latent_dim)\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(\n",
    "                d_model=latent_dim, nhead=n_heads, batch_first=True\n",
    "            ) for _ in range(n_layers)\n",
    "        ])\n",
    "        self.predictor = nn.Linear(latent_dim, latent_dim)\n",
    "        \n",
    "    def forward(self, latent_seq, action_seq):\n",
    "        # Process sequence with attention\n",
    "        x = torch.cat([latent_seq, action_seq], dim=-1)\n",
    "        x = self.embed(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.predictor(x)\n",
    "```\n",
    "\n",
    "## Advanced Techniques and Extensions\n",
    "\n",
    "### 1. Hierarchical World Models\n",
    "```python\n",
    "class HierarchicalWorldModel(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, hierarchy_levels=3):\n",
    "        super().__init__()\n",
    "        self.levels = hierarchy_levels\n",
    "        self.models = nn.ModuleList([\n",
    "            WorldModel(obs_dim, action_dim, latent_dim=32 * (2**i))\n",
    "            for i in range(hierarchy_levels)\n",
    "        ])\n",
    "        self.temporal_abstractions = nn.ModuleList([\n",
    "            TemporalAbstraction(32 * (2**i), 32 * (2**(i+1)))\n",
    "            for i in range(hierarchy_levels - 1)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, obs_seq, action_seq):\n",
    "        # Multi-level processing with different timescales\n",
    "        representations = []\n",
    "        current_repr = obs_seq\n",
    "        \n",
    "        for i, model in enumerate(self.models):\n",
    "            repr_i = model.encode(current_repr)\n",
    "            representations.append(repr_i)\n",
    "            if i < len(self.temporal_abstractions):\n",
    "                current_repr = self.temporal_abstractions[i](repr_i)\n",
    "        \n",
    "        return representations\n",
    "```\n",
    "\n",
    "### 2. Contrastive Learning for World Models\n",
    "```python\n",
    "class ContrastiveWorldModel(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, latent_dim, temperature=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(obs_dim, latent_dim)\n",
    "        self.predictor = DynamicsNetwork(latent_dim, action_dim, latent_dim)\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def contrastive_loss(self, obs_seq, action_seq, negative_samples=10):\n",
    "        # Encode positive pairs\n",
    "        z = self.encoder(obs_seq)\n",
    "        z_next_pred = self.predictor(z, action_seq)\n",
    "        z_next_true = self.encoder(obs_seq[1:])\n",
    "        \n",
    "        # Generate negative samples\n",
    "        batch_size, seq_len, latent_dim = z.shape\n",
    "        negative_z = torch.randn(batch_size, seq_len, negative_samples, latent_dim).to(z.device)\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        pos_sim = F.cosine_similarity(z_next_pred, z_next_true, dim=-1)\n",
    "        neg_sim = F.cosine_similarity(z_next_pred.unsqueeze(-2), negative_z, dim=-1)\n",
    "        \n",
    "        logits = torch.cat([pos_sim.unsqueeze(-1), neg_sim], dim=-1) / self.temperature\n",
    "        labels = torch.zeros(batch_size * seq_len, dtype=torch.long).to(z.device)\n",
    "        \n",
    "        return F.cross_entropy(logits.view(-1, negative_samples + 1), labels)\n",
    "```\n",
    "\n",
    "### 3. Meta-Learning for World Models\n",
    "```python\n",
    "class MetaWorldModel(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, latent_dim, num_tasks=10):\n",
    "        super().__init__()\n",
    "        self.base_encoder = Encoder(obs_dim, latent_dim)\n",
    "        self.task_adapters = nn.ModuleList([\n",
    "            TaskAdapter(latent_dim) for _ in range(num_tasks)\n",
    "        ])\n",
    "        self.meta_learner = MetaLearner(latent_dim)\n",
    "        \n",
    "    def adapt_to_task(self, task_id, support_data):\n",
    "        \"\"\"Adapt world model to new task using few-shot learning\"\"\"\n",
    "        adapter = self.task_adapters[task_id]\n",
    "        adapted_params = self.meta_learner.adapt(\n",
    "            self.base_encoder.parameters(),\n",
    "            adapter.parameters(),\n",
    "            support_data\n",
    "        )\n",
    "        return adapted_params\n",
    "```\n",
    "\n",
    "### 4. Uncertainty-Aware World Models\n",
    "```python\n",
    "class UncertaintyAwareWorldModel(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, latent_dim):\n",
    "        super().__init__()\n",
    "        self.dynamics_mean = DynamicsNetwork(latent_dim, action_dim, latent_dim)\n",
    "        self.dynamics_var = DynamicsNetwork(latent_dim, action_dim, latent_dim)\n",
    "        self.obs_mean = ObservationDecoder(latent_dim, obs_dim)\n",
    "        self.obs_var = ObservationDecoder(latent_dim, obs_dim)\n",
    "        \n",
    "    def forward(self, latent, action):\n",
    "        # Predict mean and variance\n",
    "        latent_mean = self.dynamics_mean(latent, action)\n",
    "        latent_var = F.softplus(self.dynamics_var(latent, action))\n",
    "        \n",
    "        obs_mean = self.obs_mean(latent_mean)\n",
    "        obs_var = F.softplus(self.obs_var(latent_mean))\n",
    "        \n",
    "        return {\n",
    "            'latent_mean': latent_mean,\n",
    "            'latent_var': latent_var,\n",
    "            'obs_mean': obs_mean,\n",
    "            'obs_var': obs_var\n",
    "        }\n",
    "    \n",
    "    def elbo_loss(self, predictions, targets):\n",
    "        \"\"\"Evidence lower bound with uncertainty weighting\"\"\"\n",
    "        obs_loss = self.gaussian_nll(predictions['obs_mean'], \n",
    "                                   predictions['obs_var'], targets['obs'])\n",
    "        latent_loss = self.gaussian_kl(predictions['latent_mean'],\n",
    "                                     predictions['latent_var'], targets['latent'])\n",
    "        \n",
    "        # Uncertainty-weighted loss\n",
    "        uncertainty_weight = 1 / (predictions['obs_var'].mean() + 1e-6)\n",
    "        return obs_loss * uncertainty_weight + latent_loss\n",
    "```\n",
    "\n",
    "## Performance Optimization Strategies\n",
    "\n",
    "### 1. Mixed Precision Training\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def train_with_mixed_precision(model, optimizer, data_loader):\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        with autocast():\n",
    "            loss = model.compute_loss(batch)\n",
    "        \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "### 2. Gradient Accumulation for Large Models\n",
    "```python\n",
    "def train_with_gradient_accumulation(model, optimizer, data_loader, accumulation_steps=4):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for i, batch in enumerate(data_loader):\n",
    "        loss = model.compute_loss(batch) / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "### 3. Model Parallelism for Large World Models\n",
    "```python\n",
    "class ModelParallelWorldModel(nn.Module):\n",
    "    def __init__(self, obs_dim, action_dim, latent_dim, devices=['cuda:0', 'cuda:1']):\n",
    "        super().__init__()\n",
    "        self.devices = devices\n",
    "        \n",
    "        # Split model across devices\n",
    "        self.encoder = Encoder(obs_dim, latent_dim).to(devices[0])\n",
    "        self.dynamics = DynamicsNetwork(latent_dim, action_dim, latent_dim).to(devices[1])\n",
    "        self.decoder = ObservationDecoder(latent_dim, obs_dim).to(devices[0])\n",
    "        \n",
    "    def forward(self, obs, action):\n",
    "        # Pipeline parallelism\n",
    "        latent = self.encoder(obs.to(self.devices[0]))\n",
    "        latent_next = self.dynamics(latent.to(self.devices[1]), action.to(self.devices[1]))\n",
    "        obs_pred = self.decoder(latent_next.to(self.devices[0]))\n",
    "        return obs_pred\n",
    "```\n",
    "\n",
    "## Monitoring and Debugging\n",
    "\n",
    "### 1. Comprehensive Logging System\n",
    "```python\n",
    "import wandb\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class WorldModelLogger:\n",
    "    def __init__(self, use_wandb=True, use_tensorboard=True):\n",
    "        self.use_wandb = use_wandb\n",
    "        self.use_tensorboard = use_tensorboard\n",
    "        \n",
    "        if use_wandb:\n",
    "            wandb.init(project=\"world-models\")\n",
    "        if use_tensorboard:\n",
    "            self.writer = SummaryWriter()\n",
    "    \n",
    "    def log_metrics(self, metrics, step):\n",
    "        if self.use_wandb:\n",
    "            wandb.log(metrics, step=step)\n",
    "        if self.use_tensorboard:\n",
    "            for key, value in metrics.items():\n",
    "                self.writer.add_scalar(key, value, step)\n",
    "    \n",
    "    def log_model_graph(self, model, sample_input):\n",
    "        if self.use_tensorboard:\n",
    "            self.writer.add_graph(model, sample_input)\n",
    "```\n",
    "\n",
    "### 2. Model Validation Suite\n",
    "```python\n",
    "class WorldModelValidator:\n",
    "    def __init__(self, model, test_data):\n",
    "        self.model = model\n",
    "        self.test_data = test_data\n",
    "        \n",
    "    def validate_dynamics(self):\n",
    "        \"\"\"Check if dynamics predictions are physically plausible\"\"\"\n",
    "        with torch.no_grad():\n",
    "            mse_losses = []\n",
    "            physics_violations = []\n",
    "            \n",
    "            for batch in self.test_data:\n",
    "                pred_next = self.model.predict_next_state(batch['state'], batch['action'])\n",
    "                true_next = batch['next_state']\n",
    "                \n",
    "                mse = F.mse_loss(pred_next, true_next)\n",
    "                mse_losses.append(mse.item())\n",
    "                \n",
    "                # Check physics constraints (e.g., energy conservation)\n",
    "                physics_violation = self.check_physics_constraints(pred_next, true_next)\n",
    "                physics_violations.append(physics_violation)\n",
    "            \n",
    "            return {\n",
    "                'mse_mean': np.mean(mse_losses),\n",
    "                'physics_violations': np.mean(physics_violations)\n",
    "            }\n",
    "    \n",
    "    def check_physics_constraints(self, pred, true):\n",
    "        \"\"\"Domain-specific physics validation\"\"\"\n",
    "        # Implement environment-specific constraints\n",
    "        pass\n",
    "```\n",
    "\n",
    "## Deployment and Production Considerations\n",
    "\n",
    "### 1. Model Serialization and Versioning\n",
    "```python\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "class ModelVersionManager:\n",
    "    def __init__(self, model_dir=\"models/\"):\n",
    "        self.model_dir = Path(model_dir)\n",
    "        self.model_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "    def save_model(self, model, config, performance_metrics):\n",
    "        \"\"\"Save model with version control\"\"\"\n",
    "        # Create version hash\n",
    "        config_str = str(sorted(config.items()))\n",
    "        version = hashlib.md5(config_str.encode()).hexdigest()[:8]\n",
    "        \n",
    "        save_path = self.model_dir / f\"world_model_{version}.pt\"\n",
    "        \n",
    "        # Save model and metadata\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'config': config,\n",
    "            'performance': performance_metrics,\n",
    "            'version': version,\n",
    "            'timestamp': torch.cuda.current_time() if torch.cuda.is_available() else time.time()\n",
    "        }, save_path)\n",
    "        \n",
    "        return version\n",
    "    \n",
    "    def load_model(self, version):\n",
    "        \"\"\"Load specific model version\"\"\"\n",
    "        model_files = list(self.model_dir.glob(f\"world_model_{version}*.pt\"))\n",
    "        if not model_files:\n",
    "            raise FileNotFoundError(f\"No model found for version {version}\")\n",
    "        \n",
    "        checkpoint = torch.load(model_files[0])\n",
    "        return checkpoint\n",
    "```\n",
    "\n",
    "### 2. Inference Optimization\n",
    "```python\n",
    "import torch.jit as jit\n",
    "\n",
    "class OptimizedWorldModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        \n",
    "    @jit.script_method\n",
    "    def scripted_forward(self, obs, action):\n",
    "        \"\"\"JIT-compiled forward pass for faster inference\"\"\"\n",
    "        return self.model(obs, action)\n",
    "    \n",
    "    def to_onnx(self, save_path, sample_input):\n",
    "        \"\"\"Export to ONNX for cross-platform deployment\"\"\"\n",
    "        torch.onnx.export(\n",
    "            self.model,\n",
    "            sample_input,\n",
    "            save_path,\n",
    "            opset_version=11,\n",
    "            input_names=['observation', 'action'],\n",
    "            output_names=['prediction']\n",
    "        )\n",
    "```\n",
    "\n",
    "### 3. Scalable Serving Infrastructure\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import uvicorn\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    observation: list\n",
    "    action: list\n",
    "\n",
    "class WorldModelAPI:\n",
    "    def __init__(self, model_path):\n",
    "        self.model = self.load_optimized_model(model_path)\n",
    "        self.app = FastAPI(title=\"World Model API\")\n",
    "        \n",
    "        @self.app.post(\"/predict\")\n",
    "        async def predict(request: PredictionRequest):\n",
    "            try:\n",
    "                obs = torch.tensor(request.observation).unsqueeze(0)\n",
    "                action = torch.tensor(request.action).unsqueeze(0)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    prediction = self.model(obs, action)\n",
    "                \n",
    "                return {\"prediction\": prediction.tolist()}\n",
    "            except Exception as e:\n",
    "                raise HTTPException(status_code=500, detail=str(e))\n",
    "    \n",
    "    def load_optimized_model(self, path):\n",
    "        \"\"\"Load JIT-compiled model for fast inference\"\"\"\n",
    "        return torch.jit.load(path)\n",
    "    \n",
    "    def serve(self, host=\"0.0.0.0\", port=8000):\n",
    "        uvicorn.run(self.app, host=host, port=port)\n",
    "```\n",
    "\n",
    "## Future Research Directions\n",
    "\n",
    "### 1. Multi-Agent World Models\n",
    "- **Challenge**: Modeling interactions between multiple agents\n",
    "- **Approaches**: Graph neural networks, attention mechanisms for agent communication\n",
    "- **Applications**: Multi-agent reinforcement learning, autonomous vehicle coordination\n",
    "\n",
    "### 2. Continual Learning World Models\n",
    "- **Challenge**: Adapting to changing environments without catastrophic forgetting\n",
    "- **Approaches**: Elastic weight consolidation, progressive neural networks\n",
    "- **Applications**: Long-term autonomy, adaptive robotics\n",
    "\n",
    "### 3. Causal World Models\n",
    "- **Challenge**: Learning causal relationships from observational data\n",
    "- **Approaches**: Causal discovery algorithms, structural equation modeling\n",
    "- **Applications**: Robust decision-making, explainable AI\n",
    "\n",
    "### 4. Quantum World Models\n",
    "- **Challenge**: Leveraging quantum computing for more efficient world modeling\n",
    "- **Approaches**: Quantum machine learning, tensor networks\n",
    "- **Applications**: Large-scale simulation, quantum RL algorithms\n",
    "\n",
    "## Best Practices Summary\n",
    "\n",
    "1. **Start Simple**: Begin with basic world models and gradually add complexity\n",
    "2. **Validate Thoroughly**: Use multiple evaluation metrics and ablation studies\n",
    "3. **Monitor Training**: Implement comprehensive logging and early stopping\n",
    "4. **Optimize Computationally**: Use mixed precision, gradient accumulation, and model parallelism\n",
    "5. **Ensure Reproducibility**: Version models, seed random number generators, document configurations\n",
    "6. **Plan for Deployment**: Consider inference optimization and serving infrastructure from the start\n",
    "7. **Stay Updated**: Follow latest research in model-based RL and world models\n",
    "\n",
    "This implementation provides a solid foundation for advanced model-based reinforcement learning research while maintaining the flexibility to incorporate cutting-edge techniques and deploy in production environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
