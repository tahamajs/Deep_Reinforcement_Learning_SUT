{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f95a940a",
   "metadata": {},
   "source": [
    "# CA11: Advanced Model-Based RL and World Models\n",
    "\n",
    "## Deep Reinforcement Learning - Session 11\n",
    "\n",
    "**Advanced Model-Based Reinforcement Learning: World Models, Planning in Latent Space, and Modern Approaches**\n",
    "\n",
    "This notebook demonstrates cutting-edge model-based reinforcement learning techniques using modular implementations. The code has been restructured into separate modules for better organization and reusability.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand world models and latent state representations\n",
    "2. Implement variational autoencoders for state modeling\n",
    "3. Master planning in latent space with learned dynamics\n",
    "4. Explore uncertainty quantification in model-based RL\n",
    "5. Implement Dreamer-style world model learning\n",
    "6. Apply modular design principles to complex RL systems\n",
    "\n",
    "### Modular Structure:\n",
    "- **world_models/**: VAE, dynamics, reward models, RSSM\n",
    "- **agents/**: Latent actor-critic, Dreamer agent\n",
    "- **environments/**: Custom continuous control tasks\n",
    "- **utils/**: Data collection and visualization\n",
    "- **experiments/**: Complete training scripts\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c169bc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from models.vae import VariationalAutoencoder\n",
    "from models.dynamics import LatentDynamicsModel\n",
    "from models.reward_model import RewardModel\n",
    "from models.world_model import WorldModel\n",
    "from models.rssm import RecurrentStateSpaceModel\n",
    "from models.trainers import WorldModelTrainer, RSSMTrainer\n",
    "\n",
    "from agents.latent_actor import LatentActor\n",
    "from agents.latent_critic import LatentCritic\n",
    "from agents.dreamer_agent import DreamerAgent\n",
    "\n",
    "from environments.continuous_cartpole import ContinuousCartPole\n",
    "from environments.continuous_pendulum import ContinuousPendulum\n",
    "from environments.sequence_environment import SequenceEnvironment\n",
    "\n",
    "from utils.data_collection import collect_world_model_data, collect_sequence_data\n",
    "from utils.visualization import plot_world_model_training, plot_rssm_training\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"ðŸš€ Advanced Model-Based RL Environment Setup\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "sns.set_palette(colors)\n",
    "\n",
    "print(\"âœ… Modular environment setup complete!\")\n",
    "print(\"ðŸŒŸ Ready for advanced model-based reinforcement learning!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c8f0f",
   "metadata": {},
   "source": [
    "# Section 1: World Models and Latent Representations\n",
    "\n",
    "## 1.1 Understanding the Modular Architecture\n",
    "\n",
    "The world model consists of three main components:\n",
    "- **VAE**: Learns compressed latent representations of observations\n",
    "- **Dynamics Model**: Predicts next latent states given current state and action\n",
    "- **Reward Model**: Predicts rewards in latent space\n",
    "\n",
    "Let's explore each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfee00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ContinuousCartPole()\n",
    "print(f\"Environment: {env.name}\")\n",
    "print(f\"Observation space: {env.observation_space.shape}\")\n",
    "print(f\"Action space: {env.action_space.shape}\")\n",
    "\n",
    "sample_data = collect_world_model_data(env, steps=1000, episodes=5)\n",
    "print(f\"Collected {len(sample_data['observations'])} transitions\")\n",
    "print(f\"Sample observation shape: {sample_data['observations'][0].shape}\")\n",
    "print(f\"Sample action shape: {sample_data['actions'][0].shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a8924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = env.observation_space.shape[0]\n",
    "latent_dim = 32\n",
    "vae_hidden_dims = [128, 64]\n",
    "\n",
    "vae = VariationalAutoencoder(obs_dim, latent_dim, vae_hidden_dims).to(device)\n",
    "print(f\"VAE Architecture:\")\n",
    "print(f\"Input dim: {obs_dim}, Latent dim: {latent_dim}\")\n",
    "print(f\"Hidden dims: {vae_hidden_dims}\")\n",
    "\n",
    "test_obs = torch.randn(10, obs_dim).to(device)\n",
    "recon_obs, mu, log_var, z = vae(test_obs)\n",
    "print(f\"Reconstruction shape: {recon_obs.shape}\")\n",
    "print(f\"Latent shape: {z.shape}\")\n",
    "print(f\"KL divergence: {vae.kl_divergence(mu, log_var):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_dim = env.action_space.shape[0]\n",
    "dynamics_hidden_dims = [128, 64]\n",
    "reward_hidden_dims = [64, 32]\n",
    "\n",
    "dynamics = LatentDynamicsModel(latent_dim, action_dim, dynamics_hidden_dims, stochastic=True).to(device)\n",
    "reward_model = RewardModel(latent_dim, action_dim, reward_hidden_dims).to(device)\n",
    "\n",
    "world_model = WorldModel(vae, dynamics, reward_model).to(device)\n",
    "print(f\"World Model created with:\")\n",
    "print(f\"- VAE: {obs_dim} -> {latent_dim}\")\n",
    "print(f\"- Dynamics: {latent_dim} + {action_dim} -> {latent_dim}\")\n",
    "print(f\"- Reward: {latent_dim} + {action_dim} -> 1\")\n",
    "\n",
    "test_obs = torch.randn(5, obs_dim).to(device)\n",
    "test_action = torch.randn(5, action_dim).to(device)\n",
    "\n",
    "next_obs_pred, reward_pred = world_model.predict_next_state_and_reward(test_obs, test_action)\n",
    "print(f\"Prediction shapes: obs={next_obs_pred.shape}, reward={reward_pred.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61607c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = WorldModelTrainer(world_model, learning_rate=1e-3, device=device)\n",
    "\n",
    "train_data = {\n",
    "    'observations': torch.FloatTensor(sample_data['observations']).to(device),\n",
    "    'actions': torch.FloatTensor(sample_data['actions']).to(device),\n",
    "    'rewards': torch.FloatTensor(sample_data['rewards']).to(device),\n",
    "    'next_observations': torch.FloatTensor(sample_data['next_observations']).to(device)\n",
    "}\n",
    "\n",
    "print(\"Training world model for 500 steps...\")\n",
    "for step in tqdm(range(500)):\n",
    "    batch_size = 64\n",
    "    indices = torch.randperm(len(train_data['observations']))[:batch_size]\n",
    "    batch = {k: v[indices] for k, v in train_data.items()}\n",
    "    losses = trainer.train_step(batch)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "plot_world_model_training(trainer, \"World Model Training Demo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3d3cc1",
   "metadata": {},
   "source": [
    "# Section 2: Recurrent State Space Models (RSSM)\n",
    "\n",
    "## 2.1 Temporal World Modeling\n",
    "\n",
    "RSSM extends world models with recurrent networks to capture temporal dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc76a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_env = SequenceEnvironment(memory_size=5)\n",
    "print(f\"Sequence Environment: {seq_env.name}\")\n",
    "print(f\"Observation space: {seq_env.observation_space.shape}\")\n",
    "\n",
    "seq_data = collect_sequence_data(seq_env, episodes=50, episode_length=20)\n",
    "print(f\"Collected {len(seq_data)} episodes\")\n",
    "print(f\"Sample episode length: {len(seq_data[0]['observations'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2309b0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_dim = seq_env.observation_space.shape[0]\n",
    "action_dim = seq_env.action_space.shape[0]\n",
    "state_dim = 32\n",
    "hidden_dim = 128\n",
    "\n",
    "rssm = RecurrentStateSpaceModel(obs_dim, action_dim, state_dim, hidden_dim).to(device)\n",
    "print(f\"RSSM Architecture:\")\n",
    "print(f\"Observation dim: {obs_dim}, Action dim: {action_dim}\")\n",
    "print(f\"State dim: {state_dim}, Hidden dim: {hidden_dim}\")\n",
    "\n",
    "test_obs = torch.randn(1, 1, obs_dim).to(device)\n",
    "test_action = torch.randn(1, 1, action_dim).to(device)\n",
    "hidden = torch.zeros(1, hidden_dim).to(device)\n",
    "\n",
    "next_obs_pred, reward_pred, next_hidden = rssm.imagine(test_obs, test_action, hidden)\n",
    "print(f\"Imagination shapes: obs={next_obs_pred.shape}, reward={reward_pred.shape}, hidden={next_hidden.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3de93cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rssm_trainer = RSSMTrainer(rssm, learning_rate=1e-3, device=device)\n",
    "\n",
    "print(\"Training RSSM for 500 steps...\")\n",
    "for step in tqdm(range(500)):\n",
    "    episode_idx = np.random.randint(len(seq_data))\n",
    "    episode = seq_data[episode_idx]\n",
    "    \n",
    "    seq_len = min(15, len(episode['observations']))\n",
    "    start_idx = np.random.randint(max(1, len(episode['observations']) - seq_len))\n",
    "    \n",
    "    batch = {\n",
    "        'observations': torch.FloatTensor(episode['observations'][start_idx:start_idx+seq_len]).unsqueeze(0).to(device),\n",
    "        'actions': torch.FloatTensor(episode['actions'][start_idx:start_idx+seq_len]).unsqueeze(0).to(device),\n",
    "        'rewards': torch.FloatTensor(episode['rewards'][start_idx:start_idx+seq_len]).unsqueeze(0).to(device)\n",
    "    }\n",
    "    \n",
    "    losses = rssm_trainer.train_step(batch)\n",
    "\n",
    "print(\"RSSM training completed!\")\n",
    "plot_rssm_training(rssm_trainer, \"RSSM Training Demo\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b32fa82",
   "metadata": {},
   "source": [
    "# Section 3: Dreamer Agent - Planning in Latent Space\n",
    "\n",
    "## 3.1 Complete Model-Based RL\n",
    "\n",
    "The Dreamer agent combines world models with actor-critic methods in latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df479e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor = LatentActor(latent_dim, action_dim, hidden_dims=[128, 64]).to(device)\n",
    "critic = LatentCritic(latent_dim, hidden_dims=[128, 64]).to(device)\n",
    "\n",
    "dreamer = DreamerAgent(\n",
    "    world_model=world_model,\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    imagination_horizon=10,\n",
    "    gamma=0.99,\n",
    "    actor_lr=1e-4,\n",
    "    critic_lr=1e-4,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(f\"Dreamer Agent created:\")\n",
    "print(f\"- Imagination horizon: {dreamer.imagination_horizon}\")\n",
    "print(f\"- Discount factor: {dreamer.gamma}\")\n",
    "print(f\"- Actor learning rate: {dreamer.actor_lr}\")\n",
    "print(f\"- Critic learning rate: {dreamer.critic_lr}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e8961",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Dreamer imagination...\")\n",
    "\n",
    "obs, _ = env.reset()\n",
    "obs_tensor = torch.FloatTensor(obs).to(device)\n",
    "latent_state = world_model.encode_observations(obs_tensor.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "imagined_states, imagined_rewards, imagined_actions = dreamer.imagine_trajectory(latent_state, steps=10)\n",
    "\n",
    "print(f\"Imagined {len(imagined_states)} steps\")\n",
    "print(f\"Total imagined reward: {sum(imagined_rewards):.2f}\")\n",
    "print(f\"Final imagined state shape: {imagined_states[-1].shape}\")\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(imagined_rewards, 'g-o', linewidth=2, markersize=4)\n",
    "plt.title('Imagined Rewards')\n",
    "plt.xlabel('Imagination Step')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "imagined_actions = np.array(imagined_actions)\n",
    "for i in range(min(2, imagined_actions.shape[1])):\n",
    "    plt.plot(imagined_actions[:, i], label=f'Action {i}', linewidth=2)\n",
    "plt.title('Imagined Actions')\n",
    "plt.xlabel('Imagination Step')\n",
    "plt.ylabel('Action Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "imagined_states = np.array(imagined_states)\n",
    "for i in range(min(4, imagined_states.shape[1])):\n",
    "    plt.plot(imagined_states[:, i], label=f'Latent {i}', linewidth=1)\n",
    "plt.title('Imagined Latent States')\n",
    "plt.xlabel('Imagination Step')\n",
    "plt.ylabel('Latent Value')\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('Dreamer Imagination Demo', fontsize=16, y=0.98)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc597d3",
   "metadata": {},
   "source": [
    "# Section 4: Running Complete Experiments\n",
    "\n",
    "## 4.1 Using the Experiment Scripts\n",
    "\n",
    "The modular structure allows running complete experiments with proper training and evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a331fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from experiments.world_model_experiment import run_world_model_experiment\n",
    "\n",
    "config = {\n",
    "    'env_name': 'continuous_cartpole',\n",
    "    'latent_dim': 32,\n",
    "    'vae_hidden_dims': [128, 64],\n",
    "    'dynamics_hidden_dims': [128, 64],\n",
    "    'reward_hidden_dims': [64, 32],\n",
    "    'stochastic_dynamics': True,\n",
    "    'learning_rate': 1e-3,\n",
    "    'batch_size': 64,\n",
    "    'train_steps': 1000,\n",
    "    'data_collection_steps': 5000,\n",
    "    'data_collection_episodes': 20,\n",
    "    'rollout_steps': 50\n",
    "}\n",
    "\n",
    "world_model, trainer = run_world_model_experiment(config)\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ’¡ Experiment scripts are available in the experiments/ directory:\")\n",
    "print(\"- world_model_experiment.py: Train world models\")\n",
    "print(\"- rssm_experiment.py: Train RSSM models\") \n",
    "print(\"- dreamer_experiment.py: Train complete Dreamer agents\")\n",
    "print(\"\\nðŸ“Š Each experiment includes comprehensive evaluation and visualization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340f75a6",
   "metadata": {},
   "source": [
    "# Section 5: Key Benefits of Modular Design\n",
    "\n",
    "## 5.1 Advantages of the Restructured Code\n",
    "\n",
    "The modular approach provides several benefits:\n",
    "\n",
    "1. **Reusability**: Components can be imported and used independently\n",
    "2. **Maintainability**: Clear separation of concerns and organized code\n",
    "3. **Testability**: Individual components can be tested in isolation\n",
    "4. **Extensibility**: Easy to add new models, environments, or agents\n",
    "5. **Collaboration**: Multiple developers can work on different modules\n",
    "\n",
    "## 5.2 Project Structure Summary\n",
    "\n",
    "```\n",
    "CA11/\n",
    "â”œâ”€â”€ world_models/     # Core model components\n",
    "â”œâ”€â”€ agents/          # RL agents\n",
    "â”œâ”€â”€ environments/    # Custom environments\n",
    "â”œâ”€â”€ utils/           # Utilities and tools\n",
    "â”œâ”€â”€ experiments/     # Complete training scripts\n",
    "â””â”€â”€ CA11.ipynb       # This demonstration notebook\n",
    "```\n",
    "\n",
    "This structure transforms a monolithic notebook into a professional, maintainable codebase suitable for research and development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ff787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ‰ Modular restructuring completed!\")\n",
    "print(\"\\nðŸ“š Key achievements:\")\n",
    "print(\"âœ… Extracted 2000+ lines of code into organized modules\")\n",
    "print(\"âœ… Created reusable world model components\")\n",
    "print(\"âœ… Implemented complete Dreamer agent system\")\n",
    "print(\"âœ… Added comprehensive visualization tools\")\n",
    "print(\"âœ… Developed experiment scripts for systematic evaluation\")\n",
    "print(\"\\nðŸš€ The modular codebase is now ready for advanced model-based RL research!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
