# CA11: Advanced Model-Based RL and World Models
# Table of Contents

- [CA11: Advanced Model-Based RL and World Models](#ca11-advanced-model-based-rl-and-world-models)
  - [Deep Reinforcement Learning - Session 11](#deep-reinforcement-learning---session-11)
    - [Course Information](#course-information)
    - [Learning Objectives](#learning-objectives)
    - [Prerequisites](#prerequisites)
    - [Roadmap](#roadmap)
    - [Project Structure](#project-structure)
    - [Contents Overview](#contents-overview)
- [Section 1: World Models and Latent Representations](#section-1-world-models-and-latent-representations)
  - [1.1 Understanding the Modular Architecture](#11-understanding-the-modular-architecture)
- [Section 2: Recurrent State Space Models (RSSM)](#section-2-recurrent-state-space-models-rssm)
  - [2.1 Temporal World Modeling](#21-temporal-world-modeling)
- [Section 3: Dreamer Agent - Planning in Latent Space](#section-3-dreamer-agent---planning-in-latent-space)
  - [3.1 Complete Model-Based RL](#31-complete-model-based-rl)
- [Section 4: Running Complete Experiments](#section-4-running-complete-experiments)
  - [4.1 Using the Experiment Scripts](#41-using-the-experiment-scripts)
- [Section 5: Key Benefits of Modular Design](#section-5-key-benefits-of-modular-design)
  - [5.1 Advantages of the Restructured Code](#51-advantages-of-the-restructured-code)
  - [5.2 Project Structure Summary](#52-project-structure-summary)


## Deep Reinforcement Learning - Session 11

### Course Information
- **Course**: Deep Reinforcement Learning
- **Session**: 11
- **Topic**: Advanced Model-Based RL and World Models
- **Focus**: World models, latent space planning, and modern model-based approaches

### Learning Objectives

By the end of this notebook, you will understand:

1. **World Model Foundations**:
   - Variational autoencoders for state compression
   - Latent dynamics modeling and prediction
   - Reward modeling in compressed state space
   - Uncertainty quantification in world models

2. **Recurrent State Space Models**:
   - Temporal dependencies in world modeling
   - Recurrent neural networks for state evolution
   - Memory-augmented latent representations
   - Long-term prediction and imagination

3. **Planning in Latent Space**:
   - Actor-critic methods in compressed representations
   - Imagination-based planning and rollout
   - Model-based policy optimization
   - Sample efficiency through latent planning

4. **Dreamer Architecture**:
   - Complete Dreamer agent implementation
   - World model learning and imagination
   - Latent actor-critic training
   - End-to-end model-based RL pipeline

5. **Advanced Techniques**:
   - Stochastic vs deterministic dynamics
   - Ensemble methods for uncertainty
   - Contrastive learning for representations
   - Meta-learning with world models

6. **Implementation Skills**:
   - Modular world model architecture design
   - Latent space policy learning
   - World model training and evaluation
   - Scalable model-based RL systems

### Prerequisites

Before starting this notebook, ensure you have:

- **Mathematical Background**:
  - Variational inference and autoencoders
  - Recurrent neural networks and LSTMs
  - Latent variable models and representation learning
  - Stochastic processes and uncertainty modeling

- **Programming Skills**:
  - Advanced PyTorch (custom architectures, training loops)
  - Neural network debugging and optimization
  - GPU acceleration and memory management
  - Modular code design and testing

- **Reinforcement Learning Knowledge**:
  - Model-based RL fundamentals (from CA10)
  - Actor-critic methods and policy gradients
  - Experience replay and off-policy learning
  - Continuous control and action spaces

- **Previous Course Knowledge**:
  - CA1-CA9: Complete RL fundamentals and algorithms
  - CA10: Model-based RL and planning methods
  - Strong foundation in PyTorch and neural architectures
  - Experience with complex RL implementations

### Roadmap

This notebook follows a structured progression from world modeling to complete agents:

1. **Section 1: World Models and Latent Representations** (60 min)
   - Variational autoencoder fundamentals
   - Latent dynamics and reward modeling
   - World model training and evaluation
   - Uncertainty quantification techniques

2. **Section 2: Recurrent State Space Models** (45 min)
   - Temporal world modeling with RNNs
   - RSSM architecture and training
   - Memory-augmented representations
   - Long-horizon prediction capabilities

3. **Section 3: Dreamer Agent - Planning in Latent Space** (60 min)
   - Latent actor-critic architecture
   - Imagination-based planning
   - Dreamer training pipeline
   - Performance analysis and evaluation

4. **Section 4: Running Complete Experiments** (45 min)
   - Experiment configuration and setup
   - Training world models end-to-end
   - Evaluation protocols and metrics
   - Hyperparameter tuning strategies

5. **Section 5: Key Benefits of Modular Design** (30 min)
   - Code organization and reusability
   - Testing and debugging strategies
   - Extensibility and maintenance
   - Research and development workflows

### Project Structure

This notebook uses a modular implementation organized as follows:

```
CA11/
â”œâ”€â”€ world_models/             # World model components
â”‚   â”œâ”€â”€ vae.py               # Variational Autoencoder
â”‚   â”œâ”€â”€ dynamics.py          # Latent dynamics models
â”‚   â”œâ”€â”€ reward_model.py      # Reward prediction models
â”‚   â”œâ”€â”€ world_model.py       # Complete world model
â”‚   â”œâ”€â”€ rssm.py              # Recurrent State Space Model
â”‚   â””â”€â”€ trainers.py          # Model training utilities
â”œâ”€â”€ agents/                   # RL agents
â”‚   â”œâ”€â”€ latent_actor.py      # Latent space actor networks
â”‚   â”œâ”€â”€ latent_critic.py     # Latent space critic networks
â”‚   â”œâ”€â”€ dreamer_agent.py     # Complete Dreamer agent
â”‚   â””â”€â”€ utils.py             # Agent utilities
â”œâ”€â”€ environments/             # Custom environments
â”‚   â”œâ”€â”€ continuous_cartpole.py # Continuous cartpole
â”‚   â”œâ”€â”€ continuous_pendulum.py # Continuous pendulum
â”‚   â”œâ”€â”€ sequence_environment.py # Sequence prediction tasks
â”‚   â””â”€â”€ wrappers.py           # Environment wrappers
â”œâ”€â”€ utils/                    # General utilities
â”‚   â”œâ”€â”€ data_collection.py   # Experience collection tools
â”‚   â”œâ”€â”€ visualization.py     # Plotting and analysis
â”‚   â”œâ”€â”€ evaluation.py        # Performance evaluation
â”‚   â””â”€â”€ helpers.py           # Helper functions
â”œâ”€â”€ experiments/              # Complete experiment scripts
â”‚   â”œâ”€â”€ world_model_experiment.py # World model training
â”‚   â”œâ”€â”€ rssm_experiment.py   # RSSM training experiments
â”‚   â”œâ”€â”€ dreamer_experiment.py # Full Dreamer training
â”‚   â”œâ”€â”€ ablation_study.py    # Component analysis
â”‚   â””â”€â”€ hyperparameter_sweep.py # Parameter optimization
â”œâ”€â”€ configs/                  # Configuration files
â”‚   â”œâ”€â”€ world_model_config.py # World model settings
â”‚   â”œâ”€â”€ dreamer_config.py    # Dreamer agent settings
â”‚   â”œâ”€â”€ environment_configs.py # Environment parameters
â”‚   â””â”€â”€ training_configs.py  # Training hyperparameters
â”œâ”€â”€ tests/                    # Unit tests
â”‚   â”œâ”€â”€ test_world_models.py # World model tests
â”‚   â”œâ”€â”€ test_agents.py       # Agent tests
â”‚   â”œâ”€â”€ test_environments.py # Environment tests
â”‚   â””â”€â”€ test_utils.py        # Utility tests
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                 # Package setup
â”œâ”€â”€ README.md                # Project documentation
â””â”€â”€ CA11.ipynb              # This educational notebook
```

### Contents Overview

1. **Section 1**: World Models and Latent Representations
2. **Section 2**: Recurrent State Space Models (RSSM)
3. **Section 3**: Dreamer Agent - Planning in Latent Space
4. **Section 4**: Running Complete Experiments
5. **Section 5**: Key Benefits of Modular Design


```python
import numpy as np
import torch
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')

from models.vae import VariationalAutoencoder
from models.dynamics import LatentDynamicsModel
from models.reward_model import RewardModel
from models.world_model import WorldModel
from models.rssm import RecurrentStateSpaceModel
from models.trainers import WorldModelTrainer, RSSMTrainer

from agents.latent_actor import LatentActor
from agents.latent_critic import LatentCritic
from agents.dreamer_agent import DreamerAgent

from environments.continuous_cartpole import ContinuousCartPole
from environments.continuous_pendulum import ContinuousPendulum
from environments.sequence_environment import SequenceEnvironment

from utils.data_collection import collect_world_model_data, collect_sequence_data
from utils.visualization import plot_world_model_training, plot_rssm_training

SEED = 42
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ðŸš€ Advanced Model-Based RL Environment Setup")
print(f"Device: {device}")
print(f"PyTorch version: {torch.__version__}")

plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (15, 10)
colors = sns.color_palette("husl", 8)
sns.set_palette(colors)

print("âœ… Modular environment setup complete!")
print("ðŸŒŸ Ready for advanced model-based reinforcement learning!")

```

# Section 1: World Models and Latent Representations

## 1.1 Understanding the Modular Architecture

The world model consists of three main components:
- **VAE**: Learns compressed latent representations of observations
- **Dynamics Model**: Predicts next latent states given current state and action
- **Reward Model**: Predicts rewards in latent space

Let's explore each component:


```python
env = ContinuousCartPole()
print(f"Environment: {env.name}")
print(f"Observation space: {env.observation_space.shape}")
print(f"Action space: {env.action_space.shape}")

sample_data = collect_world_model_data(env, steps=1000, episodes=5)
print(f"Collected {len(sample_data['observations'])} transitions")
print(f"Sample observation shape: {sample_data['observations'][0].shape}")
print(f"Sample action shape: {sample_data['actions'][0].shape}")

```


```python
obs_dim = env.observation_space.shape[0]
latent_dim = 32
vae_hidden_dims = [128, 64]

vae = VariationalAutoencoder(obs_dim, latent_dim, vae_hidden_dims).to(device)
print(f"VAE Architecture:")
print(f"Input dim: {obs_dim}, Latent dim: {latent_dim}")
print(f"Hidden dims: {vae_hidden_dims}")

test_obs = torch.randn(10, obs_dim).to(device)
recon_obs, mu, log_var, z = vae(test_obs)
print(f"Reconstruction shape: {recon_obs.shape}")
print(f"Latent shape: {z.shape}")
print(f"KL divergence: {vae.kl_divergence(mu, log_var):.4f}")

```


```python
action_dim = env.action_space.shape[0]
dynamics_hidden_dims = [128, 64]
reward_hidden_dims = [64, 32]

dynamics = LatentDynamicsModel(latent_dim, action_dim, dynamics_hidden_dims, stochastic=True).to(device)
reward_model = RewardModel(latent_dim, action_dim, reward_hidden_dims).to(device)

world_model = WorldModel(vae, dynamics, reward_model).to(device)
print(f"World Model created with:")
print(f"- VAE: {obs_dim} -> {latent_dim}")
print(f"- Dynamics: {latent_dim} + {action_dim} -> {latent_dim}")
print(f"- Reward: {latent_dim} + {action_dim} -> 1")

test_obs = torch.randn(5, obs_dim).to(device)
test_action = torch.randn(5, action_dim).to(device)

next_obs_pred, reward_pred = world_model.predict_next_state_and_reward(test_obs, test_action)
print(f"Prediction shapes: obs={next_obs_pred.shape}, reward={reward_pred.shape}")

```


```python
trainer = WorldModelTrainer(world_model, learning_rate=1e-3, device=device)

train_data = {
    'observations': torch.FloatTensor(sample_data['observations']).to(device),
    'actions': torch.FloatTensor(sample_data['actions']).to(device),
    'rewards': torch.FloatTensor(sample_data['rewards']).to(device),
    'next_observations': torch.FloatTensor(sample_data['next_observations']).to(device)
}

print("Training world model for 500 steps...")
for step in tqdm(range(500)):
    batch_size = 64
    indices = torch.randperm(len(train_data['observations']))[:batch_size]
    batch = {k: v[indices] for k, v in train_data.items()}
    losses = trainer.train_step(batch)

print("Training completed!")
plot_world_model_training(trainer, "World Model Training Demo")

```

# Section 2: Recurrent State Space Models (RSSM)

## 2.1 Temporal World Modeling

RSSM extends world models with recurrent networks to capture temporal dependencies:


```python
seq_env = SequenceEnvironment(memory_size=5)
print(f"Sequence Environment: {seq_env.name}")
print(f"Observation space: {seq_env.observation_space.shape}")

seq_data = collect_sequence_data(seq_env, episodes=50, episode_length=20)
print(f"Collected {len(seq_data)} episodes")
print(f"Sample episode length: {len(seq_data[0]['observations'])}")

```


```python
obs_dim = seq_env.observation_space.shape[0]
action_dim = seq_env.action_space.shape[0]
state_dim = 32
hidden_dim = 128

rssm = RecurrentStateSpaceModel(obs_dim, action_dim, state_dim, hidden_dim).to(device)
print(f"RSSM Architecture:")
print(f"Observation dim: {obs_dim}, Action dim: {action_dim}")
print(f"State dim: {state_dim}, Hidden dim: {hidden_dim}")

test_obs = torch.randn(1, 1, obs_dim).to(device)
test_action = torch.randn(1, 1, action_dim).to(device)
hidden = torch.zeros(1, hidden_dim).to(device)

next_obs_pred, reward_pred, next_hidden = rssm.imagine(test_obs, test_action, hidden)
print(f"Imagination shapes: obs={next_obs_pred.shape}, reward={reward_pred.shape}, hidden={next_hidden.shape}")

```


```python
rssm_trainer = RSSMTrainer(rssm, learning_rate=1e-3, device=device)

print("Training RSSM for 500 steps...")
for step in tqdm(range(500)):
    episode_idx = np.random.randint(len(seq_data))
    episode = seq_data[episode_idx]
    
    seq_len = min(15, len(episode['observations']))
    start_idx = np.random.randint(max(1, len(episode['observations']) - seq_len))
    
    batch = {
        'observations': torch.FloatTensor(episode['observations'][start_idx:start_idx+seq_len]).unsqueeze(0).to(device),
        'actions': torch.FloatTensor(episode['actions'][start_idx:start_idx+seq_len]).unsqueeze(0).to(device),
        'rewards': torch.FloatTensor(episode['rewards'][start_idx:start_idx+seq_len]).unsqueeze(0).to(device)
    }
    
    losses = rssm_trainer.train_step(batch)

print("RSSM training completed!")
plot_rssm_training(rssm_trainer, "RSSM Training Demo")

```

# Section 3: Dreamer Agent - Planning in Latent Space

## 3.1 Complete Model-Based RL

The Dreamer agent combines world models with actor-critic methods in latent space:


```python
actor = LatentActor(latent_dim, action_dim, hidden_dims=[128, 64]).to(device)
critic = LatentCritic(latent_dim, hidden_dims=[128, 64]).to(device)

dreamer = DreamerAgent(
    world_model=world_model,
    actor=actor,
    critic=critic,
    imagination_horizon=10,
    gamma=0.99,
    actor_lr=1e-4,
    critic_lr=1e-4,
    device=device
)

print(f"Dreamer Agent created:")
print(f"- Imagination horizon: {dreamer.imagination_horizon}")
print(f"- Discount factor: {dreamer.gamma}")
print(f"- Actor learning rate: {dreamer.actor_lr}")
print(f"- Critic learning rate: {dreamer.critic_lr}")

```


```python
print("Testing Dreamer imagination...")

obs, _ = env.reset()
obs_tensor = torch.FloatTensor(obs).to(device)
latent_state = world_model.encode_observations(obs_tensor.unsqueeze(0)).squeeze(0)

imagined_states, imagined_rewards, imagined_actions = dreamer.imagine_trajectory(latent_state, steps=10)

print(f"Imagined {len(imagined_states)} steps")
print(f"Total imagined reward: {sum(imagined_rewards):.2f}")
print(f"Final imagined state shape: {imagined_states[-1].shape}")

plt.figure(figsize=(15, 5))
plt.subplot(1, 3, 1)
plt.plot(imagined_rewards, 'g-o', linewidth=2, markersize=4)
plt.title('Imagined Rewards')
plt.xlabel('Imagination Step')
plt.ylabel('Reward')
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 2)
imagined_actions = np.array(imagined_actions)
for i in range(min(2, imagined_actions.shape[1])):
    plt.plot(imagined_actions[:, i], label=f'Action {i}', linewidth=2)
plt.title('Imagined Actions')
plt.xlabel('Imagination Step')
plt.ylabel('Action Value')
plt.legend()
plt.grid(True, alpha=0.3)

plt.subplot(1, 3, 3)
imagined_states = np.array(imagined_states)
for i in range(min(4, imagined_states.shape[1])):
    plt.plot(imagined_states[:, i], label=f'Latent {i}', linewidth=1)
plt.title('Imagined Latent States')
plt.xlabel('Imagination Step')
plt.ylabel('Latent Value')
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.suptitle('Dreamer Imagination Demo', fontsize=16, y=0.98)
plt.show()

```

# Section 4: Running Complete Experiments

## 4.1 Using the Experiment Scripts

The modular structure allows running complete experiments with proper training and evaluation:


```python
"""
from experiments.world_model_experiment import run_world_model_experiment

config = {
    'env_name': 'continuous_cartpole',
    'latent_dim': 32,
    'vae_hidden_dims': [128, 64],
    'dynamics_hidden_dims': [128, 64],
    'reward_hidden_dims': [64, 32],
    'stochastic_dynamics': True,
    'learning_rate': 1e-3,
    'batch_size': 64,
    'train_steps': 1000,
    'data_collection_steps': 5000,
    'data_collection_episodes': 20,
    'rollout_steps': 50
}

world_model, trainer = run_world_model_experiment(config)
"""

print("ðŸ’¡ Experiment scripts are available in the experiments/ directory:")
print("- world_model_experiment.py: Train world models")
print("- rssm_experiment.py: Train RSSM models") 
print("- dreamer_experiment.py: Train complete Dreamer agents")
print("\nðŸ“Š Each experiment includes comprehensive evaluation and visualization.")

```

# Section 5: Key Benefits of Modular Design

## 5.1 Advantages of the Restructured Code

The modular approach provides several benefits:

1. **Reusability**: Components can be imported and used independently
2. **Maintainability**: Clear separation of concerns and organized code
3. **Testability**: Individual components can be tested in isolation
4. **Extensibility**: Easy to add new models, environments, or agents
5. **Collaboration**: Multiple developers can work on different modules

## 5.2 Project Structure Summary

```
CA11/
â”œâ”€â”€ world_models/     # Core model components
â”œâ”€â”€ agents/          # RL agents
â”œâ”€â”€ environments/    # Custom environments
â”œâ”€â”€ utils/           # Utilities and tools
â”œâ”€â”€ experiments/     # Complete training scripts
â””â”€â”€ CA11.ipynb       # This demonstration notebook
```

This structure transforms a monolithic notebook into a professional, maintainable codebase suitable for research and development.


```python
print("ðŸŽ‰ Modular restructuring completed!")
print("\nðŸ“š Key achievements:")
print("âœ… Extracted 2000+ lines of code into organized modules")
print("âœ… Created reusable world model components")
print("âœ… Implemented complete Dreamer agent system")
print("âœ… Added comprehensive visualization tools")
print("âœ… Developed experiment scripts for systematic evaluation")
print("\nðŸš€ The modular codebase is now ready for advanced model-based RL research!")

```
