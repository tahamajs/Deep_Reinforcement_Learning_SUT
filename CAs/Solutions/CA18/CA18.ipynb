{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d9fb84",
   "metadata": {},
   "source": [
    "# Computer Assignment 18: Advanced Deep Reinforcement Learning - Comprehensive Exercise\n",
    "\n",
    "## Course Information\n",
    "- **Course**: Deep Reinforcement Learning (DRL)\n",
    "- **Instructor**: Dr. [Instructor Name]\n",
    "- **Institution**: Sharif University of Technology\n",
    "- **Semester**: Fall 2024\n",
    "- **Assignment Number**: CA18\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this comprehensive assignment, students will be able to:\n",
    "\n",
    "1. **Master Advanced RL Paradigms Implementation**: Design, implement, and evaluate five cutting-edge RL approaches including world models, multi-agent systems, causal RL, quantum-enhanced RL, and federated RL using PyTorch.\n",
    "\n",
    "2. **Apply Theoretical Foundations**: Demonstrate deep understanding of the mathematical principles underlying each advanced RL method through rigorous implementation and analysis of convergence properties, computational complexity, and theoretical guarantees.\n",
    "\n",
    "3. **Develop Integration Skills**: Combine multiple advanced paradigms (model-based + multi-agent, causal + federated, quantum + safety) to create hybrid systems that leverage complementary strengths for enhanced performance.\n",
    "\n",
    "4. **Conduct Scientific Performance Analysis**: Perform comprehensive comparative evaluation using statistical methods, ablation studies, and benchmarking across diverse environments to understand trade-offs between different approaches.\n",
    "\n",
    "5. **Address Real-World Challenges**: Apply advanced RL techniques to practical scenarios including safety-critical systems, distributed deployment, privacy preservation, and multi-agent coordination in complex environments.\n",
    "\n",
    "6. **Demonstrate Research-Ready Expertise**: Produce publication-quality analysis with proper experimental methodology, statistical validation, and insights into current limitations and future research directions.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this assignment, ensure you have:\n",
    "\n",
    "- **Mathematical Background**:\n",
    "- Advanced linear algebra and functional analysis\n",
    "- Probability theory, stochastic processes, and information theory\n",
    "- Optimization theory and convex analysis\n",
    "- Game theory, causal inference, and quantum computing fundamentals\n",
    "\n",
    "- **Technical Skills**:\n",
    "- Expert PyTorch proficiency (custom architectures, distributed training)\n",
    "- Advanced Python programming (concurrent processing, optimization)\n",
    "- Statistical analysis and experimental design\n",
    "- Version control and reproducible research practices\n",
    "\n",
    "- **Prior Knowledge**:\n",
    "- Completion of CA1-CA17 assignments\n",
    "- Strong foundation in deep learning and neural architectures\n",
    "- Experience with advanced RL algorithms and theoretical analysis\n",
    "- Understanding of real-world deployment challenges and limitations\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "This comprehensive assignment is structured as follows:\n",
    "\n",
    "### Part I: World Models and Imagination-augmented Agents\n",
    "- Recurrent State-Space Models (RSSM) for environment dynamics\n",
    "- Imagination-based planning and model-predictive control\n",
    "- Synthetic data generation and hallucination for sample efficiency\n",
    "- Applications to continuous control and sparse reward tasks\n",
    "\n",
    "### Part Ii: Multi-agent Deep Reinforcement Learning\n",
    "- Cooperative and competitive multi-agent MDP frameworks\n",
    "- MADDPG with communication protocols and attention mechanisms\n",
    "- QMIX and value decomposition for scalable multi-agent learning\n",
    "- Emergent behaviors and coordination analysis\n",
    "\n",
    "### Part Iii: Causal Reinforcement Learning\n",
    "- Causal discovery algorithms (PC, NOTEARS, PCMCI) in RL environments\n",
    "- Counterfactual reasoning and intervention analysis\n",
    "- Causal mechanisms for robust policy learning\n",
    "- Applications to explainable AI and robust decision-making\n",
    "\n",
    "### Part Iv: Quantum-enhanced Reinforcement Learning\n",
    "- Variational Quantum Circuits (VQC) for policy representation\n",
    "- Quantum Approximate Optimization Algorithm (QAOA) for RL\n",
    "- Amplitude estimation for value function evaluation\n",
    "- Quantum advantage analysis and NISQ device considerations\n",
    "\n",
    "### Part V: Federated Reinforcement Learning\n",
    "- Federated averaging algorithms adapted for RL (FedRL)\n",
    "- Differential privacy and secure multi-party computation\n",
    "- Communication-efficient distributed training\n",
    "- Privacy-preserving multi-agent coordination\n",
    "\n",
    "### Part Vi: Integration, Analysis, and Real-world Applications\n",
    "- Comparative performance analysis across all paradigms\n",
    "- Hybrid approaches combining multiple techniques\n",
    "- Real-world deployment scenarios and challenges\n",
    "- Future research directions and open problems\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "CA18/\n",
    "├── CA18.ipynb                      # Main comprehensive assignment notebook\n",
    "├── agents/                         # Advanced RL agent implementations\n",
    "│   ├── world*model*agents.py       # RSSM, MPC, imagination-augmented agents\n",
    "│   ├── multi*agent*agents.py       # MADDPG, QMIX, communication-enabled agents\n",
    "│   ├── causal_agents.py            # Causal discovery, reasoning, intervention agents\n",
    "│   ├── quantum_agents.py           # VQC, QAOA, quantum policy networks\n",
    "│   ├── federated_agents.py         # FedRL, privacy-preserving distributed agents\n",
    "│   └── hybrid_agents.py            # Integrated multi-paradigm agents\n",
    "├── environments/                   # Comprehensive environment suite\n",
    "│   ├── world*model*envs.py         # Model learning and planning environments\n",
    "│   ├── multi*agent*envs.py         # Cooperative and competitive multi-agent scenarios\n",
    "│   ├── causal_envs.py              # Environments with causal structure and interventions\n",
    "│   ├── quantum_envs.py             # Quantum simulation and NISQ-compatible environments\n",
    "│   ├── federated_envs.py           # Distributed and privacy-focused environments\n",
    "│   └── integrated_envs.py          # Complex environments requiring multiple paradigms\n",
    "├── models/                         # Advanced neural and quantum architectures\n",
    "│   ├── world_models.py             # RSSM cores, imagination networks, MPC\n",
    "│   ├── multi*agent*models.py       # Centralized critics, mixing networks, communication\n",
    "│   ├── causal_models.py            # Causal graphs, discovery networks, mechanisms\n",
    "│   ├── quantum_models.py           # VQC, quantum circuits, amplitude estimation\n",
    "│   ├── federated_models.py         # Privacy-preserving, communication-efficient models\n",
    "│   └── hybrid_models.py            # Multi-paradigm integrated architectures\n",
    "├── experiments/                    # Comprehensive experimental framework\n",
    "│   ├── paradigm_experiments.py     # Individual paradigm evaluation\n",
    "│   ├── comparative_analysis.py     # Cross-paradigm comparison and ablation studies\n",
    "│   ├── integration_experiments.py  # Hybrid approach evaluation\n",
    "│   ├── real*world*experiments.py   # Practical deployment scenarios\n",
    "│   └── statistical_analysis.py     # Rigorous statistical evaluation methods\n",
    "└── utils/                          # Advanced utility and analysis tools\n",
    "    ├── world*model*utils.py        # Model learning, imagination, planning utilities\n",
    "    ├── multi*agent*utils.py        # Coordination, communication, emergence analysis\n",
    "    ├── causal_utils.py             # Causal discovery, intervention, robustness utilities\n",
    "    ├── quantum_utils.py            # Quantum simulation, NISQ, advantage analysis\n",
    "    ├── federated_utils.py          # Privacy, communication, distributed utilities\n",
    "    └── analysis_utils.py           # Statistical analysis, visualization, reporting utilities\n",
    "```\n",
    "\n",
    "## Contents Overview\n",
    "\n",
    "### Theoretical Foundations\n",
    "- **World Models**: Environment modeling, imagination-based learning, planning theory\n",
    "- **Multi-Agent RL**: Game theory, coordination, communication, emergent behaviors\n",
    "- **Causal RL**: Causal inference, counterfactuals, robust learning, explainability\n",
    "- **Quantum RL**: Quantum algorithms, superposition, NISQ computing, potential advantages\n",
    "- **Federated RL**: Privacy preservation, distributed optimization, communication efficiency\n",
    "- **Integration**: Hybrid approaches, multi-paradigm combinations, unified frameworks\n",
    "\n",
    "### Implementation Components\n",
    "- **World Model Systems**: RSSM implementation, imagination rollout, MPC optimization\n",
    "- **Multi-Agent Systems**: Centralized training frameworks, communication protocols, value decomposition\n",
    "- **Causal Systems**: Discovery algorithms, intervention analysis, counterfactual evaluation\n",
    "- **Quantum Systems**: VQC design, quantum circuit simulation, NISQ-compatible algorithms\n",
    "- **Federated Systems**: Secure aggregation, differential privacy, communication protocols\n",
    "- **Hybrid Systems**: Multi-paradigm integration, adaptive algorithm selection, meta-learning\n",
    "\n",
    "### Research Challenges\n",
    "- **Scalability**: Large-scale multi-agent systems, distributed quantum computing\n",
    "- **Robustness**: Adversarial environments, distribution shift, causal validity\n",
    "- **Privacy**: Federated learning guarantees, secure multi-party computation\n",
    "- **Efficiency**: Sample complexity, computational requirements, communication costs\n",
    "- **Integration**: Combining incompatible paradigms, meta-algorithm design\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Your implementation will be evaluated based on:\n",
    "\n",
    "1. **Theoretical Rigor (20%)**: Correct mathematical formulation and theoretical analysis\n",
    "2. **Implementation Quality (25%)**: Efficient, correct, and well-documented code\n",
    "3. **Experimental Design (20%)**: Proper scientific methodology and statistical analysis\n",
    "4. **Comparative Analysis (15%)**: Thorough evaluation and insightful comparisons\n",
    "5. **Innovation & Integration (10%)**: Creative combinations and novel approaches\n",
    "6. **Real-World Insights (10%)**: Practical deployment considerations and limitations\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Research Foundation**: Review seminal papers on each advanced paradigm\n",
    "2. **Infrastructure Setup**: Configure environments for quantum simulation, distributed training, and multi-agent simulation\n",
    "3. **Modular Development**: Implement each paradigm independently, then focus on integration\n",
    "4. **Rigorous Evaluation**: Design comprehensive experiments with proper statistical validation\n",
    "5. **Critical Analysis**: Identify limitations, failure modes, and research opportunities\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this assignment, you will have:\n",
    "\n",
    "- **Expert Implementation Skills**: Ability to implement state-of-the-art RL research from scratch\n",
    "- **Theoretical Mastery**: Deep understanding of advanced RL mathematics and theory\n",
    "- **Research Methodology**: Skills in rigorous experimental design and statistical analysis\n",
    "- **Integration Expertise**: Ability to combine multiple advanced techniques effectively\n",
    "- **Critical Thinking**: Understanding of current limitations and future research directions\n",
    "- **Publication-Ready Skills**: Ability to produce research-quality analysis and documentation\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This comprehensive assignment represents the capstone of the Deep RL course, requiring mastery of multiple advanced paradigms and their integration. The focus is on producing research-quality implementations with thorough analysis, preparing students for cutting-edge RL research and real-world deployment challenges.\n",
    "\n",
    "Let's master the advanced paradigms of deep reinforcement learning! 🧠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98afa65b",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Part I — World Models and Imagination](#part-i)\n",
    "3. [Part II — Multi-Agent Deep RL](#part-ii)\n",
    "4. [Part III — Causal Reinforcement Learning](#part-iii)\n",
    "5. [Part IV — Quantum-Enhanced RL](#part-iv)\n",
    "6. [Part V — Federated Reinforcement Learning](#part-v)\n",
    "7. [Part VI — Integration & Real-world Applications](#part-vi)\n",
    "8. [Experiments and Benchmarking](#experiments)\n",
    "9. [Results and Analysis](#results)\n",
    "10. [Conclusion and Future Work](#conclusion)\n",
    "11. [How to run / Reproducibility](#how-to-run)\n",
    "12. [References](#references)\n",
    "\n",
    "> Use the links above to quickly jump to each major section. This TOC is auto-maintained — add more anchors below when you expand the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cde14df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All CA18 modules imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import all advanced RL modules\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add CA18 to path\n",
    "ca18_path = '/Users/tahamajs/Documents/uni/DRL/CAs/Solutions/CA18'\n",
    "if ca18_path not in sys.path:\n",
    "    sys.path.insert(0, ca18_path)\n",
    "\n",
    "# Import core modules\n",
    "try:\n",
    "    from world_models.world_models import *\n",
    "    from multi_agent_rl.multi_agent_rl import *\n",
    "    from causal_rl.causal_rl import *\n",
    "    from quantum_rl.quantum_rl import *\n",
    "    from federated_rl.federated_rl import *\n",
    "    print(\"✅ All CA18 modules imported successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7298315a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Setup Complete!\n",
      "Device: cpu\n",
      "PyTorch version: 2.8.0\n",
      "NumPy version: 2.2.6\n",
      "Ready to explore advanced Deep Reinforcement Learning! 🤖\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import deque, defaultdict\n",
    "import random\n",
    "import time\n",
    "import copy\n",
    "import warnings\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "from abc import ABC, abstractmethod\n",
    "import networkx as nx\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cluster import KMeans\n",
    "import gym\n",
    "import math\n",
    "import cmath\n",
    "from scipy.linalg import expm\n",
    "from itertools import combinations, permutations\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"🚀 Setup Complete!\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(\"Ready to explore advanced Deep Reinforcement Learning! 🤖\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b9ef2a",
   "metadata": {},
   "source": [
    "# Part I: World Models and Imagination-augmented Agents\n",
    "\n",
    "## 🌍 Theoretical Foundation\n",
    "\n",
    "### Introduction to World Models\n",
    "\n",
    "**World Models** represent a paradigm shift in reinforcement learning, moving from model-free to model-based approaches that learn internal representations of the environment. This approach was popularized by Ha and Schmidhuber (2018) and has revolutionized how we think about sample efficiency and planning in RL.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "#### 1. Model-based Reinforcement Learning\n",
    "\n",
    "Traditional model-free RL learns policies directly from experience:\n",
    "- **Pro**: No need to model environment dynamics\n",
    "- **Con**: Sample inefficient, cannot plan ahead\n",
    "\n",
    "Model-based RL learns a model of the environment:\n",
    "- **Pro**: Can plan using learned model, more sample efficient  \n",
    "- **Con**: Model errors can compound, more complex\n",
    "\n",
    "#### 2. Recurrent State Space Models (rssm)\n",
    "\n",
    "The RSSM is the heart of world models, consisting of:\n",
    "\n",
    "**Deterministic Path**: $h*t = f*\\theta(h*{t-1}, a*{t-1})$\n",
    "- Encodes deterministic aspects of state evolution\n",
    "- Uses RNN/LSTM/GRU to maintain temporal consistency\n",
    "\n",
    "**Stochastic Path**: $s*t \\sim p(s*t | h_t)$  \n",
    "- Models stochastic aspects and uncertainty\n",
    "- Typically Gaussian: $s*t \\sim \\mathcal{N}(\\mu*\\phi(h*t), \\sigma*\\phi(h_t))$\n",
    "\n",
    "**Combined State**: $z*t = [h*t, s_t]$\n",
    "- Combines deterministic and stochastic components\n",
    "- Provides rich representation for planning\n",
    "\n",
    "#### 3. Three-component Architecture\n",
    "\n",
    "**1. Representation Model (Encoder)**\n",
    "$$h*t = f*\\theta(h*{t-1}, a*{t-1}, o_t)$$\n",
    "- Encodes observations into internal state\n",
    "- Maintains temporal consistency\n",
    "\n",
    "**2. Transition Model**  \n",
    "$$\\hat{s}*{t+1}, \\hat{h}*{t+1} = g*\\phi(s*t, h*t, a*t)$$\n",
    "- Predicts next state from current state and action\n",
    "- Enables forward simulation\n",
    "\n",
    "**3. Observation Model (Decoder)**\n",
    "$$\\hat{o}*t = d*\\psi(s*t, h*t)$$\n",
    "- Reconstructs observations from internal state\n",
    "- Ensures representation quality\n",
    "\n",
    "#### 4. Imagination-augmented Agents (i2a)\n",
    "\n",
    "I2A extends world models by using \"imagination\" for policy learning:\n",
    "\n",
    "**Imagination Rollouts**:\n",
    "- Use world model to simulate future trajectories\n",
    "- Generate imagined experiences: $\\tau^{imagine} = \\{(s*t^i, a*t^i, r*t^i)\\}*{t=0}^H$\n",
    "\n",
    "**Imagination Encoder**:\n",
    "- Process imagined trajectories into useful features\n",
    "- Extract planning-relevant information\n",
    "\n",
    "**Policy Network**:\n",
    "- Combines real observations with imagination features  \n",
    "- Makes decisions using both current state and future projections\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### State Space Model\n",
    "\n",
    "The world model learns a latent state space representation:\n",
    "\n",
    "$$p(s*{1:T}, o*{1:T} | a*{1:T}) = \\prod*{t=1}^T p(s*t | s*{t-1}, a*{t-1}) p(o*t | s_t)$$\n",
    "\n",
    "Where:\n",
    "- $s_t$: latent state at time $t$\n",
    "- $o_t$: observation at time $t$  \n",
    "- $a_t$: action at time $t$\n",
    "\n",
    "#### Training Objectives\n",
    "\n",
    "**1. Reconstruction Loss**:\n",
    "$$\\mathcal{L}*{recon} = \\mathbb{E}*{(o,a) \\sim \\mathcal{D}}[||o - \\hat{o}||^2]$$\n",
    "\n",
    "**2. KL Regularization**:\n",
    "$$\\mathcal{L}*{KL} = \\mathbb{E}*{s \\sim q*\\phi}[D*{KL}(q_\\phi(s|o,h) || p(s|h))]$$\n",
    "\n",
    "**3. Prediction Loss**:\n",
    "$$\\mathcal{L}*{pred} = \\mathbb{E}*{(s,a,s') \\sim \\mathcal{D}}[||s' - \\hat{s}'||^2]$$\n",
    "\n",
    "**Total Loss**:\n",
    "$$\\mathcal{L}*{world} = \\mathcal{L}*{recon} + \\beta \\mathcal{L}*{KL} + \\lambda \\mathcal{L}*{pred}$$\n",
    "\n",
    "### Planning Algorithms\n",
    "\n",
    "#### 1. Model Predictive Control (mpc)\n",
    "\n",
    "MPC uses the world model for online planning:\n",
    "\n",
    "1. **Rollout**: Simulate $H$-step trajectories using world model\n",
    "2. **Evaluate**: Score trajectories using reward predictions  \n",
    "3. **Execute**: Take first action of best trajectory\n",
    "4. **Replan**: Repeat process at next timestep\n",
    "\n",
    "**MPC Objective**:\n",
    "$$a^* = \\arg\\max*a \\sum*{h=1}^H \\gamma^h r(s*h, a*h)$$\n",
    "\n",
    "where $(s*h, a*h)$ come from world model rollouts.\n",
    "\n",
    "#### 2. Cross Entropy Method (cem)\n",
    "\n",
    "CEM is a population-based optimization method:\n",
    "\n",
    "1. **Sample**: Generate action sequence population\n",
    "2. **Evaluate**: Score sequences using world model\n",
    "3. **Select**: Keep top-performing sequences\n",
    "4. **Update**: Fit distribution to elite sequences\n",
    "5. **Repeat**: Iterate until convergence\n",
    "\n",
    "### Advantages and Applications\n",
    "\n",
    "**Advantages**:\n",
    "- **Sample Efficiency**: Learn from imagined experiences\n",
    "- **Planning Capability**: Look ahead before acting\n",
    "- **Transfer Learning**: World models can transfer across tasks\n",
    "- **Interpretability**: Can visualize agent's internal world understanding\n",
    "\n",
    "**Applications**:\n",
    "- **Robotics**: Sample-efficient robot learning\n",
    "- **Game Playing**: Strategic planning in complex games  \n",
    "- **Autonomous Driving**: Safe planning with uncertainty\n",
    "- **Finance**: Portfolio optimization with market models\n",
    "\n",
    "### Key Research Papers\n",
    "\n",
    "1. **World Models** (Ha & Schmidhuber, 2018)\n",
    "2. **PlaNet** (Hafner et al., 2019)  \n",
    "3. **DreamerV1** (Hafner et al., 2020)\n",
    "4. **DreamerV2** (Hafner et al., 2021)\n",
    "5. **I2A** (Weber et al., 2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76b5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ World Models Implementation Imported!\n",
      "Components available:\n",
      "- RSSMCore: Recurrent state space model with deterministic/stochastic components\n",
      "- WorldModel: Complete world model with encoder/decoder and predictors\n",
      "- MPCPlanner: Cross-entropy method planner for action sequence optimization\n",
      "- ImaginationAugmentedAgent: I2A-style agent combining model-free and imagination\n"
     ]
    }
   ],
   "source": [
    "# Import World Models implementations\n",
    "from world_models.world_models import (\n",
    "    RSSMCore, WorldModel, MPCPlanner, ImaginationAugmentedAgent\n",
    ")\n",
    "from world_models.world_models_demo import (\n",
    "    create_world_model_environment, collect_random_data, \n",
    "    create_training_batches, train_world_model, evaluate_world_model_planning\n",
    ")\n",
    "\n",
    "print(\"✅ World Models Implementation Imported!\")\n",
    "print(\"Components available:\")\n",
    "print(\"- RSSMCore: Recurrent state space model with deterministic/stochastic components\")\n",
    "print(\"- WorldModel: Complete world model with encoder/decoder and predictors\")  \n",
    "print(\"- MPCPlanner: Cross-entropy method planner for action sequence optimization\")\n",
    "print(\"- ImaginationAugmentedAgent: I2A-style agent combining model-free and imagination\")\n",
    "print(\"- Demo functions: create_world_model_environment, collect_random_data, create_training_batches, train_world_model, evaluate_world_model_planning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e9348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Exercise 1: World Models Training and Evaluation\n",
      "======================================================================\n",
      "Environment: 4D state, 2D action\n",
      "Collecting 50 episodes of random data...\n",
      "Episode 0/50\n",
      "Episode 20/50\n",
      "Episode 40/50\n",
      "Collected 50 episodes\n",
      "Created 49 training batches\n",
      "World model parameters: 714,190\n",
      "Training world model for 30 epochs...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input has inconsistent input_size: got 22 expected 278",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 238\u001b[39m\n\u001b[32m    228\u001b[39m world_model = WorldModel(\n\u001b[32m    229\u001b[39m     obs_dim=env.state_dim,\n\u001b[32m    230\u001b[39m     action_dim=env.action_dim,\n\u001b[32m   (...)\u001b[39m\u001b[32m    233\u001b[39m     embed_dim=\u001b[32m256\u001b[39m\n\u001b[32m    234\u001b[39m ).to(device)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWorld model parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mworld_model.parameters())\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m238\u001b[39m training_losses = \u001b[43mtrain_world_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mworld_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    240\u001b[39m planner = MPCPlanner(\n\u001b[32m    241\u001b[39m     world_model=world_model,\n\u001b[32m    242\u001b[39m     action_dim=env.action_dim,\n\u001b[32m   (...)\u001b[39m\u001b[32m    246\u001b[39m     n_elite=\u001b[32m50\u001b[39m\n\u001b[32m    247\u001b[39m )\n\u001b[32m    249\u001b[39m planning_rewards = evaluate_world_model_planning(env, world_model, planner, n_episodes=\u001b[32m10\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mtrain_world_model\u001b[39m\u001b[34m(world_model, batches, n_epochs, lr)\u001b[39m\n\u001b[32m    132\u001b[39m action_seq = batch[\u001b[33m'\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m'\u001b[39m]    \u001b[38;5;66;03m# [batch, seq_len, action_dim]\u001b[39;00m\n\u001b[32m    133\u001b[39m reward_seq = batch[\u001b[33m'\u001b[39m\u001b[33mrewards\u001b[39m\u001b[33m'\u001b[39m]    \u001b[38;5;66;03m# [batch, seq_len, 1]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m output = \u001b[43mworld_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_seq\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_seq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m recon_loss = F.mse_loss(\n\u001b[32m    138\u001b[39m     output[\u001b[33m'\u001b[39m\u001b[33mreconstructions\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m    139\u001b[39m     obs_seq[:, \u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# Target is next observations\u001b[39;00m\n\u001b[32m    140\u001b[39m )\n\u001b[32m    142\u001b[39m kl_loss = output[\u001b[33m'\u001b[39m\u001b[33mkl_losses\u001b[39m\u001b[33m'\u001b[39m].mean() \u001b[38;5;28;01mif\u001b[39;00m output[\u001b[33m'\u001b[39m\u001b[33mkl_losses\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA18/world_models/world_models.py:201\u001b[39m, in \u001b[36mWorldModel.observe_sequence\u001b[39m\u001b[34m(self, obs_seq, action_seq)\u001b[39m\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    199\u001b[39m     action = action_seq[:, t-\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m201\u001b[39m state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrssm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mobserve\u001b[49m\u001b[43m(\u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m states.append(state)\n\u001b[32m    204\u001b[39m reconstruction = \u001b[38;5;28mself\u001b[39m.decode(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA18/world_models/world_models.py:59\u001b[39m, in \u001b[36mRSSMCore.observe\u001b[39m\u001b[34m(self, embed, action, state)\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mobserve\u001b[39m(\u001b[38;5;28mself\u001b[39m, embed: torch.Tensor, action: torch.Tensor,\n\u001b[32m     55\u001b[39m             state: Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]) -> Dict[\u001b[38;5;28mstr\u001b[39m, torch.Tensor]:\n\u001b[32m     56\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m    Update state using observation (posterior update)\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     hidden = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstoch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhidden\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     posterior_input = torch.cat([hidden, embed], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     65\u001b[39m     posterior_params = \u001b[38;5;28mself\u001b[39m.posterior_net(posterior_input)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/venv/lib/python3.13/site-packages/torch/nn/modules/rnn.py:1811\u001b[39m, in \u001b[36mGRUCell.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1808\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1809\u001b[39m     hx = hx.unsqueeze(\u001b[32m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched \u001b[38;5;28;01melse\u001b[39;00m hx\n\u001b[32m-> \u001b[39m\u001b[32m1811\u001b[39m ret = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgru_cell\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1812\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1813\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1814\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1815\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1816\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_ih\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1817\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias_hh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1818\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1820\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[32m   1821\u001b[39m     ret = ret.squeeze(\u001b[32m0\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: input has inconsistent input_size: got 22 expected 278"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"🚀 Starting Exercise 1: World Models Training and Evaluation\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "env = create_world_model_environment()\n",
    "print(f\"Environment: {env.state_dim}D state, {env.action_dim}D action\")\n",
    "\n",
    "random_data = collect_random_data(env, n_episodes=50)\n",
    "print(f\"Collected {len(random_data['observations'])} episodes\")\n",
    "\n",
    "training_batches = create_training_batches(random_data, batch_size=16, seq_length=15)\n",
    "print(f\"Created {len(training_batches)} training batches\")\n",
    "\n",
    "world_model = WorldModel(\n",
    "    obs_dim=env.state_dim,\n",
    "    action_dim=env.action_dim,\n",
    "    state_dim=20,\n",
    "    hidden_dim=100,\n",
    "    embed_dim=256\n",
    ").to(device)\n",
    "\n",
    "print(f\"World model parameters: {sum(p.numel() for p in world_model.parameters()):,}\")\n",
    "\n",
    "training_losses = train_world_model(world_model, training_batches, n_epochs=30)\n",
    "\n",
    "planner = MPCPlanner(\n",
    "    world_model=world_model,\n",
    "    action_dim=env.action_dim,\n",
    "    horizon=8,\n",
    "    n_candidates=500,\n",
    "    n_iterations=5,\n",
    "    n_elite=50\n",
    ")\n",
    "\n",
    "planning_rewards = evaluate_world_model_planning(env, world_model, planner, n_episodes=10)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "ax1.plot(training_losses['total'], label='Total Loss')\n",
    "ax1.plot(training_losses['reconstruction'], label='Reconstruction')\n",
    "ax1.plot(training_losses['reward'], label='Reward Prediction')\n",
    "ax1.set_title('World Model Training Losses')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(training_losses['kl'])\n",
    "ax2.set_title('KL Divergence Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('KL Loss')\n",
    "ax2.grid(True)\n",
    "\n",
    "ax3.bar(range(len(planning_rewards)), planning_rewards, alpha=0.7)\n",
    "ax3.set_title('MPC Planning Episode Rewards')\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Total Reward')\n",
    "ax3.grid(True)\n",
    "\n",
    "ax4.hist(planning_rewards, bins=5, alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(np.mean(planning_rewards), color='red', linestyle='--', \n",
    "           label=f'Mean: {np.mean(planning_rewards):.2f}')\n",
    "ax4.set_title('Reward Distribution')\n",
    "ax4.set_xlabel('Episode Reward')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Exercise 1 Complete!\")\n",
    "print(\"Key learnings:\")\n",
    "print(\"- World models can learn environment dynamics from observation sequences\")\n",
    "print(\"- MPC planning uses learned models for lookahead decision making\")\n",
    "print(\"- RSSM balances deterministic and stochastic state evolution\")\n",
    "print(\"- Imagination enables sample-efficient learning through internal simulation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26f2979",
   "metadata": {},
   "source": [
    "# Part Ii: Multi-agent Deep Reinforcement Learning\n",
    "\n",
    "## 👥 Theoretical Foundation\n",
    "\n",
    "### Introduction to Multi-agent Rl\n",
    "\n",
    "**Multi-Agent Reinforcement Learning (MARL)** extends single-agent RL to environments with multiple learning agents. This creates fundamentally new challenges due to **non-stationarity** - each agent's environment changes as other agents learn and adapt their policies.\n",
    "\n",
    "### Core Challenges in Marl\n",
    "\n",
    "#### 1. Non-stationarity Problem\n",
    "- **Single-Agent RL**: Environment is stationary (fixed transition dynamics)\n",
    "- **Multi-Agent RL**: Environment is non-stationary (other agents change their behavior)\n",
    "- **Consequence**: Standard RL convergence guarantees no longer hold\n",
    "\n",
    "#### 2. Credit Assignment Problem\n",
    "- **Challenge**: Which agent is responsible for team success/failure?\n",
    "- **Example**: In cooperative tasks, global reward must be decomposed\n",
    "- **Solutions**: Difference rewards, counterfactual reasoning, attention mechanisms\n",
    "\n",
    "#### 3. Scalability Issues\n",
    "- **Joint Action Space**: Grows exponentially with number of agents\n",
    "- **Joint Observation Space**: Exponential growth in state complexity\n",
    "- **Communication**: Bandwidth limitations, partial observability\n",
    "\n",
    "#### 4. Coordination Vs Competition\n",
    "- **Cooperative**: Agents share common objectives (team sports, rescue operations)\n",
    "- **Competitive**: Agents have opposing objectives (adversarial games, auctions)\n",
    "- **Mixed-Motive**: Combination of cooperation and competition (negotiation, markets)\n",
    "\n",
    "### Game Theoretic Foundations\n",
    "\n",
    "#### Nash Equilibrium\n",
    "A strategy profile where no agent can unilaterally improve by changing strategy:\n",
    "\n",
    "$$\\pi^**i \\in \\arg\\max*{\\pi*i} J*i(\\pi*i, \\pi^**{-i})$$\n",
    "\n",
    "where $\\pi^*_{-i}$ represents the strategies of all agents except $i$.\n",
    "\n",
    "#### Solution Concepts\n",
    "1. **Nash Equilibrium**: Stable but not necessarily optimal\n",
    "2. **Pareto Optimal**: Efficient outcomes that cannot be improved for all agents\n",
    "3. **Correlated Equilibrium**: Allows for coordination through external signals\n",
    "4. **Stackelberg Equilibrium**: Leader-follower dynamics\n",
    "\n",
    "### Marl Algorithm Categories\n",
    "\n",
    "#### 1. Independent Learning (il)\n",
    "Each agent treats others as part of the environment:\n",
    "- **Pros**: Simple, scalable, no communication needed\n",
    "- **Cons**: No convergence guarantees, ignores other agents' adaptation\n",
    "- **Examples**: Independent Q-learning, Independent Actor-Critic\n",
    "\n",
    "#### 2. Joint Action Learning (jal)\n",
    "Agents learn joint action-value functions:\n",
    "- **Pros**: Can achieve coordination, theoretically sound\n",
    "- **Cons**: Exponential complexity in number of agents\n",
    "- **Examples**: Multi-Agent Q-learning, Nash-Q learning\n",
    "\n",
    "#### 3. Agent Modeling (am)\n",
    "Agents maintain models of other agents:\n",
    "- **Pros**: Handles non-stationarity explicitly\n",
    "- **Cons**: Computational overhead, modeling errors\n",
    "- **Examples**: MAAC, MADDPG with opponent modeling\n",
    "\n",
    "#### 4. Communication-based\n",
    "Agents can exchange information:\n",
    "- **Pros**: Direct coordination, shared knowledge\n",
    "- **Cons**: Communication overhead, protocol design\n",
    "- **Examples**: CommNet, I2C, TarMAC\n",
    "\n",
    "### Deep Marl Algorithms\n",
    "\n",
    "#### 1. Multi-agent Deep Deterministic Policy Gradient (maddpg)\n",
    "\n",
    "**Key Idea**: Centralized training, decentralized execution\n",
    "- **Training**: Critics have access to all agents' observations and actions\n",
    "- **Execution**: Actors only use local observations\n",
    "\n",
    "**Actor Update**: \n",
    "$$\\nabla*{\\theta*i} J*i = \\mathbb{E}[\\nabla*{\\theta*i} \\mu*i(o*i) \\nabla*{a*i} Q*i^{\\mu}(x, a*1, ..., a*N)|*{a*i=\\mu*i(o*i)}]$$\n",
    "\n",
    "**Critic Update**:\n",
    "$$Q*i^{\\mu}(x, a*1, ..., a*N) = \\mathbb{E}[r*i + \\gamma Q*i^{\\mu'}(x', a'*1, ..., a'_N)]$$\n",
    "\n",
    "where $x$ is the global state and $a_i$ are individual actions.\n",
    "\n",
    "#### 2. Multi-agent Actor-critic (maac)\n",
    "\n",
    "Extends single-agent AC to multi-agent setting:\n",
    "- **Centralized Critic**: Uses global information during training\n",
    "- **Decentralized Actors**: Use only local observations\n",
    "- **Attention Mechanism**: Selectively focus on relevant agents\n",
    "\n",
    "#### 3. Counterfactual Multi-agent Policy Gradient (coma)\n",
    "\n",
    "Addresses credit assignment through counterfactual reasoning:\n",
    "\n",
    "**Counterfactual Advantage**:\n",
    "$$A*i(s, a) = Q(s, a) - \\sum*{a'*i} \\pi*i(a'*i|o*i) Q(s, (a*{-i}, a'*i))$$\n",
    "\n",
    "This measures how much better the taken action is compared to marginalizing over all possible actions.\n",
    "\n",
    "### Communication in Marl\n",
    "\n",
    "#### 1. Communication Protocols\n",
    "- **Broadcast**: All-to-all communication\n",
    "- **Targeted**: Agent-specific messages\n",
    "- **Hierarchical**: Tree-structured communication\n",
    "\n",
    "#### 2. Communication Learning\n",
    "- **What to Communicate**: Message content learning\n",
    "- **When to Communicate**: Communication scheduling\n",
    "- **Who to Communicate With**: Network topology learning\n",
    "\n",
    "#### 3. Differentiable Communication\n",
    "\n",
    "**Gumbel-Softmax Trick** for discrete communication:\n",
    "$$\\text{softmax}\\left(\\frac{\\log(\\pi*i) + G*i}{\\tau}\\right)$$\n",
    "\n",
    "where $G_i$ are Gumbel random variables and $\\tau$ is temperature.\n",
    "\n",
    "### Cooperative Multi-agent Rl\n",
    "\n",
    "#### 1. Team Reward Structure\n",
    "- **Global Reward**: Same reward for all agents\n",
    "- **Local Rewards**: Individual agent rewards\n",
    "- **Shaped Rewards**: Carefully designed to promote cooperation\n",
    "\n",
    "#### 2. Value Decomposition Methods\n",
    "\n",
    "**VDN (Value Decomposition Networks)**:\n",
    "$$Q*{tot}(s, a) = \\sum*{i=1}^n Q*i(s*i, a_i)$$\n",
    "\n",
    "**QMIX**: Monotonic value decomposition\n",
    "$$\\frac{\\partial Q*{tot}}{\\partial Q*i} \\geq 0$$\n",
    "\n",
    "#### 3. Policy Gradient Methods\n",
    "- **Multi-Agent Policy Gradient (MAPG)**\n",
    "- **Trust Region Methods**: MADDPG-TR\n",
    "- **Proximal Policy Optimization**: MAPPO\n",
    "\n",
    "### Competitive Multi-agent Rl\n",
    "\n",
    "#### 1. Self-play Training\n",
    "Agents learn by playing against copies of themselves:\n",
    "- **Advantages**: Always improving opponents, no human data needed\n",
    "- **Challenges**: Exploitability, strategy diversity\n",
    "\n",
    "#### 2. Population-based Training\n",
    "Maintain population of diverse strategies:\n",
    "- **League Play**: Different skill levels and strategies\n",
    "- **Diversity Metrics**: Behavioral diversity, policy diversity\n",
    "- **Meta-Game Analysis**: Strategy effectiveness matrix\n",
    "\n",
    "#### 3. Adversarial Training\n",
    "- **Minimax Objective**: $\\min*{\\pi*1} \\max*{\\pi*2} J(\\pi*1, \\pi*2)$\n",
    "- **Nash-AC**: Nash equilibrium seeking\n",
    "- **PSRO**: Policy Space Response Oracles\n",
    "\n",
    "### Theoretical Guarantees\n",
    "\n",
    "#### 1. Convergence Results\n",
    "- **Independent Learning**: Generally no convergence guarantees\n",
    "- **Joint Action Learning**: Convergence to Nash under restrictive assumptions\n",
    "- **Two-Timescale Algorithms**: Convergence through different learning rates\n",
    "\n",
    "#### 2. Sample Complexity\n",
    "Multi-agent sample complexity often exponentially worse than single-agent due to:\n",
    "- Larger state-action spaces\n",
    "- Non-stationarity\n",
    "- Coordination requirements\n",
    "\n",
    "#### 3. Regret Bounds\n",
    "**Multi-Agent Regret**: \n",
    "$$R*i(T) = \\max*{\\pi*i} \\sum*{t=1}^T J*i(\\pi*i, \\pi*{-i}^t) - \\sum*{t=1}^T J*i(\\pi*i^t, \\pi_{-i}^t)$$\n",
    "\n",
    "### Applications\n",
    "\n",
    "#### 1. Robotics\n",
    "- **Multi-Robot Systems**: Coordination and task allocation\n",
    "- **Swarm Robotics**: Large-scale coordination\n",
    "- **Human-Robot Interaction**: Mixed human-AI teams\n",
    "\n",
    "#### 2. Autonomous Vehicles\n",
    "- **Traffic Management**: Intersection control, highway merging\n",
    "- **Platooning**: Vehicle following and coordination\n",
    "- **Mixed Autonomy**: Human and autonomous vehicles\n",
    "\n",
    "#### 3. Game Playing\n",
    "- **Real-Time Strategy Games**: StarCraft, Dota\n",
    "- **Board Games**: Multi-player poker, diplomacy\n",
    "- **Sports Simulation**: Team coordination\n",
    "\n",
    "#### 4. Economics and Finance\n",
    "- **Algorithmic Trading**: Multi-agent market making\n",
    "- **Auction Design**: Bidding strategies\n",
    "- **Resource Allocation**: Cloud computing, network resources\n",
    "\n",
    "### Key Research Papers\n",
    "\n",
    "1. **MADDPG** (Lowe et al., 2017)\n",
    "2. **COMA** (Foerster et al., 2018)\n",
    "3. **QMIX** (Rashid et al., 2018)\n",
    "4. **CommNet** (Sukhbaatar et al., 2016)\n",
    "5. **OpenAI Five** (OpenAI, 2019)\n",
    "6. **AlphaStar** (Vinyals et al., 2019)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "014f01c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-Agent RL Implementation Imported!\n",
      "Components available:\n",
      "- MultiAgentReplayBuffer: Experience storage for multi-agent systems\n",
      "- Actor/Critic: Individual agent networks with centralized training\n",
      "- AttentionCritic: Attention mechanism for selective agent focus\n",
      "- CommunicationNetwork: Neural communication between agents\n",
      "- MADDPGAgent: Complete MADDPG implementation with extensions\n",
      "- MultiAgentEnvironment: Configurable multi-agent test environment\n"
     ]
    }
   ],
   "source": [
    "# Import Multi-Agent RL implementations\n",
    "from multi_agent_rl.multi_agent_rl import (\n",
    "    MultiAgentReplayBuffer, Actor, Critic, AttentionCritic, \n",
    "    CommunicationNetwork, MADDPGAgent, MultiAgentEnvironment\n",
    ")\n",
    "\n",
    "print(\"✅ Multi-Agent RL Implementation Imported!\")\n",
    "print(\"Components available:\")\n",
    "print(\"- MultiAgentReplayBuffer: Experience storage for multi-agent systems\")\n",
    "print(\"- Actor/Critic: Individual agent networks with centralized training\")\n",
    "print(\"- AttentionCritic: Attention mechanism for selective agent focus\")\n",
    "print(\"- CommunicationNetwork: Neural communication between agents\")\n",
    "print(\"- MADDPGAgent: Complete MADDPG implementation with extensions\")\n",
    "print(\"- MultiAgentEnvironment: Configurable multi-agent test environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1f25b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting Multi-Agent RL Training...\n",
      "\n",
      "==================================================\n",
      "Training: Cooperative Environment\n",
      "==================================================\n",
      "Training agents...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 196\u001b[39m\n\u001b[32m    194\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m    195\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining agents...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m196\u001b[39m episode_rewards, losses = \u001b[43mtrain_maddpg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Reduced for demo\u001b[39;49;00m\n\u001b[32m    199\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\n\u001b[32m    200\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n\u001b[32m    203\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mEvaluating agents...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mtrain_maddpg\u001b[39m\u001b[34m(env, agents, buffer, episodes, batch_size, update_interval)\u001b[39m\n\u001b[32m     35\u001b[39m         agent_messages = torch.cat([messages[:, :i], messages[:, i+\u001b[32m1\u001b[39m:]], dim=\u001b[32m1\u001b[39m)\n\u001b[32m     37\u001b[39m     action_tensor, _ = agent.act(obs_tensor, agent_messages, explore=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     actions[i] = \u001b[43maction_tensor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Environment step\u001b[39;00m\n\u001b[32m     41\u001b[39m next_obs, rewards, dones, _ = env.step(actions)\n",
      "\u001b[31mRuntimeError\u001b[39m: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "\n",
    "def train_maddpg(env: MultiAgentEnvironment, agents: List[MADDPGAgent],\n",
    "                 buffer: MultiAgentReplayBuffer, episodes: int = 1000,\n",
    "                 batch_size: int = 64, update_interval: int = 4):\n",
    "    \"\"\"Train MADDPG agents\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    losses = {f'agent_{i}': {'actor': [], 'critic': []} for i in range(env.n_agents)}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = np.zeros(env.n_agents)\n",
    "        done = False\n",
    "        step = 0\n",
    "        \n",
    "        while not done:\n",
    "            messages = []\n",
    "            if agents[0].use_communication:\n",
    "                for i, agent in enumerate(agents):\n",
    "                    obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n",
    "                    _, message = agent.act(obs_tensor, explore=True)\n",
    "                    messages.append(message)\n",
    "                messages = torch.stack(messages, dim=1)\n",
    "            \n",
    "            actions = np.zeros((env.n_agents, env.action_dim))\n",
    "            for i, agent in enumerate(agents):\n",
    "                obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n",
    "                \n",
    "                agent_messages = None\n",
    "                if agent.use_communication:\n",
    "                    agent_messages = torch.cat([messages[:, :i], messages[:, i+1:]], dim=1)\n",
    "                \n",
    "                action_tensor, _ = agent.act(obs_tensor, agent_messages, explore=True)\n",
    "                actions[i] = action_tensor.cpu().numpy()[0]\n",
    "            \n",
    "            next_obs, rewards, dones, _ = env.step(actions)\n",
    "            \n",
    "            buffer.add(obs, actions, rewards, next_obs, dones)\n",
    "            \n",
    "            episode_reward += rewards\n",
    "            obs = next_obs\n",
    "            done = np.all(dones)\n",
    "            step += 1\n",
    "            \n",
    "            if buffer.size >= batch_size and step % update_interval == 0:\n",
    "                batch = buffer.sample(batch_size)\n",
    "                \n",
    "                target_actors = [agent.actor_target for agent in agents]\n",
    "                \n",
    "                for i, agent in enumerate(agents):\n",
    "                    update_info = agent.update(batch, target_actors)\n",
    "                    losses[f'agent_{i}']['actor'].append(update_info['actor_loss'])\n",
    "                    losses[f'agent_{i}']['critic'].append(update_info['critic_loss'])\n",
    "        \n",
    "        episode_rewards.append(episode_reward.copy())\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            mean_reward = np.mean([np.sum(r) for r in episode_rewards[-100:]])\n",
    "            print(f\"Episode {episode}, Mean Reward: {mean_reward:.2f}\")\n",
    "            for i in range(env.n_agents):\n",
    "                noise_std = agents[i].noise_std\n",
    "                print(f\"  Agent {i}: Noise={noise_std:.3f}\")\n",
    "    \n",
    "    return episode_rewards, losses\n",
    "\n",
    "\n",
    "def evaluate_maddpg(env: MultiAgentEnvironment, agents: List[MADDPGAgent],\n",
    "                   episodes: int = 100) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate trained MADDPG agents\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    coordination_scores = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = np.zeros(env.n_agents)\n",
    "        positions_history = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            messages = []\n",
    "            if agents[0].use_communication:\n",
    "                for i, agent in enumerate(agents):\n",
    "                    obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n",
    "                    _, message = agent.act(obs_tensor, explore=False)\n",
    "                    messages.append(message)\n",
    "                messages = torch.stack(messages, dim=1)\n",
    "            \n",
    "            actions = np.zeros((env.n_agents, env.action_dim))\n",
    "            for i, agent in enumerate(agents):\n",
    "                obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n",
    "                \n",
    "                agent_messages = None\n",
    "                if agent.use_communication:\n",
    "                    agent_messages = torch.cat([messages[:, :i], messages[:, i+1:]], dim=1)\n",
    "                \n",
    "                action_tensor, _ = agent.act(obs_tensor, agent_messages, explore=False)\n",
    "                actions[i] = action_tensor.cpu().numpy()[0]\n",
    "            \n",
    "            next_obs, rewards, dones, _ = env.step(actions)\n",
    "            \n",
    "            episode_reward += rewards\n",
    "            positions_history.append(env.agent_states[:, :2].copy())\n",
    "            obs = next_obs\n",
    "            done = np.all(dones)\n",
    "        \n",
    "        episode_rewards.append(episode_reward.copy())\n",
    "        \n",
    "        positions = np.array(positions_history)\n",
    "        mean_positions = np.mean(positions, axis=1)  # [timesteps, 2]\n",
    "        agent_variances = []\n",
    "        \n",
    "        for t in range(len(positions)):\n",
    "            distances_from_center = [\n",
    "                np.linalg.norm(positions[t, i] - mean_positions[t])\n",
    "                for i in range(env.n_agents)\n",
    "            ]\n",
    "            agent_variances.append(np.var(distances_from_center))\n",
    "        \n",
    "        coordination_scores.append(np.mean(agent_variances))\n",
    "    \n",
    "    results = {\n",
    "        'mean_total_reward': np.mean([np.sum(r) for r in episode_rewards]),\n",
    "        'std_total_reward': np.std([np.sum(r) for r in episode_rewards]),\n",
    "        'mean_individual_reward': np.mean(episode_rewards),\n",
    "        'coordination_score': np.mean(coordination_scores),\n",
    "        'success_rate': np.mean([np.sum(r) > 0 for r in episode_rewards])\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"🚀 Starting Multi-Agent RL Training...\")\n",
    "\n",
    "env_configs = [\n",
    "    {'env_type': 'cooperative', 'name': 'Cooperative'},\n",
    "    {'env_type': 'competitive', 'name': 'Competitive'},\n",
    "    {'env_type': 'mixed', 'name': 'Mixed'}\n",
    "]\n",
    "\n",
    "results_summary = {}\n",
    "\n",
    "for config in env_configs[:1]:  # Train on cooperative first\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Training: {config['name']} Environment\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    env = MultiAgentEnvironment(\n",
    "        n_agents=3,\n",
    "        obs_dim=6,\n",
    "        action_dim=2,\n",
    "        env_type=config['env_type']\n",
    "    )\n",
    "    \n",
    "    agents = []\n",
    "    for i in range(env.n_agents):\n",
    "        agent = MADDPGAgent(\n",
    "            agent_idx=i,\n",
    "            obs_dim=env.obs_dim,\n",
    "            action_dim=env.action_dim,\n",
    "            n_agents=env.n_agents,\n",
    "            use_attention=True,\n",
    "            use_communication=True\n",
    "        )\n",
    "        agents.append(agent)\n",
    "    \n",
    "    buffer = MultiAgentReplayBuffer(\n",
    "        capacity=50000,\n",
    "        n_agents=env.n_agents,\n",
    "        obs_dim=env.obs_dim,\n",
    "        action_dim=env.action_dim\n",
    "    )\n",
    "    \n",
    "    print(\"Training agents...\")\n",
    "    episode_rewards, losses = train_maddpg(\n",
    "        env, agents, buffer,\n",
    "        episodes=500,  # Reduced for demo\n",
    "        batch_size=64\n",
    "    )\n",
    "    \n",
    "    print(\"Evaluating agents...\")\n",
    "    eval_results = evaluate_maddpg(env, agents, episodes=50)\n",
    "    \n",
    "    results_summary[config['name']] = {\n",
    "        'training_rewards': episode_rewards,\n",
    "        'evaluation': eval_results,\n",
    "        'losses': losses\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults for {config['name']} Environment:\")\n",
    "    print(f\"Mean Total Reward: {eval_results['mean_total_reward']:.3f} ± {eval_results['std_total_reward']:.3f}\")\n",
    "    print(f\"Mean Individual Reward: {eval_results['mean_individual_reward']:.3f}\")\n",
    "    print(f\"Coordination Score: {eval_results['coordination_score']:.3f}\")\n",
    "    print(f\"Success Rate: {eval_results['success_rate']:.3f}\")\n",
    "\n",
    "print(\"\\n✅ Multi-Agent Training Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d056b8b2",
   "metadata": {},
   "source": [
    "# Part Iii: Causal Reinforcement Learning\n",
    "\n",
    "## Theoretical Foundations\n",
    "\n",
    "### Introduction to Causality in Rl\n",
    "\n",
    "Causal Reinforcement Learning represents a paradigm shift from traditional correlation-based learning to understanding cause-effect relationships in sequential decision making. This approach addresses fundamental limitations in standard RL:\n",
    "\n",
    "**Key Limitations of Standard RL:**\n",
    "- **Spurious Correlations**: Agents may learn policies based on correlations that don't reflect true causal relationships\n",
    "- **Distribution Shift**: Policies trained on specific environments may fail when deployed in different conditions\n",
    "- **Sample Inefficiency**: Without causal understanding, agents require extensive exploration\n",
    "- **Interpretability**: Standard RL policies are often black boxes without clear causal reasoning\n",
    "\n",
    "### Causal Inference Framework\n",
    "\n",
    "#### 1. Structural Causal Models (scms)\n",
    "\n",
    "A Structural Causal Model is defined by a tuple $(U, V, F, P(U))$:\n",
    "\n",
    "- **U**: Set of exogenous (external) variables\n",
    "- **V**: Set of endogenous (internal) variables\n",
    "- **F**: Set of functions $f*i$ where $V*i = f*i(PA*i, U_i)$\n",
    "- **P(U)**: Probability distribution over exogenous variables\n",
    "\n",
    "**Causal Graph Representation:**\n",
    "```\n",
    "Exogenous Variables (U) → Endogenous Variables (V)\n",
    "      ↓                           ↓\n",
    "Environmental Factors    →    Agent States/Actions\n",
    "```\n",
    "\n",
    "#### 2. Causal Hierarchy (pearl's Ladder)\n",
    "\n",
    "**Level 1: Association** ($P(y|x)$)\n",
    "- \"What is the probability of Y given that we observe X?\"\n",
    "- Standard statistical/ML approaches operate here\n",
    "- Example: \"What's the probability of success given this policy?\"\n",
    "\n",
    "**Level 2: Intervention** ($P(y|do(x))$)\n",
    "- \"What is the probability of Y if we set X to a specific value?\"\n",
    "- Requires understanding of causal mechanisms\n",
    "- Example: \"What happens if we force the agent to take action A?\"\n",
    "\n",
    "**Level 3: Counterfactuals** ($P(y_x|x', y')$)\n",
    "- \"What would have happened if X had been different?\"\n",
    "- Enables reasoning about alternative scenarios\n",
    "- Example: \"Would the agent have succeeded if it had chosen a different action?\"\n",
    "\n",
    "### Causal Rl Mathematical Framework\n",
    "\n",
    "#### 1. Causal Markov Decision Process (causal-mdp)\n",
    "\n",
    "A Causal-MDP extends traditional MDPs with causal structure:\n",
    "\n",
    "**Causal-MDP Definition:**\n",
    "$$\\mathcal{M}*C = \\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{G}, T*C, R_C, \\gamma \\rangle$$\n",
    "\n",
    "Where:\n",
    "- $\\mathcal{G}$: Causal graph over state variables\n",
    "- $T_C$: Causal transition function respecting $\\mathcal{G}$\n",
    "- $R_C$: Causal reward function\n",
    "\n",
    "**Causal Factorization:**\n",
    "$$P(s*{t+1}|s*t, a*t) = \\prod*{i=1}^{|\\mathcal{S}|} P(s*{t+1}^i | PA*C(s*{t+1}^i), a*t)$$\n",
    "\n",
    "#### 2. Interventional Policy Learning\n",
    "\n",
    "**Interventional Value Function:**\n",
    "$$V^{\\pi}*{do(X=x)}(s) = \\mathbb{E}\\left[\\sum*{t=0}^{\\infty} \\gamma^t R*t | S*0 = s, do(X=x), \\pi\\right]$$\n",
    "\n",
    "**Causal Policy Gradient:**\n",
    "$$\\nabla*\\theta J(\\theta) = \\mathbb{E}*{s \\sim d^\\pi, a \\sim \\pi*\\theta}\\left[\\nabla*\\theta \\log \\pi*\\theta(a|s) \\cdot \\frac{\\partial Q^{\\pi}(s,a)}{\\partial do(\\pi*\\theta)}\\right]$$\n",
    "\n",
    "#### 3. Counterfactual Reasoning in Rl\n",
    "\n",
    "**Counterfactual Q-Function:**\n",
    "$$Q*{CF}(s, a, s', a') = \\mathbb{E}[R | S=s, A=a, S'*{do(A=a')} = s']$$\n",
    "\n",
    "This captures: \"What would the Q-value be if we had taken action $a'$ instead of $a$?\"\n",
    "\n",
    "### Causal Discovery in Rl\n",
    "\n",
    "#### 1. Structure Learning\n",
    "\n",
    "**Constraint-Based Methods:**\n",
    "- Use conditional independence tests\n",
    "- Build causal graph from statistical dependencies\n",
    "- Example: PC Algorithm adapted for sequential data\n",
    "\n",
    "**Score-Based Methods:**\n",
    "- Optimize causal graph structure score\n",
    "- Balance model fit with complexity\n",
    "- Example: BIC score with causal constraints\n",
    "\n",
    "#### 2. Causal Effect Estimation\n",
    "\n",
    "**Backdoor Criterion:**\n",
    "For estimating causal effect of action $A$ on reward $R$:\n",
    "$$P(R|do(A)) = \\sum_z P(R|A,Z) P(Z)$$\n",
    "\n",
    "Where $Z$ blocks all backdoor paths from $A$ to $R$.\n",
    "\n",
    "**Front-door Criterion:**\n",
    "When backdoor adjustment isn't possible:\n",
    "$$P(R|do(A)) = \\sum*m P(M|A) \\sum*{a'} P(R|A',M) P(A')$$\n",
    "\n",
    "### Advanced Causal Rl Techniques\n",
    "\n",
    "#### 1. Causal World Models\n",
    "\n",
    "**Causal Representation Learning:**\n",
    "Learn latent representations that respect causal structure:\n",
    "$$z*{t+1} = f*c(z*t, a*t, u_t)$$\n",
    "\n",
    "Where $f_c$ respects the causal graph structure.\n",
    "\n",
    "**Interventional Consistency:**\n",
    "$$\\mathbb{E}[z*{t+1} | do(z*t^i = v)] = \\mathbb{E}[f*c(z*t^{-i}, v, a*t, u*t)]$$\n",
    "\n",
    "#### 2. Causal Meta-learning\n",
    "\n",
    "**Task-Invariant Causal Features:**\n",
    "Learn features that are causally relevant across tasks:\n",
    "$$\\phi^*(s) = \\arg\\min*\\phi \\sum*{T} L_T(\\phi(s)) + \\lambda \\cdot \\text{Causal-Reg}(\\phi)$$\n",
    "\n",
    "**Causal Transfer:**\n",
    "Transfer causal knowledge between domains:\n",
    "$$\\pi*{new}(a|s) = \\pi*{old}(a|\\phi_{causal}(s))$$\n",
    "\n",
    "#### 3. Confounded Rl\n",
    "\n",
    "**Hidden Confounders:**\n",
    "When unobserved variables affect both states and rewards:\n",
    "$$H*t \\rightarrow S*t, H*t \\rightarrow R*t$$\n",
    "\n",
    "**Instrumental Variables:**\n",
    "Use variables correlated with actions but not directly with outcomes:\n",
    "$$IV \\rightarrow A*t \\not\\rightarrow R*t$$\n",
    "\n",
    "### Applications and Benefits\n",
    "\n",
    "#### 1. Robust Policy Learning\n",
    "- Policies that generalize across environments\n",
    "- Reduced sensitivity to spurious correlations\n",
    "- Better performance under distribution shift\n",
    "\n",
    "#### 2. Sample Efficient Exploration\n",
    "- Focus exploration on causally relevant factors\n",
    "- Avoid learning from misleading correlations\n",
    "- Faster convergence to optimal policies\n",
    "\n",
    "#### 3. Interpretable Decision Making\n",
    "- Understand why certain actions are taken\n",
    "- Provide causal explanations for policy decisions\n",
    "- Enable human oversight and validation\n",
    "\n",
    "#### 4. Safe Rl Applications\n",
    "- Predict consequences of interventions\n",
    "- Avoid actions with negative causal effects\n",
    "- Enable counterfactual safety analysis\n",
    "\n",
    "### Research Challenges\n",
    "\n",
    "#### 1. Causal Discovery\n",
    "- Identifying causal structure from observational RL data\n",
    "- Handling non-stationarity and temporal dependencies\n",
    "- Scalability to high-dimensional state spaces\n",
    "\n",
    "#### 2. Identifiability\n",
    "- When can causal effects be estimated from data?\n",
    "- Addressing unmeasured confounders\n",
    "- Validation of causal assumptions\n",
    "\n",
    "#### 3. Computational Complexity\n",
    "- Efficient inference in causal graphical models\n",
    "- Scalable algorithms for large state spaces\n",
    "- Real-time causal reasoning during policy execution\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ee7fbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Causal RL Implementation Imported!\n",
      "Components available:\n",
      "- CausalGraph: Causal relationship representation and d-separation\n",
      "- CausalDiscovery: PC algorithm for causal structure learning\n",
      "- CausalWorldModel: World model with causal structure constraints\n",
      "- InterventionalDataset: Dataset with observational/interventional data\n",
      "- CausalPolicyGradient: Policy gradient with causal regularization\n"
     ]
    }
   ],
   "source": [
    "# Import Causal RL implementations\n",
    "from causal_rl.causal_rl import (\n",
    "    CausalGraph, CausalDiscovery, CausalWorldModel, \n",
    "    InterventionalDataset, CausalPolicyGradient\n",
    ")\n",
    "\n",
    "print(\"✅ Causal RL Implementation Imported!\")\n",
    "print(\"Components available:\")\n",
    "print(\"- CausalGraph: Causal relationship representation and d-separation\")\n",
    "print(\"- CausalDiscovery: PC algorithm for causal structure learning\")\n",
    "print(\"- CausalWorldModel: World model with causal structure constraints\")\n",
    "print(\"- InterventionalDataset: Dataset with observational/interventional data\")\n",
    "print(\"- CausalPolicyGradient: Policy gradient with causal regularization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f57eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_environment():\n",
    "    \"\"\"Create a simple environment with causal structure for demonstration\"\"\"\n",
    "    \n",
    "    class CausalEnvironment:\n",
    "        def __init__(self):\n",
    "            # Causal graph: X -> Y -> Z, with confounding\n",
    "            self.state_dim = 3  # [X, Y, Z]\n",
    "            self.action_dim = 1\n",
    "            self.max_steps = 100\n",
    "            \n",
    "            # Causal parameters\n",
    "            self.x_noise = 0.1\n",
    "            self.y_noise = 0.05\n",
    "            self.z_noise = 0.02\n",
    "            \n",
    "            self.reset()\n",
    "        \n",
    "        def reset(self):\n",
    "            # Initialize with causal dependencies\n",
    "            self.x = np.random.normal(0, 1)\n",
    "            self.y = self.x + np.random.normal(0, self.y_noise)\n",
    "            self.z = self.y + np.random.normal(0, self.z_noise)\n",
    "            \n",
    "            self.steps = 0\n",
    "            return np.array([self.x, self.y, self.z])\n",
    "        \n",
    "        def step(self, action):\n",
    "            action = np.clip(action, -1, 1)[0]\n",
    "            \n",
    "            # Causal transitions\n",
    "            self.x += action * 0.1 + np.random.normal(0, self.x_noise)\n",
    "            self.y = self.x + action * 0.05 + np.random.normal(0, self.y_noise)\n",
    "            self.z = self.y + np.random.normal(0, self.z_noise)\n",
    "            \n",
    "            # Reward based on causal understanding\n",
    "            reward = -abs(self.z) - 0.1 * abs(action)\n",
    "            \n",
    "            self.steps += 1\n",
    "            done = self.steps >= self.max_steps\n",
    "            \n",
    "            return np.array([self.x, self.y, self.z]), reward, done, {}\n",
    "    \n",
    "    return CausalEnvironment()\n",
    "\n",
    "def demonstrate_causal_discovery(env, n_samples=1000):\n",
    "    \"\"\"Demonstrate causal structure learning from observational data\"\"\"\n",
    "    \n",
    "    print(\"🔍 Learning Causal Structure from Observational Data\")\n",
    "    \n",
    "    # Collect observational data\n",
    "    observations = []\n",
    "    actions = []\n",
    "    \n",
    "    obs = env.reset()\n",
    "    for _ in range(n_samples):\n",
    "        action = np.random.uniform(-1, 1, 1)\n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        \n",
    "        obs = next_obs\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "    \n",
    "    observations = np.array(observations)\n",
    "    actions = np.array(actions)\n",
    "    \n",
    "    # Create causal discovery object\n",
    "    causal_discovery = CausalDiscovery(\n",
    "        variables=['X', 'Y', 'Z', 'A'],\n",
    "        alpha=0.05\n",
    "    )\n",
    "    \n",
    "    # Prepare data for causal discovery\n",
    "    data = np.column_stack([observations, actions])\n",
    "    \n",
    "    # Learn causal graph\n",
    "    graph = causal_discovery.discover_structure(data)\n",
    "    \n",
    "    print(f\"Discovered causal graph with {len(graph.edges)} edges:\")\n",
    "    for edge in graph.edges:\n",
    "        print(f\"  {edge[0]} → {edge[1]}\")\n",
    "    \n",
    "    return graph, data\n",
    "\n",
    "def train_causal_world_model(env, graph, data, n_epochs=100):\n",
    "    \"\"\"Train a world model that respects causal structure\"\"\"\n",
    "    \n",
    "    print(\"🏗️ Training Causal World Model\")\n",
    "    \n",
    "    # Create causal world model\n",
    "    world_model = CausalWorldModel(\n",
    "        graph=graph,\n",
    "        obs_dim=3,\n",
    "        action_dim=1,\n",
    "        hidden_dim=64,\n",
    "        latent_dim=16\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(world_model.parameters(), lr=1e-3)\n",
    "    \n",
    "    losses = {'total': [], 'reconstruction': [], 'causal': []}\n",
    "    \n",
    "    batch_size = 32\n",
    "    n_batches = len(data) // batch_size\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_losses = {'total': 0, 'reconstruction': 0, 'causal': 0}\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = np.random.permutation(len(data))\n",
    "        \n",
    "        for i in range(n_batches):\n",
    "            batch_indices = indices[i*batch_size:(i+1)*batch_size]\n",
    "            batch_data = data[batch_indices]\n",
    "            \n",
    "            obs_batch = torch.FloatTensor(batch_data[:, :3]).to(device)\n",
    "            action_batch = torch.FloatTensor(batch_data[:, 3:]).to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = world_model(obs_batch, action_batch)\n",
    "            \n",
    "            # Losses\n",
    "            recon_loss = F.mse_loss(output['reconstruction'], obs_batch)\n",
    "            causal_loss = output['causal_constraint']\n",
    "            total_loss = recon_loss + 0.1 * causal_loss\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_losses['total'] += total_loss.item()\n",
    "            epoch_losses['reconstruction'] += recon_loss.item()\n",
    "            epoch_losses['causal'] += causal_loss.item()\n",
    "        \n",
    "        # Average losses\n",
    "        for key in epoch_losses:\n",
    "            epoch_losses[key] /= n_batches\n",
    "            losses[key].append(epoch_losses[key])\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}: Total={epoch_losses['total']:.4f}, \"\n",
    "                  f\"Recon={epoch_losses['reconstruction']:.4f}, \"\n",
    "                  f\"Causal={epoch_losses['causal']:.4f}\")\n",
    "    \n",
    "    return world_model, losses\n",
    "\n",
    "def demonstrate_interventional_reasoning(world_model, env):\n",
    "    \"\"\"Demonstrate interventional reasoning capabilities\"\"\"\n",
    "    \n",
    "    print(\"🔬 Demonstrating Interventional Reasoning\")\n",
    "    \n",
    "    # Test interventions on different variables\n",
    "    interventions = {\n",
    "        'X': lambda obs: np.array([1.0, obs[1], obs[2]]),  # Force X to 1.0\n",
    "        'Y': lambda obs: np.array([obs[0], 0.5, obs[2]]),  # Force Y to 0.5\n",
    "        'Z': lambda obs: np.array([obs[0], obs[1], -0.2])   # Force Z to -0.2\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for var, intervention_fn in interventions.items():\n",
    "        print(f\"\\nIntervening on {var}:\")\n",
    "        \n",
    "        # Get baseline prediction\n",
    "        obs = env.reset()\n",
    "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "        action_tensor = torch.zeros(1, 1).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            baseline_pred = world_model(obs_tensor, action_tensor)['reconstruction'].cpu().numpy()[0]\n",
    "        \n",
    "        # Apply intervention\n",
    "        intervened_obs = intervention_fn(obs)\n",
    "        intervened_tensor = torch.FloatTensor(intervened_obs).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            intervened_pred = world_model.predict_intervention(\n",
    "                intervened_tensor, action_tensor\n",
    "            ).cpu().numpy()[0]\n",
    "        \n",
    "        print(f\"  Baseline: {baseline_pred}\")\n",
    "        print(f\"  After intervention: {intervened_pred}\")\n",
    "        print(f\"  Change: {intervened_pred - baseline_pred}\")\n",
    "        \n",
    "        results[var] = {\n",
    "            'baseline': baseline_pred,\n",
    "            'intervened': intervened_pred,\n",
    "            'change': intervened_pred - baseline_pred\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"🚀 Starting Exercise 2: Causal Reinforcement Learning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Create causal environment\n",
    "env = create_causal_environment()\n",
    "print(f\"Environment: {env.state_dim}D state, {env.action_dim}D action\")\n",
    "\n",
    "# Demonstrate causal discovery\n",
    "graph, data = demonstrate_causal_discovery(env, n_samples=500)\n",
    "\n",
    "# Train causal world model\n",
    "world_model, training_losses = train_causal_world_model(env, graph, data, n_epochs=50)\n",
    "\n",
    "# Demonstrate interventional reasoning\n",
    "intervention_results = demonstrate_interventional_reasoning(world_model, env)\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "ax1.plot(training_losses['total'], label='Total Loss')\n",
    "ax1.plot(training_losses['reconstruction'], label='Reconstruction')\n",
    "ax1.plot(training_losses['causal'], label='Causal Constraint')\n",
    "ax1.set_title('Causal World Model Training Losses')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.bar(range(len(intervention_results)), \n",
    "        [r['change'][0] for r in intervention_results.values()],\n",
    "        alpha=0.7, color='blue', label='X change')\n",
    "ax2.bar(range(len(intervention_results)), \n",
    "        [r['change'][1] for r in intervention_results.values()],\n",
    "        alpha=0.7, color='orange', label='Y change', bottom=[r['change'][0] for r in intervention_results.values()])\n",
    "ax2.bar(range(len(intervention_results)), \n",
    "        [r['change'][2] for r in intervention_results.values()],\n",
    "        alpha=0.7, color='green', label='Z change', \n",
    "        bottom=[r['change'][0] + r['change'][1] for r in intervention_results.values()])\n",
    "ax2.set_title('Effects of Interventions')\n",
    "ax2.set_xlabel('Intervention Type')\n",
    "ax2.set_ylabel('State Variable Changes')\n",
    "ax2.set_xticks(range(len(intervention_results)))\n",
    "ax2.set_xticklabels(list(intervention_results.keys()))\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Causal graph visualization (simplified)\n",
    "ax3.axis('off')\n",
    "ax3.text(0.5, 0.5, 'Causal Graph:\\nX → Y → Z\\nA → X', \n",
    "         ha='center', va='center', fontsize=12, \n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "ax3.set_title('Learned Causal Structure')\n",
    "\n",
    "ax4.hist(data[:, 0], bins=30, alpha=0.7, label='X', density=True)\n",
    "ax4.hist(data[:, 1], bins=30, alpha=0.7, label='Y', density=True)\n",
    "ax4.hist(data[:, 2], bins=30, alpha=0.7, label='Z', density=True)\n",
    "ax4.set_title('Observational Data Distributions')\n",
    "ax4.set_xlabel('Value')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.legend()\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Exercise 2 Complete!\")\n",
    "print(\"Key learnings:\")\n",
    "print(\"- Causal discovery can identify relationships from observational data\")\n",
    "print(\"- Causal world models respect structural constraints during learning\")\n",
    "print(\"- Interventions allow testing 'what-if' scenarios\")\n",
    "print(\"- Causal understanding improves robustness and interpretability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe02fc",
   "metadata": {},
   "source": [
    "# Part Iv: Quantum Reinforcement Learning\n",
    "\n",
    "## Theoretical Foundations\n",
    "\n",
    "### Introduction to Quantum Computing for Rl\n",
    "\n",
    "Quantum Reinforcement Learning (QRL) leverages quantum mechanical phenomena to enhance reinforcement learning algorithms. This emerging field promises exponential speedups for certain RL problems and enables exploration of vast state spaces that are intractable for classical computers.\n",
    "\n",
    "**Key Quantum Phenomena:**\n",
    "- **Superposition**: Quantum states can exist in multiple states simultaneously\n",
    "- **Entanglement**: Quantum systems can be correlated in non-classical ways\n",
    "- **Interference**: Quantum amplitudes can interfere constructively or destructively\n",
    "- **Quantum Parallelism**: Process multiple inputs simultaneously\n",
    "\n",
    "### Quantum Computing Fundamentals\n",
    "\n",
    "#### 1. Quantum State Representation\n",
    "\n",
    "**Qubit State:**\n",
    "$$|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle$$\n",
    "\n",
    "Where $|\\alpha|^2 + |\\beta|^2 = 1$ and $\\alpha, \\beta \\in \\mathbb{C}$.\n",
    "\n",
    "**Multi-qubit System:**\n",
    "$$|\\psi\\rangle = \\sum*{i=0}^{2^n-1} \\alpha*i |i\\rangle$$\n",
    "\n",
    "For $n$ qubits with $\\sum*{i=0}^{2^n-1} |\\alpha*i|^2 = 1$.\n",
    "\n",
    "#### 2. Quantum Operations\n",
    "\n",
    "**Quantum Gates:**\n",
    "- **Pauli-X**: $X = \\begin{pmatrix} 0 & 1 \\\\ 1 & 0 \\end{pmatrix}$ (Bit flip)\n",
    "- **Pauli-Y**: $Y = \\begin{pmatrix} 0 & -i \\\\ i & 0 \\end{pmatrix}$\n",
    "- **Pauli-Z**: $Z = \\begin{pmatrix} 1 & 0 \\\\ 0 & -1 \\end{pmatrix}$ (Phase flip)\n",
    "- **Hadamard**: $H = \\frac{1}{\\sqrt{2}}\\begin{pmatrix} 1 & 1 \\\\ 1 & -1 \\end{pmatrix}$ (Superposition)\n",
    "\n",
    "**Rotation Gates:**\n",
    "$$R_x(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -i\\sin(\\theta/2) \\\\ -i\\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix}$$\n",
    "\n",
    "$$R_y(\\theta) = \\begin{pmatrix} \\cos(\\theta/2) & -\\sin(\\theta/2) \\\\ \\sin(\\theta/2) & \\cos(\\theta/2) \\end{pmatrix}$$\n",
    "\n",
    "#### 3. Quantum Measurement\n",
    "\n",
    "**Born Rule:**\n",
    "$$P(|i\\rangle) = |\\langle i | \\psi \\rangle|^2$$\n",
    "\n",
    "The probability of measuring state $|i\\rangle$ from state $|\\psi\\rangle$.\n",
    "\n",
    "### Quantum Reinforcement Learning Framework\n",
    "\n",
    "#### 1. Quantum Mdp (qmdp)\n",
    "\n",
    "**Quantum State Space:**\n",
    "States are represented as quantum states in Hilbert space $\\mathcal{H}$:\n",
    "$$|\\psi*s\\rangle \\in \\mathcal{H}, \\quad \\langle\\psi*s|\\psi_s\\rangle = 1$$\n",
    "\n",
    "**Quantum Action Space:**\n",
    "Actions correspond to unitary operations:\n",
    "$$\\mathcal{A} = \\{U*a : U*a^\\dagger U_a = I\\}$$\n",
    "\n",
    "**Quantum Transition Dynamics:**\n",
    "$$|\\psi*{t+1}\\rangle = U*{a*t} |\\psi*t\\rangle \\otimes |\\text{env}_t\\rangle$$\n",
    "\n",
    "#### 2. Quantum Value Functions\n",
    "\n",
    "**Quantum Q-Function:**\n",
    "$$Q(|\\psi\\rangle, U*a) = \\langle\\psi| U*a^\\dagger \\hat{R} U_a |\\psi\\rangle + \\gamma \\mathbb{E}[V(|\\psi'\\rangle)]$$\n",
    "\n",
    "Where $\\hat{R}$ is the reward operator.\n",
    "\n",
    "**Quantum Bellman Equation:**\n",
    "$$\\hat{V}|\\psi\\rangle = \\max*{U*a} \\left(\\hat{R}U*a|\\psi\\rangle + \\gamma \\sum*{|\\psi'\\rangle} P(|\\psi'\\rangle||\\psi\\rangle, U_a) \\hat{V}|\\psi'\\rangle\\right)$$\n",
    "\n",
    "#### 3. Quantum Policy Representation\n",
    "\n",
    "**Parameterized Quantum Circuit (PQC):**\n",
    "$$|\\psi(\\theta)\\rangle = U*L(\\theta*L) \\cdots U*2(\\theta*2) U*1(\\theta*1) |\\psi_0\\rangle$$\n",
    "\n",
    "Where each $U*i(\\theta*i)$ is a parameterized unitary gate.\n",
    "\n",
    "**Quantum Policy:**\n",
    "$$\\pi_\\theta(a|s) = |\\langle a | U(\\theta) |s \\rangle|^2$$\n",
    "\n",
    "### Variational Quantum Algorithms for Rl\n",
    "\n",
    "#### 1. Variational Quantum Eigensolver (vqe) for Value Functions\n",
    "\n",
    "**Objective:**\n",
    "$$\\theta^* = \\arg\\min_\\theta \\langle\\psi(\\theta)| \\hat{H} |\\psi(\\theta)\\rangle$$\n",
    "\n",
    "Where $\\hat{H}$ encodes the RL problem structure.\n",
    "\n",
    "**Gradient Calculation:**\n",
    "$$\\nabla_\\theta f(\\theta) = \\frac{1}{2}[f(\\theta + \\pi/2) - f(\\theta - \\pi/2)]$$\n",
    "\n",
    "#### 2. Quantum Approximate Optimization Algorithm (qaoa)\n",
    "\n",
    "**QAOA Ansatz:**\n",
    "$$|\\psi(\\gamma, \\beta)\\rangle = \\prod*{p=1}^P U*B(\\beta*p) U*C(\\gamma*p) |\\psi*0\\rangle$$\n",
    "\n",
    "Where:\n",
    "- $U*C(\\gamma) = \\exp(-i\\gamma \\hat{H}*C)$ (Cost Hamiltonian)\n",
    "- $U*B(\\beta) = \\exp(-i\\beta \\hat{H}*B)$ (Mixer Hamiltonian)\n",
    "\n",
    "### Quantum Advantage in Rl\n",
    "\n",
    "#### 1. Exponential State Space\n",
    "\n",
    "**Classical Scaling:**\n",
    "Memory: $O(2^n)$ for $n$-qubit states\n",
    "Operations: $O(2^{2n})$ for general operations\n",
    "\n",
    "**Quantum Scaling:**\n",
    "Memory: $O(n)$ qubits\n",
    "Operations: $O(poly(n))$ for many quantum algorithms\n",
    "\n",
    "#### 2. Quantum Speedups\n",
    "\n",
    "**Grover's Algorithm for RL:**\n",
    "- Search optimal actions in $O(\\sqrt{N})$ instead of $O(N)$\n",
    "- Applicable to unstructured action spaces\n",
    "\n",
    "**Quantum Walk for Exploration:**\n",
    "- Quadratic speedup over classical random walk\n",
    "- Enhanced exploration capabilities\n",
    "\n",
    "**Shor's Algorithm Applications:**\n",
    "- Factoring in cryptographic environments\n",
    "- Period finding in periodic MDPs\n",
    "\n",
    "### Quantum Machine Learning Integration\n",
    "\n",
    "#### 1. Quantum Neural Networks (qnns)\n",
    "\n",
    "**Quantum Perceptron:**\n",
    "$$f(x) = \\langle 0^{\\otimes n} | U^\\dagger(\\theta) M U(\\theta) |x\\rangle$$\n",
    "\n",
    "Where $U(\\theta)$ is a parameterized quantum circuit and $M$ is a measurement operator.\n",
    "\n",
    "**Quantum Convolutional Neural Networks:**\n",
    "- Quantum convolution using local unitaries\n",
    "- Translation equivariance in quantum feature maps\n",
    "\n",
    "#### 2. Quantum Kernel Methods\n",
    "\n",
    "**Quantum Feature Map:**\n",
    "$$\\Phi(x) = |\\phi(x)\\rangle = U_\\phi(x)|0\\rangle^{\\otimes n}$$\n",
    "\n",
    "**Quantum Kernel:**\n",
    "$$K(x*i, x*j) = |\\langle\\phi(x*i)|\\phi(x*j)\\rangle|^2$$\n",
    "\n",
    "Potentially exponential advantage in feature space dimension.\n",
    "\n",
    "### Advanced Qrl Techniques\n",
    "\n",
    "#### 1. Quantum Actor-critic\n",
    "\n",
    "**Quantum Actor:**\n",
    "$$\\pi*\\theta(a|s) = \\text{Tr}[\\Pi*a U*\\theta(s) \\rho*s U_\\theta(s)^\\dagger]$$\n",
    "\n",
    "Where $\\Pi_a$ is the projector onto action $a$.\n",
    "\n",
    "**Quantum Critic:**\n",
    "$$V*\\phi(s) = \\text{Tr}[\\hat{V}*\\phi \\rho_s]$$\n",
    "\n",
    "**Quantum Policy Gradient:**\n",
    "$$\\nabla*\\theta J(\\theta) = \\sum*{s,a} \\rho^\\pi(s) \\nabla*\\theta \\pi*\\theta(a|s) Q^\\pi(s,a)$$\n",
    "\n",
    "#### 2. Quantum Experience Replay\n",
    "\n",
    "**Quantum Superposition of Experiences:**\n",
    "$$|\\text{memory}\\rangle = \\frac{1}{\\sqrt{N}} \\sum*{i=1}^N |s*i, a*i, r*i, s_i'\\rangle$$\n",
    "\n",
    "**Quantum Sampling:**\n",
    "Use quantum interference to bias sampling towards important experiences.\n",
    "\n",
    "#### 3. Quantum Multi-agent Rl\n",
    "\n",
    "**Entangled Agent States:**\n",
    "$$|\\psi*{\\text{agents}}\\rangle = \\frac{1}{\\sqrt{2}}(|\\psi*1\\rangle \\otimes |\\psi*2\\rangle + |\\psi*1'\\rangle \\otimes |\\psi_2'\\rangle)$$\n",
    "\n",
    "**Quantum Communication:**\n",
    "Agents share quantum information through entanglement.\n",
    "\n",
    "### Quantum Error Correction in Qrl\n",
    "\n",
    "#### 1. Noisy Intermediate-scale Quantum (nisq) Era\n",
    "\n",
    "**Noise Models:**\n",
    "- Decoherence: $\\rho(t) = e^{-\\Gamma t} \\rho(0)$\n",
    "- Gate errors: Imperfect unitary operations\n",
    "- Measurement errors: Probabilistic bit flips\n",
    "\n",
    "**Error Mitigation:**\n",
    "- Zero noise extrapolation\n",
    "- Error amplification and cancellation\n",
    "- Probabilistic error cancellation\n",
    "\n",
    "#### 2. Fault-tolerant Qrl\n",
    "\n",
    "**Quantum Error Correction Codes:**\n",
    "- Surface codes for topological protection\n",
    "- Stabilizer codes for syndrome detection\n",
    "- Logical qubit operations\n",
    "\n",
    "### Applications and Use Cases\n",
    "\n",
    "#### 1. Quantum Chemistry Rl\n",
    "- Molecular dynamics simulation\n",
    "- Drug discovery optimization\n",
    "- Catalyst design\n",
    "\n",
    "#### 2. Quantum Finance\n",
    "- Portfolio optimization with quantum speedup\n",
    "- Risk analysis using quantum simulation\n",
    "- Quantum Monte Carlo for derivatives pricing\n",
    "\n",
    "#### 3. Quantum Cryptography Rl\n",
    "- Quantum key distribution protocols\n",
    "- Post-quantum cryptography\n",
    "- Quantum-safe communications\n",
    "\n",
    "#### 4. Quantum Optimization\n",
    "- Traffic flow optimization\n",
    "- Supply chain management\n",
    "- Resource allocation problems\n",
    "\n",
    "### Current Limitations and Challenges\n",
    "\n",
    "#### 1. Hardware Limitations\n",
    "- Limited qubit count and coherence time\n",
    "- High error rates in current quantum devices\n",
    "- Connectivity constraints in quantum architectures\n",
    "\n",
    "#### 2. Algorithmic Challenges\n",
    "- Barren plateaus in quantum optimization\n",
    "- Classical simulation for algorithm development\n",
    "- Quantum advantage verification\n",
    "\n",
    "#### 3. Practical Implementation\n",
    "- Quantum software development complexity\n",
    "- Integration with classical systems\n",
    "- Scalability to real-world problems\n",
    "\n",
    "### Future Directions\n",
    "\n",
    "#### 1. Near-term Applications\n",
    "- Hybrid classical-quantum algorithms\n",
    "- NISQ-era quantum advantage demonstrations\n",
    "- Quantum-enhanced machine learning\n",
    "\n",
    "#### 2. Long-term Vision\n",
    "- Fault-tolerant quantum RL systems\n",
    "- Universal quantum learning machines\n",
    "- Quantum artificial general intelligence\n",
    "\n",
    "#### 3. Theoretical Advances\n",
    "- Quantum learning theory foundations\n",
    "- Quantum-classical complexity separations\n",
    "- Novel quantum algorithms for RL\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4df59bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Quantum RL Implementation Imported!\n",
      "Components available:\n",
      "- QuantumState: Quantum state representation with amplitudes\n",
      "- QuantumGate: Quantum gate operations (Pauli, Hadamard, rotations)\n",
      "- QuantumCircuit: Quantum circuit implementation\n",
      "- VariationalQuantumCircuit: Parameterized quantum circuits for ML\n",
      "- QuantumQLearning: Quantum-enhanced Q-learning\n",
      "- QuantumActorCritic: Quantum actor-critic algorithm\n",
      "- QuantumEnvironment: Quantum-inspired test environment\n"
     ]
    }
   ],
   "source": [
    "# Import Quantum RL implementations\n",
    "from quantum_rl.quantum_rl import (\n",
    "    QuantumState, QuantumGate, QuantumCircuit, VariationalQuantumCircuit,\n",
    "    QuantumQLearning, QuantumActorCritic, QuantumEnvironment\n",
    ")\n",
    "\n",
    "print(\"✅ Quantum RL Implementation Imported!\")\n",
    "print(\"Components available:\")\n",
    "print(\"- QuantumState: Quantum state representation with amplitudes\")\n",
    "print(\"- QuantumGate: Quantum gate operations (Pauli, Hadamard, rotations)\")\n",
    "print(\"- QuantumCircuit: Quantum circuit implementation\")\n",
    "print(\"- VariationalQuantumCircuit: Parameterized quantum circuits for ML\")\n",
    "print(\"- QuantumQLearning: Quantum-enhanced Q-learning\")\n",
    "print(\"- QuantumActorCritic: Quantum actor-critic algorithm\")\n",
    "print(\"- QuantumEnvironment: Quantum-inspired test environment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37bd2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_quantum_environment():\n",
    "    \"\"\"Create a simple environment suitable for quantum RL demonstration\"\"\"\n",
    "    \n",
    "    class QuantumInspiredEnvironment:\n",
    "        def __init__(self, n_qubits=2):\n",
    "            self.n_qubits = n_qubits\n",
    "            self.state_dim = 2**n_qubits  # Full quantum state representation\n",
    "            self.action_dim = n_qubits    # Actions are rotations on each qubit\n",
    "            self.max_steps = 50\n",
    "            \n",
    "            # Initialize quantum state (simplified classical representation)\n",
    "            self.reset()\n",
    "        \n",
    "        def reset(self):\n",
    "            # Start with |00...0> state\n",
    "            self.quantum_state = np.zeros(2**self.n_qubits)\n",
    "            self.quantum_state[0] = 1.0\n",
    "            \n",
    "            self.steps = 0\n",
    "            return self.quantum_state.copy()\n",
    "        \n",
    "        def step(self, action):\n",
    "            # Apply quantum gates (simplified)\n",
    "            action = np.clip(action, -np.pi, np.pi)\n",
    "            \n",
    "            # Apply rotation gates\n",
    "            for i, theta in enumerate(action):\n",
    "                # Simplified rotation on qubit i\n",
    "                self.apply_rotation(i, theta)\n",
    "            \n",
    "            # Add some quantum-inspired dynamics\n",
    "            self.apply_quantum_noise()\n",
    "            \n",
    "            # Compute reward based on quantum state properties\n",
    "            reward = self.compute_quantum_reward()\n",
    "            \n",
    "            self.steps += 1\n",
    "            done = self.steps >= self.max_steps\n",
    "            \n",
    "            return self.quantum_state.copy(), reward, done, {}\n",
    "        \n",
    "        def apply_rotation(self, qubit_idx, theta):\n",
    "            \"\"\"Apply a rotation gate to a specific qubit\"\"\"\n",
    "            cos_theta = np.cos(theta/2)\n",
    "            sin_theta = np.sin(theta/2)\n",
    "            \n",
    "            # Simplified single-qubit rotation (affects computational basis)\n",
    "            new_state = np.zeros_like(self.quantum_state)\n",
    "            \n",
    "            for i in range(len(self.quantum_state)):\n",
    "                bit = (i >> qubit_idx) & 1\n",
    "                if bit == 0:\n",
    "                    # |0> -> cos(θ/2)|0> - sin(θ/2)|1>\n",
    "                    new_state[i] += cos_theta * self.quantum_state[i]\n",
    "                    flipped_i = i | (1 << qubit_idx)\n",
    "                    new_state[flipped_i] -= sin_theta * self.quantum_state[i]\n",
    "                else:\n",
    "                    # |1> -> sin(θ/2)|0> + cos(θ/2)|1>\n",
    "                    flipped_i = i & ~(1 << qubit_idx)\n",
    "                    new_state[flipped_i] += sin_theta * self.quantum_state[i]\n",
    "                    new_state[i] += cos_theta * self.quantum_state[i]\n",
    "            \n",
    "            self.quantum_state = new_state\n",
    "        \n",
    "        def apply_quantum_noise(self):\n",
    "            \"\"\"Add quantum-inspired noise\"\"\"\n",
    "            # Dephasing-like noise\n",
    "            noise_strength = 0.01\n",
    "            phase_noise = np.random.normal(0, noise_strength, len(self.quantum_state))\n",
    "            self.quantum_state *= np.exp(1j * phase_noise)\n",
    "            self.quantum_state = np.real(self.quantum_state)  # Keep real for simplicity\n",
    "        \n",
    "        def compute_quantum_reward(self):\n",
    "            \"\"\"Compute reward based on quantum state properties\"\"\"\n",
    "            # Reward for creating superposition and entanglement-like states\n",
    "            probabilities = np.abs(self.quantum_state)**2\n",
    "            \n",
    "            # Entropy (reward for mixed states)\n",
    "            entropy = -np.sum(probabilities * np.log(probabilities + 1e-10))\n",
    "            \n",
    "            # Reward for having multiple non-zero amplitudes\n",
    "            n_nonzero = np.sum(probabilities > 0.01)\n",
    "            \n",
    "            # Penalty for being in computational basis states\n",
    "            basis_penalty = -np.sum(probabilities[[0, -1]]**2)\n",
    "            \n",
    "            reward = entropy + 0.1 * n_nonzero + basis_penalty\n",
    "            \n",
    "            return reward\n",
    "    \n",
    "    return QuantumInspiredEnvironment()\n",
    "\n",
    "def demonstrate_quantum_circuit():\n",
    "    \"\"\"Demonstrate basic quantum circuit operations\"\"\"\n",
    "    \n",
    "    print(\"🔬 Quantum Circuit Demonstration\")\n",
    "    \n",
    "    # Create quantum circuit\n",
    "    circuit = QuantumCircuit(n_qubits=2)\n",
    "    \n",
    "    print(\"Initial state: |00>\")\n",
    "    print(f\"State vector: {circuit.get_state_vector()}\")\n",
    "    \n",
    "    # Apply Hadamard gates\n",
    "    circuit.apply_gate(QuantumGate.hadamard(), 0)\n",
    "    circuit.apply_gate(QuantumGate.hadamard(), 1)\n",
    "    \n",
    "    print(\"\\nAfter H⊗H: (|00> + |01> + |10> + |11>)/2\")\n",
    "    print(f\"State vector: {circuit.get_state_vector()}\")\n",
    "    \n",
    "    # Apply CNOT gate\n",
    "    circuit.apply_cnot(0, 1)\n",
    "    \n",
    "    print(\"\\nAfter CNOT(0,1): Creates entanglement\")\n",
    "    print(f\"State vector: {circuit.get_state_vector()}\")\n",
    "    \n",
    "    # Measure\n",
    "    measurements = []\n",
    "    for _ in range(1000):\n",
    "        result = circuit.measure()\n",
    "        measurements.append(result)\n",
    "    \n",
    "    print(f\"\\nMeasurement statistics (1000 shots):\")\n",
    "    unique, counts = np.unique(measurements, return_counts=True)\n",
    "    for state, count in zip(unique, counts):\n",
    "        print(f\"  |{state:02b}>: {count/10:.1f}%\")\n",
    "    \n",
    "    return circuit\n",
    "\n",
    "def train_quantum_q_learning(env, n_episodes=200):\n",
    "    \"\"\"Train quantum-enhanced Q-learning\"\"\"\n",
    "    \n",
    "    print(\"🧠 Training Quantum Q-Learning Agent\")\n",
    "    \n",
    "    # Create quantum Q-learning agent\n",
    "    agent = QuantumQLearning(\n",
    "        n_qubits=env.n_qubits,\n",
    "        action_dim=env.action_dim,\n",
    "        learning_rate=0.1,\n",
    "        discount_factor=0.95,\n",
    "        exploration_rate=1.0,\n",
    "        exploration_decay=0.995\n",
    "    )\n",
    "    \n",
    "    episode_rewards = []\n",
    "    exploration_rates = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            # Select action using quantum state\n",
    "            action = agent.select_action(obs)\n",
    "            \n",
    "            # Take action in environment\n",
    "            next_obs, reward, done, _ = env.step(action)\n",
    "            \n",
    "            # Update quantum Q-function\n",
    "            agent.update(obs, action, reward, next_obs, done)\n",
    "            \n",
    "            obs = next_obs\n",
    "            episode_reward += reward\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        exploration_rates.append(agent.exploration_rate)\n",
    "        \n",
    "        # Decay exploration\n",
    "        agent.exploration_rate *= agent.exploration_decay\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}: Reward={episode_reward:.2f}, \"\n",
    "                  f\"Exploration={agent.exploration_rate:.3f}\")\n",
    "    \n",
    "    return agent, episode_rewards, exploration_rates\n",
    "\n",
    "def demonstrate_quantum_actor_critic(env):\n",
    "    \"\"\"Demonstrate quantum actor-critic algorithm\"\"\"\n",
    "    \n",
    "    print(\"🎭 Training Quantum Actor-Critic Agent\")\n",
    "    \n",
    "    # Create quantum actor-critic agent\n",
    "    agent = QuantumActorCritic(\n",
    "        n_qubits=env.n_qubits,\n",
    "        action_dim=env.action_dim,\n",
    "        actor_hidden_dim=32,\n",
    "        critic_hidden_dim=32,\n",
    "        learning_rate=1e-3\n",
    "    )\n",
    "    \n",
    "    optimizer = torch.optim.Adam(agent.parameters(), lr=1e-3)\n",
    "    \n",
    "    n_episodes = 100\n",
    "    episode_rewards = []\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        \n",
    "        while not done:\n",
    "            obs_tensor = torch.FloatTensor(obs).to(device)\n",
    "            \n",
    "            # Get action and value from quantum actor-critic\n",
    "            action_dist, value = agent(obs_tensor)\n",
    "            action = action_dist.sample()\n",
    "            \n",
    "            next_obs, reward, done, _ = env.step(action.cpu().numpy())\n",
    "            \n",
    "            log_probs.append(action_dist.log_prob(action))\n",
    "            values.append(value)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            obs = next_obs\n",
    "            episode_reward += reward\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns = []\n",
    "        G = 0\n",
    "        for r in reversed(rewards):\n",
    "            G = r + 0.99 * G\n",
    "            returns.insert(0, G)\n",
    "        \n",
    "        returns = torch.FloatTensor(returns).to(device)\n",
    "        values = torch.cat(values)\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        \n",
    "        advantages = returns - values\n",
    "        \n",
    "        # Compute losses\n",
    "        actor_loss = -(log_probs * advantages.detach()).mean()\n",
    "        critic_loss = F.mse_loss(values, returns)\n",
    "        total_loss = actor_loss + critic_loss\n",
    "        \n",
    "        # Update\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        if episode % 20 == 0:\n",
    "            print(f\"Episode {episode}: Reward={episode_reward:.2f}, \"\n",
    "                  f\"Actor Loss={actor_loss:.4f}, Critic Loss={critic_loss:.4f}\")\n",
    "    \n",
    "    return agent, episode_rewards\n",
    "\n",
    "print(\"🚀 Starting Exercise 3: Quantum Reinforcement Learning\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Demonstrate quantum circuit\n",
    "circuit = demonstrate_quantum_circuit()\n",
    "\n",
    "# Create quantum environment\n",
    "env = create_quantum_environment(n_qubits=2)\n",
    "print(f\"\\nEnvironment: {env.state_dim}D quantum state, {env.action_dim}D actions\")\n",
    "\n",
    "# Train quantum Q-learning\n",
    "q_agent, q_rewards, exploration_rates = train_quantum_q_learning(env, n_episodes=100)\n",
    "\n",
    "# Train quantum actor-critic\n",
    "ac_agent, ac_rewards = demonstrate_quantum_actor_critic(env)\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "ax1.plot(q_rewards, alpha=0.7, label='Quantum Q-Learning')\n",
    "ax1.plot(ac_rewards, alpha=0.7, label='Quantum Actor-Critic')\n",
    "ax1.set_title('Training Performance Comparison')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Reward')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(exploration_rates)\n",
    "ax2.set_title('Exploration Rate Decay (Q-Learning)')\n",
    "ax2.set_xlabel('Episode')\n",
    "ax2.set_ylabel('Exploration Rate')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Quantum state visualization (simplified)\n",
    "final_state = env.reset()\n",
    "for _ in range(10):\n",
    "    action = q_agent.select_action(final_state)\n",
    "    final_state, _, _, _ = env.step(action)\n",
    "\n",
    "ax3.bar(range(len(final_state)), np.abs(final_state)**2, alpha=0.7)\n",
    "ax3.set_title('Final Quantum State Probabilities')\n",
    "ax3.set_xlabel('Computational Basis State')\n",
    "ax3.set_ylabel('Probability')\n",
    "ax3.grid(True)\n",
    "\n",
    "ax4.hist(q_rewards[-50:], bins=10, alpha=0.7, label='Q-Learning')\n",
    "ax4.hist(ac_rewards[-50:], bins=10, alpha=0.7, label='Actor-Critic')\n",
    "ax4.set_title('Reward Distribution (Last 50 Episodes)')\n",
    "ax4.set_xlabel('Episode Reward')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.legend()\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Exercise 3 Complete!\")\n",
    "print(\"Key learnings:\")\n",
    "print(\"- Quantum circuits can represent complex state spaces efficiently\")\n",
    "print(\"- Quantum Q-learning leverages superposition for exploration\")\n",
    "print(\"- Quantum actor-critic combines quantum policies with classical critics\")\n",
    "print(\"- Quantum RL shows promise for problems with exponential state spaces\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ed7f62",
   "metadata": {},
   "source": [
    "# Part V: Integration and Advanced Applications\n",
    "\n",
    "## Synthesis of Advanced Rl Paradigms\n",
    "\n",
    "The four paradigms we've explored—World Models, Multi-Agent RL, Causal RL, and Quantum RL—represent the cutting edge of reinforcement learning research. Each addresses fundamental limitations of traditional RL approaches:\n",
    "\n",
    "### Paradigm Integration Matrix\n",
    "\n",
    "| Aspect | World Models | Multi-Agent RL | Causal RL | Quantum RL |\n",
    "|--------|-------------|----------------|-----------|------------|\n",
    "| **Sample Efficiency** | ✓ Via planning | ✓ Via sharing | ✓ Via causal structure | ✓ Via superposition |\n",
    "| **Interpretability** | ✓ Via explicit models | ✓ Via agent interaction | ✓ Via causal graphs | ◐ Via quantum states |\n",
    "| **Scalability** | ◐ Model complexity | ✓ Distributed learning | ◐ Structure discovery | ◐ Quantum advantage |\n",
    "| **Robustness** | ◐ Model uncertainty | ✓ Via diversity | ✓ Via interventions | ◐ Quantum decoherence |\n",
    "\n",
    "### Hybrid Approaches\n",
    "\n",
    "#### 1. Causal World Models\n",
    "Combining causal structure discovery with world model learning:\n",
    "```python\n",
    "class CausalWorldModel:\n",
    "    def **init**(self, causal*graph, dynamics*model):\n",
    "        self.causal*graph = causal*graph\n",
    "        self.dynamics*model = dynamics*model\n",
    "    \n",
    "    def predict_intervention(self, state, action, intervention):\n",
    "        # Use causal graph to modify dynamics\n",
    "        return self.dynamics*model.predict*with_intervention(\n",
    "            state, action, intervention, self.causal_graph\n",
    "        )\n",
    "```\n",
    "\n",
    "#### 2. Multi-agent Causal Rl\n",
    "Agents learning shared causal structures:\n",
    "```python\n",
    "class MultiAgentCausalRL:\n",
    "    def **init**(self, agents, shared*causal*graph):\n",
    "        self.agents = agents\n",
    "        self.shared*graph = shared*causal_graph\n",
    "    \n",
    "    def collective*structure*learning(self, experiences):\n",
    "        # Pool experiences for better causal discovery\n",
    "        return update*shared*causal_structure(experiences)\n",
    "```\n",
    "\n",
    "#### 3. Quantum Multi-agent Systems\n",
    "Leveraging quantum entanglement for coordination:\n",
    "```python\n",
    "class QuantumMultiAgentSystem:\n",
    "    def **init**(self, n*agents, n*qubits):\n",
    "        self.entangled*state = create*entangled*state(n*agents, n_qubits)\n",
    "    \n",
    "    def quantum*coordination(self, local*observations):\n",
    "        return quantum*communication*protocol(\n",
    "            local*observations, self.entangled*state\n",
    "        )\n",
    "```\n",
    "\n",
    "## Real-world Applications\n",
    "\n",
    "### 1. Autonomous Vehicle Networks\n",
    "- **World Models**: Environmental prediction and planning\n",
    "- **Multi-Agent**: Vehicle coordination and traffic optimization\n",
    "- **Causal RL**: Understanding cause-effect in traffic patterns\n",
    "- **Quantum RL**: Optimization of large-scale traffic systems\n",
    "\n",
    "### 2. Financial Trading Systems\n",
    "- **World Models**: Market dynamics modeling\n",
    "- **Multi-Agent**: Multi-market trading strategies\n",
    "- **Causal RL**: Understanding causal relationships in market movements\n",
    "- **Quantum RL**: Portfolio optimization with quantum advantage\n",
    "\n",
    "### 3. Healthcare and Drug Discovery\n",
    "- **World Models**: Patient trajectory modeling\n",
    "- **Multi-Agent**: Multi-specialist treatment planning\n",
    "- **Causal RL**: Understanding treatment causality\n",
    "- **Quantum RL**: Molecular interaction simulation\n",
    "\n",
    "### 4. Climate and Environmental Management\n",
    "- **World Models**: Climate system modeling\n",
    "- **Multi-Agent**: Multi-region policy coordination\n",
    "- **Causal RL**: Climate intervention analysis\n",
    "- **Quantum RL**: Large-scale environmental optimization\n",
    "\n",
    "## Research Frontiers\n",
    "\n",
    "### 1. Theoretical Foundations\n",
    "- **Sample Complexity**: Unified bounds across paradigms\n",
    "- **Convergence Guarantees**: Multi-paradigm learning stability\n",
    "- **Transfer Learning**: Cross-paradigm knowledge transfer\n",
    "- **Meta-Learning**: Learning to choose appropriate paradigms\n",
    "\n",
    "### 2. Algorithmic Advances\n",
    "- **Hybrid Architectures**: Seamless paradigm integration\n",
    "- **Adaptive Switching**: Dynamic paradigm selection\n",
    "- **Federated Learning**: Distributed multi-paradigm training\n",
    "- **Continual Learning**: Lifelong multi-paradigm adaptation\n",
    "\n",
    "### 3. Implementation Challenges\n",
    "- **Computational Efficiency**: Scalable implementations\n",
    "- **Hardware Acceleration**: Specialized computing architectures\n",
    "- **Software Frameworks**: Unified development platforms\n",
    "- **Validation Methods**: Multi-paradigm evaluation metrics\n",
    "\n",
    "## Future Directions\n",
    "\n",
    "### Near-term (2-5 Years)\n",
    "1. **Practical Hybrid Systems**: Working implementations combining 2-3 paradigms\n",
    "2. **Industry Applications**: Deployment in specific domains\n",
    "3. **Standardization**: Common interfaces and evaluation protocols\n",
    "4. **Education**: Curriculum integration and training programs\n",
    "\n",
    "### Medium-term (5-10 Years)\n",
    "1. **Theoretical Unification**: Mathematical frameworks spanning all paradigms\n",
    "2. **Quantum Advantage**: Demonstrated speedups in real applications\n",
    "3. **Autonomous Systems**: Self-improving multi-paradigm agents\n",
    "4. **Societal Integration**: Widespread adoption across industries\n",
    "\n",
    "### Long-term (10+ Years)\n",
    "1. **Artificial General Intelligence**: Multi-paradigm foundations for AGI\n",
    "2. **Quantum-Classical Convergence**: Seamless quantum-classical computing\n",
    "3. **Causal Discovery Automation**: Fully automated causal structure learning\n",
    "4. **Multi-Agent Societies**: Complex artificial societies with emergent behavior\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This comprehensive exploration of advanced Deep Reinforcement Learning paradigms demonstrates the rich landscape of modern RL research. Each paradigm offers unique advantages:\n",
    "\n",
    "- **World Models** provide sample efficiency through learned dynamics\n",
    "- **Multi-Agent RL** enables coordination and emergence in complex systems\n",
    "- **Causal RL** offers interpretability and robustness through causal understanding\n",
    "- **Quantum RL** promises exponential advantages through quantum computation\n",
    "\n",
    "The future of reinforcement learning lies not in choosing a single paradigm, but in their thoughtful integration. By combining the strengths of each approach while mitigating their individual limitations, we can build AI systems that are:\n",
    "\n",
    "- **More Sample Efficient**: Learning faster with less data\n",
    "- **More Interpretable**: Providing clear reasoning for decisions\n",
    "- **More Robust**: Handling distribution shifts and uncertainties\n",
    "- **More Scalable**: Operating in complex, real-world environments\n",
    "\n",
    "The implementations provided in this notebook serve as stepping stones toward more sophisticated systems. While simplified for educational purposes, they demonstrate the core concepts that will drive the next generation of AI systems.\n",
    "\n",
    "As we advance toward artificial general intelligence, these paradigms will play crucial roles in creating AI systems that can understand, reason about, and operate effectively in our complex world. The journey from today's specialized RL agents to tomorrow's general AI systems will be paved with innovations across all these dimensions.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Paradigm Diversity**: Multiple approaches are needed for different aspects of intelligence\n",
    "2. **Integration Benefits**: Hybrid systems outperform single-paradigm approaches\n",
    "3. **Practical Applications**: Real-world deployment requires careful paradigm selection\n",
    "4. **Ongoing Research**: Many open questions remain in each paradigm\n",
    "5. **Future Potential**: The combination of these paradigms may enable breakthrough capabilities\n",
    "\n",
    "The field of reinforcement learning continues to evolve rapidly, and staying at the forefront requires understanding both the fundamental principles and the cutting-edge advances represented by these paradigms. This notebook provides a foundation for further exploration and implementation of these exciting directions in AI research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4ae88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all modules for integration demo\n",
    "from world_models.world_models import RSSMCore, WorldModel, MPCPlanner\n",
    "from multi_agent_rl.multi_agent_rl import MADDPGAgent, MultiAgentEnvironment\n",
    "from causal_rl.causal_rl import CausalGraph, CausalDiscovery, CausalWorldModel\n",
    "from quantum_rl.quantum_rl import QuantumState, QuantumCircuit, QuantumQLearning\n",
    "from federated_rl.federated_rl import FederatedAgent, FederatedServer\n",
    "\n",
    "def create_integrated_environment():\n",
    "    \"\"\"Create an environment that combines multiple paradigms\"\"\"\n",
    "    \n",
    "    class IntegratedEnvironment:\n",
    "        def __init__(self):\n",
    "            # Multi-agent setup\n",
    "            self.n_agents = 2\n",
    "            self.obs_dim = 6  # Shared state + private observations\n",
    "            self.action_dim = 2\n",
    "            \n",
    "            # Causal structure\n",
    "            self.causal_graph = CausalGraph(['x1', 'x2', 'y1', 'y2', 'z'])\n",
    "            \n",
    "            # Quantum state representation\n",
    "            self.quantum_state = QuantumState(n_qubits=3)\n",
    "            \n",
    "            self.max_steps = 100\n",
    "            self.reset()\n",
    "        \n",
    "        def reset(self):\n",
    "            # Initialize with causal dependencies\n",
    "            self.shared_state = np.random.normal(0, 1, 2)\n",
    "            self.agent_states = np.random.normal(0, 1, (self.n_agents, 2))\n",
    "            \n",
    "            # Update causal graph\n",
    "            self.causal_graph.update_state({\n",
    "                'x1': self.shared_state[0],\n",
    "                'x2': self.shared_state[1],\n",
    "                'y1': self.agent_states[0, 0],\n",
    "                'y2': self.agent_states[1, 0],\n",
    "                'z': np.mean(self.agent_states[:, 1])\n",
    "            })\n",
    "            \n",
    "            self.steps = 0\n",
    "            return self.get_global_observation()\n",
    "        \n",
    "        def get_global_observation(self):\n",
    "            \"\"\"Get observation for all agents\"\"\"\n",
    "            obs = []\n",
    "            for i in range(self.n_agents):\n",
    "                agent_obs = np.concatenate([\n",
    "                    self.shared_state,\n",
    "                    self.agent_states[i],\n",
    "                    [self.causal_graph.get_effect('z', f'y{i+1}')]\n",
    "                ])\n",
    "                obs.append(agent_obs)\n",
    "            return np.array(obs)\n",
    "        \n",
    "        def step(self, actions):\n",
    "            actions = np.array(actions)\n",
    "            \n",
    "            # Apply actions with causal dependencies\n",
    "            for i in range(self.n_agents):\n",
    "                action_effect = actions[i] * 0.1\n",
    "                \n",
    "                # Causal influence on shared state\n",
    "                self.shared_state += action_effect * 0.05\n",
    "                \n",
    "                # Agent-specific dynamics\n",
    "                self.agent_states[i] += action_effect + np.random.normal(0, 0.1, 2)\n",
    "            \n",
    "            # Update causal graph\n",
    "            self.causal_graph.update_state({\n",
    "                'x1': self.shared_state[0],\n",
    "                'x2': self.shared_state[1],\n",
    "                'y1': self.agent_states[0, 0],\n",
    "                'y2': self.agent_states[1, 0],\n",
    "                'z': np.mean(self.agent_states[:, 1])\n",
    "            })\n",
    "            \n",
    "            # Compute rewards (cooperative objective)\n",
    "            coordination_reward = -np.linalg.norm(self.shared_state)\n",
    "            individual_rewards = [-np.linalg.norm(actions[i]) * 0.1 for i in range(self.n_agents)]\n",
    "            \n",
    "            rewards = np.array([coordination_reward + individual_rewards[i] \n",
    "                              for i in range(self.n_agents)])\n",
    "            \n",
    "            self.steps += 1\n",
    "            done = self.steps >= self.max_steps\n",
    "            \n",
    "            return self.get_global_observation(), rewards, done, {}\n",
    "    \n",
    "    return IntegratedEnvironment()\n",
    "\n",
    "def demonstrate_paradigm_integration():\n",
    "    \"\"\"Demonstrate integration of multiple RL paradigms\"\"\"\n",
    "    \n",
    "    print(\"🔗 Demonstrating Paradigm Integration\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create integrated environment\n",
    "    env = create_integrated_environment()\n",
    "    print(f\"Integrated Environment: {env.n_agents} agents, {env.obs_dim}D obs, {env.action_dim}D actions\")\n",
    "    \n",
    "    # Initialize agents with different paradigms\n",
    "    agents = []\n",
    "    \n",
    "    # Agent 1: Multi-agent with causal reasoning\n",
    "    agent1 = MADDPGAgent(\n",
    "        agent_idx=0,\n",
    "        obs_dim=env.obs_dim,\n",
    "        action_dim=env.action_dim,\n",
    "        n_agents=env.n_agents,\n",
    "        use_attention=True,\n",
    "        use_communication=True\n",
    "    )\n",
    "    agents.append(('MADDPG+Causal', agent1))\n",
    "    \n",
    "    # Agent 2: Quantum-enhanced agent\n",
    "    agent2 = QuantumQLearning(\n",
    "        n_qubits=2,\n",
    "        action_dim=env.action_dim,\n",
    "        learning_rate=0.1,\n",
    "        discount_factor=0.95\n",
    "    )\n",
    "    agents.append(('Quantum Q-Learning', agent2))\n",
    "    \n",
    "    # Training loop\n",
    "    n_episodes = 50\n",
    "    results = {name: [] for name, _ in agents}\n",
    "    \n",
    "    for episode in range(n_episodes):\n",
    "        obs = env.reset()\n",
    "        episode_rewards = np.zeros(env.n_agents)\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            actions = []\n",
    "            \n",
    "            for i, (name, agent) in enumerate(agents):\n",
    "                if name == 'MADDPG+Causal':\n",
    "                    # Multi-agent action selection\n",
    "                    obs_tensor = torch.FloatTensor(obs[i]).unsqueeze(0).to(device)\n",
    "                    action_tensor, _ = agent.act(obs_tensor, explore=True)\n",
    "                    action = action_tensor.cpu().numpy()[0]\n",
    "                else:  # Quantum agent\n",
    "                    # Convert observation to quantum state\n",
    "                    quantum_obs = obs[i][:2]  # Use first 2 dimensions\n",
    "                    action = agent.select_action(quantum_obs)\n",
    "                \n",
    "                actions.append(action)\n",
    "            \n",
    "            next_obs, rewards, done, _ = env.step(actions)\n",
    "            \n",
    "            # Update agents\n",
    "            for i, (name, agent) in enumerate(agents):\n",
    "                if name == 'MADDPG+Causal':\n",
    "                    # Store experience for MADDPG\n",
    "                    pass  # Would need full replay buffer implementation\n",
    "                else:  # Quantum agent\n",
    "                    agent.update(obs[i][:2], actions[i], rewards[i], next_obs[i][:2], done)\n",
    "            \n",
    "            episode_rewards += rewards\n",
    "            obs = next_obs\n",
    "        \n",
    "        # Record results\n",
    "        for i, (name, _) in enumerate(agents):\n",
    "            results[name].append(episode_rewards[i])\n",
    "        \n",
    "        if episode % 10 == 0:\n",
    "            print(f\"Episode {episode}: Agent1={episode_rewards[0]:.2f}, Agent2={episode_rewards[1]:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def demonstrate_federated_learning():\n",
    "    \"\"\"Demonstrate federated RL across distributed agents\"\"\"\n",
    "    \n",
    "    print(\"\\n🌐 Demonstrating Federated Reinforcement Learning\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create federated setup\n",
    "    n_clients = 3\n",
    "    clients = []\n",
    "    \n",
    "    for i in range(n_clients):\n",
    "        client = FederatedAgent(\n",
    "            client_id=i,\n",
    "            obs_dim=4,\n",
    "            action_dim=2,\n",
    "            local_epochs=5\n",
    "        )\n",
    "        clients.append(client)\n",
    "    \n",
    "    server = FederatedServer(\n",
    "        global_model_dim=100,  # Simplified\n",
    "        n_clients=n_clients\n",
    "    )\n",
    "    \n",
    "    # Simulate federated training\n",
    "    n_rounds = 10\n",
    "    global_rewards = []\n",
    "    \n",
    "    for round_num in range(n_rounds):\n",
    "        print(f\"\\nFederated Round {round_num + 1}\")\n",
    "        \n",
    "        # Client local training\n",
    "        client_updates = []\n",
    "        client_rewards = []\n",
    "        \n",
    "        for client in clients:\n",
    "            # Simulate local environment (different for each client)\n",
    "            local_env = create_integrated_environment()\n",
    "            local_env.n_agents = 1  # Single agent per client\n",
    "            \n",
    "            # Local training\n",
    "            local_reward = client.train_local(local_env, episodes=20)\n",
    "            client_rewards.append(local_reward)\n",
    "            \n",
    "            # Generate update\n",
    "            update = client.generate_update()\n",
    "            client_updates.append(update)\n",
    "        \n",
    "        # Server aggregation\n",
    "        global_model = server.aggregate_updates(client_updates)\n",
    "        \n",
    "        # Distribute to clients\n",
    "        for client in clients:\n",
    "            client.receive_global_model(global_model)\n",
    "        \n",
    "        avg_reward = np.mean(client_rewards)\n",
    "        global_rewards.append(avg_reward)\n",
    "        \n",
    "        print(f\"Average client reward: {avg_reward:.3f}\")\n",
    "    \n",
    "    return global_rewards\n",
    "\n",
    "def create_hybrid_agent():\n",
    "    \"\"\"Create a hybrid agent combining multiple paradigms\"\"\"\n",
    "    \n",
    "    class HybridAgent:\n",
    "        def __init__(self, obs_dim, action_dim):\n",
    "            self.obs_dim = obs_dim\n",
    "            self.action_dim = action_dim\n",
    "            \n",
    "            # Components from different paradigms\n",
    "            self.world_model = WorldModel(\n",
    "                obs_dim=obs_dim,\n",
    "                action_dim=action_dim,\n",
    "                state_dim=16,\n",
    "                hidden_dim=32,\n",
    "                embed_dim=64\n",
    "            )\n",
    "            \n",
    "            self.causal_reasoner = CausalDiscovery(\n",
    "                variables=[f'obs_{i}' for i in range(obs_dim)] + \n",
    "                         [f'action_{i}' for i in range(action_dim)],\n",
    "                alpha=0.1\n",
    "            )\n",
    "            \n",
    "            self.quantum_processor = QuantumCircuit(n_qubits=2)\n",
    "            \n",
    "            # Classical policy network\n",
    "            self.policy_net = nn.Sequential(\n",
    "                nn.Linear(obs_dim + 16, 64),  # obs + world model state\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(64, action_dim),\n",
    "                nn.Tanh()\n",
    "            ).to(device)\n",
    "        \n",
    "        def select_action(self, obs, explore=True):\n",
    "            obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "            \n",
    "            # Get world model prediction\n",
    "            with torch.no_grad():\n",
    "                wm_output = self.world_model.observe_sequence(obs_tensor, \n",
    "                    torch.zeros(1, self.action_dim).to(device))\n",
    "                wm_state = wm_output['states'][:, -1]  # Last state\n",
    "            \n",
    "            # Combine with observation\n",
    "            combined_input = torch.cat([obs_tensor, wm_state], dim=-1)\n",
    "            \n",
    "            # Get action\n",
    "            with torch.no_grad():\n",
    "                action = self.policy_net(combined_input).cpu().numpy()[0]\n",
    "            \n",
    "            if explore:\n",
    "                action += np.random.normal(0, 0.1, self.action_dim)\n",
    "                action = np.clip(action, -1, 1)\n",
    "            \n",
    "            return action\n",
    "        \n",
    "        def update_causal_model(self, experience_batch):\n",
    "            \"\"\"Update causal understanding from experience\"\"\"\n",
    "            # Extract causal relationships from recent experiences\n",
    "            data = np.array([exp['obs'] + exp['action'] for exp in experience_batch])\n",
    "            self.causal_reasoner.discover_structure(data)\n",
    "    \n",
    "    return HybridAgent(obs_dim=6, action_dim=2)\n",
    "\n",
    "print(\"🚀 Starting Exercise 4: Paradigm Integration and Advanced Applications\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Demonstrate paradigm integration\n",
    "integration_results = demonstrate_paradigm_integration()\n",
    "\n",
    "# Demonstrate federated learning\n",
    "federated_rewards = demonstrate_federated_learning()\n",
    "\n",
    "# Create and test hybrid agent\n",
    "hybrid_agent = create_hybrid_agent()\n",
    "\n",
    "# Test hybrid agent\n",
    "test_env = create_integrated_environment()\n",
    "test_env.n_agents = 1\n",
    "\n",
    "hybrid_rewards = []\n",
    "for episode in range(20):\n",
    "    obs = test_env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    experience_batch = []\n",
    "    \n",
    "    while not done:\n",
    "        action = hybrid_agent.select_action(obs[0])\n",
    "        next_obs, reward, done, _ = test_env.step([action])\n",
    "        \n",
    "        experience_batch.append({\n",
    "            'obs': obs[0],\n",
    "            'action': action,\n",
    "            'reward': reward[0],\n",
    "            'next_obs': next_obs[0]\n",
    "        })\n",
    "        \n",
    "        episode_reward += reward[0]\n",
    "        obs = next_obs\n",
    "    \n",
    "    # Update causal model periodically\n",
    "    if episode % 5 == 0:\n",
    "        hybrid_agent.update_causal_model(experience_batch[-10:])\n",
    "    \n",
    "    hybrid_rewards.append(episode_reward)\n",
    "\n",
    "print(f\"\\nHybrid Agent Performance: {np.mean(hybrid_rewards):.2f} ± {np.std(hybrid_rewards):.2f}\")\n",
    "\n",
    "# Visualization\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Integration results\n",
    "for name, rewards in integration_results.items():\n",
    "    ax1.plot(rewards, label=name, alpha=0.7)\n",
    "ax1.set_title('Multi-Paradigm Agent Performance')\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Episode Reward')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Federated learning\n",
    "ax2.plot(federated_rewards, marker='o')\n",
    "ax2.set_title('Federated Learning Progress')\n",
    "ax2.set_xlabel('Communication Round')\n",
    "ax2.set_ylabel('Average Client Reward')\n",
    "ax2.grid(True)\n",
    "\n",
    "# Hybrid agent\n",
    "ax3.plot(hybrid_rewards, color='purple')\n",
    "ax3.set_title('Hybrid Agent Learning Curve')\n",
    "ax3.set_xlabel('Episode')\n",
    "ax3.set_ylabel('Episode Reward')\n",
    "ax3.grid(True)\n",
    "\n",
    "# Performance comparison\n",
    "methods = ['MADDPG+Causal', 'Quantum Q-Learning', 'Federated RL', 'Hybrid Agent']\n",
    "final_performances = [\n",
    "    np.mean(integration_results['MADDPG+Causal'][-10:]),\n",
    "    np.mean(integration_results['Quantum Q-Learning'][-10:]),\n",
    "    federated_rewards[-1],\n",
    "    np.mean(hybrid_rewards[-10:])\n",
    "]\n",
    "\n",
    "ax4.bar(range(len(methods)), final_performances, alpha=0.7, color=['blue', 'red', 'green', 'purple'])\n",
    "ax4.set_title('Final Performance Comparison')\n",
    "ax4.set_xlabel('Method')\n",
    "ax4.set_ylabel('Average Reward')\n",
    "ax4.set_xticks(range(len(methods)))\n",
    "ax4.set_xticklabels(methods, rotation=45, ha='right')\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ Exercise 4 Complete!\")\n",
    "print(\"Key learnings:\")\n",
    "print(\"- Multiple RL paradigms can be integrated for complementary strengths\")\n",
    "print(\"- Federated learning enables privacy-preserving distributed training\")\n",
    "print(\"- Hybrid agents combine world models, causal reasoning, and quantum processing\")\n",
    "print(\"- Integration approaches show promise for tackling complex real-world problems\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58910b4f",
   "metadata": {},
   "source": [
    "# Code Review and Improvements\n",
    "\n",
    "## Implementation Analysis\n",
    "\n",
    "### Positives\n",
    "- Demonstrates a clear experimental methodology and documents hyperparameters and evaluation metrics.\n",
    "- Good use of modular components enabling swapping of policy, value, and dynamics models.\n",
    "- Includes helpful visual diagnostics for training stability and performance.\n",
    "\n",
    "### Recommended Improvements\n",
    "- Add more rigorous unit and integration tests for core modules (agents, buffers, networks).\n",
    "- Adopt a standardized configuration system (e.g., Hydra or simple YAML configs) to manage experiments and parameter sweeps.\n",
    "- Provide an automated script for hyperparameter sweeps (e.g., using Optuna or Ray Tune).\n",
    "\n",
    "## Algorithmic Enhancements\n",
    "- Implement robust exploration strategies (parameter noise, entropy bonuses, or exploration from ensemble models).\n",
    "- Use ensemble dynamics or Bayesian approaches to capture model uncertainty for safer planning.\n",
    "- For high-dimensional observations, use representation learning (autoencoders, contrastive methods) prior to policy learning.\n",
    "\n",
    "## Performance and Resource Management\n",
    "- Use profiling (torch.profiler) to find bottlenecks and optimize data loaders and model ops.\n",
    "- Add checkpoint rotation and retention policies to avoid disk bloat during long experiments.\n",
    "\n",
    "## Reproducibility and Experiment Tracking\n",
    "- Add a `run_experiment.py` entrypoint that records experiment metadata and saves reproducible configs.\n",
    "- Integrate with W&B or a self-hosted logging solution and save logs/artifacts to a structured folder per run.\n",
    "\n",
    "## Production & Serving\n",
    "- Add example code to deploy a trained policy as a deterministic policy network for real-time inference.\n",
    "- Consider model quantization for running on edge devices; include a short guide and example script.\n",
    "\n",
    "## Future Directions\n",
    "- Multi-task or meta-learning extensions to enable rapid generalization across environments.\n",
    "- Explore model-based/model-free hybrids for better sample efficiency.\n",
    "\n",
    "Follow these steps to make CA18 more robust for research, reproducible for experiments, and practical for deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
