{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baa92e7d",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning - Session 3\n",
    "## Temporal Difference Learning and Q-Learning\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will understand:\n",
    "\n",
    "**Core Concepts:**\n",
    "- **Temporal Difference (TD) Learning**: Learning from experience without knowing the model\n",
    "- **Q-Learning Algorithm**: Off-policy TD control for finding optimal policies\n",
    "- **SARSA Algorithm**: On-policy TD control method\n",
    "- **Exploration vs Exploitation**: Balancing learning and performance\n",
    "\n",
    "**Practical Skills:**\n",
    "- Implement TD(0) for policy evaluation\n",
    "- Build Q-Learning agent from scratch\n",
    "- Compare SARSA and Q-Learning performance\n",
    "- Design exploration strategies (epsilon-greedy, decaying epsilon)\n",
    "- Analyze convergence and learning curves\n",
    "\n",
    "**Real-World Applications:**\n",
    "- Game playing (Chess, Go, Atari games)\n",
    "- Robotics control and navigation\n",
    "- Resource allocation and scheduling\n",
    "- Autonomous trading systems\n",
    "\n",
    "---\n",
    "\n",
    "## Session Overview\n",
    "\n",
    "1. **Part 1**: From Dynamic Programming to Temporal Difference\n",
    "2. **Part 2**: TD(0) Learning - Bootstrapping from Experience\n",
    "3. **Part 3**: Q-Learning - Off-Policy Control\n",
    "4. **Part 4**: SARSA - On-Policy Control\n",
    "5. **Part 5**: Exploration Strategies\n",
    "6. **Part 6**: Comparative Analysis and Experiments\n",
    "\n",
    "---\n",
    "\n",
    "## Transition from Session 2\n",
    "\n",
    "**Previous Session (Session 2):**\n",
    "- MDPs and Bellman equations\n",
    "- Policy evaluation and improvement\n",
    "- **Model-based** approaches (knowing P and R)\n",
    "\n",
    "**Current Session (Session 3):**\n",
    "- **Model-free** learning (no knowledge of P and R)\n",
    "- Learning directly from experience\n",
    "- Online learning algorithms\n",
    "\n",
    "**Key Transition:**\n",
    "From \"I know the environment model\" to \"I learn by trying actions and observing results\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be8ae1",
   "metadata": {},
   "source": [
    "## Part 1: Introduction to Temporal Difference Learning\n",
    "\n",
    "### The Limitation of Dynamic Programming\n",
    "\n",
    "In Session 2, we used **Dynamic Programming** methods like Policy Iteration, which required:\n",
    "- Complete knowledge of the environment model (transition probabilities P(s'|s,a))\n",
    "- Complete knowledge of reward function R(s,a,s')\n",
    "- Ability to sweep through all states multiple times\n",
    "\n",
    "**Real-World Challenge**: In most practical scenarios, we don't have complete knowledge of the environment.\n",
    "\n",
    "### What is Temporal Difference Learning?\n",
    "\n",
    "**Temporal Difference (TD) Learning** is a method that combines ideas from:\n",
    "- **Monte Carlo methods**: Learning from experience samples\n",
    "- **Dynamic Programming**: Bootstrapping from current estimates\n",
    "\n",
    "**Key Principle**: Update value estimates based on observed transitions, without needing the complete model.\n",
    "\n",
    "### Core TD Concept: Bootstrapping\n",
    "\n",
    "Instead of waiting for complete episodes (Monte Carlo), TD methods update estimates using:\n",
    "- **Current estimate**: V(s_t)\n",
    "- **Observed reward**: R_{t+1}\n",
    "- **Next state estimate**: V(s_{t+1})\n",
    "\n",
    "**TD Update Rule**:\n",
    "```\n",
    "V(s_t) ← V(s_t) + α[R_{t+1} + γV(s_{t+1}) - V(s_t)]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- α (alpha): Learning rate (0 < α ≤ 1)\n",
    "- γ (gamma): Discount factor\n",
    "- [R_{t+1} + γV(s_{t+1}) - V(s_t)]: **TD Error**\n",
    "\n",
    "### The Three Learning Paradigms\n",
    "\n",
    "| Method | Model Required | Update Frequency | Variance | Bias |\n",
    "|--------|----------------|------------------|----------|------|\n",
    "| **Dynamic Programming** | Yes | After full sweep | None | None (exact) |\n",
    "| **Monte Carlo** | No | After episode | High | None |\n",
    "| **Temporal Difference** | No | After each step | Low | Some (bootstrap) |\n",
    "\n",
    "### TD Learning Advantages\n",
    "\n",
    "1. **Online Learning**: Can learn while interacting with environment\n",
    "2. **No Model Required**: Works without knowing P(s'|s,a) or R(s,a,s')\n",
    "3. **Lower Variance**: More stable than Monte Carlo\n",
    "4. **Faster Learning**: Updates after each step, not episode\n",
    "\n",
    "### Real-World Analogy: Restaurant Reviews\n",
    "\n",
    "**Monte Carlo**: Read all reviews after trying every dish (complete episode)\n",
    "**TD Learning**: Update opinion about restaurant after each dish, considering what you expect from remaining dishes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a799463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import random\n",
    "from collections import defaultdict, deque\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Environment configured for Temporal Difference Learning\")\n",
    "print(\"Session 3: Ready to explore model-free reinforcement learning!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776122ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    \"\"\"\n",
    "    GridWorld environment for demonstrating TD learning algorithms\n",
    "    Modified from Session 2 to support episodic interaction\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, size=4, goal_reward=10, step_reward=-0.1, obstacle_reward=-5):\n",
    "        self.size = size\n",
    "        self.goal_reward = goal_reward\n",
    "        self.step_reward = step_reward\n",
    "        self.obstacle_reward = obstacle_reward\n",
    "        \n",
    "        self.states = [(i, j) for i in range(size) for j in range(size)]\n",
    "        \n",
    "        self.actions = ['up', 'down', 'left', 'right']\n",
    "        self.action_effects = {\n",
    "            'up': (-1, 0),\n",
    "            'down': (1, 0),\n",
    "            'left': (0, -1),\n",
    "            'right': (0, 1)\n",
    "        }\n",
    "        \n",
    "        self.start_state = (0, 0)\n",
    "        self.goal_state = (3, 3)\n",
    "        self.obstacles = [(1, 1), (2, 1), (1, 2)]\n",
    "        \n",
    "        self.current_state = self.start_state\n",
    "        \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to start state\"\"\"\n",
    "        self.current_state = self.start_state\n",
    "        return self.current_state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take action and return (next_state, reward, done, info)\n",
    "        Compatible with standard RL environment interface\n",
    "        \"\"\"\n",
    "        if self.is_terminal(self.current_state):\n",
    "            return self.current_state, 0, True, {}\n",
    "        \n",
    "        dx, dy = self.action_effects[action]\n",
    "        next_x, next_y = self.current_state[0] + dx, self.current_state[1] + dy\n",
    "        \n",
    "        if not (0 <= next_x < self.size and 0 <= next_y < self.size):\n",
    "            next_state = self.current_state  # Stay in place\n",
    "        else:\n",
    "            next_state = (next_x, next_y)\n",
    "        \n",
    "        if next_state == self.goal_state:\n",
    "            reward = self.goal_reward\n",
    "        elif next_state in self.obstacles:\n",
    "            reward = self.obstacle_reward\n",
    "            next_state = self.current_state  # Can't move into obstacle\n",
    "        else:\n",
    "            reward = self.step_reward\n",
    "        \n",
    "        done = (next_state == self.goal_state)\n",
    "        \n",
    "        self.current_state = next_state\n",
    "        \n",
    "        return next_state, reward, done, {}\n",
    "    \n",
    "    def get_valid_actions(self, state):\n",
    "        \"\"\"Get valid actions from a state\"\"\"\n",
    "        if self.is_terminal(state):\n",
    "            return []\n",
    "        return self.actions\n",
    "    \n",
    "    def is_terminal(self, state):\n",
    "        \"\"\"Check if state is terminal\"\"\"\n",
    "        return state == self.goal_state\n",
    "    \n",
    "    def visualize_values(self, values, title=\"State Values\", policy=None):\n",
    "        \"\"\"Visualize state values and optional policy\"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        for i, j in self.obstacles:\n",
    "            grid[i, j] = min(values.values()) - 1  # Make obstacles darker\n",
    "        \n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                state = (i, j)\n",
    "                if state not in self.obstacles:\n",
    "                    grid[i, j] = values.get(state, 0)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        im = ax.imshow(grid, cmap='RdYlGn', aspect='equal')\n",
    "        \n",
    "        arrow_map = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                state = (i, j)\n",
    "                if state == self.goal_state:\n",
    "                    ax.text(j, i, 'G', ha='center', va='center', \n",
    "                           fontsize=16, fontweight='bold', color='darkgreen')\n",
    "                elif state in self.obstacles:\n",
    "                    ax.text(j, i, 'X', ha='center', va='center', \n",
    "                           fontsize=16, fontweight='bold', color='darkred')\n",
    "                elif state == self.start_state:\n",
    "                    ax.text(j, i-0.3, 'S', ha='center', va='center', \n",
    "                           fontsize=12, fontweight='bold', color='blue')\n",
    "                    ax.text(j, i+0.2, f'{values.get(state, 0):.1f}', \n",
    "                           ha='center', va='center', fontsize=10)\n",
    "                else:\n",
    "                    ax.text(j, i, f'{values.get(state, 0):.1f}', \n",
    "                           ha='center', va='center', fontsize=10)\n",
    "                \n",
    "                if policy and state in policy and not self.is_terminal(state):\n",
    "                    action = policy[state]\n",
    "                    if action in arrow_map:\n",
    "                        ax.text(j+0.3, i-0.3, arrow_map[action], \n",
    "                               ha='center', va='center', fontsize=8, color='blue')\n",
    "        \n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.set_xticks(range(self.size))\n",
    "        ax.set_yticks(range(self.size))\n",
    "        plt.colorbar(im, ax=ax)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "env = GridWorld()\n",
    "print(\"GridWorld environment created!\")\n",
    "print(f\"State space: {len(env.states)} states\")\n",
    "print(f\"Action space: {len(env.actions)} actions\")\n",
    "print(f\"Start state: {env.start_state}\")\n",
    "print(f\"Goal state: {env.goal_state}\")\n",
    "print(f\"Obstacles: {env.obstacles}\")\n",
    "\n",
    "state = env.reset()\n",
    "print(f\"\\nEnvironment reset. Current state: {state}\")\n",
    "next_state, reward, done, info = env.step('right')\n",
    "print(f\"Action 'right': next_state={next_state}, reward={reward}, done={done}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc84925",
   "metadata": {},
   "source": [
    "## Part 2: TD(0) Learning - Policy Evaluation\n",
    "\n",
    "### Understanding TD(0) Algorithm\n",
    "\n",
    "**TD(0)** is the simplest temporal difference method for policy evaluation. It updates value estimates after each step using the observed reward and the current estimate of the next state.\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "**Bellman Equation for V^π(s)**:\n",
    "```\n",
    "V^π(s) = E[R_{t+1} + γV^π(S_{t+1}) | S_t = s]\n",
    "```\n",
    "\n",
    "**TD(0) Update Rule**:\n",
    "```\n",
    "V(S_t) ← V(S_t) + α[R_{t+1} + γV(S_{t+1}) - V(S_t)]\n",
    "```\n",
    "\n",
    "**Components**:\n",
    "- **V(S_t)**: Current value estimate\n",
    "- **α**: Learning rate (step size)\n",
    "- **R_{t+1}**: Observed immediate reward\n",
    "- **γ**: Discount factor\n",
    "- **TD Target**: R_{t+1} + γV(S_{t+1})\n",
    "- **TD Error**: R_{t+1} + γV(S_{t+1}) - V(S_t)\n",
    "\n",
    "### TD(0) vs Other Methods\n",
    "\n",
    "| Aspect | Monte Carlo | TD(0) | Dynamic Programming |\n",
    "|--------|-------------|-------|-------------------|\n",
    "| **Model** | Not required | Not required | Required |\n",
    "| **Update** | End of episode | Every step | Full sweep |\n",
    "| **Target** | Actual return G_t | R_{t+1} + γV(S_{t+1}) | Expected value |\n",
    "| **Bias** | Unbiased | Biased (bootstrap) | Unbiased |\n",
    "| **Variance** | High | Low | None |\n",
    "\n",
    "### Key Properties of TD(0)\n",
    "\n",
    "1. **Bootstrapping**: Uses current estimates to update estimates\n",
    "2. **Online Learning**: Can learn during interaction\n",
    "3. **Model-Free**: No need for transition probabilities\n",
    "4. **Convergence**: Converges to V^π under certain conditions\n",
    "\n",
    "### Learning Rate (α) Impact\n",
    "\n",
    "- **High α (e.g., 0.8)**: Fast learning, high sensitivity to recent experience\n",
    "- **Low α (e.g., 0.1)**: Slow learning, more stable, averages over many experiences\n",
    "- **Optimal α**: Often requires tuning based on problem characteristics\n",
    "\n",
    "### Convergence Conditions\n",
    "\n",
    "TD(0) converges to V^π if:\n",
    "1. Policy π is fixed\n",
    "2. Learning rate α satisfies: Σα_t = ∞ and Σα_t² < ∞\n",
    "3. All state-action pairs are visited infinitely often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD0Agent:\n",
    "    \"\"\"\n",
    "    TD(0) agent for policy evaluation\n",
    "    Learns state values V(s) for a given policy\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, policy, alpha=0.1, gamma=0.9):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        \n",
    "        self.V = defaultdict(float)\n",
    "        \n",
    "        self.episode_rewards = []\n",
    "        self.value_history = []\n",
    "        \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Get action from policy\"\"\"\n",
    "        if hasattr(self.policy, 'get_action'):\n",
    "            return self.policy.get_action(state)\n",
    "        else:\n",
    "            valid_actions = self.env.get_valid_actions(state)\n",
    "            return np.random.choice(valid_actions) if valid_actions else None\n",
    "    \n",
    "    def td_update(self, state, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Perform TD(0) update\n",
    "        V(s) ← V(s) + α[R + γV(s') - V(s)]\n",
    "        \"\"\"\n",
    "        if done:\n",
    "            td_target = reward  # No next state value for terminal states\n",
    "        else:\n",
    "            td_target = reward + self.gamma * self.V[next_state]\n",
    "        \n",
    "        td_error = td_target - self.V[state]\n",
    "        self.V[state] += self.alpha * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def run_episode(self, max_steps=100):\n",
    "        \"\"\"Run one episode and learn\"\"\"\n",
    "        state = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            action = self.get_action(state)\n",
    "            if action is None:\n",
    "                break\n",
    "            \n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            td_error = self.td_update(state, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return episode_reward, steps\n",
    "    \n",
    "    def train(self, num_episodes=1000, print_every=100):\n",
    "        \"\"\"Train the agent over multiple episodes\"\"\"\n",
    "        print(f\"Training TD(0) agent for {num_episodes} episodes...\")\n",
    "        print(f\"Learning rate α = {self.alpha}, Discount factor γ = {self.gamma}\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            episode_reward, steps = self.run_episode()\n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            \n",
    "            if episode % 10 == 0:\n",
    "                self.value_history.append(dict(self.V))\n",
    "            \n",
    "            if (episode + 1) % print_every == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-print_every:])\n",
    "                print(f\"Episode {episode + 1}: Average reward = {avg_reward:.2f}\")\n",
    "        \n",
    "        print(\"Training completed!\")\n",
    "        return self.V\n",
    "    \n",
    "    def get_value_function(self):\n",
    "        \"\"\"Get current value function as dictionary\"\"\"\n",
    "        return dict(self.V)\n",
    "\n",
    "class RandomPolicy:\n",
    "    \"\"\"Random policy for testing TD(0)\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Return random valid action\"\"\"\n",
    "        valid_actions = self.env.get_valid_actions(state)\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        return np.random.choice(valid_actions)\n",
    "\n",
    "print(\"Creating TD(0) agent with random policy...\")\n",
    "\n",
    "random_policy = RandomPolicy(env)\n",
    "td_agent = TD0Agent(env, random_policy, alpha=0.1, gamma=0.9)\n",
    "\n",
    "print(\"TD(0) agent created successfully!\")\n",
    "print(f\"Initial value function (should be all zeros): {len(td_agent.V)} states initialized\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1143b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training TD(0) agent...\")\n",
    "V_td = td_agent.train(num_episodes=500, print_every=100)\n",
    "\n",
    "print(\"\\nLearned Value Function:\")\n",
    "env.visualize_values(V_td, title=\"TD(0) Learned Value Function - Random Policy\")\n",
    "\n",
    "def plot_learning_curve(episode_rewards, title=\"Learning Curve\"):\n",
    "    \"\"\"Plot learning curve showing episode rewards over time\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(episode_rewards, alpha=0.6, color='blue', linewidth=0.8)\n",
    "    \n",
    "    window_size = 50\n",
    "    if len(episode_rewards) >= window_size:\n",
    "        moving_avg = []\n",
    "        for i in range(len(episode_rewards)):\n",
    "            start_idx = max(0, i - window_size + 1)\n",
    "            moving_avg.append(np.mean(episode_rewards[start_idx:i+1]))\n",
    "        plt.plot(moving_avg, color='red', linewidth=2, label=f'Moving Average ({window_size} episodes)')\n",
    "        plt.legend()\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode Reward')\n",
    "    plt.title(f'{title} - Episode Rewards')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(episode_rewards, bins=30, alpha=0.7, color='green', edgecolor='black')\n",
    "    plt.xlabel('Episode Reward')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Reward Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Learning Statistics:\")\n",
    "    print(f\"Total episodes: {len(episode_rewards)}\")\n",
    "    print(f\"Average reward: {np.mean(episode_rewards):.2f}\")\n",
    "    print(f\"Reward std: {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"Min reward: {np.min(episode_rewards):.2f}\")\n",
    "    print(f\"Max reward: {np.max(episode_rewards):.2f}\")\n",
    "\n",
    "plot_learning_curve(td_agent.episode_rewards, \"TD(0) Learning\")\n",
    "\n",
    "key_states = [(0, 0), (1, 0), (2, 0), (3, 2), (2, 2)]\n",
    "print(f\"\\nLearned values for key states:\")\n",
    "print(\"State\\t\\tTD(0) Value\")\n",
    "print(\"-\" * 30)\n",
    "for state in key_states:\n",
    "    if state in V_td:\n",
    "        print(f\"{state}\\t\\t{V_td[state]:.3f}\")\n",
    "    else:\n",
    "        print(f\"{state}\\t\\t0.000\")\n",
    "\n",
    "print(f\"\\nTD(0) Value Function Learning Complete!\")\n",
    "print(f\"The agent learned state values through interaction with the environment.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a69e6d3",
   "metadata": {},
   "source": [
    "## Part 3: Q-Learning - Off-Policy Control\n",
    "\n",
    "### From Policy Evaluation to Control\n",
    "\n",
    "**TD(0)** solves the **policy evaluation** problem: given a policy π, learn V^π(s).\n",
    "\n",
    "**Q-Learning** solves the **control** problem: find the optimal policy π* and optimal action-value function Q*(s,a).\n",
    "\n",
    "### Q-Learning Algorithm\n",
    "\n",
    "**Objective**: Learn Q*(s,a) = optimal action-value function\n",
    "\n",
    "**Q-Learning Update Rule**:\n",
    "```\n",
    "Q(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]\n",
    "```\n",
    "\n",
    "**Key Components**:\n",
    "- **Q(S_t, A_t)**: Current Q-value estimate\n",
    "- **α**: Learning rate\n",
    "- **R_{t+1}**: Observed reward\n",
    "- **γ**: Discount factor\n",
    "- **max_a Q(S_{t+1}, a)**: Maximum Q-value for next state (greedy action)\n",
    "- **TD Target**: R_{t+1} + γ max_a Q(S_{t+1}, a)\n",
    "- **TD Error**: R_{t+1} + γ max_a Q(S_{t+1}, a) - Q(S_t, A_t)\n",
    "\n",
    "### Off-Policy Nature\n",
    "\n",
    "**Q-Learning is Off-Policy**:\n",
    "- **Behavior Policy**: The policy used to generate actions (e.g., ε-greedy)\n",
    "- **Target Policy**: The policy being learned (greedy w.r.t. Q)\n",
    "- **Independence**: Can learn optimal policy while following exploratory policy\n",
    "\n",
    "### Q-Learning vs SARSA Comparison\n",
    "\n",
    "| Aspect | Q-Learning | SARSA |\n",
    "|--------|------------|--------|\n",
    "| **Type** | Off-policy | On-policy |\n",
    "| **Update Target** | max_a Q(s',a) | Q(s',a') where a' ~ π |\n",
    "| **Policy Learned** | Optimal (greedy) | Current policy |\n",
    "| **Exploration Impact** | No direct impact on target | Affects learning target |\n",
    "| **Convergence** | To Q* under conditions | To Q^π of current policy |\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "**Bellman Optimality Equation**:\n",
    "```\n",
    "Q*(s,a) = E[R_{t+1} + γ max_{a'} Q*(S_{t+1}, a') | S_t = s, A_t = a]\n",
    "```\n",
    "\n",
    "**Q-Learning approximates this by**:\n",
    "1. Using sample transitions instead of expectations\n",
    "2. Using current Q estimates instead of true Q*\n",
    "3. Updating incrementally with learning rate α\n",
    "\n",
    "### Convergence Properties\n",
    "\n",
    "Q-Learning converges to Q* under these conditions:\n",
    "1. **Infinite exploration**: All state-action pairs visited infinitely often\n",
    "2. **Learning rate conditions**: Σα_t = ∞ and Σα_t² < ∞\n",
    "3. **Bounded rewards**: |R| ≤ R_max < ∞\n",
    "\n",
    "### Exploration-Exploitation Trade-off\n",
    "\n",
    "**Problem**: Pure greedy policy may never discover optimal actions\n",
    "\n",
    "**Solution**: ε-greedy policy\n",
    "- With probability ε: Choose random action (explore)\n",
    "- With probability 1-ε: Choose greedy action (exploit)\n",
    "\n",
    "**ε-greedy variants**:\n",
    "- **Fixed ε**: Constant exploration rate\n",
    "- **Decaying ε**: ε decreases over time (ε_t = ε_0 / (1 + decay_rate * t))\n",
    "- **Adaptive ε**: ε based on learning progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60fffb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Q-Learning agent for finding optimal policy\n",
    "    Learns Q*(s,a) through off-policy temporal difference learning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.alpha = alpha          # Learning rate\n",
    "        self.gamma = gamma          # Discount factor\n",
    "        self.epsilon = epsilon      # Exploration rate\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.epsilon_history = []\n",
    "        self.q_value_history = []\n",
    "        \n",
    "    def get_action(self, state, explore=True):\n",
    "        \"\"\"\n",
    "        Get action using ε-greedy policy\n",
    "        \"\"\"\n",
    "        if not explore:\n",
    "            return self.get_greedy_action(state)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            valid_actions = self.env.get_valid_actions(state)\n",
    "            return np.random.choice(valid_actions) if valid_actions else None\n",
    "        else:\n",
    "            return self.get_greedy_action(state)\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\"Get greedy action (highest Q-value)\"\"\"\n",
    "        valid_actions = self.env.get_valid_actions(state)\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        \n",
    "        q_values = {action: self.Q[state][action] for action in valid_actions}\n",
    "        max_q = max(q_values.values())\n",
    "        \n",
    "        best_actions = [action for action, q in q_values.items() if q == max_q]\n",
    "        return np.random.choice(best_actions)\n",
    "    \n",
    "    def update_q(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Q-Learning update:\n",
    "        Q(s,a) ← Q(s,a) + α[R + γ max_a' Q(s',a') - Q(s,a)]\n",
    "        \"\"\"\n",
    "        current_q = self.Q[state][action]\n",
    "        \n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            valid_next_actions = self.env.get_valid_actions(next_state)\n",
    "            if valid_next_actions:\n",
    "                max_next_q = max([self.Q[next_state][a] for a in valid_next_actions])\n",
    "            else:\n",
    "                max_next_q = 0.0\n",
    "            td_target = reward + self.gamma * max_next_q\n",
    "        \n",
    "        td_error = td_target - current_q\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def run_episode(self, max_steps=200):\n",
    "        \"\"\"Run one episode and learn\"\"\"\n",
    "        state = self.env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < max_steps:\n",
    "            action = self.get_action(state, explore=True)\n",
    "            if action is None:\n",
    "                break\n",
    "            \n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            td_error = self.update_q(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return episode_reward, steps\n",
    "    \n",
    "    def train(self, num_episodes=1000, print_every=100):\n",
    "        \"\"\"Train the Q-learning agent\"\"\"\n",
    "        print(f\"Training Q-Learning agent for {num_episodes} episodes...\")\n",
    "        print(f\"Parameters: α={self.alpha}, γ={self.gamma}, ε={self.epsilon}\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            episode_reward, steps = self.run_episode()\n",
    "            \n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_steps.append(steps)\n",
    "            self.epsilon_history.append(self.epsilon)\n",
    "            \n",
    "            if episode % 50 == 0:\n",
    "                q_snapshot = {}\n",
    "                for state in self.env.states:\n",
    "                    q_snapshot[state] = dict(self.Q[state])\n",
    "                self.q_value_history.append(q_snapshot)\n",
    "            \n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            if (episode + 1) % print_every == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-print_every:])\n",
    "                avg_steps = np.mean(self.episode_steps[-print_every:])\n",
    "                print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}, \"\n",
    "                      f\"Avg Steps = {avg_steps:.1f}, ε = {self.epsilon:.3f}\")\n",
    "        \n",
    "        print(\"Q-Learning training completed!\")\n",
    "    \n",
    "    def get_value_function(self):\n",
    "        \"\"\"Extract value function V*(s) = max_a Q*(s,a)\"\"\"\n",
    "        V = {}\n",
    "        for state in self.env.states:\n",
    "            valid_actions = self.env.get_valid_actions(state)\n",
    "            if valid_actions:\n",
    "                V[state] = max([self.Q[state][action] for action in valid_actions])\n",
    "            else:\n",
    "                V[state] = 0.0\n",
    "        return V\n",
    "    \n",
    "    def get_policy(self):\n",
    "        \"\"\"Extract optimal policy π*(s) = argmax_a Q*(s,a)\"\"\"\n",
    "        policy = {}\n",
    "        for state in self.env.states:\n",
    "            if not self.env.is_terminal(state):\n",
    "                policy[state] = self.get_greedy_action(state)\n",
    "        return policy\n",
    "    \n",
    "    def evaluate_policy(self, num_episodes=100):\n",
    "        \"\"\"Evaluate learned policy (no exploration)\"\"\"\n",
    "        rewards = []\n",
    "        steps_list = []\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < 200:\n",
    "                action = self.get_action(state, explore=False)  # No exploration\n",
    "                if action is None:\n",
    "                    break\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            rewards.append(episode_reward)\n",
    "            steps_list.append(steps)\n",
    "        \n",
    "        return {\n",
    "            'avg_reward': np.mean(rewards),\n",
    "            'std_reward': np.std(rewards),\n",
    "            'avg_steps': np.mean(steps_list),\n",
    "            'success_rate': sum(1 for r in rewards if r > 5) / len(rewards)\n",
    "        }\n",
    "\n",
    "print(\"Creating Q-Learning agent...\")\n",
    "q_agent = QLearningAgent(env, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.995)\n",
    "print(\"Q-Learning agent created successfully!\")\n",
    "print(\"Ready to learn optimal Q-function Q*(s,a)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc1dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Q-Learning agent...\")\n",
    "q_agent.train(num_episodes=1000, print_every=200)\n",
    "\n",
    "V_optimal = q_agent.get_value_function()\n",
    "optimal_policy = q_agent.get_policy()\n",
    "\n",
    "print(\"\\nLearned Optimal Value Function V*(s):\")\n",
    "env.visualize_values(V_optimal, title=\"Q-Learning: Optimal Value Function V*\", policy=optimal_policy)\n",
    "\n",
    "print(\"\\nEvaluating learned policy...\")\n",
    "evaluation = q_agent.evaluate_policy(num_episodes=100)\n",
    "print(f\"Policy Evaluation Results:\")\n",
    "print(f\"Average reward: {evaluation['avg_reward']:.2f} ± {evaluation['std_reward']:.2f}\")\n",
    "print(f\"Average steps to goal: {evaluation['avg_steps']:.1f}\")\n",
    "print(f\"Success rate: {evaluation['success_rate']*100:.1f}%\")\n",
    "\n",
    "def plot_q_learning_analysis(agent):\n",
    "    \"\"\"Comprehensive analysis of Q-Learning performance\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.plot(agent.episode_rewards, alpha=0.6, color='blue', linewidth=0.8, label='Episode Reward')\n",
    "    \n",
    "    window = 50\n",
    "    if len(agent.episode_rewards) >= window:\n",
    "        moving_avg = pd.Series(agent.episode_rewards).rolling(window=window).mean()\n",
    "        ax1.plot(moving_avg, color='red', linewidth=2, label=f'Moving Average ({window})')\n",
    "    \n",
    "    ax1.set_xlabel('Episode')\n",
    "    ax1.set_ylabel('Episode Reward')\n",
    "    ax1.set_title('Q-Learning: Episode Rewards')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.plot(agent.episode_steps, alpha=0.7, color='green', linewidth=0.8)\n",
    "    \n",
    "    if len(agent.episode_steps) >= window:\n",
    "        steps_avg = pd.Series(agent.episode_steps).rolling(window=window).mean()\n",
    "        ax2.plot(steps_avg, color='darkgreen', linewidth=2, label=f'Moving Average ({window})')\n",
    "    \n",
    "    ax2.set_xlabel('Episode')\n",
    "    ax2.set_ylabel('Steps to Goal')\n",
    "    ax2.set_title('Q-Learning: Steps per Episode')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.plot(agent.epsilon_history, color='purple', linewidth=2)\n",
    "    ax3.set_xlabel('Episode')\n",
    "    ax3.set_ylabel('Epsilon (ε)')\n",
    "    ax3.set_title('Exploration Rate Decay')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    final_rewards = agent.episode_rewards[-200:]  # Last 200 episodes\n",
    "    ax4.hist(final_rewards, bins=20, alpha=0.7, color='orange', edgecolor='black')\n",
    "    ax4.axvline(np.mean(final_rewards), color='red', linestyle='--', \n",
    "                label=f'Mean: {np.mean(final_rewards):.2f}')\n",
    "    ax4.set_xlabel('Episode Reward')\n",
    "    ax4.set_ylabel('Frequency')\n",
    "    ax4.set_title('Final Performance Distribution')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_q_learning_analysis(q_agent)\n",
    "\n",
    "def show_q_values(agent, states_to_show=[(0,0), (1,0), (2,0), (0,1), (2,2)]):\n",
    "    \"\"\"Display Q-values for specific states\"\"\"\n",
    "    print(\"\\nLearned Q-values for key states:\")\n",
    "    print(\"State\\t\\tAction\\t\\tQ-value\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for state in states_to_show:\n",
    "        if not agent.env.is_terminal(state):\n",
    "            valid_actions = agent.env.get_valid_actions(state)\n",
    "            for action in valid_actions:\n",
    "                q_val = agent.Q[state][action]\n",
    "                print(f\"{state}\\t\\t{action}\\t\\t{q_val:.3f}\")\n",
    "            print(\"-\" * 40)\n",
    "\n",
    "show_q_values(q_agent)\n",
    "\n",
    "print(\"\\nQ-Learning has successfully learned the optimal policy!\")\n",
    "print(\"The agent can now navigate efficiently to the goal while avoiding obstacles.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700eb34",
   "metadata": {},
   "source": [
    "## Part 4: SARSA - On-Policy Control\n",
    "\n",
    "### Understanding SARSA Algorithm\n",
    "\n",
    "**SARSA** (State-Action-Reward-State-Action) is an **on-policy** temporal difference control algorithm that learns the action-value function Q^π(s,a) for the policy it is following.\n",
    "\n",
    "### SARSA vs Q-Learning: Key Differences\n",
    "\n",
    "| Aspect | SARSA | Q-Learning |\n",
    "|--------|--------|------------|\n",
    "| **Policy Type** | On-policy | Off-policy |\n",
    "| **Update Target** | Q(S', A') | max_a Q(S', a) |\n",
    "| **Policy Learning** | Current behavior policy | Optimal policy |\n",
    "| **Exploration Effect** | Affects learned Q-values | Only affects experience collection |\n",
    "| **Safety** | More conservative | More aggressive |\n",
    "\n",
    "### SARSA Update Rule\n",
    "\n",
    "```\n",
    "Q(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γQ(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]\n",
    "```\n",
    "\n",
    "**SARSA Tuple**: (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})\n",
    "- **S_t**: Current state\n",
    "- **A_t**: Current action\n",
    "- **R_{t+1}**: Reward received\n",
    "- **S_{t+1}**: Next state\n",
    "- **A_{t+1}**: Next action (chosen by current policy)\n",
    "\n",
    "### SARSA Algorithm Steps\n",
    "\n",
    "1. Initialize Q(s,a) arbitrarily\n",
    "2. **For each episode**:\n",
    "   - Initialize S\n",
    "   - Choose A from S using policy derived from Q (e.g., ε-greedy)\n",
    "   - **For each step of episode**:\n",
    "     - Take action A, observe R, S'\n",
    "     - Choose A' from S' using policy derived from Q\n",
    "     - **Update**: Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n",
    "     - S ← S', A ← A'\n",
    "\n",
    "### On-Policy Nature\n",
    "\n",
    "**SARSA learns Q^π** where π is the policy being followed:\n",
    "- The policy used to select actions IS the policy being evaluated\n",
    "- Exploration actions directly affect the learned Q-values\n",
    "- More conservative in dangerous environments\n",
    "\n",
    "### Expected SARSA\n",
    "\n",
    "**Variant**: Instead of using the next action A', use the expected value:\n",
    "\n",
    "```\n",
    "Q(S_t, A_t) ← Q(S_t, A_t) + α[R_{t+1} + γE[Q(S_{t+1}, A_{t+1})|S_{t+1}] - Q(S_t, A_t)]\n",
    "```\n",
    "\n",
    "Where: E[Q(S_{t+1}, A_{t+1})|S_{t+1}] = Σ_a π(a|S_{t+1}) Q(S_{t+1}, a)\n",
    "\n",
    "### When to Use SARSA vs Q-Learning\n",
    "\n",
    "**Use SARSA when**:\n",
    "- Safety is important (e.g., robot navigation)\n",
    "- You want to learn the policy you're actually following\n",
    "- Environment has \"cliffs\" or dangerous states\n",
    "- Conservative behavior is preferred\n",
    "\n",
    "**Use Q-Learning when**:\n",
    "- You want optimal performance\n",
    "- Exploration is safe\n",
    "- You can afford aggressive learning\n",
    "- Sample efficiency is important\n",
    "\n",
    "### Convergence Properties\n",
    "\n",
    "**SARSA Convergence**:\n",
    "- Converges to Q^π for the policy π being followed\n",
    "- If π converges to greedy policy, SARSA converges to Q*\n",
    "- Requires same conditions as Q-Learning for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f277b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSAAgent:\n",
    "    \"\"\"\n",
    "    SARSA agent for on-policy control\n",
    "    Learns Q^π(s,a) for the policy being followed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        \n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        \n",
    "        self.episode_rewards = []\n",
    "        self.episode_steps = []\n",
    "        self.epsilon_history = []\n",
    "        \n",
    "    def get_action(self, state, explore=True):\n",
    "        \"\"\"Get action using ε-greedy policy\"\"\"\n",
    "        if not explore:\n",
    "            return self.get_greedy_action(state)\n",
    "        \n",
    "        if np.random.random() < self.epsilon:\n",
    "            valid_actions = self.env.get_valid_actions(state)\n",
    "            return np.random.choice(valid_actions) if valid_actions else None\n",
    "        else:\n",
    "            return self.get_greedy_action(state)\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\"Get greedy action\"\"\"\n",
    "        valid_actions = self.env.get_valid_actions(state)\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        \n",
    "        q_values = {action: self.Q[state][action] for action in valid_actions}\n",
    "        max_q = max(q_values.values())\n",
    "        best_actions = [action for action, q in q_values.items() if q == max_q]\n",
    "        return np.random.choice(best_actions)\n",
    "    \n",
    "    def update_q_sarsa(self, state, action, reward, next_state, next_action, done):\n",
    "        \"\"\"\n",
    "        SARSA update: Q(S,A) ← Q(S,A) + α[R + γQ(S',A') - Q(S,A)]\n",
    "        \"\"\"\n",
    "        current_q = self.Q[state][action]\n",
    "        \n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            next_q = self.Q[next_state][next_action] if next_action else 0.0\n",
    "            td_target = reward + self.gamma * next_q\n",
    "        \n",
    "        td_error = td_target - current_q\n",
    "        self.Q[state][action] += self.alpha * td_error\n",
    "        \n",
    "        return td_error\n",
    "    \n",
    "    def run_episode(self, max_steps=200):\n",
    "        \"\"\"Run one episode using SARSA\"\"\"\n",
    "        state = self.env.reset()\n",
    "        action = self.get_action(state, explore=True)\n",
    "        \n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        \n",
    "        while steps < max_steps and action is not None:\n",
    "            next_state, reward, done, _ = self.env.step(action)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                next_action = None\n",
    "            else:\n",
    "                next_action = self.get_action(next_state, explore=True)\n",
    "            \n",
    "            td_error = self.update_q_sarsa(state, action, reward, next_state, next_action, done)\n",
    "            \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        return episode_reward, steps\n",
    "    \n",
    "    def train(self, num_episodes=1000, print_every=100):\n",
    "        \"\"\"Train SARSA agent\"\"\"\n",
    "        print(f\"Training SARSA agent for {num_episodes} episodes...\")\n",
    "        print(f\"Parameters: α={self.alpha}, γ={self.gamma}, ε={self.epsilon}\")\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            episode_reward, steps = self.run_episode()\n",
    "            \n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.episode_steps.append(steps)\n",
    "            self.epsilon_history.append(self.epsilon)\n",
    "            \n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "            \n",
    "            if (episode + 1) % print_every == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-print_every:])\n",
    "                avg_steps = np.mean(self.episode_steps[-print_every:])\n",
    "                print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}, \"\n",
    "                      f\"Avg Steps = {avg_steps:.1f}, ε = {self.epsilon:.3f}\")\n",
    "        \n",
    "        print(\"SARSA training completed!\")\n",
    "    \n",
    "    def get_value_function(self):\n",
    "        \"\"\"Extract value function\"\"\"\n",
    "        V = {}\n",
    "        for state in self.env.states:\n",
    "            valid_actions = self.env.get_valid_actions(state)\n",
    "            if valid_actions:\n",
    "                V[state] = max([self.Q[state][action] for action in valid_actions])\n",
    "            else:\n",
    "                V[state] = 0.0\n",
    "        return V\n",
    "    \n",
    "    def get_policy(self):\n",
    "        \"\"\"Extract learned policy\"\"\"\n",
    "        policy = {}\n",
    "        for state in self.env.states:\n",
    "            if not self.env.is_terminal(state):\n",
    "                policy[state] = self.get_greedy_action(state)\n",
    "        return policy\n",
    "    \n",
    "    def evaluate_policy(self, num_episodes=100):\n",
    "        \"\"\"Evaluate learned policy\"\"\"\n",
    "        rewards = []\n",
    "        steps_list = []\n",
    "        \n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < 200:\n",
    "                action = self.get_action(state, explore=False)\n",
    "                if action is None:\n",
    "                    break\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            rewards.append(episode_reward)\n",
    "            steps_list.append(steps)\n",
    "        \n",
    "        return {\n",
    "            'avg_reward': np.mean(rewards),\n",
    "            'std_reward': np.std(rewards),\n",
    "            'avg_steps': np.mean(steps_list),\n",
    "            'success_rate': sum(1 for r in rewards if r > 5) / len(rewards)\n",
    "        }\n",
    "\n",
    "print(\"Creating SARSA agent...\")\n",
    "sarsa_agent = SARSAAgent(env, alpha=0.1, gamma=0.9, epsilon=0.1, epsilon_decay=0.995)\n",
    "\n",
    "print(\"SARSA agent created successfully!\")\n",
    "print(\"Training SARSA agent...\")\n",
    "sarsa_agent.train(num_episodes=1000, print_every=200)\n",
    "\n",
    "V_sarsa = sarsa_agent.get_value_function()\n",
    "sarsa_policy = sarsa_agent.get_policy()\n",
    "\n",
    "print(\"\\nSARSA Learned Value Function:\")\n",
    "env.visualize_values(V_sarsa, title=\"SARSA: Learned Value Function\", policy=sarsa_policy)\n",
    "\n",
    "print(\"\\nEvaluating SARSA policy...\")\n",
    "sarsa_evaluation = sarsa_agent.evaluate_policy(num_episodes=100)\n",
    "print(f\"SARSA Policy Evaluation:\")\n",
    "print(f\"Average reward: {sarsa_evaluation['avg_reward']:.2f} ± {sarsa_evaluation['std_reward']:.2f}\")\n",
    "print(f\"Average steps: {sarsa_evaluation['avg_steps']:.1f}\")\n",
    "print(f\"Success rate: {sarsa_evaluation['success_rate']*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e71ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_algorithms():\n",
    "    \"\"\"Compare TD(0), Q-Learning, and SARSA performance\"\"\"\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE ALGORITHM COMPARISON\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    algorithms = {\n",
    "        'TD(0)': {\n",
    "            'agent': td_agent,\n",
    "            'type': 'Policy Evaluation',\n",
    "            'policy_type': 'Model-free evaluation',\n",
    "            'learned_values': V_td,\n",
    "            'evaluation': None\n",
    "        },\n",
    "        'Q-Learning': {\n",
    "            'agent': q_agent,\n",
    "            'type': 'Off-policy Control',\n",
    "            'policy_type': 'Optimal policy',\n",
    "            'learned_values': V_optimal,\n",
    "            'evaluation': evaluation\n",
    "        },\n",
    "        'SARSA': {\n",
    "            'agent': sarsa_agent,\n",
    "            'type': 'On-policy Control',\n",
    "            'policy_type': 'Behavior policy',\n",
    "            'learned_values': V_sarsa,\n",
    "            'evaluation': sarsa_evaluation\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"\\n1. PERFORMANCE COMPARISON\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Algorithm':<12} {'Type':<20} {'Avg Reward':<12} {'Success Rate':<12}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for name, info in algorithms.items():\n",
    "        if info['evaluation']:\n",
    "            avg_reward = info['evaluation']['avg_reward']\n",
    "            success_rate = info['evaluation']['success_rate'] * 100\n",
    "            print(f\"{name:<12} {info['type']:<20} {avg_reward:<12.2f} {success_rate:<12.1f}%\")\n",
    "        else:\n",
    "            print(f\"{name:<12} {info['type']:<20} {'N/A':<12} {'N/A':<12}\")\n",
    "    \n",
    "    print(\"\\n2. LEARNING CURVES COMPARISON\")\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    if hasattr(td_agent, 'episode_rewards'):\n",
    "        plt.plot(td_agent.episode_rewards, label='TD(0)', alpha=0.7, color='blue')\n",
    "    plt.plot(q_agent.episode_rewards, label='Q-Learning', alpha=0.7, color='red')\n",
    "    plt.plot(sarsa_agent.episode_rewards, label='SARSA', alpha=0.7, color='green')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode Reward')\n",
    "    plt.title('Episode Rewards Comparison')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    window = 50\n",
    "    \n",
    "    if len(q_agent.episode_rewards) >= window:\n",
    "        q_avg = pd.Series(q_agent.episode_rewards).rolling(window=window).mean()\n",
    "        plt.plot(q_avg, label='Q-Learning', linewidth=2, color='red')\n",
    "    \n",
    "    if len(sarsa_agent.episode_rewards) >= window:\n",
    "        sarsa_avg = pd.Series(sarsa_agent.episode_rewards).rolling(window=window).mean()\n",
    "        plt.plot(sarsa_avg, label='SARSA', linewidth=2, color='green')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Moving Average Reward')\n",
    "    plt.title(f'Moving Average ({window} episodes)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(q_agent.epsilon_history, label='Q-Learning', color='red')\n",
    "    plt.plot(sarsa_agent.epsilon_history, label='SARSA', color='green')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Epsilon (ε)')\n",
    "    plt.title('Exploration Rate Decay')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n3. VALUE FUNCTION COMPARISON\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    if V_td:\n",
    "        grid_td = np.zeros((env.size, env.size))\n",
    "        for i, j in env.obstacles:\n",
    "            grid_td[i, j] = min(V_td.values()) - 1\n",
    "        for i in range(env.size):\n",
    "            for j in range(env.size):\n",
    "                state = (i, j)\n",
    "                if state not in env.obstacles:\n",
    "                    grid_td[i, j] = V_td.get(state, 0)\n",
    "        \n",
    "        im1 = axes[0].imshow(grid_td, cmap='RdYlGn', aspect='equal')\n",
    "        axes[0].set_title('TD(0) Values')\n",
    "        plt.colorbar(im1, ax=axes[0])\n",
    "    \n",
    "    grid_q = np.zeros((env.size, env.size))\n",
    "    for i, j in env.obstacles:\n",
    "        grid_q[i, j] = min(V_optimal.values()) - 1\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            state = (i, j)\n",
    "            if state not in env.obstacles:\n",
    "                grid_q[i, j] = V_optimal.get(state, 0)\n",
    "    \n",
    "    im2 = axes[1].imshow(grid_q, cmap='RdYlGn', aspect='equal')\n",
    "    axes[1].set_title('Q-Learning Values')\n",
    "    plt.colorbar(im2, ax=axes[1])\n",
    "    \n",
    "    grid_s = np.zeros((env.size, env.size))\n",
    "    for i, j in env.obstacles:\n",
    "        grid_s[i, j] = min(V_sarsa.values()) - 1\n",
    "    for i in range(env.size):\n",
    "        for j in range(env.size):\n",
    "            state = (i, j)\n",
    "            if state not in env.obstacles:\n",
    "                grid_s[i, j] = V_sarsa.get(state, 0)\n",
    "    \n",
    "    im3 = axes[2].imshow(grid_s, cmap='RdYlGn', aspect='equal')\n",
    "    axes[2].set_title('SARSA Values')\n",
    "    plt.colorbar(im3, ax=axes[2])\n",
    "    \n",
    "    for ax in axes:\n",
    "        ax.set_xticks(range(env.size))\n",
    "        ax.set_yticks(range(env.size))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n4. STATISTICAL ANALYSIS\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    key_states = [(0, 0), (1, 0), (2, 0), (3, 2), (2, 2)]\n",
    "    print(f\"{'State':<10} {'TD(0)':<10} {'Q-Learning':<12} {'SARSA':<10} {'Q-S Diff':<10}\")\n",
    "    print(\"-\" * 55)\n",
    "    \n",
    "    for state in key_states:\n",
    "        td_val = V_td.get(state, 0) if V_td else 0\n",
    "        q_val = V_optimal.get(state, 0)\n",
    "        s_val = V_sarsa.get(state, 0)\n",
    "        diff = abs(q_val - s_val)\n",
    "        \n",
    "        print(f\"{str(state):<10} {td_val:<10.2f} {q_val:<12.2f} {s_val:<10.2f} {diff:<10.3f}\")\n",
    "    \n",
    "    return algorithms\n",
    "\n",
    "comparison_results = compare_algorithms()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ALGORITHM ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Q-Learning: Learns optimal policy, aggressive exploration\")\n",
    "print(\"2. SARSA: Learns policy being followed, more conservative\")\n",
    "print(\"3. TD(0): Policy evaluation only, foundation for control methods\")\n",
    "print(\"4. Both Q-Learning and SARSA converge to good policies\")\n",
    "print(\"5. Choice depends on application requirements (safety vs optimality)\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271c294",
   "metadata": {},
   "source": [
    "## Part 5: Exploration Strategies in Reinforcement Learning\n",
    "\n",
    "### The Exploration-Exploitation Dilemma\n",
    "\n",
    "**The Problem**: How to balance between:\n",
    "- **Exploitation**: Choose actions that are currently believed to be best\n",
    "- **Exploration**: Try actions that might lead to better long-term performance\n",
    "\n",
    "**Why It Matters**: Without proper exploration, agents may:\n",
    "- Get stuck in suboptimal policies\n",
    "- Never discover better strategies\n",
    "- Fail to adapt to changing environments\n",
    "\n",
    "### Common Exploration Strategies\n",
    "\n",
    "#### 1. Epsilon-Greedy (ε-greedy)\n",
    "\n",
    "**Basic ε-greedy**:\n",
    "- With probability ε: choose random action\n",
    "- With probability 1-ε: choose greedy action\n",
    "\n",
    "**Advantages**: Simple, widely used, theoretical guarantees\n",
    "**Disadvantages**: Uniform random exploration, may be inefficient\n",
    "\n",
    "#### 2. Decaying Epsilon\n",
    "\n",
    "**Exponential Decay**: ε_t = ε_0 × decay_rate^t\n",
    "**Linear Decay**: ε_t = max(ε_min, ε_0 - decay_rate × t)\n",
    "**Inverse Decay**: ε_t = ε_0 / (1 + decay_rate × t)\n",
    "\n",
    "**Rationale**: High exploration early, more exploitation as learning progresses\n",
    "\n",
    "#### 3. Boltzmann Exploration (Softmax)\n",
    "\n",
    "**Softmax Action Selection**:\n",
    "```\n",
    "P(a|s) = e^(Q(s,a)/τ) / Σ_b e^(Q(s,b)/τ)\n",
    "```\n",
    "\n",
    "Where τ (tau) is the **temperature** parameter:\n",
    "- High τ: More random (high exploration)\n",
    "- Low τ: More greedy (low exploration)\n",
    "- τ → 0: Pure greedy\n",
    "- τ → ∞: Pure random\n",
    "\n",
    "#### 4. Upper Confidence Bound (UCB)\n",
    "\n",
    "**UCB Action Selection**:\n",
    "```\n",
    "A_t = argmax_a [Q_t(a) + c√(ln(t)/N_t(a))]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- Q_t(a): Current value estimate\n",
    "- c: Confidence parameter\n",
    "- t: Time step\n",
    "- N_t(a): Number of times action a has been selected\n",
    "\n",
    "#### 5. Thompson Sampling (Bayesian)\n",
    "\n",
    "**Concept**: Maintain probability distributions over Q-values, sample from these distributions to make decisions.\n",
    "\n",
    "**Process**:\n",
    "1. Maintain beliefs about action values\n",
    "2. Sample Q-values from belief distributions\n",
    "3. Choose action with highest sampled value\n",
    "4. Update beliefs based on observed rewards\n",
    "\n",
    "### Exploration in Different Environments\n",
    "\n",
    "**Stationary Environments**: ε-greedy with decay works well\n",
    "**Non-stationary Environments**: Constant ε or adaptive methods\n",
    "**Sparse Reward Environments**: More sophisticated exploration needed\n",
    "**Dangerous Environments**: Conservative exploration (lower ε)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896a9732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExplorationStrategies:\n",
    "    \"\"\"Collection of exploration strategies for RL agents\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def epsilon_greedy(Q, state, valid_actions, epsilon):\n",
    "        \"\"\"Standard ε-greedy exploration\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(valid_actions)\n",
    "        else:\n",
    "            q_values = {action: Q[state][action] for action in valid_actions}\n",
    "            max_q = max(q_values.values())\n",
    "            best_actions = [a for a, q in q_values.items() if q == max_q]\n",
    "            return np.random.choice(best_actions)\n",
    "    \n",
    "    @staticmethod\n",
    "    def boltzmann_exploration(Q, state, valid_actions, temperature):\n",
    "        \"\"\"Boltzmann (softmax) exploration\"\"\"\n",
    "        if temperature <= 0:\n",
    "            q_values = {action: Q[state][action] for action in valid_actions}\n",
    "            max_q = max(q_values.values())\n",
    "            best_actions = [a for a, q in q_values.items() if q == max_q]\n",
    "            return np.random.choice(best_actions)\n",
    "        \n",
    "        q_values = np.array([Q[state][action] for action in valid_actions])\n",
    "        exp_q = np.exp(q_values / temperature)\n",
    "        probabilities = exp_q / np.sum(exp_q)\n",
    "        \n",
    "        return np.random.choice(valid_actions, p=probabilities)\n",
    "    \n",
    "    @staticmethod\n",
    "    def decay_epsilon(initial_epsilon, episode, decay_rate, min_epsilon, decay_type='exponential'):\n",
    "        \"\"\"Different epsilon decay strategies\"\"\"\n",
    "        if decay_type == 'exponential':\n",
    "            return max(min_epsilon, initial_epsilon * (decay_rate ** episode))\n",
    "        elif decay_type == 'linear':\n",
    "            return max(min_epsilon, initial_epsilon - decay_rate * episode)\n",
    "        elif decay_type == 'inverse':\n",
    "            return max(min_epsilon, initial_epsilon / (1 + decay_rate * episode))\n",
    "        else:\n",
    "            return initial_epsilon\n",
    "\n",
    "class ExplorationExperiment:\n",
    "    \"\"\"Experiment with different exploration strategies\"\"\"\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        \n",
    "    def run_exploration_experiment(self, strategies, num_episodes=500, num_runs=3):\n",
    "        \"\"\"Compare different exploration strategies\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for strategy_name, params in strategies.items():\n",
    "            print(f\"Testing {strategy_name}...\")\n",
    "            \n",
    "            strategy_results = []\n",
    "            for run in range(num_runs):\n",
    "                if strategy_name.startswith('epsilon'):\n",
    "                    agent = QLearningAgent(self.env, alpha=0.1, gamma=0.9, \n",
    "                                         epsilon=params['epsilon'], \n",
    "                                         epsilon_decay=params.get('decay', 0.995))\n",
    "                    agent.train(num_episodes=num_episodes, print_every=num_episodes)\n",
    "                \n",
    "                elif strategy_name == 'boltzmann':\n",
    "                    agent = BoltzmannQLearning(self.env, alpha=0.1, gamma=0.9, \n",
    "                                             temperature=params['temperature'])\n",
    "                    agent.train(num_episodes=num_episodes, print_every=num_episodes)\n",
    "                \n",
    "                evaluation = agent.evaluate_policy(num_episodes=100)\n",
    "                strategy_results.append({\n",
    "                    'rewards': agent.episode_rewards,\n",
    "                    'evaluation': evaluation,\n",
    "                    'final_epsilon': getattr(agent, 'epsilon', None)\n",
    "                })\n",
    "            \n",
    "            results[strategy_name] = strategy_results\n",
    "        \n",
    "        return results\n",
    "\n",
    "class BoltzmannQLearning:\n",
    "    \"\"\"Q-Learning with Boltzmann exploration\"\"\"\n",
    "    \n",
    "    def __init__(self, env, alpha=0.1, gamma=0.9, temperature=1.0, temp_decay=0.99, min_temp=0.01):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.temperature = temperature\n",
    "        self.temp_decay = temp_decay\n",
    "        self.min_temp = min_temp\n",
    "        \n",
    "        self.Q = defaultdict(lambda: defaultdict(float))\n",
    "        self.episode_rewards = []\n",
    "        self.temperature_history = []\n",
    "        \n",
    "    def get_action(self, state, explore=True):\n",
    "        \"\"\"Boltzmann action selection\"\"\"\n",
    "        valid_actions = self.env.get_valid_actions(state)\n",
    "        if not valid_actions:\n",
    "            return None\n",
    "        \n",
    "        if not explore:\n",
    "            q_values = {action: self.Q[state][action] for action in valid_actions}\n",
    "            max_q = max(q_values.values())\n",
    "            best_actions = [a for a, q in q_values.items() if q == max_q]\n",
    "            return np.random.choice(best_actions)\n",
    "        \n",
    "        return ExplorationStrategies.boltzmann_exploration(\n",
    "            self.Q, state, valid_actions, self.temperature)\n",
    "    \n",
    "    def train(self, num_episodes=1000, print_every=100):\n",
    "        \"\"\"Train with Boltzmann exploration\"\"\"\n",
    "        for episode in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < 200:\n",
    "                action = self.get_action(state, explore=True)\n",
    "                if action is None:\n",
    "                    break\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                current_q = self.Q[state][action]\n",
    "                if done:\n",
    "                    td_target = reward\n",
    "                else:\n",
    "                    valid_next_actions = self.env.get_valid_actions(next_state)\n",
    "                    if valid_next_actions:\n",
    "                        max_next_q = max([self.Q[next_state][a] for a in valid_next_actions])\n",
    "                    else:\n",
    "                        max_next_q = 0.0\n",
    "                    td_target = reward + self.gamma * max_next_q\n",
    "                \n",
    "                self.Q[state][action] += self.alpha * (td_target - current_q)\n",
    "                \n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            self.episode_rewards.append(episode_reward)\n",
    "            self.temperature_history.append(self.temperature)\n",
    "            \n",
    "            self.temperature = max(self.min_temp, self.temperature * self.temp_decay)\n",
    "            \n",
    "            if (episode + 1) % print_every == 0:\n",
    "                avg_reward = np.mean(self.episode_rewards[-print_every:])\n",
    "                print(f\"Episode {episode + 1}: Avg Reward = {avg_reward:.2f}, Temp = {self.temperature:.3f}\")\n",
    "    \n",
    "    def evaluate_policy(self, num_episodes=100):\n",
    "        \"\"\"Evaluate learned policy\"\"\"\n",
    "        rewards = []\n",
    "        for _ in range(num_episodes):\n",
    "            state = self.env.reset()\n",
    "            episode_reward = 0\n",
    "            steps = 0\n",
    "            \n",
    "            while steps < 200:\n",
    "                action = self.get_action(state, explore=False)\n",
    "                if action is None:\n",
    "                    break\n",
    "                \n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                episode_reward += reward\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "            \n",
    "            rewards.append(episode_reward)\n",
    "        \n",
    "        return {\n",
    "            'avg_reward': np.mean(rewards),\n",
    "            'std_reward': np.std(rewards),\n",
    "            'success_rate': sum(1 for r in rewards if r > 5) / len(rewards)\n",
    "        }\n",
    "\n",
    "print(\"EXPLORATION STRATEGIES EXPERIMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "exploration_experiment = ExplorationExperiment(env)\n",
    "\n",
    "strategies = {\n",
    "    'epsilon_0.1': {'epsilon': 0.1, 'decay': 1.0},  # Fixed epsilon\n",
    "    'epsilon_0.3': {'epsilon': 0.3, 'decay': 1.0},  # Higher fixed epsilon\n",
    "    'epsilon_decay_fast': {'epsilon': 0.9, 'decay': 0.99},  # Fast decay\n",
    "    'epsilon_decay_slow': {'epsilon': 0.5, 'decay': 0.995},  # Slow decay\n",
    "    'boltzmann': {'temperature': 2.0}  # Boltzmann exploration\n",
    "}\n",
    "\n",
    "results = exploration_experiment.run_exploration_experiment(strategies, num_episodes=300, num_runs=2)\n",
    "\n",
    "def analyze_exploration_results(results):\n",
    "    \"\"\"Analyze and visualize exploration experiment results\"\"\"\n",
    "    \n",
    "    print(\"\\nEXPLORATION STRATEGY COMPARISON\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Strategy':<20} {'Avg Reward':<12} {'Success Rate':<15} {'Std Reward':<12}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    strategy_performance = {}\n",
    "    \n",
    "    for strategy, runs in results.items():\n",
    "        avg_rewards = [run['evaluation']['avg_reward'] for run in runs]\n",
    "        success_rates = [run['evaluation']['success_rate'] for run in runs]\n",
    "        \n",
    "        mean_reward = np.mean(avg_rewards)\n",
    "        mean_success = np.mean(success_rates)\n",
    "        std_reward = np.std(avg_rewards)\n",
    "        \n",
    "        strategy_performance[strategy] = {\n",
    "            'mean_reward': mean_reward,\n",
    "            'mean_success': mean_success,\n",
    "            'std_reward': std_reward\n",
    "        }\n",
    "        \n",
    "        print(f\"{strategy:<20} {mean_reward:<12.2f} {mean_success*100:<15.1f}% {std_reward:<12.3f}\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    for strategy, runs in results.items():\n",
    "        avg_rewards = np.mean([run['rewards'] for run in runs], axis=0)\n",
    "        plt.plot(avg_rewards, label=strategy, alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Episode Reward')\n",
    "    plt.title('Learning Curves by Exploration Strategy')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 2)\n",
    "    strategies_list = list(strategy_performance.keys())\n",
    "    rewards = [strategy_performance[s]['mean_reward'] for s in strategies_list]\n",
    "    errors = [strategy_performance[s]['std_reward'] for s in strategies_list]\n",
    "    \n",
    "    bars = plt.bar(range(len(strategies_list)), rewards, yerr=errors, \n",
    "                   capsize=5, alpha=0.7, color=['blue', 'red', 'green', 'orange', 'purple'])\n",
    "    plt.xticks(range(len(strategies_list)), strategies_list, rotation=45)\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.title('Final Performance Comparison')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    success_rates = [strategy_performance[s]['mean_success']*100 for s in strategies_list]\n",
    "    plt.bar(range(len(strategies_list)), success_rates, alpha=0.7, color='green')\n",
    "    plt.xticks(range(len(strategies_list)), strategies_list, rotation=45)\n",
    "    plt.ylabel('Success Rate (%)')\n",
    "    plt.title('Success Rate Comparison')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(2, 2, 4)\n",
    "    for strategy, runs in results.items():\n",
    "        if 'epsilon' in strategy and hasattr(runs[0], 'final_epsilon'):\n",
    "            pass\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Exploration Parameter')\n",
    "    plt.title('Exploration Parameter Evolution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return strategy_performance\n",
    "\n",
    "performance_analysis = analyze_exploration_results(results)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"EXPLORATION STRATEGY INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"1. Fixed epsilon strategies provide consistent exploration\")\n",
    "print(\"2. Decaying epsilon balances exploration and exploitation over time\")\n",
    "print(\"3. Boltzmann exploration provides principled probabilistic action selection\")\n",
    "print(\"4. Higher initial epsilon may find better solutions but converge slower\")\n",
    "print(\"5. The best strategy depends on environment characteristics\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba872f3",
   "metadata": {},
   "source": [
    "## Part 6: Advanced Topics and Extensions\n",
    "\n",
    "### Double Q-Learning\n",
    "\n",
    "**Problem with Q-Learning**: Maximization bias due to using the same Q-values for both action selection and evaluation.\n",
    "\n",
    "**Solution**: Double Q-Learning maintains two Q-functions:\n",
    "- Q_A and Q_B\n",
    "- Randomly choose which one to update\n",
    "- Use one for action selection, the other for evaluation\n",
    "\n",
    "**Update Rule**:\n",
    "```\n",
    "If random() < 0.5:\n",
    "    Q_A(S,A) ← Q_A(S,A) + α[R + γQ_B(S', argmax_a Q_A(S',a)) - Q_A(S,A)]\n",
    "Else:\n",
    "    Q_B(S,A) ← Q_B(S,A) + α[R + γQ_A(S', argmax_a Q_B(S',a)) - Q_B(S,A)]\n",
    "```\n",
    "\n",
    "### Experience Replay\n",
    "\n",
    "**Concept**: Store experiences in a replay buffer and sample randomly for learning.\n",
    "\n",
    "**Benefits**:\n",
    "- Breaks temporal correlations in experience\n",
    "- More sample efficient\n",
    "- Enables offline learning from stored experiences\n",
    "\n",
    "**Implementation**:\n",
    "1. Store (s, a, r, s', done) tuples in buffer\n",
    "2. Sample random mini-batches for updates\n",
    "3. Update Q-function using sampled experiences\n",
    "\n",
    "### Multi-Step Learning\n",
    "\n",
    "**TD(λ)**: Generalization of TD(0) using eligibility traces\n",
    "**n-step Q-learning**: Updates based on n-step returns\n",
    "\n",
    "**n-step Return**:\n",
    "```\n",
    "G_t^{(n)} = R_{t+1} + γR_{t+2} + ... + γ^{n-1}R_{t+n} + γ^n Q(S_{t+n}, A_{t+n})\n",
    "```\n",
    "\n",
    "### Function Approximation\n",
    "\n",
    "**Problem**: Large state spaces make tabular methods infeasible\n",
    "\n",
    "**Solution**: Approximate Q(s,a) with function approximator:\n",
    "- Linear functions: Q(s,a) = θ^T φ(s,a)\n",
    "- Neural networks: Deep Q-Networks (DQN)\n",
    "\n",
    "**Challenges**:\n",
    "- Stability issues with function approximation\n",
    "- Requires careful hyperparameter tuning\n",
    "- May not converge to optimal solution\n",
    "\n",
    "### Applications and Extensions\n",
    "\n",
    "#### 1. Game Playing\n",
    "- **Atari Games**: DQN and variants\n",
    "- **Board Games**: AlphaGo, AlphaZero\n",
    "- **Real-time Strategy**: StarCraft II\n",
    "\n",
    "#### 2. Robotics\n",
    "- **Navigation**: Path planning with obstacles\n",
    "- **Manipulation**: Grasping and object manipulation\n",
    "- **Control**: Drone flight, walking robots\n",
    "\n",
    "#### 3. Finance and Trading\n",
    "- **Portfolio Management**: Asset allocation\n",
    "- **Algorithmic Trading**: Buy/sell decisions\n",
    "- **Risk Management**: Dynamic hedging\n",
    "\n",
    "#### 4. Resource Management\n",
    "- **Cloud Computing**: Server allocation\n",
    "- **Energy Systems**: Grid management\n",
    "- **Transportation**: Traffic optimization\n",
    "\n",
    "### Recent Developments\n",
    "\n",
    "#### Deep Reinforcement Learning\n",
    "- **DQN**: Deep Q-Networks with experience replay\n",
    "- **DDQN**: Double Deep Q-Networks\n",
    "- **Dueling DQN**: Separate value and advantage streams\n",
    "- **Rainbow**: Combination of multiple improvements\n",
    "\n",
    "#### Policy Gradient Methods\n",
    "- **REINFORCE**: Basic policy gradient\n",
    "- **Actor-Critic**: Combined value and policy learning\n",
    "- **PPO**: Proximal Policy Optimization\n",
    "- **SAC**: Soft Actor-Critic\n",
    "\n",
    "#### Model-Based RL\n",
    "- **Dyna-Q**: Learning with simulated experience\n",
    "- **MCTS**: Monte Carlo Tree Search\n",
    "- **Model-Predictive Control**: Planning with learned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f119868",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SESSION 3 SUMMARY: TEMPORAL DIFFERENCE LEARNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def print_session_summary():\n",
    "    \"\"\"Print comprehensive session summary\"\"\"\n",
    "    \n",
    "    summary_points = {\n",
    "        \"Core Concepts Learned\": [\n",
    "            \"Temporal Difference Learning: Bootstrap from current estimates\",\n",
    "            \"Q-Learning: Off-policy control for optimal policies\",\n",
    "            \"SARSA: On-policy control for behavior policies\",\n",
    "            \"Exploration strategies: ε-greedy, Boltzmann, decay schedules\",\n",
    "            \"Model-free learning: No environment model required\"\n",
    "        ],\n",
    "        \n",
    "        \"Mathematical Foundations\": [\n",
    "            \"TD(0): V(s) ← V(s) + α[R + γV(s') - V(s)]\",\n",
    "            \"Q-Learning: Q(s,a) ← Q(s,a) + α[R + γmax_a'Q(s',a') - Q(s,a)]\",\n",
    "            \"SARSA: Q(s,a) ← Q(s,a) + α[R + γQ(s',a') - Q(s,a)]\",\n",
    "            \"TD Error: R + γV(s') - V(s) quantifies prediction error\",\n",
    "            \"Convergence conditions: Infinite exploration + learning rate conditions\"\n",
    "        ],\n",
    "        \n",
    "        \"Algorithm Comparisons\": [\n",
    "            \"TD(0): Policy evaluation, foundation for control methods\",\n",
    "            \"Q-Learning: Learns optimal policy, aggressive, off-policy\",\n",
    "            \"SARSA: Learns current policy, conservative, on-policy\",\n",
    "            \"Exploration: Critical for discovering good policies\",\n",
    "            \"Sample efficiency: All methods learn from individual transitions\"\n",
    "        ],\n",
    "        \n",
    "        \"Practical Insights\": [\n",
    "            \"Learning rate α controls update step size\",\n",
    "            \"Discount factor γ balances immediate vs future rewards\", \n",
    "            \"Exploration rate ε balances exploration vs exploitation\",\n",
    "            \"Decaying exploration: High initial exploration, reduce over time\",\n",
    "            \"Environment characteristics determine best algorithm choice\"\n",
    "        ],\n",
    "        \n",
    "        \"Implementation Skills\": [\n",
    "            \"Q-table implementation for discrete state-action spaces\",\n",
    "            \"ε-greedy exploration strategy implementation\",\n",
    "            \"Learning curve analysis and performance evaluation\",\n",
    "            \"Hyperparameter tuning for learning rate and exploration\",\n",
    "            \"Comparative analysis between different algorithms\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for category, points in summary_points.items():\n",
    "        print(f\"\\n{category}:\")\n",
    "        print(\"-\" * len(category))\n",
    "        for i, point in enumerate(points, 1):\n",
    "            print(f\"{i}. {point}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ALGORITHM SELECTION GUIDE\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    selection_guide = {\n",
    "        \"Use TD(0) when\": [\n",
    "            \"You need to evaluate a specific policy\",\n",
    "            \"Building foundation for control algorithms\",\n",
    "            \"Understanding temporal difference principles\"\n",
    "        ],\n",
    "        \n",
    "        \"Use Q-Learning when\": [\n",
    "            \"You want optimal performance\",\n",
    "            \"Environment allows aggressive exploration\",\n",
    "            \"Off-policy learning is acceptable\",\n",
    "            \"Sample efficiency is important\"\n",
    "        ],\n",
    "        \n",
    "        \"Use SARSA when\": [\n",
    "            \"Safety is a primary concern\", \n",
    "            \"Environment has dangerous states\",\n",
    "            \"You want conservative behavior\",\n",
    "            \"On-policy learning is required\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    for when, reasons in selection_guide.items():\n",
    "        print(f\"\\n{when}:\")\n",
    "        for reason in reasons:\n",
    "            print(f\"  • {reason}\")\n",
    "\n",
    "print_session_summary()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    final_comparison = {\n",
    "        \"Q-Learning\": {\n",
    "            \"Type\": \"Off-policy Control\",\n",
    "            \"Performance\": evaluation if 'evaluation' in globals() else \"Not evaluated\",\n",
    "            \"Convergence\": \"Fast to optimal policy\",\n",
    "            \"Exploration\": \"ε-greedy with decay\"\n",
    "        },\n",
    "        \"SARSA\": {\n",
    "            \"Type\": \"On-policy Control\", \n",
    "            \"Performance\": sarsa_evaluation if 'sarsa_evaluation' in globals() else \"Not evaluated\",\n",
    "            \"Convergence\": \"Slower but safer\",\n",
    "            \"Exploration\": \"ε-greedy with decay\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Algorithm Performance Summary:\")\n",
    "    for algo, details in final_comparison.items():\n",
    "        print(f\"\\n{algo}:\")\n",
    "        for key, value in details.items():\n",
    "            if isinstance(value, dict) and 'avg_reward' in value:\n",
    "                print(f\"  {key}: Avg Reward = {value['avg_reward']:.2f}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "                \n",
    "except NameError:\n",
    "    print(\"Run all algorithm implementations to see performance comparison\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"NEXT STEPS AND ADVANCED TOPICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "next_steps = [\n",
    "    \"Deep Q-Networks (DQN) for large state spaces\",\n",
    "    \"Policy Gradient methods (REINFORCE, Actor-Critic)\",\n",
    "    \"Advanced exploration (UCB, Thompson Sampling)\",\n",
    "    \"Multi-agent reinforcement learning\",\n",
    "    \"Continuous action spaces and control\",\n",
    "    \"Model-based reinforcement learning\",\n",
    "    \"Real-world applications and deployment\"\n",
    "]\n",
    "\n",
    "print(\"Recommended next learning topics:\")\n",
    "for i, topic in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {topic}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONGRATULATIONS!\")\n",
    "print(\"You have completed a comprehensive study of Temporal Difference Learning\")\n",
    "print(\"Key achievements:\")\n",
    "print(\"✓ Implemented TD(0) for policy evaluation\") \n",
    "print(\"✓ Built Q-Learning agent from scratch\")\n",
    "print(\"✓ Implemented SARSA for on-policy control\")\n",
    "print(\"✓ Explored different exploration strategies\")\n",
    "print(\"✓ Conducted comparative algorithm analysis\")\n",
    "print(\"✓ Understanding of model-free reinforcement learning\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73663b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"INTERACTIVE LEARNING EXERCISES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def self_check_questions():\n",
    "    \"\"\"Self-assessment questions for TD learning concepts\"\"\"\n",
    "    \n",
    "    questions = [\n",
    "        {\n",
    "            \"question\": \"What is the main advantage of TD learning over Monte Carlo methods?\",\n",
    "            \"options\": [\n",
    "                \"A) TD learning requires complete episodes\",\n",
    "                \"B) TD learning can learn online from incomplete episodes\", \n",
    "                \"C) TD learning has no bias\",\n",
    "                \"D) TD learning requires a model\"\n",
    "            ],\n",
    "            \"answer\": \"B\",\n",
    "            \"explanation\": \"TD learning updates after each step using bootstrapped estimates, enabling online learning without waiting for episode completion.\"\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"question\": \"What is the key difference between Q-Learning and SARSA?\",\n",
    "            \"options\": [\n",
    "                \"A) Q-Learning uses different learning rates\",\n",
    "                \"B) Q-Learning is on-policy, SARSA is off-policy\",\n",
    "                \"C) Q-Learning uses max operation, SARSA uses actual next action\",\n",
    "                \"D) Q-Learning requires more memory\"\n",
    "            ],\n",
    "            \"answer\": \"C\", \n",
    "            \"explanation\": \"Q-Learning uses max_a Q(s',a) (off-policy), while SARSA uses Q(s',a') where a' is the actual next action chosen by the current policy (on-policy).\"\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"question\": \"Why is exploration important in reinforcement learning?\",\n",
    "            \"options\": [\n",
    "                \"A) To make the algorithm run faster\",\n",
    "                \"B) To reduce memory requirements\", \n",
    "                \"C) To discover potentially better actions and avoid local optima\",\n",
    "                \"D) To satisfy convergence conditions\"\n",
    "            ],\n",
    "            \"answer\": \"C\",\n",
    "            \"explanation\": \"Without exploration, the agent might never discover better actions and could get stuck in suboptimal policies.\"\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"question\": \"What happens when the learning rate α is too high?\",\n",
    "            \"options\": [\n",
    "                \"A) Learning becomes too slow\",\n",
    "                \"B) The algorithm may not converge and become unstable\",\n",
    "                \"C) Memory usage increases\",\n",
    "                \"D) Exploration decreases\"\n",
    "            ],\n",
    "            \"answer\": \"B\",\n",
    "            \"explanation\": \"High learning rates cause large updates that can overshoot optimal values and prevent convergence, making learning unstable.\"\n",
    "        },\n",
    "        \n",
    "        {\n",
    "            \"question\": \"In what situation would you prefer SARSA over Q-Learning?\",\n",
    "            \"options\": [\n",
    "                \"A) When you want the fastest convergence\",\n",
    "                \"B) When the environment has dangerous states and safety is important\",\n",
    "                \"C) When you have unlimited computational resources\", \n",
    "                \"D) When the state space is very large\"\n",
    "            ],\n",
    "            \"answer\": \"B\",\n",
    "            \"explanation\": \"SARSA is more conservative because it learns the policy being followed (including exploration), making it safer in dangerous environments.\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    print(\"SELF-CHECK QUESTIONS\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Test your understanding of TD learning concepts:\")\n",
    "    print(\"(Think about each question, then check the answers below)\\n\")\n",
    "    \n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"Question {i}: {q['question']}\")\n",
    "        for option in q['options']:\n",
    "            print(f\"  {option}\")\n",
    "        print()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"ANSWERS AND EXPLANATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for i, q in enumerate(questions, 1):\n",
    "        print(f\"Question {i}: Answer {q['answer']}\")\n",
    "        print(f\"Explanation: {q['explanation']}\")\n",
    "        print()\n",
    "\n",
    "self_check_questions()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"HANDS-ON CHALLENGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "challenges = {\n",
    "    \"Challenge 1: Parameter Sensitivity Analysis\": {\n",
    "        \"description\": \"Investigate how different hyperparameters affect learning\",\n",
    "        \"tasks\": [\n",
    "            \"Test learning rates: α ∈ {0.01, 0.1, 0.3, 0.5, 0.9}\",\n",
    "            \"Test discount factors: γ ∈ {0.5, 0.7, 0.9, 0.95, 0.99}\",\n",
    "            \"Test exploration rates: ε ∈ {0.01, 0.1, 0.3, 0.5}\",\n",
    "            \"Plot learning curves for each parameter setting\",\n",
    "            \"Identify optimal parameter combinations\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Challenge 2: Environment Modifications\": {\n",
    "        \"description\": \"Test algorithms on modified environments\",\n",
    "        \"tasks\": [\n",
    "            \"Create larger grid (6x6, 8x8)\",\n",
    "            \"Add more obstacles in different patterns\",\n",
    "            \"Implement stochastic transitions (wind effects)\",\n",
    "            \"Create multiple goals with different rewards\",\n",
    "            \"Compare algorithm performance across environments\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Challenge 3: Advanced Exploration\": {\n",
    "        \"description\": \"Implement and compare advanced exploration strategies\",\n",
    "        \"tasks\": [\n",
    "            \"Implement UCB (Upper Confidence Bound) exploration\",\n",
    "            \"Implement optimistic initialization\", \n",
    "            \"Implement curiosity-driven exploration\",\n",
    "            \"Compare convergence speed and final performance\",\n",
    "            \"Analyze exploration efficiency in different environments\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Challenge 4: Algorithm Extensions\": {\n",
    "        \"description\": \"Implement extensions and variants\",\n",
    "        \"tasks\": [\n",
    "            \"Implement Double Q-Learning to reduce maximization bias\",\n",
    "            \"Implement Expected SARSA\",\n",
    "            \"Implement n-step Q-Learning\",\n",
    "            \"Add experience replay buffer\",\n",
    "            \"Compare performance with basic algorithms\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"Challenge 5: Real-World Application\": {\n",
    "        \"description\": \"Apply TD learning to a practical problem\",\n",
    "        \"tasks\": [\n",
    "            \"Design a simple inventory management problem\",\n",
    "            \"Implement a basic trading strategy simulation\", \n",
    "            \"Create a path planning scenario with dynamic obstacles\",\n",
    "            \"Apply Q-Learning or SARSA to solve the problem\",\n",
    "            \"Analyze and visualize the learned policies\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for challenge_name, details in challenges.items():\n",
    "    print(f\"{challenge_name}:\")\n",
    "    print(f\"Description: {details['description']}\")\n",
    "    print(\"Tasks:\")\n",
    "    for i, task in enumerate(details['tasks'], 1):\n",
    "        print(f\"  {i}. {task}\")\n",
    "    print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEBUGGING AND TROUBLESHOOTING GUIDE\") \n",
    "print(\"=\" * 80)\n",
    "\n",
    "debugging_tips = [\n",
    "    \"Learning not converging? Try reducing learning rate (α)\",\n",
    "    \"Convergence too slow? Check if exploration rate is too high\",\n",
    "    \"Poor final performance? Increase exploration during training\",\n",
    "    \"Unstable learning? Check for implementation bugs in TD updates\",\n",
    "    \"Agent taking random actions? Verify ε-greedy implementation\",\n",
    "    \"Q-values exploding? Add bounds or reduce learning rate\",\n",
    "    \"Not reaching goal? Check environment transition logic\",\n",
    "    \"Identical performance across runs? Verify random seed handling\"\n",
    "]\n",
    "\n",
    "print(\"Common issues and solutions:\")\n",
    "for i, tip in enumerate(debugging_tips, 1):\n",
    "    print(f\"{i}. {tip}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL THOUGHTS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Temporal Difference learning bridges the gap between model-based\")\n",
    "print(\"dynamic programming and model-free Monte Carlo methods.\")\n",
    "print(\"\")\n",
    "print(\"Key insights from this session:\")\n",
    "print(\"• TD learning enables online learning from experience\")\n",
    "print(\"• Exploration is crucial for discovering optimal policies\") \n",
    "print(\"• Algorithm choice depends on problem characteristics\")\n",
    "print(\"• Hyperparameter tuning significantly affects performance\")\n",
    "print(\"• TD methods form the foundation of modern RL algorithms\")\n",
    "print(\"\")\n",
    "print(\"You are now ready to explore deep reinforcement learning,\")\n",
    "print(\"policy gradient methods, and advanced RL applications!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}