# Computer Assignment 13: Advanced Model-based Rl and World Models
# Table of Contents

- [Computer Assignment 13: Advanced Model-Based RL and World Models](#computer-assignment-13-advanced-model-based-rl-and-world-models)
- [Course Information](#course-information)
- [Learning Objectives](#learning-objectives)
- [Prerequisites](#prerequisites)
- [Roadmap](#roadmap)
- [Section 1: Model-Free vs Model-Based Reinforcement Learning](#section-1-model-free-vs-model-based-reinforcement-learning)
- [Section 2: World Models and Imagination-Based Learning](#section-2-world-models-and-imagination-based-learning)
- [Section 3: Sample Efficiency and Transfer Learning](#section-3-sample-efficiency-and-transfer-learning)
- [Section 4: Hierarchical Reinforcement Learning](#section-4-hierarchical-reinforcement-learning)
- [Project Structure](#project-structure)
- [Contents Overview](#contents-overview)
- [Theoretical Foundations](#theoretical-foundations)
- [Implementation Components](#implementation-components)
- [Advanced Topics](#advanced-topics)
- [Evaluation Criteria](#evaluation-criteria)
- [Getting Started](#getting-started)
- [Expected Outcomes](#expected-outcomes)
- [Section 2: World Models and Imagination-Based Learning](#section-2-world-models-and-imagination-based-learning)
- [2.1 Theoretical Foundations of World Models](#21-theoretical-foundations-of-world-models)
- [Core Concepts](#core-concepts)
- [2.2 Variational World Models](#22-variational-world-models)
- [Variational Autoencoders (VAE) for World Modeling](#variational-autoencoders-vae-for-world-modeling)
- [2.3 Planning in Learned Latent Space](#23-planning-in-learned-latent-space)
- [Model Predictive Control (MPC) in Latent Space](#model-predictive-control-mpc-in-latent-space)
- [Dreamer Algorithm](#dreamer-algorithm)
- [2.4 Advantages and Challenges](#24-advantages-and-challenges)
- [Advantages of World Models:](#advantages-of-world-models)
- [Challenges:](#challenges)
- [2.5 Modern Approaches](#25-modern-approaches)
- [MuZero](#muzero)
- [Dreamer V2/V3](#dreamer-v2v3)
- [Model-Based Meta-Learning](#model-based-meta-learning)
- [Section 3: Sample Efficiency and Transfer Learning](#section-3-sample-efficiency-and-transfer-learning)
- [3.1 Sample Efficiency Challenges in Deep RL](#31-sample-efficiency-challenges-in-deep-rl)
- [Why is Sample Efficiency Important?](#why-is-sample-efficiency-important)
- [3.2 Sample Efficiency Techniques](#32-sample-efficiency-techniques)
- [3.2.1 Experience Replay and Prioritization](#321-experience-replay-and-prioritization)
- [3.2.2 Data Augmentation](#322-data-augmentation)
- [3.2.3 Auxiliary Tasks](#323-auxiliary-tasks)
- [3.3 Transfer Learning in Reinforcement Learning](#33-transfer-learning-in-reinforcement-learning)
- [3.3.1 Types of Transfer in RL](#331-types-of-transfer-in-rl)
- [3.3.2 Transfer Learning Approaches](#332-transfer-learning-approaches)
- [Fine-tuning](#fine-tuning)
- [Progressive Networks](#progressive-networks)
- [Universal Value Functions (UVF)](#universal-value-functions-uvf)
- [3.4 Meta-Learning and Few-Shot Adaptation](#34-meta-learning-and-few-shot-adaptation)
- [3.4.1 Model-Agnostic Meta-Learning (MAML)](#341-model-agnostic-meta-learning-maml)
- [3.4.2 Gradient-Based Meta-Learning](#342-gradient-based-meta-learning)
- [3.5 Domain Adaptation and Sim-to-Real Transfer](#35-domain-adaptation-and-sim-to-real-transfer)
- [3.5.1 Domain Randomization](#351-domain-randomization)
- [3.5.2 Domain Adversarial Training](#352-domain-adversarial-training)
- [3.6 Curriculum Learning](#36-curriculum-learning)
- [3.6.1 Curriculum Design Principles](#361-curriculum-design-principles)
- [3.6.2 Curriculum Learning Algorithms](#362-curriculum-learning-algorithms)
- [Section 4: Hierarchical Reinforcement Learning](#section-4-hierarchical-reinforcement-learning)
- [4.1 Theory: Hierarchical Decision Making](#41-theory-hierarchical-decision-making)
- [Key Components](#key-components)
- [Options Framework](#options-framework)
- [Hierarchical Value Functions](#hierarchical-value-functions)
- [Feudal Networks](#feudal-networks)
- [Mathematical Framework](#mathematical-framework)
- [Intrinsic Reward Signal](#intrinsic-reward-signal)
- [Hierarchical Policy Gradient](#hierarchical-policy-gradient)
- [4.2 Implementation: Hierarchical RL Architectures](#42-implementation-hierarchical-rl-architectures)
- [Section 5: Comprehensive Evaluation and Advanced Techniques Integration](#section-5-comprehensive-evaluation-and-advanced-techniques-integration)
- [5.1 Multi-Method Performance Analysis](#51-multi-method-performance-analysis)
- [Performance Metrics](#performance-metrics)
- [Evaluation Framework](#evaluation-framework)
- [5.2 Practical Implementation Considerations](#52-practical-implementation-considerations)
- [When to Use Each Method:](#when-to-use-each-method)
- [Model-Free Methods (DQN, Policy Gradient)](#model-free-methods-dqn-policy-gradient)
- [Model-Based Methods  ](#model-based-methods--)
- [World Models](#world-models)
- [Hierarchical Methods](#hierarchical-methods)
- [Sample Efficiency Techniques](#sample-efficiency-techniques)
- [5.3 Advanced Techniques Summary](#53-advanced-techniques-summary)
- [Core Contributions:](#core-contributions)
- [CA13: Advanced Deep Reinforcement Learning - Model-Free vs Model-Based Methods and Real-World Applications](#ca13-advanced-deep-reinforcement-learning---model-free-vs-model-based-methods-and-real-world-applications)
- [Deep Reinforcement Learning - Session 13](#deep-reinforcement-learning---session-13)
- [Learning Objectives:](#learning-objectives)
- [Notebook Structure:](#notebook-structure)



## Course Information
- **Course**: Deep Reinforcement Learning (DRL)
- **Instructor**: Dr. [Instructor Name]
- **Institution**: Sharif University of Technology
- **Semester**: Fall 2024
- **Assignment Number**: CA13

## Learning Objectives

By completing this assignment, students will be able to:

1. **Understand Model-Based vs Model-Free RL Trade-offs**: Analyze the fundamental differences between model-free and model-based reinforcement learning approaches, including their respective advantages, limitations, and appropriate use cases.

2. **Master World Model Architectures**: Design and implement variational world models using VAEs for learning compact latent representations of environment dynamics, including encoder-decoder architectures and stochastic dynamics modeling.

3. **Implement Imagination-Based Learning**: Develop agents that leverage learned world models for planning and decision-making in latent space, enabling sample-efficient learning through imagined trajectories.

4. **Apply Sample Efficiency Techniques**: Utilize advanced techniques such as prioritized experience replay, data augmentation, and auxiliary tasks to improve learning efficiency in deep RL.

5. **Design Transfer Learning Systems**: Build agents capable of transferring knowledge across related tasks through shared representations, fine-tuning, and meta-learning approaches.

6. **Develop Hierarchical RL Frameworks**: Implement hierarchical decision-making systems using options framework, enabling temporal abstraction and skill composition for complex task solving.

## Prerequisites

Before starting this assignment, ensure you have:

- **Mathematical Background**: 
- Probability theory and stochastic processes
- Linear algebra and matrix operations
- Optimization and gradient-based methods
- Information theory (KL divergence, entropy)

- **Technical Skills**:
- Python programming with PyTorch
- Deep learning fundamentals (neural networks, autoencoders)
- Basic reinforcement learning concepts (MDPs, value functions, policies)
- Experience with Gymnasium environments

- **Prior Knowledge**:
- Completion of CA1-CA12 assignments
- Understanding of model-free RL algorithms (DQN, policy gradients)
- Familiarity with neural network architectures

## Roadmap

This assignment is structured as follows:

### Section 1: Model-free Vs Model-based Reinforcement Learning
- Theoretical foundations of model-free and model-based approaches
- Mathematical formulations and trade-off analysis
- Hybrid algorithms combining both paradigms
- Practical implementation and comparison

### Section 2: World Models and Imagination-based Learning
- Variational autoencoders for world modeling
- Stochastic dynamics prediction in latent space
- Imagination-based planning and policy optimization
- Dreamer algorithm and modern variants

### Section 3: Sample Efficiency and Transfer Learning
- Prioritized experience replay and data augmentation
- Auxiliary tasks for improved learning
- Transfer learning techniques and meta-learning
- Domain adaptation and curriculum learning

### Section 4: Hierarchical Reinforcement Learning
- Options framework and temporal abstraction
- Hierarchical policy architectures
- Skill discovery and composition
- Applications to complex task domains

## Project Structure

```
CA13/
â”œâ”€â”€ CA13.ipynb              # Main assignment notebook
â”œâ”€â”€ agents/                 # RL agent implementations
â”‚   â”œâ”€â”€ model*free*agent.py # Model-free RL agents
â”‚   â”œâ”€â”€ model*based*agent.py# Model-based RL agents
â”‚   â”œâ”€â”€ world*model*agent.py# World model-based agents
â”‚   â””â”€â”€ hierarchical_agent.py# Hierarchical RL agents
â”œâ”€â”€ models/                 # Neural network architectures
â”‚   â”œâ”€â”€ world_model.py      # VAE-based world models
â”‚   â”œâ”€â”€ dynamics_model.py   # Environment dynamics models
â”‚   â””â”€â”€ policy_networks.py  # Hierarchical policy networks
â”œâ”€â”€ environments/           # Custom environments
â”‚   â”œâ”€â”€ wrappers.py         # Environment wrappers
â”‚   â””â”€â”€ complex_tasks.py    # Complex task environments
â”œâ”€â”€ experiments/            # Training and evaluation scripts
â”‚   â”œâ”€â”€ train*world*model.py# World model training
â”‚   â”œâ”€â”€ compare_efficiency.py# Sample efficiency comparison
â”‚   â””â”€â”€ transfer_learning.py# Transfer learning experiments
â””â”€â”€ utils/                  # Utility functions
    â”œâ”€â”€ visualization.py    # Plotting and analysis tools
    â”œâ”€â”€ data_augmentation.py# Data augmentation utilities
    â””â”€â”€ evaluation.py       # Performance evaluation metrics
```

## Contents Overview

### Theoretical Foundations
- **Model-Based RL Mathematics**: Transition and reward model learning, planning algorithms
- **World Model Theory**: Variational inference, latent space dynamics, imagination-based learning
- **Sample Efficiency**: Experience replay, prioritization, auxiliary learning objectives
- **Transfer Learning**: Representation learning, fine-tuning, meta-learning algorithms

### Implementation Components
- **VAE World Models**: Encoder-decoder architectures with stochastic latent variables
- **Imagination-Based Agents**: Planning in learned latent space using world models
- **Sample-Efficient Algorithms**: Prioritized replay, data augmentation, auxiliary tasks
- **Transfer Learning Systems**: Multi-task learning, fine-tuning, domain adaptation

### Advanced Topics
- **Hierarchical RL**: Options framework, skill hierarchies, temporal abstraction
- **Meta-Learning**: Few-shot adaptation, gradient-based meta-learning
- **Curriculum Learning**: Automatic difficulty progression, teacher-student frameworks

## Evaluation Criteria

Your implementation will be evaluated based on:

1. **Correctness (40%)**: Accurate implementation of algorithms and mathematical formulations
2. **Efficiency (25%)**: Sample efficiency improvements and computational performance
3. **Innovation (20%)**: Creative extensions and novel approaches to the problems
4. **Analysis (15%)**: Quality of experimental analysis and insights

## Getting Started

1. **Environment Setup**: Ensure all dependencies are installed
2. **Code Review**: Understand the provided base implementations
3. **Incremental Development**: Start with simpler components and build complexity
4. **Testing**: Validate each component before integration
5. **Experimentation**: Run comprehensive experiments and analyze results

## Expected Outcomes

By the end of this assignment, you will have:

- **Comprehensive Understanding**: Deep knowledge of advanced model-based RL techniques
- **Practical Skills**: Ability to implement complex RL systems from scratch
- **Research Perspective**: Insight into current challenges and future directions
- **Portfolio Piece**: High-quality implementation demonstrating advanced RL capabilities

---

**Note**: This assignment represents the culmination of the Deep RL course, integrating concepts from model-free and model-based learning, advanced architectures, and practical deployment considerations. Focus on understanding the theoretical foundations while developing robust, efficient implementations.

Let's begin our exploration of advanced model-based reinforcement learning and world models! ðŸš€


```python

class ModelFreeAgent:
    """Base class for model-free RL agents."""
    
    def **init**(self, state*dim, action*dim, lr=1e-3):
        self.state*dim = state*dim
        self.action*dim = action*dim
        self.lr = lr
        self.replay_buffer = ReplayBuffer(10000)
        
    def act(self, state, epsilon=0.1):
        """Select action using epsilon-greedy policy."""
        if np.random.random() < epsilon:
            return np.random.randint(self.action_dim)
        return self.get*best*action(state)
    
    def get*best*action(self, state):
        """Get best action according to current policy."""
        raise NotImplementedError
    
    def update(self, batch):
        """Update agent from batch of experiences."""
        raise NotImplementedError

class DQNAgent(ModelFreeAgent):
    """Deep Q-Network agent (model-free)."""
    
    def **init**(self, state*dim, action*dim, lr=1e-3, gamma=0.99):
        super().**init**(state*dim, action*dim, lr)
        self.gamma = gamma
        
        self.q_network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, action_dim)
        )
        
        self.target*network = copy.deepcopy(self.q*network)
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)
        
        self.update_count = 0
        self.losses = []
        
    def get*best*action(self, state):
        """Get action with highest Q-value."""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q*values = self.q*network(state_tensor)
            return q_values.argmax().item()
    
    def update(self, batch):
        """Update Q-network using DQN loss."""
        states, actions, rewards, next_states, dones = batch
        
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next*states = torch.FloatTensor(next*states)
        dones = torch.BoolTensor(dones)
        
        current*q = self.q*network(states).gather(1, actions.unsqueeze(1))
        
        with torch.no_grad():
            next*q = self.target*network(next_states).max(1)[0]
            target*q = rewards + (self.gamma * next*q * (~dones))
        
        loss = F.mse*loss(current*q.squeeze(), target_q)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        self.losses.append(loss.item())
        self.update_count += 1
        
        if self.update_count % 100 == 0:
            self.target*network.load*state*dict(self.q*network.state_dict())
        
        return loss.item()

class ModelBasedAgent:
    """Model-based RL agent using learned dynamics."""
    
    def **init**(self, state*dim, action*dim, lr=1e-3, planning_horizon=5):
        self.state*dim = state*dim
        self.action*dim = action*dim
        self.lr = lr
        self.planning*horizon = planning*horizon
        
        self.dynamics_model = nn.Sequential(
            nn.Linear(state*dim + action*dim, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, state_dim)
        )
        
        self.reward_model = nn.Sequential(
            nn.Linear(state*dim + action*dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self.value_network = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self.dynamics*optimizer = optim.Adam(self.dynamics*model.parameters(), lr=lr)
        self.reward*optimizer = optim.Adam(self.reward*model.parameters(), lr=lr)
        self.value*optimizer = optim.Adam(self.value*network.parameters(), lr=lr)
        
        self.model_buffer = ReplayBuffer(10000)
        self.planning_buffer = ReplayBuffer(5000)
        
        self.model_losses = []
        self.value_losses = []
        
    def act(self, state, epsilon=0.1):
        """Select action using model-based planning."""
        if np.random.random() < epsilon:
            return np.random.randint(self.action_dim)
        
        return self.plan_action(state)
    
    def plan_action(self, state):
        """Plan best action using learned model."""
        best_action = 0
        best_value = float('-inf')
        
        for action in range(self.action_dim):
            value = self.simulate_trajectory(state, action)
            if value > best_value:
                best_value = value
                best_action = action
        
        return best_action
    
    def simulate*trajectory(self, initial*state, initial_action):
        """Simulate trajectory using learned model."""
        state = torch.FloatTensor(initial_state)
        total_reward = 0.0
        gamma = 0.99
        
        for step in range(self.planning_horizon):
            if step == 0:
                action = initial_action
            else:
                action = self.get*greedy*action(state)
            
            action_tensor = torch.FloatTensor([action])
            action*one*hot = F.one*hot(action*tensor.long(), self.action_dim).float()
            
            model*input = torch.cat([state, action*one_hot], dim=-1)
            
            with torch.no_grad():
                next*state = self.dynamics*model(model_input)
                reward = self.reward*model(model*input).item()
            
            total_reward += (gamma ** step) * reward
            state = next_state
        
        with torch.no_grad():
            terminal*value = self.value*network(state).item()
            total*reward += (gamma ** self.planning*horizon) * terminal_value
        
        return total_reward
    
    def get*greedy*action(self, state):
        """Get greedy action for planning."""
        best_action = 0
        best_q = float('-inf')
        
        for action in range(self.action_dim):
            action_tensor = torch.FloatTensor([action])
            action*one*hot = F.one*hot(action*tensor.long(), self.action_dim).float()
            model*input = torch.cat([state, action*one_hot], dim=-1)
            
            with torch.no_grad():
                q*value = self.reward*model(model_input).item()
            
            if q*value > best*q:
                best*q = q*value
                best_action = action
        
        return best_action
    
    def update_model(self, batch):
        """Update dynamics and reward models."""
        states, actions, rewards, next_states, dones = batch
        
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next*states = torch.FloatTensor(next*states)
        
        actions*one*hot = F.one*hot(actions, self.action*dim).float()
        model*input = torch.cat([states, actions*one_hot], dim=-1)
        
        pred*next*states = self.dynamics*model(model*input)
        dynamics*loss = F.mse*loss(pred*next*states, next_states)
        
        self.dynamics*optimizer.zero*grad()
        dynamics_loss.backward()
        self.dynamics_optimizer.step()
        
        pred*rewards = self.reward*model(model_input).squeeze()
        reward*loss = F.mse*loss(pred_rewards, rewards)
        
        self.reward*optimizer.zero*grad()
        reward_loss.backward()
        self.reward_optimizer.step()
        
        total*model*loss = dynamics*loss.item() + reward*loss.item()
        self.model*losses.append(total*model_loss)
        
        return total*model*loss
    
    def update*value*function(self, batch):
        """Update value function using temporal difference learning."""
        states, actions, rewards, next_states, dones = batch
        
        states = torch.FloatTensor(states)
        rewards = torch.FloatTensor(rewards)
        next*states = torch.FloatTensor(next*states)
        dones = torch.BoolTensor(dones)
        
        current*values = self.value*network(states).squeeze()
        
        with torch.no_grad():
            next*values = self.value*network(next_states).squeeze()
            targets = rewards + 0.99 * next_values * (~dones)
        
        value*loss = F.mse*loss(current_values, targets)
        
        self.value*optimizer.zero*grad()
        value_loss.backward()
        self.value_optimizer.step()
        
        self.value*losses.append(value*loss.item())
        
        return value_loss.item()

class HybridDynaAgent:
    """Dyna-Q style hybrid agent combining model-free and model-based learning."""
    
    def **init**(self, state*dim, action*dim, lr=1e-3, planning_steps=5):
        self.state*dim = state*dim
        self.action*dim = action*dim
        self.planning*steps = planning*steps
        
        self.q*table = defaultdict(lambda: np.zeros(action*dim))
        self.lr = lr
        self.gamma = 0.99
        
        self.model = {}  # (s,a) -> (r, s', done)
        self.visited_states = set()       
        self.experience_buffer = deque(maxlen=10000)
        
    def act(self, state, epsilon=0.1):
        """Epsilon-greedy action selection."""
        if np.random.random() < epsilon:
            return np.random.randint(self.action_dim)
        
        state_key = tuple(state) if isinstance(state, np.ndarray) else state
        return np.argmax(self.q*table[state*key])
    
    def update(self, state, action, reward, next_state, done):
        """Dyna-Q update: direct RL + model learning + planning."""
        state_key = tuple(state) if isinstance(state, np.ndarray) else state
        next*state*key = tuple(next*state) if isinstance(next*state, np.ndarray) else next_state
        
        if done:
            target = reward
        else:
            target = reward + self.gamma * np.max(self.q*table[next*state_key])
        
        self.q*table[state*key][action] += self.lr * (target - self.q*table[state*key][action])
        
        self.model[(state*key, action)] = (reward, next*state_key, done)
        self.visited*states.add(state*key)
        self.experience*buffer.append((state*key, action, reward, next*state*key, done))
        
        self.planning_updates()
    
    def planning_updates(self):
        """Perform planning updates using learned model."""
        if len(self.experience_buffer) == 0:
            return
        
        for * in range(self.planning*steps):
            if len(self.experience_buffer) > 0:
                state*key, action, reward, next*state*key, done = random.choice(self.experience*buffer)
                
                if done:
                    target = reward
                else:
                    target = reward + self.gamma * np.max(self.q*table[next*state_key])
                
                self.q*table[state*key][action] += self.lr * (target - self.q*table[state*key][action])

class ReplayBuffer:
    """Experience replay buffer for storing transitions."""
    
    def **init**(self, capacity):
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        batch = random.sample(self.buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        return states, actions, rewards, next_states, dones
    
    def **len**(self):
        return len(self.buffer)

class SimpleGridWorld:
    
    def **init**(self, size=5):
        self.size = size
        self.state = [0, 0]  # [row, col]
        self.goal = [size-1, size-1]
        self.action_space = 4  # up, right, down, left
        
    def reset(self):
        self.state = [0, 0]
        return np.array(self.state, dtype=np.float32)
    
    def step(self, action):
        moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]
        
        new_row = max(0, min(self.size-1, self.state[0] + moves[action][0]))
        new_col = max(0, min(self.size-1, self.state[1] + moves[action][1]))
        
        self.state = [new*row, new*col]
        
        if self.state == self.goal:
            reward = 10.0
            done = True
        else:
            reward = -0.1  # Small negative reward for each step
            done = False
        
        return np.array(self.state, dtype=np.float32), reward, done, {}

def compare*agents*performance():
    print("ðŸ”¬ Comparing Model-Free vs Model-Based RL Performance")
    
    env = SimpleGridWorld(size=5)
    
    model*free*agent = DQNAgent(state*dim=2, action*dim=4, lr=1e-3)
    model*based*agent = ModelBasedAgent(state*dim=2, action*dim=4, lr=1e-3)
    hybrid*agent = HybridDynaAgent(state*dim=2, action_dim=4, lr=0.1)
    
    agents = {
        'Model-Free (DQN)': model*free*agent,
        'Model-Based': model*based*agent,
        'Hybrid (Dyna-Q)': hybrid_agent
    }
    
    results = {name: {'episodes': [], 'rewards': [], 'steps': []} for name in agents.keys()}
    
    num_episodes = 200
    batch_size = 32
    
    for episode in range(num_episodes):
        for agent_name, agent in agents.items():
            state = env.reset()
            episode_reward = 0
            episode_steps = 0
            max_steps = 100
            
            for step in range(max_steps):
                if agent_name == 'Hybrid (Dyna-Q)':
                    action = agent.act(state, epsilon=max(0.1, 1.0 - episode/100))
                else:
                    action = agent.act(state, epsilon=max(0.1, 1.0 - episode/100))
                
                next*state, reward, done, * = env.step(action)
                episode_reward += reward
                episode_steps += 1
                
                if agent_name == 'Model-Free (DQN)':
                    agent.replay*buffer.push(state, action, reward, next*state, done)
                    if len(agent.replay*buffer) > batch*size:
                        batch = agent.replay*buffer.sample(batch*size)
                        agent.update(batch)
                
                elif agent_name == 'Model-Based':
                    agent.model*buffer.push(state, action, reward, next*state, done)
                    if len(agent.model*buffer) > batch*size:
                        batch = agent.model*buffer.sample(batch*size)
                        agent.update_model(batch)
                        agent.update*value*function(batch)
                
                elif agent_name == 'Hybrid (Dyna-Q)':
                    agent.update(state, action, reward, next_state, done)
                
                if done:
                    break
                
                state = next_state
            
            results[agent_name]['episodes'].append(episode)
            results[agent*name]['rewards'].append(episode*reward)
            results[agent*name]['steps'].append(episode*steps)
        
        if episode % 50 == 0:
            print(f"Episode {episode}:"
                  for agent_name in agents.keys():
                      recent*reward = np.mean(results[agent*name]['rewards'][-10:]) if len(results[agent_name]['rewards']) >= 10 else 0
                      print(f"  {agent*name}: {recent*reward:.2f} avg reward")
    
    return results

def visualize_comparison(results):
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    for agent_name, data in results.items():
        window_size = 20
        if len(data['rewards']) >= window_size:
            smoothed*rewards = pd.Series(data['rewards']).rolling(window*size).mean()
            axes[0,0].plot(data['episodes'], smoothed*rewards, label=agent*name, linewidth=2)
    
    axes[0,0].set_title('Learning Curves (Smoothed Rewards)')
    axes[0,0].set_xlabel('Episode')
    axes[0,0].set_ylabel('Episode Reward')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    for agent_name, data in results.items():
        cumulative_steps = np.cumsum(data['steps'])
        axes[0,1].plot(cumulative*steps, data['rewards'], label=agent*name, linewidth=2)
    
    axes[0,1].set_title('Sample Efficiency')
    axes[0,1].set_xlabel('Total Environment Steps')
    axes[0,1].set_ylabel('Episode Reward')
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)
    
    final_performance = {}
    for agent_name, data in results.items():
        final*performance[agent*name] = np.mean(data['rewards'][-20:])  # Last 20 episodes
    
    agent*names = list(final*performance.keys())
    performance*values = list(final*performance.values())
    
    bars = axes[1,0].bar(agent*names, performance*values, color=['skyblue', 'lightcoral', 'lightgreen'])
    axes[1,0].set_title('Final Performance (Last 20 Episodes)')
    axes[1,0].set_ylabel('Average Episode Reward')
    axes[1,0].tick_params(axis='x', rotation=45)
    
    for bar, value in zip(bars, performance_values):
        axes[1,0].text(bar.get*x() + bar.get*width()/2, bar.get_height() + 0.1,
                      f'{value:.2f}', ha='center', va='bottom')
    
    steps*to*completion = {}
    for agent_name, data in results.items():
        steps*to*completion[agent_name] = np.mean(data['steps'][-20:])
    
    agent*names = list(steps*to_completion.keys())
    steps*values = list(steps*to_completion.values())
    
    bars = axes[1,1].bar(agent*names, steps*values, color=['skyblue', 'lightcoral', 'lightgreen'])
    axes[1,1].set_title('Average Steps to Completion')
    axes[1,1].set_ylabel('Steps per Episode')
    axes[1,1].tick_params(axis='x', rotation=45)
    
    for bar, value in zip(bars, steps_values):
        axes[1,1].text(bar.get*x() + bar.get*width()/2, bar.get_height() + 0.5,
                      f'{value:.1f}', ha='center', va='bottom')
    
    plt.tight_layout()
    plt.show()
    
    return final_performance

print("ðŸš€ Starting Model-Free vs Model-Based RL Comparison!")
comparison*results = compare*agents_performance()
final*performance = visualize*comparison(comparison_results)

print("\\nðŸ“Š Comparison Results Summary:")
for agent*name, performance in final*performance.items():
    print(f"  {agent_name}: {performance:.2f} average reward")
    
print("\\nðŸ’¡ Key Insights:")
print("  â€¢ Model-free methods often achieve higher asymptotic performance")
print ("  â€¢ Model-based methods typically learn faster initially")
print("  â€¢ Hybrid approaches can combine benefits of both")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

```


      Cell In[1], line 557
        print("  â€¢ Hybrid approaches can combine benefits of both")"
                                                                   ^
    SyntaxError: unterminated string literal (detected at line 557)



# Section 2: World Models and Imagination-based Learning

## 2.1 Theoretical Foundations of World Models

World models represent learned internal representations of environment dynamics that enable agents to "imagine" and plan without direct interaction with the environment.

### Core Concepts

**World Model Components:**
1. **Representation Learning**: Encode high-dimensional observations into compact latent states
2. **Dynamics Model**: Predict next latent state given current state and action
3. **Reward Model**: Predict rewards in the latent space
4. **Decoder Model**: Reconstruct observations from latent states

**Mathematical Framework:**
- **Encoder**: $z*t = \text{Encode}(o*t)$ maps observation $o*t$ to latent state $z*t$
- **Dynamics**: $z*{t+1} = f(z*t, a*t) + \epsilon*t$ where $\epsilon_t \sim \mathcal{N}(0, \Sigma)$
- **Reward**: $r*t = R(z*t, a_t)$
- **Decoder**: $\hat{o}*t = \text{Decode}(z*t)$

## 2.2 Variational World Models

### Variational Autoencoders (vae) for World Modeling

World models often use VAEs to learn stochastic latent representations:

**Encoder (Recognition Model):**
$$q*\phi(z*t | o*t) = \mathcal{N}(z*t; \mu*\phi(o*t), \sigma*\phi^2(o*t))$$

**Prior (Dynamics Model):**
$$p*\theta(z*{t+1} | z*t, a*t) = \mathcal{N}(z*{t+1}; \mu*\theta(z*t, a*t), \sigma*\theta^2(z*t, a_t))$$

**Decoder (Generative Model):**
$$p*\psi(o*t | z*t) = \mathcal{N}(o*t; \mu*\psi(z*t), \sigma*\psi^2(z*t))$$

**ELBO Objective:**
$$\mathcal{L}*{ELBO} = \mathbb{E}*{q*\phi(z|o)} [\log p*\psi(o|z)] - D*{KL}[q*\phi(z|o) || p(z)]$$

## 2.3 Planning in Learned Latent Space

Once a world model is learned, planning can be performed in the compact latent space:

### Model Predictive Control (mpc) in Latent Space
1. **Imagination Rollout**: Use world model to simulate future trajectories
2. **Action Optimization**: Optimize action sequences to maximize predicted rewards
3. **Execution**: Execute only the first action, then replan

**Planning Objective:**
$$a^**{1:H} = \arg\max*{a*{1:H}} \mathbb{E}*{z*{1:H} \sim p*\theta} \left[ \sum*{t=1}^H R(z*t, a_t) \right]$$

### Dreamer Algorithm
Dreamer combines world models with policy gradients:
1. **Collect Experience**: Gather real environment data
2. **Learn World Model**: Train VAE-based world model
3. **Imagine Trajectories**: Generate synthetic experience in latent space  
4. **Learn Behaviors**: Train actor-critic in imagined trajectories

## 2.4 Advantages and Challenges

### Advantages of World Models:
- **Sample Efficiency**: Learn from imagined experience
- **Transfer Learning**: Models can generalize across tasks
- **Interpretability**: Learned representations can be visualized
- **Planning**: Enable sophisticated planning algorithms

### Challenges:
- **Model Bias**: Errors compound during long rollouts
- **Representation Learning**: High-dimensional observations are challenging
- **Stochasticity**: Modeling complex stochastic dynamics
- **Computational Cost**: Training and maintaining world models

## 2.5 Modern Approaches

### Muzero
Combines tree search with learned models:
- Learns value, policy, and dynamics jointly
- Uses tree search for planning
- Achieves superhuman performance in Go, Chess, and Shogi

### Dreamer V2/v3
Improvements to original Dreamer:
- Better regularization techniques
- Improved world model architectures
- Enhanced policy learning in imagination

### Model-based Meta-learning
Using world models for few-shot adaptation:
- Learn generalizable world model components
- Quickly adapt to new environments
- Transfer dynamics knowledge across domains


```python

class VariationalWorldModel(nn.Module):
    """VAE-based world model for learning environment dynamics."""
    
    def **init**(self, obs*dim, action*dim, latent*dim=64, hidden*dim=128):
        super().**init**()
        self.obs*dim = obs*dim
        self.action*dim = action*dim
        self.latent*dim = latent*dim
        self.hidden*dim = hidden*dim
        
        self.encoder = nn.Sequential(
            nn.Linear(obs*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, hidden*dim),
            nn.ReLU()
        )
        
        self.encoder*mu = nn.Linear(hidden*dim, latent_dim)
        self.encoder*logvar = nn.Linear(hidden*dim, latent_dim)
        
        self.dynamics = nn.Sequential(
            nn.Linear(latent*dim + action*dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, hidden*dim),
            nn.ReLU()
        )
        
        self.dynamics*mu = nn.Linear(hidden*dim, latent_dim)
        self.dynamics*logvar = nn.Linear(hidden*dim, latent_dim)
        
        self.reward_model = nn.Sequential(
            nn.Linear(latent*dim + action*dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, hidden*dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )
        
        self.decoder = nn.Sequential(
            nn.Linear(latent*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, obs*dim)
        )
    
    def encode(self, obs):
        """Encode observation to latent distribution parameters."""
        h = self.encoder(obs)
        mu = self.encoder_mu(h)
        logvar = self.encoder_logvar(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        """Reparameterization trick for VAE."""
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            return mu + eps * std
        else:
            return mu
    
    def dynamics*forward(self, latent*state, action):
        """Predict next latent state given current state and action."""
        if len(action.shape) == 1:
            action*one*hot = F.one*hot(action.long(), self.action*dim).float()
        else:
            action*one*hot = action
        
        dynamics*input = torch.cat([latent*state, action*one*hot], dim=-1)
        h = self.dynamics(dynamics_input)
        
        mu = self.dynamics_mu(h)
        logvar = self.dynamics_logvar(h)
        
        return mu, logvar
    
    def predict*reward(self, latent*state, action):
        """Predict reward given latent state and action."""
        if len(action.shape) == 1:
            action*one*hot = F.one*hot(action.long(), self.action*dim).float()
        else:
            action*one*hot = action
        
        reward*input = torch.cat([latent*state, action*one*hot], dim=-1)
        return self.reward*model(reward*input)
    
    def decode(self, latent_state):
        """Decode latent state to observation."""
        return self.decoder(latent_state)
    
    def forward(self, obs, action=None):
        """Full forward pass through world model."""
        mu*enc, logvar*enc = self.encode(obs)
        latent*state = self.reparameterize(mu*enc, logvar_enc)
        
        recon*obs = self.decode(latent*state)
        
        results = {
            'latent*state': latent*state,
            'mu*enc': mu*enc,
            'logvar*enc': logvar*enc,
            'recon*obs': recon*obs
        }
        
        if action is not None:
            mu*dyn, logvar*dyn = self.dynamics*forward(latent*state, action)
            next*latent = self.reparameterize(mu*dyn, logvar_dyn)
            pred*reward = self.predict*reward(latent_state, action)
            
            results.update({
                'mu*dyn': mu*dyn,
                'logvar*dyn': logvar*dyn,
                'next*latent': next*latent,
                'pred*reward': pred*reward
            })
        
        return results
    
    def imagine*trajectory(self, initial*obs, actions):
        """Imagine trajectory given initial observation and action sequence."""
        batch*size = initial*obs.shape[0]
        sequence_length = len(actions)
        
        mu*enc, logvar*enc = self.encode(initial_obs)
        current*latent = self.reparameterize(mu*enc, logvar_enc)
        
        trajectory = {
            'latent*states': [current*latent],
            'observations': [self.decode(current_latent)],
            'rewards': [],
            'actions': []
        }
        
        for t in range(sequence_length):
            action = actions[t]
            trajectory['actions'].append(action)
            
            pred*reward = self.predict*reward(current_latent, action)
            trajectory['rewards'].append(pred_reward)
            
            mu*dyn, logvar*dyn = self.dynamics*forward(current*latent, action)
            next*latent = self.reparameterize(mu*dyn, logvar_dyn)
            
            current*latent = next*latent
            trajectory['latent*states'].append(current*latent)
            trajectory['observations'].append(self.decode(current_latent))
        
        return trajectory

class WorldModelLoss:
    """Loss functions for training world models."""
    
    def **init**(self, recon*weight=1.0, kl*weight=1.0, reward*weight=1.0, dynamics*weight=1.0):
        self.recon*weight = recon*weight
        self.kl*weight = kl*weight
        self.reward*weight = reward*weight
        self.dynamics*weight = dynamics*weight
    
    def reconstruction*loss(self, recon*obs, target_obs):
        """Reconstruction loss between predicted and actual observations."""
        return F.mse*loss(recon*obs, target_obs)
    
    def kl*divergence*loss(self, mu, logvar):
        """KL divergence loss for VAE regularization."""
        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / mu.shape[0]
    
    def reward*loss(self, pred*reward, target_reward):
        """Reward prediction loss."""
        return F.mse*loss(pred*reward.squeeze(), target_reward)
    
    def dynamics*loss(self, pred*next*latent, target*next_latent):
        """Dynamics prediction loss in latent space."""
        return F.mse*loss(pred*next*latent, target*next_latent)
    
    def compute*total*loss(self, model*output, target*obs, target*reward=None, target*next_obs=None):
        """Compute total loss for world model training."""
        losses = {}
        
        recon*loss = self.reconstruction*loss(model*output['recon*obs'], target_obs)
        losses['reconstruction'] = recon_loss
        
        kl*loss = self.kl*divergence*loss(model*output['mu*enc'], model*output['logvar_enc'])
        losses['kl*divergence'] = kl*loss
        
        total*loss = self.recon*weight * recon*loss + self.kl*weight * kl_loss
        
        if target*reward is not None and 'pred*reward' in model_output:
            reward*loss = self.reward*loss(model*output['pred*reward'], target_reward)
            losses['reward'] = reward_loss
            total*loss += self.reward*weight * reward_loss
        
        if target*next*obs is not None and 'mu*dyn' in model*output:
            with torch.no_grad():
                target*mu, * = model*output['mu*enc'], model*output['logvar*enc']  # Placeholder - need next obs encoding
            
            dynamics*loss = F.mse*loss(model*output['mu*dyn'], model*output['next*latent'])
            losses['dynamics'] = dynamics_loss
            total*loss += self.dynamics*weight * dynamics_loss
        
        losses['total'] = total_loss
        return losses

class ImaginationBasedAgent:
    """Agent that uses world model for planning and learning."""
    
    def **init**(self, obs*dim, action*dim, latent*dim=64, planning*horizon=8):
        self.obs*dim = obs*dim
        self.action*dim = action*dim
        self.latent*dim = latent*dim
        self.planning*horizon = planning*horizon
        
        self.world*model = VariationalWorldModel(obs*dim, action*dim, latent*dim)
        self.world*model*optimizer = optim.Adam(self.world_model.parameters(), lr=1e-3)
        self.world*model*loss = WorldModelLoss()
        
        self.policy_net = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )
        
        self.value_net = nn.Sequential(
            nn.Linear(latent_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        self.policy*optimizer = optim.Adam(self.policy*net.parameters(), lr=1e-3)
        self.value*optimizer = optim.Adam(self.value*net.parameters(), lr=1e-3)
        
        self.experience_buffer = ReplayBuffer(10000)
        
        self.world*model*losses = []
        self.policy_losses = []
        
    def act(self, obs, epsilon=0.1):
        """Select action using imagination-based planning."""
        if np.random.random() < epsilon:
            return np.random.randint(self.action_dim)
        
        return self.plan*with*world_model(obs)
    
    def plan*with*world_model(self, obs):
        """Plan action using world model imagination."""
        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)
        
        with torch.no_grad():
            mu, logvar = self.world*model.encode(obs*tensor)
            current*latent = self.world*model.reparameterize(mu, logvar)
        
        best_action = 0
        best_value = float('-inf')
        
        for action in range(self.action_dim):
            imagined*value = self.imagine*value(current_latent, action)
            if imagined*value > best*value:
                best*value = imagined*value
                best_action = action
        
        return best_action
    
    def imagine*value(self, initial*latent, initial_action):
        """Estimate value of taking initial action using imagination."""
        current*latent = initial*latent
        total_value = 0.0
        gamma = 0.99
        
        with torch.no_grad():
            for step in range(self.planning_horizon):
                if step == 0:
                    action = initial_action
                else:
                    action*probs = self.policy*net(current_latent)
                    action = action_probs.argmax().item()
                
                action_tensor = torch.tensor([action])
                pred*reward = self.world*model.predict*reward(current*latent, action_tensor)
                total*value += (gamma ** step) * pred*reward.item()
                
                mu*dyn, logvar*dyn = self.world*model.dynamics*forward(current*latent, action*tensor)
                current*latent = self.world*model.reparameterize(mu*dyn, logvar*dyn)
            
            terminal*value = self.value*net(current_latent)
            total*value += (gamma ** self.planning*horizon) * terminal_value.item()
        
        return total_value
    
    def update*world*model(self, batch_size=32):
        """Update world model from experience buffer."""
        if len(self.experience*buffer) < batch*size:
            return None
        
        states, actions, rewards, next*states, dones = self.experience*buffer.sample(batch_size)
        
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next*states = torch.FloatTensor(next*states)
        
        model*output = self.world*model(states, actions)
        
        losses = self.world*model*loss.compute*total*loss(
            model*output, states, rewards, next*states
        )
        
        self.world*model*optimizer.zero_grad()
        losses['total'].backward()
        self.world*model*optimizer.step()
        
        self.world*model*losses.append(losses['total'].item())
        
        return losses
    
    def update*policy*with*imagination(self, num*imagination_episodes=10):
        """Update policy using imagined trajectories."""
        if len(self.experience_buffer) == 0:
            return None
        
        total*policy*loss = 0.0
        total*value*loss = 0.0
        
        for * in range(num*imagination_episodes):
            states, *, *, *, * = self.experience_buffer.sample(1)
            initial_obs = torch.FloatTensor(states[0]).unsqueeze(0)
            
            actions = [torch.randint(0, self.action*dim, (1,)) for * in range(self.planning_horizon)]
            
            trajectory = self.world*model.imagine*trajectory(initial_obs, actions)
            
            policy_loss = 0.0
            value_loss = 0.0
            
            for t, (latent_state, action, reward) in enumerate(zip(
                trajectory['latent_states'][:-1], 
                trajectory['actions'], 
                trajectory['rewards']
            )):
                action*probs = self.policy*net(latent_state)
                log*prob = torch.log(action*probs.gather(1, action.unsqueeze(1)))
                policy*loss -= log*prob * reward.detach()
                
                value*pred = self.value*net(latent_state)
                value*loss += F.mse*loss(value_pred, reward.detach())
            
            total*policy*loss += policy_loss.item()
            total*value*loss += value_loss.item()
            
            self.policy*optimizer.zero*grad()
            policy*loss.backward(retain*graph=True)
            self.policy_optimizer.step()
            
            self.value*optimizer.zero*grad()
            value_loss.backward()
            self.value_optimizer.step()
        
        avg*policy*loss = total*policy*loss / num*imagination*episodes
        avg*value*loss = total*value*loss / num*imagination*episodes
        
        self.policy*losses.append(avg*policy_loss)
        
        return {
            'policy*loss': avg*policy_loss,
            'value*loss': avg*value_loss
        }

def demonstrate*world*model_learning():
    """Demonstrate world model learning and imagination-based planning."""
    print("ðŸŒ Demonstrating World Model Learning and Imagination")
    
    env = SimpleGridWorld(size=4)
    
    agent = ImaginationBasedAgent(obs*dim=2, action*dim=4, latent*dim=16, planning*horizon=5)
    
    num_episodes = 150
    batch_size = 16
    world*model*updates = 5
    imagination_updates = 3
    
    episode_rewards = []
    world*model*loss_history = []
    
    print("Starting training...")
    
    for episode in range(num_episodes):
        state = env.reset()
        episode_reward = 0
        episode_steps = 0
        max_steps = 50
        
        for step in range(max_steps):
            epsilon = max(0.1, 1.0 - episode / 80)
            action = agent.act(state, epsilon=epsilon)
            
            next*state, reward, done, * = env.step(action)
            episode_reward += reward
            episode_steps += 1
            
            agent.experience*buffer.push(state, action, reward, next*state, done)
            
            if len(agent.experience*buffer) > batch*size:
                for * in range(world*model_updates):
                    losses = agent.update*world*model(batch_size)
                    if losses:
                        world*model*loss_history.append(losses['total'].item())
            
            if episode > 20 and len(agent.experience*buffer) > batch*size:
                agent.update*policy*with*imagination(imagination*updates)
            
            if done:
                break
            
            state = next_state
        
        episode*rewards.append(episode*reward)
        
        if episode % 30 == 0 and episode > 0:
            recent*reward = np.mean(episode*rewards[-10:])
            recent*wm*loss = np.mean(world*model*loss*history[-50:]) if world*model*loss*history else 0
            print(f"Episode {episode}: Avg Reward: {recent*reward:.2f}, WM Loss: {recent*wm_loss:.4f}")
    
    return agent, episode*rewards, world*model*loss*history

def visualize*world*model*performance(agent, episode*rewards, world*model*losses):
    """Visualize world model learning performance."""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    window_size = 10
    if len(episode*rewards) >= window*size:
        smoothed*rewards = pd.Series(episode*rewards).rolling(window_size).mean()
        axes[0,0].plot(smoothed_rewards, linewidth=2, color='blue')
    else:
        axes[0,0].plot(episode_rewards, linewidth=2, color='blue')
    
    axes[0,0].set_title('Learning Curve (Episode Rewards)')
    axes[0,0].set_xlabel('Episode')
    axes[0,0].set_ylabel('Episode Reward')
    axes[0,0].grid(True, alpha=0.3)
    
    if world*model*losses:
        axes[0,1].plot(world*model*losses, linewidth=1, alpha=0.7, color='red')
        if len(world*model*losses) >= 20:
            smoothed*losses = pd.Series(world*model_losses).rolling(20).mean()
            axes[0,1].plot(smoothed_losses, linewidth=2, color='darkred')
    
    axes[0,1].set_title('World Model Training Loss')
    axes[0,1].set_xlabel('Update Step')
    axes[0,1].set_ylabel('Loss')
    axes[0,1].grid(True, alpha=0.3)
    
    if len(agent.experience_buffer) > 50:
        states, *, *, *, * = agent.experience_buffer.sample(50)
        states_tensor = torch.FloatTensor(states)
        
        with torch.no_grad():
            mu, * = agent.world*model.encode(states_tensor)
            latent_states = mu.numpy()
        
        if latent_states.shape[1] >= 2:
            axes[1,0].scatter(latent*states[:, 0], latent*states[:, 1], alpha=0.6, c=range(len(latent_states)))
            axes[1,0].set_title('Learned Latent Space Representation')
            axes[1,0].set_xlabel('Latent Dimension 1')
            axes[1,0].set_ylabel('Latent Dimension 2')
            axes[1,0].grid(True, alpha=0.3)
    
    planning_horizons = [1, 3, 5, 8, 10]
    planning_performance = []
    
    for horizon in planning_horizons:
        test*agent = ImaginationBasedAgent(obs*dim=2, action*dim=4, planning*horizon=horizon)
        test*agent.world*model = agent.world_model  # Use trained world model
        test*agent.policy*net = agent.policy_net    # Use trained policy
        test*agent.value*net = agent.value_net      # Use trained value function
        
        test_env = SimpleGridWorld(size=4)
        test_rewards = []
        
        for _ in range(10):  # Quick test
            state = test_env.reset()
            episode_reward = 0
            
            for _ in range(20):
                action = test*agent.plan*with*world*model(state)
                next*state, reward, done, * = test_env.step(action)
                episode_reward += reward
                
                if done:
                    break
                state = next_state
            
            test*rewards.append(episode*reward)
        
        planning*performance.append(np.mean(test*rewards))
    
    axes[1,1].plot(planning*horizons, planning*performance, 'o-', linewidth=2, markersize=8)
    axes[1,1].set_title('Planning Horizon vs Performance')
    axes[1,1].set_xlabel('Planning Horizon')
    axes[1,1].set_ylabel('Average Reward')
    axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return planning_performance

print("ðŸš€ Starting World Model Learning Demonstration!")
trained*agent, episode*rewards, wm*losses = demonstrate*world*model*learning()
planning*analysis = visualize*world*model*performance(trained*agent, episode*rewards, wm_losses)

print("\nðŸŒ World Model Learning Results:")
print(f"  â€¢ Final average reward: {np.mean(episode_rewards[-10:]):.2f}")
print(f"  â€¢ World model converged to loss: {np.mean(wm_losses[-20:]):.4f}")
print(f"  â€¢ Optimal planning horizon: {[1,3,5,8,10][np.argmax(planning_analysis)]}")

print("\nðŸ’¡ Key Insights from World Model Learning:")
print("  â€¢ World models enable sample-efficient learning through imagination")
print("  â€¢ Planning horizon affects performance - too short lacks foresight, too long accumulates errors")
print("  â€¢ Learned latent representations capture environment structure")
print("  â€¢ Imagination-based policy updates improve without real environment interaction")

```

# Section 3: Sample Efficiency and Transfer Learning

## 3.1 Sample Efficiency Challenges in Deep Rl

Sample efficiency is one of the most critical challenges in deep reinforcement learning, particularly for real-world applications where data collection is expensive or dangerous.

### Why Is Sample Efficiency Important?

**Real-World Constraints:**
- **Cost**: Real-world interactions can be expensive (robotics, autonomous vehicles)
- **Time**: Learning from millions of samples is often impractical
- **Safety**: Exploratory actions in safety-critical domains can be dangerous
- **Reproducibility**: Limited samples make experiments more reliable

**Sample Complexity Factors:**
- **Environment Complexity**: High-dimensional state/action spaces
- **Sparse Rewards**: Learning signals are infrequent
- **Stochasticity**: Environmental noise requires more samples
- **Exploration**: Discovering good policies requires extensive exploration

## 3.2 Sample Efficiency Techniques

### 3.2.1 Experience Replay and Prioritization

**Experience Replay Benefits:**
- Reuse past experiences multiple times
- Break temporal correlations in data
- Enable off-policy learning

**Prioritized Experience Replay:**
Prioritize experiences based on temporal difference (TD) error:
$$P(i) = \frac{p*i^\alpha}{\sum*k p_k^\alpha}$$

Where $p*i = |\delta*i| + \epsilon$ and $\delta_i$ is the TD error.

### 3.2.2 Data Augmentation

**Techniques:**
- **Random Crops**: For image-based environments
- **Color Jittering**: Robust to lighting variations  
- **Random Shifts**: Translation invariance
- **Gaussian Noise**: Regularization effect

### 3.2.3 Auxiliary Tasks

Learn multiple tasks simultaneously to improve sample efficiency:
- **Pixel Control**: Predict pixel changes
- **Feature Control**: Control learned feature representations
- **Reward Prediction**: Predict future rewards
- **Value Function Replay**: Replay value function updates

## 3.3 Transfer Learning in Reinforcement Learning

Transfer learning enables agents to leverage knowledge from previous tasks to learn new tasks more efficiently.

### 3.3.1 Types of Transfer in Rl

**Policy Transfer:**
$$\pi*{target}(a|s) = f(\pi*{source}(a|s), s, \theta_{adapt})$$

**Value Function Transfer:**
$$Q*{target}(s,a) = g(Q*{source}(s,a), s, a, \phi_{adapt})$$

**Representation Transfer:**
$$\phi*{target}(s) = h(\phi*{source}(s), \psi_{adapt})$$

### 3.3.2 Transfer Learning Approaches

#### Fine-tuning
1. Pre-train on source task
2. Initialize target model with source weights
3. Fine-tune on target task with lower learning rate

#### Progressive Networks
- Freeze source network columns
- Add new columns for target tasks
- Use lateral connections between columns

#### Universal Value Functions (uvf)
Learn value functions conditioned on goals:
$$Q(s, a, g) = \text{Value of action } a \text{ in state } s \text{ for goal } g$$

## 3.4 Meta-learning and Few-shot Adaptation

Meta-learning enables agents to quickly adapt to new tasks with limited experience.

### 3.4.1 Model-agnostic Meta-learning (maml)

**Objective:**
$$\min*\theta \sum*{\tau \sim p(\mathcal{T})} \mathcal{L}*\tau(f*{\theta_\tau'})$$

Where $\theta*\tau' = \theta - \alpha \nabla*\theta \mathcal{L}*\tau(f*\theta)$

**MAML Algorithm:**
1. Sample batch of tasks
2. For each task, compute adapted parameters via gradient descent
3. Update meta-parameters using gradient through adaptation process

### 3.4.2 Gradient-based Meta-learning

**Reptile Algorithm:**
Simpler alternative to MAML:
$$\theta \leftarrow \theta + \beta \frac{1}{n} \sum*{i=1}^n (\phi*i - \theta)$$

Where $\phi_i$ is the result of training on task $i$.

## 3.5 Domain Adaptation and Sim-to-real Transfer

### 3.5.1 Domain Randomization

**Technique:**
Randomize simulation parameters during training:
- Physical properties (mass, friction, damping)
- Visual appearance (textures, lighting, colors)
- Sensor characteristics (noise, resolution, field of view)

**Benefits:**
- Learned policies are robust to domain variations
- Improved transfer from simulation to real world
- Reduced need for domain-specific engineering

### 3.5.2 Domain Adversarial Training

**Objective:**
$$\min*\theta \mathcal{L}*{task}(\theta) + \lambda \mathcal{L}_{domain}(\theta)$$

Where $\mathcal{L}_{domain}$ encourages domain-invariant features.

## 3.6 Curriculum Learning

Structure learning to progress from simple to complex tasks.

### 3.6.1 Curriculum Design Principles

**Manual Curriculum:**
- Hand-designed progression of tasks
- Expert knowledge of difficulty ordering
- Fixed curriculum regardless of agent performance

**Automatic Curriculum:**
- Adaptive task selection based on agent performance
- Learning progress as curriculum signal
- Self-paced learning approaches

### 3.6.2 Curriculum Learning Algorithms

**Teacher-Student Framework:**
- Teacher selects appropriate tasks for student
- Task difficulty based on student's current capability
- Optimize task selection for maximum learning progress

**Self-Play Curriculum:**
- Agent plays against previous versions of itself
- Automatic difficulty adjustment
- Prevents catastrophic forgetting of simpler strategies


```python

class PrioritizedReplayBuffer:
    """Prioritized experience replay buffer for improved sample efficiency."""
    
    def **init**(self, capacity, alpha=0.6, beta=0.4, beta_increment=1e-4):
        self.capacity = capacity
        self.alpha = alpha  # Priority exponent
        self.beta = beta    # Importance sampling exponent
        self.beta*increment = beta*increment
        
        self.buffer = []
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.position = 0
        self.max_priority = 1.0
        
    def push(self, state, action, reward, next_state, done):
        """Add experience to buffer with maximum priority."""
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        
        self.buffer[self.position] = (state, action, reward, next_state, done)
        self.priorities[self.position] = self.max_priority
        
        self.position = (self.position + 1) % self.capacity
    
    def sample(self, batch_size):
        """Sample batch with prioritized sampling."""
        if len(self.buffer) < batch_size:
            return None
        
        valid_priorities = self.priorities[:len(self.buffer)]
        probs = valid_priorities ** self.alpha
        probs /= probs.sum()
        
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        
        experiences = [self.buffer[idx] for idx in indices]
        states, actions, rewards, next_states, dones = zip(*experiences)
        
        total = len(self.buffer)
        weights = (total * probs[indices]) ** (-self.beta)
        weights /= weights.max()
        
        self.beta = min(1.0, self.beta + self.beta_increment)
        
        return (states, actions, rewards, next_states, dones), indices, weights
    
    def update*priorities(self, indices, td*errors):
        """Update priorities based on TD errors."""
        for idx, td*error in zip(indices, td*errors):
            priority = (abs(td_error) + 1e-6) ** self.alpha
            self.priorities[idx] = priority
            self.max*priority = max(self.max*priority, priority)
    
    def **len**(self):
        return len(self.buffer)

class DataAugmentationDQN(nn.Module):
    """DQN with data augmentation for improved sample efficiency."""
    
    def **init**(self, input*dim, action*dim, hidden_dim=128):
        super().**init**()
        self.input*dim = input*dim
        self.action*dim = action*dim
        
        self.q_network = nn.Sequential(
            nn.Linear(input*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, action*dim)
        )
        
        self.reward_predictor = nn.Sequential(
            nn.Linear(input*dim + action*dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self.next*state*predictor = nn.Sequential(
            nn.Linear(input*dim + action*dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, input*dim)
        )
    
    def forward(self, state, action=None):
        """Forward pass with optional auxiliary predictions."""
        q*values = self.q*network(state)
        
        if action is not None:
            if len(action.shape) == 1:
                action*one*hot = F.one*hot(action.long(), self.action*dim).float()
            else:
                action*one*hot = action
            
            aux*input = torch.cat([state, action*one_hot], dim=-1)
            reward*pred = self.reward*predictor(aux_input)
            next*state*pred = self.next*state*predictor(aux_input)
            
            return q*values, reward*pred, next*state*pred
        
        return q_values
    
    def apply*augmentation(self, state, augmentation*type='noise'):
        """Apply data augmentation to state."""
        if augmentation_type == 'noise':
            noise = torch.randn_like(state) * 0.1
            return state + noise
        
        elif augmentation_type == 'dropout':
            dropout*mask = torch.rand*like(state) > 0.1
            return state * dropout_mask.float()
        
        elif augmentation_type == 'scaling':
            scale = torch.rand(1).item() * 0.4 + 0.8  # Scale between 0.8 and 1.2
            return state * scale
        
        return state

class SampleEfficientAgent:
    """Agent with multiple sample efficiency techniques."""
    
    def **init**(self, state*dim, action*dim, lr=1e-3):
        self.state*dim = state*dim
        self.action*dim = action*dim
        
        self.network = DataAugmentationDQN(state*dim, action*dim)
        self.target_network = copy.deepcopy(self.network)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
        
        self.replay_buffer = PrioritizedReplayBuffer(capacity=10000)
        
        self.gamma = 0.99
        self.target*update*freq = 100
        self.update_count = 0
        
        self.aux*reward*weight = 0.1
        self.aux*dynamics*weight = 0.1
        
        self.losses = []
        self.td_errors = []
    
    def act(self, state, epsilon=0.1):
        """Select action with epsilon-greedy policy."""
        if np.random.random() < epsilon:
            return np.random.randint(self.action_dim)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            q*values = self.network(state*tensor)
            return q_values.argmax().item()
    
    def update(self, batch*size=32, use*aux_tasks=True, augmentation=True):
        """Update agent with prioritized replay and auxiliary tasks."""
        sample*result = self.replay*buffer.sample(batch_size)
        if sample_result is None:
            return None
        
        experiences, indices, weights = sample_result
        states, actions, rewards, next_states, dones = experiences
        
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next*states = torch.FloatTensor(next*states)
        dones = torch.BoolTensor(dones)
        weights = torch.FloatTensor(weights)
        
        if augmentation:
            aug*type = np.random.choice(['noise', 'dropout', 'scaling'])\n            states = self.network.apply*augmentation(states, aug_type)
            next*states = self.network.apply*augmentation(next*states, aug*type)
        
        current*q*values = self.network(states).gather(1, actions.unsqueeze(1))
        
        with torch.no_grad():
            next*q*values = self.target*network(next*states).max(1)[0]
            target*q*values = rewards + (self.gamma * next*q*values * (~dones))
        
        td*errors = (current*q*values.squeeze() - target*q_values).detach().numpy()
        
        q*loss = (weights * F.mse*loss(current*q*values.squeeze(), target*q*values, reduction='none')).mean()
        
        total*loss = q*loss
        
        if use*aux*tasks:
            q*values, reward*pred, next*state*pred = self.network(states, actions)
            
            aux*reward*loss = F.mse*loss(reward*pred.squeeze(), rewards)
            aux*dynamics*loss = F.mse*loss(next*state*pred, next*states)
            
            total*loss += self.aux*reward*weight * aux*reward_loss
            total*loss += self.aux*dynamics*weight * aux*dynamics_loss
        
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip*grad*norm*(self.network.parameters(), max*norm=1.0)
        self.optimizer.step()
        
        self.replay*buffer.update*priorities(indices, td_errors)
        
        self.update_count += 1
        if self.update*count % self.target*update_freq == 0:
            self.target*network.load*state*dict(self.network.state*dict())
        
        self.losses.append(total_loss.item())
        self.td*errors.extend(td*errors.tolist())
        
        return {
            'total*loss': total*loss.item(),
            'q*loss': q*loss.item(),
            'aux*reward*loss': aux*reward*loss.item() if use*aux*tasks else 0,
            'aux*dynamics*loss': aux*dynamics*loss.item() if use*aux*tasks else 0
        }

class TransferLearningAgent:
    """Agent with transfer learning capabilities."""
    
    def **init**(self, state*dim, action*dim, lr=1e-3):
        self.state*dim = state*dim
        self.action*dim = action*dim
        
        self.feature_extractor = nn.Sequential(
            nn.Linear(state_dim, 128),
            nn.ReLU(),
            nn.Linear(128, 64),
            nn.ReLU()
        )
        
        self.policy_heads = {}
        self.value_heads = {}
        
        self.feature*optimizer = optim.Adam(self.feature*extractor.parameters(), lr=lr)
        self.head_optimizers = {}
        
        self.transfer_performance = {}
    
    def add*task(self, task*name, action_dim=None):
        """Add a new task with its own policy and value heads."""
        if action_dim is None:
            action*dim = self.action*dim
        
        self.policy*heads[task*name] = nn.Sequential(
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, action_dim),
            nn.Softmax(dim=-1)
        )
        
        self.value*heads[task*name] = nn.Sequential(
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )
        
        task*params = list(self.policy*heads[task_name].parameters()) + \
                     list(self.value*heads[task*name].parameters())
        self.head*optimizers[task*name] = optim.Adam(task_params, lr=1e-3)
        
        self.transfer*performance[task*name] = []
    
    def get*action(self, state, task*name):
        """Get action for specific task."""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            features = self.feature*extractor(state*tensor)
            action*probs = self.policy*heads[task_name](features)
            return Categorical(action_probs).sample().item()
    
    def get*value(self, state, task*name):
        """Get value estimate for specific task."""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            features = self.feature*extractor(state*tensor)
            return self.value*heads[task*name](features).item()
    
    def update(self, states, actions, rewards, task*name, update*features=True):
        """Update agent for specific task."""
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        
        features = self.feature_extractor(states)
        action*probs = self.policy*heads[task_name](features)
        values = self.value*heads[task*name](features).squeeze()
        
        log*probs = torch.log(action*probs.gather(1, actions.unsqueeze(1))).squeeze()
        advantages = rewards - values.detach()
        policy*loss = -(log*probs * advantages).mean()
        
        value*loss = F.mse*loss(values, rewards)
        
        total*loss = policy*loss + 0.5 * value_loss
        
        self.head*optimizers[task*name].zero_grad()
        if update_features:
            self.feature*optimizer.zero*grad()
        
        total_loss.backward()
        
        self.head*optimizers[task*name].step()
        if update_features:
            self.feature_optimizer.step()
        
        return {
            'policy*loss': policy*loss.item(),
            'value*loss': value*loss.item()
        }
    
    def fine*tune*for*task(self, source*task, target*task, fine*tune_lr=1e-4):
        """Fine-tune from source task to target task."""
        self.policy*heads[target*task] = copy.deepcopy(self.policy*heads[source*task])
        self.value*heads[target*task] = copy.deepcopy(self.value*heads[source*task])
        
        task*params = list(self.policy*heads[target_task].parameters()) + \
                     list(self.value*heads[target*task].parameters())
        self.head*optimizers[target*task] = optim.Adam(task*params, lr=fine*tune_lr)
        
        self.transfer*performance[target*task] = []

class CurriculumLearningFramework:
    """Framework for curriculum learning with automatic difficulty adjustment."""
    
    def **init**(self, environments, agent, difficulty*measure='success*rate'):
        self.environments = environments  # List of environments with increasing difficulty
        self.agent = agent
        self.difficulty*measure = difficulty*measure
        
        self.current_level = 0
        self.level*performance = [[] for * in environments]
        self.progression_threshold = 0.8  # Success rate threshold to advance
        self.regression_threshold = 0.3   # Success rate threshold to regress
        
        self.curriculum_history = []
    
    def get*current*environment(self):
        """Get current environment based on curriculum level."""
        return self.environments[self.current_level]
    
    def evaluate*performance(self, episode*rewards, episode_successes=None):
        """Evaluate performance on current level."""
        if self.difficulty*measure == 'success*rate' and episode_successes is not None:
            return np.mean(episode*successes[-10:]) if len(episode*successes) >= 10 else 0
        elif self.difficulty_measure == 'reward':
            return np.mean(episode*rewards[-10:]) if len(episode*rewards) >= 10 else 0
        else:
            return np.mean(episode*rewards[-10:]) if len(episode*rewards) >= 10 else 0
    
    def update_curriculum(self, performance):
        """Update curriculum level based on performance."""
        old*level = self.current*level
        
        if performance >= self.progression*threshold and self.current*level < len(self.environments) - 1:
            self.current_level += 1
            print(f\"ðŸ“ˆ Advanced to level {self.current_level} (performance: {performance:.2f})\"
        
        elif performance < self.regression*threshold and self.current*level > 0:
            self.current*level = max(0, self.current*level - 1)
            print(f\"ðŸ“‰ Regressed to level {self.current_level} (performance: {performance:.2f})\"
        
        if old*level != self.current*level:
            self.curriculum_history.append({
                'episode': len(self.level*performance[old*level]),
                'old*level': old*level,
                'new*level': self.current*level,
                'performance': performance
            })
        
        return self.current*level != old*level
    
    def train*with*curriculum(self, num_episodes=1000):
        """Train agent using curriculum learning."""
        episode_rewards = []
        episode_successes = []
        
        for episode in range(num_episodes):
            env = self.get*current*environment()
            
            state = env.reset()
            episode_reward = 0
            episode_success = False
            
            for step in range(100):  # Max episode length
                action = self.agent.act(state, epsilon=max(0.1, 1.0 - episode/500))
                next_state, reward, done, info = env.step(action)
                
                self.agent.replay*buffer.push(state, action, reward, next*state, done)
                
                episode_reward += reward
                if done and reward > 5:  # Define success condition
                    episode_success = True
                
                if done:
                    break
                
                state = next_state
            
            if len(self.agent.replay_buffer) > 32:
                self.agent.update(32)
            
            episode*rewards.append(episode*reward)
            episode*successes.append(episode*success)
            self.level*performance[self.current*level].append(episode_reward)
            
            if episode % 20 == 0:
                performance = self.evaluate*performance(episode*rewards, episode_successes)
                self.update_curriculum(performance)
            
            if episode % 100 == 0:
                recent*reward = np.mean(episode*rewards[-10:])
                recent*success = np.mean(episode*successes[-10:])
                print(f\"Episode {episode}: Level {self.current_level}, \"
                      f\"Reward: {recent*reward:.2f}, Success: {recent*success:.2f}\"
        
        return episode*rewards, episode*successes

def compare*sample*efficiency():
    \"\"\"Compare sample efficiency of different techniques.\"\"\"
    print(\"âš¡ Comparing Sample Efficiency Techniques\")
    
    env = SimpleGridWorld(size=6)
    
    baseline*agent = DQNAgent(state*dim=2, action_dim=4)
    efficient*agent = SampleEfficientAgent(state*dim=2, action_dim=4)
    
    agents = {
        'Baseline DQN': baseline_agent,
        'Sample Efficient': efficient_agent
    }
    
    results = {name: {'rewards': [], 'episodes': []} for name in agents.keys()}
    
    num_episodes = 300
    
    for episode in range(num_episodes):
        for agent_name, agent in agents.items():
            state = env.reset()
            episode_reward = 0
            
            for step in range(50):
                action = agent.act(state, epsilon=max(0.1, 1.0 - episode/200))
                next*state, reward, done, * = env.step(action)
                episode_reward += reward
                
                if agent_name == 'Baseline DQN':
                    agent.replay*buffer.push(state, action, reward, next*state, done)
                    if len(agent.replay_buffer) > 32:
                        batch = agent.replay_buffer.sample(32)
                        agent.update(batch)
                else:
                    agent.replay*buffer.push(state, action, reward, next*state, done)
                    if len(agent.replay_buffer) > 32:
                        agent.update(32)
                
                if done:
                    break
                
                state = next_state
            
            results[agent*name]['rewards'].append(episode*reward)
            results[agent_name]['episodes'].append(episode)
    
    return results

def demonstrate*transfer*learning():
    \"\"\"Demonstrate transfer learning between related tasks.\"\"\"
    print(\"ðŸ”„ Demonstrating Transfer Learning\")
    
    agent = TransferLearningAgent(state*dim=2, action*dim=4)
    
    def create*task*env(goal*position, reward*scale=1.0):
        env = SimpleGridWorld(size=4)
        env.goal = goal_position
        env.reward*scale = reward*scale
        return env
    
    tasks = {
        'task*1': create*task_env([3, 3], 1.0),     # Original goal
        'task*2': create*task_env([3, 0], 1.0),     # Different goal
        'task*3': create*task_env([0, 3], 1.0),     # Another goal
    }
    
    for task_name in tasks.keys():
        agent.add*task(task*name)
    
    print(\"Training on Task 1...\")
    task*1*env = tasks['task_1']
    
    for episode in range(200):
        state = task*1*env.reset()
        episode*states, episode*actions, episode_rewards = [], [], []
        
        for step in range(30):
            action = agent.get*action(state, 'task*1')
            next*state, reward, done, * = task*1*env.step(action)
            
            episode_states.append(state)
            episode_actions.append(action)
            episode_rewards.append(reward)
            
            if done:
                break
            
            state = next_state
        
        if episode_rewards:
            agent.update(episode*states, episode*actions, episode*rewards, 'task*1')
    
    transfer_results = {}
    
    for new*task in ['task*2', 'task_3']:
        print(f\"Transferring to {new_task}...\")
        
        agent.fine*tune*for*task('task*1', new_task)
        
        task*env = tasks[new*task]
        task_rewards = []
        
        for episode in range(50):  # Limited training
            state = task_env.reset()
            episode_reward = 0
            episode*states, episode*actions, episode_rewards = [], [], []
            
            for step in range(30):
                action = agent.get*action(state, new*task)
                next*state, reward, done, * = task_env.step(action)
                
                episode_states.append(state)
                episode_actions.append(action)
                episode_rewards.append(reward)
                episode_reward += reward
                
                if done:
                    break
                
                state = next_state
            
            if episode_rewards:
                agent.update(episode*states, episode*actions, episode_rewards, 
                           new*task, update*features=False)
            
            task*rewards.append(episode*reward)
        
        transfer*results[new*task] = task_rewards
        print(f\"  Final performance on {new*task}: {np.mean(task*rewards[-10:]):.2f}\")
    
    return transfer_results

print(\"ðŸš€ Starting Sample Efficiency and Transfer Learning Demonstrations!\")

efficiency*results = compare*sample_efficiency()

transfer*results = demonstrate*transfer_learning()

fig, axes = plt.subplots(1, 2, figsize=(15, 6))

for agent*name, data in efficiency*results.items():
    window_size = 20
    if len(data['rewards']) >= window_size:
        smoothed*rewards = pd.Series(data['rewards']).rolling(window*size).mean()
        axes[0].plot(data['episodes'], smoothed*rewards, label=agent*name, linewidth=2)

axes[0].set_title('Sample Efficiency Comparison')
axes[0].set_xlabel('Episode')
axes[0].set_ylabel('Episode Reward (Smoothed)')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

for task*name, rewards in transfer*results.items():
    axes[1].plot(rewards, label=f'Transfer to {task_name}', linewidth=2)

axes[1].set_title('Transfer Learning Performance')
axes[1].set_xlabel('Episode (Limited Training)')
axes[1].set_ylabel('Episode Reward')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(\"\\nðŸ“Š Sample Efficiency Results:\")
for agent*name, data in efficiency*results.items():
    final_perf = np.mean(data['rewards'][-20:])
    print(f\"  {agent*name}: {final*perf:.2f} final performance\")

print(\"\\nðŸ”„ Transfer Learning Results:\")
for task*name, rewards in transfer*results.items():
    final_perf = np.mean(rewards[-10:])
    print(f\"  {task*name}: {final*perf:.2f} final performance with limited training\")

print(\"\\nðŸ’¡ Key Insights:\")
print(\"  â€¢ Prioritized replay and auxiliary tasks improve sample efficiency\")
print(\"  â€¢ Data augmentation provides regularization benefits\")
print(\"  â€¢ Transfer learning enables rapid adaptation to new tasks\")
print(\"  â€¢ Shared representations capture generalizable knowledge\")

```

# Section 4: Hierarchical Reinforcement Learning

## 4.1 Theory: Hierarchical Decision Making

Hierarchical Reinforcement Learning (HRL) addresses the challenge of learning complex behaviors by decomposing tasks into hierarchical structures. This approach enables agents to:

1. **Learn at Multiple Time Scales**: High-level policies select goals or skills, while low-level policies execute primitive actions
2. **Achieve Better Generalization**: Skills learned in one context can be reused in others
3. **Improve Sample Efficiency**: By leveraging temporal abstractions and skill composition

### Key Components

#### Options Framework
An **option** $\omega$ is defined by a tuple $(I*\omega, \pi*\omega, \beta_\omega)$:
- **Initiation Set** $I_\omega \subseteq \mathcal{S}$: States where the option can be initiated
- **Policy** $\pi_\omega: \mathcal{S} \times \mathcal{A} \rightarrow [0,1]$: Action selection within the option
- **Termination Condition** $\beta_\omega: \mathcal{S} \rightarrow [0,1]$: Probability of termination

#### Hierarchical Value Functions
The value function for options follows the Bellman equation:
$$Q^\pi(s,\omega) = \mathbb{E}*\pi\left[\sum*{t=0}^{\tau-1} \gamma^t r*{t+1} + \gamma^\tau Q^\pi(s*\tau, \omega') \mid s*0=s, \omega*0=\omega\right]$$

where $\tau$ is the termination time and $\omega'$ is the next option selected.

#### Feudal Networks
Feudal Networks implement a manager-worker hierarchy:
- **Manager Network**: Sets goals $g*t$ for workers: $g*t = f*{manager}(s*t, h_{t-1}^{manager})$
- **Worker Network**: Executes actions conditioned on goals: $a*t = \pi*{worker}(s*t, g*t)$
- **Intrinsic Motivation**: Workers receive intrinsic rewards based on goal achievement

### Mathematical Framework

#### Intrinsic Reward Signal
The intrinsic reward for achieving subgoals:
$$r*t^{intrinsic} = \cos(\text{achieved\*goal}*t - \text{desired\*goal}*t) \cdot ||s*{t+1} - s_t||$$

#### Hierarchical Policy Gradient
The gradient for the manager policy:
$$\nabla*{\theta*m} J*m = \mathbb{E}\left[\nabla*{\theta*m} \log \pi*m(g*t|s*t) \cdot A*m(s*t, g_t)\right]$$

And for the worker policy:
$$\nabla*{\theta*w} J*w = \mathbb{E}\left[\nabla*{\theta*w} \log \pi*w(a*t|s*t, g*t) \cdot A*w(s*t, a*t, g_t)\right]$$

## 4.2 Implementation: Hierarchical Rl Architectures

We'll implement several HRL approaches:
1. **Options-Critic Architecture**: Learn options and policies jointly
2. **Feudal Networks**: Manager-worker hierarchies
3. **Hindsight Experience Replay with Goals**: Sample efficiency for goal-conditioned tasks


```python

class OptionsCriticNetwork(nn.Module):
    """Options-Critic architecture for learning hierarchical policies."""
    
    def **init**(self, state*dim, action*dim, num*options=4, hidden*dim=128):
        super().**init**()
        self.state*dim = state*dim
        self.action*dim = action*dim
        self.num*options = num*options
        
        self.feature_net = nn.Sequential(
            nn.Linear(state*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, hidden*dim),
            nn.ReLU()
        )
        
        self.option_net = nn.Sequential(
            nn.Linear(hidden*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, num*options),
            nn.Softmax(dim=-1)
        )
        
        self.intra*option*nets = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden*dim, hidden*dim),
                nn.ReLU(),
                nn.Linear(hidden*dim, action*dim),
                nn.Softmax(dim=-1)
            ) for * in range(num*options)
        ])
        
        self.termination_nets = nn.ModuleList([
            nn.Sequential(
                nn.Linear(hidden*dim, hidden*dim // 2),
                nn.ReLU(),
                nn.Linear(hidden_dim // 2, 1),
                nn.Sigmoid()
            ) for * in range(num*options)
        ])
        
        self.value_net = nn.Sequential(
            nn.Linear(hidden*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, num*options)
        )
    
    def forward(self, state):
        """Forward pass through the Options-Critic architecture."""
        features = self.feature_net(state)
        
        option*probs = self.option*net(features)
        
        action*probs = torch.stack([net(features) for net in self.intra*option_nets], dim=1)
        
        termination*probs = torch.stack([net(features) for net in self.termination*nets], dim=1).squeeze(-1)
        
        option*values = self.value*net(features)
        
        return option*probs, action*probs, termination*probs, option*values

class OptionsCriticAgent:
    """Agent using Options-Critic for hierarchical learning."""
    
    def **init**(self, state*dim, action*dim, num_options=4, lr=1e-3):
        self.state*dim = state*dim
        self.action*dim = action*dim
        self.num*options = num*options
        
        self.network = OptionsCriticNetwork(state*dim, action*dim, num_options)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
        
        self.current_option = None
        self.option_length = 0
        self.max*option*length = 10
        
        self.gamma = 0.99
        self.beta_reg = 0.01  # Regularization for termination
        
        self.option*usage = np.zeros(num*options)
        self.option_lengths = []
        self.losses = []
    
    def select_option(self, state):
        """Select option using epsilon-greedy on option probabilities."""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            option*probs, *, *, * = self.network(state_tensor)
            return Categorical(option_probs).sample().item()
    
    def select_action(self, state, option):
        """Select action using the intra-option policy."""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            *, action*probs, *, * = self.network(state_tensor)
            return Categorical(action_probs[0, option]).sample().item()
    
    def should_terminate(self, state, option):
        """Check if current option should terminate."""
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            *, *, termination*probs, * = self.network(state_tensor)
            return np.random.random() < termination_probs[0, option].item()
    
    def act(self, state):
        """Full action selection with option management."""
        if self.current*option is None or self.should*terminate(state, self.current_option) or \
           self.option*length >= self.max*option_length:
            self.current*option = self.select*option(state)
            self.option*usage[self.current*option] += 1
            if self.option_length > 0:
                self.option*lengths.append(self.option*length)
            self.option_length = 0
        
        action = self.select*action(state, self.current*option)
        self.option_length += 1
        
        return action, self.current_option
    
    def update(self, trajectory):
        """Update using Options-Critic learning algorithm."""
        if len(trajectory) < 2:
            return None
        
        states, actions, rewards, options = zip(*trajectory)
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        options = torch.LongTensor(options)
        
        option*probs, action*probs, termination*probs, option*values = self.network(states)
        
        returns = torch.zeros_like(rewards)
        G = 0
        for t in reversed(range(len(rewards))):
            G = rewards[t] + self.gamma * G
            returns[t] = G
        
        current*option*values = option_values.gather(1, options.unsqueeze(1)).squeeze()
        value*loss = F.mse*loss(current*option*values, returns.detach())
        
        advantages = returns - current*option*values.detach()
        
        selected*action*probs = []
        for i in range(len(actions)):
            selected*action*probs.append(action_probs[i, options[i], actions[i]])
        selected*action*probs = torch.stack(selected*action*probs)
        
        policy*loss = -(torch.log(selected*action_probs) * advantages).mean()
        
        termination*reg = self.beta*reg * termination_probs.mean()
        
        total*loss = value*loss + policy*loss + termination*reg
        
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip*grad*norm*(self.network.parameters(), max*norm=1.0)
        self.optimizer.step()
        
        self.losses.append(total_loss.item())
        
        return {
            'total*loss': total*loss.item(),
            'value*loss': value*loss.item(),
            'policy*loss': policy*loss.item(),
            'termination*reg': termination*reg.item()
        }

class FeudalNetwork(nn.Module):
    """Feudal Network with Manager-Worker hierarchy."""
    
    def **init**(self, state*dim, action*dim, goal*dim=8, hidden*dim=128, temporal_horizon=10):
        super().**init**()
        self.state*dim = state*dim
        self.action*dim = action*dim
        self.goal*dim = goal*dim
        self.temporal*horizon = temporal*horizon
        
        self.manager_net = nn.Sequential(
            nn.Linear(state*dim, hidden*dim),
            nn.ReLU(),
            nn.LSTM(hidden*dim, hidden*dim),
        )
        self.manager*goal*net = nn.Linear(hidden*dim, goal*dim)
        
        self.worker_net = nn.Sequential(
            nn.Linear(state*dim + goal*dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden*dim, action*dim),
            nn.Softmax(dim=-1)
        )
        
        self.manager*value*net = nn.Sequential(
            nn.Linear(hidden*dim, hidden*dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self.worker*value*net = nn.Sequential(
            nn.Linear(state*dim + goal*dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
        
        self.manager_hidden = None
    
    def forward(self, state, goal=None):
        """Forward pass through Feudal Network."""
        batch_size = state.size(0) if len(state.shape) > 1 else 1
        if len(state.shape) == 1:
            state = state.unsqueeze(0)
        
        manager*features = self.manager*net[0](state)  # First layer
        manager*features = self.manager*net[1](manager_features)  # ReLU
        
        if self.manager*hidden is None or self.manager*hidden[0].size(1) != batch_size:
            self.manager_hidden = (
                torch.zeros(1, batch*size, self.manager*net[2].hidden_size),
                torch.zeros(1, batch*size, self.manager*net[2].hidden_size)
            )
        
        lstm*out, self.manager*hidden = self.manager_net[2](
            manager*features.unsqueeze(0), self.manager*hidden
        )
        manager*features = lstm*out.squeeze(0)
        
        goals = self.manager*goal*net(manager_features)
        goals = F.normalize(goals, p=2, dim=-1)  # Unit normalize goals
        
        manager*value = self.manager*value*net(manager*features)
        
        if goal is None:
            goal = goals
        
        worker_input = torch.cat([state, goal], dim=-1)
        action*probs = self.worker*net(worker_input)
        worker*value = self.worker*value*net(worker*input)
        
        return goals, action*probs, manager*value, worker_value
    
    def reset*manager*state(self):
        \"\"\"Reset manager LSTM state.\"\"\"
        self.manager_hidden = None

class FeudalAgent:
    \"\"\"Feudal Networks agent with hierarchical learning.\"\"\"
    
    def **init**(self, state*dim, action*dim, goal*dim=8, lr=1e-3, temporal*horizon=10):
        self.state*dim = state*dim
        self.action*dim = action*dim
        self.goal*dim = goal*dim
        self.temporal*horizon = temporal*horizon
        
        self.network = FeudalNetwork(state*dim, action*dim, goal*dim, temporal*horizon=temporal_horizon)
        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)
        
        self.current_goal = None
        self.goal*step*count = 0
        
        self.gamma = 0.99
        self.intrinsic*reward*scale = 0.5
        
        self.manager_losses = []
        self.worker_losses = []
        self.goal_changes = []
    
    def compute*intrinsic*reward(self, state, next_state, goal):
        \"\"\"Compute intrinsic reward based on goal achievement.\"\"\"
        state*diff = next*state - state
        state*diff*norm = np.linalg.norm(state_diff)
        
        if state*diff*norm > 1e-6:
            cosine*sim = np.dot(state*diff, goal) / (state*diff*norm * np.linalg.norm(goal))
            return self.intrinsic*reward*scale * cosine*sim * state*diff_norm
        return 0.0
    
    def act(self, state):
        \"\"\"Select action using feudal hierarchy.\"\"\"
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            
            if self.current*goal is None or self.goal*step*count >= self.temporal*horizon:
                goals, *, *, * = self.network(state*tensor)
                self.current_goal = goals[0].numpy()
                self.goal*step*count = 0
                self.goal*changes.append(len(self.goal*changes))
            
            goal*tensor = torch.FloatTensor(self.current*goal).unsqueeze(0)
            *, action*probs, *, * = self.network(state*tensor, goal*tensor)
            action = Categorical(action_probs).sample().item()
            
            self.goal*step*count += 1
        
        return action
    
    def update(self, trajectories):
        \"\"\"Update feudal networks using hierarchical returns.\"\"\"
        if not trajectories:
            return None
        
        total*manager*loss = 0
        total*worker*loss = 0
        num_updates = 0
        
        for traj in trajectories:
            if len(traj) < 2:
                continue
            
            states, actions, rewards, next_states = zip(*traj)
            states = torch.FloatTensor(states)
            actions = torch.LongTensor(actions)
            rewards = torch.FloatTensor(rewards)
            next*states = torch.FloatTensor(next*states)
            
            self.network.reset*manager*state()
            
            goals, action*probs, manager*values, worker_values = self.network(states)
            
            intrinsic_rewards = []
            for i in range(len(states)-1):
                intrinsic*reward = self.compute*intrinsic_reward(
                    states[i].numpy(), next_states[i].numpy(), goals[i].numpy()
                )
                intrinsic*rewards.append(intrinsic*reward)
            intrinsic*rewards = torch.FloatTensor(intrinsic*rewards)
            
            manager*returns = torch.zeros*like(rewards)
            G = 0
            for t in reversed(range(len(rewards))):
                G = rewards[t] + self.gamma * G
                manager_returns[t] = G
            
            manager*advantages = manager*returns - manager_values.squeeze()
            manager*loss = (manager*advantages ** 2).mean()
            
            total*rewards = rewards[:-1] + intrinsic*rewards
            worker*returns = torch.zeros*like(total_rewards)
            G = 0
            for t in reversed(range(len(total_rewards))):
                G = total_rewards[t] + self.gamma * G
                worker_returns[t] = G
            
            worker*advantages = worker*returns - worker_values[:-1].squeeze()
            
            selected*action*probs = action_probs[:-1].gather(1, actions[:-1].unsqueeze(1)).squeeze()
            worker*policy*loss = -(torch.log(selected*action*probs) * worker_advantages.detach()).mean()
            worker*value*loss = (worker_advantages ** 2).mean()
            worker*loss = worker*policy*loss + 0.5 * worker*value_loss
            
            total*manager*loss += manager_loss
            total*worker*loss += worker_loss
            num_updates += 1
        
        if num_updates == 0:
            return None
        
        avg*manager*loss = total*manager*loss / num_updates
        avg*worker*loss = total*worker*loss / num_updates
        total*loss = avg*manager*loss + avg*worker_loss
        
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip*grad*norm*(self.network.parameters(), max*norm=1.0)
        self.optimizer.step()
        
        self.manager*losses.append(avg*manager_loss.item())
        self.worker*losses.append(avg*worker_loss.item())
        
        return {
            'manager*loss': avg*manager_loss.item(),
            'worker*loss': avg*worker_loss.item(),
            'total*loss': total*loss.item()
        }

class HindsightExperienceReplay:
    \"\"\"Hindsight Experience Replay for goal-conditioned RL.\"\"\"
    
    def **init**(self, capacity, goal_dim, strategy='future', k=4):
        self.capacity = capacity
        self.goal*dim = goal*dim
        self.strategy = strategy  # 'future', 'final', 'episode', 'random'
        self.k = k  # Number of additional goals to sample
        
        self.buffer = []
        self.position = 0
    
    def push*episode(self, episode*trajectory):
        \"\"\"Store an entire episode and generate hindsight goals.\"\"\"
        if not episode_trajectory:
            return
        
        states, actions, rewards, next*states, goals, achieved*goals = zip(*episode_trajectory)
        
        for i, transition in enumerate(episode_trajectory):
            self.*store*transition(transition)
        
        if self.strategy == 'future':
            self.*generate*future*goals(episode*trajectory)
        elif self.strategy == 'final':
            self.*generate*final*goals(episode*trajectory)
        elif self.strategy == 'episode':
            self.*generate*episode*goals(episode*trajectory)
        elif self.strategy == 'random':
            self.*generate*random*goals(episode*trajectory)
    
    def *store*transition(self, transition):
        \"\"\"Store a single transition in the buffer.\"\"\"
        if len(self.buffer) < self.capacity:
            self.buffer.append(None)
        
        self.buffer[self.position] = transition
        self.position = (self.position + 1) % self.capacity
    
    def *generate*future*goals(self, episode*trajectory):
        \"\"\"Generate goals from future achieved goals in the episode.\"\"\"
        for i in range(len(episode_trajectory)):
            future_indices = np.random.choice(
                range(i, len(episode_trajectory)), 
                size=min(self.k, len(episode_trajectory) - i), 
                replace=False
            )
            
            for future*idx in future*indices:
                state, action, *, next*state, *, * = episode_trajectory[i]
                *, *, *, *, *, achieved*goal = episode*trajectory[future*idx]
                
                reward = self.*compute*goal*reward(next*state, achieved_goal)
                
                hindsight*transition = (state, action, reward, next*state, achieved*goal, achieved*goal)
                self.*store*transition(hindsight_transition)
    
    def *generate*final*goals(self, episode*trajectory):
        \"\"\"Use the final achieved goal for all transitions.\"\"\"
        if not episode_trajectory:
            return
        
        final*achieved*goal = episode*trajectory[-1][5]  # achieved*goal from last transition
        
        for transition in episode_trajectory:
            state, action, *, next*state, *, * = transition
            reward = self.*compute*goal*reward(next*state, final*achieved*goal)
            
            hindsight*transition = (state, action, reward, next*state, final*achieved*goal, final*achieved*goal)
            self.*store*transition(hindsight_transition)
    
    def *compute*goal*reward(self, achieved*goal, desired_goal, threshold=0.1):
        \"\"\"Compute reward based on goal achievement.\"\"\"
        distance = np.linalg.norm(achieved*goal - desired*goal)
        return 1.0 if distance < threshold else -1.0
    
    def sample(self, batch_size):
        \"\"\"Sample a batch of transitions.\"\"\"
        if len(self.buffer) < batch_size:
            return None
        
        indices = np.random.choice(len(self.buffer), batch_size, replace=False)
        batch = [self.buffer[i] for i in indices]
        
        states, actions, rewards, next*states, goals, achieved*goals = zip(*batch)
        
        return (
            torch.FloatTensor(states),
            torch.LongTensor(actions), 
            torch.FloatTensor(rewards),
            torch.FloatTensor(next_states),
            torch.FloatTensor(goals),
            torch.FloatTensor(achieved_goals)
        )
    
    def **len**(self):
        return len(self.buffer)

def demonstrate*hierarchical*rl():
    \"\"\"Demonstrate hierarchical RL approaches.\"\"\"
    print(\"ðŸ—ï¸ Demonstrating Hierarchical Reinforcement Learning\")
    
    env = SimpleGridWorld(size=8)
    
    agents = {
        'Options-Critic': OptionsCriticAgent(state*dim=2, action*dim=4, num_options=4),
        'Feudal Network': FeudalAgent(state*dim=2, action*dim=4, goal_dim=4)
    }
    
    results = {name: {'rewards': [], 'episode_lengths': []} for name in agents.keys()}
    
    num_episodes = 200
    
    for episode in range(num_episodes):
        for agent_name, agent in agents.items():
            state = env.reset()
            episode_reward = 0
            episode_length = 0
            trajectory = []
            
            for step in range(100):  # Max episode length
                if agent_name == 'Options-Critic':
                    action, option = agent.act(state)
                    trajectory.append((state, action, 0, option))  # Reward added later
                else:
                    action = agent.act(state)
                
                next*state, reward, done, * = env.step(action)
                episode_reward += reward
                episode_length += 1
                
                if agent_name == 'Options-Critic':
                    trajectory[-1] = (state, action, reward, option)
                else:
                    trajectory.append((state, action, reward, next_state))
                
                if done:
                    break
                
                state = next_state
            
            if agent_name == 'Options-Critic' and len(trajectory) > 1:
                agent.update(trajectory)
            elif agent_name == 'Feudal Network' and len(trajectory) > 1:
                agent.update([trajectory])
            
            results[agent*name]['rewards'].append(episode*reward)
            results[agent*name]['episode*lengths'].append(episode_length)
    
    return results, agents

print(\"ðŸš€ Starting Hierarchical RL Demonstration!\")
hierarchical*results, hierarchical*agents = demonstrate*hierarchical*rl()

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

for agent*name, data in hierarchical*results.items():
    window_size = 20
    if len(data['rewards']) >= window_size:
        smoothed*rewards = pd.Series(data['rewards']).rolling(window*size).mean()
        axes[0, 0].plot(smoothed*rewards, label=agent*name, linewidth=2)

axes[0, 0].set_title('Hierarchical RL Learning Curves')
axes[0, 0].set_xlabel('Episode')
axes[0, 0].set_ylabel('Episode Reward (Smoothed)')
axes[0, 0].legend()
axes[0, 0].grid(True, alpha=0.3)

for agent*name, data in hierarchical*results.items():
    window_size = 20
    if len(data['episode*lengths']) >= window*size:
        smoothed*lengths = pd.Series(data['episode*lengths']).rolling(window_size).mean()
        axes[0, 1].plot(smoothed*lengths, label=agent*name, linewidth=2)

axes[0, 1].set_title('Episode Length Over Time')
axes[0, 1].set_xlabel('Episode')
axes[0, 1].set_ylabel('Episode Length (Smoothed)')
axes[0, 1].legend()
axes[0, 1].grid(True, alpha=0.3)

if 'Options-Critic' in hierarchical_agents:
    agent = hierarchical_agents['Options-Critic']
    axes[1, 0].bar(range(agent.num*options), agent.option*usage)
    axes[1, 0].set_title('Option Usage Distribution')
    axes[1, 0].set_xlabel('Option ID')
    axes[1, 0].set_ylabel('Usage Count')
    axes[1, 0].grid(True, alpha=0.3)

if 'Options-Critic' in hierarchical_agents:
    agent = hierarchical_agents['Options-Critic']
    if agent.option_lengths:
        axes[1, 1].hist(agent.option_lengths, bins=20, alpha=0.7, edgecolor='black')
        axes[1, 1].set_title('Option Length Distribution')
        axes[1, 1].set_xlabel('Option Length')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].grid(True, alpha=0.3)

plt.tight_layout()
plt.show()

print(\"\\nðŸ“Š Hierarchical RL Results:\")
for agent*name, data in hierarchical*results.items():
    final_perf = np.mean(data['rewards'][-20:])
    avg*length = np.mean(data['episode*lengths'][-20:])
    print(f\"  {agent_name}:\")
    print(f\"    Final Performance: {final_perf:.2f}\")
    print(f\"    Avg Episode Length: {avg_length:.2f}\")

if 'Options-Critic' in hierarchical_agents:
    agent = hierarchical_agents['Options-Critic']
    print(f\"\\nðŸŽ¯ Options-Critic Analysis:\")
    print(f\"  Options Used: {np.sum(agent.option*usage > 0)} / {agent.num*options}\")
    print(f\"  Most Used Option: {np.argmax(agent.option_usage)}\")
    if agent.option_lengths:
        print(f\"  Avg Option Length: {np.mean(agent.option_lengths):.2f}\")

print(\"\\nðŸ’¡ Key Insights:\")
print(\"  â€¢ Hierarchical methods learn temporal abstractions\")
print(\"  â€¢ Options provide reusable behavioral primitives\")
print(\"  â€¢ Feudal networks enable goal-directed exploration\")
print(\"  â€¢ HRL scales to complex, long-horizon tasks\")

```

# Section 5: Comprehensive Evaluation and Advanced Techniques Integration

## 5.1 Multi-method Performance Analysis

This section provides comprehensive evaluation comparing all implemented advanced Deep RL techniques:

### Performance Metrics
1. **Sample Efficiency**: Episodes to convergence
2. **Final Performance**: Asymptotic reward
3. **Robustness**: Performance variance
4. **Computational Efficiency**: Training time and memory usage
5. **Transfer Capability**: Performance on related tasks

### Evaluation Framework
We evaluate methods across multiple dimensions:
- **Simple Tasks**: Basic navigation and control
- **Complex Tasks**: Multi-step reasoning and planning
- **Transfer Tasks**: Adaptation to new environments
- **Long-Horizon Tasks**: Extended episode planning

## 5.2 Practical Implementation Considerations

### When to Use Each Method:

#### Model-free Methods (dqn, Policy Gradient)
- âœ… **Use when**: Simple tasks, abundant data, unknown dynamics
- âŒ **Avoid when**: Sample efficiency critical, complex planning needed

#### Model-based Methods
- âœ… **Use when**: Sample efficiency critical, dynamics learnable
- âŒ **Avoid when**: High-dimensional observations, stochastic dynamics

#### World Models
- âœ… **Use when**: Rich sensory input, imagination beneficial
- âŒ **Avoid when**: Simple state spaces, real-time constraints

#### Hierarchical Methods
- âœ… **Use when**: Long-horizon tasks, reusable skills needed
- âŒ **Avoid when**: Simple tasks, flat action spaces

#### Sample Efficiency Techniques
- âœ… **Use when**: Limited data, expensive environments
- âŒ **Avoid when**: Abundant cheap data, simple tasks

## 5.3 Advanced Techniques Summary

This comprehensive assignment covered cutting-edge Deep RL methods:

### Core Contributions:
1. **Sample Efficiency**: Prioritized replay, data augmentation, auxiliary tasks
2. **World Models**: VAE-based dynamics, imagination planning
3. **Transfer Learning**: Shared representations, meta-learning
4. **Hierarchical Learning**: Options framework, feudal networks
5. **Integration**: Multi-method evaluation and practical guidelines


```python

class AdvancedRLEvaluator:
    """Comprehensive evaluation framework for advanced RL methods."""
    
    def **init**(self, environments, agents, metrics=['reward', 'sample_efficiency', 'robustness']):
        self.environments = environments
        self.agents = agents
        self.metrics = metrics
        self.results = {}
        
        self.num_trials = 5
        self.num_episodes = 300
        self.evaluation_interval = 50
        
    def evaluate*sample*efficiency(self, agent, env, convergence_threshold=0.8):
        """Measure episodes to convergence."""
        max_rewards = []
        convergence_episodes = []
        
        for trial in range(self.num_trials):
            episode_rewards = []
            
            if hasattr(agent, 'reset'):
                agent.reset()
            
            for episode in range(self.num_episodes):
                state = env.reset()
                episode_reward = 0
                
                for step in range(100):
                    if hasattr(agent, 'act'):
                        if 'Options' in str(type(agent)):
                            action, _ = agent.act(state)
                        else:
                            action = agent.act(state)
                    else:
                        action = env.action_space.sample()
                    
                    next*state, reward, done, * = env.step(action)
                    episode_reward += reward
                    
                    if hasattr(agent, 'replay_buffer'):
                        agent.replay*buffer.push(state, action, reward, next*state, done)
                        if len(agent.replay_buffer) > 32:
                            if hasattr(agent, 'update'):
                                agent.update(32)
                    
                    if done:
                        break
                    
                    state = next_state
                
                episode*rewards.append(episode*reward)
                
                if len(episode_rewards) >= 20:
                    recent*performance = np.mean(episode*rewards[-20:])
                    if recent*performance >= convergence*threshold * np.max(episode_rewards[:max(1, episode-20)]):
                        convergence_episodes.append(episode)
                        break
            
            max*rewards.append(np.max(episode*rewards))
            if not convergence*episodes or len(convergence*episodes) <= trial:
                convergence*episodes.append(self.num*episodes)
        
        return {
            'convergence*episodes': np.mean(convergence*episodes),
            'convergence*std': np.std(convergence*episodes),
            'max*reward': np.mean(max*rewards),
            'max*reward*std': np.std(max_rewards)
        }
    
    def evaluate*transfer*capability(self, agent, source*env, target*envs):
        """Evaluate transfer learning capability."""
        source_performance = []
        state = source_env.reset()
        
        for episode in range(100):  # Limited training
            episode_reward = 0
            for step in range(50):
                action = agent.act(state) if hasattr(agent, 'act') else source*env.action*space.sample()
                next*state, reward, done, * = source_env.step(action)
                episode_reward += reward
                
                if hasattr(agent, 'replay_buffer'):
                    agent.replay*buffer.push(state, action, reward, next*state, done)
                    if len(agent.replay_buffer) > 32 and hasattr(agent, 'update'):
                        agent.update(32)
                
                if done:
                    break
                state = next_state
            
            source*performance.append(episode*reward)
        
        transfer_results = {}
        for target*name, target*env in target_envs.items():
            target_rewards = []
            
            for episode in range(20):  # Quick evaluation
                state = target_env.reset()
                episode_reward = 0
                
                for step in range(50):
                    action = agent.act(state) if hasattr(agent, 'act') else target*env.action*space.sample()
                    next*state, reward, done, * = target_env.step(action)
                    episode_reward += reward
                    
                    if done:
                        break
                    state = next_state
                
                target*rewards.append(episode*reward)
            
            transfer*results[target*name] = {
                'mean*reward': np.mean(target*rewards),
                'std*reward': np.std(target*rewards)
            }
        
        return {
            'source*performance': np.mean(source*performance[-20:]),
            'transfer*results': transfer*results
        }
    
    def comprehensive_evaluation(self):
        """Run comprehensive evaluation across all agents and environments."""
        print(\"ðŸ”¬ Starting Comprehensive Evaluation...\")
        
        for agent_name, agent in self.agents.items():
            print(f\"\\nðŸ“Š Evaluating {agent_name}...\")
            self.results[agent_name] = {}
            
            if 'sample_efficiency' in self.metrics:
                env = self.environments[0] if self.environments else SimpleGridWorld(size=5)
                efficiency*results = self.evaluate*sample_efficiency(agent, env)
                self.results[agent*name]['sample*efficiency'] = efficiency_results
                print(f\"  Sample Efficiency: {efficiency*results['convergence*episodes']:.1f} Â± {efficiency*results['convergence*std']:.1f} episodes\")
            
            if 'transfer' in self.metrics and len(self.environments) > 1:
                source_env = self.environments[0]
                target*envs = {f'env*{i}': env for i, env in enumerate(self.environments[1:])}
                transfer*results = self.evaluate*transfer*capability(agent, source*env, target_envs)
                self.results[agent*name]['transfer'] = transfer*results
                print(f\"  Transfer Capability: Source performance {transfer*results['source*performance']:.2f}\")
        
        return self.results
    
    def generate_report(self):
        \"\"\"Generate comprehensive evaluation report.\"\"\"
        if not self.results:
            self.comprehensive_evaluation()
        
        print(\"\\n\" + \"=\"*60)
        print(\"ðŸ† COMPREHENSIVE EVALUATION REPORT\")
        print(\"=\"*60)
        
        if any('sample_efficiency' in results for results in self.results.values()):
            print(\"\\nðŸ“ˆ Sample Efficiency Ranking:\")
            efficiency_scores = []
            for agent_name, results in self.results.items():
                if 'sample_efficiency' in results:
                    score = results['sample*efficiency']['convergence*episodes']
                    efficiency*scores.append((agent*name, score))
            
            efficiency_scores.sort(key=lambda x: x[1])  # Lower is better
            for rank, (agent*name, score) in enumerate(efficiency*scores, 1):
                print(f\"  {rank}. {agent_name}: {score:.1f} episodes to convergence\")
        
        print(\"\\nðŸŽ¯ Final Performance Comparison:\")
        performance_scores = []
        for agent_name, results in self.results.items():
            if 'sample_efficiency' in results:
                score = results['sample*efficiency']['max*reward']
                performance*scores.append((agent*name, score))
        
        performance_scores.sort(key=lambda x: x[1], reverse=True)  # Higher is better
        for rank, (agent*name, score) in enumerate(performance*scores, 1):
            print(f\"  {rank}. {agent_name}: {score:.2f} max reward\")
        
        print(\"\\nðŸ’¡ Method Recommendations:\")
        
        if efficiency_scores:
            best*efficiency = efficiency*scores[0][0]
            print(f\"  â€¢ Best Sample Efficiency: {best_efficiency}\")
        
        if performance_scores:
            best*performance = performance*scores[0][0]
            print(f\"  â€¢ Best Final Performance: {best_performance}\")
        
        print(\"\\nðŸ”§ Implementation Guidelines:\")
        print(\"  â€¢ Use prioritized replay for sample efficiency\")
        print(\"  â€¢ Apply data augmentation for robustness\")
        print(\"  â€¢ Consider world models for planning tasks\")
        print(\"  â€¢ Employ hierarchical methods for long-horizon problems\")
        print(\"  â€¢ Leverage transfer learning for related domains\")

class IntegratedAdvancedAgent:
    \"\"\"Agent integrating multiple advanced RL techniques.\"\"\"
    
    def **init**(self, state*dim, action*dim, config=None):
        self.state*dim = state*dim
        self.action*dim = action*dim
        
        default_config = {
            'use*prioritized*replay': True,
            'use*auxiliary*tasks': True,
            'use*data*augmentation': True,
            'use*world*model': False,
            'use_hierarchical': False,
            'lr': 1e-3,
            'buffer_size': 10000
        }
        self.config = {**default_config, **(config or {})}
        
        self.*initialize*components()
        
        self.training_stats = {
            'episode_rewards': [],
            'losses': [],
            'sample_efficiency': [],
            'component_usage': {}
        }
    
    def *initialize*components(self):
        \"\"\"Initialize RL components based on configuration.\"\"\"
        if self.config['use*auxiliary*tasks']:
            self.network = DataAugmentationDQN(self.state*dim, self.action*dim)
        else:
            self.network = DQNAgent(self.state*dim, self.action*dim).network
        
        self.target_network = copy.deepcopy(self.network)
        self.optimizer = optim.Adam(self.network.parameters(), lr=self.config['lr'])
        
        if self.config['use*prioritized*replay']:
            self.replay*buffer = PrioritizedReplayBuffer(self.config['buffer*size'])
        else:
            self.replay*buffer = ReplayBuffer(self.config['buffer*size'])
        
        if self.config['use*world*model']:
            self.world*model = VariationalWorldModel(self.state*dim, self.action_dim)
        
        if self.config['use_hierarchical']:
            self.hierarchical*agent = OptionsCriticAgent(self.state*dim, self.action_dim)
        
        self.gamma = 0.99
        self.update_count = 0
        self.target*update*freq = 100
    
    def act(self, state, epsilon=0.1):
        \"\"\"Select action using integrated approach.\"\"\"
        if self.config['use_hierarchical']:
            action, option = self.hierarchical_agent.act(state)
            self.training*stats['component*usage']['hierarchical'] = \
                self.training*stats['component*usage'].get('hierarchical', 0) + 1
            return action
        
        if np.random.random() < epsilon:
            return np.random.randint(self.action_dim)
        
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0)
            
            if self.config['use*data*augmentation'] and np.random.random() < 0.1:
                state*tensor = self.network.apply*augmentation(state_tensor, 'noise')
            
            q*values = self.network(state*tensor)
            if isinstance(q_values, tuple):
                q*values = q*values[0]  # Extract Q-values from auxiliary network
            
            return q_values.argmax().item()
    
    def update(self, batch_size=32):
        \"\"\"Update agent using integrated advanced techniques.\"\"\"
        if isinstance(self.replay_buffer, PrioritizedReplayBuffer):
            sample*result = self.replay*buffer.sample(batch_size)
            if sample_result is None:
                return None
            experiences, indices, weights = sample_result
        else:
            batch = self.replay*buffer.sample(batch*size)
            if batch is None:
                return None
            experiences = batch
            weights = torch.ones(batch_size)
            indices = None
        
        states, actions, rewards, next_states, dones = experiences
        states = torch.FloatTensor(states)
        actions = torch.LongTensor(actions)
        rewards = torch.FloatTensor(rewards)
        next*states = torch.FloatTensor(next*states)
        dones = torch.BoolTensor(dones)
        weights = torch.FloatTensor(weights) if not isinstance(weights, torch.Tensor) else weights
        
        if self.config['use*data*augmentation']:
            aug_type = np.random.choice(['noise', 'dropout', 'scaling'])
            states = self.network.apply*augmentation(states, aug*type)
            next*states = self.network.apply*augmentation(next*states, aug*type)
            self.training*stats['component*usage']['augmentation'] = \
                self.training*stats['component*usage'].get('augmentation', 0) + 1
        
        if self.config['use*auxiliary*tasks']:
            current*q*values, reward*pred, next*state_pred = self.network(states, actions)
            current*q*values = current*q*values.gather(1, actions.unsqueeze(1)).squeeze()
        else:
            current*q*values = self.network(states).gather(1, actions.unsqueeze(1)).squeeze()
        
        with torch.no_grad():
            if self.config['use*auxiliary*tasks'] and hasattr(self.target_network, 'forward'):
                next*q*values = self.target*network(next*states)
                if isinstance(next*q*values, tuple):
                    next*q*values = next*q*values[0]
            else:
                next*q*values = self.target*network(next*states)
            
            max*next*q*values = next*q_values.max(1)[0]
            target*q*values = rewards + (self.gamma * max*next*q_values * (~dones))
        
        td*errors = (current*q*values - target*q_values).detach()
        q*loss = (weights * F.mse*loss(current*q*values, target*q*values, reduction='none')).mean()
        
        total*loss = q*loss
        
        if self.config['use*auxiliary*tasks']:
            aux*reward*loss = F.mse*loss(reward*pred.squeeze(), rewards)
            aux*dynamics*loss = F.mse*loss(next*state*pred, next*states)
            total*loss += 0.1 * aux*reward*loss + 0.1 * aux*dynamics_loss
            self.training*stats['component*usage']['auxiliary'] = \
                self.training*stats['component*usage'].get('auxiliary', 0) + 1
        
        if self.config['use*world*model']:
            world*model*loss = self.world*model.compute*loss(states, actions, next_states)
            total*loss += 0.1 * world*model_loss
            self.training*stats['component*usage']['world_model'] = \
                self.training*stats['component*usage'].get('world_model', 0) + 1
        
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip*grad*norm*(self.network.parameters(), max*norm=1.0)
        self.optimizer.step()
        
        if indices is not None:
            self.replay*buffer.update*priorities(indices, td_errors.numpy())
        
        self.update_count += 1
        if self.update*count % self.target*update_freq == 0:
            self.target*network.load*state*dict(self.network.state*dict())
        
        self.training*stats['losses'].append(total*loss.item())
        
        return {
            'total*loss': total*loss.item(),
            'q*loss': q*loss.item()
        }

def comprehensive*advanced*rl_demo():
    \"\"\"Comprehensive demonstration of all advanced RL techniques.\"\"\"
    print(\"ðŸŽ“ COMPREHENSIVE ADVANCED DEEP RL DEMONSTRATION\")
    print(\"=\" * 55)
    
    environments = [
        SimpleGridWorld(size=5),
        SimpleGridWorld(size=6),
        SimpleGridWorld(size=7)
    ]
    
    agents = {
        'Baseline DQN': DQNAgent(state*dim=2, action*dim=4),
        'Sample Efficient': SampleEfficientAgent(state*dim=2, action*dim=4),
        'Options-Critic': OptionsCriticAgent(state*dim=2, action*dim=4),
        'Feudal Network': FeudalAgent(state*dim=2, action*dim=4),
        'Integrated Advanced': IntegratedAdvancedAgent(
            state_dim=2, 
            action_dim=4, 
            config={
                'use*prioritized*replay': True,
                'use*auxiliary*tasks': True,
                'use*data*augmentation': True
            }
        )
    }
    
    evaluator = AdvancedRLEvaluator(
        environments=environments,
        agents=agents,
        metrics=['sample_efficiency', 'reward', 'transfer']
    )
    
    evaluator.generate_report()
    
    print(\"\\nðŸŽ¯ ADVANCED DEEP RL ASSIGNMENT COMPLETED!\")
    print(\"\\nðŸ“š Concepts Covered:\")
    print(\"  âœ“ Model-Free vs Model-Based RL Comparison\")
    print(\"  âœ“ World Models with VAE Architecture\") 
    print(\"  âœ“ Imagination-Based Planning\")
    print(\"  âœ“ Sample Efficiency Techniques\")
    print(\"  âœ“ Prioritized Experience Replay\")
    print(\"  âœ“ Data Augmentation & Auxiliary Tasks\")
    print(\"  âœ“ Transfer Learning & Meta-Learning\")
    print(\"  âœ“ Hierarchical Reinforcement Learning\")
    print(\"  âœ“ Options-Critic Architecture\")
    print(\"  âœ“ Feudal Networks\")
    print(\"  âœ“ Comprehensive Evaluation Framework\")
    
    print(\"\\nðŸ”¬ Key Takeaways:\")
    print(\"  â€¢ Advanced RL methods address sample efficiency and scalability\")
    print(\"  â€¢ World models enable planning and imagination\")
    print(\"  â€¢ Hierarchical methods tackle long-horizon tasks\")
    print(\"  â€¢ Transfer learning accelerates adaptation\")
    print(\"  â€¢ Integration of techniques often yields best results\")
    
    print(\"\\nðŸš€ Ready for Real-World Advanced RL Applications!\")
    
    return evaluator.results

print(\"Starting final comprehensive demonstration...\"
final*results = comprehensive*advanced*rl*demo()

print(\"\\n\" + \"=\" * 60)
print(\"ðŸ“– ASSIGNMENT 13: ADVANCED DEEP RL - COMPLETE! âœ…\"
print(\"=\" * 60)

```

# Ca13: Advanced Deep Reinforcement Learning - Model-free Vs Model-based Methods and Real-world Applications

## Deep Reinforcement Learning - Session 13

**Advanced Deep RL Topics: Model-Free vs Model-Based Methods, World Models, and Real-World Deployment**

This notebook explores advanced deep reinforcement learning concepts, including the comparison between model-free and model-based approaches, world models, sample efficiency techniques, transfer learning, and practical considerations for real-world deployment.

### Learning Objectives:
1. Understand the fundamental differences between model-free and model-based RL
2. Implement and compare various world modeling approaches
3. Master sample-efficient learning techniques and transfer learning
4. Explore hierarchical reinforcement learning and temporal abstraction
5. Understand safe reinforcement learning and constrained optimization
6. Implement real-world deployment strategies and robustness techniques
7. Analyze offline reinforcement learning and batch methods
8. Apply meta-learning and few-shot adaptation in RL contexts

### Notebook Structure:
1. **Model-Free vs Model-Based RL** - Theoretical foundations and trade-offs
2. **World Models and Imagination** - Learning environment dynamics
3. **Sample Efficiency Techniques** - Maximizing learning from limited data
4. **Hierarchical Reinforcement Learning** - Temporal abstraction and skills
5. **Safe and Constrained RL** - Safety-aware learning algorithms
6. **Transfer Learning and Meta-Learning** - Knowledge reuse and adaptation
7. **Offline and Batch RL** - Learning from pre-collected data
8. **Real-World Applications** - Deployment strategies and case studies

---


```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Normal, Categorical

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import deque, defaultdict
import random
import gym
import copy
from typing import List, Tuple, Dict, Optional, Union
import pickle
import json
import time
from dataclasses import dataclass
from abc import ABC, abstractmethod

import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import pandas as pd

plt.style.use('seaborn-v0_8')
sns.set_palette("husl")

torch.manual_seed(42)
np.random.seed(42)
random.seed(42)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"ðŸš€ Using device: {device}")
print(f"ðŸ“Š PyTorch version: {torch.**version**}")
print(f"ðŸ¤– Starting Advanced Deep RL Session 13!")

```


```python


```


```python


```
