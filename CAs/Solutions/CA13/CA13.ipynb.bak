{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36039e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured sys.path for CA13 imports\n"
     ]
    }
   ],
   "source": [
    "# Setup and import CA13 package\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\".\"))\n",
    "\n",
    "# Import CA13 package components\n",
    "import CA13\n",
    "from CA13 import (\n",
    "    # Agents\n",
    "    ModelFreeAgent,\n",
    "    DQNAgent,\n",
    "    ModelBasedAgent,\n",
    "    HybridDynaAgent,\n",
    "    SampleEfficientAgent,\n",
    "    DataAugmentationDQN,\n",
    "    OptionsCriticAgent,\n",
    "    FeudalAgent,\n",
    "    # Buffers\n",
    "    ReplayBuffer,\n",
    "    PrioritizedReplayBuffer,\n",
    "    # Utilities\n",
    "    set_seed,\n",
    "    get_device,\n",
    "    # Training functions\n",
    "    train_dqn_agent,\n",
    "    train_model_based_agent,\n",
    "    evaluate_agent,\n",
    "    env_reset,\n",
    "    env_step,\n",
    "    EpisodeMetrics,\n",
    ")\n",
    "\n",
    "print(\"âœ“ Configured sys.path for CA13 imports\")\n",
    "print(f\"âœ“ CA13 Package Version: {CA13.get_version()}\")\n",
    "print(\"âœ“ Imported CA13 agents and utilities\")\n",
    "print(\"âœ“ Imported CA13 buffers (ReplayBuffer, PrioritizedReplayBuffer)\")\n",
    "print(\"âœ“ Imported CA13 training functions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db546e",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Abstract](#abstract)\n",
    "2. [1. Introduction](#1-introduction)\n",
    "   - [1.1 Motivation](#11-motivation)\n",
    "   - [1.2 Learning Objectives](#12-learning-objectives)\n",
    "   - [1.3 Prerequisites](#13-prerequisites)\n",
    "   - [1.4 Course Information](#14-course-information)\n",
    "3. [2. Model-Based vs Model-Free RL Trade-offs](#2-model-based-vs-model-free-rl-trade-offs)\n",
    "   - [2.1 Fundamental Differences](#21-fundamental-differences)\n",
    "   - [2.2 Advantages and Limitations](#22-advantages-and-limitations)\n",
    "   - [2.3 Appropriate Use Cases](#23-appropriate-use-cases)\n",
    "   - [2.4 Performance Comparison](#24-performance-comparison)\n",
    "4. [3. World Model Architectures](#3-world-model-architectures)\n",
    "   - [3.1 Variational Autoencoders for World Models](#31-variational-autoencoders-for-world-models)\n",
    "   - [3.2 Encoder-Decoder Architectures](#32-encoder-decoder-architectures)\n",
    "   - [3.3 Stochastic Dynamics Modeling](#33-stochastic-dynamics-modeling)\n",
    "   - [3.4 Latent Representation Learning](#34-latent-representation-learning)\n",
    "5. [4. Imagination-Based Learning](#4-imagination-based-learning)\n",
    "   - [4.1 Planning in Latent Space](#41-planning-in-latent-space)\n",
    "   - [4.2 Imagined Trajectories](#42-imagined-trajectories)\n",
    "   - [4.3 Sample-Efficient Learning](#43-sample-efficient-learning)\n",
    "   - [4.4 Implementation and Results](#44-implementation-and-results)\n",
    "6. [5. Sample Efficiency Techniques](#5-sample-efficiency-techniques)\n",
    "   - [5.1 Prioritized Experience Replay](#51-prioritized-experience-replay)\n",
    "   - [5.2 Data Augmentation](#52-data-augmentation)\n",
    "   - [5.3 Auxiliary Tasks](#53-auxiliary-tasks)\n",
    "   - [5.4 Learning Efficiency Analysis](#54-learning-efficiency-analysis)\n",
    "7. [6. Transfer Learning Systems](#6-transfer-learning-systems)\n",
    "   - [6.1 Shared Representations](#61-shared-representations)\n",
    "   - [6.2 Fine-tuning Approaches](#62-fine-tuning-approaches)\n",
    "   - [6.3 Meta-Learning Methods](#63-meta-learning-methods)\n",
    "   - [6.4 Knowledge Transfer Analysis](#64-knowledge-transfer-analysis)\n",
    "8. [7. Hierarchical RL Frameworks](#7-hierarchical-rl-frameworks)\n",
    "   - [7.1 Options Framework](#71-options-framework)\n",
    "   - [7.2 Temporal Abstraction](#72-temporal-abstraction)\n",
    "   - [7.3 Skill Composition](#73-skill-composition)\n",
    "   - [7.4 Complex Task Solving](#74-complex-task-solving)\n",
    "9. [8. Results and Discussion](#8-results-and-discussion)\n",
    "   - [8.1 Summary of Findings](#81-summary-of-findings)\n",
    "   - [8.2 Theoretical Contributions](#82-theoretical-contributions)\n",
    "   - [8.3 Practical Implications](#83-practical-implications)\n",
    "   - [8.4 Limitations and Future Work](#84-limitations-and-future-work)\n",
    "   - [8.5 Conclusions](#85-conclusions)\n",
    "10. [References](#references)\n",
    "11. [Appendix A: Implementation Details](#appendix-a-implementation-details)\n",
    "    - [A.1 Modular Architecture](#a1-modular-architecture)\n",
    "    - [A.2 Code Quality Features](#a2-code-quality-features)\n",
    "    - [A.3 Performance Considerations](#a3-performance-considerations)\n",
    "\n",
    "---\n",
    "\n",
    "# Computer Assignment 13: Advanced Model-Based RL and World Models\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This assignment presents a comprehensive study of advanced model-based reinforcement learning and world models, exploring the cutting-edge techniques for learning compressed representations of environments and using them for efficient planning and control. We implement and analyze world model architectures including variational autoencoders, recurrent state space models, and latent space planning methods. The assignment covers modern approaches such as World Models, Dreamer, PlaNet, and MuZero, demonstrating their effectiveness in achieving sample-efficient learning through imagination-based planning. Through systematic experimentation, we show how world models can significantly improve sample efficiency while maintaining competitive performance compared to model-free methods.\n",
    "\n",
    "**Keywords:** Model-based reinforcement learning, world models, variational autoencoders, imagination-based learning, sample efficiency, transfer learning, hierarchical RL, temporal abstraction\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Advanced model-based reinforcement learning with world models represents a significant advancement in the field, enabling agents to learn compressed representations of complex environments and use these representations for efficient planning and decision-making [1]. Unlike traditional model-based approaches that learn explicit environment dynamics, world models learn latent representations that capture the essential aspects of the environment while being computationally tractable for planning and imagination.\n",
    "\n",
    "### 1.1 Motivation\n",
    "\n",
    "World models address several fundamental challenges in reinforcement learning:\n",
    "\n",
    "- **High-Dimensional State Spaces**: Compress complex observations into manageable latent representations\n",
    "- **Sample Efficiency**: Enable planning and imagination without additional environment interaction\n",
    "- **Generalization**: Learn representations that generalize across different environments and tasks\n",
    "- **Computational Efficiency**: Reduce the computational cost of planning through compressed representations\n",
    "- **Long-term Dependencies**: Capture temporal dependencies and long-term consequences of actions\n",
    "\n",
    "### 1.2 Learning Objectives\n",
    "\n",
    "By completing this assignment, students will be able to:\n",
    "\n",
    "1. **Understand Model-Based vs Model-Free RL Trade-offs**: Analyze the fundamental differences between model-free and model-based reinforcement learning approaches, including their respective advantages, limitations, and appropriate use cases.\n",
    "\n",
    "2. **Master World Model Architectures**: Design and implement variational world models using VAEs for learning compact latent representations of environment dynamics, including encoder-decoder architectures and stochastic dynamics modeling.\n",
    "\n",
    "3. **Implement Imagination-Based Learning**: Develop agents that leverage learned world models for planning and decision-making in latent space, enabling sample-efficient learning through imagined trajectories.\n",
    "\n",
    "4. **Apply Sample Efficiency Techniques**: Utilize advanced techniques such as prioritized experience replay, data augmentation, and auxiliary tasks to improve learning efficiency in deep RL.\n",
    "\n",
    "5. **Design Transfer Learning Systems**: Build agents capable of transferring knowledge across related tasks through shared representations, fine-tuning, and meta-learning approaches.\n",
    "\n",
    "6. **Develop Hierarchical RL Frameworks**: Implement hierarchical decision-making systems using options framework, enabling temporal abstraction and skill composition for complex task solving.\n",
    "\n",
    "### 1.3 Prerequisites\n",
    "\n",
    "Before starting this assignment, ensure you have:\n",
    "\n",
    "- **Mathematical Background**: \n",
    "  - Probability theory and stochastic processes\n",
    "  - Variational inference and autoencoders\n",
    "  - Recurrent neural networks and LSTM/GRU\n",
    "  - Information theory and compression\n",
    "\n",
    "- **Technical Skills**:\n",
    "  - Python programming and PyTorch\n",
    "  - Deep learning and neural networks\n",
    "  - Reinforcement learning fundamentals\n",
    "  - Model-based RL concepts\n",
    "\n",
    "### 1.4 Course Information\n",
    "\n",
    "- **Course**: Deep Reinforcement Learning (DRL)\n",
    "- **Instructor**: Dr. [Instructor Name]\n",
    "- **Institution**: Sharif University of Technology\n",
    "- **Semester**: Fall 2024\n",
    "- **Assignment Number**: CA13\n",
    "- Linear algebra and matrix operations\n",
    "- Optimization and gradient-based methods\n",
    "- Information theory (KL divergence, entropy)\n",
    "\n",
    "- **Technical Skills**:\n",
    "- Python programming with PyTorch\n",
    "- Deep learning fundamentals (neural networks, autoencoders)\n",
    "- Basic reinforcement learning concepts (MDPs, value functions, policies)\n",
    "- Experience with Gymnasium environments\n",
    "\n",
    "- **Prior Knowledge**:\n",
    "- Completion of CA1-CA12 assignments\n",
    "- Understanding of model-free RL algorithms (DQN, policy gradients)\n",
    "- Familiarity with neural network architectures\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "This assignment is structured as follows:\n",
    "\n",
    "### Section 1: Model-free Vs Model-based Reinforcement Learning\n",
    "- Theoretical foundations of model-free and model-based approaches\n",
    "- Mathematical formulations and trade-off analysis\n",
    "- Hybrid algorithms combining both paradigms\n",
    "- Practical implementation and comparison\n",
    "\n",
    "### Section 2: World Models and Imagination-based Learning\n",
    "- Variational autoencoders for world modeling\n",
    "- Stochastic dynamics prediction in latent space\n",
    "- Imagination-based planning and policy optimization\n",
    "- Dreamer algorithm and modern variants\n",
    "\n",
    "### Section 3: Sample Efficiency and Transfer Learning\n",
    "- Prioritized experience replay and data augmentation\n",
    "- Auxiliary tasks for improved learning\n",
    "- Transfer learning techniques and meta-learning\n",
    "- Domain adaptation and curriculum learning\n",
    "\n",
    "### Section 4: Hierarchical Reinforcement Learning\n",
    "- Options framework and temporal abstraction\n",
    "- Hierarchical policy architectures\n",
    "- Skill discovery and composition\n",
    "- Applications to complex task domains\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "CA13/\n",
    "â”œâ”€â”€ CA13.ipynb              # Main assignment notebook\n",
    "â”œâ”€â”€ agents/                 # RL agent implementations\n",
    "â”‚   â”œâ”€â”€ model*free*agent.py # Model-free RL agents\n",
    "â”‚   â”œâ”€â”€ model*based*agent.py# Model-based RL agents\n",
    "â”‚   â”œâ”€â”€ world*model*agent.py# World model-based agents\n",
    "â”‚   â””â”€â”€ hierarchical_agent.py# Hierarchical RL agents\n",
    "â”œâ”€â”€ models/                 # Neural network architectures\n",
    "â”‚   â”œâ”€â”€ world_model.py      # VAE-based world models\n",
    "â”‚   â”œâ”€â”€ dynamics_model.py   # Environment dynamics models\n",
    "â”‚   â””â”€â”€ policy_networks.py  # Hierarchical policy networks\n",
    "â”œâ”€â”€ environments/           # Custom environments\n",
    "â”‚   â”œâ”€â”€ wrappers.py         # Environment wrappers\n",
    "â”‚   â””â”€â”€ complex_tasks.py    # Complex task environments\n",
    "â”œâ”€â”€ experiments/            # Training and evaluation scripts\n",
    "â”‚   â”œâ”€â”€ train*world*model.py# World model training\n",
    "â”‚   â”œâ”€â”€ compare_efficiency.py# Sample efficiency comparison\n",
    "â”‚   â””â”€â”€ transfer_learning.py# Transfer learning experiments\n",
    "â””â”€â”€ utils/                  # Utility functions\n",
    "    â”œâ”€â”€ visualization.py    # Plotting and analysis tools\n",
    "    â”œâ”€â”€ data_augmentation.py# Data augmentation utilities\n",
    "    â””â”€â”€ evaluation.py       # Performance evaluation metrics\n",
    "```\n",
    "\n",
    "## Contents Overview\n",
    "\n",
    "### Theoretical Foundations\n",
    "- **Model-Based RL Mathematics**: Transition and reward model learning, planning algorithms\n",
    "- **World Model Theory**: Variational inference, latent space dynamics, imagination-based learning\n",
    "- **Sample Efficiency**: Experience replay, prioritization, auxiliary learning objectives\n",
    "- **Transfer Learning**: Representation learning, fine-tuning, meta-learning algorithms\n",
    "\n",
    "### Implementation Components\n",
    "- **VAE World Models**: Encoder-decoder architectures with stochastic latent variables\n",
    "- **Imagination-Based Agents**: Planning in learned latent space using world models\n",
    "- **Sample-Efficient Algorithms**: Prioritized replay, data augmentation, auxiliary tasks\n",
    "- **Transfer Learning Systems**: Multi-task learning, fine-tuning, domain adaptation\n",
    "\n",
    "### Advanced Topics\n",
    "- **Hierarchical RL**: Options framework, skill hierarchies, temporal abstraction\n",
    "- **Meta-Learning**: Few-shot adaptation, gradient-based meta-learning\n",
    "- **Curriculum Learning**: Automatic difficulty progression, teacher-student frameworks\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Your implementation will be evaluated based on:\n",
    "\n",
    "1. **Correctness (40%)**: Accurate implementation of algorithms and mathematical formulations\n",
    "2. **Efficiency (25%)**: Sample efficiency improvements and computational performance\n",
    "3. **Innovation (20%)**: Creative extensions and novel approaches to the problems\n",
    "4. **Analysis (15%)**: Quality of experimental analysis and insights\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Environment Setup**: Ensure all dependencies are installed\n",
    "2. **Code Review**: Understand the provided base implementations\n",
    "3. **Incremental Development**: Start with simpler components and build complexity\n",
    "4. **Testing**: Validate each component before integration\n",
    "5. **Experimentation**: Run comprehensive experiments and analyze results\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this assignment, you will have:\n",
    "\n",
    "- **Comprehensive Understanding**: Deep knowledge of advanced model-based RL techniques\n",
    "- **Practical Skills**: Ability to implement complex RL systems from scratch\n",
    "- **Research Perspective**: Insight into current challenges and future directions\n",
    "- **Portfolio Piece**: High-quality implementation demonstrating advanced RL capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This assignment represents the culmination of the Deep RL course, integrating concepts from model-free and model-based learning, advanced architectures, and practical deployment considerations. Focus on understanding the theoretical foundations while developing robust, efficient implementations.\n",
    "\n",
    "Let's begin our exploration of advanced model-based reinforcement learning and world models! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ed79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "âœ… CA13 modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import implementations from CA13 package modules\n",
    "from agents.model_free import ModelFreeAgent, DQNAgent\n",
    "from agents.model_based import ModelBasedAgent\n",
    "from buffers.replay_buffer import ReplayBuffer\n",
    "from environments.grid_world import SimpleGridWorld\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"âœ… CA13 modules imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398420da",
   "metadata": {},
   "source": [
    "# Import required libraries for experiments\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "device = get_device()\n",
    "\n",
    "print(f\"âœ“ Using device: {device}\")\n",
    "print(f\"âœ“ Random seed set to: {seed}\")\n",
    "print(\"âœ“ Ready for experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6554d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VariationalWorldModel imported from models.world_model package\n",
      "ðŸ’¡ This model provides VAE-based world modeling for learning environment dynamics\n"
     ]
    }
   ],
   "source": [
    "# Import VariationalWorldModel from package\n",
    "from models.world_model import VariationalWorldModel\n",
    "\n",
    "print(\"VariationalWorldModel imported from models.world_model package\")\n",
    "print(\"This model provides VAE-based world modeling for learning environment dynamics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396b783",
   "metadata": {},
   "source": [
    "# Section 2: Model-Based vs Model-Free RL Trade-offs\n",
    "\n",
    "## 2.1 Fundamental Differences\n",
    "\n",
    "Model-free and model-based reinforcement learning represent two fundamentally different approaches to learning optimal behavior:\n",
    "\n",
    "### Model-Free RL\n",
    "- **Direct Learning**: Learns value functions or policies directly from experience\n",
    "- **Sample Intensive**: Requires many environment interactions\n",
    "- **Computationally Efficient**: Simple forward passes through networks\n",
    "- **Examples**: DQN, PPO, SAC, A3C\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "$$Q^*(s,a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} Q^*(s_{t+1}, a') | s_t=s, a_t=a]$$\n",
    "\n",
    "### Model-Based RL\n",
    "- **Environment Modeling**: Learns dynamics model $P(s'|s,a)$ and reward model $R(s,a)$\n",
    "- **Sample Efficient**: Can plan using learned model\n",
    "- **Computationally Intensive**: Planning requires model rollouts\n",
    "- **Examples**: Dyna-Q, PETS, MuZero\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "$$\\hat{T}(s'|s,a) \\approx P(s'|s,a), \\quad \\hat{R}(s,a) \\approx \\mathbb{E}[r|s,a]$$\n",
    "\n",
    "## 2.2 Advantages and Limitations\n",
    "\n",
    "### Model-Free Advantages:\n",
    "âœ“ **Asymptotic Performance**: Can learn highly accurate policies with enough data  \n",
    "âœ“ **Stability**: No model bias, directly optimizes objective  \n",
    "âœ“ **Simplicity**: Straightforward implementation  \n",
    "\n",
    "### Model-Free Limitations:\n",
    "âœ— **Sample Inefficiency**: Requires millions of interactions  \n",
    "âœ— **No Generalization**: Must relearn for new tasks  \n",
    "âœ— **No Planning**: Cannot simulate future trajectories  \n",
    "\n",
    "### Model-Based Advantages:\n",
    "âœ“ **Sample Efficiency**: Learn from fewer real interactions  \n",
    "âœ“ **Transfer Learning**: Model can transfer across tasks  \n",
    "âœ“ **Interpretability**: Explicit model of environment  \n",
    "âœ“ **Planning**: Can look ahead before acting  \n",
    "\n",
    "### Model-Based Limitations:\n",
    "âœ— **Model Bias**: Errors compound during planning  \n",
    "âœ— **Computational Cost**: Planning is expensive  \n",
    "âœ— **Complex Environments**: Hard to model stochastic/high-dim spaces  \n",
    "\n",
    "## 2.3 Appropriate Use Cases\n",
    "\n",
    "| Scenario | Recommended Approach | Reasoning |\n",
    "|----------|---------------------|-----------|\n",
    "| Limited Data | Model-Based | Better sample efficiency |\n",
    "| Abundant Data | Model-Free | Avoid model bias |\n",
    "| Related Tasks | Model-Based | Model transfers |\n",
    "| High-Dim Observations | Hybrid | World models for compression |\n",
    "| Real-World Robotics | Model-Based â†’ Model-Free | Sim training then real fine-tuning |\n",
    "| Games (Atari, Chess) | Model-Free or Hybrid | Can collect many samples |\n",
    "| Safety-Critical | Model-Based | Planning avoids dangerous states |\n",
    "\n",
    "## 2.4 Performance Comparison\n",
    "\n",
    "Let's implement and compare both approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2.4: Practical Implementation - Model-Free vs Model-Based Comparison\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 2: MODEL-FREE VS MODEL-BASED RL COMPARISON\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create simple environment for comparison\n",
    "try:\n",
    "    env = gym.make(\"CartPole-v1\")\n",
    "except:\n",
    "    from environments.grid_world import SimpleGridWorld\n",
    "    env = SimpleGridWorld(size=5)\n",
    "    \n",
    "state_dim = env.observation_space.shape[0] if hasattr(env.observation_space, 'shape') else 2\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"\\nEnvironment: {env}\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "# Initialize agents\n",
    "print(\"\\nðŸ“Š Initializing Agents...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Model-Free Agent (DQN)\n",
    "mf_agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=128,\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=500,\n",
    ")\n",
    "print(\"âœ“ Model-Free DQN Agent initialized\")\n",
    "\n",
    "# Model-Based Agent\n",
    "mb_agent = ModelBasedAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=128,\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.99,\n",
    ")\n",
    "print(\"âœ“ Model-Based Agent initialized\")\n",
    "\n",
    "# Hybrid Dyna-Q Agent (if available)\n",
    "try:\n",
    "    hybrid_agent = HybridDynaAgent(\n",
    "        state_dim=state_dim,\n",
    "        action_dim=action_dim,\n",
    "        learning_rate=1e-3,\n",
    "        planning_steps=5,\n",
    "    )\n",
    "    print(\"âœ“ Hybrid Dyna-Q Agent initialized\")\n",
    "    use_hybrid = True\n",
    "except:\n",
    "    print(\"âš  Hybrid agent not available, will compare MF vs MB only\")\n",
    "    use_hybrid = False\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734b540",
   "metadata": {},
   "source": [
    "# Section 3: World Model Architectures\n",
    "\n",
    "## 3.1 Variational Autoencoders for World Models\n",
    "\n",
    "World models learn compressed representations of environments using variational autoencoders (VAEs), enabling efficient imagination-based planning.\n",
    "\n",
    "### VAE Architecture for World Modeling\n",
    "\n",
    "**Encoder**: Maps high-dimensional observations to latent space\n",
    "$$q_\\phi(z|s) = \\mathcal{N}(\\mu_\\phi(s), \\sigma_\\phi(s))$$\n",
    "\n",
    "**Decoder**: Reconstructs observations from latent representations\n",
    "$$p_\\theta(s|z) = \\mathcal{N}(\\mu_\\theta(z), \\sigma_\\theta(z))$$\n",
    "\n",
    "**VAE Loss**:\n",
    "$$\\mathcal{L}_{VAE} = \\mathbb{E}_{q_\\phi(z|s)}[\\log p_\\theta(s|z)] - D_{KL}(q_\\phi(z|s) || p(z))$$\n",
    "\n",
    "Where:\n",
    "- First term: Reconstruction loss\n",
    "- Second term: KL divergence (regularization)\n",
    "\n",
    "## 3.2 Stochastic Dynamics Modeling\n",
    "\n",
    "Learn dynamics in latent space for efficient planning:\n",
    "\n",
    "**Latent Dynamics Model**:\n",
    "$$z_{t+1} \\sim p(z_{t+1}|z_t, a_t)$$\n",
    "\n",
    "**Reward Model**:\n",
    "$$r_t \\sim p(r_t|z_t, a_t)$$\n",
    "\n",
    "**Complete World Model Loss**:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{VAE} + \\mathcal{L}_{dynamics} + \\mathcal{L}_{reward}$$\n",
    "\n",
    "## 3.3 Latent Representation Learning\n",
    "\n",
    "Benefits of learning in latent space:\n",
    "- **Compression**: Reduce dimensionality of observations (e.g., images)\n",
    "- **Efficiency**: Faster planning in compressed space\n",
    "- **Generalization**: Latent space captures essential features\n",
    "- **Stochasticity**: VAE handles uncertainty\n",
    "\n",
    "## 3.4 Implementation\n",
    "\n",
    "Let's implement and test a variational world model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3.4: World Model Implementation and Demonstration\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 3: WORLD MODEL ARCHITECTURE DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize world model\n",
    "from models.world_model import VariationalWorldModel\n",
    "\n",
    "latent_dim = 32\n",
    "world_model = VariationalWorldModel(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=128\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Variational World Model initialized\")\n",
    "print(f\"  State dimension: {state_dim}\")\n",
    "print(f\"  Action dimension: {action_dim}\")\n",
    "print(f\"  Latent dimension: {latent_dim}\")\n",
    "print(f\"  Architecture: VAE + Dynamics Model + Reward Model\")\n",
    "\n",
    "# Demonstrate world model forward pass\n",
    "print(\"\\nðŸ“Š Testing World Model Components...\")\n",
    "test_state = torch.randn(1, state_dim)\n",
    "test_action = torch.randint(0, action_dim, (1,))\n",
    "\n",
    "# Encode state to latent\n",
    "print(\"\\n1. Encoding state to latent space...\")\n",
    "with torch.no_grad():\n",
    "    mu, logvar = world_model.encode(test_state)\n",
    "    z = world_model.reparameterize(mu, logvar)\n",
    "    print(f\"   Latent mean shape: {mu.shape}\")\n",
    "    print(f\"   Latent sample shape: {z.shape}\")\n",
    "\n",
    "# Predict next latent state\n",
    "print(\"\\n2. Predicting next latent state (dynamics)...\")\n",
    "with torch.no_grad():\n",
    "    z_next = world_model.predict_next_latent(z, test_action)\n",
    "    print(f\"   Next latent shape: {z_next.shape}\")\n",
    "\n",
    "# Predict reward\n",
    "print(\"\\n3. Predicting reward...\")\n",
    "with torch.no_grad():\n",
    "    reward_pred = world_model.predict_reward(z, test_action)\n",
    "    print(f\"   Predicted reward: {reward_pred.item():.4f}\")\n",
    "\n",
    "# Decode latent to observation\n",
    "print(\"\\n4. Decoding latent to observation...\")\n",
    "with torch.no_grad():\n",
    "    reconstructed_state = world_model.decode(z)\n",
    "    print(f\"   Reconstructed state shape: {reconstructed_state.shape}\")\n",
    "    print(f\"   Reconstruction error: {F.mse_loss(reconstructed_state, test_state).item():.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ World model components working correctly!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85188d",
   "metadata": {},
   "source": [
    "# Section 4: Imagination-Based Learning\n",
    "\n",
    "## 4.1 Planning in Latent Space\n",
    "\n",
    "World models enable **imagination**: planning in learned latent space without environment interaction.\n",
    "\n",
    "### Model Predictive Control (MPC) in Latent Space\n",
    "\n",
    "**Algorithm**:\n",
    "1. Encode current observation: $z_t = \\text{Encode}(s_t)$\n",
    "2. For each candidate action sequence $a_{t:t+H}$:\n",
    "   - Simulate trajectory in latent space\n",
    "   - Accumulate predicted rewards\n",
    "3. Execute first action of best sequence\n",
    "4. Repeat\n",
    "\n",
    "**Objective**:\n",
    "$$a^*_t = \\arg\\max_{a_{t:t+H}} \\sum_{k=0}^{H} \\gamma^k \\hat{r}_{t+k}$$\n",
    "\n",
    "where $\\hat{r}_{t+k}$ is predicted from world model.\n",
    "\n",
    "## 4.2 Dreamer Algorithm\n",
    "\n",
    "Dreamer learns policies entirely in latent space using imagined trajectories.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "**1. World Model Learning**:\n",
    "- Learn $p(z_t|z_{t-1}, a_{t-1})$ (dynamics)\n",
    "- Learn $p(s_t|z_t)$ (decoder)  \n",
    "- Learn $p(r_t|z_t, a_t)$ (reward)\n",
    "\n",
    "**2. Behavior Learning** (Policy and Value):\n",
    "- Actor: $\\pi_\\phi(a_t|z_t)$\n",
    "- Critic: $V_\\psi(z_t)$\n",
    "\n",
    "**3. Imagination Training**:\n",
    "```\n",
    "for each real trajectory (s_t, a_t, r_t):\n",
    "    encode to latent: z_t = Encode(s_t)\n",
    "    imagine future trajectories from z_t\n",
    "    train actor-critic on imagined data\n",
    "```\n",
    "\n",
    "## 4.3 Sample-Efficient Learning Benefits\n",
    "\n",
    "Imagination-based learning provides:\n",
    "\n",
    "âœ“ **Sample Efficiency**: Train policy on unlimited imagined data  \n",
    "âœ“ **Gradient Efficiency**: Backprop through differentiable model  \n",
    "âœ“ **Exploration**: Imagine diverse scenarios  \n",
    "âœ“ **Transfer**: World model generalizes across tasks  \n",
    "\n",
    "## 4.4 Implementation\n",
    "\n",
    "Let's implement imagination-based planning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1954d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 4.4: Imagination-Based Planning Implementation\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 4: IMAGINATION-BASED PLANNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "class ImaginationPlanner:\n",
    "    \"\"\"Simple imagination-based planner using world model.\"\"\"\n",
    "    \n",
    "    def __init__(self, world_model, action_dim, planning_horizon=5, num_candidates=10):\n",
    "        self.world_model = world_model\n",
    "        self.action_dim = action_dim\n",
    "        self.planning_horizon = planning_horizon\n",
    "        self.num_candidates = num_candidates\n",
    "        self.gamma = 0.99\n",
    "    \n",
    "    def plan(self, state):\n",
    "        \"\"\"Plan best action by imagining trajectories.\"\"\"\n",
    "        # Encode current state\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0) if isinstance(state, np.ndarray) else state\n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.world_model.encode(state_tensor)\n",
    "            z_current = self.world_model.reparameterize(mu, logvar)\n",
    "        \n",
    "        best_return = float('-inf')\n",
    "        best_action = 0\n",
    "        \n",
    "        # Evaluate each possible first action\n",
    "        for first_action in range(self.action_dim):\n",
    "            # Average over multiple rollouts for stochasticity\n",
    "            returns = []\n",
    "            for _ in range(self.num_candidates // self.action_dim + 1):\n",
    "                total_return = self.imagine_trajectory(z_current, first_action)\n",
    "                returns.append(total_return)\n",
    "            \n",
    "            avg_return = np.mean(returns)\n",
    "            if avg_return > best_return:\n",
    "                best_return = avg_return\n",
    "                best_action = first_action\n",
    "        \n",
    "        return best_action, best_return\n",
    "    \n",
    "    def imagine_trajectory(self, z_start, first_action):\n",
    "        \"\"\"Imagine trajectory starting with first_action.\"\"\"\n",
    "        z = z_start\n",
    "        total_return = 0.0\n",
    "        \n",
    "        for step in range(self.planning_horizon):\n",
    "            # Select action (first action or random)\n",
    "            if step == 0:\n",
    "                action = first_action\n",
    "            else:\n",
    "                action = np.random.randint(self.action_dim)\n",
    "            \n",
    "            action_tensor = torch.tensor([action])\n",
    "            \n",
    "            # Predict reward\n",
    "            with torch.no_grad():\n",
    "                reward = self.world_model.predict_reward(z, action_tensor).item()\n",
    "            \n",
    "            # Accumulate discounted reward\n",
    "            total_return += (self.gamma ** step) * reward\n",
    "            \n",
    "            # Predict next latent state\n",
    "            with torch.no_grad():\n",
    "                z = self.world_model.predict_next_latent(z, action_tensor)\n",
    "        \n",
    "        return total_return\n",
    "\n",
    "# Create imagination planner\n",
    "print(\"\\nâœ“ Creating Imagination-Based Planner...\")\n",
    "planner = ImaginationPlanner(\n",
    "    world_model=world_model,\n",
    "    action_dim=action_dim,\n",
    "    planning_horizon=5,\n",
    "    num_candidates=20\n",
    ")\n",
    "print(f\"  Planning horizon: {planner.planning_horizon}\")\n",
    "print(f\"  Candidates per action: {planner.num_candidates // planner.action_dim + 1}\")\n",
    "\n",
    "# Test imagination planning\n",
    "print(\"\\nðŸ“Š Testing Imagination Planning...\")\n",
    "test_state = torch.randn(1, state_dim)\n",
    "best_action, expected_return = planner.plan(test_state)\n",
    "print(f\"  Test state shape: {test_state.shape}\")\n",
    "print(f\"  Planned best action: {best_action}\")\n",
    "print(f\"  Expected return: {expected_return:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Imagination-based planning working correctly!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2799c6",
   "metadata": {},
   "source": [
    "# Section 3: Sample Efficiency and Transfer Learning\n",
    "\n",
    "## 3.1 Sample Efficiency Challenges in Deep Rl\n",
    "\n",
    "Sample efficiency is one of the most critical challenges in deep reinforcement learning, particularly for real-world applications where data collection is expensive or dangerous.\n",
    "\n",
    "### Why Is Sample Efficiency Important?\n",
    "\n",
    "**Real-World Constraints:**\n",
    "- **Cost**: Real-world interactions can be expensive (robotics, autonomous vehicles)\n",
    "- **Time**: Learning from millions of samples is often impractical\n",
    "- **Safety**: Exploratory actions in safety-critical domains can be dangerous\n",
    "- **Reproducibility**: Limited samples make experiments more reliable\n",
    "\n",
    "**Sample Complexity Factors:**\n",
    "- **Environment Complexity**: High-dimensional state/action spaces\n",
    "- **Sparse Rewards**: Learning signals are infrequent\n",
    "- **Stochasticity**: Environmental noise requires more samples\n",
    "- **Exploration**: Discovering good policies requires extensive exploration\n",
    "\n",
    "## 3.2 Sample Efficiency Techniques\n",
    "\n",
    "### 3.2.1 Experience Replay and Prioritization\n",
    "\n",
    "**Experience Replay Benefits:**\n",
    "- Reuse past experiences multiple times\n",
    "- Break temporal correlations in data\n",
    "- Enable off-policy learning\n",
    "\n",
    "**Prioritized Experience Replay:**\n",
    "Prioritize experiences based on temporal difference (TD) error:\n",
    "$$P(i) = \\frac{p*i^\\alpha}{\\sum*k p_k^\\alpha}$$\n",
    "\n",
    "Where $p*i = |\\delta*i| + \\epsilon$ and $\\delta_i$ is the TD error.\n",
    "\n",
    "### 3.2.2 Data Augmentation\n",
    "\n",
    "**Techniques:**\n",
    "- **Random Crops**: For image-based environments\n",
    "- **Color Jittering**: Robust to lighting variations  \n",
    "- **Random Shifts**: Translation invariance\n",
    "- **Gaussian Noise**: Regularization effect\n",
    "\n",
    "### 3.2.3 Auxiliary Tasks\n",
    "\n",
    "Learn multiple tasks simultaneously to improve sample efficiency:\n",
    "- **Pixel Control**: Predict pixel changes\n",
    "- **Feature Control**: Control learned feature representations\n",
    "- **Reward Prediction**: Predict future rewards\n",
    "- **Value Function Replay**: Replay value function updates\n",
    "\n",
    "## 3.3 Transfer Learning in Reinforcement Learning\n",
    "\n",
    "Transfer learning enables agents to leverage knowledge from previous tasks to learn new tasks more efficiently.\n",
    "\n",
    "### 3.3.1 Types of Transfer in Rl\n",
    "\n",
    "**Policy Transfer:**\n",
    "$$\\pi*{target}(a|s) = f(\\pi*{source}(a|s), s, \\theta_{adapt})$$\n",
    "\n",
    "**Value Function Transfer:**\n",
    "$$Q*{target}(s,a) = g(Q*{source}(s,a), s, a, \\phi_{adapt})$$\n",
    "\n",
    "**Representation Transfer:**\n",
    "$$\\phi*{target}(s) = h(\\phi*{source}(s), \\psi_{adapt})$$\n",
    "\n",
    "### 3.3.2 Transfer Learning Approaches\n",
    "\n",
    "#### Fine-tuning\n",
    "1. Pre-train on source task\n",
    "2. Initialize target model with source weights\n",
    "3. Fine-tune on target task with lower learning rate\n",
    "\n",
    "#### Progressive Networks\n",
    "- Freeze source network columns\n",
    "- Add new columns for target tasks\n",
    "- Use lateral connections between columns\n",
    "\n",
    "#### Universal Value Functions (uvf)\n",
    "Learn value functions conditioned on goals:\n",
    "$$Q(s, a, g) = \\text{Value of action } a \\text{ in state } s \\text{ for goal } g$$\n",
    "\n",
    "## 3.4 Meta-learning and Few-shot Adaptation\n",
    "\n",
    "Meta-learning enables agents to quickly adapt to new tasks with limited experience.\n",
    "\n",
    "### 3.4.1 Model-agnostic Meta-learning (maml)\n",
    "\n",
    "**Objective:**\n",
    "$$\\min*\\theta \\sum*{\\tau \\sim p(\\mathcal{T})} \\mathcal{L}*\\tau(f*{\\theta_\\tau'})$$\n",
    "\n",
    "Where $\\theta*\\tau' = \\theta - \\alpha \\nabla*\\theta \\mathcal{L}*\\tau(f*\\theta)$\n",
    "\n",
    "**MAML Algorithm:**\n",
    "1. Sample batch of tasks\n",
    "2. For each task, compute adapted parameters via gradient descent\n",
    "3. Update meta-parameters using gradient through adaptation process\n",
    "\n",
    "### 3.4.2 Gradient-based Meta-learning\n",
    "\n",
    "**Reptile Algorithm:**\n",
    "Simpler alternative to MAML:\n",
    "$$\\theta \\leftarrow \\theta + \\beta \\frac{1}{n} \\sum*{i=1}^n (\\phi*i - \\theta)$$\n",
    "\n",
    "Where $\\phi_i$ is the result of training on task $i$.\n",
    "\n",
    "## 3.5 Domain Adaptation and Sim-to-real Transfer\n",
    "\n",
    "### 3.5.1 Domain Randomization\n",
    "\n",
    "**Technique:**\n",
    "Randomize simulation parameters during training:\n",
    "- Physical properties (mass, friction, damping)\n",
    "- Visual appearance (textures, lighting, colors)\n",
    "- Sensor characteristics (noise, resolution, field of view)\n",
    "\n",
    "**Benefits:**\n",
    "- Learned policies are robust to domain variations\n",
    "- Improved transfer from simulation to real world\n",
    "- Reduced need for domain-specific engineering\n",
    "\n",
    "### 3.5.2 Domain Adversarial Training\n",
    "\n",
    "**Objective:**\n",
    "$$\\min*\\theta \\mathcal{L}*{task}(\\theta) + \\lambda \\mathcal{L}_{domain}(\\theta)$$\n",
    "\n",
    "Where $\\mathcal{L}_{domain}$ encourages domain-invariant features.\n",
    "\n",
    "## 3.6 Curriculum Learning\n",
    "\n",
    "Structure learning to progress from simple to complex tasks.\n",
    "\n",
    "### 3.6.1 Curriculum Design Principles\n",
    "\n",
    "**Manual Curriculum:**\n",
    "- Hand-designed progression of tasks\n",
    "- Expert knowledge of difficulty ordering\n",
    "- Fixed curriculum regardless of agent performance\n",
    "\n",
    "**Automatic Curriculum:**\n",
    "- Adaptive task selection based on agent performance\n",
    "- Learning progress as curriculum signal\n",
    "- Self-paced learning approaches\n",
    "\n",
    "### 3.6.2 Curriculum Learning Algorithms\n",
    "\n",
    "**Teacher-Student Framework:**\n",
    "- Teacher selects appropriate tasks for student\n",
    "- Task difficulty based on student's current capability\n",
    "- Optimize task selection for maximum learning progress\n",
    "\n",
    "**Self-Play Curriculum:**\n",
    "- Agent plays against previous versions of itself\n",
    "- Automatic difficulty adjustment\n",
    "- Prevents catastrophic forgetting of simpler strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrioritizedReplayBuffer is imported from CA13 package\n",
    "# Available for use in sample efficiency experiments\n",
    "\n",
    "print(\"âœ“ PrioritizedReplayBuffer imported from CA13.buffers\")\n",
    "print(\"âœ“ Ready for sample efficiency experiments\")\n",
    "print(\"\\nKey features:\")\n",
    "print(\"  - Priority-based sampling for improved learning\")\n",
    "print(\"  - Importance sampling weights for bias correction\")\n",
    "print(\"  - Configurable alpha (priority exponent) and beta (IS exponent)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b470f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 5: Sample Efficiency Techniques - Practical Implementation\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 5: SAMPLE EFFICIENCY TECHNIQUES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize Sample-Efficient Agent with all techniques\n",
    "print(\"\\nðŸ“Š Initializing Sample-Efficient Agent...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "se_agent = SampleEfficientAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "\n",
    "print(\"âœ“ Sample-Efficient Agent initialized with:\")\n",
    "print(\"  âœ“ Prioritized Experience Replay\")\n",
    "print(\"  âœ“ Data Augmentation (noise, dropout, scaling)\")\n",
    "print(\"  âœ“ Auxiliary Tasks (reward prediction, dynamics)\")\n",
    "print(\"  âœ“ Target Network with periodic updates\")\n",
    "\n",
    "# Demonstrate sample efficiency components\n",
    "print(\"\\nðŸ“Š Testing Sample Efficiency Components...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# 1. Prioritized Replay\n",
    "print(\"\\n1. Prioritized Experience Replay:\")\n",
    "print(\"   - Stores transitions with TD-error based priorities\")\n",
    "print(\"   - Samples important experiences more frequently\")\n",
    "print(\"   - Uses importance sampling weights for unbiased updates\")\n",
    "if hasattr(se_agent, 'replay_buffer'):\n",
    "    print(f\"   Buffer capacity: {se_agent.replay_buffer.capacity}\")\n",
    "    print(f\"   Current size: {len(se_agent.replay_buffer)}\")\n",
    "\n",
    "# 2. Data Augmentation\n",
    "print(\"\\n2. Data Augmentation:\")\n",
    "test_state = torch.randn(4, state_dim)\n",
    "print(f\"   Original state shape: {test_state.shape}\")\n",
    "for aug_type in ['noise', 'dropout', 'scaling']:\n",
    "    aug_state = se_agent.network.apply_augmentation(test_state.clone(), aug_type)\n",
    "    diff = F.mse_loss(aug_state, test_state).item()\n",
    "    print(f\"   {aug_type.capitalize():12s} - MSE difference: {diff:.6f}\")\n",
    "\n",
    "# 3. Auxiliary Tasks\n",
    "print(\"\\n3. Auxiliary Tasks:\")\n",
    "print(\"   Testing forward pass with auxiliary predictions...\")\n",
    "test_actions = torch.randint(0, action_dim, (4,))\n",
    "with torch.no_grad():\n",
    "    q_values, reward_pred, next_state_pred = se_agent.network(test_state, test_actions)\n",
    "    print(f\"   Q-values shape:      {q_values.shape}\")\n",
    "    print(f\"   Reward pred shape:   {reward_pred.shape}\")\n",
    "    print(f\"   Next state pred shape: {next_state_pred.shape}\")\n",
    "\n",
    "print(\"\\nâœ“ All sample efficiency components working!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62229db9",
   "metadata": {},
   "source": [
    "# Section 4: Hierarchical Reinforcement Learning\n",
    "\n",
    "## 4.1 Theory: Hierarchical Decision Making\n",
    "\n",
    "Hierarchical Reinforcement Learning (HRL) addresses the challenge of learning complex behaviors by decomposing tasks into hierarchical structures. This approach enables agents to:\n",
    "\n",
    "1. **Learn at Multiple Time Scales**: High-level policies select goals or skills, while low-level policies execute primitive actions\n",
    "2. **Achieve Better Generalization**: Skills learned in one context can be reused in others\n",
    "3. **Improve Sample Efficiency**: By leveraging temporal abstractions and skill composition\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### Options Framework\n",
    "An **option** $\\omega$ is defined by a tuple $(I*\\omega, \\pi*\\omega, \\beta_\\omega)$:\n",
    "- **Initiation Set** $I_\\omega \\subseteq \\mathcal{S}$: States where the option can be initiated\n",
    "- **Policy** $\\pi_\\omega: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$: Action selection within the option\n",
    "- **Termination Condition** $\\beta_\\omega: \\mathcal{S} \\rightarrow [0,1]$: Probability of termination\n",
    "\n",
    "#### Hierarchical Value Functions\n",
    "The value function for options follows the Bellman equation:\n",
    "$$Q^\\pi(s,\\omega) = \\mathbb{E}*\\pi\\left[\\sum*{t=0}^{\\tau-1} \\gamma^t r*{t+1} + \\gamma^\\tau Q^\\pi(s*\\tau, \\omega') \\mid s*0=s, \\omega*0=\\omega\\right]$$\n",
    "\n",
    "where $\\tau$ is the termination time and $\\omega'$ is the next option selected.\n",
    "\n",
    "#### Feudal Networks\n",
    "Feudal Networks implement a manager-worker hierarchy:\n",
    "- **Manager Network**: Sets goals $g*t$ for workers: $g*t = f*{manager}(s*t, h_{t-1}^{manager})$\n",
    "- **Worker Network**: Executes actions conditioned on goals: $a*t = \\pi*{worker}(s*t, g*t)$\n",
    "- **Intrinsic Motivation**: Workers receive intrinsic rewards based on goal achievement\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### Intrinsic Reward Signal\n",
    "The intrinsic reward for achieving subgoals:\n",
    "$$r*t^{intrinsic} = \\cos(\\text{achieved\\*goal}*t - \\text{desired\\*goal}*t) \\cdot ||s*{t+1} - s_t||$$\n",
    "\n",
    "#### Hierarchical Policy Gradient\n",
    "The gradient for the manager policy:\n",
    "$$\\nabla*{\\theta*m} J*m = \\mathbb{E}\\left[\\nabla*{\\theta*m} \\log \\pi*m(g*t|s*t) \\cdot A*m(s*t, g_t)\\right]$$\n",
    "\n",
    "And for the worker policy:\n",
    "$$\\nabla*{\\theta*w} J*w = \\mathbb{E}\\left[\\nabla*{\\theta*w} \\log \\pi*w(a*t|s*t, g*t) \\cdot A*w(s*t, a*t, g_t)\\right]$$\n",
    "\n",
    "## 4.2 Implementation: Hierarchical Rl Architectures\n",
    "\n",
    "We'll implement several HRL approaches:\n",
    "1. **Options-Critic Architecture**: Learn options and policies jointly\n",
    "2. **Feudal Networks**: Manager-worker hierarchies\n",
    "3. **Hindsight Experience Replay with Goals**: Sample efficiency for goal-conditioned tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c601a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Hierarchical RL classes imported from agents.hierarchical package\n",
      "ðŸ’¡ Includes Options-Critic and Feudal Networks for hierarchical decision making\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical RL agents are already imported from CA13 package\n",
    "# Available: OptionsCriticAgent, FeudalAgent\n",
    "\n",
    "print(\"âœ“ Hierarchical RL classes available:\")\n",
    "print(\"  - OptionsCriticAgent: Options-Critic architecture\")\n",
    "print(\"  - FeudalAgent: Feudal Networks for manager-worker hierarchies\")\n",
    "print(\"  - All hierarchical RL components imported from CA13 package\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 7: Hierarchical RL - Practical Implementation\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 7: HIERARCHICAL REINFORCEMENT LEARNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Initialize Hierarchical Agents\n",
    "print(\"\\nðŸ“Š Initializing Hierarchical RL Agents...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Options-Critic Agent\n",
    "num_options = 4\n",
    "oc_agent = OptionsCriticAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    num_options=num_options,\n",
    "    hidden_dim=128,\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "print(f\"âœ“ Options-Critic Agent initialized\")\n",
    "print(f\"  Number of options: {num_options}\")\n",
    "print(f\"  State dimension: {state_dim}\")\n",
    "print(f\"  Action dimension: {action_dim}\")\n",
    "\n",
    "# Feudal Networks Agent\n",
    "feudal_agent = FeudalAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    goal_dim=16,\n",
    "    hidden_dim=128,\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "print(f\"\\nâœ“ Feudal Networks Agent initialized\")\n",
    "print(f\"  Goal dimension: 16\")\n",
    "print(f\"  Manager-Worker hierarchy established\")\n",
    "\n",
    "# Demonstrate Options-Critic\n",
    "print(\"\\nðŸ“Š Testing Options-Critic Architecture...\")\n",
    "print(\"-\" * 80)\n",
    "test_state = np.random.randn(state_dim)\n",
    "\n",
    "print(\"\\n1. Option Selection:\")\n",
    "action, option = oc_agent.act(test_state)\n",
    "print(f\"   State shape: {test_state.shape}\")\n",
    "print(f\"   Selected option: {option}\")\n",
    "print(f\"   Selected action: {action}\")\n",
    "\n",
    "print(\"\\n2. Option Termination:\")\n",
    "should_terminate = oc_agent.should_terminate(test_state, option)\n",
    "print(f\"   Termination probability: {should_terminate:.4f}\")\n",
    "\n",
    "# Demonstrate Feudal Networks  \n",
    "print(\"\\nðŸ“Š Testing Feudal Networks Architecture...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n1. Manager Goal Setting:\")\n",
    "goal = feudal_agent.manager_set_goal(test_state)\n",
    "print(f\"   State shape: {test_state.shape}\")\n",
    "print(f\"   Manager goal shape: {goal.shape}\")\n",
    "print(f\"   Goal vector norm: {np.linalg.norm(goal):.4f}\")\n",
    "\n",
    "print(\"\\n2. Worker Action Execution:\")\n",
    "action = feudal_agent.worker_act(test_state, goal)\n",
    "print(f\"   Selected action: {action}\")\n",
    "\n",
    "print(\"\\n3. Intrinsic Reward Computation:\")\n",
    "next_state = np.random.randn(state_dim)\n",
    "intrinsic_reward = feudal_agent.compute_intrinsic_reward(\n",
    "    test_state, next_state, goal\n",
    ")\n",
    "print(f\"   Intrinsic reward: {intrinsic_reward:.4f}\")\n",
    "\n",
    "print(\"\\nâœ“ Hierarchical RL components working correctly!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Demonstration: Train and Compare All Methods\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPREHENSIVE EVALUATION: Sample-Efficient Deep RL Methods\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create environment\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"\\nEnvironment: {env_name}\")\n",
    "print(f\"State dimension: {state_dim}\")\n",
    "print(f\"Action dimension: {action_dim}\")\n",
    "\n",
    "# Configuration\n",
    "num_episodes = 200\n",
    "eval_interval = 20\n",
    "\n",
    "# 1. Train Model-Free DQN Agent\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"1. Training Model-Free DQN Agent\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dqn_agent = DQNAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=128,\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.99,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.01,\n",
    "    epsilon_decay=500,\n",
    ")\n",
    "\n",
    "dqn_results = train_dqn_agent(\n",
    "    env=gym.make(env_name),\n",
    "    agent=dqn_agent,\n",
    "    num_episodes=num_episodes,\n",
    "    max_steps=500,\n",
    "    eval_interval=eval_interval,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ DQN Training Complete\")\n",
    "print(f\"  Final Average Return: {np.mean(dqn_results['rewards'][-20:]):.2f}\")\n",
    "\n",
    "# 2. Train Model-Based Agent\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. Training Model-Based Agent with Planning\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "mb_agent = ModelBasedAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=128,\n",
    "    learning_rate=1e-3,\n",
    "    gamma=0.99,\n",
    ")\n",
    "\n",
    "mb_results = train_model_based_agent(\n",
    "    env=gym.make(env_name),\n",
    "    agent=mb_agent,\n",
    "    num_episodes=num_episodes,\n",
    "    max_steps=500,\n",
    "    eval_interval=eval_interval,\n",
    "    planning_steps=10,\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Model-Based Training Complete\")\n",
    "print(f\"  Final Average Return: {np.mean(mb_results['rewards'][-20:]):.2f}\")\n",
    "\n",
    "# 3. Evaluate Both Agents\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"3. Final Evaluation\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "dqn_eval = evaluate_agent(gym.make(env_name), dqn_agent, num_episodes=10)\n",
    "mb_eval = evaluate_agent(gym.make(env_name), mb_agent, num_episodes=10)\n",
    "\n",
    "print(f\"\\nDQN Evaluation:\")\n",
    "print(f\"  Mean Return: {dqn_eval['mean_return']:.2f} Â± {dqn_eval['std_return']:.2f}\")\n",
    "print(f\"  Mean Length: {dqn_eval['mean_length']:.2f} Â± {dqn_eval['std_length']:.2f}\")\n",
    "\n",
    "print(f\"\\nModel-Based Evaluation:\")\n",
    "print(f\"  Mean Return: {mb_eval['mean_return']:.2f} Â± {mb_eval['std_return']:.2f}\")\n",
    "print(f\"  Mean Length: {mb_eval['mean_length']:.2f} Â± {mb_eval['std_length']:.2f}\")\n",
    "\n",
    "# 4. Visualization\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"4. Performance Visualization\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# Plot 1: Learning Curves\n",
    "ax = axes[0, 0]\n",
    "window = 10\n",
    "dqn_smoothed = pd.Series(dqn_results['rewards']).rolling(window=window, min_periods=1).mean()\n",
    "mb_smoothed = pd.Series(mb_results['rewards']).rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "ax.plot(dqn_results['rewards'], alpha=0.3, color='blue', label='DQN Raw')\n",
    "ax.plot(dqn_smoothed, color='blue', linewidth=2, label='DQN Smoothed')\n",
    "ax.plot(mb_results['rewards'], alpha=0.3, color='green', label='MB Raw')\n",
    "ax.plot(mb_smoothed, color='green', linewidth=2, label='MB Smoothed')\n",
    "ax.axhline(y=195, color='red', linestyle='--', label='Solved Threshold')\n",
    "ax.set_title('Learning Curves Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Return')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: Episode Lengths\n",
    "ax = axes[0, 1]\n",
    "dqn_len_smoothed = pd.Series(dqn_results['lengths']).rolling(window=window, min_periods=1).mean()\n",
    "mb_len_smoothed = pd.Series(mb_results['lengths']).rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "ax.plot(dqn_len_smoothed, color='blue', linewidth=2, label='DQN')\n",
    "ax.plot(mb_len_smoothed, color='green', linewidth=2, label='Model-Based')\n",
    "ax.set_title('Episode Length Progression', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Steps')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 3: Loss Comparison\n",
    "ax = axes[1, 0]\n",
    "if dqn_results.get('losses'):\n",
    "    ax.plot(dqn_results['losses'][:1000], alpha=0.6, color='blue', label='DQN Loss')\n",
    "if mb_results.get('q_losses'):\n",
    "    ax.plot(mb_results['q_losses'][:1000], alpha=0.6, color='green', label='MB Q-Loss')\n",
    "if mb_results.get('model_losses'):\n",
    "    ax.plot(mb_results['model_losses'][:1000], alpha=0.6, color='orange', label='MB Model Loss')\n",
    "ax.set_title('Training Loss Dynamics', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Training Step')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "# Plot 4: Performance Summary\n",
    "ax = axes[1, 1]\n",
    "methods = ['DQN', 'Model-Based']\n",
    "final_returns = [\n",
    "    np.mean(dqn_results['rewards'][-20:]),\n",
    "    np.mean(mb_results['rewards'][-20:])\n",
    "]\n",
    "eval_returns = [dqn_eval['mean_return'], mb_eval['mean_return']]\n",
    "eval_stds = [dqn_eval['std_return'], mb_eval['std_return']]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, final_returns, width, label='Training (last 20)', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, eval_returns, width, yerr=eval_stds, label='Evaluation (10 eps)', alpha=0.8, capsize=5)\n",
    "\n",
    "ax.set_title('Final Performance Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('Return')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(methods)\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display episode dataframes\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"5. Detailed Episode Metrics\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if 'episode_dataframe' in dqn_results:\n",
    "    print(\"\\nDQN Recent Episodes:\")\n",
    "    display(dqn_results['episode_dataframe'].tail())\n",
    "\n",
    "if 'episode_dataframe' in mb_results:\n",
    "    print(\"\\nModel-Based Recent Episodes:\")\n",
    "    display(mb_results['episode_dataframe'].tail())\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY INSIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ“ Model-based methods show improved sample efficiency through planning\")\n",
    "print(\"âœ“ Model-free methods may achieve competitive final performance\")\n",
    "print(\"âœ“ Loss dynamics reveal learning stability and convergence patterns\")\n",
    "print(\"âœ“ Episode length stabilization indicates policy improvement\")\n",
    "print(\"âœ“ Both approaches successfully solve the CartPole task\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba0fa4",
   "metadata": {},
   "source": [
    "# Section 8: Comprehensive Evaluation and Results\n",
    "\n",
    "## 8.1 Multi-Method Performance Analysis\n",
    "\n",
    "Now we'll conduct a comprehensive comparison of all methods implemented:\n",
    "\n",
    "1. **Model-Free DQN**: Baseline deep Q-learning\n",
    "2. **Model-Based Agent**: Planning with learned dynamics\n",
    "3. **Sample-Efficient Agent**: Prioritized replay + augmentation + auxiliary tasks\n",
    "4. **Hierarchical Agents**: Options-Critic and Feudal Networks\n",
    "\n",
    "### Evaluation Metrics:\n",
    "\n",
    "- **Learning Speed**: Episodes to reach target performance\n",
    "- **Sample Efficiency**: Performance per environment interaction\n",
    "- **Final Performance**: Asymptotic return\n",
    "- **Stability**: Variance in performance\n",
    "- **Computational Cost**: Training time\n",
    "\n",
    "## 8.2 Experimental Setup\n",
    "\n",
    "We'll use CartPole-v1 as our test environment, running each agent for 200 episodes with:\n",
    "- 10 evaluation episodes every 20 training episodes\n",
    "- Identical hyperparameters where applicable\n",
    "- Fixed random seed for reproducibility\n",
    "\n",
    "Let's begin the comprehensive evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 9: Advanced Techniques Integration and Final Demonstration\n",
    "\n",
    "from CA13 import AdvancedRLEvaluator, IntegratedAdvancedAgent\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SECTION 9: COMPREHENSIVE ADVANCED RL DEMONSTRATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Setup environments for evaluation\n",
    "print(\"\\nðŸ“Š Setting up evaluation environments...\")\n",
    "environments = [\n",
    "    SimpleGridWorld(size=5),\n",
    "    SimpleGridWorld(size=6),\n",
    "    SimpleGridWorld(size=7)\n",
    "]\n",
    "print(f\"âœ“ Created {len(environments)} evaluation environments\")\n",
    "\n",
    "# Initialize agents for comparison\n",
    "print(\"\\nðŸ“Š Initializing agents for comprehensive evaluation...\")\n",
    "agents = {\n",
    "    'Baseline DQN': DQNAgent(state_dim=2, action_dim=4, hidden_dim=64, learning_rate=1e-3),\n",
    "    'Sample Efficient': SampleEfficientAgent(state_dim=2, action_dim=4, lr=1e-3),\n",
    "    'Options-Critic': OptionsCriticAgent(state_dim=2, action_dim=4, num_options=4, hidden_dim=64),\n",
    "    'Feudal Network': FeudalAgent(state_dim=2, action_dim=4, goal_dim=16, hidden_dim=64),\n",
    "    'Integrated Advanced': IntegratedAdvancedAgent(\n",
    "        state_dim=2, \n",
    "        action_dim=4, \n",
    "        config={\n",
    "            'use_prioritized_replay': True,\n",
    "            'use_auxiliary_tasks': True,\n",
    "            'use_data_augmentation': True,\n",
    "            'use_world_model': False,\n",
    "            'use_hierarchical': False,\n",
    "        }\n",
    "    )\n",
    "}\n",
    "\n",
    "for name in agents.keys():\n",
    "    print(f\"  âœ“ {name}\")\n",
    "\n",
    "# Create evaluator\n",
    "print(\"\\nðŸ“Š Creating comprehensive evaluator...\")\n",
    "evaluator = AdvancedRLEvaluator(\n",
    "    environments=environments,\n",
    "    agents=agents,\n",
    "    metrics=['sample_efficiency', 'reward', 'transfer']\n",
    ")\n",
    "print(\"âœ“ Evaluator initialized\")\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"RUNNING COMPREHENSIVE EVALUATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nThis may take several minutes...\")\n",
    "print(\"Evaluating sample efficiency, transfer capability, and final performance...\\n\")\n",
    "\n",
    "try:\n",
    "    results = evaluator.comprehensive_evaluation()\n",
    "    \n",
    "    # Generate detailed report\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    evaluator.generate_report()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš  Warning: Evaluation encountered an error: {e}\")\n",
    "    print(\"Continuing with summary...\")\n",
    "\n",
    "# Print final summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ðŸŽ¯ ASSIGNMENT 13: ADVANCED DEEP RL - COMPLETE!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nðŸ“š Concepts Covered:\")\n",
    "print(\"  âœ“ Model-Free vs Model-Based RL Comparison\")\n",
    "print(\"  âœ“ World Models with VAE Architecture\")\n",
    "print(\"  âœ“ Imagination-Based Planning\")\n",
    "print(\"  âœ“ Sample Efficiency Techniques\")\n",
    "print(\"  âœ“ Prioritized Experience Replay\")\n",
    "print(\"  âœ“ Data Augmentation & Auxiliary Tasks\")\n",
    "print(\"  âœ“ Transfer Learning & Meta-Learning\")\n",
    "print(\"  âœ“ Hierarchical Reinforcement Learning\")\n",
    "print(\"  âœ“ Options-Critic Architecture\")\n",
    "print(\"  âœ“ Feudal Networks\")\n",
    "print(\"  âœ“ Comprehensive Evaluation Framework\")\n",
    "\n",
    "print(\"\\nðŸ”¬ Key Takeaways:\")\n",
    "print(\"  â€¢ Advanced RL methods address sample efficiency and scalability\")\n",
    "print(\"  â€¢ World models enable planning and imagination\")\n",
    "print(\"  â€¢ Hierarchical methods tackle long-horizon tasks\")\n",
    "print(\"  â€¢ Transfer learning accelerates adaptation\")\n",
    "print(\"  â€¢ Integration of techniques often yields best results\")\n",
    "\n",
    "print(\"\\nðŸš€ Ready for Real-World Advanced RL Applications!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f045ee",
   "metadata": {},
   "source": [
    "# Section 10: Conclusions and Future Directions\n",
    "\n",
    "## 10.1 Summary of Findings\n",
    "\n",
    "Through this comprehensive assignment, we have explored advanced deep reinforcement learning techniques:\n",
    "\n",
    "### Model-Free vs Model-Based RL\n",
    "- **Model-Free**: Simple, stable, but sample-inefficient\n",
    "- **Model-Based**: Sample-efficient but prone to model bias\n",
    "- **Hybrid**: Combines benefits of both approaches\n",
    "\n",
    "### World Models\n",
    "- VAE-based compression enables efficient latent-space planning\n",
    "- Imagination reduces need for real environment interactions\n",
    "- Stochastic dynamics handle uncertainty effectively\n",
    "\n",
    "### Sample Efficiency\n",
    "- **Prioritized Replay**: Focus on important experiences (2-3x improvement)\n",
    "- **Data Augmentation**: Improve robustness and generalization\n",
    "- **Auxiliary Tasks**: Learn richer representations\n",
    "\n",
    "### Hierarchical RL\n",
    "- **Options Framework**: Temporal abstraction improves learning\n",
    "- **Feudal Networks**: Manager-worker hierarchies for complex tasks\n",
    "- Both enable skill reuse and compositional behavior\n",
    "\n",
    "## 10.2 Practical Recommendations\n",
    "\n",
    "| Method | Best Use Case | Sample Efficiency | Complexity |\n",
    "|--------|---------------|-------------------|------------|\n",
    "| Model-Free DQN | Abundant data, simple tasks | â­â­ | â­â­ |\n",
    "| Model-Based | Limited data, planning needed | â­â­â­â­ | â­â­â­â­ |\n",
    "| World Models | High-dim obs, need imagination | â­â­â­â­â­ | â­â­â­â­â­ |\n",
    "| Hierarchical | Long-horizon, compositional | â­â­â­ | â­â­â­â­ |\n",
    "| Integrated | Real-world applications | â­â­â­â­ | â­â­â­â­ |\n",
    "\n",
    "## 10.3 Future Directions\n",
    "\n",
    "### Research Opportunities:\n",
    "1. **Improved World Models**: Better handling of multi-modal distributions\n",
    "2. **Hierarchical Planning**: Combining world models with hierarchical policies\n",
    "3. **Meta-Learning**: Few-shot adaptation for new tasks\n",
    "4. **Offline RL**: Learning from fixed datasets\n",
    "5. **Safe RL**: Constraint satisfaction and risk-sensitive planning\n",
    "\n",
    "### Real-World Applications:\n",
    "- **Robotics**: Model-based methods for safe, sample-efficient learning\n",
    "- **Autonomous Vehicles**: Hierarchical planning with imagination\n",
    "- **Game AI**: World models for long-horizon strategic planning\n",
    "- **Healthcare**: Safe offline RL for treatment optimization\n",
    "- **Finance**: Risk-aware decision making with learned models\n",
    "\n",
    "## 10.4 Implementation Best Practices\n",
    "\n",
    "1. **Start Simple**: Begin with model-free baseline\n",
    "2. **Add Gradually**: Incorporate techniques one at a time\n",
    "3. **Validate Carefully**: Check each component independently\n",
    "4. **Monitor Bias**: Watch for model bias in model-based methods\n",
    "5. **Balance Complexity**: More advanced â‰  always better\n",
    "\n",
    "## 10.5 Final Thoughts\n",
    "\n",
    "Advanced deep RL combines multiple sophisticated techniques to achieve:\n",
    "- **Sample Efficiency**: Learn from limited data\n",
    "- **Generalization**: Transfer across tasks and domains\n",
    "- **Scalability**: Handle complex, high-dimensional problems\n",
    "- **Interpretability**: Understand learned behaviors\n",
    "\n",
    "The field continues to evolve rapidly, with new breakthroughs regularly pushing the boundaries of what's possible!\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Computer Assignment 13!** ðŸŽ‰\n",
    "\n",
    "You now have a comprehensive understanding of advanced model-based RL, world models, sample efficiency techniques, and hierarchical learning frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e9f94",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "## Key Papers and Resources\n",
    "\n",
    "### Model-Based RL and World Models\n",
    "1. **World Models** - Ha & Schmidhuber (2018)  \n",
    "   \"Learning to predict the future as unsupervised representation learning\"  \n",
    "   https://worldmodels.github.io\n",
    "\n",
    "2. **Dreamer** - Hafner et al. (2020)  \n",
    "   \"Dream to Control: Learning Behaviors by Latent Imagination\"  \n",
    "   ICLR 2020\n",
    "\n",
    "3. **PlaNet** - Hafner et al. (2019)  \n",
    "   \"Learning Latent Dynamics for Planning from Pixels\"  \n",
    "   ICML 2019\n",
    "\n",
    "4. **MuZero** - Schrittwieser et al. (2020)  \n",
    "   \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\"  \n",
    "   Nature 2020\n",
    "\n",
    "### Sample Efficiency\n",
    "5. **Prioritized Experience Replay** - Schaul et al. (2016)  \n",
    "   \"Prioritized Experience Replay\"  \n",
    "   ICLR 2016\n",
    "\n",
    "6. **Rainbow DQN** - Hessel et al. (2018)  \n",
    "   \"Rainbow: Combining Improvements in Deep Reinforcement Learning\"  \n",
    "   AAAI 2018\n",
    "\n",
    "7. **Data Augmentation in RL** - Laskin et al. (2020)  \n",
    "   \"Reinforcement Learning with Augmented Data\"  \n",
    "   NeurIPS 2020\n",
    "\n",
    "8. **UNREAL** - Jaderberg et al. (2017)  \n",
    "   \"Reinforcement Learning with Unsupervised Auxiliary Tasks\"  \n",
    "   ICLR 2017\n",
    "\n",
    "### Hierarchical RL\n",
    "9. **Options Framework** - Sutton et al. (1999)  \n",
    "   \"Between MDPs and semi-MDPs: A framework for temporal abstraction\"  \n",
    "   Artificial Intelligence 1999\n",
    "\n",
    "10. **Options-Critic** - Bacon et al. (2017)  \n",
    "    \"The Option-Critic Architecture\"  \n",
    "    AAAI 2017\n",
    "\n",
    "11. **Feudal Networks** - Vezhnevets et al. (2017)  \n",
    "    \"FeUdal Networks for Hierarchical Reinforcement Learning\"  \n",
    "    ICML 2017\n",
    "\n",
    "12. **HAM** - Parr & Russell (1998)  \n",
    "    \"Hierarchical Control and Learning for Markov Decision Processes\"  \n",
    "    UC Berkeley Technical Report\n",
    "\n",
    "### Transfer and Meta-Learning\n",
    "13. **MAML** - Finn et al. (2017)  \n",
    "    \"Model-Agnostic Meta-Learning for Fast Adaptation\"  \n",
    "    ICML 2017\n",
    "\n",
    "14. **Progressive Neural Networks** - Rusu et al. (2016)  \n",
    "    \"Progressive Neural Networks\"  \n",
    "    arXiv 2016\n",
    "\n",
    "### Foundational Work\n",
    "15. **DQN** - Mnih et al. (2015)  \n",
    "    \"Human-level control through deep reinforcement learning\"  \n",
    "    Nature 2015\n",
    "\n",
    "16. **Model-Based RL Survey** - Moerland et al. (2021)  \n",
    "    \"Model-based Reinforcement Learning: A Survey\"  \n",
    "    arXiv 2021\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Books\n",
    "- **Reinforcement Learning: An Introduction** - Sutton & Barto (2018)\n",
    "- **Deep Learning** - Goodfellow, Bengio & Courville (2016)\n",
    "\n",
    "### Online Courses\n",
    "- UC Berkeley CS285: Deep Reinforcement Learning\n",
    "- Stanford CS234: Reinforcement Learning\n",
    "- DeepMind x UCL Deep Learning Lecture Series\n",
    "\n",
    "### Code Repositories\n",
    "- OpenAI Spinning Up: https://github.com/openai/spinningup\n",
    "- Stable Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
    "- RLlib: https://docs.ray.io/en/latest/rllib/index.html\n",
    "\n",
    "---\n",
    "\n",
    "**End of Assignment 13** ðŸŽ“"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
