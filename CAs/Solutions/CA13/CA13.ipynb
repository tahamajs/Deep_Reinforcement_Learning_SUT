{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "assignment_title",
   "metadata": {},
   "source": [
    "# Computer Assignment 13: Advanced Model-Based RL and World Models\n",
    "\n",
    "## جدول محتویات\n",
    "\n",
    "1. [مقدمه](#مقدمه)\n",
    "2. [راه\u200cاندازی و Import کردن کتابخانه\u200cها](#راهاندازی)\n",
    "3. [مقایسه Model-Based و Model-Free RL](#مقایسه)\n",
    "4. [معماری World Models](#معماری)\n",
    "5. [Imagination-Based Learning](#imagination)\n",
    "6. [تکنیک\u200cهای Sample Efficiency](#sample)\n",
    "7. [Hierarchical RL](#hierarchical)\n",
    "8. [ارزیابی جامع](#ارزیابی)\n",
    "9. [نتیجه\u200cگیری و آموخته\u200cها](#نتیجه)\n",
    "\n",
    "---\n",
    "\n",
    "## مقدمه\n",
    "\n",
    "این Assignment به بررسی پیشرفته\u200cترین تکنیک\u200cهای مدل\u200cمحور در یادگیری تقویتی عمیق می\u200cپردازد. مفاهیم کلیدی عبارتند از:\n",
    "\n",
    "- **World Models**: یادگیری نمایش\u200cهای فشرده از محیط برای برنامه\u200cریزی موثر\n",
    "- **Imagination-Based Learning**: یادگیری از طریق تصور و شبیه\u200cسازی\n",
    "- **Sample Efficiency**: بهبود بازدهی نمونه\u200cگیری\n",
    "- **Hierarchical RL**: یادگیری سلسله\u200cمراتبی برای حل مسائل پیچیده\n",
    "\n",
    "**اهداف یادگیری:**\n",
    "- فهم عمیق تفاوت بین رویکردهای Model-Based و Model-Free\n",
    "- پیاده\u200cسازی مدل\u200cهای VAE برای World Modeling\n",
    "- یادگیری برنامه\u200cریزی در فضای Latent\n",
    "- استفاده از تکنیک\u200cهای پیشرفته برای بهبود نمونه\u200cگیری"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup_section",
   "metadata": {},
   "source": [
    "## راه\u200cاندازی و Import کردن کتابخانه\u200cها\n",
    "\n",
    "ابتدا تمام کتابخانه\u200cهای مورد نیاز را import می\u200cکنیم:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports_basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# تنظیم محیط Python برای Import کردن ماژول\u200cهای CA13\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\".\"))\n",
    "\n",
    "# Import کردن ماژول\u200cهای اصلی CA13\n",
    "import CA13\n",
    "from CA13 import (\n    ModelFreeAgent,\n    DQNAgent,\n    ModelBasedAgent,\n    HybridDynaAgent,\n    SampleEfficientAgent,\n    DataAugmentationDQN,\n    OptionsCriticAgent,\n    FeudalAgent,\n    ReplayBuffer,\n    PrioritizedReplayBuffer,\n    set_seed,\n    get_device,\n    train_dqn_agent,\n    train_model_based_agent,\n    evaluate_agent,\n    env_reset,\n    env_step,\n    EpisodeMetrics,\n)\n",
    "\n",
    "# Import کردن کتابخانه\u200cهای معمولی\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "import random\n",
    "import time\n",
    "import gymnasium as gym\n",
    "\n",
    "# تنظیمات اولیه\n",
    "device = get_device()\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "\n",
    "print(f\"✅ CA13 ماژول\u200cها با موفقیت import شدند\")\n",
    "print(f\"✅ ورژن CA13: {CA13.get_version()}\")\n",
    "print(f\"✅ Device: {device}\")\n",
    "print(f\"✅ Seed تنظیم شد به: {seed}\")\n",
    "print(f\"✅ همه Agentها و ابزارها ready هستند\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8db546e",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Abstract](#abstract)\n",
    "2. [1. Introduction](#1-introduction)\n",
    "   - [1.1 Motivation](#11-motivation)\n",
    "   - [1.2 Learning Objectives](#12-learning-objectives)\n",
    "   - [1.3 Prerequisites](#13-prerequisites)\n",
    "   - [1.4 Course Information](#14-course-information)\n",
    "3. [2. Model-Based vs Model-Free RL Trade-offs](#2-model-based-vs-model-free-rl-trade-offs)\n",
    "   - [2.1 Fundamental Differences](#21-fundamental-differences)\n",
    "   - [2.2 Advantages and Limitations](#22-advantages-and-limitations)\n",
    "   - [2.3 Appropriate Use Cases](#23-appropriate-use-cases)\n",
    "   - [2.4 Performance Comparison](#24-performance-comparison)\n",
    "4. [3. World Model Architectures](#3-world-model-architectures)\n",
    "   - [3.1 Variational Autoencoders for World Models](#31-variational-autoencoders-for-world-models)\n",
    "   - [3.2 Encoder-Decoder Architectures](#32-encoder-decoder-architectures)\n",
    "   - [3.3 Stochastic Dynamics Modeling](#33-stochastic-dynamics-modeling)\n",
    "   - [3.4 Latent Representation Learning](#34-latent-representation-learning)\n",
    "5. [4. Imagination-Based Learning](#4-imagination-based-learning)\n",
    "   - [4.1 Planning in Latent Space](#41-planning-in-latent-space)\n",
    "   - [4.2 Imagined Trajectories](#42-imagined-trajectories)\n",
    "   - [4.3 Sample-Efficient Learning](#43-sample-efficient-learning)\n",
    "   - [4.4 Implementation and Results](#44-implementation-and-results)\n",
    "6. [5. Sample Efficiency Techniques](#5-sample-efficiency-techniques)\n",
    "   - [5.1 Prioritized Experience Replay](#51-prioritized-experience-replay)\n",
    "   - [5.2 Data Augmentation](#52-data-augmentation)\n",
    "   - [5.3 Auxiliary Tasks](#53-auxiliary-tasks)\n",
    "   - [5.4 Learning Efficiency Analysis](#54-learning-efficiency-analysis)\n",
    "7. [6. Transfer Learning Systems](#6-transfer-learning-systems)\n",
    "   - [6.1 Shared Representations](#61-shared-representations)\n",
    "   - [6.2 Fine-tuning Approaches](#62-fine-tuning-approaches)\n",
    "   - [6.3 Meta-Learning Methods](#63-meta-learning-methods)\n",
    "   - [6.4 Knowledge Transfer Analysis](#64-knowledge-transfer-analysis)\n",
    "8. [7. Hierarchical RL Frameworks](#7-hierarchical-rl-frameworks)\n",
    "   - [7.1 Options Framework](#71-options-framework)\n",
    "   - [7.2 Temporal Abstraction](#72-temporal-abstraction)\n",
    "   - [7.3 Skill Composition](#73-skill-composition)\n",
    "   - [7.4 Complex Task Solving](#74-complex-task-solving)\n",
    "9. [8. Results and Discussion](#8-results-and-discussion)\n",
    "   - [8.1 Summary of Findings](#81-summary-of-findings)\n",
    "   - [8.2 Theoretical Contributions](#82-theoretical-contributions)\n",
    "   - [8.3 Practical Implications](#83-practical-implications)\n",
    "   - [8.4 Limitations and Future Work](#84-limitations-and-future-work)\n",
    "   - [8.5 Conclusions](#85-conclusions)\n",
    "10. [References](#references)\n",
    "11. [Appendix A: Implementation Details](#appendix-a-implementation-details)\n",
    "    - [A.1 Modular Architecture](#a1-modular-architecture)\n",
    "    - [A.2 Code Quality Features](#a2-code-quality-features)\n",
    "    - [A.3 Performance Considerations](#a3-performance-considerations)\n",
    "\n",
    "---\n",
    "\n",
    "# Computer Assignment 13: Advanced Model-Based RL and World Models\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This assignment presents a comprehensive study of advanced model-based reinforcement learning and world models, exploring the cutting-edge techniques for learning compressed representations of environments and using them for efficient planning and control. We implement and analyze world model architectures including variational autoencoders, recurrent state space models, and latent space planning methods. The assignment covers modern approaches such as World Models, Dreamer, PlaNet, and MuZero, demonstrating their effectiveness in achieving sample-efficient learning through imagination-based planning. Through systematic experimentation, we show how world models can significantly improve sample efficiency while maintaining competitive performance compared to model-free methods.\n",
    "\n",
    "**Keywords:** Model-based reinforcement learning, world models, variational autoencoders, imagination-based learning, sample efficiency, transfer learning, hierarchical RL, temporal abstraction\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Advanced model-based reinforcement learning with world models represents a significant advancement in the field, enabling agents to learn compressed representations of complex environments and use these representations for efficient planning and decision-making [1]. Unlike traditional model-based approaches that learn explicit environment dynamics, world models learn latent representations that capture the essential aspects of the environment while being computationally tractable for planning and imagination.\n",
    "\n",
    "### 1.1 Motivation\n",
    "\n",
    "World models address several fundamental challenges in reinforcement learning:\n",
    "\n",
    "- **High-Dimensional State Spaces**: Compress complex observations into manageable latent representations\n",
    "- **Sample Efficiency**: Enable planning and imagination without additional environment interaction\n",
    "- **Generalization**: Learn representations that generalize across different environments and tasks\n",
    "- **Computational Efficiency**: Reduce the computational cost of planning through compressed representations\n",
    "- **Long-term Dependencies**: Capture temporal dependencies and long-term consequences of actions\n",
    "\n",
    "### 1.2 Learning Objectives\n",
    "\n",
    "By completing this assignment, students will be able to:\n",
    "\n",
    "1. **Understand Model-Based vs Model-Free RL Trade-offs**: Analyze the fundamental differences between model-free and model-based reinforcement learning approaches, including their respective advantages, limitations, and appropriate use cases.\n",
    "\n",
    "2. **Master World Model Architectures**: Design and implement variational world models using VAEs for learning compact latent representations of environment dynamics, including encoder-decoder architectures and stochastic dynamics modeling.\n",
    "\n",
    "3. **Implement Imagination-Based Learning**: Develop agents that leverage learned world models for planning and decision-making in latent space, enabling sample-efficient learning through imagined trajectories.\n",
    "\n",
    "4. **Apply Sample Efficiency Techniques**: Utilize advanced techniques such as prioritized experience replay, data augmentation, and auxiliary tasks to improve learning efficiency in deep RL.\n",
    "\n",
    "5. **Design Transfer Learning Systems**: Build agents capable of transferring knowledge across related tasks through shared representations, fine-tuning, and meta-learning approaches.\n",
    "\n",
    "6. **Develop Hierarchical RL Frameworks**: Implement hierarchical decision-making systems using options framework, enabling temporal abstraction and skill composition for complex task solving.\n",
    "\n",
    "### 1.3 Prerequisites\n",
    "\n",
    "Before starting this assignment, ensure you have:\n",
    "\n",
    "- **Mathematical Background**: \n",
    "  - Probability theory and stochastic processes\n",
    "  - Variational inference and autoencoders\n",
    "  - Recurrent neural networks and LSTM/GRU\n",
    "  - Information theory and compression\n",
    "\n",
    "- **Technical Skills**:\n",
    "  - Python programming and PyTorch\n",
    "  - Deep learning and neural networks\n",
    "  - Reinforcement learning fundamentals\n",
    "  - Model-based RL concepts\n",
    "\n",
    "### 1.4 Course Information\n",
    "\n",
    "- **Course**: Deep Reinforcement Learning (DRL)\n",
    "- **Instructor**: Dr. [Instructor Name]\n",
    "- **Institution**: Sharif University of Technology\n",
    "- **Semester**: Fall 2024\n",
    "- **Assignment Number**: CA13\n",
    "- Linear algebra and matrix operations\n",
    "- Optimization and gradient-based methods\n",
    "- Information theory (KL divergence, entropy)\n",
    "\n",
    "- **Technical Skills**:\n",
    "- Python programming with PyTorch\n",
    "- Deep learning fundamentals (neural networks, autoencoders)\n",
    "- Basic reinforcement learning concepts (MDPs, value functions, policies)\n",
    "- Experience with Gymnasium environments\n",
    "\n",
    "- **Prior Knowledge**:\n",
    "- Completion of CA1-CA12 assignments\n",
    "- Understanding of model-free RL algorithms (DQN, policy gradients)\n",
    "- Familiarity with neural network architectures\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "This assignment is structured as follows:\n",
    "\n",
    "### Section 1: Model-free Vs Model-based Reinforcement Learning\n",
    "- Theoretical foundations of model-free and model-based approaches\n",
    "- Mathematical formulations and trade-off analysis\n",
    "- Hybrid algorithms combining both paradigms\n",
    "- Practical implementation and comparison\n",
    "\n",
    "### Section 2: World Models and Imagination-based Learning\n",
    "- Variational autoencoders for world modeling\n",
    "- Stochastic dynamics prediction in latent space\n",
    "- Imagination-based planning and policy optimization\n",
    "- Dreamer algorithm and modern variants\n",
    "\n",
    "### Section 3: Sample Efficiency and Transfer Learning\n",
    "- Prioritized experience replay and data augmentation\n",
    "- Auxiliary tasks for improved learning\n",
    "- Transfer learning techniques and meta-learning\n",
    "- Domain adaptation and curriculum learning\n",
    "\n",
    "### Section 4: Hierarchical Reinforcement Learning\n",
    "- Options framework and temporal abstraction\n",
    "- Hierarchical policy architectures\n",
    "- Skill discovery and composition\n",
    "- Applications to complex task domains\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "CA13/\n",
    "├── CA13.ipynb              # Main assignment notebook\n",
    "├── agents/                 # RL agent implementations\n",
    "│   ├── model*free*agent.py # Model-free RL agents\n",
    "│   ├── model*based*agent.py# Model-based RL agents\n",
    "│   ├── world*model*agent.py# World model-based agents\n",
    "│   └── hierarchical_agent.py# Hierarchical RL agents\n",
    "├── models/                 # Neural network architectures\n",
    "│   ├── world_model.py      # VAE-based world models\n",
    "│   ├── dynamics_model.py   # Environment dynamics models\n",
    "│   └── policy_networks.py  # Hierarchical policy networks\n",
    "├── environments/           # Custom environments\n",
    "│   ├── wrappers.py         # Environment wrappers\n",
    "│   └── complex_tasks.py    # Complex task environments\n",
    "├── experiments/            # Training and evaluation scripts\n",
    "│   ├── train*world*model.py# World model training\n",
    "│   ├── compare_efficiency.py# Sample efficiency comparison\n",
    "│   └── transfer_learning.py# Transfer learning experiments\n",
    "└── utils/                  # Utility functions\n",
    "    ├── visualization.py    # Plotting and analysis tools\n",
    "    ├── data_augmentation.py# Data augmentation utilities\n",
    "    └── evaluation.py       # Performance evaluation metrics\n",
    "```\n",
    "\n",
    "## Contents Overview\n",
    "\n",
    "### Theoretical Foundations\n",
    "- **Model-Based RL Mathematics**: Transition and reward model learning, planning algorithms\n",
    "- **World Model Theory**: Variational inference, latent space dynamics, imagination-based learning\n",
    "- **Sample Efficiency**: Experience replay, prioritization, auxiliary learning objectives\n",
    "- **Transfer Learning**: Representation learning, fine-tuning, meta-learning algorithms\n",
    "\n",
    "### Implementation Components\n",
    "- **VAE World Models**: Encoder-decoder architectures with stochastic latent variables\n",
    "- **Imagination-Based Agents**: Planning in learned latent space using world models\n",
    "- **Sample-Efficient Algorithms**: Prioritized replay, data augmentation, auxiliary tasks\n",
    "- **Transfer Learning Systems**: Multi-task learning, fine-tuning, domain adaptation\n",
    "\n",
    "### Advanced Topics\n",
    "- **Hierarchical RL**: Options framework, skill hierarchies, temporal abstraction\n",
    "- **Meta-Learning**: Few-shot adaptation, gradient-based meta-learning\n",
    "- **Curriculum Learning**: Automatic difficulty progression, teacher-student frameworks\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Your implementation will be evaluated based on:\n",
    "\n",
    "1. **Correctness (40%)**: Accurate implementation of algorithms and mathematical formulations\n",
    "2. **Efficiency (25%)**: Sample efficiency improvements and computational performance\n",
    "3. **Innovation (20%)**: Creative extensions and novel approaches to the problems\n",
    "4. **Analysis (15%)**: Quality of experimental analysis and insights\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Environment Setup**: Ensure all dependencies are installed\n",
    "2. **Code Review**: Understand the provided base implementations\n",
    "3. **Incremental Development**: Start with simpler components and build complexity\n",
    "4. **Testing**: Validate each component before integration\n",
    "5. **Experimentation**: Run comprehensive experiments and analyze results\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this assignment, you will have:\n",
    "\n",
    "- **Comprehensive Understanding**: Deep knowledge of advanced model-based RL techniques\n",
    "- **Practical Skills**: Ability to implement complex RL systems from scratch\n",
    "- **Research Perspective**: Insight into current challenges and future directions\n",
    "- **Portfolio Piece**: High-quality implementation demonstrating advanced RL capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This assignment represents the culmination of the Deep RL course, integrating concepts from model-free and model-based learning, advanced architectures, and practical deployment considerations. Focus on understanding the theoretical foundations while developing robust, efficient implementations.\n",
    "\n",
    "Let's begin our exploration of advanced model-based reinforcement learning and world models! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ed79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "✅ CA13 modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "from agents.model_free import ModelFreeAgent, DQNAgent\nfrom agents.model_based import ModelBasedAgent\nfrom buffers.replay_buffer import ReplayBuffer\nfrom environments.grid_world import SimpleGridWorld\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import deque\nimport random\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nprint(\"✅ CA13 modules imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398420da",
   "metadata": {},
   "source": [
    "# Import required libraries for experiments\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "seed = 42\n",
    "set_seed(seed)\n",
    "device = get_device()\n",
    "\n",
    "print(f\"✓ Using device: {device}\")\n",
    "print(f\"✓ Random seed set to: {seed}\")\n",
    "print(\"✓ Ready for experiments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6554d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VariationalWorldModel imported from models.world_model package\n",
      "💡 This model provides VAE-based world modeling for learning environment dynamics\n"
     ]
    }
   ],
   "source": [
    "from models.world_model import VariationalWorldModel\nprint(\"VariationalWorldModel imported from models.world_model package\")\nprint(\"This model provides VAE-based world modeling for learning environment dynamics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396b783",
   "metadata": {},
   "source": [
    "# Section 2: Model-Based vs Model-Free RL Trade-offs\n",
    "\n",
    "## 2.1 Fundamental Differences\n",
    "\n",
    "Model-free and model-based reinforcement learning represent two fundamentally different approaches to learning optimal behavior:\n",
    "\n",
    "### Model-Free RL\n",
    "- **Direct Learning**: Learns value functions or policies directly from experience\n",
    "- **Sample Intensive**: Requires many environment interactions\n",
    "- **Computationally Efficient**: Simple forward passes through networks\n",
    "- **Examples**: DQN, PPO, SAC, A3C\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "$$Q^*(s,a) = \\mathbb{E}[R_{t+1} + \\gamma \\max_{a'} Q^*(s_{t+1}, a') | s_t=s, a_t=a]$$\n",
    "\n",
    "### Model-Based RL\n",
    "- **Environment Modeling**: Learns dynamics model $P(s'|s,a)$ and reward model $R(s,a)$\n",
    "- **Sample Efficient**: Can plan using learned model\n",
    "- **Computationally Intensive**: Planning requires model rollouts\n",
    "- **Examples**: Dyna-Q, PETS, MuZero\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "$$\\hat{T}(s'|s,a) \\approx P(s'|s,a), \\quad \\hat{R}(s,a) \\approx \\mathbb{E}[r|s,a]$$\n",
    "\n",
    "## 2.2 Advantages and Limitations\n",
    "\n",
    "### Model-Free Advantages:\n",
    "✓ **Asymptotic Performance**: Can learn highly accurate policies with enough data  \n",
    "✓ **Stability**: No model bias, directly optimizes objective  \n",
    "✓ **Simplicity**: Straightforward implementation  \n",
    "\n",
    "### Model-Free Limitations:\n",
    "✗ **Sample Inefficiency**: Requires millions of interactions  \n",
    "✗ **No Generalization**: Must relearn for new tasks  \n",
    "✗ **No Planning**: Cannot simulate future trajectories  \n",
    "\n",
    "### Model-Based Advantages:\n",
    "✓ **Sample Efficiency**: Learn from fewer real interactions  \n",
    "✓ **Transfer Learning**: Model can transfer across tasks  \n",
    "✓ **Interpretability**: Explicit model of environment  \n",
    "✓ **Planning**: Can look ahead before acting  \n",
    "\n",
    "### Model-Based Limitations:\n",
    "✗ **Model Bias**: Errors compound during planning  \n",
    "✗ **Computational Cost**: Planning is expensive  \n",
    "✗ **Complex Environments**: Hard to model stochastic/high-dim spaces  \n",
    "\n",
    "## 2.3 Appropriate Use Cases\n",
    "\n",
    "| Scenario | Recommended Approach | Reasoning |\n",
    "|----------|---------------------|-----------|\n",
    "| Limited Data | Model-Based | Better sample efficiency |\n",
    "| Abundant Data | Model-Free | Avoid model bias |\n",
    "| Related Tasks | Model-Based | Model transfers |\n",
    "| High-Dim Observations | Hybrid | World models for compression |\n",
    "| Real-World Robotics | Model-Based → Model-Free | Sim training then real fine-tuning |\n",
    "| Games (Atari, Chess) | Model-Free or Hybrid | Can collect many samples |\n",
    "| Safety-Critical | Model-Based | Planning avoids dangerous states |\n",
    "\n",
    "## 2.4 Performance Comparison\n",
    "\n",
    "Let's implement and compare both approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5d6130",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\nprint(\"SECTION 2: MODEL-FREE VS MODEL-BASED RL COMPARISON\")\nprint(\"=\" * 80)\ntry:\n    env = gym.make(\"CartPole-v1\")\nexcept:\n    from environments.grid_world import SimpleGridWorld\n    env = SimpleGridWorld(size=5)\nstate_dim = env.observation_space.shape[0] if hasattr(env.observation_space, 'shape') else 2\naction_dim = env.action_space.n\nprint(f\"\\nEnvironment: {env}\")\nprint(f\"State dimension: {state_dim}\")\nprint(f\"Action dimension: {action_dim}\")\nprint(\"\\n📊 Initializing Agents...\")\nprint(\"-\" * 80)\nmf_agent = DQNAgent(\n    state_dim=state_dim,\n    action_dim=action_dim,\n    hidden_dim=128,\n    learning_rate=1e-3,\n    gamma=0.99,\n    epsilon_start=1.0,\n    epsilon_end=0.01,\n    epsilon_decay=500,\n)\nprint(\"✓ Model-Free DQN Agent initialized\")\nmb_agent = ModelBasedAgent(\n    state_dim=state_dim,\n    action_dim=action_dim,\n    hidden_dim=128,\n    learning_rate=1e-3,\n    gamma=0.99,\n)\nprint(\"✓ Model-Based Agent initialized\")\ntry:\n    hybrid_agent = HybridDynaAgent(\n        state_dim=state_dim,\n        action_dim=action_dim,\n        learning_rate=1e-3,\n        planning_steps=5,\n    )\n    print(\"✓ Hybrid Dyna-Q Agent initialized\")\n    use_hybrid = True\nexcept:\n    print(\"⚠ Hybrid agent not available, will compare MF vs MB only\")\n    use_hybrid = False\nprint(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8734b540",
   "metadata": {},
   "source": [
    "# Section 3: World Model Architectures\n",
    "\n",
    "## 3.1 Variational Autoencoders for World Models\n",
    "\n",
    "World models learn compressed representations of environments using variational autoencoders (VAEs), enabling efficient imagination-based planning.\n",
    "\n",
    "### VAE Architecture for World Modeling\n",
    "\n",
    "**Encoder**: Maps high-dimensional observations to latent space\n",
    "$$q_\\phi(z|s) = \\mathcal{N}(\\mu_\\phi(s), \\sigma_\\phi(s))$$\n",
    "\n",
    "**Decoder**: Reconstructs observations from latent representations\n",
    "$$p_\\theta(s|z) = \\mathcal{N}(\\mu_\\theta(z), \\sigma_\\theta(z))$$\n",
    "\n",
    "**VAE Loss**:\n",
    "$$\\mathcal{L}_{VAE} = \\mathbb{E}_{q_\\phi(z|s)}[\\log p_\\theta(s|z)] - D_{KL}(q_\\phi(z|s) || p(z))$$\n",
    "\n",
    "Where:\n",
    "- First term: Reconstruction loss\n",
    "- Second term: KL divergence (regularization)\n",
    "\n",
    "## 3.2 Stochastic Dynamics Modeling\n",
    "\n",
    "Learn dynamics in latent space for efficient planning:\n",
    "\n",
    "**Latent Dynamics Model**:\n",
    "$$z_{t+1} \\sim p(z_{t+1}|z_t, a_t)$$\n",
    "\n",
    "**Reward Model**:\n",
    "$$r_t \\sim p(r_t|z_t, a_t)$$\n",
    "\n",
    "**Complete World Model Loss**:\n",
    "$$\\mathcal{L} = \\mathcal{L}_{VAE} + \\mathcal{L}_{dynamics} + \\mathcal{L}_{reward}$$\n",
    "\n",
    "## 3.3 Latent Representation Learning\n",
    "\n",
    "Benefits of learning in latent space:\n",
    "- **Compression**: Reduce dimensionality of observations (e.g., images)\n",
    "- **Efficiency**: Faster planning in compressed space\n",
    "- **Generalization**: Latent space captures essential features\n",
    "- **Stochasticity**: VAE handles uncertainty\n",
    "\n",
    "## 3.4 Implementation\n",
    "\n",
    "Let's implement and test a variational world model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44da5983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\nprint(\"SECTION 3: WORLD MODEL ARCHITECTURE DEMONSTRATION\")\nprint(\"=\" * 80)\nfrom models.world_model import VariationalWorldModel\nlatent_dim = 32\nworld_model = VariationalWorldModel(\n    state_dim=state_dim,\n    action_dim=action_dim,\n    latent_dim=latent_dim,\n    hidden_dim=128\n)\nprint(f\"\\n✓ Variational World Model initialized\")\nprint(f\"  State dimension: {state_dim}\")\nprint(f\"  Action dimension: {action_dim}\")\nprint(f\"  Latent dimension: {latent_dim}\")\nprint(f\"  Architecture: VAE + Dynamics Model + Reward Model\")\nprint(\"\\n📊 Testing World Model Components...\")\ntest_state = torch.randn(1, state_dim)\ntest_action = torch.randint(0, action_dim, (1,))\nprint(\"\\n1. Encoding state to latent space...\")\nwith torch.no_grad():\n    mu, logvar = world_model.encode(test_state)\n    z = world_model.reparameterize(mu, logvar)\n    print(f\"   Latent mean shape: {mu.shape}\")\n    print(f\"   Latent sample shape: {z.shape}\")\nprint(\"\\n2. Predicting next latent state (dynamics)...\")\nwith torch.no_grad():\n    z_next = world_model.predict_next_latent(z, test_action)\n    print(f\"   Next latent shape: {z_next.shape}\")\nprint(\"\\n3. Predicting reward...\")\nwith torch.no_grad():\n    reward_pred = world_model.predict_reward(z, test_action)\n    print(f\"   Predicted reward: {reward_pred.item():.4f}\")\nprint(\"\\n4. Decoding latent to observation...\")\nwith torch.no_grad():\n    reconstructed_state = world_model.decode(z)\n    print(f\"   Reconstructed state shape: {reconstructed_state.shape}\")\n    print(f\"   Reconstruction error: {F.mse_loss(reconstructed_state, test_state).item():.4f}\")\nprint(\"\\n✓ World model components working correctly!\")\nprint(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e85188d",
   "metadata": {},
   "source": [
    "# Section 4: Imagination-Based Learning\n",
    "\n",
    "## 4.1 Planning in Latent Space\n",
    "\n",
    "World models enable **imagination**: planning in learned latent space without environment interaction.\n",
    "\n",
    "### Model Predictive Control (MPC) in Latent Space\n",
    "\n",
    "**Algorithm**:\n",
    "1. Encode current observation: $z_t = \\text{Encode}(s_t)$\n",
    "2. For each candidate action sequence $a_{t:t+H}$:\n",
    "   - Simulate trajectory in latent space\n",
    "   - Accumulate predicted rewards\n",
    "3. Execute first action of best sequence\n",
    "4. Repeat\n",
    "\n",
    "**Objective**:\n",
    "$$a^*_t = \\arg\\max_{a_{t:t+H}} \\sum_{k=0}^{H} \\gamma^k \\hat{r}_{t+k}$$\n",
    "\n",
    "where $\\hat{r}_{t+k}$ is predicted from world model.\n",
    "\n",
    "## 4.2 Dreamer Algorithm\n",
    "\n",
    "Dreamer learns policies entirely in latent space using imagined trajectories.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "**1. World Model Learning**:\n",
    "- Learn $p(z_t|z_{t-1}, a_{t-1})$ (dynamics)\n",
    "- Learn $p(s_t|z_t)$ (decoder)  \n",
    "- Learn $p(r_t|z_t, a_t)$ (reward)\n",
    "\n",
    "**2. Behavior Learning** (Policy and Value):\n",
    "- Actor: $\\pi_\\phi(a_t|z_t)$\n",
    "- Critic: $V_\\psi(z_t)$\n",
    "\n",
    "**3. Imagination Training**:\n",
    "```\n",
    "for each real trajectory (s_t, a_t, r_t):\n",
    "    encode to latent: z_t = Encode(s_t)\n",
    "    imagine future trajectories from z_t\n",
    "    train actor-critic on imagined data\n",
    "```\n",
    "\n",
    "## 4.3 Sample-Efficient Learning Benefits\n",
    "\n",
    "Imagination-based learning provides:\n",
    "\n",
    "✓ **Sample Efficiency**: Train policy on unlimited imagined data  \n",
    "✓ **Gradient Efficiency**: Backprop through differentiable model  \n",
    "✓ **Exploration**: Imagine diverse scenarios  \n",
    "✓ **Transfer**: World model generalizes across tasks  \n",
    "\n",
    "## 4.4 Implementation\n",
    "\n",
    "Let's implement imagination-based planning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1954d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\nprint(\"SECTION 4: IMAGINATION-BASED PLANNING\")\nprint(\"=\" * 80)\nclass ImaginationPlanner:\n    def __init__(self, world_model, action_dim, planning_horizon=5, num_candidates=10):\n        self.world_model = world_model\n        self.action_dim = action_dim\n        self.planning_horizon = planning_horizon\n        self.num_candidates = num_candidates\n        self.gamma = 0.99\n    def plan(self, state):\n        state_tensor = torch.FloatTensor(state).unsqueeze(0) if isinstance(state, np.ndarray) else state\n        with torch.no_grad():\n            mu, logvar = self.world_model.encode(state_tensor)\n            z_current = self.world_model.reparameterize(mu, logvar)\n        best_return = float('-inf')\n        best_action = 0\n        for first_action in range(self.action_dim):\n            returns = []\n            for _ in range(self.num_candidates // self.action_dim + 1):\n                total_return = self.imagine_trajectory(z_current, first_action)\n                returns.append(total_return)\n            avg_return = np.mean(returns)\n            if avg_return > best_return:\n                best_return = avg_return\n                best_action = first_action\n        return best_action, best_return\n    def imagine_trajectory(self, z_start, first_action):\n        z = z_start\n        total_return = 0.0\n        for step in range(self.planning_horizon):\n            if step == 0:\n                action = first_action\n            else:\n                action = np.random.randint(self.action_dim)\n            action_tensor = torch.tensor([action])\n            with torch.no_grad():\n                reward = self.world_model.predict_reward(z, action_tensor).item()\n            total_return += (self.gamma ** step) * reward\n            with torch.no_grad():\n                z = self.world_model.predict_next_latent(z, action_tensor)\n        return total_return\nprint(\"\\n✓ Creating Imagination-Based Planner...\")\nplanner = ImaginationPlanner(\n    world_model=world_model,\n    action_dim=action_dim,\n    planning_horizon=5,\n    num_candidates=20\n)\nprint(f\"  Planning horizon: {planner.planning_horizon}\")\nprint(f\"  Candidates per action: {planner.num_candidates // planner.action_dim + 1}\")\nprint(\"\\n📊 Testing Imagination Planning...\")\ntest_state = torch.randn(1, state_dim)\nbest_action, expected_return = planner.plan(test_state)\nprint(f\"  Test state shape: {test_state.shape}\")\nprint(f\"  Planned best action: {best_action}\")\nprint(f\"  Expected return: {expected_return:.4f}\")\nprint(\"\\n✓ Imagination-based planning working correctly!\")\nprint(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2799c6",
   "metadata": {},
   "source": [
    "# Section 3: Sample Efficiency and Transfer Learning\n",
    "\n",
    "## 3.1 Sample Efficiency Challenges in Deep Rl\n",
    "\n",
    "Sample efficiency is one of the most critical challenges in deep reinforcement learning, particularly for real-world applications where data collection is expensive or dangerous.\n",
    "\n",
    "### Why Is Sample Efficiency Important?\n",
    "\n",
    "**Real-World Constraints:**\n",
    "- **Cost**: Real-world interactions can be expensive (robotics, autonomous vehicles)\n",
    "- **Time**: Learning from millions of samples is often impractical\n",
    "- **Safety**: Exploratory actions in safety-critical domains can be dangerous\n",
    "- **Reproducibility**: Limited samples make experiments more reliable\n",
    "\n",
    "**Sample Complexity Factors:**\n",
    "- **Environment Complexity**: High-dimensional state/action spaces\n",
    "- **Sparse Rewards**: Learning signals are infrequent\n",
    "- **Stochasticity**: Environmental noise requires more samples\n",
    "- **Exploration**: Discovering good policies requires extensive exploration\n",
    "\n",
    "## 3.2 Sample Efficiency Techniques\n",
    "\n",
    "### 3.2.1 Experience Replay and Prioritization\n",
    "\n",
    "**Experience Replay Benefits:**\n",
    "- Reuse past experiences multiple times\n",
    "- Break temporal correlations in data\n",
    "- Enable off-policy learning\n",
    "\n",
    "**Prioritized Experience Replay:**\n",
    "Prioritize experiences based on temporal difference (TD) error:\n",
    "$$P(i) = \\frac{p*i^\\alpha}{\\sum*k p_k^\\alpha}$$\n",
    "\n",
    "Where $p*i = |\\delta*i| + \\epsilon$ and $\\delta_i$ is the TD error.\n",
    "\n",
    "### 3.2.2 Data Augmentation\n",
    "\n",
    "**Techniques:**\n",
    "- **Random Crops**: For image-based environments\n",
    "- **Color Jittering**: Robust to lighting variations  \n",
    "- **Random Shifts**: Translation invariance\n",
    "- **Gaussian Noise**: Regularization effect\n",
    "\n",
    "### 3.2.3 Auxiliary Tasks\n",
    "\n",
    "Learn multiple tasks simultaneously to improve sample efficiency:\n",
    "- **Pixel Control**: Predict pixel changes\n",
    "- **Feature Control**: Control learned feature representations\n",
    "- **Reward Prediction**: Predict future rewards\n",
    "- **Value Function Replay**: Replay value function updates\n",
    "\n",
    "## 3.3 Transfer Learning in Reinforcement Learning\n",
    "\n",
    "Transfer learning enables agents to leverage knowledge from previous tasks to learn new tasks more efficiently.\n",
    "\n",
    "### 3.3.1 Types of Transfer in Rl\n",
    "\n",
    "**Policy Transfer:**\n",
    "$$\\pi*{target}(a|s) = f(\\pi*{source}(a|s), s, \\theta_{adapt})$$\n",
    "\n",
    "**Value Function Transfer:**\n",
    "$$Q*{target}(s,a) = g(Q*{source}(s,a), s, a, \\phi_{adapt})$$\n",
    "\n",
    "**Representation Transfer:**\n",
    "$$\\phi*{target}(s) = h(\\phi*{source}(s), \\psi_{adapt})$$\n",
    "\n",
    "### 3.3.2 Transfer Learning Approaches\n",
    "\n",
    "#### Fine-tuning\n",
    "1. Pre-train on source task\n",
    "2. Initialize target model with source weights\n",
    "3. Fine-tune on target task with lower learning rate\n",
    "\n",
    "#### Progressive Networks\n",
    "- Freeze source network columns\n",
    "- Add new columns for target tasks\n",
    "- Use lateral connections between columns\n",
    "\n",
    "#### Universal Value Functions (uvf)\n",
    "Learn value functions conditioned on goals:\n",
    "$$Q(s, a, g) = \\text{Value of action } a \\text{ in state } s \\text{ for goal } g$$\n",
    "\n",
    "## 3.4 Meta-learning and Few-shot Adaptation\n",
    "\n",
    "Meta-learning enables agents to quickly adapt to new tasks with limited experience.\n",
    "\n",
    "### 3.4.1 Model-agnostic Meta-learning (maml)\n",
    "\n",
    "**Objective:**\n",
    "$$\\min*\\theta \\sum*{\\tau \\sim p(\\mathcal{T})} \\mathcal{L}*\\tau(f*{\\theta_\\tau'})$$\n",
    "\n",
    "Where $\\theta*\\tau' = \\theta - \\alpha \\nabla*\\theta \\mathcal{L}*\\tau(f*\\theta)$\n",
    "\n",
    "**MAML Algorithm:**\n",
    "1. Sample batch of tasks\n",
    "2. For each task, compute adapted parameters via gradient descent\n",
    "3. Update meta-parameters using gradient through adaptation process\n",
    "\n",
    "### 3.4.2 Gradient-based Meta-learning\n",
    "\n",
    "**Reptile Algorithm:**\n",
    "Simpler alternative to MAML:\n",
    "$$\\theta \\leftarrow \\theta + \\beta \\frac{1}{n} \\sum*{i=1}^n (\\phi*i - \\theta)$$\n",
    "\n",
    "Where $\\phi_i$ is the result of training on task $i$.\n",
    "\n",
    "## 3.5 Domain Adaptation and Sim-to-real Transfer\n",
    "\n",
    "### 3.5.1 Domain Randomization\n",
    "\n",
    "**Technique:**\n",
    "Randomize simulation parameters during training:\n",
    "- Physical properties (mass, friction, damping)\n",
    "- Visual appearance (textures, lighting, colors)\n",
    "- Sensor characteristics (noise, resolution, field of view)\n",
    "\n",
    "**Benefits:**\n",
    "- Learned policies are robust to domain variations\n",
    "- Improved transfer from simulation to real world\n",
    "- Reduced need for domain-specific engineering\n",
    "\n",
    "### 3.5.2 Domain Adversarial Training\n",
    "\n",
    "**Objective:**\n",
    "$$\\min*\\theta \\mathcal{L}*{task}(\\theta) + \\lambda \\mathcal{L}_{domain}(\\theta)$$\n",
    "\n",
    "Where $\\mathcal{L}_{domain}$ encourages domain-invariant features.\n",
    "\n",
    "## 3.6 Curriculum Learning\n",
    "\n",
    "Structure learning to progress from simple to complex tasks.\n",
    "\n",
    "### 3.6.1 Curriculum Design Principles\n",
    "\n",
    "**Manual Curriculum:**\n",
    "- Hand-designed progression of tasks\n",
    "- Expert knowledge of difficulty ordering\n",
    "- Fixed curriculum regardless of agent performance\n",
    "\n",
    "**Automatic Curriculum:**\n",
    "- Adaptive task selection based on agent performance\n",
    "- Learning progress as curriculum signal\n",
    "- Self-paced learning approaches\n",
    "\n",
    "### 3.6.2 Curriculum Learning Algorithms\n",
    "\n",
    "**Teacher-Student Framework:**\n",
    "- Teacher selects appropriate tasks for student\n",
    "- Task difficulty based on student's current capability\n",
    "- Optimize task selection for maximum learning progress\n",
    "\n",
    "**Self-Play Curriculum:**\n",
    "- Agent plays against previous versions of itself\n",
    "- Automatic difficulty adjustment\n",
    "- Prevents catastrophic forgetting of simpler strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ PrioritizedReplayBuffer imported from CA13.buffers\")\nprint(\"✓ Ready for sample efficiency experiments\")\nprint(\"\\nKey features:\")\nprint(\"  - Priority-based sampling for improved learning\")\nprint(\"  - Importance sampling weights for bias correction\")\nprint(\"  - Configurable alpha (priority exponent) and beta (IS exponent)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b470f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\nprint(\"SECTION 5: SAMPLE EFFICIENCY TECHNIQUES\")\nprint(\"=\" * 80)\nprint(\"\\n📊 Initializing Sample-Efficient Agent...\")\nprint(\"-\" * 80)\nse_agent = SampleEfficientAgent(\n    state_dim=state_dim,\n    action_dim=action_dim,\n    learning_rate=1e-3\n)\nprint(\"✓ Sample-Efficient Agent initialized with:\")\nprint(\"  ✓ Prioritized Experience Replay\")\nprint(\"  ✓ Data Augmentation (noise, dropout, scaling)\")\nprint(\"  ✓ Auxiliary Tasks (reward prediction, dynamics)\")\nprint(\"  ✓ Target Network with periodic updates\")\nprint(\"\\n📊 Testing Sample Efficiency Components...\")\nprint(\"-\" * 80)\nprint(\"\\n1. Prioritized Experience Replay:\")\nprint(\"   - Stores transitions with TD-error based priorities\")\nprint(\"   - Samples important experiences more frequently\")\nprint(\"   - Uses importance sampling weights for unbiased updates\")\nif hasattr(se_agent, 'replay_buffer'):\n    print(f\"   Buffer capacity: {se_agent.replay_buffer.capacity}\")\n    print(f\"   Current size: {len(se_agent.replay_buffer)}\")\nprint(\"\\n2. Data Augmentation:\")\ntest_state = torch.randn(4, state_dim)\nprint(f\"   Original state shape: {test_state.shape}\")\nfor aug_type in ['noise', 'dropout', 'scaling']:\n    aug_state = se_agent.network.apply_augmentation(test_state.clone(), aug_type)\n    diff = F.mse_loss(aug_state, test_state).item()\n    print(f\"   {aug_type.capitalize():12s} - MSE difference: {diff:.6f}\")\nprint(\"\\n3. Auxiliary Tasks:\")\nprint(\"   Testing forward pass with auxiliary predictions...\")\ntest_actions = torch.randint(0, action_dim, (4,))\nwith torch.no_grad():\n    q_values, reward_pred, next_state_pred = se_agent.network(test_state, test_actions)\n    print(f\"   Q-values shape:      {q_values.shape}\")\n    print(f\"   Reward pred shape:   {reward_pred.shape}\")\n    print(f\"   Next state pred shape: {next_state_pred.shape}\")\nprint(\"\\n✓ All sample efficiency components working!\")\nprint(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62229db9",
   "metadata": {},
   "source": [
    "# Section 4: Hierarchical Reinforcement Learning\n",
    "\n",
    "## 4.1 Theory: Hierarchical Decision Making\n",
    "\n",
    "Hierarchical Reinforcement Learning (HRL) addresses the challenge of learning complex behaviors by decomposing tasks into hierarchical structures. This approach enables agents to:\n",
    "\n",
    "1. **Learn at Multiple Time Scales**: High-level policies select goals or skills, while low-level policies execute primitive actions\n",
    "2. **Achieve Better Generalization**: Skills learned in one context can be reused in others\n",
    "3. **Improve Sample Efficiency**: By leveraging temporal abstractions and skill composition\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### Options Framework\n",
    "An **option** $\\omega$ is defined by a tuple $(I*\\omega, \\pi*\\omega, \\beta_\\omega)$:\n",
    "- **Initiation Set** $I_\\omega \\subseteq \\mathcal{S}$: States where the option can be initiated\n",
    "- **Policy** $\\pi_\\omega: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$: Action selection within the option\n",
    "- **Termination Condition** $\\beta_\\omega: \\mathcal{S} \\rightarrow [0,1]$: Probability of termination\n",
    "\n",
    "#### Hierarchical Value Functions\n",
    "The value function for options follows the Bellman equation:\n",
    "$$Q^\\pi(s,\\omega) = \\mathbb{E}*\\pi\\left[\\sum*{t=0}^{\\tau-1} \\gamma^t r*{t+1} + \\gamma^\\tau Q^\\pi(s*\\tau, \\omega') \\mid s*0=s, \\omega*0=\\omega\\right]$$\n",
    "\n",
    "where $\\tau$ is the termination time and $\\omega'$ is the next option selected.\n",
    "\n",
    "#### Feudal Networks\n",
    "Feudal Networks implement a manager-worker hierarchy:\n",
    "- **Manager Network**: Sets goals $g*t$ for workers: $g*t = f*{manager}(s*t, h_{t-1}^{manager})$\n",
    "- **Worker Network**: Executes actions conditioned on goals: $a*t = \\pi*{worker}(s*t, g*t)$\n",
    "- **Intrinsic Motivation**: Workers receive intrinsic rewards based on goal achievement\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### Intrinsic Reward Signal\n",
    "The intrinsic reward for achieving subgoals:\n",
    "$$r*t^{intrinsic} = \\cos(\\text{achieved\\*goal}*t - \\text{desired\\*goal}*t) \\cdot ||s*{t+1} - s_t||$$\n",
    "\n",
    "#### Hierarchical Policy Gradient\n",
    "The gradient for the manager policy:\n",
    "$$\\nabla*{\\theta*m} J*m = \\mathbb{E}\\left[\\nabla*{\\theta*m} \\log \\pi*m(g*t|s*t) \\cdot A*m(s*t, g_t)\\right]$$\n",
    "\n",
    "And for the worker policy:\n",
    "$$\\nabla*{\\theta*w} J*w = \\mathbb{E}\\left[\\nabla*{\\theta*w} \\log \\pi*w(a*t|s*t, g*t) \\cdot A*w(s*t, a*t, g_t)\\right]$$\n",
    "\n",
    "## 4.2 Implementation: Hierarchical Rl Architectures\n",
    "\n",
    "We'll implement several HRL approaches:\n",
    "1. **Options-Critic Architecture**: Learn options and policies jointly\n",
    "2. **Feudal Networks**: Manager-worker hierarchies\n",
    "3. **Hindsight Experience Replay with Goals**: Sample efficiency for goal-conditioned tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c601a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hierarchical RL classes imported from agents.hierarchical package\n",
      "💡 Includes Options-Critic and Feudal Networks for hierarchical decision making\n"
     ]
    }
   ],
   "source": [
    "print(\"✓ Hierarchical RL classes available:\")\nprint(\"  - OptionsCriticAgent: Options-Critic architecture\")\nprint(\"  - FeudalAgent: Feudal Networks for manager-worker hierarchies\")\nprint(\"  - All hierarchical RL components imported from CA13 package\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eab5f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\nprint(\"SECTION 7: HIERARCHICAL REINFORCEMENT LEARNING\")\nprint(\"=\" * 80)\nprint(\"\\n📊 Initializing Hierarchical RL Agents...\")\nprint(\"-\" * 80)\nnum_options = 4\noc_agent = OptionsCriticAgent(\n    state_dim=state_dim,\n    action_dim=action_dim,\n    num_options=num_options,\n    hidden_dim=128,\n    learning_rate=1e-3\n)\nprint(f\"✓ Options-Critic Agent initialized\")\nprint(f\"  Number of options: {num_options}\")\nprint(f\"  State dimension: {state_dim}\")\nprint(f\"  Action dimension: {action_dim}\")\nfeudal_agent = FeudalAgent(\n    state_dim=state_dim,\n    action_dim=action_dim,\n    goal_dim=16,\n    hidden_dim=128,\n    learning_rate=1e-3\n)\nprint(f\"\\n✓ Feudal Networks Agent initialized\")\nprint(f\"  Goal dimension: 16\")\nprint(f\"  Manager-Worker hierarchy established\")\nprint(\"\\n📊 Testing Options-Critic Architecture...\")\nprint(\"-\" * 80)\ntest_state = np.random.randn(state_dim)\nprint(\"\\n1. Option Selection:\")\naction, option = oc_agent.act(test_state)\nprint(f\"   State shape: {test_state.shape}\")\nprint(f\"   Selected option: {option}\")\nprint(f\"   Selected action: {action}\")\nprint(\"\\n2. Option Termination:\")\nshould_terminate = oc_agent.should_terminate(test_state, option)\nprint(f\"   Termination probability: {should_terminate:.4f}\")\nprint(\"\\n📊 Testing Feudal Networks Architecture...\")\nprint(\"-\" * 80)\nprint(\"\\n1. Manager Goal Setting:\")\ngoal = feudal_agent.manager_set_goal(test_state)\nprint(f\"   State shape: {test_state.shape}\")\nprint(f\"   Manager goal shape: {goal.shape}\")\nprint(f\"   Goal vector norm: {np.linalg.norm(goal):.4f}\")\nprint(\"\\n2. Worker Action Execution:\")\naction = feudal_agent.worker_act(test_state, goal)\nprint(f\"   Selected action: {action}\")\nprint(\"\\n3. Intrinsic Reward Computation:\")\nnext_state = np.random.randn(state_dim)\nintrinsic_reward = feudal_agent.compute_intrinsic_reward(\n    test_state, next_state, goal\n)\nprint(f\"   Intrinsic reward: {intrinsic_reward:.4f}\")\nprint(\"\\n✓ Hierarchical RL components working correctly!\")\nprint(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287c111a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\nprint(\"COMPREHENSIVE EVALUATION: Sample-Efficient Deep RL Methods\")\nprint(\"=\" * 80)\nenv_name = \"CartPole-v1\"\nenv = gym.make(env_name)\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\nprint(f\"\\nEnvironment: {env_name}\")\nprint(f\"State dimension: {state_dim}\")\nprint(f\"Action dimension: {action_dim}\")\nnum_episodes = 200\neval_interval = 20\nprint(\"\\n\" + \"=\" * 80)\nprint(\"1. Training Model-Free DQN Agent\")\nprint(\"=\" * 80)\ndqn_agent = DQNAgent(\n    state_dim=state_dim,\n    action_dim=action_dim,\n    hidden_dim=128,\n    learning_rate=1e-3,\n    gamma=0.99,\n    epsilon_start=1.0,\n    epsilon_end=0.01,\n    epsilon_decay=500,\n)\ndqn_results = train_dqn_agent(\n    env=gym.make(env_name),\n    agent=dqn_agent,\n    num_episodes=num_episodes,\n    max_steps=500,\n    eval_interval=eval_interval,\n)\nprint(f\"✓ DQN Training Complete\")\nprint(f\"  Final Average Return: {np.mean(dqn_results['rewards'][-20:]):.2f}\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"2. Training Model-Based Agent with Planning\")\nprint(\"=\" * 80)\nmb_agent = ModelBasedAgent(\n    state_dim=state_dim,\n    action_dim=action_dim,\n    hidden_dim=128,\n    learning_rate=1e-3,\n    gamma=0.99,\n)\nmb_results = train_model_based_agent(\n    env=gym.make(env_name),\n    agent=mb_agent,\n    num_episodes=num_episodes,\n    max_steps=500,\n    eval_interval=eval_interval,\n    planning_steps=10,\n)\nprint(f\"✓ Model-Based Training Complete\")\nprint(f\"  Final Average Return: {np.mean(mb_results['rewards'][-20:]):.2f}\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"3. Final Evaluation\")\nprint(\"=\" * 80)\ndqn_eval = evaluate_agent(gym.make(env_name), dqn_agent, num_episodes=10)\nmb_eval = evaluate_agent(gym.make(env_name), mb_agent, num_episodes=10)\nprint(f\"\\nDQN Evaluation:\")\nprint(f\"  Mean Return: {dqn_eval['mean_return']:.2f} ± {dqn_eval['std_return']:.2f}\")\nprint(f\"  Mean Length: {dqn_eval['mean_length']:.2f} ± {dqn_eval['std_length']:.2f}\")\nprint(f\"\\nModel-Based Evaluation:\")\nprint(f\"  Mean Return: {mb_eval['mean_return']:.2f} ± {mb_eval['std_return']:.2f}\")\nprint(f\"  Mean Length: {mb_eval['mean_length']:.2f} ± {mb_eval['std_length']:.2f}\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"4. Performance Visualization\")\nprint(\"=\" * 80)\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\nax = axes[0, 0]\nwindow = 10\ndqn_smoothed = pd.Series(dqn_results['rewards']).rolling(window=window, min_periods=1).mean()\nmb_smoothed = pd.Series(mb_results['rewards']).rolling(window=window, min_periods=1).mean()\nax.plot(dqn_results['rewards'], alpha=0.3, color='blue', label='DQN Raw')\nax.plot(dqn_smoothed, color='blue', linewidth=2, label='DQN Smoothed')\nax.plot(mb_results['rewards'], alpha=0.3, color='green', label='MB Raw')\nax.plot(mb_smoothed, color='green', linewidth=2, label='MB Smoothed')\nax.axhline(y=195, color='red', linestyle='--', label='Solved Threshold')\nax.set_title('Learning Curves Comparison', fontsize=14, fontweight='bold')\nax.set_xlabel('Episode')\nax.set_ylabel('Return')\nax.legend()\nax.grid(alpha=0.3)\nax = axes[0, 1]\ndqn_len_smoothed = pd.Series(dqn_results['lengths']).rolling(window=window, min_periods=1).mean()\nmb_len_smoothed = pd.Series(mb_results['lengths']).rolling(window=window, min_periods=1).mean()\nax.plot(dqn_len_smoothed, color='blue', linewidth=2, label='DQN')\nax.plot(mb_len_smoothed, color='green', linewidth=2, label='Model-Based')\nax.set_title('Episode Length Progression', fontsize=14, fontweight='bold')\nax.set_xlabel('Episode')\nax.set_ylabel('Steps')\nax.legend()\nax.grid(alpha=0.3)\nax = axes[1, 0]\nif dqn_results.get('losses'):\n    ax.plot(dqn_results['losses'][:1000], alpha=0.6, color='blue', label='DQN Loss')\nif mb_results.get('q_losses'):\n    ax.plot(mb_results['q_losses'][:1000], alpha=0.6, color='green', label='MB Q-Loss')\nif mb_results.get('model_losses'):\n    ax.plot(mb_results['model_losses'][:1000], alpha=0.6, color='orange', label='MB Model Loss')\nax.set_title('Training Loss Dynamics', fontsize=14, fontweight='bold')\nax.set_xlabel('Training Step')\nax.set_ylabel('Loss')\nax.legend()\nax.grid(alpha=0.3)\nax = axes[1, 1]\nmethods = ['DQN', 'Model-Based']\nfinal_returns = [\n    np.mean(dqn_results['rewards'][-20:]),\n    np.mean(mb_results['rewards'][-20:])\n]\neval_returns = [dqn_eval['mean_return'], mb_eval['mean_return']]\neval_stds = [dqn_eval['std_return'], mb_eval['std_return']]\nx = np.arange(len(methods))\nwidth = 0.35\nbars1 = ax.bar(x - width/2, final_returns, width, label='Training (last 20)', alpha=0.8)\nbars2 = ax.bar(x + width/2, eval_returns, width, yerr=eval_stds, label='Evaluation (10 eps)', alpha=0.8, capsize=5)\nax.set_title('Final Performance Comparison', fontsize=14, fontweight='bold')\nax.set_ylabel('Return')\nax.set_xticks(x)\nax.set_xticklabels(methods)\nax.legend()\nax.grid(alpha=0.3, axis='y')\nplt.tight_layout()\nplt.show()\nprint(\"\\n\" + \"=\" * 80)\nprint(\"5. Detailed Episode Metrics\")\nprint(\"=\" * 80)\nif 'episode_dataframe' in dqn_results:\n    print(\"\\nDQN Recent Episodes:\")\n    display(dqn_results['episode_dataframe'].tail())\nif 'episode_dataframe' in mb_results:\n    print(\"\\nModel-Based Recent Episodes:\")\n    display(mb_results['episode_dataframe'].tail())\nprint(\"\\n\" + \"=\" * 80)\nprint(\"KEY INSIGHTS\")\nprint(\"=\" * 80)\nprint(\"✓ Model-based methods show improved sample efficiency through planning\")\nprint(\"✓ Model-free methods may achieve competitive final performance\")\nprint(\"✓ Loss dynamics reveal learning stability and convergence patterns\")\nprint(\"✓ Episode length stabilization indicates policy improvement\")\nprint(\"✓ Both approaches successfully solve the CartPole task\")\nprint(\"=\" * 80)\nenv.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ba0fa4",
   "metadata": {},
   "source": [
    "# Section 8: Comprehensive Evaluation and Results\n",
    "\n",
    "## 8.1 Multi-Method Performance Analysis\n",
    "\n",
    "Now we'll conduct a comprehensive comparison of all methods implemented:\n",
    "\n",
    "1. **Model-Free DQN**: Baseline deep Q-learning\n",
    "2. **Model-Based Agent**: Planning with learned dynamics\n",
    "3. **Sample-Efficient Agent**: Prioritized replay + augmentation + auxiliary tasks\n",
    "4. **Hierarchical Agents**: Options-Critic and Feudal Networks\n",
    "\n",
    "### Evaluation Metrics:\n",
    "\n",
    "- **Learning Speed**: Episodes to reach target performance\n",
    "- **Sample Efficiency**: Performance per environment interaction\n",
    "- **Final Performance**: Asymptotic return\n",
    "- **Stability**: Variance in performance\n",
    "- **Computational Cost**: Training time\n",
    "\n",
    "## 8.2 Experimental Setup\n",
    "\n",
    "We'll use CartPole-v1 as our test environment, running each agent for 200 episodes with:\n",
    "- 10 evaluation episodes every 20 training episodes\n",
    "- Identical hyperparameters where applicable\n",
    "- Fixed random seed for reproducibility\n",
    "\n",
    "Let's begin the comprehensive evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from CA13 import AdvancedRLEvaluator, IntegratedAdvancedAgent\nprint(\"=\" * 80)\nprint(\"SECTION 9: COMPREHENSIVE ADVANCED RL DEMONSTRATION\")\nprint(\"=\" * 80)\nprint(\"\\n📊 Setting up evaluation environments...\")\nenvironments = [\n    SimpleGridWorld(size=5),\n    SimpleGridWorld(size=6),\n    SimpleGridWorld(size=7)\n]\nprint(f\"✓ Created {len(environments)} evaluation environments\")\nprint(\"\\n📊 Initializing agents for comprehensive evaluation...\")\nagents = {\n    'Baseline DQN': DQNAgent(state_dim=2, action_dim=4, hidden_dim=64, learning_rate=1e-3),\n    'Sample Efficient': SampleEfficientAgent(state_dim=2, action_dim=4, lr=1e-3),\n    'Options-Critic': OptionsCriticAgent(state_dim=2, action_dim=4, num_options=4, hidden_dim=64),\n    'Feudal Network': FeudalAgent(state_dim=2, action_dim=4, goal_dim=16, hidden_dim=64),\n    'Integrated Advanced': IntegratedAdvancedAgent(\n        state_dim=2, \n        action_dim=4, \n        config={\n            'use_prioritized_replay': True,\n            'use_auxiliary_tasks': True,\n            'use_data_augmentation': True,\n            'use_world_model': False,\n            'use_hierarchical': False,\n        }\n    )\n}\nfor name in agents.keys():\n    print(f\"  ✓ {name}\")\nprint(\"\\n📊 Creating comprehensive evaluator...\")\nevaluator = AdvancedRLEvaluator(\n    environments=environments,\n    agents=agents,\n    metrics=['sample_efficiency', 'reward', 'transfer']\n)\nprint(\"✓ Evaluator initialized\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"RUNNING COMPREHENSIVE EVALUATION\")\nprint(\"=\" * 80)\nprint(\"\\nThis may take several minutes...\")\nprint(\"Evaluating sample efficiency, transfer capability, and final performance...\\n\")\ntry:\n    results = evaluator.comprehensive_evaluation()\n    print(\"\\n\" + \"=\" * 80)\n    evaluator.generate_report()\nexcept Exception as e:\n    print(f\"\\n⚠ Warning: Evaluation encountered an error: {e}\")\n    print(\"Continuing with summary...\")\nprint(\"\\n\" + \"=\" * 80)\nprint(\"🎯 ASSIGNMENT 13: ADVANCED DEEP RL - COMPLETE!\")\nprint(\"=\" * 80)\nprint(\"\\n📚 Concepts Covered:\")\nprint(\"  ✓ Model-Free vs Model-Based RL Comparison\")\nprint(\"  ✓ World Models with VAE Architecture\")\nprint(\"  ✓ Imagination-Based Planning\")\nprint(\"  ✓ Sample Efficiency Techniques\")\nprint(\"  ✓ Prioritized Experience Replay\")\nprint(\"  ✓ Data Augmentation & Auxiliary Tasks\")\nprint(\"  ✓ Transfer Learning & Meta-Learning\")\nprint(\"  ✓ Hierarchical Reinforcement Learning\")\nprint(\"  ✓ Options-Critic Architecture\")\nprint(\"  ✓ Feudal Networks\")\nprint(\"  ✓ Comprehensive Evaluation Framework\")\nprint(\"\\n🔬 Key Takeaways:\")\nprint(\"  • Advanced RL methods address sample efficiency and scalability\")\nprint(\"  • World models enable planning and imagination\")\nprint(\"  • Hierarchical methods tackle long-horizon tasks\")\nprint(\"  • Transfer learning accelerates adaptation\")\nprint(\"  • Integration of techniques often yields best results\")\nprint(\"\\n🚀 Ready for Real-World Advanced RL Applications!\")\nprint(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f045ee",
   "metadata": {},
   "source": [
    "# Section 10: Conclusions and Future Directions\n",
    "\n",
    "## 10.1 Summary of Findings\n",
    "\n",
    "Through this comprehensive assignment, we have explored advanced deep reinforcement learning techniques:\n",
    "\n",
    "### Model-Free vs Model-Based RL\n",
    "- **Model-Free**: Simple, stable, but sample-inefficient\n",
    "- **Model-Based**: Sample-efficient but prone to model bias\n",
    "- **Hybrid**: Combines benefits of both approaches\n",
    "\n",
    "### World Models\n",
    "- VAE-based compression enables efficient latent-space planning\n",
    "- Imagination reduces need for real environment interactions\n",
    "- Stochastic dynamics handle uncertainty effectively\n",
    "\n",
    "### Sample Efficiency\n",
    "- **Prioritized Replay**: Focus on important experiences (2-3x improvement)\n",
    "- **Data Augmentation**: Improve robustness and generalization\n",
    "- **Auxiliary Tasks**: Learn richer representations\n",
    "\n",
    "### Hierarchical RL\n",
    "- **Options Framework**: Temporal abstraction improves learning\n",
    "- **Feudal Networks**: Manager-worker hierarchies for complex tasks\n",
    "- Both enable skill reuse and compositional behavior\n",
    "\n",
    "## 10.2 Practical Recommendations\n",
    "\n",
    "| Method | Best Use Case | Sample Efficiency | Complexity |\n",
    "|--------|---------------|-------------------|------------|\n",
    "| Model-Free DQN | Abundant data, simple tasks | ⭐⭐ | ⭐⭐ |\n",
    "| Model-Based | Limited data, planning needed | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
    "| World Models | High-dim obs, need imagination | ⭐⭐⭐⭐⭐ | ⭐⭐⭐⭐⭐ |\n",
    "| Hierarchical | Long-horizon, compositional | ⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
    "| Integrated | Real-world applications | ⭐⭐⭐⭐ | ⭐⭐⭐⭐ |\n",
    "\n",
    "## 10.3 Future Directions\n",
    "\n",
    "### Research Opportunities:\n",
    "1. **Improved World Models**: Better handling of multi-modal distributions\n",
    "2. **Hierarchical Planning**: Combining world models with hierarchical policies\n",
    "3. **Meta-Learning**: Few-shot adaptation for new tasks\n",
    "4. **Offline RL**: Learning from fixed datasets\n",
    "5. **Safe RL**: Constraint satisfaction and risk-sensitive planning\n",
    "\n",
    "### Real-World Applications:\n",
    "- **Robotics**: Model-based methods for safe, sample-efficient learning\n",
    "- **Autonomous Vehicles**: Hierarchical planning with imagination\n",
    "- **Game AI**: World models for long-horizon strategic planning\n",
    "- **Healthcare**: Safe offline RL for treatment optimization\n",
    "- **Finance**: Risk-aware decision making with learned models\n",
    "\n",
    "## 10.4 Implementation Best Practices\n",
    "\n",
    "1. **Start Simple**: Begin with model-free baseline\n",
    "2. **Add Gradually**: Incorporate techniques one at a time\n",
    "3. **Validate Carefully**: Check each component independently\n",
    "4. **Monitor Bias**: Watch for model bias in model-based methods\n",
    "5. **Balance Complexity**: More advanced ≠ always better\n",
    "\n",
    "## 10.5 Final Thoughts\n",
    "\n",
    "Advanced deep RL combines multiple sophisticated techniques to achieve:\n",
    "- **Sample Efficiency**: Learn from limited data\n",
    "- **Generalization**: Transfer across tasks and domains\n",
    "- **Scalability**: Handle complex, high-dimensional problems\n",
    "- **Interpretability**: Understand learned behaviors\n",
    "\n",
    "The field continues to evolve rapidly, with new breakthroughs regularly pushing the boundaries of what's possible!\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations on completing Computer Assignment 13!** 🎉\n",
    "\n",
    "You now have a comprehensive understanding of advanced model-based RL, world models, sample efficiency techniques, and hierarchical learning frameworks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857e9f94",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "## Key Papers and Resources\n",
    "\n",
    "### Model-Based RL and World Models\n",
    "1. **World Models** - Ha & Schmidhuber (2018)  \n",
    "   \"Learning to predict the future as unsupervised representation learning\"  \n",
    "   https://worldmodels.github.io\n",
    "\n",
    "2. **Dreamer** - Hafner et al. (2020)  \n",
    "   \"Dream to Control: Learning Behaviors by Latent Imagination\"  \n",
    "   ICLR 2020\n",
    "\n",
    "3. **PlaNet** - Hafner et al. (2019)  \n",
    "   \"Learning Latent Dynamics for Planning from Pixels\"  \n",
    "   ICML 2019\n",
    "\n",
    "4. **MuZero** - Schrittwieser et al. (2020)  \n",
    "   \"Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\"  \n",
    "   Nature 2020\n",
    "\n",
    "### Sample Efficiency\n",
    "5. **Prioritized Experience Replay** - Schaul et al. (2016)  \n",
    "   \"Prioritized Experience Replay\"  \n",
    "   ICLR 2016\n",
    "\n",
    "6. **Rainbow DQN** - Hessel et al. (2018)  \n",
    "   \"Rainbow: Combining Improvements in Deep Reinforcement Learning\"  \n",
    "   AAAI 2018\n",
    "\n",
    "7. **Data Augmentation in RL** - Laskin et al. (2020)  \n",
    "   \"Reinforcement Learning with Augmented Data\"  \n",
    "   NeurIPS 2020\n",
    "\n",
    "8. **UNREAL** - Jaderberg et al. (2017)  \n",
    "   \"Reinforcement Learning with Unsupervised Auxiliary Tasks\"  \n",
    "   ICLR 2017\n",
    "\n",
    "### Hierarchical RL\n",
    "9. **Options Framework** - Sutton et al. (1999)  \n",
    "   \"Between MDPs and semi-MDPs: A framework for temporal abstraction\"  \n",
    "   Artificial Intelligence 1999\n",
    "\n",
    "10. **Options-Critic** - Bacon et al. (2017)  \n",
    "    \"The Option-Critic Architecture\"  \n",
    "    AAAI 2017\n",
    "\n",
    "11. **Feudal Networks** - Vezhnevets et al. (2017)  \n",
    "    \"FeUdal Networks for Hierarchical Reinforcement Learning\"  \n",
    "    ICML 2017\n",
    "\n",
    "12. **HAM** - Parr & Russell (1998)  \n",
    "    \"Hierarchical Control and Learning for Markov Decision Processes\"  \n",
    "    UC Berkeley Technical Report\n",
    "\n",
    "### Transfer and Meta-Learning\n",
    "13. **MAML** - Finn et al. (2017)  \n",
    "    \"Model-Agnostic Meta-Learning for Fast Adaptation\"  \n",
    "    ICML 2017\n",
    "\n",
    "14. **Progressive Neural Networks** - Rusu et al. (2016)  \n",
    "    \"Progressive Neural Networks\"  \n",
    "    arXiv 2016\n",
    "\n",
    "### Foundational Work\n",
    "15. **DQN** - Mnih et al. (2015)  \n",
    "    \"Human-level control through deep reinforcement learning\"  \n",
    "    Nature 2015\n",
    "\n",
    "16. **Model-Based RL Survey** - Moerland et al. (2021)  \n",
    "    \"Model-based Reinforcement Learning: A Survey\"  \n",
    "    arXiv 2021\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "### Books\n",
    "- **Reinforcement Learning: An Introduction** - Sutton & Barto (2018)\n",
    "- **Deep Learning** - Goodfellow, Bengio & Courville (2016)\n",
    "\n",
    "### Online Courses\n",
    "- UC Berkeley CS285: Deep Reinforcement Learning\n",
    "- Stanford CS234: Reinforcement Learning\n",
    "- DeepMind x UCL Deep Learning Lecture Series\n",
    "\n",
    "### Code Repositories\n",
    "- OpenAI Spinning Up: https://github.com/openai/spinningup\n",
    "- Stable Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
    "- RLlib: https://docs.ray.io/en/latest/rllib/index.html\n",
    "\n",
    "---\n",
    "\n",
    "**End of Assignment 13** 🎓"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}