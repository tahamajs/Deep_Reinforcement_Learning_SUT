{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment 13: Advanced Model-Based RL and World Models\n",
    "\n",
    "## Course Information\n",
    "- **Course**: Deep Reinforcement Learning (DRL)\n",
    "- **Instructor**: Dr. [Instructor Name]\n",
    "- **Institution**: Sharif University of Technology\n",
    "- **Semester**: Fall 2024\n",
    "- **Assignment Number**: CA13\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this assignment, students will be able to:\n",
    "\n",
    "1. **Understand Model-Based vs Model-Free RL Trade-offs**: Analyze the fundamental differences between model-free and model-based reinforcement learning approaches, including their respective advantages, limitations, and appropriate use cases.\n",
    "\n",
    "2. **Master World Model Architectures**: Design and implement variational world models using VAEs for learning compact latent representations of environment dynamics, including encoder-decoder architectures and stochastic dynamics modeling.\n",
    "\n",
    "3. **Implement Imagination-Based Learning**: Develop agents that leverage learned world models for planning and decision-making in latent space, enabling sample-efficient learning through imagined trajectories.\n",
    "\n",
    "4. **Apply Sample Efficiency Techniques**: Utilize advanced techniques such as prioritized experience replay, data augmentation, and auxiliary tasks to improve learning efficiency in deep RL.\n",
    "\n",
    "5. **Design Transfer Learning Systems**: Build agents capable of transferring knowledge across related tasks through shared representations, fine-tuning, and meta-learning approaches.\n",
    "\n",
    "6. **Develop Hierarchical RL Frameworks**: Implement hierarchical decision-making systems using options framework, enabling temporal abstraction and skill composition for complex task solving.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this assignment, ensure you have:\n",
    "\n",
    "- **Mathematical Background**: \n",
    "  - Probability theory and stochastic processes\n",
    "  - Linear algebra and matrix operations\n",
    "  - Optimization and gradient-based methods\n",
    "  - Information theory (KL divergence, entropy)\n",
    "\n",
    "- **Technical Skills**:\n",
    "  - Python programming with PyTorch\n",
    "  - Deep learning fundamentals (neural networks, autoencoders)\n",
    "  - Basic reinforcement learning concepts (MDPs, value functions, policies)\n",
    "  - Experience with Gymnasium environments\n",
    "\n",
    "- **Prior Knowledge**:\n",
    "  - Completion of CA1-CA12 assignments\n",
    "  - Understanding of model-free RL algorithms (DQN, policy gradients)\n",
    "  - Familiarity with neural network architectures\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "This assignment is structured as follows:\n",
    "\n",
    "### Section 1: Model-Free vs Model-Based Reinforcement Learning\n",
    "- Theoretical foundations of model-free and model-based approaches\n",
    "- Mathematical formulations and trade-off analysis\n",
    "- Hybrid algorithms combining both paradigms\n",
    "- Practical implementation and comparison\n",
    "\n",
    "### Section 2: World Models and Imagination-Based Learning\n",
    "- Variational autoencoders for world modeling\n",
    "- Stochastic dynamics prediction in latent space\n",
    "- Imagination-based planning and policy optimization\n",
    "- Dreamer algorithm and modern variants\n",
    "\n",
    "### Section 3: Sample Efficiency and Transfer Learning\n",
    "- Prioritized experience replay and data augmentation\n",
    "- Auxiliary tasks for improved learning\n",
    "- Transfer learning techniques and meta-learning\n",
    "- Domain adaptation and curriculum learning\n",
    "\n",
    "### Section 4: Hierarchical Reinforcement Learning\n",
    "- Options framework and temporal abstraction\n",
    "- Hierarchical policy architectures\n",
    "- Skill discovery and composition\n",
    "- Applications to complex task domains\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "CA13/\n",
    "â”œâ”€â”€ CA13.ipynb              # Main assignment notebook\n",
    "â”œâ”€â”€ agents/                 # RL agent implementations\n",
    "â”‚   â”œâ”€â”€ model_free_agent.py # Model-free RL agents\n",
    "â”‚   â”œâ”€â”€ model_based_agent.py# Model-based RL agents\n",
    "â”‚   â”œâ”€â”€ world_model_agent.py# World model-based agents\n",
    "â”‚   â””â”€â”€ hierarchical_agent.py# Hierarchical RL agents\n",
    "â”œâ”€â”€ models/                 # Neural network architectures\n",
    "â”‚   â”œâ”€â”€ world_model.py      # VAE-based world models\n",
    "â”‚   â”œâ”€â”€ dynamics_model.py   # Environment dynamics models\n",
    "â”‚   â””â”€â”€ policy_networks.py  # Hierarchical policy networks\n",
    "â”œâ”€â”€ environments/           # Custom environments\n",
    "â”‚   â”œâ”€â”€ wrappers.py         # Environment wrappers\n",
    "â”‚   â””â”€â”€ complex_tasks.py    # Complex task environments\n",
    "â”œâ”€â”€ experiments/            # Training and evaluation scripts\n",
    "â”‚   â”œâ”€â”€ train_world_model.py# World model training\n",
    "â”‚   â”œâ”€â”€ compare_efficiency.py# Sample efficiency comparison\n",
    "â”‚   â””â”€â”€ transfer_learning.py# Transfer learning experiments\n",
    "â””â”€â”€ utils/                  # Utility functions\n",
    "    â”œâ”€â”€ visualization.py    # Plotting and analysis tools\n",
    "    â”œâ”€â”€ data_augmentation.py# Data augmentation utilities\n",
    "    â””â”€â”€ evaluation.py       # Performance evaluation metrics\n",
    "```\n",
    "\n",
    "## Contents Overview\n",
    "\n",
    "### Theoretical Foundations\n",
    "- **Model-Based RL Mathematics**: Transition and reward model learning, planning algorithms\n",
    "- **World Model Theory**: Variational inference, latent space dynamics, imagination-based learning\n",
    "- **Sample Efficiency**: Experience replay, prioritization, auxiliary learning objectives\n",
    "- **Transfer Learning**: Representation learning, fine-tuning, meta-learning algorithms\n",
    "\n",
    "### Implementation Components\n",
    "- **VAE World Models**: Encoder-decoder architectures with stochastic latent variables\n",
    "- **Imagination-Based Agents**: Planning in learned latent space using world models\n",
    "- **Sample-Efficient Algorithms**: Prioritized replay, data augmentation, auxiliary tasks\n",
    "- **Transfer Learning Systems**: Multi-task learning, fine-tuning, domain adaptation\n",
    "\n",
    "### Advanced Topics\n",
    "- **Hierarchical RL**: Options framework, skill hierarchies, temporal abstraction\n",
    "- **Meta-Learning**: Few-shot adaptation, gradient-based meta-learning\n",
    "- **Curriculum Learning**: Automatic difficulty progression, teacher-student frameworks\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Your implementation will be evaluated based on:\n",
    "\n",
    "1. **Correctness (40%)**: Accurate implementation of algorithms and mathematical formulations\n",
    "2. **Efficiency (25%)**: Sample efficiency improvements and computational performance\n",
    "3. **Innovation (20%)**: Creative extensions and novel approaches to the problems\n",
    "4. **Analysis (15%)**: Quality of experimental analysis and insights\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Environment Setup**: Ensure all dependencies are installed\n",
    "2. **Code Review**: Understand the provided base implementations\n",
    "3. **Incremental Development**: Start with simpler components and build complexity\n",
    "4. **Testing**: Validate each component before integration\n",
    "5. **Experimentation**: Run comprehensive experiments and analyze results\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this assignment, you will have:\n",
    "\n",
    "- **Comprehensive Understanding**: Deep knowledge of advanced model-based RL techniques\n",
    "- **Practical Skills**: Ability to implement complex RL systems from scratch\n",
    "- **Research Perspective**: Insight into current challenges and future directions\n",
    "- **Portfolio Piece**: High-quality implementation demonstrating advanced RL capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This assignment represents the culmination of the Deep RL course, integrating concepts from model-free and model-based learning, advanced architectures, and practical deployment considerations. Focus on understanding the theoretical foundations while developing robust, efficient implementations.\n",
    "\n",
    "Let's begin our exploration of advanced model-based reinforcement learning and world models! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ed79da",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 557) (4118917461.py, line 557)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 557\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"  â€¢ Hybrid approaches can combine benefits of both\")\"\u001b[39m\n                                                               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 557)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class ModelFreeAgent:\n",
    "    \"\"\"Base class for model-free RL agents.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "        self.replay_buffer = ReplayBuffer(10000)\n",
    "        \n",
    "    def act(self, state, epsilon=0.1):\n",
    "        \"\"\"Select action using epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        return self.get_best_action(state)\n",
    "    \n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"Get best action according to current policy.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update agent from batch of experiences.\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class DQNAgent(ModelFreeAgent):\n",
    "    \"\"\"Deep Q-Network agent (model-free).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n",
    "        super().__init__(state_dim, action_dim, lr)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.target_network = copy.deepcopy(self.q_network)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.update_count = 0\n",
    "        self.losses = []\n",
    "        \n",
    "    def get_best_action(self, state):\n",
    "        \"\"\"Get action with highest Q-value.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.q_network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update Q-network using DQN loss.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        current_q = self.q_network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_states).max(1)[0]\n",
    "            target_q = rewards + (self.gamma * next_q * (~dones))\n",
    "        \n",
    "        loss = F.mse_loss(current_q.squeeze(), target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        self.update_count += 1\n",
    "        \n",
    "        if self.update_count % 100 == 0:\n",
    "            self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "        return loss.item()\n",
    "\n",
    "class ModelBasedAgent:\n",
    "    \"\"\"Model-based RL agent using learned dynamics.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, planning_horizon=5):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.lr = lr\n",
    "        self.planning_horizon = planning_horizon\n",
    "        \n",
    "        self.dynamics_model = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, state_dim)\n",
    "        )\n",
    "        \n",
    "        self.reward_model = nn.Sequential(\n",
    "            nn.Linear(state_dim + action_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.dynamics_optimizer = optim.Adam(self.dynamics_model.parameters(), lr=lr)\n",
    "        self.reward_optimizer = optim.Adam(self.reward_model.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.model_buffer = ReplayBuffer(10000)\n",
    "        self.planning_buffer = ReplayBuffer(5000)\n",
    "        \n",
    "        self.model_losses = []\n",
    "        self.value_losses = []\n",
    "        \n",
    "    def act(self, state, epsilon=0.1):\n",
    "        \"\"\"Select action using model-based planning.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        return self.plan_action(state)\n",
    "    \n",
    "    def plan_action(self, state):\n",
    "        \"\"\"Plan best action using learned model.\"\"\"\n",
    "        best_action = 0\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for action in range(self.action_dim):\n",
    "            value = self.simulate_trajectory(state, action)\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def simulate_trajectory(self, initial_state, initial_action):\n",
    "        \"\"\"Simulate trajectory using learned model.\"\"\"\n",
    "        state = torch.FloatTensor(initial_state)\n",
    "        total_reward = 0.0\n",
    "        gamma = 0.99\n",
    "        \n",
    "        for step in range(self.planning_horizon):\n",
    "            if step == 0:\n",
    "                action = initial_action\n",
    "            else:\n",
    "                action = self.get_greedy_action(state)\n",
    "            \n",
    "            action_tensor = torch.FloatTensor([action])\n",
    "            action_one_hot = F.one_hot(action_tensor.long(), self.action_dim).float()\n",
    "            \n",
    "            model_input = torch.cat([state, action_one_hot], dim=-1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                next_state = self.dynamics_model(model_input)\n",
    "                reward = self.reward_model(model_input).item()\n",
    "            \n",
    "            total_reward += (gamma ** step) * reward\n",
    "            state = next_state\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            terminal_value = self.value_network(state).item()\n",
    "            total_reward += (gamma ** self.planning_horizon) * terminal_value\n",
    "        \n",
    "        return total_reward\n",
    "    \n",
    "    def get_greedy_action(self, state):\n",
    "        \"\"\"Get greedy action for planning.\"\"\"\n",
    "        best_action = 0\n",
    "        best_q = float('-inf')\n",
    "        \n",
    "        for action in range(self.action_dim):\n",
    "            action_tensor = torch.FloatTensor([action])\n",
    "            action_one_hot = F.one_hot(action_tensor.long(), self.action_dim).float()\n",
    "            model_input = torch.cat([state, action_one_hot], dim=-1)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                q_value = self.reward_model(model_input).item()\n",
    "            \n",
    "            if q_value > best_q:\n",
    "                best_q = q_value\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def update_model(self, batch):\n",
    "        \"\"\"Update dynamics and reward models.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        \n",
    "        actions_one_hot = F.one_hot(actions, self.action_dim).float()\n",
    "        model_input = torch.cat([states, actions_one_hot], dim=-1)\n",
    "        \n",
    "        pred_next_states = self.dynamics_model(model_input)\n",
    "        dynamics_loss = F.mse_loss(pred_next_states, next_states)\n",
    "        \n",
    "        self.dynamics_optimizer.zero_grad()\n",
    "        dynamics_loss.backward()\n",
    "        self.dynamics_optimizer.step()\n",
    "        \n",
    "        pred_rewards = self.reward_model(model_input).squeeze()\n",
    "        reward_loss = F.mse_loss(pred_rewards, rewards)\n",
    "        \n",
    "        self.reward_optimizer.zero_grad()\n",
    "        reward_loss.backward()\n",
    "        self.reward_optimizer.step()\n",
    "        \n",
    "        total_model_loss = dynamics_loss.item() + reward_loss.item()\n",
    "        self.model_losses.append(total_model_loss)\n",
    "        \n",
    "        return total_model_loss\n",
    "    \n",
    "    def update_value_function(self, batch):\n",
    "        \"\"\"Update value function using temporal difference learning.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        current_values = self.value_network(states).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_values = self.value_network(next_states).squeeze()\n",
    "            targets = rewards + 0.99 * next_values * (~dones)\n",
    "        \n",
    "        value_loss = F.mse_loss(current_values, targets)\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        self.value_losses.append(value_loss.item())\n",
    "        \n",
    "        return value_loss.item()\n",
    "\n",
    "class HybridDynaAgent:\n",
    "    \"\"\"Dyna-Q style hybrid agent combining model-free and model-based learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, planning_steps=5):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.planning_steps = planning_steps\n",
    "        \n",
    "        self.q_table = defaultdict(lambda: np.zeros(action_dim))\n",
    "        self.lr = lr\n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.model = {}  # (s,a) -> (r, s', done)\n",
    "        self.visited_states = set()       \n",
    "        self.experience_buffer = deque(maxlen=10000)\n",
    "        \n",
    "    def act(self, state, epsilon=0.1):\n",
    "        \"\"\"Epsilon-greedy action selection.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        state_key = tuple(state) if isinstance(state, np.ndarray) else state\n",
    "        return np.argmax(self.q_table[state_key])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Dyna-Q update: direct RL + model learning + planning.\"\"\"\n",
    "        state_key = tuple(state) if isinstance(state, np.ndarray) else state\n",
    "        next_state_key = tuple(next_state) if isinstance(next_state, np.ndarray) else next_state\n",
    "        \n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.gamma * np.max(self.q_table[next_state_key])\n",
    "        \n",
    "        self.q_table[state_key][action] += self.lr * (target - self.q_table[state_key][action])\n",
    "        \n",
    "        self.model[(state_key, action)] = (reward, next_state_key, done)\n",
    "        self.visited_states.add(state_key)\n",
    "        self.experience_buffer.append((state_key, action, reward, next_state_key, done))\n",
    "        \n",
    "        self.planning_updates()\n",
    "    \n",
    "    def planning_updates(self):\n",
    "        \"\"\"Perform planning updates using learned model.\"\"\"\n",
    "        if len(self.experience_buffer) == 0:\n",
    "            return\n",
    "        \n",
    "        for _ in range(self.planning_steps):\n",
    "            if len(self.experience_buffer) > 0:\n",
    "                state_key, action, reward, next_state_key, done = random.choice(self.experience_buffer)\n",
    "                \n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    target = reward + self.gamma * np.max(self.q_table[next_state_key])\n",
    "                \n",
    "                self.q_table[state_key][action] += self.lr * (target - self.q_table[state_key][action])\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Experience replay buffer for storing transitions.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return states, actions, rewards, next_states, dones\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class SimpleGridWorld:\n",
    "    \n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "        self.state = [0, 0]  # [row, col]\n",
    "        self.goal = [size-1, size-1]\n",
    "        self.action_space = 4  # up, right, down, left\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = [0, 0]\n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        moves = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        \n",
    "        new_row = max(0, min(self.size-1, self.state[0] + moves[action][0]))\n",
    "        new_col = max(0, min(self.size-1, self.state[1] + moves[action][1]))\n",
    "        \n",
    "        self.state = [new_row, new_col]\n",
    "        \n",
    "        if self.state == self.goal:\n",
    "            reward = 10.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.1  # Small negative reward for each step\n",
    "            done = False\n",
    "        \n",
    "        return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "\n",
    "def compare_agents_performance():\n",
    "    print(\"ðŸ”¬ Comparing Model-Free vs Model-Based RL Performance\")\n",
    "    \n",
    "    env = SimpleGridWorld(size=5)\n",
    "    \n",
    "    model_free_agent = DQNAgent(state_dim=2, action_dim=4, lr=1e-3)\n",
    "    model_based_agent = ModelBasedAgent(state_dim=2, action_dim=4, lr=1e-3)\n",
    "    hybrid_agent = HybridDynaAgent(state_dim=2, action_dim=4, lr=0.1)\n",
    "    \n",
    "    agents = {\n",
    "        'Model-Free (DQN)': model_free_agent,\n",
    "        'Model-Based': model_based_agent,\n",
    "        'Hybrid (Dyna-Q)': hybrid_agent\n",
    "    }\n",
    "    \n",
    "    results = {name: {'episodes': [], 'rewards': [], 'steps': []} for name in agents.keys()}\n",
    "    \n",
    "    num_episodes = 200\n",
    "    batch_size = 32\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        for agent_name, agent in agents.items():\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_steps = 0\n",
    "            max_steps = 100\n",
    "            \n",
    "            for step in range(max_steps):\n",
    "                if agent_name == 'Hybrid (Dyna-Q)':\n",
    "                    action = agent.act(state, epsilon=max(0.1, 1.0 - episode/100))\n",
    "                else:\n",
    "                    action = agent.act(state, epsilon=max(0.1, 1.0 - episode/100))\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_steps += 1\n",
    "                \n",
    "                if agent_name == 'Model-Free (DQN)':\n",
    "                    agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                    if len(agent.replay_buffer) > batch_size:\n",
    "                        batch = agent.replay_buffer.sample(batch_size)\n",
    "                        agent.update(batch)\n",
    "                \n",
    "                elif agent_name == 'Model-Based':\n",
    "                    agent.model_buffer.push(state, action, reward, next_state, done)\n",
    "                    if len(agent.model_buffer) > batch_size:\n",
    "                        batch = agent.model_buffer.sample(batch_size)\n",
    "                        agent.update_model(batch)\n",
    "                        agent.update_value_function(batch)\n",
    "                \n",
    "                elif agent_name == 'Hybrid (Dyna-Q)':\n",
    "                    agent.update(state, action, reward, next_state, done)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            results[agent_name]['episodes'].append(episode)\n",
    "            results[agent_name]['rewards'].append(episode_reward)\n",
    "            results[agent_name]['steps'].append(episode_steps)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"Episode {episode}:\"\n",
    "                  for agent_name in agents.keys():\n",
    "                      recent_reward = np.mean(results[agent_name]['rewards'][-10:]) if len(results[agent_name]['rewards']) >= 10 else 0\n",
    "                      print(f\"  {agent_name}: {recent_reward:.2f} avg reward\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_comparison(results):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    for agent_name, data in results.items():\n",
    "        window_size = 20\n",
    "        if len(data['rewards']) >= window_size:\n",
    "            smoothed_rewards = pd.Series(data['rewards']).rolling(window_size).mean()\n",
    "            axes[0,0].plot(data['episodes'], smoothed_rewards, label=agent_name, linewidth=2)\n",
    "    \n",
    "    axes[0,0].set_title('Learning Curves (Smoothed Rewards)')\n",
    "    axes[0,0].set_xlabel('Episode')\n",
    "    axes[0,0].set_ylabel('Episode Reward')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    for agent_name, data in results.items():\n",
    "        cumulative_steps = np.cumsum(data['steps'])\n",
    "        axes[0,1].plot(cumulative_steps, data['rewards'], label=agent_name, linewidth=2)\n",
    "    \n",
    "    axes[0,1].set_title('Sample Efficiency')\n",
    "    axes[0,1].set_xlabel('Total Environment Steps')\n",
    "    axes[0,1].set_ylabel('Episode Reward')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    final_performance = {}\n",
    "    for agent_name, data in results.items():\n",
    "        final_performance[agent_name] = np.mean(data['rewards'][-20:])  # Last 20 episodes\n",
    "    \n",
    "    agent_names = list(final_performance.keys())\n",
    "    performance_values = list(final_performance.values())\n",
    "    \n",
    "    bars = axes[1,0].bar(agent_names, performance_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    axes[1,0].set_title('Final Performance (Last 20 Episodes)')\n",
    "    axes[1,0].set_ylabel('Average Episode Reward')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, performance_values):\n",
    "        axes[1,0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,\n",
    "                      f'{value:.2f}', ha='center', va='bottom')\n",
    "    \n",
    "    steps_to_completion = {}\n",
    "    for agent_name, data in results.items():\n",
    "        steps_to_completion[agent_name] = np.mean(data['steps'][-20:])\n",
    "    \n",
    "    agent_names = list(steps_to_completion.keys())\n",
    "    steps_values = list(steps_to_completion.values())\n",
    "    \n",
    "    bars = axes[1,1].bar(agent_names, steps_values, color=['skyblue', 'lightcoral', 'lightgreen'])\n",
    "    axes[1,1].set_title('Average Steps to Completion')\n",
    "    axes[1,1].set_ylabel('Steps per Episode')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    for bar, value in zip(bars, steps_values):\n",
    "        axes[1,1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "                      f'{value:.1f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return final_performance\n",
    "\n",
    "print(\"ðŸš€ Starting Model-Free vs Model-Based RL Comparison!\")\n",
    "comparison_results = compare_agents_performance()\n",
    "final_performance = visualize_comparison(comparison_results)\n",
    "\n",
    "print(\"\\\\nðŸ“Š Comparison Results Summary:\")\n",
    "for agent_name, performance in final_performance.items():\n",
    "    print(f\"  {agent_name}: {performance:.2f} average reward\")\n",
    "    \n",
    "print(\"\\\\nðŸ’¡ Key Insights:\")\n",
    "print(\"  â€¢ Model-free methods often achieve higher asymptotic performance\")\n",
    "print (\"  â€¢ Model-based methods typically learn faster initially\")\n",
    "print(\"  â€¢ Hybrid approaches can combine benefits of both\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398420da",
   "metadata": {},
   "source": [
    "# Section 2: World Models and Imagination-Based Learning\n",
    "\n",
    "## 2.1 Theoretical Foundations of World Models\n",
    "\n",
    "World models represent learned internal representations of environment dynamics that enable agents to \"imagine\" and plan without direct interaction with the environment.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**World Model Components:**\n",
    "1. **Representation Learning**: Encode high-dimensional observations into compact latent states\n",
    "2. **Dynamics Model**: Predict next latent state given current state and action\n",
    "3. **Reward Model**: Predict rewards in the latent space\n",
    "4. **Decoder Model**: Reconstruct observations from latent states\n",
    "\n",
    "**Mathematical Framework:**\n",
    "- **Encoder**: $z_t = \\text{Encode}(o_t)$ maps observation $o_t$ to latent state $z_t$\n",
    "- **Dynamics**: $z_{t+1} = f(z_t, a_t) + \\epsilon_t$ where $\\epsilon_t \\sim \\mathcal{N}(0, \\Sigma)$\n",
    "- **Reward**: $r_t = R(z_t, a_t)$\n",
    "- **Decoder**: $\\hat{o}_t = \\text{Decode}(z_t)$\n",
    "\n",
    "## 2.2 Variational World Models\n",
    "\n",
    "### Variational Autoencoders (VAE) for World Modeling\n",
    "\n",
    "World models often use VAEs to learn stochastic latent representations:\n",
    "\n",
    "**Encoder (Recognition Model):**\n",
    "$$q_\\phi(z_t | o_t) = \\mathcal{N}(z_t; \\mu_\\phi(o_t), \\sigma_\\phi^2(o_t))$$\n",
    "\n",
    "**Prior (Dynamics Model):**\n",
    "$$p_\\theta(z_{t+1} | z_t, a_t) = \\mathcal{N}(z_{t+1}; \\mu_\\theta(z_t, a_t), \\sigma_\\theta^2(z_t, a_t))$$\n",
    "\n",
    "**Decoder (Generative Model):**\n",
    "$$p_\\psi(o_t | z_t) = \\mathcal{N}(o_t; \\mu_\\psi(z_t), \\sigma_\\psi^2(z_t))$$\n",
    "\n",
    "**ELBO Objective:**\n",
    "$$\\mathcal{L}_{ELBO} = \\mathbb{E}_{q_\\phi(z|o)} [\\log p_\\psi(o|z)] - D_{KL}[q_\\phi(z|o) || p(z)]$$\n",
    "\n",
    "## 2.3 Planning in Learned Latent Space\n",
    "\n",
    "Once a world model is learned, planning can be performed in the compact latent space:\n",
    "\n",
    "### Model Predictive Control (MPC) in Latent Space\n",
    "1. **Imagination Rollout**: Use world model to simulate future trajectories\n",
    "2. **Action Optimization**: Optimize action sequences to maximize predicted rewards\n",
    "3. **Execution**: Execute only the first action, then replan\n",
    "\n",
    "**Planning Objective:**\n",
    "$$a^*_{1:H} = \\arg\\max_{a_{1:H}} \\mathbb{E}_{z_{1:H} \\sim p_\\theta} \\left[ \\sum_{t=1}^H R(z_t, a_t) \\right]$$\n",
    "\n",
    "### Dreamer Algorithm\n",
    "Dreamer combines world models with policy gradients:\n",
    "1. **Collect Experience**: Gather real environment data\n",
    "2. **Learn World Model**: Train VAE-based world model\n",
    "3. **Imagine Trajectories**: Generate synthetic experience in latent space  \n",
    "4. **Learn Behaviors**: Train actor-critic in imagined trajectories\n",
    "\n",
    "## 2.4 Advantages and Challenges\n",
    "\n",
    "### Advantages of World Models:\n",
    "- **Sample Efficiency**: Learn from imagined experience\n",
    "- **Transfer Learning**: Models can generalize across tasks\n",
    "- **Interpretability**: Learned representations can be visualized\n",
    "- **Planning**: Enable sophisticated planning algorithms\n",
    "\n",
    "### Challenges:\n",
    "- **Model Bias**: Errors compound during long rollouts\n",
    "- **Representation Learning**: High-dimensional observations are challenging\n",
    "- **Stochasticity**: Modeling complex stochastic dynamics\n",
    "- **Computational Cost**: Training and maintaining world models\n",
    "\n",
    "## 2.5 Modern Approaches\n",
    "\n",
    "### MuZero\n",
    "Combines tree search with learned models:\n",
    "- Learns value, policy, and dynamics jointly\n",
    "- Uses tree search for planning\n",
    "- Achieves superhuman performance in Go, Chess, and Shogi\n",
    "\n",
    "### Dreamer V2/V3\n",
    "Improvements to original Dreamer:\n",
    "- Better regularization techniques\n",
    "- Improved world model architectures\n",
    "- Enhanced policy learning in imagination\n",
    "\n",
    "### Model-Based Meta-Learning\n",
    "Using world models for few-shot adaptation:\n",
    "- Learn generalizable world model components\n",
    "- Quickly adapt to new environments\n",
    "- Transfer dynamics knowledge across domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6554d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VariationalWorldModel(nn.Module):\n",
    "    \"\"\"VAE-based world model for learning environment dynamics.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, latent_dim=64, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.encoder_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.encoder_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.dynamics = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.dynamics_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.dynamics_logvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        self.reward_model = nn.Sequential(\n",
    "            nn.Linear(latent_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, obs_dim)\n",
    "        )\n",
    "    \n",
    "    def encode(self, obs):\n",
    "        \"\"\"Encode observation to latent distribution parameters.\"\"\"\n",
    "        h = self.encoder(obs)\n",
    "        mu = self.encoder_mu(h)\n",
    "        logvar = self.encoder_logvar(h)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"Reparameterization trick for VAE.\"\"\"\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return mu + eps * std\n",
    "        else:\n",
    "            return mu\n",
    "    \n",
    "    def dynamics_forward(self, latent_state, action):\n",
    "        \"\"\"Predict next latent state given current state and action.\"\"\"\n",
    "        if len(action.shape) == 1:\n",
    "            action_one_hot = F.one_hot(action.long(), self.action_dim).float()\n",
    "        else:\n",
    "            action_one_hot = action\n",
    "        \n",
    "        dynamics_input = torch.cat([latent_state, action_one_hot], dim=-1)\n",
    "        h = self.dynamics(dynamics_input)\n",
    "        \n",
    "        mu = self.dynamics_mu(h)\n",
    "        logvar = self.dynamics_logvar(h)\n",
    "        \n",
    "        return mu, logvar\n",
    "    \n",
    "    def predict_reward(self, latent_state, action):\n",
    "        \"\"\"Predict reward given latent state and action.\"\"\"\n",
    "        if len(action.shape) == 1:\n",
    "            action_one_hot = F.one_hot(action.long(), self.action_dim).float()\n",
    "        else:\n",
    "            action_one_hot = action\n",
    "        \n",
    "        reward_input = torch.cat([latent_state, action_one_hot], dim=-1)\n",
    "        return self.reward_model(reward_input)\n",
    "    \n",
    "    def decode(self, latent_state):\n",
    "        \"\"\"Decode latent state to observation.\"\"\"\n",
    "        return self.decoder(latent_state)\n",
    "    \n",
    "    def forward(self, obs, action=None):\n",
    "        \"\"\"Full forward pass through world model.\"\"\"\n",
    "        mu_enc, logvar_enc = self.encode(obs)\n",
    "        latent_state = self.reparameterize(mu_enc, logvar_enc)\n",
    "        \n",
    "        recon_obs = self.decode(latent_state)\n",
    "        \n",
    "        results = {\n",
    "            'latent_state': latent_state,\n",
    "            'mu_enc': mu_enc,\n",
    "            'logvar_enc': logvar_enc,\n",
    "            'recon_obs': recon_obs\n",
    "        }\n",
    "        \n",
    "        if action is not None:\n",
    "            mu_dyn, logvar_dyn = self.dynamics_forward(latent_state, action)\n",
    "            next_latent = self.reparameterize(mu_dyn, logvar_dyn)\n",
    "            pred_reward = self.predict_reward(latent_state, action)\n",
    "            \n",
    "            results.update({\n",
    "                'mu_dyn': mu_dyn,\n",
    "                'logvar_dyn': logvar_dyn,\n",
    "                'next_latent': next_latent,\n",
    "                'pred_reward': pred_reward\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def imagine_trajectory(self, initial_obs, actions):\n",
    "        \"\"\"Imagine trajectory given initial observation and action sequence.\"\"\"\n",
    "        batch_size = initial_obs.shape[0]\n",
    "        sequence_length = len(actions)\n",
    "        \n",
    "        mu_enc, logvar_enc = self.encode(initial_obs)\n",
    "        current_latent = self.reparameterize(mu_enc, logvar_enc)\n",
    "        \n",
    "        trajectory = {\n",
    "            'latent_states': [current_latent],\n",
    "            'observations': [self.decode(current_latent)],\n",
    "            'rewards': [],\n",
    "            'actions': []\n",
    "        }\n",
    "        \n",
    "        for t in range(sequence_length):\n",
    "            action = actions[t]\n",
    "            trajectory['actions'].append(action)\n",
    "            \n",
    "            pred_reward = self.predict_reward(current_latent, action)\n",
    "            trajectory['rewards'].append(pred_reward)\n",
    "            \n",
    "            mu_dyn, logvar_dyn = self.dynamics_forward(current_latent, action)\n",
    "            next_latent = self.reparameterize(mu_dyn, logvar_dyn)\n",
    "            \n",
    "            current_latent = next_latent\n",
    "            trajectory['latent_states'].append(current_latent)\n",
    "            trajectory['observations'].append(self.decode(current_latent))\n",
    "        \n",
    "        return trajectory\n",
    "\n",
    "class WorldModelLoss:\n",
    "    \"\"\"Loss functions for training world models.\"\"\"\n",
    "    \n",
    "    def __init__(self, recon_weight=1.0, kl_weight=1.0, reward_weight=1.0, dynamics_weight=1.0):\n",
    "        self.recon_weight = recon_weight\n",
    "        self.kl_weight = kl_weight\n",
    "        self.reward_weight = reward_weight\n",
    "        self.dynamics_weight = dynamics_weight\n",
    "    \n",
    "    def reconstruction_loss(self, recon_obs, target_obs):\n",
    "        \"\"\"Reconstruction loss between predicted and actual observations.\"\"\"\n",
    "        return F.mse_loss(recon_obs, target_obs)\n",
    "    \n",
    "    def kl_divergence_loss(self, mu, logvar):\n",
    "        \"\"\"KL divergence loss for VAE regularization.\"\"\"\n",
    "        return -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / mu.shape[0]\n",
    "    \n",
    "    def reward_loss(self, pred_reward, target_reward):\n",
    "        \"\"\"Reward prediction loss.\"\"\"\n",
    "        return F.mse_loss(pred_reward.squeeze(), target_reward)\n",
    "    \n",
    "    def dynamics_loss(self, pred_next_latent, target_next_latent):\n",
    "        \"\"\"Dynamics prediction loss in latent space.\"\"\"\n",
    "        return F.mse_loss(pred_next_latent, target_next_latent)\n",
    "    \n",
    "    def compute_total_loss(self, model_output, target_obs, target_reward=None, target_next_obs=None):\n",
    "        \"\"\"Compute total loss for world model training.\"\"\"\n",
    "        losses = {}\n",
    "        \n",
    "        recon_loss = self.reconstruction_loss(model_output['recon_obs'], target_obs)\n",
    "        losses['reconstruction'] = recon_loss\n",
    "        \n",
    "        kl_loss = self.kl_divergence_loss(model_output['mu_enc'], model_output['logvar_enc'])\n",
    "        losses['kl_divergence'] = kl_loss\n",
    "        \n",
    "        total_loss = self.recon_weight * recon_loss + self.kl_weight * kl_loss\n",
    "        \n",
    "        if target_reward is not None and 'pred_reward' in model_output:\n",
    "            reward_loss = self.reward_loss(model_output['pred_reward'], target_reward)\n",
    "            losses['reward'] = reward_loss\n",
    "            total_loss += self.reward_weight * reward_loss\n",
    "        \n",
    "        if target_next_obs is not None and 'mu_dyn' in model_output:\n",
    "            with torch.no_grad():\n",
    "                target_mu, _ = model_output['mu_enc'], model_output['logvar_enc']  # Placeholder - need next obs encoding\n",
    "            \n",
    "            dynamics_loss = F.mse_loss(model_output['mu_dyn'], model_output['next_latent'])\n",
    "            losses['dynamics'] = dynamics_loss\n",
    "            total_loss += self.dynamics_weight * dynamics_loss\n",
    "        \n",
    "        losses['total'] = total_loss\n",
    "        return losses\n",
    "\n",
    "class ImaginationBasedAgent:\n",
    "    \"\"\"Agent that uses world model for planning and learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, latent_dim=64, planning_horizon=8):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.planning_horizon = planning_horizon\n",
    "        \n",
    "        self.world_model = VariationalWorldModel(obs_dim, action_dim, latent_dim)\n",
    "        self.world_model_optimizer = optim.Adam(self.world_model.parameters(), lr=1e-3)\n",
    "        self.world_model_loss = WorldModelLoss()\n",
    "        \n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=1e-3)\n",
    "        self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=1e-3)\n",
    "        \n",
    "        self.experience_buffer = ReplayBuffer(10000)\n",
    "        \n",
    "        self.world_model_losses = []\n",
    "        self.policy_losses = []\n",
    "        \n",
    "    def act(self, obs, epsilon=0.1):\n",
    "        \"\"\"Select action using imagination-based planning.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        return self.plan_with_world_model(obs)\n",
    "    \n",
    "    def plan_with_world_model(self, obs):\n",
    "        \"\"\"Plan action using world model imagination.\"\"\"\n",
    "        obs_tensor = torch.FloatTensor(obs).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu, logvar = self.world_model.encode(obs_tensor)\n",
    "            current_latent = self.world_model.reparameterize(mu, logvar)\n",
    "        \n",
    "        best_action = 0\n",
    "        best_value = float('-inf')\n",
    "        \n",
    "        for action in range(self.action_dim):\n",
    "            imagined_value = self.imagine_value(current_latent, action)\n",
    "            if imagined_value > best_value:\n",
    "                best_value = imagined_value\n",
    "                best_action = action\n",
    "        \n",
    "        return best_action\n",
    "    \n",
    "    def imagine_value(self, initial_latent, initial_action):\n",
    "        \"\"\"Estimate value of taking initial action using imagination.\"\"\"\n",
    "        current_latent = initial_latent\n",
    "        total_value = 0.0\n",
    "        gamma = 0.99\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step in range(self.planning_horizon):\n",
    "                if step == 0:\n",
    "                    action = initial_action\n",
    "                else:\n",
    "                    action_probs = self.policy_net(current_latent)\n",
    "                    action = action_probs.argmax().item()\n",
    "                \n",
    "                action_tensor = torch.tensor([action])\n",
    "                pred_reward = self.world_model.predict_reward(current_latent, action_tensor)\n",
    "                total_value += (gamma ** step) * pred_reward.item()\n",
    "                \n",
    "                mu_dyn, logvar_dyn = self.world_model.dynamics_forward(current_latent, action_tensor)\n",
    "                current_latent = self.world_model.reparameterize(mu_dyn, logvar_dyn)\n",
    "            \n",
    "            terminal_value = self.value_net(current_latent)\n",
    "            total_value += (gamma ** self.planning_horizon) * terminal_value.item()\n",
    "        \n",
    "        return total_value\n",
    "    \n",
    "    def update_world_model(self, batch_size=32):\n",
    "        \"\"\"Update world model from experience buffer.\"\"\"\n",
    "        if len(self.experience_buffer) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.experience_buffer.sample(batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        \n",
    "        model_output = self.world_model(states, actions)\n",
    "        \n",
    "        losses = self.world_model_loss.compute_total_loss(\n",
    "            model_output, states, rewards, next_states\n",
    "        )\n",
    "        \n",
    "        self.world_model_optimizer.zero_grad()\n",
    "        losses['total'].backward()\n",
    "        self.world_model_optimizer.step()\n",
    "        \n",
    "        self.world_model_losses.append(losses['total'].item())\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def update_policy_with_imagination(self, num_imagination_episodes=10):\n",
    "        \"\"\"Update policy using imagined trajectories.\"\"\"\n",
    "        if len(self.experience_buffer) == 0:\n",
    "            return None\n",
    "        \n",
    "        total_policy_loss = 0.0\n",
    "        total_value_loss = 0.0\n",
    "        \n",
    "        for _ in range(num_imagination_episodes):\n",
    "            states, _, _, _, _ = self.experience_buffer.sample(1)\n",
    "            initial_obs = torch.FloatTensor(states[0]).unsqueeze(0)\n",
    "            \n",
    "            actions = [torch.randint(0, self.action_dim, (1,)) for _ in range(self.planning_horizon)]\n",
    "            \n",
    "            trajectory = self.world_model.imagine_trajectory(initial_obs, actions)\n",
    "            \n",
    "            policy_loss = 0.0\n",
    "            value_loss = 0.0\n",
    "            \n",
    "            for t, (latent_state, action, reward) in enumerate(zip(\n",
    "                trajectory['latent_states'][:-1], \n",
    "                trajectory['actions'], \n",
    "                trajectory['rewards']\n",
    "            )):\n",
    "                action_probs = self.policy_net(latent_state)\n",
    "                log_prob = torch.log(action_probs.gather(1, action.unsqueeze(1)))\n",
    "                policy_loss -= log_prob * reward.detach()\n",
    "                \n",
    "                value_pred = self.value_net(latent_state)\n",
    "                value_loss += F.mse_loss(value_pred, reward.detach())\n",
    "            \n",
    "            total_policy_loss += policy_loss.item()\n",
    "            total_value_loss += value_loss.item()\n",
    "            \n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward(retain_graph=True)\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            self.value_optimizer.step()\n",
    "        \n",
    "        avg_policy_loss = total_policy_loss / num_imagination_episodes\n",
    "        avg_value_loss = total_value_loss / num_imagination_episodes\n",
    "        \n",
    "        self.policy_losses.append(avg_policy_loss)\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': avg_policy_loss,\n",
    "            'value_loss': avg_value_loss\n",
    "        }\n",
    "\n",
    "def demonstrate_world_model_learning():\n",
    "    \"\"\"Demonstrate world model learning and imagination-based planning.\"\"\"\n",
    "    print(\"ðŸŒ Demonstrating World Model Learning and Imagination\")\n",
    "    \n",
    "    env = SimpleGridWorld(size=4)\n",
    "    \n",
    "    agent = ImaginationBasedAgent(obs_dim=2, action_dim=4, latent_dim=16, planning_horizon=5)\n",
    "    \n",
    "    num_episodes = 150\n",
    "    batch_size = 16\n",
    "    world_model_updates = 5\n",
    "    imagination_updates = 3\n",
    "    \n",
    "    episode_rewards = []\n",
    "    world_model_loss_history = []\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        max_steps = 50\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            epsilon = max(0.1, 1.0 - episode / 80)\n",
    "            action = agent.act(state, epsilon=epsilon)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "            \n",
    "            agent.experience_buffer.push(state, action, reward, next_state, done)\n",
    "            \n",
    "            if len(agent.experience_buffer) > batch_size:\n",
    "                for _ in range(world_model_updates):\n",
    "                    losses = agent.update_world_model(batch_size)\n",
    "                    if losses:\n",
    "                        world_model_loss_history.append(losses['total'].item())\n",
    "            \n",
    "            if episode > 20 and len(agent.experience_buffer) > batch_size:\n",
    "                agent.update_policy_with_imagination(imagination_updates)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        if episode % 30 == 0 and episode > 0:\n",
    "            recent_reward = np.mean(episode_rewards[-10:])\n",
    "            recent_wm_loss = np.mean(world_model_loss_history[-50:]) if world_model_loss_history else 0\n",
    "            print(f\"Episode {episode}: Avg Reward: {recent_reward:.2f}, WM Loss: {recent_wm_loss:.4f}\")\n",
    "    \n",
    "    return agent, episode_rewards, world_model_loss_history\n",
    "\n",
    "def visualize_world_model_performance(agent, episode_rewards, world_model_losses):\n",
    "    \"\"\"Visualize world model learning performance.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    window_size = 10\n",
    "    if len(episode_rewards) >= window_size:\n",
    "        smoothed_rewards = pd.Series(episode_rewards).rolling(window_size).mean()\n",
    "        axes[0,0].plot(smoothed_rewards, linewidth=2, color='blue')\n",
    "    else:\n",
    "        axes[0,0].plot(episode_rewards, linewidth=2, color='blue')\n",
    "    \n",
    "    axes[0,0].set_title('Learning Curve (Episode Rewards)')\n",
    "    axes[0,0].set_xlabel('Episode')\n",
    "    axes[0,0].set_ylabel('Episode Reward')\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    if world_model_losses:\n",
    "        axes[0,1].plot(world_model_losses, linewidth=1, alpha=0.7, color='red')\n",
    "        if len(world_model_losses) >= 20:\n",
    "            smoothed_losses = pd.Series(world_model_losses).rolling(20).mean()\n",
    "            axes[0,1].plot(smoothed_losses, linewidth=2, color='darkred')\n",
    "    \n",
    "    axes[0,1].set_title('World Model Training Loss')\n",
    "    axes[0,1].set_xlabel('Update Step')\n",
    "    axes[0,1].set_ylabel('Loss')\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    if len(agent.experience_buffer) > 50:\n",
    "        states, _, _, _, _ = agent.experience_buffer.sample(50)\n",
    "        states_tensor = torch.FloatTensor(states)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            mu, _ = agent.world_model.encode(states_tensor)\n",
    "            latent_states = mu.numpy()\n",
    "        \n",
    "        if latent_states.shape[1] >= 2:\n",
    "            axes[1,0].scatter(latent_states[:, 0], latent_states[:, 1], alpha=0.6, c=range(len(latent_states)))\n",
    "            axes[1,0].set_title('Learned Latent Space Representation')\n",
    "            axes[1,0].set_xlabel('Latent Dimension 1')\n",
    "            axes[1,0].set_ylabel('Latent Dimension 2')\n",
    "            axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    planning_horizons = [1, 3, 5, 8, 10]\n",
    "    planning_performance = []\n",
    "    \n",
    "    for horizon in planning_horizons:\n",
    "        test_agent = ImaginationBasedAgent(obs_dim=2, action_dim=4, planning_horizon=horizon)\n",
    "        test_agent.world_model = agent.world_model  # Use trained world model\n",
    "        test_agent.policy_net = agent.policy_net    # Use trained policy\n",
    "        test_agent.value_net = agent.value_net      # Use trained value function\n",
    "        \n",
    "        test_env = SimpleGridWorld(size=4)\n",
    "        test_rewards = []\n",
    "        \n",
    "        for _ in range(10):  # Quick test\n",
    "            state = test_env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            for _ in range(20):\n",
    "                action = test_agent.plan_with_world_model(state)\n",
    "                next_state, reward, done, _ = test_env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "            \n",
    "            test_rewards.append(episode_reward)\n",
    "        \n",
    "        planning_performance.append(np.mean(test_rewards))\n",
    "    \n",
    "    axes[1,1].plot(planning_horizons, planning_performance, 'o-', linewidth=2, markersize=8)\n",
    "    axes[1,1].set_title('Planning Horizon vs Performance')\n",
    "    axes[1,1].set_xlabel('Planning Horizon')\n",
    "    axes[1,1].set_ylabel('Average Reward')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return planning_performance\n",
    "\n",
    "print(\"ðŸš€ Starting World Model Learning Demonstration!\")\n",
    "trained_agent, episode_rewards, wm_losses = demonstrate_world_model_learning()\n",
    "planning_analysis = visualize_world_model_performance(trained_agent, episode_rewards, wm_losses)\n",
    "\n",
    "print(\"\\nðŸŒ World Model Learning Results:\")\n",
    "print(f\"  â€¢ Final average reward: {np.mean(episode_rewards[-10:]):.2f}\")\n",
    "print(f\"  â€¢ World model converged to loss: {np.mean(wm_losses[-20:]):.4f}\")\n",
    "print(f\"  â€¢ Optimal planning horizon: {[1,3,5,8,10][np.argmax(planning_analysis)]}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights from World Model Learning:\")\n",
    "print(\"  â€¢ World models enable sample-efficient learning through imagination\")\n",
    "print(\"  â€¢ Planning horizon affects performance - too short lacks foresight, too long accumulates errors\")\n",
    "print(\"  â€¢ Learned latent representations capture environment structure\")\n",
    "print(\"  â€¢ Imagination-based policy updates improve without real environment interaction\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2799c6",
   "metadata": {},
   "source": [
    "# Section 3: Sample Efficiency and Transfer Learning\n",
    "\n",
    "## 3.1 Sample Efficiency Challenges in Deep RL\n",
    "\n",
    "Sample efficiency is one of the most critical challenges in deep reinforcement learning, particularly for real-world applications where data collection is expensive or dangerous.\n",
    "\n",
    "### Why is Sample Efficiency Important?\n",
    "\n",
    "**Real-World Constraints:**\n",
    "- **Cost**: Real-world interactions can be expensive (robotics, autonomous vehicles)\n",
    "- **Time**: Learning from millions of samples is often impractical\n",
    "- **Safety**: Exploratory actions in safety-critical domains can be dangerous\n",
    "- **Reproducibility**: Limited samples make experiments more reliable\n",
    "\n",
    "**Sample Complexity Factors:**\n",
    "- **Environment Complexity**: High-dimensional state/action spaces\n",
    "- **Sparse Rewards**: Learning signals are infrequent\n",
    "- **Stochasticity**: Environmental noise requires more samples\n",
    "- **Exploration**: Discovering good policies requires extensive exploration\n",
    "\n",
    "## 3.2 Sample Efficiency Techniques\n",
    "\n",
    "### 3.2.1 Experience Replay and Prioritization\n",
    "\n",
    "**Experience Replay Benefits:**\n",
    "- Reuse past experiences multiple times\n",
    "- Break temporal correlations in data\n",
    "- Enable off-policy learning\n",
    "\n",
    "**Prioritized Experience Replay:**\n",
    "Prioritize experiences based on temporal difference (TD) error:\n",
    "$$P(i) = \\frac{p_i^\\alpha}{\\sum_k p_k^\\alpha}$$\n",
    "\n",
    "Where $p_i = |\\delta_i| + \\epsilon$ and $\\delta_i$ is the TD error.\n",
    "\n",
    "### 3.2.2 Data Augmentation\n",
    "\n",
    "**Techniques:**\n",
    "- **Random Crops**: For image-based environments\n",
    "- **Color Jittering**: Robust to lighting variations  \n",
    "- **Random Shifts**: Translation invariance\n",
    "- **Gaussian Noise**: Regularization effect\n",
    "\n",
    "### 3.2.3 Auxiliary Tasks\n",
    "\n",
    "Learn multiple tasks simultaneously to improve sample efficiency:\n",
    "- **Pixel Control**: Predict pixel changes\n",
    "- **Feature Control**: Control learned feature representations\n",
    "- **Reward Prediction**: Predict future rewards\n",
    "- **Value Function Replay**: Replay value function updates\n",
    "\n",
    "## 3.3 Transfer Learning in Reinforcement Learning\n",
    "\n",
    "Transfer learning enables agents to leverage knowledge from previous tasks to learn new tasks more efficiently.\n",
    "\n",
    "### 3.3.1 Types of Transfer in RL\n",
    "\n",
    "**Policy Transfer:**\n",
    "$$\\pi_{target}(a|s) = f(\\pi_{source}(a|s), s, \\theta_{adapt})$$\n",
    "\n",
    "**Value Function Transfer:**\n",
    "$$Q_{target}(s,a) = g(Q_{source}(s,a), s, a, \\phi_{adapt})$$\n",
    "\n",
    "**Representation Transfer:**\n",
    "$$\\phi_{target}(s) = h(\\phi_{source}(s), \\psi_{adapt})$$\n",
    "\n",
    "### 3.3.2 Transfer Learning Approaches\n",
    "\n",
    "#### Fine-tuning\n",
    "1. Pre-train on source task\n",
    "2. Initialize target model with source weights\n",
    "3. Fine-tune on target task with lower learning rate\n",
    "\n",
    "#### Progressive Networks\n",
    "- Freeze source network columns\n",
    "- Add new columns for target tasks\n",
    "- Use lateral connections between columns\n",
    "\n",
    "#### Universal Value Functions (UVF)\n",
    "Learn value functions conditioned on goals:\n",
    "$$Q(s, a, g) = \\text{Value of action } a \\text{ in state } s \\text{ for goal } g$$\n",
    "\n",
    "## 3.4 Meta-Learning and Few-Shot Adaptation\n",
    "\n",
    "Meta-learning enables agents to quickly adapt to new tasks with limited experience.\n",
    "\n",
    "### 3.4.1 Model-Agnostic Meta-Learning (MAML)\n",
    "\n",
    "**Objective:**\n",
    "$$\\min_\\theta \\sum_{\\tau \\sim p(\\mathcal{T})} \\mathcal{L}_\\tau(f_{\\theta_\\tau'})$$\n",
    "\n",
    "Where $\\theta_\\tau' = \\theta - \\alpha \\nabla_\\theta \\mathcal{L}_\\tau(f_\\theta)$\n",
    "\n",
    "**MAML Algorithm:**\n",
    "1. Sample batch of tasks\n",
    "2. For each task, compute adapted parameters via gradient descent\n",
    "3. Update meta-parameters using gradient through adaptation process\n",
    "\n",
    "### 3.4.2 Gradient-Based Meta-Learning\n",
    "\n",
    "**Reptile Algorithm:**\n",
    "Simpler alternative to MAML:\n",
    "$$\\theta \\leftarrow \\theta + \\beta \\frac{1}{n} \\sum_{i=1}^n (\\phi_i - \\theta)$$\n",
    "\n",
    "Where $\\phi_i$ is the result of training on task $i$.\n",
    "\n",
    "## 3.5 Domain Adaptation and Sim-to-Real Transfer\n",
    "\n",
    "### 3.5.1 Domain Randomization\n",
    "\n",
    "**Technique:**\n",
    "Randomize simulation parameters during training:\n",
    "- Physical properties (mass, friction, damping)\n",
    "- Visual appearance (textures, lighting, colors)\n",
    "- Sensor characteristics (noise, resolution, field of view)\n",
    "\n",
    "**Benefits:**\n",
    "- Learned policies are robust to domain variations\n",
    "- Improved transfer from simulation to real world\n",
    "- Reduced need for domain-specific engineering\n",
    "\n",
    "### 3.5.2 Domain Adversarial Training\n",
    "\n",
    "**Objective:**\n",
    "$$\\min_\\theta \\mathcal{L}_{task}(\\theta) + \\lambda \\mathcal{L}_{domain}(\\theta)$$\n",
    "\n",
    "Where $\\mathcal{L}_{domain}$ encourages domain-invariant features.\n",
    "\n",
    "## 3.6 Curriculum Learning\n",
    "\n",
    "Structure learning to progress from simple to complex tasks.\n",
    "\n",
    "### 3.6.1 Curriculum Design Principles\n",
    "\n",
    "**Manual Curriculum:**\n",
    "- Hand-designed progression of tasks\n",
    "- Expert knowledge of difficulty ordering\n",
    "- Fixed curriculum regardless of agent performance\n",
    "\n",
    "**Automatic Curriculum:**\n",
    "- Adaptive task selection based on agent performance\n",
    "- Learning progress as curriculum signal\n",
    "- Self-paced learning approaches\n",
    "\n",
    "### 3.6.2 Curriculum Learning Algorithms\n",
    "\n",
    "**Teacher-Student Framework:**\n",
    "- Teacher selects appropriate tasks for student\n",
    "- Task difficulty based on student's current capability\n",
    "- Optimize task selection for maximum learning progress\n",
    "\n",
    "**Self-Play Curriculum:**\n",
    "- Agent plays against previous versions of itself\n",
    "- Automatic difficulty adjustment\n",
    "- Prevents catastrophic forgetting of simpler strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized experience replay buffer for improved sample efficiency.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=1e-4):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # Priority exponent\n",
    "        self.beta = beta    # Importance sampling exponent\n",
    "        self.beta_increment = beta_increment\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.max_priority = 1.0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience to buffer with maximum priority.\"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.priorities[self.position] = self.max_priority\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch with prioritized sampling.\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        valid_priorities = self.priorities[:len(self.buffer)]\n",
    "        probs = valid_priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        \n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"Update priorities based on TD errors.\"\"\"\n",
    "        for idx, td_error in zip(indices, td_errors):\n",
    "            priority = (abs(td_error) + 1e-6) ** self.alpha\n",
    "            self.priorities[idx] = priority\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DataAugmentationDQN(nn.Module):\n",
    "    \"\"\"DQN with data augmentation for improved sample efficiency.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.reward_predictor = nn.Sequential(\n",
    "            nn.Linear(input_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.next_state_predictor = nn.Sequential(\n",
    "            nn.Linear(input_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action=None):\n",
    "        \"\"\"Forward pass with optional auxiliary predictions.\"\"\"\n",
    "        q_values = self.q_network(state)\n",
    "        \n",
    "        if action is not None:\n",
    "            if len(action.shape) == 1:\n",
    "                action_one_hot = F.one_hot(action.long(), self.action_dim).float()\n",
    "            else:\n",
    "                action_one_hot = action\n",
    "            \n",
    "            aux_input = torch.cat([state, action_one_hot], dim=-1)\n",
    "            reward_pred = self.reward_predictor(aux_input)\n",
    "            next_state_pred = self.next_state_predictor(aux_input)\n",
    "            \n",
    "            return q_values, reward_pred, next_state_pred\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def apply_augmentation(self, state, augmentation_type='noise'):\n",
    "        \"\"\"Apply data augmentation to state.\"\"\"\n",
    "        if augmentation_type == 'noise':\n",
    "            noise = torch.randn_like(state) * 0.1\n",
    "            return state + noise\n",
    "        \n",
    "        elif augmentation_type == 'dropout':\n",
    "            dropout_mask = torch.rand_like(state) > 0.1\n",
    "            return state * dropout_mask.float()\n",
    "        \n",
    "        elif augmentation_type == 'scaling':\n",
    "            scale = torch.rand(1).item() * 0.4 + 0.8  # Scale between 0.8 and 1.2\n",
    "            return state * scale\n",
    "        \n",
    "        return state\n",
    "\n",
    "class SampleEfficientAgent:\n",
    "    \"\"\"Agent with multiple sample efficiency techniques.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.network = DataAugmentationDQN(state_dim, action_dim)\n",
    "        self.target_network = copy.deepcopy(self.network)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        self.replay_buffer = PrioritizedReplayBuffer(capacity=10000)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.target_update_freq = 100\n",
    "        self.update_count = 0\n",
    "        \n",
    "        self.aux_reward_weight = 0.1\n",
    "        self.aux_dynamics_weight = 0.1\n",
    "        \n",
    "        self.losses = []\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def act(self, state, epsilon=0.1):\n",
    "        \"\"\"Select action with epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def update(self, batch_size=32, use_aux_tasks=True, augmentation=True):\n",
    "        \"\"\"Update agent with prioritized replay and auxiliary tasks.\"\"\"\n",
    "        sample_result = self.replay_buffer.sample(batch_size)\n",
    "        if sample_result is None:\n",
    "            return None\n",
    "        \n",
    "        experiences, indices, weights = sample_result\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        \n",
    "        if augmentation:\n",
    "            aug_type = np.random.choice(['noise', 'dropout', 'scaling'])\\n            states = self.network.apply_augmentation(states, aug_type)\n",
    "            next_states = self.network.apply_augmentation(next_states, aug_type)\n",
    "        \n",
    "        current_q_values = self.network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "        \n",
    "        td_errors = (current_q_values.squeeze() - target_q_values).detach().numpy()\n",
    "        \n",
    "        q_loss = (weights * F.mse_loss(current_q_values.squeeze(), target_q_values, reduction='none')).mean()\n",
    "        \n",
    "        total_loss = q_loss\n",
    "        \n",
    "        if use_aux_tasks:\n",
    "            q_values, reward_pred, next_state_pred = self.network(states, actions)\n",
    "            \n",
    "            aux_reward_loss = F.mse_loss(reward_pred.squeeze(), rewards)\n",
    "            aux_dynamics_loss = F.mse_loss(next_state_pred, next_states)\n",
    "            \n",
    "            total_loss += self.aux_reward_weight * aux_reward_loss\n",
    "            total_loss += self.aux_dynamics_weight * aux_dynamics_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.replay_buffer.update_priorities(indices, td_errors)\n",
    "        \n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        \n",
    "        self.losses.append(total_loss.item())\n",
    "        self.td_errors.extend(td_errors.tolist())\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'q_loss': q_loss.item(),\n",
    "            'aux_reward_loss': aux_reward_loss.item() if use_aux_tasks else 0,\n",
    "            'aux_dynamics_loss': aux_dynamics_loss.item() if use_aux_tasks else 0\n",
    "        }\n",
    "\n",
    "class TransferLearningAgent:\n",
    "    \"\"\"Agent with transfer learning capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policy_heads = {}\n",
    "        self.value_heads = {}\n",
    "        \n",
    "        self.feature_optimizer = optim.Adam(self.feature_extractor.parameters(), lr=lr)\n",
    "        self.head_optimizers = {}\n",
    "        \n",
    "        self.transfer_performance = {}\n",
    "    \n",
    "    def add_task(self, task_name, action_dim=None):\n",
    "        \"\"\"Add a new task with its own policy and value heads.\"\"\"\n",
    "        if action_dim is None:\n",
    "            action_dim = self.action_dim\n",
    "        \n",
    "        self.policy_heads[task_name] = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.value_heads[task_name] = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        task_params = list(self.policy_heads[task_name].parameters()) + \\\n",
    "                     list(self.value_heads[task_name].parameters())\n",
    "        self.head_optimizers[task_name] = optim.Adam(task_params, lr=1e-3)\n",
    "        \n",
    "        self.transfer_performance[task_name] = []\n",
    "    \n",
    "    def get_action(self, state, task_name):\n",
    "        \"\"\"Get action for specific task.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            features = self.feature_extractor(state_tensor)\n",
    "            action_probs = self.policy_heads[task_name](features)\n",
    "            return Categorical(action_probs).sample().item()\n",
    "    \n",
    "    def get_value(self, state, task_name):\n",
    "        \"\"\"Get value estimate for specific task.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            features = self.feature_extractor(state_tensor)\n",
    "            return self.value_heads[task_name](features).item()\n",
    "    \n",
    "    def update(self, states, actions, rewards, task_name, update_features=True):\n",
    "        \"\"\"Update agent for specific task.\"\"\"\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        \n",
    "        features = self.feature_extractor(states)\n",
    "        action_probs = self.policy_heads[task_name](features)\n",
    "        values = self.value_heads[task_name](features).squeeze()\n",
    "        \n",
    "        log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1))).squeeze()\n",
    "        advantages = rewards - values.detach()\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        value_loss = F.mse_loss(values, rewards)\n",
    "        \n",
    "        total_loss = policy_loss + 0.5 * value_loss\n",
    "        \n",
    "        self.head_optimizers[task_name].zero_grad()\n",
    "        if update_features:\n",
    "            self.feature_optimizer.zero_grad()\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        self.head_optimizers[task_name].step()\n",
    "        if update_features:\n",
    "            self.feature_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item()\n",
    "        }\n",
    "    \n",
    "    def fine_tune_for_task(self, source_task, target_task, fine_tune_lr=1e-4):\n",
    "        \"\"\"Fine-tune from source task to target task.\"\"\"\n",
    "        self.policy_heads[target_task] = copy.deepcopy(self.policy_heads[source_task])\n",
    "        self.value_heads[target_task] = copy.deepcopy(self.value_heads[source_task])\n",
    "        \n",
    "        task_params = list(self.policy_heads[target_task].parameters()) + \\\n",
    "                     list(self.value_heads[target_task].parameters())\n",
    "        self.head_optimizers[target_task] = optim.Adam(task_params, lr=fine_tune_lr)\n",
    "        \n",
    "        self.transfer_performance[target_task] = []\n",
    "\n",
    "class CurriculumLearningFramework:\n",
    "    \"\"\"Framework for curriculum learning with automatic difficulty adjustment.\"\"\"\n",
    "    \n",
    "    def __init__(self, environments, agent, difficulty_measure='success_rate'):\n",
    "        self.environments = environments  # List of environments with increasing difficulty\n",
    "        self.agent = agent\n",
    "        self.difficulty_measure = difficulty_measure\n",
    "        \n",
    "        self.current_level = 0\n",
    "        self.level_performance = [[] for _ in environments]\n",
    "        self.progression_threshold = 0.8  # Success rate threshold to advance\n",
    "        self.regression_threshold = 0.3   # Success rate threshold to regress\n",
    "        \n",
    "        self.curriculum_history = []\n",
    "    \n",
    "    def get_current_environment(self):\n",
    "        \"\"\"Get current environment based on curriculum level.\"\"\"\n",
    "        return self.environments[self.current_level]\n",
    "    \n",
    "    def evaluate_performance(self, episode_rewards, episode_successes=None):\n",
    "        \"\"\"Evaluate performance on current level.\"\"\"\n",
    "        if self.difficulty_measure == 'success_rate' and episode_successes is not None:\n",
    "            return np.mean(episode_successes[-10:]) if len(episode_successes) >= 10 else 0\n",
    "        elif self.difficulty_measure == 'reward':\n",
    "            return np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else 0\n",
    "        else:\n",
    "            return np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else 0\n",
    "    \n",
    "    def update_curriculum(self, performance):\n",
    "        \"\"\"Update curriculum level based on performance.\"\"\"\n",
    "        old_level = self.current_level\n",
    "        \n",
    "        if performance >= self.progression_threshold and self.current_level < len(self.environments) - 1:\n",
    "            self.current_level += 1\n",
    "            print(f\\\"ðŸ“ˆ Advanced to level {self.current_level} (performance: {performance:.2f})\\\"\n",
    "        \n",
    "        elif performance < self.regression_threshold and self.current_level > 0:\n",
    "            self.current_level = max(0, self.current_level - 1)\n",
    "            print(f\\\"ðŸ“‰ Regressed to level {self.current_level} (performance: {performance:.2f})\\\"\n",
    "        \n",
    "        if old_level != self.current_level:\n",
    "            self.curriculum_history.append({\n",
    "                'episode': len(self.level_performance[old_level]),\n",
    "                'old_level': old_level,\n",
    "                'new_level': self.current_level,\n",
    "                'performance': performance\n",
    "            })\n",
    "        \n",
    "        return self.current_level != old_level\n",
    "    \n",
    "    def train_with_curriculum(self, num_episodes=1000):\n",
    "        \"\"\"Train agent using curriculum learning.\"\"\"\n",
    "        episode_rewards = []\n",
    "        episode_successes = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            env = self.get_current_environment()\n",
    "            \n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_success = False\n",
    "            \n",
    "            for step in range(100):  # Max episode length\n",
    "                action = self.agent.act(state, epsilon=max(0.1, 1.0 - episode/500))\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                self.agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                if done and reward > 5:  # Define success condition\n",
    "                    episode_success = True\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            if len(self.agent.replay_buffer) > 32:\n",
    "                self.agent.update(32)\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_successes.append(episode_success)\n",
    "            self.level_performance[self.current_level].append(episode_reward)\n",
    "            \n",
    "            if episode % 20 == 0:\n",
    "                performance = self.evaluate_performance(episode_rewards, episode_successes)\n",
    "                self.update_curriculum(performance)\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                recent_reward = np.mean(episode_rewards[-10:])\n",
    "                recent_success = np.mean(episode_successes[-10:])\n",
    "                print(f\\\"Episode {episode}: Level {self.current_level}, \\\"\n",
    "                      f\\\"Reward: {recent_reward:.2f}, Success: {recent_success:.2f}\\\"\n",
    "        \n",
    "        return episode_rewards, episode_successes\n",
    "\n",
    "def compare_sample_efficiency():\n",
    "    \\\"\\\"\\\"Compare sample efficiency of different techniques.\\\"\\\"\\\"\n",
    "    print(\\\"âš¡ Comparing Sample Efficiency Techniques\\\")\n",
    "    \n",
    "    env = SimpleGridWorld(size=6)\n",
    "    \n",
    "    baseline_agent = DQNAgent(state_dim=2, action_dim=4)\n",
    "    efficient_agent = SampleEfficientAgent(state_dim=2, action_dim=4)\n",
    "    \n",
    "    agents = {\n",
    "        'Baseline DQN': baseline_agent,\n",
    "        'Sample Efficient': efficient_agent\n",
    "    }\n",
    "    \n",
    "    results = {name: {'rewards': [], 'episodes': []} for name in agents.keys()}\n",
    "    \n",
    "    num_episodes = 300\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        for agent_name, agent in agents.items():\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            for step in range(50):\n",
    "                action = agent.act(state, epsilon=max(0.1, 1.0 - episode/200))\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if agent_name == 'Baseline DQN':\n",
    "                    agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                    if len(agent.replay_buffer) > 32:\n",
    "                        batch = agent.replay_buffer.sample(32)\n",
    "                        agent.update(batch)\n",
    "                else:\n",
    "                    agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                    if len(agent.replay_buffer) > 32:\n",
    "                        agent.update(32)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            results[agent_name]['rewards'].append(episode_reward)\n",
    "            results[agent_name]['episodes'].append(episode)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def demonstrate_transfer_learning():\n",
    "    \\\"\\\"\\\"Demonstrate transfer learning between related tasks.\\\"\\\"\\\"\n",
    "    print(\\\"ðŸ”„ Demonstrating Transfer Learning\\\")\n",
    "    \n",
    "    agent = TransferLearningAgent(state_dim=2, action_dim=4)\n",
    "    \n",
    "    def create_task_env(goal_position, reward_scale=1.0):\n",
    "        env = SimpleGridWorld(size=4)\n",
    "        env.goal = goal_position\n",
    "        env.reward_scale = reward_scale\n",
    "        return env\n",
    "    \n",
    "    tasks = {\n",
    "        'task_1': create_task_env([3, 3], 1.0),     # Original goal\n",
    "        'task_2': create_task_env([3, 0], 1.0),     # Different goal\n",
    "        'task_3': create_task_env([0, 3], 1.0),     # Another goal\n",
    "    }\n",
    "    \n",
    "    for task_name in tasks.keys():\n",
    "        agent.add_task(task_name)\n",
    "    \n",
    "    print(\\\"Training on Task 1...\\\")\n",
    "    task_1_env = tasks['task_1']\n",
    "    \n",
    "    for episode in range(200):\n",
    "        state = task_1_env.reset()\n",
    "        episode_states, episode_actions, episode_rewards = [], [], []\n",
    "        \n",
    "        for step in range(30):\n",
    "            action = agent.get_action(state, 'task_1')\n",
    "            next_state, reward, done, _ = task_1_env.step(action)\n",
    "            \n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        if episode_rewards:\n",
    "            agent.update(episode_states, episode_actions, episode_rewards, 'task_1')\n",
    "    \n",
    "    transfer_results = {}\n",
    "    \n",
    "    for new_task in ['task_2', 'task_3']:\n",
    "        print(f\\\"Transferring to {new_task}...\\\")\n",
    "        \n",
    "        agent.fine_tune_for_task('task_1', new_task)\n",
    "        \n",
    "        task_env = tasks[new_task]\n",
    "        task_rewards = []\n",
    "        \n",
    "        for episode in range(50):  # Limited training\n",
    "            state = task_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_states, episode_actions, episode_rewards = [], [], []\n",
    "            \n",
    "            for step in range(30):\n",
    "                action = agent.get_action(state, new_task)\n",
    "                next_state, reward, done, _ = task_env.step(action)\n",
    "                \n",
    "                episode_states.append(state)\n",
    "                episode_actions.append(action)\n",
    "                episode_rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            if episode_rewards:\n",
    "                agent.update(episode_states, episode_actions, episode_rewards, \n",
    "                           new_task, update_features=False)\n",
    "            \n",
    "            task_rewards.append(episode_reward)\n",
    "        \n",
    "        transfer_results[new_task] = task_rewards\n",
    "        print(f\\\"  Final performance on {new_task}: {np.mean(task_rewards[-10:]):.2f}\\\")\n",
    "    \n",
    "    return transfer_results\n",
    "\n",
    "print(\\\"ðŸš€ Starting Sample Efficiency and Transfer Learning Demonstrations!\\\")\n",
    "\n",
    "efficiency_results = compare_sample_efficiency()\n",
    "\n",
    "transfer_results = demonstrate_transfer_learning()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for agent_name, data in efficiency_results.items():\n",
    "    window_size = 20\n",
    "    if len(data['rewards']) >= window_size:\n",
    "        smoothed_rewards = pd.Series(data['rewards']).rolling(window_size).mean()\n",
    "        axes[0].plot(data['episodes'], smoothed_rewards, label=agent_name, linewidth=2)\n",
    "\n",
    "axes[0].set_title('Sample Efficiency Comparison')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Episode Reward (Smoothed)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "for task_name, rewards in transfer_results.items():\n",
    "    axes[1].plot(rewards, label=f'Transfer to {task_name}', linewidth=2)\n",
    "\n",
    "axes[1].set_title('Transfer Learning Performance')\n",
    "axes[1].set_xlabel('Episode (Limited Training)')\n",
    "axes[1].set_ylabel('Episode Reward')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\\\"\\\\nðŸ“Š Sample Efficiency Results:\\\")\n",
    "for agent_name, data in efficiency_results.items():\n",
    "    final_perf = np.mean(data['rewards'][-20:])\n",
    "    print(f\\\"  {agent_name}: {final_perf:.2f} final performance\\\")\n",
    "\n",
    "print(\\\"\\\\nðŸ”„ Transfer Learning Results:\\\")\n",
    "for task_name, rewards in transfer_results.items():\n",
    "    final_perf = np.mean(rewards[-10:])\n",
    "    print(f\\\"  {task_name}: {final_perf:.2f} final performance with limited training\\\")\n",
    "\n",
    "print(\\\"\\\\nðŸ’¡ Key Insights:\\\")\n",
    "print(\\\"  â€¢ Prioritized replay and auxiliary tasks improve sample efficiency\\\")\n",
    "print(\\\"  â€¢ Data augmentation provides regularization benefits\\\")\n",
    "print(\\\"  â€¢ Transfer learning enables rapid adaptation to new tasks\\\")\n",
    "print(\\\"  â€¢ Shared representations capture generalizable knowledge\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62229db9",
   "metadata": {},
   "source": [
    "# Section 4: Hierarchical Reinforcement Learning\n",
    "\n",
    "## 4.1 Theory: Hierarchical Decision Making\n",
    "\n",
    "Hierarchical Reinforcement Learning (HRL) addresses the challenge of learning complex behaviors by decomposing tasks into hierarchical structures. This approach enables agents to:\n",
    "\n",
    "1. **Learn at Multiple Time Scales**: High-level policies select goals or skills, while low-level policies execute primitive actions\n",
    "2. **Achieve Better Generalization**: Skills learned in one context can be reused in others\n",
    "3. **Improve Sample Efficiency**: By leveraging temporal abstractions and skill composition\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### Options Framework\n",
    "An **option** $\\omega$ is defined by a tuple $(I_\\omega, \\pi_\\omega, \\beta_\\omega)$:\n",
    "- **Initiation Set** $I_\\omega \\subseteq \\mathcal{S}$: States where the option can be initiated\n",
    "- **Policy** $\\pi_\\omega: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$: Action selection within the option\n",
    "- **Termination Condition** $\\beta_\\omega: \\mathcal{S} \\rightarrow [0,1]$: Probability of termination\n",
    "\n",
    "#### Hierarchical Value Functions\n",
    "The value function for options follows the Bellman equation:\n",
    "$$Q^\\pi(s,\\omega) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\tau-1} \\gamma^t r_{t+1} + \\gamma^\\tau Q^\\pi(s_\\tau, \\omega') \\mid s_0=s, \\omega_0=\\omega\\right]$$\n",
    "\n",
    "where $\\tau$ is the termination time and $\\omega'$ is the next option selected.\n",
    "\n",
    "#### Feudal Networks\n",
    "Feudal Networks implement a manager-worker hierarchy:\n",
    "- **Manager Network**: Sets goals $g_t$ for workers: $g_t = f_{manager}(s_t, h_{t-1}^{manager})$\n",
    "- **Worker Network**: Executes actions conditioned on goals: $a_t = \\pi_{worker}(s_t, g_t)$\n",
    "- **Intrinsic Motivation**: Workers receive intrinsic rewards based on goal achievement\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### Intrinsic Reward Signal\n",
    "The intrinsic reward for achieving subgoals:\n",
    "$$r_t^{intrinsic} = \\cos(\\text{achieved\\_goal}_t - \\text{desired\\_goal}_t) \\cdot ||s_{t+1} - s_t||$$\n",
    "\n",
    "#### Hierarchical Policy Gradient\n",
    "The gradient for the manager policy:\n",
    "$$\\nabla_{\\theta_m} J_m = \\mathbb{E}\\left[\\nabla_{\\theta_m} \\log \\pi_m(g_t|s_t) \\cdot A_m(s_t, g_t)\\right]$$\n",
    "\n",
    "And for the worker policy:\n",
    "$$\\nabla_{\\theta_w} J_w = \\mathbb{E}\\left[\\nabla_{\\theta_w} \\log \\pi_w(a_t|s_t, g_t) \\cdot A_w(s_t, a_t, g_t)\\right]$$\n",
    "\n",
    "## 4.2 Implementation: Hierarchical RL Architectures\n",
    "\n",
    "We'll implement several HRL approaches:\n",
    "1. **Options-Critic Architecture**: Learn options and policies jointly\n",
    "2. **Feudal Networks**: Manager-worker hierarchies\n",
    "3. **Hindsight Experience Replay with Goals**: Sample efficiency for goal-conditioned tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c601a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class OptionsCriticNetwork(nn.Module):\n",
    "    \"\"\"Options-Critic architecture for learning hierarchical policies.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, num_options=4, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_options = num_options\n",
    "        \n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.option_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_options),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.intra_option_nets = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "            ) for _ in range(num_options)\n",
    "        ])\n",
    "        \n",
    "        self.termination_nets = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden_dim // 2, 1),\n",
    "                nn.Sigmoid()\n",
    "            ) for _ in range(num_options)\n",
    "        ])\n",
    "        \n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, num_options)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass through the Options-Critic architecture.\"\"\"\n",
    "        features = self.feature_net(state)\n",
    "        \n",
    "        option_probs = self.option_net(features)\n",
    "        \n",
    "        action_probs = torch.stack([net(features) for net in self.intra_option_nets], dim=1)\n",
    "        \n",
    "        termination_probs = torch.stack([net(features) for net in self.termination_nets], dim=1).squeeze(-1)\n",
    "        \n",
    "        option_values = self.value_net(features)\n",
    "        \n",
    "        return option_probs, action_probs, termination_probs, option_values\n",
    "\n",
    "class OptionsCriticAgent:\n",
    "    \"\"\"Agent using Options-Critic for hierarchical learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, num_options=4, lr=1e-3):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_options = num_options\n",
    "        \n",
    "        self.network = OptionsCriticNetwork(state_dim, action_dim, num_options)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        self.current_option = None\n",
    "        self.option_length = 0\n",
    "        self.max_option_length = 10\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.beta_reg = 0.01  # Regularization for termination\n",
    "        \n",
    "        self.option_usage = np.zeros(num_options)\n",
    "        self.option_lengths = []\n",
    "        self.losses = []\n",
    "    \n",
    "    def select_option(self, state):\n",
    "        \"\"\"Select option using epsilon-greedy on option probabilities.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            option_probs, _, _, _ = self.network(state_tensor)\n",
    "            return Categorical(option_probs).sample().item()\n",
    "    \n",
    "    def select_action(self, state, option):\n",
    "        \"\"\"Select action using the intra-option policy.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            _, action_probs, _, _ = self.network(state_tensor)\n",
    "            return Categorical(action_probs[0, option]).sample().item()\n",
    "    \n",
    "    def should_terminate(self, state, option):\n",
    "        \"\"\"Check if current option should terminate.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            _, _, termination_probs, _ = self.network(state_tensor)\n",
    "            return np.random.random() < termination_probs[0, option].item()\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Full action selection with option management.\"\"\"\n",
    "        if self.current_option is None or self.should_terminate(state, self.current_option) or \\\n",
    "           self.option_length >= self.max_option_length:\n",
    "            self.current_option = self.select_option(state)\n",
    "            self.option_usage[self.current_option] += 1\n",
    "            if self.option_length > 0:\n",
    "                self.option_lengths.append(self.option_length)\n",
    "            self.option_length = 0\n",
    "        \n",
    "        action = self.select_action(state, self.current_option)\n",
    "        self.option_length += 1\n",
    "        \n",
    "        return action, self.current_option\n",
    "    \n",
    "    def update(self, trajectory):\n",
    "        \"\"\"Update using Options-Critic learning algorithm.\"\"\"\n",
    "        if len(trajectory) < 2:\n",
    "            return None\n",
    "        \n",
    "        states, actions, rewards, options = zip(*trajectory)\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        options = torch.LongTensor(options)\n",
    "        \n",
    "        option_probs, action_probs, termination_probs, option_values = self.network(states)\n",
    "        \n",
    "        returns = torch.zeros_like(rewards)\n",
    "        G = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            G = rewards[t] + self.gamma * G\n",
    "            returns[t] = G\n",
    "        \n",
    "        current_option_values = option_values.gather(1, options.unsqueeze(1)).squeeze()\n",
    "        value_loss = F.mse_loss(current_option_values, returns.detach())\n",
    "        \n",
    "        advantages = returns - current_option_values.detach()\n",
    "        \n",
    "        selected_action_probs = []\n",
    "        for i in range(len(actions)):\n",
    "            selected_action_probs.append(action_probs[i, options[i], actions[i]])\n",
    "        selected_action_probs = torch.stack(selected_action_probs)\n",
    "        \n",
    "        policy_loss = -(torch.log(selected_action_probs) * advantages).mean()\n",
    "        \n",
    "        termination_reg = self.beta_reg * termination_probs.mean()\n",
    "        \n",
    "        total_loss = value_loss + policy_loss + termination_reg\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.losses.append(total_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'termination_reg': termination_reg.item()\n",
    "        }\n",
    "\n",
    "class FeudalNetwork(nn.Module):\n",
    "    \"\"\"Feudal Network with Manager-Worker hierarchy.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, goal_dim=8, hidden_dim=128, temporal_horizon=10):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.goal_dim = goal_dim\n",
    "        self.temporal_horizon = temporal_horizon\n",
    "        \n",
    "        self.manager_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.LSTM(hidden_dim, hidden_dim),\n",
    "        )\n",
    "        self.manager_goal_net = nn.Linear(hidden_dim, goal_dim)\n",
    "        \n",
    "        self.worker_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + goal_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.manager_value_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.worker_value_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + goal_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.manager_hidden = None\n",
    "    \n",
    "    def forward(self, state, goal=None):\n",
    "        \"\"\"Forward pass through Feudal Network.\"\"\"\n",
    "        batch_size = state.size(0) if len(state.shape) > 1 else 1\n",
    "        if len(state.shape) == 1:\n",
    "            state = state.unsqueeze(0)\n",
    "        \n",
    "        manager_features = self.manager_net[0](state)  # First layer\n",
    "        manager_features = self.manager_net[1](manager_features)  # ReLU\n",
    "        \n",
    "        if self.manager_hidden is None or self.manager_hidden[0].size(1) != batch_size:\n",
    "            self.manager_hidden = (\n",
    "                torch.zeros(1, batch_size, self.manager_net[2].hidden_size),\n",
    "                torch.zeros(1, batch_size, self.manager_net[2].hidden_size)\n",
    "            )\n",
    "        \n",
    "        lstm_out, self.manager_hidden = self.manager_net[2](\n",
    "            manager_features.unsqueeze(0), self.manager_hidden\n",
    "        )\n",
    "        manager_features = lstm_out.squeeze(0)\n",
    "        \n",
    "        goals = self.manager_goal_net(manager_features)\n",
    "        goals = F.normalize(goals, p=2, dim=-1)  # Unit normalize goals\n",
    "        \n",
    "        manager_value = self.manager_value_net(manager_features)\n",
    "        \n",
    "        if goal is None:\n",
    "            goal = goals\n",
    "        \n",
    "        worker_input = torch.cat([state, goal], dim=-1)\n",
    "        action_probs = self.worker_net(worker_input)\n",
    "        worker_value = self.worker_value_net(worker_input)\n",
    "        \n",
    "        return goals, action_probs, manager_value, worker_value\n",
    "    \n",
    "    def reset_manager_state(self):\n",
    "        \\\"\\\"\\\"Reset manager LSTM state.\\\"\\\"\\\"\n",
    "        self.manager_hidden = None\n",
    "\n",
    "class FeudalAgent:\n",
    "    \\\"\\\"\\\"Feudal Networks agent with hierarchical learning.\\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, goal_dim=8, lr=1e-3, temporal_horizon=10):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.goal_dim = goal_dim\n",
    "        self.temporal_horizon = temporal_horizon\n",
    "        \n",
    "        self.network = FeudalNetwork(state_dim, action_dim, goal_dim, temporal_horizon=temporal_horizon)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        self.current_goal = None\n",
    "        self.goal_step_count = 0\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.intrinsic_reward_scale = 0.5\n",
    "        \n",
    "        self.manager_losses = []\n",
    "        self.worker_losses = []\n",
    "        self.goal_changes = []\n",
    "    \n",
    "    def compute_intrinsic_reward(self, state, next_state, goal):\n",
    "        \\\"\\\"\\\"Compute intrinsic reward based on goal achievement.\\\"\\\"\\\"\n",
    "        state_diff = next_state - state\n",
    "        state_diff_norm = np.linalg.norm(state_diff)\n",
    "        \n",
    "        if state_diff_norm > 1e-6:\n",
    "            cosine_sim = np.dot(state_diff, goal) / (state_diff_norm * np.linalg.norm(goal))\n",
    "            return self.intrinsic_reward_scale * cosine_sim * state_diff_norm\n",
    "        return 0.0\n",
    "    \n",
    "    def act(self, state):\n",
    "        \\\"\\\"\\\"Select action using feudal hierarchy.\\\"\\\"\\\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            if self.current_goal is None or self.goal_step_count >= self.temporal_horizon:\n",
    "                goals, _, _, _ = self.network(state_tensor)\n",
    "                self.current_goal = goals[0].numpy()\n",
    "                self.goal_step_count = 0\n",
    "                self.goal_changes.append(len(self.goal_changes))\n",
    "            \n",
    "            goal_tensor = torch.FloatTensor(self.current_goal).unsqueeze(0)\n",
    "            _, action_probs, _, _ = self.network(state_tensor, goal_tensor)\n",
    "            action = Categorical(action_probs).sample().item()\n",
    "            \n",
    "            self.goal_step_count += 1\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def update(self, trajectories):\n",
    "        \\\"\\\"\\\"Update feudal networks using hierarchical returns.\\\"\\\"\\\"\n",
    "        if not trajectories:\n",
    "            return None\n",
    "        \n",
    "        total_manager_loss = 0\n",
    "        total_worker_loss = 0\n",
    "        num_updates = 0\n",
    "        \n",
    "        for traj in trajectories:\n",
    "            if len(traj) < 2:\n",
    "                continue\n",
    "            \n",
    "            states, actions, rewards, next_states = zip(*traj)\n",
    "            states = torch.FloatTensor(states)\n",
    "            actions = torch.LongTensor(actions)\n",
    "            rewards = torch.FloatTensor(rewards)\n",
    "            next_states = torch.FloatTensor(next_states)\n",
    "            \n",
    "            self.network.reset_manager_state()\n",
    "            \n",
    "            goals, action_probs, manager_values, worker_values = self.network(states)\n",
    "            \n",
    "            intrinsic_rewards = []\n",
    "            for i in range(len(states)-1):\n",
    "                intrinsic_reward = self.compute_intrinsic_reward(\n",
    "                    states[i].numpy(), next_states[i].numpy(), goals[i].numpy()\n",
    "                )\n",
    "                intrinsic_rewards.append(intrinsic_reward)\n",
    "            intrinsic_rewards = torch.FloatTensor(intrinsic_rewards)\n",
    "            \n",
    "            manager_returns = torch.zeros_like(rewards)\n",
    "            G = 0\n",
    "            for t in reversed(range(len(rewards))):\n",
    "                G = rewards[t] + self.gamma * G\n",
    "                manager_returns[t] = G\n",
    "            \n",
    "            manager_advantages = manager_returns - manager_values.squeeze()\n",
    "            manager_loss = (manager_advantages ** 2).mean()\n",
    "            \n",
    "            total_rewards = rewards[:-1] + intrinsic_rewards\n",
    "            worker_returns = torch.zeros_like(total_rewards)\n",
    "            G = 0\n",
    "            for t in reversed(range(len(total_rewards))):\n",
    "                G = total_rewards[t] + self.gamma * G\n",
    "                worker_returns[t] = G\n",
    "            \n",
    "            worker_advantages = worker_returns - worker_values[:-1].squeeze()\n",
    "            \n",
    "            selected_action_probs = action_probs[:-1].gather(1, actions[:-1].unsqueeze(1)).squeeze()\n",
    "            worker_policy_loss = -(torch.log(selected_action_probs) * worker_advantages.detach()).mean()\n",
    "            worker_value_loss = (worker_advantages ** 2).mean()\n",
    "            worker_loss = worker_policy_loss + 0.5 * worker_value_loss\n",
    "            \n",
    "            total_manager_loss += manager_loss\n",
    "            total_worker_loss += worker_loss\n",
    "            num_updates += 1\n",
    "        \n",
    "        if num_updates == 0:\n",
    "            return None\n",
    "        \n",
    "        avg_manager_loss = total_manager_loss / num_updates\n",
    "        avg_worker_loss = total_worker_loss / num_updates\n",
    "        total_loss = avg_manager_loss + avg_worker_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.manager_losses.append(avg_manager_loss.item())\n",
    "        self.worker_losses.append(avg_worker_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'manager_loss': avg_manager_loss.item(),\n",
    "            'worker_loss': avg_worker_loss.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "\n",
    "class HindsightExperienceReplay:\n",
    "    \\\"\\\"\\\"Hindsight Experience Replay for goal-conditioned RL.\\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self, capacity, goal_dim, strategy='future', k=4):\n",
    "        self.capacity = capacity\n",
    "        self.goal_dim = goal_dim\n",
    "        self.strategy = strategy  # 'future', 'final', 'episode', 'random'\n",
    "        self.k = k  # Number of additional goals to sample\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push_episode(self, episode_trajectory):\n",
    "        \\\"\\\"\\\"Store an entire episode and generate hindsight goals.\\\"\\\"\\\"\n",
    "        if not episode_trajectory:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, goals, achieved_goals = zip(*episode_trajectory)\n",
    "        \n",
    "        for i, transition in enumerate(episode_trajectory):\n",
    "            self._store_transition(transition)\n",
    "        \n",
    "        if self.strategy == 'future':\n",
    "            self._generate_future_goals(episode_trajectory)\n",
    "        elif self.strategy == 'final':\n",
    "            self._generate_final_goals(episode_trajectory)\n",
    "        elif self.strategy == 'episode':\n",
    "            self._generate_episode_goals(episode_trajectory)\n",
    "        elif self.strategy == 'random':\n",
    "            self._generate_random_goals(episode_trajectory)\n",
    "    \n",
    "    def _store_transition(self, transition):\n",
    "        \\\"\\\"\\\"Store a single transition in the buffer.\\\"\\\"\\\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = transition\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def _generate_future_goals(self, episode_trajectory):\n",
    "        \\\"\\\"\\\"Generate goals from future achieved goals in the episode.\\\"\\\"\\\"\n",
    "        for i in range(len(episode_trajectory)):\n",
    "            future_indices = np.random.choice(\n",
    "                range(i, len(episode_trajectory)), \n",
    "                size=min(self.k, len(episode_trajectory) - i), \n",
    "                replace=False\n",
    "            )\n",
    "            \n",
    "            for future_idx in future_indices:\n",
    "                state, action, _, next_state, _, _ = episode_trajectory[i]\n",
    "                _, _, _, _, _, achieved_goal = episode_trajectory[future_idx]\n",
    "                \n",
    "                reward = self._compute_goal_reward(next_state, achieved_goal)\n",
    "                \n",
    "                hindsight_transition = (state, action, reward, next_state, achieved_goal, achieved_goal)\n",
    "                self._store_transition(hindsight_transition)\n",
    "    \n",
    "    def _generate_final_goals(self, episode_trajectory):\n",
    "        \\\"\\\"\\\"Use the final achieved goal for all transitions.\\\"\\\"\\\"\n",
    "        if not episode_trajectory:\n",
    "            return\n",
    "        \n",
    "        final_achieved_goal = episode_trajectory[-1][5]  # achieved_goal from last transition\n",
    "        \n",
    "        for transition in episode_trajectory:\n",
    "            state, action, _, next_state, _, _ = transition\n",
    "            reward = self._compute_goal_reward(next_state, final_achieved_goal)\n",
    "            \n",
    "            hindsight_transition = (state, action, reward, next_state, final_achieved_goal, final_achieved_goal)\n",
    "            self._store_transition(hindsight_transition)\n",
    "    \n",
    "    def _compute_goal_reward(self, achieved_goal, desired_goal, threshold=0.1):\n",
    "        \\\"\\\"\\\"Compute reward based on goal achievement.\\\"\\\"\\\"\n",
    "        distance = np.linalg.norm(achieved_goal - desired_goal)\n",
    "        return 1.0 if distance < threshold else -1.0\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \\\"\\\"\\\"Sample a batch of transitions.\\\"\\\"\\\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[i] for i in indices]\n",
    "        \n",
    "        states, actions, rewards, next_states, goals, achieved_goals = zip(*batch)\n",
    "        \n",
    "        return (\n",
    "            torch.FloatTensor(states),\n",
    "            torch.LongTensor(actions), \n",
    "            torch.FloatTensor(rewards),\n",
    "            torch.FloatTensor(next_states),\n",
    "            torch.FloatTensor(goals),\n",
    "            torch.FloatTensor(achieved_goals)\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "def demonstrate_hierarchical_rl():\n",
    "    \\\"\\\"\\\"Demonstrate hierarchical RL approaches.\\\"\\\"\\\"\n",
    "    print(\\\"ðŸ—ï¸ Demonstrating Hierarchical Reinforcement Learning\\\")\n",
    "    \n",
    "    env = SimpleGridWorld(size=8)\n",
    "    \n",
    "    agents = {\n",
    "        'Options-Critic': OptionsCriticAgent(state_dim=2, action_dim=4, num_options=4),\n",
    "        'Feudal Network': FeudalAgent(state_dim=2, action_dim=4, goal_dim=4)\n",
    "    }\n",
    "    \n",
    "    results = {name: {'rewards': [], 'episode_lengths': []} for name in agents.keys()}\n",
    "    \n",
    "    num_episodes = 200\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        for agent_name, agent in agents.items():\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_length = 0\n",
    "            trajectory = []\n",
    "            \n",
    "            for step in range(100):  # Max episode length\n",
    "                if agent_name == 'Options-Critic':\n",
    "                    action, option = agent.act(state)\n",
    "                    trajectory.append((state, action, 0, option))  # Reward added later\n",
    "                else:\n",
    "                    action = agent.act(state)\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                episode_length += 1\n",
    "                \n",
    "                if agent_name == 'Options-Critic':\n",
    "                    trajectory[-1] = (state, action, reward, option)\n",
    "                else:\n",
    "                    trajectory.append((state, action, reward, next_state))\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            if agent_name == 'Options-Critic' and len(trajectory) > 1:\n",
    "                agent.update(trajectory)\n",
    "            elif agent_name == 'Feudal Network' and len(trajectory) > 1:\n",
    "                agent.update([trajectory])\n",
    "            \n",
    "            results[agent_name]['rewards'].append(episode_reward)\n",
    "            results[agent_name]['episode_lengths'].append(episode_length)\n",
    "    \n",
    "    return results, agents\n",
    "\n",
    "print(\\\"ðŸš€ Starting Hierarchical RL Demonstration!\\\")\n",
    "hierarchical_results, hierarchical_agents = demonstrate_hierarchical_rl()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "for agent_name, data in hierarchical_results.items():\n",
    "    window_size = 20\n",
    "    if len(data['rewards']) >= window_size:\n",
    "        smoothed_rewards = pd.Series(data['rewards']).rolling(window_size).mean()\n",
    "        axes[0, 0].plot(smoothed_rewards, label=agent_name, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Hierarchical RL Learning Curves')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Episode Reward (Smoothed)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for agent_name, data in hierarchical_results.items():\n",
    "    window_size = 20\n",
    "    if len(data['episode_lengths']) >= window_size:\n",
    "        smoothed_lengths = pd.Series(data['episode_lengths']).rolling(window_size).mean()\n",
    "        axes[0, 1].plot(smoothed_lengths, label=agent_name, linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('Episode Length Over Time')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Episode Length (Smoothed)')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "if 'Options-Critic' in hierarchical_agents:\n",
    "    agent = hierarchical_agents['Options-Critic']\n",
    "    axes[1, 0].bar(range(agent.num_options), agent.option_usage)\n",
    "    axes[1, 0].set_title('Option Usage Distribution')\n",
    "    axes[1, 0].set_xlabel('Option ID')\n",
    "    axes[1, 0].set_ylabel('Usage Count')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "if 'Options-Critic' in hierarchical_agents:\n",
    "    agent = hierarchical_agents['Options-Critic']\n",
    "    if agent.option_lengths:\n",
    "        axes[1, 1].hist(agent.option_lengths, bins=20, alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].set_title('Option Length Distribution')\n",
    "        axes[1, 1].set_xlabel('Option Length')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\\\"\\\\nðŸ“Š Hierarchical RL Results:\\\")\n",
    "for agent_name, data in hierarchical_results.items():\n",
    "    final_perf = np.mean(data['rewards'][-20:])\n",
    "    avg_length = np.mean(data['episode_lengths'][-20:])\n",
    "    print(f\\\"  {agent_name}:\\\")\n",
    "    print(f\\\"    Final Performance: {final_perf:.2f}\\\")\n",
    "    print(f\\\"    Avg Episode Length: {avg_length:.2f}\\\")\n",
    "\n",
    "if 'Options-Critic' in hierarchical_agents:\n",
    "    agent = hierarchical_agents['Options-Critic']\n",
    "    print(f\\\"\\\\nðŸŽ¯ Options-Critic Analysis:\\\")\n",
    "    print(f\\\"  Options Used: {np.sum(agent.option_usage > 0)} / {agent.num_options}\\\")\n",
    "    print(f\\\"  Most Used Option: {np.argmax(agent.option_usage)}\\\")\n",
    "    if agent.option_lengths:\n",
    "        print(f\\\"  Avg Option Length: {np.mean(agent.option_lengths):.2f}\\\")\n",
    "\n",
    "print(\\\"\\\\nðŸ’¡ Key Insights:\\\")\n",
    "print(\\\"  â€¢ Hierarchical methods learn temporal abstractions\\\")\n",
    "print(\\\"  â€¢ Options provide reusable behavioral primitives\\\")\n",
    "print(\\\"  â€¢ Feudal networks enable goal-directed exploration\\\")\n",
    "print(\\\"  â€¢ HRL scales to complex, long-horizon tasks\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cb83f",
   "metadata": {},
   "source": [
    "# Section 5: Comprehensive Evaluation and Advanced Techniques Integration\n",
    "\n",
    "## 5.1 Multi-Method Performance Analysis\n",
    "\n",
    "This section provides comprehensive evaluation comparing all implemented advanced Deep RL techniques:\n",
    "\n",
    "### Performance Metrics\n",
    "1. **Sample Efficiency**: Episodes to convergence\n",
    "2. **Final Performance**: Asymptotic reward\n",
    "3. **Robustness**: Performance variance\n",
    "4. **Computational Efficiency**: Training time and memory usage\n",
    "5. **Transfer Capability**: Performance on related tasks\n",
    "\n",
    "### Evaluation Framework\n",
    "We evaluate methods across multiple dimensions:\n",
    "- **Simple Tasks**: Basic navigation and control\n",
    "- **Complex Tasks**: Multi-step reasoning and planning\n",
    "- **Transfer Tasks**: Adaptation to new environments\n",
    "- **Long-Horizon Tasks**: Extended episode planning\n",
    "\n",
    "## 5.2 Practical Implementation Considerations\n",
    "\n",
    "### When to Use Each Method:\n",
    "\n",
    "#### Model-Free Methods (DQN, Policy Gradient)\n",
    "- âœ… **Use when**: Simple tasks, abundant data, unknown dynamics\n",
    "- âŒ **Avoid when**: Sample efficiency critical, complex planning needed\n",
    "\n",
    "#### Model-Based Methods  \n",
    "- âœ… **Use when**: Sample efficiency critical, dynamics learnable\n",
    "- âŒ **Avoid when**: High-dimensional observations, stochastic dynamics\n",
    "\n",
    "#### World Models\n",
    "- âœ… **Use when**: Rich sensory input, imagination beneficial\n",
    "- âŒ **Avoid when**: Simple state spaces, real-time constraints\n",
    "\n",
    "#### Hierarchical Methods\n",
    "- âœ… **Use when**: Long-horizon tasks, reusable skills needed\n",
    "- âŒ **Avoid when**: Simple tasks, flat action spaces\n",
    "\n",
    "#### Sample Efficiency Techniques\n",
    "- âœ… **Use when**: Limited data, expensive environments\n",
    "- âŒ **Avoid when**: Abundant cheap data, simple tasks\n",
    "\n",
    "## 5.3 Advanced Techniques Summary\n",
    "\n",
    "This comprehensive assignment covered cutting-edge Deep RL methods:\n",
    "\n",
    "### Core Contributions:\n",
    "1. **Sample Efficiency**: Prioritized replay, data augmentation, auxiliary tasks\n",
    "2. **World Models**: VAE-based dynamics, imagination planning\n",
    "3. **Transfer Learning**: Shared representations, meta-learning\n",
    "4. **Hierarchical Learning**: Options framework, feudal networks\n",
    "5. **Integration**: Multi-method evaluation and practical guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdvancedRLEvaluator:\n",
    "    \"\"\"Comprehensive evaluation framework for advanced RL methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, environments, agents, metrics=['reward', 'sample_efficiency', 'robustness']):\n",
    "        self.environments = environments\n",
    "        self.agents = agents\n",
    "        self.metrics = metrics\n",
    "        self.results = {}\n",
    "        \n",
    "        self.num_trials = 5\n",
    "        self.num_episodes = 300\n",
    "        self.evaluation_interval = 50\n",
    "        \n",
    "    def evaluate_sample_efficiency(self, agent, env, convergence_threshold=0.8):\n",
    "        \"\"\"Measure episodes to convergence.\"\"\"\n",
    "        max_rewards = []\n",
    "        convergence_episodes = []\n",
    "        \n",
    "        for trial in range(self.num_trials):\n",
    "            episode_rewards = []\n",
    "            \n",
    "            if hasattr(agent, 'reset'):\n",
    "                agent.reset()\n",
    "            \n",
    "            for episode in range(self.num_episodes):\n",
    "                state = env.reset()\n",
    "                episode_reward = 0\n",
    "                \n",
    "                for step in range(100):\n",
    "                    if hasattr(agent, 'act'):\n",
    "                        if 'Options' in str(type(agent)):\n",
    "                            action, _ = agent.act(state)\n",
    "                        else:\n",
    "                            action = agent.act(state)\n",
    "                    else:\n",
    "                        action = env.action_space.sample()\n",
    "                    \n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    if hasattr(agent, 'replay_buffer'):\n",
    "                        agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                        if len(agent.replay_buffer) > 32:\n",
    "                            if hasattr(agent, 'update'):\n",
    "                                agent.update(32)\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                    \n",
    "                    state = next_state\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                \n",
    "                if len(episode_rewards) >= 20:\n",
    "                    recent_performance = np.mean(episode_rewards[-20:])\n",
    "                    if recent_performance >= convergence_threshold * np.max(episode_rewards[:max(1, episode-20)]):\n",
    "                        convergence_episodes.append(episode)\n",
    "                        break\n",
    "            \n",
    "            max_rewards.append(np.max(episode_rewards))\n",
    "            if not convergence_episodes or len(convergence_episodes) <= trial:\n",
    "                convergence_episodes.append(self.num_episodes)\n",
    "        \n",
    "        return {\n",
    "            'convergence_episodes': np.mean(convergence_episodes),\n",
    "            'convergence_std': np.std(convergence_episodes),\n",
    "            'max_reward': np.mean(max_rewards),\n",
    "            'max_reward_std': np.std(max_rewards)\n",
    "        }\n",
    "    \n",
    "    def evaluate_transfer_capability(self, agent, source_env, target_envs):\n",
    "        \"\"\"Evaluate transfer learning capability.\"\"\"\n",
    "        source_performance = []\n",
    "        state = source_env.reset()\n",
    "        \n",
    "        for episode in range(100):  # Limited training\n",
    "            episode_reward = 0\n",
    "            for step in range(50):\n",
    "                action = agent.act(state) if hasattr(agent, 'act') else source_env.action_space.sample()\n",
    "                next_state, reward, done, _ = source_env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if hasattr(agent, 'replay_buffer'):\n",
    "                    agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                    if len(agent.replay_buffer) > 32 and hasattr(agent, 'update'):\n",
    "                        agent.update(32)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "            \n",
    "            source_performance.append(episode_reward)\n",
    "        \n",
    "        transfer_results = {}\n",
    "        for target_name, target_env in target_envs.items():\n",
    "            target_rewards = []\n",
    "            \n",
    "            for episode in range(20):  # Quick evaluation\n",
    "                state = target_env.reset()\n",
    "                episode_reward = 0\n",
    "                \n",
    "                for step in range(50):\n",
    "                    action = agent.act(state) if hasattr(agent, 'act') else target_env.action_space.sample()\n",
    "                    next_state, reward, done, _ = target_env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                    state = next_state\n",
    "                \n",
    "                target_rewards.append(episode_reward)\n",
    "            \n",
    "            transfer_results[target_name] = {\n",
    "                'mean_reward': np.mean(target_rewards),\n",
    "                'std_reward': np.std(target_rewards)\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'source_performance': np.mean(source_performance[-20:]),\n",
    "            'transfer_results': transfer_results\n",
    "        }\n",
    "    \n",
    "    def comprehensive_evaluation(self):\n",
    "        \"\"\"Run comprehensive evaluation across all agents and environments.\"\"\"\n",
    "        print(\\\"ðŸ”¬ Starting Comprehensive Evaluation...\\\")\n",
    "        \n",
    "        for agent_name, agent in self.agents.items():\n",
    "            print(f\\\"\\\\nðŸ“Š Evaluating {agent_name}...\\\")\n",
    "            self.results[agent_name] = {}\n",
    "            \n",
    "            if 'sample_efficiency' in self.metrics:\n",
    "                env = self.environments[0] if self.environments else SimpleGridWorld(size=5)\n",
    "                efficiency_results = self.evaluate_sample_efficiency(agent, env)\n",
    "                self.results[agent_name]['sample_efficiency'] = efficiency_results\n",
    "                print(f\\\"  Sample Efficiency: {efficiency_results['convergence_episodes']:.1f} Â± {efficiency_results['convergence_std']:.1f} episodes\\\")\n",
    "            \n",
    "            if 'transfer' in self.metrics and len(self.environments) > 1:\n",
    "                source_env = self.environments[0]\n",
    "                target_envs = {f'env_{i}': env for i, env in enumerate(self.environments[1:])}\n",
    "                transfer_results = self.evaluate_transfer_capability(agent, source_env, target_envs)\n",
    "                self.results[agent_name]['transfer'] = transfer_results\n",
    "                print(f\\\"  Transfer Capability: Source performance {transfer_results['source_performance']:.2f}\\\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \\\"\\\"\\\"Generate comprehensive evaluation report.\\\"\\\"\\\"\n",
    "        if not self.results:\n",
    "            self.comprehensive_evaluation()\n",
    "        \n",
    "        print(\\\"\\\\n\\\" + \\\"=\\\"*60)\n",
    "        print(\\\"ðŸ† COMPREHENSIVE EVALUATION REPORT\\\")\n",
    "        print(\\\"=\\\"*60)\n",
    "        \n",
    "        if any('sample_efficiency' in results for results in self.results.values()):\n",
    "            print(\\\"\\\\nðŸ“ˆ Sample Efficiency Ranking:\\\")\n",
    "            efficiency_scores = []\n",
    "            for agent_name, results in self.results.items():\n",
    "                if 'sample_efficiency' in results:\n",
    "                    score = results['sample_efficiency']['convergence_episodes']\n",
    "                    efficiency_scores.append((agent_name, score))\n",
    "            \n",
    "            efficiency_scores.sort(key=lambda x: x[1])  # Lower is better\n",
    "            for rank, (agent_name, score) in enumerate(efficiency_scores, 1):\n",
    "                print(f\\\"  {rank}. {agent_name}: {score:.1f} episodes to convergence\\\")\n",
    "        \n",
    "        print(\\\"\\\\nðŸŽ¯ Final Performance Comparison:\\\")\n",
    "        performance_scores = []\n",
    "        for agent_name, results in self.results.items():\n",
    "            if 'sample_efficiency' in results:\n",
    "                score = results['sample_efficiency']['max_reward']\n",
    "                performance_scores.append((agent_name, score))\n",
    "        \n",
    "        performance_scores.sort(key=lambda x: x[1], reverse=True)  # Higher is better\n",
    "        for rank, (agent_name, score) in enumerate(performance_scores, 1):\n",
    "            print(f\\\"  {rank}. {agent_name}: {score:.2f} max reward\\\")\n",
    "        \n",
    "        print(\\\"\\\\nðŸ’¡ Method Recommendations:\\\")\n",
    "        \n",
    "        if efficiency_scores:\n",
    "            best_efficiency = efficiency_scores[0][0]\n",
    "            print(f\\\"  â€¢ Best Sample Efficiency: {best_efficiency}\\\")\n",
    "        \n",
    "        if performance_scores:\n",
    "            best_performance = performance_scores[0][0]\n",
    "            print(f\\\"  â€¢ Best Final Performance: {best_performance}\\\")\n",
    "        \n",
    "        print(\\\"\\\\nðŸ”§ Implementation Guidelines:\\\")\n",
    "        print(\\\"  â€¢ Use prioritized replay for sample efficiency\\\")\n",
    "        print(\\\"  â€¢ Apply data augmentation for robustness\\\")\n",
    "        print(\\\"  â€¢ Consider world models for planning tasks\\\")\n",
    "        print(\\\"  â€¢ Employ hierarchical methods for long-horizon problems\\\")\n",
    "        print(\\\"  â€¢ Leverage transfer learning for related domains\\\")\n",
    "\n",
    "class IntegratedAdvancedAgent:\n",
    "    \\\"\\\"\\\"Agent integrating multiple advanced RL techniques.\\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, config=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        default_config = {\n",
    "            'use_prioritized_replay': True,\n",
    "            'use_auxiliary_tasks': True,\n",
    "            'use_data_augmentation': True,\n",
    "            'use_world_model': False,\n",
    "            'use_hierarchical': False,\n",
    "            'lr': 1e-3,\n",
    "            'buffer_size': 10000\n",
    "        }\n",
    "        self.config = {**default_config, **(config or {})}\n",
    "        \n",
    "        self._initialize_components()\n",
    "        \n",
    "        self.training_stats = {\n",
    "            'episode_rewards': [],\n",
    "            'losses': [],\n",
    "            'sample_efficiency': [],\n",
    "            'component_usage': {}\n",
    "        }\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \\\"\\\"\\\"Initialize RL components based on configuration.\\\"\\\"\\\"\n",
    "        if self.config['use_auxiliary_tasks']:\n",
    "            self.network = DataAugmentationDQN(self.state_dim, self.action_dim)\n",
    "        else:\n",
    "            self.network = DQNAgent(self.state_dim, self.action_dim).network\n",
    "        \n",
    "        self.target_network = copy.deepcopy(self.network)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=self.config['lr'])\n",
    "        \n",
    "        if self.config['use_prioritized_replay']:\n",
    "            self.replay_buffer = PrioritizedReplayBuffer(self.config['buffer_size'])\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer(self.config['buffer_size'])\n",
    "        \n",
    "        if self.config['use_world_model']:\n",
    "            self.world_model = VariationalWorldModel(self.state_dim, self.action_dim)\n",
    "        \n",
    "        if self.config['use_hierarchical']:\n",
    "            self.hierarchical_agent = OptionsCriticAgent(self.state_dim, self.action_dim)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.update_count = 0\n",
    "        self.target_update_freq = 100\n",
    "    \n",
    "    def act(self, state, epsilon=0.1):\n",
    "        \\\"\\\"\\\"Select action using integrated approach.\\\"\\\"\\\"\n",
    "        if self.config['use_hierarchical']:\n",
    "            action, option = self.hierarchical_agent.act(state)\n",
    "            self.training_stats['component_usage']['hierarchical'] = \\\n",
    "                self.training_stats['component_usage'].get('hierarchical', 0) + 1\n",
    "            return action\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            if self.config['use_data_augmentation'] and np.random.random() < 0.1:\n",
    "                state_tensor = self.network.apply_augmentation(state_tensor, 'noise')\n",
    "            \n",
    "            q_values = self.network(state_tensor)\n",
    "            if isinstance(q_values, tuple):\n",
    "                q_values = q_values[0]  # Extract Q-values from auxiliary network\n",
    "            \n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def update(self, batch_size=32):\n",
    "        \\\"\\\"\\\"Update agent using integrated advanced techniques.\\\"\\\"\\\"\n",
    "        if isinstance(self.replay_buffer, PrioritizedReplayBuffer):\n",
    "            sample_result = self.replay_buffer.sample(batch_size)\n",
    "            if sample_result is None:\n",
    "                return None\n",
    "            experiences, indices, weights = sample_result\n",
    "        else:\n",
    "            batch = self.replay_buffer.sample(batch_size)\n",
    "            if batch is None:\n",
    "                return None\n",
    "            experiences = batch\n",
    "            weights = torch.ones(batch_size)\n",
    "            indices = None\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        weights = torch.FloatTensor(weights) if not isinstance(weights, torch.Tensor) else weights\n",
    "        \n",
    "        if self.config['use_data_augmentation']:\n",
    "            aug_type = np.random.choice(['noise', 'dropout', 'scaling'])\n",
    "            states = self.network.apply_augmentation(states, aug_type)\n",
    "            next_states = self.network.apply_augmentation(next_states, aug_type)\n",
    "            self.training_stats['component_usage']['augmentation'] = \\\n",
    "                self.training_stats['component_usage'].get('augmentation', 0) + 1\n",
    "        \n",
    "        if self.config['use_auxiliary_tasks']:\n",
    "            current_q_values, reward_pred, next_state_pred = self.network(states, actions)\n",
    "            current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        else:\n",
    "            current_q_values = self.network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.config['use_auxiliary_tasks'] and hasattr(self.target_network, 'forward'):\n",
    "                next_q_values = self.target_network(next_states)\n",
    "                if isinstance(next_q_values, tuple):\n",
    "                    next_q_values = next_q_values[0]\n",
    "            else:\n",
    "                next_q_values = self.target_network(next_states)\n",
    "            \n",
    "            max_next_q_values = next_q_values.max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * max_next_q_values * (~dones))\n",
    "        \n",
    "        td_errors = (current_q_values - target_q_values).detach()\n",
    "        q_loss = (weights * F.mse_loss(current_q_values, target_q_values, reduction='none')).mean()\n",
    "        \n",
    "        total_loss = q_loss\n",
    "        \n",
    "        if self.config['use_auxiliary_tasks']:\n",
    "            aux_reward_loss = F.mse_loss(reward_pred.squeeze(), rewards)\n",
    "            aux_dynamics_loss = F.mse_loss(next_state_pred, next_states)\n",
    "            total_loss += 0.1 * aux_reward_loss + 0.1 * aux_dynamics_loss\n",
    "            self.training_stats['component_usage']['auxiliary'] = \\\n",
    "                self.training_stats['component_usage'].get('auxiliary', 0) + 1\n",
    "        \n",
    "        if self.config['use_world_model']:\n",
    "            world_model_loss = self.world_model.compute_loss(states, actions, next_states)\n",
    "            total_loss += 0.1 * world_model_loss\n",
    "            self.training_stats['component_usage']['world_model'] = \\\n",
    "                self.training_stats['component_usage'].get('world_model', 0) + 1\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if indices is not None:\n",
    "            self.replay_buffer.update_priorities(indices, td_errors.numpy())\n",
    "        \n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        \n",
    "        self.training_stats['losses'].append(total_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'q_loss': q_loss.item()\n",
    "        }\n",
    "\n",
    "def comprehensive_advanced_rl_demo():\n",
    "    \\\"\\\"\\\"Comprehensive demonstration of all advanced RL techniques.\\\"\\\"\\\"\n",
    "    print(\\\"ðŸŽ“ COMPREHENSIVE ADVANCED DEEP RL DEMONSTRATION\\\")\n",
    "    print(\\\"=\\\" * 55)\n",
    "    \n",
    "    environments = [\n",
    "        SimpleGridWorld(size=5),\n",
    "        SimpleGridWorld(size=6),\n",
    "        SimpleGridWorld(size=7)\n",
    "    ]\n",
    "    \n",
    "    agents = {\n",
    "        'Baseline DQN': DQNAgent(state_dim=2, action_dim=4),\n",
    "        'Sample Efficient': SampleEfficientAgent(state_dim=2, action_dim=4),\n",
    "        'Options-Critic': OptionsCriticAgent(state_dim=2, action_dim=4),\n",
    "        'Feudal Network': FeudalAgent(state_dim=2, action_dim=4),\n",
    "        'Integrated Advanced': IntegratedAdvancedAgent(\n",
    "            state_dim=2, \n",
    "            action_dim=4, \n",
    "            config={\n",
    "                'use_prioritized_replay': True,\n",
    "                'use_auxiliary_tasks': True,\n",
    "                'use_data_augmentation': True\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    evaluator = AdvancedRLEvaluator(\n",
    "        environments=environments,\n",
    "        agents=agents,\n",
    "        metrics=['sample_efficiency', 'reward', 'transfer']\n",
    "    )\n",
    "    \n",
    "    evaluator.generate_report()\n",
    "    \n",
    "    print(\\\"\\\\nðŸŽ¯ ADVANCED DEEP RL ASSIGNMENT COMPLETED!\\\")\n",
    "    print(\\\"\\\\nðŸ“š Concepts Covered:\\\")\n",
    "    print(\\\"  âœ“ Model-Free vs Model-Based RL Comparison\\\")\n",
    "    print(\\\"  âœ“ World Models with VAE Architecture\\\") \n",
    "    print(\\\"  âœ“ Imagination-Based Planning\\\")\n",
    "    print(\\\"  âœ“ Sample Efficiency Techniques\\\")\n",
    "    print(\\\"  âœ“ Prioritized Experience Replay\\\")\n",
    "    print(\\\"  âœ“ Data Augmentation & Auxiliary Tasks\\\")\n",
    "    print(\\\"  âœ“ Transfer Learning & Meta-Learning\\\")\n",
    "    print(\\\"  âœ“ Hierarchical Reinforcement Learning\\\")\n",
    "    print(\\\"  âœ“ Options-Critic Architecture\\\")\n",
    "    print(\\\"  âœ“ Feudal Networks\\\")\n",
    "    print(\\\"  âœ“ Comprehensive Evaluation Framework\\\")\n",
    "    \n",
    "    print(\\\"\\\\nðŸ”¬ Key Takeaways:\\\")\n",
    "    print(\\\"  â€¢ Advanced RL methods address sample efficiency and scalability\\\")\n",
    "    print(\\\"  â€¢ World models enable planning and imagination\\\")\n",
    "    print(\\\"  â€¢ Hierarchical methods tackle long-horizon tasks\\\")\n",
    "    print(\\\"  â€¢ Transfer learning accelerates adaptation\\\")\n",
    "    print(\\\"  â€¢ Integration of techniques often yields best results\\\")\n",
    "    \n",
    "    print(\\\"\\\\nðŸš€ Ready for Real-World Advanced RL Applications!\\\")\n",
    "    \n",
    "    return evaluator.results\n",
    "\n",
    "print(\\\"Starting final comprehensive demonstration...\\\"\n",
    "final_results = comprehensive_advanced_rl_demo()\n",
    "\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\n",
    "print(\\\"ðŸ“– ASSIGNMENT 13: ADVANCED DEEP RL - COMPLETE! âœ…\\\"\n",
    "print(\\\"=\\\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7549258",
   "metadata": {},
   "source": [
    "# CA13: Advanced Deep Reinforcement Learning - Model-Free vs Model-Based Methods and Real-World Applications\n",
    "\n",
    "## Deep Reinforcement Learning - Session 13\n",
    "\n",
    "**Advanced Deep RL Topics: Model-Free vs Model-Based Methods, World Models, and Real-World Deployment**\n",
    "\n",
    "This notebook explores advanced deep reinforcement learning concepts, including the comparison between model-free and model-based approaches, world models, sample efficiency techniques, transfer learning, and practical considerations for real-world deployment.\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Understand the fundamental differences between model-free and model-based RL\n",
    "2. Implement and compare various world modeling approaches\n",
    "3. Master sample-efficient learning techniques and transfer learning\n",
    "4. Explore hierarchical reinforcement learning and temporal abstraction\n",
    "5. Understand safe reinforcement learning and constrained optimization\n",
    "6. Implement real-world deployment strategies and robustness techniques\n",
    "7. Analyze offline reinforcement learning and batch methods\n",
    "8. Apply meta-learning and few-shot adaptation in RL contexts\n",
    "\n",
    "### Notebook Structure:\n",
    "1. **Model-Free vs Model-Based RL** - Theoretical foundations and trade-offs\n",
    "2. **World Models and Imagination** - Learning environment dynamics\n",
    "3. **Sample Efficiency Techniques** - Maximizing learning from limited data\n",
    "4. **Hierarchical Reinforcement Learning** - Temporal abstraction and skills\n",
    "5. **Safe and Constrained RL** - Safety-aware learning algorithms\n",
    "6. **Transfer Learning and Meta-Learning** - Knowledge reuse and adaptation\n",
    "7. **Offline and Batch RL** - Learning from pre-collected data\n",
    "8. **Real-World Applications** - Deployment strategies and case studies\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque, defaultdict\n",
    "import random\n",
    "import gym\n",
    "import copy\n",
    "from typing import List, Tuple, Dict, Optional, Union\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "print(f\"ðŸ“Š PyTorch version: {torch.__version__}\")\n",
    "print(f\"ðŸ¤– Starting Advanced Deep RL Session 13!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a7034c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c258d84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
