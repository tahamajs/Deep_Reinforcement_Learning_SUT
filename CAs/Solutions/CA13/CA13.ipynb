{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c36039e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured sys.path for CA13 imports\n"
     ]
    }
   ],
   "source": [
    "# Setup sys.path for CA13 package imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath(\".\"))\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "print(\"Configured sys.path for CA13 imports\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Assignment 13: Advanced Model-based Rl and World Models\n",
    "\n",
    "## Course Information\n",
    "- **Course**: Deep Reinforcement Learning (DRL)\n",
    "- **Instructor**: Dr. [Instructor Name]\n",
    "- **Institution**: Sharif University of Technology\n",
    "- **Semester**: Fall 2024\n",
    "- **Assignment Number**: CA13\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this assignment, students will be able to:\n",
    "\n",
    "1. **Understand Model-Based vs Model-Free RL Trade-offs**: Analyze the fundamental differences between model-free and model-based reinforcement learning approaches, including their respective advantages, limitations, and appropriate use cases.\n",
    "\n",
    "2. **Master World Model Architectures**: Design and implement variational world models using VAEs for learning compact latent representations of environment dynamics, including encoder-decoder architectures and stochastic dynamics modeling.\n",
    "\n",
    "3. **Implement Imagination-Based Learning**: Develop agents that leverage learned world models for planning and decision-making in latent space, enabling sample-efficient learning through imagined trajectories.\n",
    "\n",
    "4. **Apply Sample Efficiency Techniques**: Utilize advanced techniques such as prioritized experience replay, data augmentation, and auxiliary tasks to improve learning efficiency in deep RL.\n",
    "\n",
    "5. **Design Transfer Learning Systems**: Build agents capable of transferring knowledge across related tasks through shared representations, fine-tuning, and meta-learning approaches.\n",
    "\n",
    "6. **Develop Hierarchical RL Frameworks**: Implement hierarchical decision-making systems using options framework, enabling temporal abstraction and skill composition for complex task solving.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this assignment, ensure you have:\n",
    "\n",
    "- **Mathematical Background**: \n",
    "- Probability theory and stochastic processes\n",
    "- Linear algebra and matrix operations\n",
    "- Optimization and gradient-based methods\n",
    "- Information theory (KL divergence, entropy)\n",
    "\n",
    "- **Technical Skills**:\n",
    "- Python programming with PyTorch\n",
    "- Deep learning fundamentals (neural networks, autoencoders)\n",
    "- Basic reinforcement learning concepts (MDPs, value functions, policies)\n",
    "- Experience with Gymnasium environments\n",
    "\n",
    "- **Prior Knowledge**:\n",
    "- Completion of CA1-CA12 assignments\n",
    "- Understanding of model-free RL algorithms (DQN, policy gradients)\n",
    "- Familiarity with neural network architectures\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "This assignment is structured as follows:\n",
    "\n",
    "### Section 1: Model-free Vs Model-based Reinforcement Learning\n",
    "- Theoretical foundations of model-free and model-based approaches\n",
    "- Mathematical formulations and trade-off analysis\n",
    "- Hybrid algorithms combining both paradigms\n",
    "- Practical implementation and comparison\n",
    "\n",
    "### Section 2: World Models and Imagination-based Learning\n",
    "- Variational autoencoders for world modeling\n",
    "- Stochastic dynamics prediction in latent space\n",
    "- Imagination-based planning and policy optimization\n",
    "- Dreamer algorithm and modern variants\n",
    "\n",
    "### Section 3: Sample Efficiency and Transfer Learning\n",
    "- Prioritized experience replay and data augmentation\n",
    "- Auxiliary tasks for improved learning\n",
    "- Transfer learning techniques and meta-learning\n",
    "- Domain adaptation and curriculum learning\n",
    "\n",
    "### Section 4: Hierarchical Reinforcement Learning\n",
    "- Options framework and temporal abstraction\n",
    "- Hierarchical policy architectures\n",
    "- Skill discovery and composition\n",
    "- Applications to complex task domains\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "CA13/\n",
    "├── CA13.ipynb              # Main assignment notebook\n",
    "├── agents/                 # RL agent implementations\n",
    "│   ├── model*free*agent.py # Model-free RL agents\n",
    "│   ├── model*based*agent.py# Model-based RL agents\n",
    "│   ├── world*model*agent.py# World model-based agents\n",
    "│   └── hierarchical_agent.py# Hierarchical RL agents\n",
    "├── models/                 # Neural network architectures\n",
    "│   ├── world_model.py      # VAE-based world models\n",
    "│   ├── dynamics_model.py   # Environment dynamics models\n",
    "│   └── policy_networks.py  # Hierarchical policy networks\n",
    "├── environments/           # Custom environments\n",
    "│   ├── wrappers.py         # Environment wrappers\n",
    "│   └── complex_tasks.py    # Complex task environments\n",
    "├── experiments/            # Training and evaluation scripts\n",
    "│   ├── train*world*model.py# World model training\n",
    "│   ├── compare_efficiency.py# Sample efficiency comparison\n",
    "│   └── transfer_learning.py# Transfer learning experiments\n",
    "└── utils/                  # Utility functions\n",
    "    ├── visualization.py    # Plotting and analysis tools\n",
    "    ├── data_augmentation.py# Data augmentation utilities\n",
    "    └── evaluation.py       # Performance evaluation metrics\n",
    "```\n",
    "\n",
    "## Contents Overview\n",
    "\n",
    "### Theoretical Foundations\n",
    "- **Model-Based RL Mathematics**: Transition and reward model learning, planning algorithms\n",
    "- **World Model Theory**: Variational inference, latent space dynamics, imagination-based learning\n",
    "- **Sample Efficiency**: Experience replay, prioritization, auxiliary learning objectives\n",
    "- **Transfer Learning**: Representation learning, fine-tuning, meta-learning algorithms\n",
    "\n",
    "### Implementation Components\n",
    "- **VAE World Models**: Encoder-decoder architectures with stochastic latent variables\n",
    "- **Imagination-Based Agents**: Planning in learned latent space using world models\n",
    "- **Sample-Efficient Algorithms**: Prioritized replay, data augmentation, auxiliary tasks\n",
    "- **Transfer Learning Systems**: Multi-task learning, fine-tuning, domain adaptation\n",
    "\n",
    "### Advanced Topics\n",
    "- **Hierarchical RL**: Options framework, skill hierarchies, temporal abstraction\n",
    "- **Meta-Learning**: Few-shot adaptation, gradient-based meta-learning\n",
    "- **Curriculum Learning**: Automatic difficulty progression, teacher-student frameworks\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Your implementation will be evaluated based on:\n",
    "\n",
    "1. **Correctness (40%)**: Accurate implementation of algorithms and mathematical formulations\n",
    "2. **Efficiency (25%)**: Sample efficiency improvements and computational performance\n",
    "3. **Innovation (20%)**: Creative extensions and novel approaches to the problems\n",
    "4. **Analysis (15%)**: Quality of experimental analysis and insights\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Environment Setup**: Ensure all dependencies are installed\n",
    "2. **Code Review**: Understand the provided base implementations\n",
    "3. **Incremental Development**: Start with simpler components and build complexity\n",
    "4. **Testing**: Validate each component before integration\n",
    "5. **Experimentation**: Run comprehensive experiments and analyze results\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this assignment, you will have:\n",
    "\n",
    "- **Comprehensive Understanding**: Deep knowledge of advanced model-based RL techniques\n",
    "- **Practical Skills**: Ability to implement complex RL systems from scratch\n",
    "- **Research Perspective**: Insight into current challenges and future directions\n",
    "- **Portfolio Piece**: High-quality implementation demonstrating advanced RL capabilities\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This assignment represents the culmination of the Deep RL course, integrating concepts from model-free and model-based learning, advanced architectures, and practical deployment considerations. Focus on understanding the theoretical foundations while developing robust, efficient implementations.\n",
    "\n",
    "Let's begin our exploration of advanced model-based reinforcement learning and world models! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5ed79da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "✅ CA13 modules imported successfully\n"
     ]
    }
   ],
   "source": [
    "# Import implementations from CA13 package modules\n",
    "from agents.model_free import ModelFreeAgent, DQNAgent\n",
    "from agents.model_based import ModelBasedAgent\n",
    "from buffers.replay_buffer import ReplayBuffer\n",
    "from environments.grid_world import SimpleGridWorld\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "print(\"✅ CA13 modules imported successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398420da",
   "metadata": {},
   "source": [
    "# Section 2: World Models and Imagination-based Learning\n",
    "\n",
    "## 2.1 Theoretical Foundations of World Models\n",
    "\n",
    "World models represent learned internal representations of environment dynamics that enable agents to \"imagine\" and plan without direct interaction with the environment.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "**World Model Components:**\n",
    "1. **Representation Learning**: Encode high-dimensional observations into compact latent states\n",
    "2. **Dynamics Model**: Predict next latent state given current state and action\n",
    "3. **Reward Model**: Predict rewards in the latent space\n",
    "4. **Decoder Model**: Reconstruct observations from latent states\n",
    "\n",
    "**Mathematical Framework:**\n",
    "- **Encoder**: $z*t = \\text{Encode}(o*t)$ maps observation $o*t$ to latent state $z*t$\n",
    "- **Dynamics**: $z*{t+1} = f(z*t, a*t) + \\epsilon*t$ where $\\epsilon_t \\sim \\mathcal{N}(0, \\Sigma)$\n",
    "- **Reward**: $r*t = R(z*t, a_t)$\n",
    "- **Decoder**: $\\hat{o}*t = \\text{Decode}(z*t)$\n",
    "\n",
    "## 2.2 Variational World Models\n",
    "\n",
    "### Variational Autoencoders (vae) for World Modeling\n",
    "\n",
    "World models often use VAEs to learn stochastic latent representations:\n",
    "\n",
    "**Encoder (Recognition Model):**\n",
    "$$q*\\phi(z*t | o*t) = \\mathcal{N}(z*t; \\mu*\\phi(o*t), \\sigma*\\phi^2(o*t))$$\n",
    "\n",
    "**Prior (Dynamics Model):**\n",
    "$$p*\\theta(z*{t+1} | z*t, a*t) = \\mathcal{N}(z*{t+1}; \\mu*\\theta(z*t, a*t), \\sigma*\\theta^2(z*t, a_t))$$\n",
    "\n",
    "**Decoder (Generative Model):**\n",
    "$$p*\\psi(o*t | z*t) = \\mathcal{N}(o*t; \\mu*\\psi(z*t), \\sigma*\\psi^2(z*t))$$\n",
    "\n",
    "**ELBO Objective:**\n",
    "$$\\mathcal{L}*{ELBO} = \\mathbb{E}*{q*\\phi(z|o)} [\\log p*\\psi(o|z)] - D*{KL}[q*\\phi(z|o) || p(z)]$$\n",
    "\n",
    "## 2.3 Planning in Learned Latent Space\n",
    "\n",
    "Once a world model is learned, planning can be performed in the compact latent space:\n",
    "\n",
    "### Model Predictive Control (mpc) in Latent Space\n",
    "1. **Imagination Rollout**: Use world model to simulate future trajectories\n",
    "2. **Action Optimization**: Optimize action sequences to maximize predicted rewards\n",
    "3. **Execution**: Execute only the first action, then replan\n",
    "\n",
    "**Planning Objective:**\n",
    "$$a^**{1:H} = \\arg\\max*{a*{1:H}} \\mathbb{E}*{z*{1:H} \\sim p*\\theta} \\left[ \\sum*{t=1}^H R(z*t, a_t) \\right]$$\n",
    "\n",
    "### Dreamer Algorithm\n",
    "Dreamer combines world models with policy gradients:\n",
    "1. **Collect Experience**: Gather real environment data\n",
    "2. **Learn World Model**: Train VAE-based world model\n",
    "3. **Imagine Trajectories**: Generate synthetic experience in latent space  \n",
    "4. **Learn Behaviors**: Train actor-critic in imagined trajectories\n",
    "\n",
    "## 2.4 Advantages and Challenges\n",
    "\n",
    "### Advantages of World Models:\n",
    "- **Sample Efficiency**: Learn from imagined experience\n",
    "- **Transfer Learning**: Models can generalize across tasks\n",
    "- **Interpretability**: Learned representations can be visualized\n",
    "- **Planning**: Enable sophisticated planning algorithms\n",
    "\n",
    "### Challenges:\n",
    "- **Model Bias**: Errors compound during long rollouts\n",
    "- **Representation Learning**: High-dimensional observations are challenging\n",
    "- **Stochasticity**: Modeling complex stochastic dynamics\n",
    "- **Computational Cost**: Training and maintaining world models\n",
    "\n",
    "## 2.5 Modern Approaches\n",
    "\n",
    "### Muzero\n",
    "Combines tree search with learned models:\n",
    "- Learns value, policy, and dynamics jointly\n",
    "- Uses tree search for planning\n",
    "- Achieves superhuman performance in Go, Chess, and Shogi\n",
    "\n",
    "### Dreamer V2/v3\n",
    "Improvements to original Dreamer:\n",
    "- Better regularization techniques\n",
    "- Improved world model architectures\n",
    "- Enhanced policy learning in imagination\n",
    "\n",
    "### Model-based Meta-learning\n",
    "Using world models for few-shot adaptation:\n",
    "- Learn generalizable world model components\n",
    "- Quickly adapt to new environments\n",
    "- Transfer dynamics knowledge across domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6554d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ VariationalWorldModel imported from models.world_model package\n",
      "💡 This model provides VAE-based world modeling for learning environment dynamics\n"
     ]
    }
   ],
   "source": [
    "# Import VariationalWorldModel from package\n",
    "from models.world_model import VariationalWorldModel\n",
    "\n",
    "print(\"VariationalWorldModel imported from models.world_model package\")\n",
    "print(\"This model provides VAE-based world modeling for learning environment dynamics\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2799c6",
   "metadata": {},
   "source": [
    "# Section 3: Sample Efficiency and Transfer Learning\n",
    "\n",
    "## 3.1 Sample Efficiency Challenges in Deep Rl\n",
    "\n",
    "Sample efficiency is one of the most critical challenges in deep reinforcement learning, particularly for real-world applications where data collection is expensive or dangerous.\n",
    "\n",
    "### Why Is Sample Efficiency Important?\n",
    "\n",
    "**Real-World Constraints:**\n",
    "- **Cost**: Real-world interactions can be expensive (robotics, autonomous vehicles)\n",
    "- **Time**: Learning from millions of samples is often impractical\n",
    "- **Safety**: Exploratory actions in safety-critical domains can be dangerous\n",
    "- **Reproducibility**: Limited samples make experiments more reliable\n",
    "\n",
    "**Sample Complexity Factors:**\n",
    "- **Environment Complexity**: High-dimensional state/action spaces\n",
    "- **Sparse Rewards**: Learning signals are infrequent\n",
    "- **Stochasticity**: Environmental noise requires more samples\n",
    "- **Exploration**: Discovering good policies requires extensive exploration\n",
    "\n",
    "## 3.2 Sample Efficiency Techniques\n",
    "\n",
    "### 3.2.1 Experience Replay and Prioritization\n",
    "\n",
    "**Experience Replay Benefits:**\n",
    "- Reuse past experiences multiple times\n",
    "- Break temporal correlations in data\n",
    "- Enable off-policy learning\n",
    "\n",
    "**Prioritized Experience Replay:**\n",
    "Prioritize experiences based on temporal difference (TD) error:\n",
    "$$P(i) = \\frac{p*i^\\alpha}{\\sum*k p_k^\\alpha}$$\n",
    "\n",
    "Where $p*i = |\\delta*i| + \\epsilon$ and $\\delta_i$ is the TD error.\n",
    "\n",
    "### 3.2.2 Data Augmentation\n",
    "\n",
    "**Techniques:**\n",
    "- **Random Crops**: For image-based environments\n",
    "- **Color Jittering**: Robust to lighting variations  \n",
    "- **Random Shifts**: Translation invariance\n",
    "- **Gaussian Noise**: Regularization effect\n",
    "\n",
    "### 3.2.3 Auxiliary Tasks\n",
    "\n",
    "Learn multiple tasks simultaneously to improve sample efficiency:\n",
    "- **Pixel Control**: Predict pixel changes\n",
    "- **Feature Control**: Control learned feature representations\n",
    "- **Reward Prediction**: Predict future rewards\n",
    "- **Value Function Replay**: Replay value function updates\n",
    "\n",
    "## 3.3 Transfer Learning in Reinforcement Learning\n",
    "\n",
    "Transfer learning enables agents to leverage knowledge from previous tasks to learn new tasks more efficiently.\n",
    "\n",
    "### 3.3.1 Types of Transfer in Rl\n",
    "\n",
    "**Policy Transfer:**\n",
    "$$\\pi*{target}(a|s) = f(\\pi*{source}(a|s), s, \\theta_{adapt})$$\n",
    "\n",
    "**Value Function Transfer:**\n",
    "$$Q*{target}(s,a) = g(Q*{source}(s,a), s, a, \\phi_{adapt})$$\n",
    "\n",
    "**Representation Transfer:**\n",
    "$$\\phi*{target}(s) = h(\\phi*{source}(s), \\psi_{adapt})$$\n",
    "\n",
    "### 3.3.2 Transfer Learning Approaches\n",
    "\n",
    "#### Fine-tuning\n",
    "1. Pre-train on source task\n",
    "2. Initialize target model with source weights\n",
    "3. Fine-tune on target task with lower learning rate\n",
    "\n",
    "#### Progressive Networks\n",
    "- Freeze source network columns\n",
    "- Add new columns for target tasks\n",
    "- Use lateral connections between columns\n",
    "\n",
    "#### Universal Value Functions (uvf)\n",
    "Learn value functions conditioned on goals:\n",
    "$$Q(s, a, g) = \\text{Value of action } a \\text{ in state } s \\text{ for goal } g$$\n",
    "\n",
    "## 3.4 Meta-learning and Few-shot Adaptation\n",
    "\n",
    "Meta-learning enables agents to quickly adapt to new tasks with limited experience.\n",
    "\n",
    "### 3.4.1 Model-agnostic Meta-learning (maml)\n",
    "\n",
    "**Objective:**\n",
    "$$\\min*\\theta \\sum*{\\tau \\sim p(\\mathcal{T})} \\mathcal{L}*\\tau(f*{\\theta_\\tau'})$$\n",
    "\n",
    "Where $\\theta*\\tau' = \\theta - \\alpha \\nabla*\\theta \\mathcal{L}*\\tau(f*\\theta)$\n",
    "\n",
    "**MAML Algorithm:**\n",
    "1. Sample batch of tasks\n",
    "2. For each task, compute adapted parameters via gradient descent\n",
    "3. Update meta-parameters using gradient through adaptation process\n",
    "\n",
    "### 3.4.2 Gradient-based Meta-learning\n",
    "\n",
    "**Reptile Algorithm:**\n",
    "Simpler alternative to MAML:\n",
    "$$\\theta \\leftarrow \\theta + \\beta \\frac{1}{n} \\sum*{i=1}^n (\\phi*i - \\theta)$$\n",
    "\n",
    "Where $\\phi_i$ is the result of training on task $i$.\n",
    "\n",
    "## 3.5 Domain Adaptation and Sim-to-real Transfer\n",
    "\n",
    "### 3.5.1 Domain Randomization\n",
    "\n",
    "**Technique:**\n",
    "Randomize simulation parameters during training:\n",
    "- Physical properties (mass, friction, damping)\n",
    "- Visual appearance (textures, lighting, colors)\n",
    "- Sensor characteristics (noise, resolution, field of view)\n",
    "\n",
    "**Benefits:**\n",
    "- Learned policies are robust to domain variations\n",
    "- Improved transfer from simulation to real world\n",
    "- Reduced need for domain-specific engineering\n",
    "\n",
    "### 3.5.2 Domain Adversarial Training\n",
    "\n",
    "**Objective:**\n",
    "$$\\min*\\theta \\mathcal{L}*{task}(\\theta) + \\lambda \\mathcal{L}_{domain}(\\theta)$$\n",
    "\n",
    "Where $\\mathcal{L}_{domain}$ encourages domain-invariant features.\n",
    "\n",
    "## 3.6 Curriculum Learning\n",
    "\n",
    "Structure learning to progress from simple to complex tasks.\n",
    "\n",
    "### 3.6.1 Curriculum Design Principles\n",
    "\n",
    "**Manual Curriculum:**\n",
    "- Hand-designed progression of tasks\n",
    "- Expert knowledge of difficulty ordering\n",
    "- Fixed curriculum regardless of agent performance\n",
    "\n",
    "**Automatic Curriculum:**\n",
    "- Adaptive task selection based on agent performance\n",
    "- Learning progress as curriculum signal\n",
    "- Self-paced learning approaches\n",
    "\n",
    "### 3.6.2 Curriculum Learning Algorithms\n",
    "\n",
    "**Teacher-Student Framework:**\n",
    "- Teacher selects appropriate tasks for student\n",
    "- Task difficulty based on student's current capability\n",
    "- Optimize task selection for maximum learning progress\n",
    "\n",
    "**Self-Play Curriculum:**\n",
    "- Agent plays against previous versions of itself\n",
    "- Automatic difficulty adjustment\n",
    "- Prevents catastrophic forgetting of simpler strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96bcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PrioritizedReplayBuffer:\n",
    "    \"\"\"Prioritized experience replay buffer for improved sample efficiency.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity, alpha=0.6, beta=0.4, beta_increment=1e-4):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # Priority exponent\n",
    "        self.beta = beta    # Importance sampling exponent\n",
    "        self.beta_increment = beta_increment\n",
    "        \n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "        self.max_priority = 1.0\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience to buffer with maximum priority.\"\"\"\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        \n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.priorities[self.position] = self.max_priority\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch with prioritized sampling.\"\"\"\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return None\n",
    "        \n",
    "        valid_priorities = self.priorities[:len(self.buffer)]\n",
    "        probs = valid_priorities ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "        \n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        \n",
    "        experiences = [self.buffer[idx] for idx in indices]\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        \n",
    "        total = len(self.buffer)\n",
    "        weights = (total * probs[indices]) ** (-self.beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        self.beta = min(1.0, self.beta + self.beta_increment)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones), indices, weights\n",
    "    \n",
    "    def update_priorities(self, indices, td_errors):\n",
    "        \"\"\"Update priorities based on TD errors.\"\"\"\n",
    "        for idx, td_error in zip(indices, td_errors):\n",
    "            priority = (abs(td_error) + 1e-6) ** self.alpha\n",
    "            self.priorities[idx] = priority\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DataAugmentationDQN(nn.Module):\n",
    "    \"\"\"DQN with data augmentation for improved sample efficiency.\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, action_dim, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.reward_predictor = nn.Sequential(\n",
    "            nn.Linear(input_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        \n",
    "        self.next_state_predictor = nn.Sequential(\n",
    "            nn.Linear(input_dim + action_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state, action=None):\n",
    "        \"\"\"Forward pass with optional auxiliary predictions.\"\"\"\n",
    "        q_values = self.q_network(state)\n",
    "        \n",
    "        if action is not None:\n",
    "            if len(action.shape) == 1:\n",
    "                action_one_hot = F.one_hot(action.long(), self.action_dim).float()\n",
    "            else:\n",
    "                action_one_hot = action\n",
    "            \n",
    "            aux_input = torch.cat([state, action_one_hot], dim=-1)\n",
    "            reward_pred = self.reward_predictor(aux_input)\n",
    "            next_state_pred = self.next_state_predictor(aux_input)\n",
    "            \n",
    "            return q_values, reward_pred, next_state_pred\n",
    "        \n",
    "        return q_values\n",
    "    \n",
    "    def apply_augmentation(self, state, augmentation_type='noise'):\n",
    "        \"\"\"Apply data augmentation to state.\"\"\"\n",
    "        if augmentation_type == 'noise':\n",
    "            noise = torch.randn_like(state) * 0.1\n",
    "            return state + noise\n",
    "        \n",
    "        elif augmentation_type == 'dropout':\n",
    "            dropout_mask = torch.rand_like(state) > 0.1\n",
    "            return state * dropout_mask.float()\n",
    "        \n",
    "        elif augmentation_type == 'scaling':\n",
    "            scale = torch.rand(1).item() * 0.4 + 0.8  # Scale between 0.8 and 1.2\n",
    "            return state * scale\n",
    "        \n",
    "        return state\n",
    "\n",
    "class SampleEfficientAgent:\n",
    "    \"\"\"Agent with multiple sample efficiency techniques.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.network = DataAugmentationDQN(state_dim, action_dim)\n",
    "        self.target_network = copy.deepcopy(self.network)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=lr)\n",
    "        \n",
    "        self.replay_buffer = PrioritizedReplayBuffer(capacity=10000)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.target_update_freq = 100\n",
    "        self.update_count = 0\n",
    "        \n",
    "        self.aux_reward_weight = 0.1\n",
    "        self.aux_dynamics_weight = 0.1\n",
    "        \n",
    "        self.losses = []\n",
    "        self.td_errors = []\n",
    "    \n",
    "    def act(self, state, epsilon=0.1):\n",
    "        \"\"\"Select action with epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.network(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def update(self, batch_size=32, use_aux_tasks=True, augmentation=True):\n",
    "        \"\"\"Update agent with prioritized replay and auxiliary tasks.\"\"\"\n",
    "        sample_result = self.replay_buffer.sample(batch_size)\n",
    "        if sample_result is None:\n",
    "            return None\n",
    "        \n",
    "        experiences, indices, weights = sample_result\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        \n",
    "        if augmentation:\n",
    "            aug_type = np.random.choice(['noise', 'dropout', 'scaling'])\\n            states = self.network.apply_augmentation(states, aug_type)\n",
    "            next_states = self.network.apply_augmentation(next_states, aug_type)\n",
    "        \n",
    "        current_q_values = self.network(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * next_q_values * (~dones))\n",
    "        \n",
    "        td_errors = (current_q_values.squeeze() - target_q_values).detach().numpy()\n",
    "        \n",
    "        q_loss = (weights * F.mse_loss(current_q_values.squeeze(), target_q_values, reduction='none')).mean()\n",
    "        \n",
    "        total_loss = q_loss\n",
    "        \n",
    "        if use_aux_tasks:\n",
    "            q_values, reward_pred, next_state_pred = self.network(states, actions)\n",
    "            \n",
    "            aux_reward_loss = F.mse_loss(reward_pred.squeeze(), rewards)\n",
    "            aux_dynamics_loss = F.mse_loss(next_state_pred, next_states)\n",
    "            \n",
    "            total_loss += self.aux_reward_weight * aux_reward_loss\n",
    "            total_loss += self.aux_dynamics_weight * aux_dynamics_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.replay_buffer.update_priorities(indices, td_errors)\n",
    "        \n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        \n",
    "        self.losses.append(total_loss.item())\n",
    "        self.td_errors.extend(td_errors.tolist())\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'q_loss': q_loss.item(),\n",
    "            'aux_reward_loss': aux_reward_loss.item() if use_aux_tasks else 0,\n",
    "            'aux_dynamics_loss': aux_dynamics_loss.item() if use_aux_tasks else 0\n",
    "        }\n",
    "\n",
    "class TransferLearningAgent:\n",
    "    \"\"\"Agent with transfer learning capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.feature_extractor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.policy_heads = {}\n",
    "        self.value_heads = {}\n",
    "        \n",
    "        self.feature_optimizer = optim.Adam(self.feature_extractor.parameters(), lr=lr)\n",
    "        self.head_optimizers = {}\n",
    "        \n",
    "        self.transfer_performance = {}\n",
    "    \n",
    "    def add_task(self, task_name, action_dim=None):\n",
    "        \"\"\"Add a new task with its own policy and value heads.\"\"\"\n",
    "        if action_dim is None:\n",
    "            action_dim = self.action_dim\n",
    "        \n",
    "        self.policy_heads[task_name] = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        self.value_heads[task_name] = nn.Sequential(\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        task_params = list(self.policy_heads[task_name].parameters()) + \\\n",
    "                     list(self.value_heads[task_name].parameters())\n",
    "        self.head_optimizers[task_name] = optim.Adam(task_params, lr=1e-3)\n",
    "        \n",
    "        self.transfer_performance[task_name] = []\n",
    "    \n",
    "    def get_action(self, state, task_name):\n",
    "        \"\"\"Get action for specific task.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            features = self.feature_extractor(state_tensor)\n",
    "            action_probs = self.policy_heads[task_name](features)\n",
    "            return Categorical(action_probs).sample().item()\n",
    "    \n",
    "    def get_value(self, state, task_name):\n",
    "        \"\"\"Get value estimate for specific task.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            features = self.feature_extractor(state_tensor)\n",
    "            return self.value_heads[task_name](features).item()\n",
    "    \n",
    "    def update(self, states, actions, rewards, task_name, update_features=True):\n",
    "        \"\"\"Update agent for specific task.\"\"\"\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        \n",
    "        features = self.feature_extractor(states)\n",
    "        action_probs = self.policy_heads[task_name](features)\n",
    "        values = self.value_heads[task_name](features).squeeze()\n",
    "        \n",
    "        log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1))).squeeze()\n",
    "        advantages = rewards - values.detach()\n",
    "        policy_loss = -(log_probs * advantages).mean()\n",
    "        \n",
    "        value_loss = F.mse_loss(values, rewards)\n",
    "        \n",
    "        total_loss = policy_loss + 0.5 * value_loss\n",
    "        \n",
    "        self.head_optimizers[task_name].zero_grad()\n",
    "        if update_features:\n",
    "            self.feature_optimizer.zero_grad()\n",
    "        \n",
    "        total_loss.backward()\n",
    "        \n",
    "        self.head_optimizers[task_name].step()\n",
    "        if update_features:\n",
    "            self.feature_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item()\n",
    "        }\n",
    "    \n",
    "    def fine_tune_for_task(self, source_task, target_task, fine_tune_lr=1e-4):\n",
    "        \"\"\"Fine-tune from source task to target task.\"\"\"\n",
    "        self.policy_heads[target_task] = copy.deepcopy(self.policy_heads[source_task])\n",
    "        self.value_heads[target_task] = copy.deepcopy(self.value_heads[source_task])\n",
    "        \n",
    "        task_params = list(self.policy_heads[target_task].parameters()) + \\\n",
    "                     list(self.value_heads[target_task].parameters())\n",
    "        self.head_optimizers[target_task] = optim.Adam(task_params, lr=fine_tune_lr)\n",
    "        \n",
    "        self.transfer_performance[target_task] = []\n",
    "\n",
    "class CurriculumLearningFramework:\n",
    "    \"\"\"Framework for curriculum learning with automatic difficulty adjustment.\"\"\"\n",
    "    \n",
    "    def __init__(self, environments, agent, difficulty_measure='success_rate'):\n",
    "        self.environments = environments  # List of environments with increasing difficulty\n",
    "        self.agent = agent\n",
    "        self.difficulty_measure = difficulty_measure\n",
    "        \n",
    "        self.current_level = 0\n",
    "        self.level_performance = [[] for _ in environments]\n",
    "        self.progression_threshold = 0.8  # Success rate threshold to advance\n",
    "        self.regression_threshold = 0.3   # Success rate threshold to regress\n",
    "        \n",
    "        self.curriculum_history = []\n",
    "    \n",
    "    def get_current_environment(self):\n",
    "        \"\"\"Get current environment based on curriculum level.\"\"\"\n",
    "        return self.environments[self.current_level]\n",
    "    \n",
    "    def evaluate_performance(self, episode_rewards, episode_successes=None):\n",
    "        \"\"\"Evaluate performance on current level.\"\"\"\n",
    "        if self.difficulty_measure == 'success_rate' and episode_successes is not None:\n",
    "            return np.mean(episode_successes[-10:]) if len(episode_successes) >= 10 else 0\n",
    "        elif self.difficulty_measure == 'reward':\n",
    "            return np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else 0\n",
    "        else:\n",
    "            return np.mean(episode_rewards[-10:]) if len(episode_rewards) >= 10 else 0\n",
    "    \n",
    "    def update_curriculum(self, performance):\n",
    "        \"\"\"Update curriculum level based on performance.\"\"\"\n",
    "        old_level = self.current_level\n",
    "        \n",
    "        if performance >= self.progression_threshold and self.current_level < len(self.environments) - 1:\n",
    "            self.current_level += 1\n",
    "            print(f\\\"📈 Advanced to level {self.current_level} (performance: {performance:.2f})\\\"\n",
    "        \n",
    "        elif performance < self.regression_threshold and self.current_level > 0:\n",
    "            self.current_level = max(0, self.current_level - 1)\n",
    "            print(f\\\"📉 Regressed to level {self.current_level} (performance: {performance:.2f})\\\"\n",
    "        \n",
    "        if old_level != self.current_level:\n",
    "            self.curriculum_history.append({\n",
    "                'episode': len(self.level_performance[old_level]),\n",
    "                'old_level': old_level,\n",
    "                'new_level': self.current_level,\n",
    "                'performance': performance\n",
    "            })\n",
    "        \n",
    "        return self.current_level != old_level\n",
    "    \n",
    "    def train_with_curriculum(self, num_episodes=1000):\n",
    "        \"\"\"Train agent using curriculum learning.\"\"\"\n",
    "        episode_rewards = []\n",
    "        episode_successes = []\n",
    "        \n",
    "        for episode in range(num_episodes):\n",
    "            env = self.get_current_environment()\n",
    "            \n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_success = False\n",
    "            \n",
    "            for step in range(100):  # Max episode length\n",
    "                action = self.agent.act(state, epsilon=max(0.1, 1.0 - episode/500))\n",
    "                next_state, reward, done, info = env.step(action)\n",
    "                \n",
    "                self.agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                \n",
    "                episode_reward += reward\n",
    "                if done and reward > 5:  # Define success condition\n",
    "                    episode_success = True\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            if len(self.agent.replay_buffer) > 32:\n",
    "                self.agent.update(32)\n",
    "            \n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_successes.append(episode_success)\n",
    "            self.level_performance[self.current_level].append(episode_reward)\n",
    "            \n",
    "            if episode % 20 == 0:\n",
    "                performance = self.evaluate_performance(episode_rewards, episode_successes)\n",
    "                self.update_curriculum(performance)\n",
    "            \n",
    "            if episode % 100 == 0:\n",
    "                recent_reward = np.mean(episode_rewards[-10:])\n",
    "                recent_success = np.mean(episode_successes[-10:])\n",
    "                print(f\\\"Episode {episode}: Level {self.current_level}, \\\"\n",
    "                      f\\\"Reward: {recent_reward:.2f}, Success: {recent_success:.2f}\\\"\n",
    "        \n",
    "        return episode_rewards, episode_successes\n",
    "\n",
    "def compare_sample_efficiency():\n",
    "    \\\"\\\"\\\"Compare sample efficiency of different techniques.\\\"\\\"\\\"\n",
    "    print(\\\"⚡ Comparing Sample Efficiency Techniques\\\")\n",
    "    \n",
    "    env = SimpleGridWorld(size=6)\n",
    "    \n",
    "    baseline_agent = DQNAgent(state_dim=2, action_dim=4)\n",
    "    efficient_agent = SampleEfficientAgent(state_dim=2, action_dim=4)\n",
    "    \n",
    "    agents = {\n",
    "        'Baseline DQN': baseline_agent,\n",
    "        'Sample Efficient': efficient_agent\n",
    "    }\n",
    "    \n",
    "    results = {name: {'rewards': [], 'episodes': []} for name in agents.keys()}\n",
    "    \n",
    "    num_episodes = 300\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        for agent_name, agent in agents.items():\n",
    "            state = env.reset()\n",
    "            episode_reward = 0\n",
    "            \n",
    "            for step in range(50):\n",
    "                action = agent.act(state, epsilon=max(0.1, 1.0 - episode/200))\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if agent_name == 'Baseline DQN':\n",
    "                    agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                    if len(agent.replay_buffer) > 32:\n",
    "                        batch = agent.replay_buffer.sample(32)\n",
    "                        agent.update(batch)\n",
    "                else:\n",
    "                    agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                    if len(agent.replay_buffer) > 32:\n",
    "                        agent.update(32)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            results[agent_name]['rewards'].append(episode_reward)\n",
    "            results[agent_name]['episodes'].append(episode)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def demonstrate_transfer_learning():\n",
    "    \\\"\\\"\\\"Demonstrate transfer learning between related tasks.\\\"\\\"\\\"\n",
    "    print(\\\"🔄 Demonstrating Transfer Learning\\\")\n",
    "    \n",
    "    agent = TransferLearningAgent(state_dim=2, action_dim=4)\n",
    "    \n",
    "    def create_task_env(goal_position, reward_scale=1.0):\n",
    "        env = SimpleGridWorld(size=4)\n",
    "        env.goal = goal_position\n",
    "        env.reward_scale = reward_scale\n",
    "        return env\n",
    "    \n",
    "    tasks = {\n",
    "        'task_1': create_task_env([3, 3], 1.0),     # Original goal\n",
    "        'task_2': create_task_env([3, 0], 1.0),     # Different goal\n",
    "        'task_3': create_task_env([0, 3], 1.0),     # Another goal\n",
    "    }\n",
    "    \n",
    "    for task_name in tasks.keys():\n",
    "        agent.add_task(task_name)\n",
    "    \n",
    "    print(\\\"Training on Task 1...\\\")\n",
    "    task_1_env = tasks['task_1']\n",
    "    \n",
    "    for episode in range(200):\n",
    "        state = task_1_env.reset()\n",
    "        episode_states, episode_actions, episode_rewards = [], [], []\n",
    "        \n",
    "        for step in range(30):\n",
    "            action = agent.get_action(state, 'task_1')\n",
    "            next_state, reward, done, _ = task_1_env.step(action)\n",
    "            \n",
    "            episode_states.append(state)\n",
    "            episode_actions.append(action)\n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        if episode_rewards:\n",
    "            agent.update(episode_states, episode_actions, episode_rewards, 'task_1')\n",
    "    \n",
    "    transfer_results = {}\n",
    "    \n",
    "    for new_task in ['task_2', 'task_3']:\n",
    "        print(f\\\"Transferring to {new_task}...\\\")\n",
    "        \n",
    "        agent.fine_tune_for_task('task_1', new_task)\n",
    "        \n",
    "        task_env = tasks[new_task]\n",
    "        task_rewards = []\n",
    "        \n",
    "        for episode in range(50):  # Limited training\n",
    "            state = task_env.reset()\n",
    "            episode_reward = 0\n",
    "            episode_states, episode_actions, episode_rewards = [], [], []\n",
    "            \n",
    "            for step in range(30):\n",
    "                action = agent.get_action(state, new_task)\n",
    "                next_state, reward, done, _ = task_env.step(action)\n",
    "                \n",
    "                episode_states.append(state)\n",
    "                episode_actions.append(action)\n",
    "                episode_rewards.append(reward)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            if episode_rewards:\n",
    "                agent.update(episode_states, episode_actions, episode_rewards, \n",
    "                           new_task, update_features=False)\n",
    "            \n",
    "            task_rewards.append(episode_reward)\n",
    "        \n",
    "        transfer_results[new_task] = task_rewards\n",
    "        print(f\\\"  Final performance on {new_task}: {np.mean(task_rewards[-10:]):.2f}\\\")\n",
    "    \n",
    "    return transfer_results\n",
    "\n",
    "print(\\\"🚀 Starting Sample Efficiency and Transfer Learning Demonstrations!\\\")\n",
    "\n",
    "efficiency_results = compare_sample_efficiency()\n",
    "\n",
    "transfer_results = demonstrate_transfer_learning()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "for agent_name, data in efficiency_results.items():\n",
    "    window_size = 20\n",
    "    if len(data['rewards']) >= window_size:\n",
    "        smoothed_rewards = pd.Series(data['rewards']).rolling(window_size).mean()\n",
    "        axes[0].plot(data['episodes'], smoothed_rewards, label=agent_name, linewidth=2)\n",
    "\n",
    "axes[0].set_title('Sample Efficiency Comparison')\n",
    "axes[0].set_xlabel('Episode')\n",
    "axes[0].set_ylabel('Episode Reward (Smoothed)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "for task_name, rewards in transfer_results.items():\n",
    "    axes[1].plot(rewards, label=f'Transfer to {task_name}', linewidth=2)\n",
    "\n",
    "axes[1].set_title('Transfer Learning Performance')\n",
    "axes[1].set_xlabel('Episode (Limited Training)')\n",
    "axes[1].set_ylabel('Episode Reward')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\\\"\\\\n📊 Sample Efficiency Results:\\\")\n",
    "for agent_name, data in efficiency_results.items():\n",
    "    final_perf = np.mean(data['rewards'][-20:])\n",
    "    print(f\\\"  {agent_name}: {final_perf:.2f} final performance\\\")\n",
    "\n",
    "print(\\\"\\\\n🔄 Transfer Learning Results:\\\")\n",
    "for task_name, rewards in transfer_results.items():\n",
    "    final_perf = np.mean(rewards[-10:])\n",
    "    print(f\\\"  {task_name}: {final_perf:.2f} final performance with limited training\\\")\n",
    "\n",
    "print(\\\"\\\\n💡 Key Insights:\\\")\n",
    "print(\\\"  • Prioritized replay and auxiliary tasks improve sample efficiency\\\")\n",
    "print(\\\"  • Data augmentation provides regularization benefits\\\")\n",
    "print(\\\"  • Transfer learning enables rapid adaptation to new tasks\\\")\n",
    "print(\\\"  • Shared representations capture generalizable knowledge\\\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62229db9",
   "metadata": {},
   "source": [
    "# Section 4: Hierarchical Reinforcement Learning\n",
    "\n",
    "## 4.1 Theory: Hierarchical Decision Making\n",
    "\n",
    "Hierarchical Reinforcement Learning (HRL) addresses the challenge of learning complex behaviors by decomposing tasks into hierarchical structures. This approach enables agents to:\n",
    "\n",
    "1. **Learn at Multiple Time Scales**: High-level policies select goals or skills, while low-level policies execute primitive actions\n",
    "2. **Achieve Better Generalization**: Skills learned in one context can be reused in others\n",
    "3. **Improve Sample Efficiency**: By leveraging temporal abstractions and skill composition\n",
    "\n",
    "### Key Components\n",
    "\n",
    "#### Options Framework\n",
    "An **option** $\\omega$ is defined by a tuple $(I*\\omega, \\pi*\\omega, \\beta_\\omega)$:\n",
    "- **Initiation Set** $I_\\omega \\subseteq \\mathcal{S}$: States where the option can be initiated\n",
    "- **Policy** $\\pi_\\omega: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0,1]$: Action selection within the option\n",
    "- **Termination Condition** $\\beta_\\omega: \\mathcal{S} \\rightarrow [0,1]$: Probability of termination\n",
    "\n",
    "#### Hierarchical Value Functions\n",
    "The value function for options follows the Bellman equation:\n",
    "$$Q^\\pi(s,\\omega) = \\mathbb{E}*\\pi\\left[\\sum*{t=0}^{\\tau-1} \\gamma^t r*{t+1} + \\gamma^\\tau Q^\\pi(s*\\tau, \\omega') \\mid s*0=s, \\omega*0=\\omega\\right]$$\n",
    "\n",
    "where $\\tau$ is the termination time and $\\omega'$ is the next option selected.\n",
    "\n",
    "#### Feudal Networks\n",
    "Feudal Networks implement a manager-worker hierarchy:\n",
    "- **Manager Network**: Sets goals $g*t$ for workers: $g*t = f*{manager}(s*t, h_{t-1}^{manager})$\n",
    "- **Worker Network**: Executes actions conditioned on goals: $a*t = \\pi*{worker}(s*t, g*t)$\n",
    "- **Intrinsic Motivation**: Workers receive intrinsic rewards based on goal achievement\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### Intrinsic Reward Signal\n",
    "The intrinsic reward for achieving subgoals:\n",
    "$$r*t^{intrinsic} = \\cos(\\text{achieved\\*goal}*t - \\text{desired\\*goal}*t) \\cdot ||s*{t+1} - s_t||$$\n",
    "\n",
    "#### Hierarchical Policy Gradient\n",
    "The gradient for the manager policy:\n",
    "$$\\nabla*{\\theta*m} J*m = \\mathbb{E}\\left[\\nabla*{\\theta*m} \\log \\pi*m(g*t|s*t) \\cdot A*m(s*t, g_t)\\right]$$\n",
    "\n",
    "And for the worker policy:\n",
    "$$\\nabla*{\\theta*w} J*w = \\mathbb{E}\\left[\\nabla*{\\theta*w} \\log \\pi*w(a*t|s*t, g*t) \\cdot A*w(s*t, a*t, g_t)\\right]$$\n",
    "\n",
    "## 4.2 Implementation: Hierarchical Rl Architectures\n",
    "\n",
    "We'll implement several HRL approaches:\n",
    "1. **Options-Critic Architecture**: Learn options and policies jointly\n",
    "2. **Feudal Networks**: Manager-worker hierarchies\n",
    "3. **Hindsight Experience Replay with Goals**: Sample efficiency for goal-conditioned tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c601a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Hierarchical RL classes imported from agents.hierarchical package\n",
      "💡 Includes Options-Critic and Feudal Networks for hierarchical decision making\n"
     ]
    }
   ],
   "source": [
    "# Import hierarchical RL classes from package\n",
    "from agents.hierarchical import (\n",
    "    OptionsCriticNetwork,\n",
    "    OptionsCriticAgent,\n",
    "    FeudalNetwork,\n",
    "    FeudalAgent\n",
    ")\n",
    "\n",
    "print(\"Hierarchical RL classes imported from agents.hierarchical package\")\n",
    "print(\"Includes Options-Critic and Feudal Networks for hierarchical decision making\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3cb83f",
   "metadata": {},
   "source": [
    "# Section 5: Comprehensive Evaluation and Advanced Techniques Integration\n",
    "\n",
    "## 5.1 Multi-method Performance Analysis\n",
    "\n",
    "This section provides comprehensive evaluation comparing all implemented advanced Deep RL techniques:\n",
    "\n",
    "### Performance Metrics\n",
    "1. **Sample Efficiency**: Episodes to convergence\n",
    "2. **Final Performance**: Asymptotic reward\n",
    "3. **Robustness**: Performance variance\n",
    "4. **Computational Efficiency**: Training time and memory usage\n",
    "5. **Transfer Capability**: Performance on related tasks\n",
    "\n",
    "### Evaluation Framework\n",
    "We evaluate methods across multiple dimensions:\n",
    "- **Simple Tasks**: Basic navigation and control\n",
    "- **Complex Tasks**: Multi-step reasoning and planning\n",
    "- **Transfer Tasks**: Adaptation to new environments\n",
    "- **Long-Horizon Tasks**: Extended episode planning\n",
    "\n",
    "## 5.2 Practical Implementation Considerations\n",
    "\n",
    "### When to Use Each Method:\n",
    "\n",
    "#### Model-free Methods (dqn, Policy Gradient)\n",
    "- ✅ **Use when**: Simple tasks, abundant data, unknown dynamics\n",
    "- ❌ **Avoid when**: Sample efficiency critical, complex planning needed\n",
    "\n",
    "#### Model-based Methods\n",
    "- ✅ **Use when**: Sample efficiency critical, dynamics learnable\n",
    "- ❌ **Avoid when**: High-dimensional observations, stochastic dynamics\n",
    "\n",
    "#### World Models\n",
    "- ✅ **Use when**: Rich sensory input, imagination beneficial\n",
    "- ❌ **Avoid when**: Simple state spaces, real-time constraints\n",
    "\n",
    "#### Hierarchical Methods\n",
    "- ✅ **Use when**: Long-horizon tasks, reusable skills needed\n",
    "- ❌ **Avoid when**: Simple tasks, flat action spaces\n",
    "\n",
    "#### Sample Efficiency Techniques\n",
    "- ✅ **Use when**: Limited data, expensive environments\n",
    "- ❌ **Avoid when**: Abundant cheap data, simple tasks\n",
    "\n",
    "## 5.3 Advanced Techniques Summary\n",
    "\n",
    "This comprehensive assignment covered cutting-edge Deep RL methods:\n",
    "\n",
    "### Core Contributions:\n",
    "1. **Sample Efficiency**: Prioritized replay, data augmentation, auxiliary tasks\n",
    "2. **World Models**: VAE-based dynamics, imagination planning\n",
    "3. **Transfer Learning**: Shared representations, meta-learning\n",
    "4. **Hierarchical Learning**: Options framework, feudal networks\n",
    "5. **Integration**: Multi-method evaluation and practical guidelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d0267",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AdvancedRLEvaluator:\n",
    "    \"\"\"Comprehensive evaluation framework for advanced RL methods.\"\"\"\n",
    "    \n",
    "    def __init__(self, environments, agents, metrics=['reward', 'sample_efficiency', 'robustness']):\n",
    "        self.environments = environments\n",
    "        self.agents = agents\n",
    "        self.metrics = metrics\n",
    "        self.results = {}\n",
    "        \n",
    "        self.num_trials = 5\n",
    "        self.num_episodes = 300\n",
    "        self.evaluation_interval = 50\n",
    "        \n",
    "    def evaluate_sample_efficiency(self, agent, env, convergence_threshold=0.8):\n",
    "        \"\"\"Measure episodes to convergence.\"\"\"\n",
    "        max_rewards = []\n",
    "        convergence_episodes = []\n",
    "        \n",
    "        for trial in range(self.num_trials):\n",
    "            episode_rewards = []\n",
    "            \n",
    "            if hasattr(agent, 'reset'):\n",
    "                agent.reset()\n",
    "            \n",
    "            for episode in range(self.num_episodes):\n",
    "                state = env.reset()\n",
    "                episode_reward = 0\n",
    "                \n",
    "                for step in range(100):\n",
    "                    if hasattr(agent, 'act'):\n",
    "                        if 'Options' in str(type(agent)):\n",
    "                            action, _ = agent.act(state)\n",
    "                        else:\n",
    "                            action = agent.act(state)\n",
    "                    else:\n",
    "                        action = env.action_space.sample()\n",
    "                    \n",
    "                    next_state, reward, done, _ = env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    if hasattr(agent, 'replay_buffer'):\n",
    "                        agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                        if len(agent.replay_buffer) > 32:\n",
    "                            if hasattr(agent, 'update'):\n",
    "                                agent.update(32)\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                    \n",
    "                    state = next_state\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                \n",
    "                if len(episode_rewards) >= 20:\n",
    "                    recent_performance = np.mean(episode_rewards[-20:])\n",
    "                    if recent_performance >= convergence_threshold * np.max(episode_rewards[:max(1, episode-20)]):\n",
    "                        convergence_episodes.append(episode)\n",
    "                        break\n",
    "            \n",
    "            max_rewards.append(np.max(episode_rewards))\n",
    "            if not convergence_episodes or len(convergence_episodes) <= trial:\n",
    "                convergence_episodes.append(self.num_episodes)\n",
    "        \n",
    "        return {\n",
    "            'convergence_episodes': np.mean(convergence_episodes),\n",
    "            'convergence_std': np.std(convergence_episodes),\n",
    "            'max_reward': np.mean(max_rewards),\n",
    "            'max_reward_std': np.std(max_rewards)\n",
    "        }\n",
    "    \n",
    "    def evaluate_transfer_capability(self, agent, source_env, target_envs):\n",
    "        \"\"\"Evaluate transfer learning capability.\"\"\"\n",
    "        source_performance = []\n",
    "        state = source_env.reset()\n",
    "        \n",
    "        for episode in range(100):  # Limited training\n",
    "            episode_reward = 0\n",
    "            for step in range(50):\n",
    "                action = agent.act(state) if hasattr(agent, 'act') else source_env.action_space.sample()\n",
    "                next_state, reward, done, _ = source_env.step(action)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                if hasattr(agent, 'replay_buffer'):\n",
    "                    agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "                    if len(agent.replay_buffer) > 32 and hasattr(agent, 'update'):\n",
    "                        agent.update(32)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                state = next_state\n",
    "            \n",
    "            source_performance.append(episode_reward)\n",
    "        \n",
    "        transfer_results = {}\n",
    "        for target_name, target_env in target_envs.items():\n",
    "            target_rewards = []\n",
    "            \n",
    "            for episode in range(20):  # Quick evaluation\n",
    "                state = target_env.reset()\n",
    "                episode_reward = 0\n",
    "                \n",
    "                for step in range(50):\n",
    "                    action = agent.act(state) if hasattr(agent, 'act') else target_env.action_space.sample()\n",
    "                    next_state, reward, done, _ = target_env.step(action)\n",
    "                    episode_reward += reward\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                    state = next_state\n",
    "                \n",
    "                target_rewards.append(episode_reward)\n",
    "            \n",
    "            transfer_results[target_name] = {\n",
    "                'mean_reward': np.mean(target_rewards),\n",
    "                'std_reward': np.std(target_rewards)\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'source_performance': np.mean(source_performance[-20:]),\n",
    "            'transfer_results': transfer_results\n",
    "        }\n",
    "    \n",
    "    def comprehensive_evaluation(self):\n",
    "        \"\"\"Run comprehensive evaluation across all agents and environments.\"\"\"\n",
    "        print(\\\"🔬 Starting Comprehensive Evaluation...\\\")\n",
    "        \n",
    "        for agent_name, agent in self.agents.items():\n",
    "            print(f\\\"\\\\n📊 Evaluating {agent_name}...\\\")\n",
    "            self.results[agent_name] = {}\n",
    "            \n",
    "            if 'sample_efficiency' in self.metrics:\n",
    "                env = self.environments[0] if self.environments else SimpleGridWorld(size=5)\n",
    "                efficiency_results = self.evaluate_sample_efficiency(agent, env)\n",
    "                self.results[agent_name]['sample_efficiency'] = efficiency_results\n",
    "                print(f\\\"  Sample Efficiency: {efficiency_results['convergence_episodes']:.1f} ± {efficiency_results['convergence_std']:.1f} episodes\\\")\n",
    "            \n",
    "            if 'transfer' in self.metrics and len(self.environments) > 1:\n",
    "                source_env = self.environments[0]\n",
    "                target_envs = {f'env_{i}': env for i, env in enumerate(self.environments[1:])}\n",
    "                transfer_results = self.evaluate_transfer_capability(agent, source_env, target_envs)\n",
    "                self.results[agent_name]['transfer'] = transfer_results\n",
    "                print(f\\\"  Transfer Capability: Source performance {transfer_results['source_performance']:.2f}\\\")\n",
    "        \n",
    "        return self.results\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \\\"\\\"\\\"Generate comprehensive evaluation report.\\\"\\\"\\\"\n",
    "        if not self.results:\n",
    "            self.comprehensive_evaluation()\n",
    "        \n",
    "        print(\\\"\\\\n\\\" + \\\"=\\\"*60)\n",
    "        print(\\\"🏆 COMPREHENSIVE EVALUATION REPORT\\\")\n",
    "        print(\\\"=\\\"*60)\n",
    "        \n",
    "        if any('sample_efficiency' in results for results in self.results.values()):\n",
    "            print(\\\"\\\\n📈 Sample Efficiency Ranking:\\\")\n",
    "            efficiency_scores = []\n",
    "            for agent_name, results in self.results.items():\n",
    "                if 'sample_efficiency' in results:\n",
    "                    score = results['sample_efficiency']['convergence_episodes']\n",
    "                    efficiency_scores.append((agent_name, score))\n",
    "            \n",
    "            efficiency_scores.sort(key=lambda x: x[1])  # Lower is better\n",
    "            for rank, (agent_name, score) in enumerate(efficiency_scores, 1):\n",
    "                print(f\\\"  {rank}. {agent_name}: {score:.1f} episodes to convergence\\\")\n",
    "        \n",
    "        print(\\\"\\\\n🎯 Final Performance Comparison:\\\")\n",
    "        performance_scores = []\n",
    "        for agent_name, results in self.results.items():\n",
    "            if 'sample_efficiency' in results:\n",
    "                score = results['sample_efficiency']['max_reward']\n",
    "                performance_scores.append((agent_name, score))\n",
    "        \n",
    "        performance_scores.sort(key=lambda x: x[1], reverse=True)  # Higher is better\n",
    "        for rank, (agent_name, score) in enumerate(performance_scores, 1):\n",
    "            print(f\\\"  {rank}. {agent_name}: {score:.2f} max reward\\\")\n",
    "        \n",
    "        print(\\\"\\\\n💡 Method Recommendations:\\\")\n",
    "        \n",
    "        if efficiency_scores:\n",
    "            best_efficiency = efficiency_scores[0][0]\n",
    "            print(f\\\"  • Best Sample Efficiency: {best_efficiency}\\\")\n",
    "        \n",
    "        if performance_scores:\n",
    "            best_performance = performance_scores[0][0]\n",
    "            print(f\\\"  • Best Final Performance: {best_performance}\\\")\n",
    "        \n",
    "        print(\\\"\\\\n🔧 Implementation Guidelines:\\\")\n",
    "        print(\\\"  • Use prioritized replay for sample efficiency\\\")\n",
    "        print(\\\"  • Apply data augmentation for robustness\\\")\n",
    "        print(\\\"  • Consider world models for planning tasks\\\")\n",
    "        print(\\\"  • Employ hierarchical methods for long-horizon problems\\\")\n",
    "        print(\\\"  • Leverage transfer learning for related domains\\\")\n",
    "\n",
    "class IntegratedAdvancedAgent:\n",
    "    \\\"\\\"\\\"Agent integrating multiple advanced RL techniques.\\\"\\\"\\\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, config=None):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        default_config = {\n",
    "            'use_prioritized_replay': True,\n",
    "            'use_auxiliary_tasks': True,\n",
    "            'use_data_augmentation': True,\n",
    "            'use_world_model': False,\n",
    "            'use_hierarchical': False,\n",
    "            'lr': 1e-3,\n",
    "            'buffer_size': 10000\n",
    "        }\n",
    "        self.config = {**default_config, **(config or {})}\n",
    "        \n",
    "        self._initialize_components()\n",
    "        \n",
    "        self.training_stats = {\n",
    "            'episode_rewards': [],\n",
    "            'losses': [],\n",
    "            'sample_efficiency': [],\n",
    "            'component_usage': {}\n",
    "        }\n",
    "    \n",
    "    def _initialize_components(self):\n",
    "        \\\"\\\"\\\"Initialize RL components based on configuration.\\\"\\\"\\\"\n",
    "        if self.config['use_auxiliary_tasks']:\n",
    "            self.network = DataAugmentationDQN(self.state_dim, self.action_dim)\n",
    "        else:\n",
    "            self.network = DQNAgent(self.state_dim, self.action_dim).network\n",
    "        \n",
    "        self.target_network = copy.deepcopy(self.network)\n",
    "        self.optimizer = optim.Adam(self.network.parameters(), lr=self.config['lr'])\n",
    "        \n",
    "        if self.config['use_prioritized_replay']:\n",
    "            self.replay_buffer = PrioritizedReplayBuffer(self.config['buffer_size'])\n",
    "        else:\n",
    "            self.replay_buffer = ReplayBuffer(self.config['buffer_size'])\n",
    "        \n",
    "        if self.config['use_world_model']:\n",
    "            self.world_model = VariationalWorldModel(self.state_dim, self.action_dim)\n",
    "        \n",
    "        if self.config['use_hierarchical']:\n",
    "            self.hierarchical_agent = OptionsCriticAgent(self.state_dim, self.action_dim)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.update_count = 0\n",
    "        self.target_update_freq = 100\n",
    "    \n",
    "    def act(self, state, epsilon=0.1):\n",
    "        \\\"\\\"\\\"Select action using integrated approach.\\\"\\\"\\\"\n",
    "        if self.config['use_hierarchical']:\n",
    "            action, option = self.hierarchical_agent.act(state)\n",
    "            self.training_stats['component_usage']['hierarchical'] = \\\n",
    "                self.training_stats['component_usage'].get('hierarchical', 0) + 1\n",
    "            return action\n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            \n",
    "            if self.config['use_data_augmentation'] and np.random.random() < 0.1:\n",
    "                state_tensor = self.network.apply_augmentation(state_tensor, 'noise')\n",
    "            \n",
    "            q_values = self.network(state_tensor)\n",
    "            if isinstance(q_values, tuple):\n",
    "                q_values = q_values[0]  # Extract Q-values from auxiliary network\n",
    "            \n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def update(self, batch_size=32):\n",
    "        \\\"\\\"\\\"Update agent using integrated advanced techniques.\\\"\\\"\\\"\n",
    "        if isinstance(self.replay_buffer, PrioritizedReplayBuffer):\n",
    "            sample_result = self.replay_buffer.sample(batch_size)\n",
    "            if sample_result is None:\n",
    "                return None\n",
    "            experiences, indices, weights = sample_result\n",
    "        else:\n",
    "            batch = self.replay_buffer.sample(batch_size)\n",
    "            if batch is None:\n",
    "                return None\n",
    "            experiences = batch\n",
    "            weights = torch.ones(batch_size)\n",
    "            indices = None\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        weights = torch.FloatTensor(weights) if not isinstance(weights, torch.Tensor) else weights\n",
    "        \n",
    "        if self.config['use_data_augmentation']:\n",
    "            aug_type = np.random.choice(['noise', 'dropout', 'scaling'])\n",
    "            states = self.network.apply_augmentation(states, aug_type)\n",
    "            next_states = self.network.apply_augmentation(next_states, aug_type)\n",
    "            self.training_stats['component_usage']['augmentation'] = \\\n",
    "                self.training_stats['component_usage'].get('augmentation', 0) + 1\n",
    "        \n",
    "        if self.config['use_auxiliary_tasks']:\n",
    "            current_q_values, reward_pred, next_state_pred = self.network(states, actions)\n",
    "            current_q_values = current_q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        else:\n",
    "            current_q_values = self.network(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.config['use_auxiliary_tasks'] and hasattr(self.target_network, 'forward'):\n",
    "                next_q_values = self.target_network(next_states)\n",
    "                if isinstance(next_q_values, tuple):\n",
    "                    next_q_values = next_q_values[0]\n",
    "            else:\n",
    "                next_q_values = self.target_network(next_states)\n",
    "            \n",
    "            max_next_q_values = next_q_values.max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * max_next_q_values * (~dones))\n",
    "        \n",
    "        td_errors = (current_q_values - target_q_values).detach()\n",
    "        q_loss = (weights * F.mse_loss(current_q_values, target_q_values, reduction='none')).mean()\n",
    "        \n",
    "        total_loss = q_loss\n",
    "        \n",
    "        if self.config['use_auxiliary_tasks']:\n",
    "            aux_reward_loss = F.mse_loss(reward_pred.squeeze(), rewards)\n",
    "            aux_dynamics_loss = F.mse_loss(next_state_pred, next_states)\n",
    "            total_loss += 0.1 * aux_reward_loss + 0.1 * aux_dynamics_loss\n",
    "            self.training_stats['component_usage']['auxiliary'] = \\\n",
    "                self.training_stats['component_usage'].get('auxiliary', 0) + 1\n",
    "        \n",
    "        if self.config['use_world_model']:\n",
    "            world_model_loss = self.world_model.compute_loss(states, actions, next_states)\n",
    "            total_loss += 0.1 * world_model_loss\n",
    "            self.training_stats['component_usage']['world_model'] = \\\n",
    "                self.training_stats['component_usage'].get('world_model', 0) + 1\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if indices is not None:\n",
    "            self.replay_buffer.update_priorities(indices, td_errors.numpy())\n",
    "        \n",
    "        self.update_count += 1\n",
    "        if self.update_count % self.target_update_freq == 0:\n",
    "            self.target_network.load_state_dict(self.network.state_dict())\n",
    "        \n",
    "        self.training_stats['losses'].append(total_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'q_loss': q_loss.item()\n",
    "        }\n",
    "\n",
    "def comprehensive_advanced_rl_demo():\n",
    "    \\\"\\\"\\\"Comprehensive demonstration of all advanced RL techniques.\\\"\\\"\\\"\n",
    "    print(\\\"🎓 COMPREHENSIVE ADVANCED DEEP RL DEMONSTRATION\\\")\n",
    "    print(\\\"=\\\" * 55)\n",
    "    \n",
    "    environments = [\n",
    "        SimpleGridWorld(size=5),\n",
    "        SimpleGridWorld(size=6),\n",
    "        SimpleGridWorld(size=7)\n",
    "    ]\n",
    "    \n",
    "    agents = {\n",
    "        'Baseline DQN': DQNAgent(state_dim=2, action_dim=4),\n",
    "        'Sample Efficient': SampleEfficientAgent(state_dim=2, action_dim=4),\n",
    "        'Options-Critic': OptionsCriticAgent(state_dim=2, action_dim=4),\n",
    "        'Feudal Network': FeudalAgent(state_dim=2, action_dim=4),\n",
    "        'Integrated Advanced': IntegratedAdvancedAgent(\n",
    "            state_dim=2, \n",
    "            action_dim=4, \n",
    "            config={\n",
    "                'use_prioritized_replay': True,\n",
    "                'use_auxiliary_tasks': True,\n",
    "                'use_data_augmentation': True\n",
    "            }\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    evaluator = AdvancedRLEvaluator(\n",
    "        environments=environments,\n",
    "        agents=agents,\n",
    "        metrics=['sample_efficiency', 'reward', 'transfer']\n",
    "    )\n",
    "    \n",
    "    evaluator.generate_report()\n",
    "    \n",
    "    print(\\\"\\\\n🎯 ADVANCED DEEP RL ASSIGNMENT COMPLETED!\\\")\n",
    "    print(\\\"\\\\n📚 Concepts Covered:\\\")\n",
    "    print(\\\"  ✓ Model-Free vs Model-Based RL Comparison\\\")\n",
    "    print(\\\"  ✓ World Models with VAE Architecture\\\") \n",
    "    print(\\\"  ✓ Imagination-Based Planning\\\")\n",
    "    print(\\\"  ✓ Sample Efficiency Techniques\\\")\n",
    "    print(\\\"  ✓ Prioritized Experience Replay\\\")\n",
    "    print(\\\"  ✓ Data Augmentation & Auxiliary Tasks\\\")\n",
    "    print(\\\"  ✓ Transfer Learning & Meta-Learning\\\")\n",
    "    print(\\\"  ✓ Hierarchical Reinforcement Learning\\\")\n",
    "    print(\\\"  ✓ Options-Critic Architecture\\\")\n",
    "    print(\\\"  ✓ Feudal Networks\\\")\n",
    "    print(\\\"  ✓ Comprehensive Evaluation Framework\\\")\n",
    "    \n",
    "    print(\\\"\\\\n🔬 Key Takeaways:\\\")\n",
    "    print(\\\"  • Advanced RL methods address sample efficiency and scalability\\\")\n",
    "    print(\\\"  • World models enable planning and imagination\\\")\n",
    "    print(\\\"  • Hierarchical methods tackle long-horizon tasks\\\")\n",
    "    print(\\\"  • Transfer learning accelerates adaptation\\\")\n",
    "    print(\\\"  • Integration of techniques often yields best results\\\")\n",
    "    \n",
    "    print(\\\"\\\\n🚀 Ready for Real-World Advanced RL Applications!\\\")\n",
    "    \n",
    "    return evaluator.results\n",
    "\n",
    "print(\\\"Starting final comprehensive demonstration...\\\"\n",
    "final_results = comprehensive_advanced_rl_demo()\n",
    "\n",
    "print(\\\"\\\\n\\\" + \\\"=\\\" * 60)\n",
    "print(\\\"📖 ASSIGNMENT 13: ADVANCED DEEP RL - COMPLETE! ✅\\\"\n",
    "print(\\\"=\\\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
