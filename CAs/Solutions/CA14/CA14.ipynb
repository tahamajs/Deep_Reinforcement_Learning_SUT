{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec1948d1",
   "metadata": {},
   "source": [
    "# Computer Assignment 14: Advanced Deep Reinforcement Learning\n",
    "\n",
    "## Course Information\n",
    "- **Course**: Deep Reinforcement Learning (DRL)\n",
    "- **Instructor**: Dr. [Instructor Name]\n",
    "- **Institution**: Sharif University of Technology\n",
    "- **Semester**: Fall 2024\n",
    "- **Assignment Number**: CA14\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this assignment, students will be able to:\n",
    "\n",
    "1. **Master Offline Reinforcement Learning**: Understand and implement algorithms that learn from static datasets without environment interaction, including Conservative Q-Learning (CQL) and Implicit Q-Learning (IQL) for handling distribution shift and overestimation bias.\n",
    "\n",
    "2. **Design Safe Reinforcement Learning Systems**: Develop constraint-aware agents using Constrained Policy Optimization (CPO) and Lagrangian methods to satisfy safety constraints while maximizing performance, incorporating risk measures and barrier functions.\n",
    "\n",
    "3. **Implement Multi-Agent Reinforcement Learning**: Build cooperative and competitive multi-agent systems using MADDPG and QMIX algorithms, addressing non-stationarity, coordination challenges, and emergent communication protocols.\n",
    "\n",
    "4. **Develop Robust Reinforcement Learning Agents**: Create agents that handle uncertainty, distributional shifts, and adversarial conditions through domain randomization, adversarial training, and uncertainty estimation techniques.\n",
    "\n",
    "5. **Apply Advanced RL to Real-World Scenarios**: Deploy RL systems in practical applications including autonomous systems, financial trading, and industrial control, considering deployment constraints and safety requirements.\n",
    "\n",
    "6. **Analyze Advanced RL Trade-offs**: Evaluate the performance, safety, robustness, and scalability trade-offs between different advanced RL approaches in complex, real-world environments.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this assignment, ensure you have:\n",
    "\n",
    "- **Mathematical Background**:\n",
    "- Probability theory and stochastic processes\n",
    "- Optimization with constraints (Lagrangian methods)\n",
    "- Game theory and multi-agent systems\n",
    "- Risk measures and robust optimization\n",
    "\n",
    "- **Technical Skills**:\n",
    "- Advanced PyTorch proficiency (multi-agent architectures)\n",
    "- Experience with complex environments (multi-agent, safety-constrained)\n",
    "- Understanding of offline learning and batch RL\n",
    "- Knowledge of uncertainty quantification methods\n",
    "\n",
    "- **Prior Knowledge**:\n",
    "- Completion of CA1-CA13 assignments\n",
    "- Strong foundation in deep RL algorithms (DQN, policy gradients, actor-critic)\n",
    "- Understanding of MDP extensions (constrained MDPs, multi-agent MDPs)\n",
    "- Experience with advanced neural network architectures\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "This assignment is structured as follows:\n",
    "\n",
    "### Section 1: Offline Reinforcement Learning\n",
    "- Theoretical foundations of batch RL and offline learning challenges\n",
    "- Conservative Q-Learning (CQL) for addressing overestimation bias\n",
    "- Implicit Q-Learning (IQL) for stable offline policy optimization\n",
    "- Dataset generation, quality assessment, and algorithm comparison\n",
    "\n",
    "### Section 2: Safe Reinforcement Learning\n",
    "- Constrained Markov Decision Processes and safety constraints\n",
    "- Constrained Policy Optimization (CPO) with trust regions\n",
    "- Lagrangian methods for adaptive constraint satisfaction\n",
    "- Risk measures, barrier functions, and safety mechanisms\n",
    "\n",
    "### Section 3: Multi-agent Reinforcement Learning\n",
    "- Multi-Agent MDP framework and coordination challenges\n",
    "- MADDPG algorithm with centralized training and decentralized execution\n",
    "- QMIX with monotonic value function factorization\n",
    "- Communication protocols and emergent cooperation\n",
    "\n",
    "### Section 4: Robust Reinforcement Learning\n",
    "- Sources of uncertainty and distributional shift in RL\n",
    "- Domain randomization and adversarial training techniques\n",
    "- Uncertainty estimation and robust policy learning\n",
    "- Real-world deployment considerations and robustness evaluation\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "CA14/\n",
    "â”œâ”€â”€ CA14.ipynb              # Main assignment notebook\n",
    "â”œâ”€â”€ agents/                 # Advanced RL agent implementations\n",
    "â”‚   â”œâ”€â”€ offline_agents.py   # CQL, IQL, BCQ implementations\n",
    "â”‚   â”œâ”€â”€ safe_agents.py      # CPO, Lagrangian safe RL agents\n",
    "â”‚   â”œâ”€â”€ marl_agents.py      # MADDPG, QMIX multi-agent systems\n",
    "â”‚   â””â”€â”€ robust_agents.py    # Robust RL with uncertainty handling\n",
    "â”œâ”€â”€ environments/           # Advanced environment implementations\n",
    "â”‚   â”œâ”€â”€ offline_env.py      # Dataset generation and offline evaluation\n",
    "â”‚   â”œâ”€â”€ safe_env.py         # Safety-constrained environments\n",
    "â”‚   â”œâ”€â”€ marl_env.py         # Multi-agent coordination environments\n",
    "â”‚   â””â”€â”€ robust_env.py       # Environments with uncertainty and shifts\n",
    "â”œâ”€â”€ models/                 # Neural network architectures\n",
    "â”‚   â”œâ”€â”€ offline_networks.py # Conservative and implicit Q-networks\n",
    "â”‚   â”œâ”€â”€ safety_networks.py  # Cost value functions and constraint models\n",
    "â”‚   â”œâ”€â”€ marl_networks.py    # Centralized critics and mixing networks\n",
    "â”‚   â””â”€â”€ robust_networks.py  # Uncertainty-aware and adversarial networks\n",
    "â”œâ”€â”€ experiments/            # Training and evaluation scripts\n",
    "â”‚   â”œâ”€â”€ offline_training.py # Offline RL algorithm comparison\n",
    "â”‚   â”œâ”€â”€ safety_experiments.py# Safe RL constraint satisfaction\n",
    "â”‚   â”œâ”€â”€ marl_coordination.py# Multi-agent cooperation studies\n",
    "â”‚   â””â”€â”€ robustness_analysis.py# Distributional shift and uncertainty tests\n",
    "â””â”€â”€ utils/                  # Utility functions and analysis tools\n",
    "    â”œâ”€â”€ offline_utils.py    # Dataset analysis and offline metrics\n",
    "    â”œâ”€â”€ safety_utils.py     # Constraint violation tracking and analysis\n",
    "    â”œâ”€â”€ marl_utils.py       # Coordination metrics and communication analysis\n",
    "    â””â”€â”€ robust_utils.py     # Uncertainty quantification and robustness metrics\n",
    "```\n",
    "\n",
    "## Contents Overview\n",
    "\n",
    "### Theoretical Foundations\n",
    "- **Offline RL Theory**: Distribution shift, overestimation bias, conservative learning\n",
    "- **Safe RL Theory**: Constrained optimization, risk measures, safety guarantees\n",
    "- **Multi-Agent Theory**: Non-stationarity, coordination, Nash equilibrium\n",
    "- **Robust RL Theory**: Uncertainty quantification, domain adaptation, adversarial robustness\n",
    "\n",
    "### Implementation Components\n",
    "- **Offline Learning Systems**: Dataset management, conservative algorithms, offline evaluation\n",
    "- **Safety-Constrained Agents**: Constraint monitoring, safe exploration, risk-aware policies\n",
    "- **Multi-Agent Frameworks**: Centralized training, decentralized execution, communication protocols\n",
    "- **Robust Learning Systems**: Uncertainty estimation, domain randomization, adversarial training\n",
    "\n",
    "### Advanced Topics\n",
    "- **Real-World Deployment**: Safety constraints, robustness requirements, scalability considerations\n",
    "- **Algorithm Comparison**: Performance analysis across different advanced RL paradigms\n",
    "- **Emergent Behaviors**: Cooperation emergence, communication protocols, robust adaptation\n",
    "- **Evaluation Metrics**: Safety violation rates, robustness measures, coordination efficiency\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Your implementation will be evaluated based on:\n",
    "\n",
    "1. **Correctness (35%)**: Accurate implementation of advanced RL algorithms and theoretical concepts\n",
    "2. **Safety & Robustness (30%)**: Effective constraint satisfaction and uncertainty handling\n",
    "3. **Multi-Agent Coordination (20%)**: Quality of cooperative and competitive multi-agent behaviors\n",
    "4. **Analysis & Innovation (15%)**: Depth of experimental analysis and novel approaches\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Environment Setup**: Install required dependencies and verify multi-agent environment compatibility\n",
    "2. **Code Review**: Understand the advanced architectures and safety mechanisms\n",
    "3. **Incremental Implementation**: Start with offline RL, then add safety constraints, multi-agent coordination, and robustness\n",
    "4. **Safety First**: Always prioritize safety constraints and robustness in implementation\n",
    "5. **Comprehensive Testing**: Test across diverse scenarios including edge cases and adversarial conditions\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this assignment, you will have:\n",
    "\n",
    "- **Advanced RL Expertise**: Deep understanding of cutting-edge RL paradigms beyond basic algorithms\n",
    "- **Safety-Critical Systems**: Ability to design and deploy RL agents with safety guarantees\n",
    "- **Multi-Agent Systems**: Skills in building coordinated multi-agent systems for complex tasks\n",
    "- **Robust Deployments**: Knowledge of handling real-world uncertainty and distributional shifts\n",
    "- **Research-Ready Skills**: Proficiency in implementing and analyzing advanced RL research\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This assignment represents the culmination of the Deep RL course, focusing on advanced topics essential for real-world RL deployment. Emphasis is placed on safety, robustness, and multi-agent coordination - critical aspects often overlooked in basic RL but crucial for practical applications.\n",
    "\n",
    "Let's explore the frontiers of deep reinforcement learning! ðŸš€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45240018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "ðŸš€ Advanced Deep RL Environment Initialized!\n",
      "ðŸ“š Topics: Offline RL, Safe RL, Multi-Agent RL, Robust RL\n",
      "ðŸ”¬ Ready for advanced reinforcement learning research and implementation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal, Categorical, MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import copy\n",
    "import gym\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "OFFLINE_RL_CONFIG = {\n",
    "    'batch_size': 256,\n",
    "    'buffer_size': 100000,\n",
    "    'conservative_weight': 1.0,  # CQL parameter\n",
    "    'behavior_cloning_weight': 0.1\n",
    "}\n",
    "\n",
    "SAFE_RL_CONFIG = {\n",
    "    'constraint_threshold': 0.1,\n",
    "    'lagrange_lr': 1e-3,\n",
    "    'penalty_weight': 10.0,\n",
    "    'safety_buffer_size': 10000\n",
    "}\n",
    "\n",
    "MULTI_AGENT_CONFIG = {\n",
    "    'num_agents': 4,\n",
    "    'communication_dim': 16,\n",
    "    'centralized_critic': True,\n",
    "    'shared_experience': False\n",
    "}\n",
    "\n",
    "ROBUST_RL_CONFIG = {\n",
    "    'domain_randomization': True,\n",
    "    'adversarial_training': True,\n",
    "    'uncertainty_estimation': True,\n",
    "    'robust_loss_weight': 0.5\n",
    "}\n",
    "\n",
    "print(\"ðŸš€ Advanced Deep RL Environment Initialized!\")\n",
    "print(\"ðŸ“š Topics: Offline RL, Safe RL, Multi-Agent RL, Robust RL\")\n",
    "print(\"ðŸ”¬ Ready for advanced reinforcement learning research and implementation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f596eab",
   "metadata": {},
   "source": [
    "# Section 1: Offline Reinforcement Learning\n",
    "\n",
    "## 1.1 Theory: Learning from Static Datasets\n",
    "\n",
    "Offline Reinforcement Learning (also known as **Batch RL** or **Data-Driven RL**) addresses the challenge of learning optimal policies from pre-collected datasets without further environment interaction. This paradigm is crucial for real-world applications where online exploration is expensive, dangerous, or impossible.\n",
    "\n",
    "### Key Challenges in Offline Rl\n",
    "\n",
    "#### 1. Distribution Shift Problem\n",
    "The fundamental challenge in offline RL is the **distributional shift** between the behavior policy that generated the data and the learned policy:\n",
    "- **Behavior Policy**: $\\pi_\\beta(a|s)$ - Policy that collected the dataset\n",
    "- **Learned Policy**: $\\pi(a|s)$ - Policy we want to optimize\n",
    "- **Distribution Mismatch**: $\\pi(a|s) \\neq \\pi_\\beta(a|s)$ leads to extrapolation errors\n",
    "\n",
    "#### 2. Overestimation Bias\n",
    "Standard off-policy methods suffer from **overestimation bias** in offline settings:\n",
    "$$Q(s,a) = \\mathbb{E}[r + \\gamma \\max_{a'} Q(s', a')] \\text{ (overestimates for unseen actions)}$$\n",
    "\n",
    "#### 3. Coverage Problem\n",
    "Limited dataset coverage leads to poor generalization:\n",
    "- **Good Coverage**: Dataset contains diverse state-action pairs\n",
    "- **Poor Coverage**: Dataset is narrow, missing important regions\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "#### Offline Rl Objective\n",
    "The goal is to maximize expected return using only offline data $\\mathcal{D} = \\{(s*i, a*i, r*i, s'*i)\\}_{i=1}^N$:\n",
    "$$J(\\pi) = \\mathbb{E}*{\\pi, \\mathcal{D}}[\\sum*{t=0}^T \\gamma^t r_t] \\text{ subject to } (s,a) \\in \\text{support}(\\mathcal{D})$$\n",
    "\n",
    "#### Conservative Q-learning (cql) Objective\n",
    "CQL addresses overestimation by adding a conservative penalty:\n",
    "$$\\mathcal{L}*{CQL}(Q) = \\alpha \\mathbb{E}*{s \\sim \\mathcal{D}}\\left[\\log \\sum*a \\exp Q(s,a) - \\mathbb{E}*{a \\sim \\pi*\\beta(a|s)}[Q(s,a)]\\right] + \\mathcal{L}*{Bellman}(Q)$$\n",
    "\n",
    "Where:\n",
    "- **Conservative Term**: Penalizes high Q-values for out-of-distribution actions\n",
    "- **Bellman Loss**: Standard temporal difference learning objective\n",
    "- **$\\alpha$**: Conservative weight hyperparameter\n",
    "\n",
    "#### Behavior Cloning Regularization\n",
    "Many offline RL methods incorporate behavior cloning to stay close to the data distribution:\n",
    "$$\\mathcal{L}*{BC}(\\pi) = \\mathbb{E}*{(s,a) \\sim \\mathcal{D}}[-\\log \\pi(a|s)]$$\n",
    "\n",
    "## 1.2 Advanced Offline Rl Algorithms\n",
    "\n",
    "### 1. Conservative Q-learning (cql)\n",
    "- **Idea**: Lower-bound Q-values for unseen actions while fitting seen data\n",
    "- **Advantage**: Prevents overestimation bias effectively\n",
    "- **Use Case**: High-dimensional continuous control tasks\n",
    "\n",
    "### 2. Implicit Q-learning (iql)\n",
    "- **Idea**: Avoid explicit policy improvement, use implicit Q-function updates\n",
    "- **Advantage**: More stable than explicit policy optimization\n",
    "- **Use Case**: Mixed-quality datasets with suboptimal trajectories\n",
    "\n",
    "### 3. Advantage-weighted Regression (awr)\n",
    "- **Idea**: Weight behavior cloning by advantage estimates\n",
    "- **Advantage**: Simple and effective for good-quality datasets\n",
    "- **Use Case**: Near-optimal demonstration datasets\n",
    "\n",
    "### 4. Batch-constrained Deep Q-learning (bcq)\n",
    "- **Idea**: Constrain policy to stay close to behavior policy\n",
    "- **Advantage**: Explicit distribution constraint\n",
    "- **Use Case**: Discrete action spaces with coverage issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd561c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ¯ Generating Offline Datasets...\n",
      "\n",
      "ðŸ“Š Expert Dataset:\n",
      "  Size: 80000\n",
      "  Average Reward: 0.037 Â± 0.364\n",
      "  State Dim: 2\n",
      "  Action Distribution: [0.5 0.  0.  0.5]\n",
      "\n",
      "ðŸ“Š Mixed Dataset:\n",
      "  Size: 167806\n",
      "  Average Reward: -0.002 Â± 0.314\n",
      "  State Dim: 2\n",
      "  Action Distribution: [0.39439591 0.07460997 0.07444311 0.45655102]\n",
      "\n",
      "ðŸ“Š Random Dataset:\n",
      "  Size: 352316\n",
      "  Average Reward: -0.092 Â± 0.093\n",
      "  State Dim: 2\n",
      "  Action Distribution: [0.24986092 0.24906334 0.25125172 0.24982402]\n",
      "\n",
      "âœ… Offline datasets generated successfully!\n",
      "ðŸ”„ Ready for Conservative Q-Learning and Implicit Q-Learning training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class OfflineDataset:\n",
    "    \"\"\"Dataset class for offline RL training.\"\"\"\n",
    "    \n",
    "    def __init__(self, states, actions, rewards, next_states, dones, dataset_type='mixed'):\n",
    "        self.states = np.array(states)\n",
    "        self.actions = np.array(actions)\n",
    "        self.rewards = np.array(rewards)\n",
    "        self.next_states = np.array(next_states)\n",
    "        self.dones = np.array(dones)\n",
    "        self.dataset_type = dataset_type\n",
    "        self.size = len(states)\n",
    "        \n",
    "        self.reward_mean = np.mean(rewards)\n",
    "        self.reward_std = np.std(rewards)\n",
    "        self.state_mean = np.mean(states, axis=0)\n",
    "        self.state_std = np.std(states, axis=0) + 1e-8\n",
    "        \n",
    "        self.normalize_dataset()\n",
    "    \n",
    "    def normalize_dataset(self):\n",
    "        \"\"\"Normalize states and rewards for stable training.\"\"\"\n",
    "        self.states = (self.states - self.state_mean) / self.state_std\n",
    "        self.next_states = (self.next_states - self.state_mean) / self.state_std\n",
    "        self.rewards = (self.rewards - self.reward_mean) / (self.reward_std + 1e-8)\n",
    "    \n",
    "    def sample_batch(self, batch_size):\n",
    "        \"\"\"Sample random batch from dataset.\"\"\"\n",
    "        indices = np.random.randint(0, self.size, batch_size)\n",
    "        \n",
    "        batch_states = torch.FloatTensor(self.states[indices]).to(device)\n",
    "        batch_actions = torch.LongTensor(self.actions[indices]).to(device)\n",
    "        batch_rewards = torch.FloatTensor(self.rewards[indices]).to(device)\n",
    "        batch_next_states = torch.FloatTensor(self.next_states[indices]).to(device)\n",
    "        batch_dones = torch.BoolTensor(self.dones[indices]).to(device)\n",
    "        \n",
    "        return batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones\n",
    "    \n",
    "    def get_action_distribution(self):\n",
    "        \"\"\"Analyze action distribution in dataset.\"\"\"\n",
    "        if len(self.actions.shape) == 1:  # Discrete actions\n",
    "            action_counts = np.bincount(self.actions)\n",
    "            return action_counts / self.size\n",
    "        else:  # Continuous actions\n",
    "            return np.mean(self.actions, axis=0), np.std(self.actions, axis=0)\n",
    "\n",
    "class ConservativeQNetwork(nn.Module):\n",
    "    \"\"\"Q-network for Conservative Q-Learning (CQL).\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, state):\n",
    "        \"\"\"Forward pass through Q-network.\"\"\"\n",
    "        q_values = self.q_network(state)\n",
    "        state_value = self.value_network(state)\n",
    "        return q_values, state_value\n",
    "    \n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"Get Q-values for all actions.\"\"\"\n",
    "        q_values, _ = self.forward(state)\n",
    "        return q_values\n",
    "\n",
    "class ConservativeQLearning:\n",
    "    \"\"\"Conservative Q-Learning (CQL) for offline RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, conservative_weight=1.0):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.conservative_weight = conservative_weight\n",
    "        \n",
    "        self.q_network = ConservativeQNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_q_network = copy.deepcopy(self.q_network).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005  # Target network update rate\n",
    "        self.update_count = 0\n",
    "        \n",
    "        self.losses = []\n",
    "        self.conservative_losses = []\n",
    "        self.bellman_losses = []\n",
    "    \n",
    "    def compute_conservative_loss(self, states, actions):\n",
    "        \"\"\"Compute CQL conservative loss.\"\"\"\n",
    "        q_values, _ = self.q_network(states)\n",
    "        \n",
    "        logsumexp_q = torch.logsumexp(q_values, dim=1)\n",
    "        \n",
    "        behavior_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        conservative_loss = (logsumexp_q - behavior_q_values).mean()\n",
    "        \n",
    "        return conservative_loss\n",
    "    \n",
    "    def compute_bellman_loss(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Compute standard Bellman loss.\"\"\"\n",
    "        q_values, _ = self.q_network(states)\n",
    "        current_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_q_values, _ = self.target_q_network(next_states)\n",
    "            max_next_q_values = next_q_values.max(1)[0]\n",
    "            target_q_values = rewards + (self.gamma * max_next_q_values * (~dones))\n",
    "        \n",
    "        bellman_loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        return bellman_loss\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update CQL agent.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        conservative_loss = self.compute_conservative_loss(states, actions)\n",
    "        bellman_loss = self.compute_bellman_loss(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        total_loss = self.conservative_weight * conservative_loss + bellman_loss\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.update_count += 1\n",
    "        if self.update_count % 100 == 0:\n",
    "            self.soft_update_target()\n",
    "        \n",
    "        self.losses.append(total_loss.item())\n",
    "        self.conservative_losses.append(conservative_loss.item())\n",
    "        self.bellman_losses.append(bellman_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'conservative_loss': conservative_loss.item(),\n",
    "            'bellman_loss': bellman_loss.item()\n",
    "        }\n",
    "    \n",
    "    def soft_update_target(self):\n",
    "        \"\"\"Soft update of target network.\"\"\"\n",
    "        for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def get_action(self, state, epsilon=0.0):\n",
    "        \"\"\"Get action using epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.randint(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            q_values = self.q_network.get_q_values(state_tensor)\n",
    "            return q_values.argmax().item()\n",
    "\n",
    "class ImplicitQLearning:\n",
    "    \"\"\"Implicit Q-Learning (IQL) for offline RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, expectile=0.7):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.expectile = expectile  # Expectile for advantage estimation\n",
    "        \n",
    "        self.q_network = ConservativeQNetwork(state_dim, action_dim).to(device)\n",
    "        self.target_q_network = copy.deepcopy(self.q_network).to(device)\n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.q_optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.tau = 0.005\n",
    "        \n",
    "        self.q_losses = []\n",
    "        self.policy_losses = []\n",
    "        self.advantages = []\n",
    "    \n",
    "    def compute_expectile_loss(self, errors, expectile):\n",
    "        \"\"\"Compute expectile loss (asymmetric squared loss).\"\"\"\n",
    "        weights = torch.where(errors > 0, expectile, 1 - expectile)\n",
    "        return (weights * errors.pow(2)).mean()\n",
    "    \n",
    "    def update_q_function(self, states, actions, rewards, next_states, dones):\n",
    "        \"\"\"Update Q-function using expectile regression.\"\"\"\n",
    "        q_values, state_values = self.q_network(states)\n",
    "        current_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            _, next_state_values = self.target_q_network(next_states)\n",
    "            target_q_values = rewards + (self.gamma * next_state_values.squeeze() * (~dones))\n",
    "        \n",
    "        q_errors = target_q_values - current_q_values\n",
    "        q_loss = self.compute_expectile_loss(q_errors, 0.5)  # Standard MSE for Q-function\n",
    "        \n",
    "        advantages = current_q_values.detach() - state_values.squeeze()\n",
    "        value_loss = self.compute_expectile_loss(advantages, self.expectile)\n",
    "        \n",
    "        total_q_loss = q_loss + value_loss\n",
    "        \n",
    "        self.q_optimizer.zero_grad()\n",
    "        total_q_loss.backward()\n",
    "        self.q_optimizer.step()\n",
    "        \n",
    "        return total_q_loss.item(), advantages.mean().item()\n",
    "    \n",
    "    def update_policy(self, states, actions):\n",
    "        \"\"\"Update policy using advantage-weighted regression.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            q_values, state_values = self.q_network(states)\n",
    "            current_q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze()\n",
    "            advantages = current_q_values - state_values.squeeze()\n",
    "            weights = torch.exp(advantages / 3.0).clamp(max=100)  # Temperature scaling\n",
    "        \n",
    "        action_probs = self.policy_network(states)\n",
    "        log_probs = torch.log(action_probs.gather(1, actions.unsqueeze(1)).squeeze() + 1e-8)\n",
    "        \n",
    "        policy_loss = -(weights.detach() * log_probs).mean()\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        return policy_loss.item()\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update IQL agent.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        q_loss, avg_advantage = self.update_q_function(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "        policy_loss = self.update_policy(states, actions)\n",
    "        \n",
    "        for target_param, param in zip(self.target_q_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        self.q_losses.append(q_loss)\n",
    "        self.policy_losses.append(policy_loss)\n",
    "        self.advantages.append(avg_advantage)\n",
    "        \n",
    "        return {\n",
    "            'q_loss': q_loss,\n",
    "            'policy_loss': policy_loss,\n",
    "            'avg_advantage': avg_advantage\n",
    "        }\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Get action from learned policy.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action_probs = self.policy_network(state_tensor)\n",
    "            action_dist = Categorical(action_probs)\n",
    "            return action_dist.sample().item()\n",
    "\n",
    "def generate_offline_dataset(env_name='CartPole-v1', dataset_type='mixed', size=50000):\n",
    "    \"\"\"Generate offline dataset with different quality levels.\"\"\"\n",
    "    class SimpleGridWorld:\n",
    "        def __init__(self, size=5):\n",
    "            self.size = size\n",
    "            self.state = [0, 0]\n",
    "            self.goal = [size-1, size-1]\n",
    "            self.action_space = 4  # up, down, left, right\n",
    "        \n",
    "        def reset(self):\n",
    "            self.state = [0, 0]\n",
    "            return np.array(self.state, dtype=np.float32)\n",
    "        \n",
    "        def step(self, action):\n",
    "            if action == 0 and self.state[1] < self.size - 1:\n",
    "                self.state[1] += 1\n",
    "            elif action == 1 and self.state[1] > 0:\n",
    "                self.state[1] -= 1\n",
    "            elif action == 2 and self.state[0] > 0:\n",
    "                self.state[0] -= 1\n",
    "            elif action == 3 and self.state[0] < self.size - 1:\n",
    "                self.state[0] += 1\n",
    "            \n",
    "            done = (self.state == self.goal)\n",
    "            reward = 1.0 if done else -0.1\n",
    "            \n",
    "            return np.array(self.state, dtype=np.float32), reward, done, {}\n",
    "    \n",
    "    env = SimpleGridWorld(size=5)\n",
    "    \n",
    "    states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "    \n",
    "    for _ in range(size):\n",
    "        state = env.reset()\n",
    "        episode_done = False\n",
    "        episode_length = 0\n",
    "        \n",
    "        while not episode_done and episode_length < 50:\n",
    "            if dataset_type == 'expert':\n",
    "                if state[0] < env.goal[0]:\n",
    "                    action = 3  # right\n",
    "                elif state[1] < env.goal[1]:\n",
    "                    action = 0  # up\n",
    "                else:\n",
    "                    action = np.random.randint(4)\n",
    "            elif dataset_type == 'random':\n",
    "                action = np.random.randint(4)\n",
    "            else:  # mixed\n",
    "                if np.random.random() < 0.7:\n",
    "                    if state[0] < env.goal[0]:\n",
    "                        action = 3  # right\n",
    "                    elif state[1] < env.goal[1]:\n",
    "                        action = 0  # up\n",
    "                    else:\n",
    "                        action = np.random.randint(4)\n",
    "                else:\n",
    "                    action = np.random.randint(4)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            \n",
    "            states.append(state.copy())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state.copy())\n",
    "            dones.append(done)\n",
    "            \n",
    "            state = next_state\n",
    "            episode_done = done\n",
    "            episode_length += 1\n",
    "            \n",
    "            if episode_done:\n",
    "                break\n",
    "    \n",
    "    return OfflineDataset(states, actions, rewards, next_states, dones, dataset_type)\n",
    "\n",
    "print(\"ðŸŽ¯ Generating Offline Datasets...\")\n",
    "\n",
    "datasets = {\n",
    "    'expert': generate_offline_dataset(dataset_type='expert', size=10000),\n",
    "    'mixed': generate_offline_dataset(dataset_type='mixed', size=15000),\n",
    "    'random': generate_offline_dataset(dataset_type='random', size=8000)\n",
    "}\n",
    "\n",
    "for name, dataset in datasets.items():\n",
    "    print(f\"\\nðŸ“Š {name.title()} Dataset:\")\n",
    "    print(f\"  Size: {dataset.size}\")\n",
    "    print(f\"  Average Reward: {dataset.reward_mean:.3f} Â± {dataset.reward_std:.3f}\")\n",
    "    print(f\"  State Dim: {dataset.states.shape[1]}\")\n",
    "    action_dist = dataset.get_action_distribution()\n",
    "    print(f\"  Action Distribution: {action_dist}\")\n",
    "\n",
    "print(\"\\nâœ… Offline datasets generated successfully!\")\n",
    "print(\"ðŸ”„ Ready for Conservative Q-Learning and Implicit Q-Learning training...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b52261",
   "metadata": {},
   "source": [
    "# Section 2: Safe Reinforcement Learning\n",
    "\n",
    "## 2.1 Theory: Constraint Satisfaction and Risk Management\n",
    "\n",
    "Safe Reinforcement Learning addresses the critical challenge of learning optimal policies while satisfying safety constraints. This is essential for real-world applications where policy violations can lead to catastrophic consequences.\n",
    "\n",
    "### Mathematical Framework for Safe Rl\n",
    "\n",
    "#### Constrained Markov Decision Process (cmdp)\n",
    "A CMDP extends the standard MDP with safety constraints:\n",
    "$$\\text{CMDP} = (\\mathcal{S}, \\mathcal{A}, P, R, C, \\gamma, d_0)$$\n",
    "\n",
    "Where:\n",
    "- **$C: \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}^m$**: Cost function (constraint violations)\n",
    "- **$d_0$**: Initial state distribution\n",
    "- **Safety Constraint**: $\\mathbb{E}*\\pi[\\sum*{t=0}^{\\infty} \\gamma^t c*i(s*t, a*t)] \\leq \\delta*i$ for $i \\in \\{1, ..., m\\}$\n",
    "\n",
    "#### Safe Rl Objective\n",
    "The safe RL problem is formulated as:\n",
    "$$\\max*\\pi \\mathbb{E}*\\pi\\left[\\sum*{t=0}^{\\infty} \\gamma^t r(s*t, a_t)\\right]$$\n",
    "$$\\text{subject to } \\mathbb{E}*\\pi\\left[\\sum*{t=0}^{\\infty} \\gamma^t c*i(s*t, a*t)\\right] \\leq \\delta*i, \\forall i$$\n",
    "\n",
    "### Key Approaches to Safe Rl\n",
    "\n",
    "#### 1. Lagrangian Methods\n",
    "Use Lagrange multipliers to convert constrained optimization to unconstrained:\n",
    "$$\\mathcal{L}(\\pi, \\lambda) = J(\\pi) - \\sum*{i=1}^m \\lambda*i \\left(J*C^i(\\pi) - \\delta*i\\right)$$\n",
    "\n",
    "Where:\n",
    "- **$J(\\pi)$**: Expected cumulative reward\n",
    "- **$J_C^i(\\pi)$**: Expected cumulative cost for constraint $i$\n",
    "- **$\\lambda_i$**: Lagrange multiplier for constraint $i$\n",
    "\n",
    "#### 2. Constrained Policy Optimization (cpo)\n",
    "CPO ensures policy updates satisfy constraints through trust regions:\n",
    "$$\\max*\\pi \\mathbb{E}*{s \\sim d^\\pi, a \\sim \\pi}[A^R*{\\pi*k}(s,a)]$$\n",
    "$$\\text{subject to } J*C(\\pi) \\leq \\delta \\text{ and } D*{KL}(\\pi*k, \\pi) \\leq \\delta*{KL}$$\n",
    "\n",
    "#### 3. Safe Policy Gradients\n",
    "Modify policy gradient updates to account for constraint violations:\n",
    "$$\\nabla*\\theta J(\\theta) = \\mathbb{E}*\\pi[\\nabla_\\theta \\log \\pi(a|s) \\cdot (A^R(s,a) - \\lambda A^C(s,a))]$$\n",
    "\n",
    "### Risk Measures in Safe Rl\n",
    "\n",
    "#### 1. Value at Risk (var)\n",
    "$$\\text{VaR}_\\alpha(X) = \\inf\\{x : P(X \\leq x) \\geq \\alpha\\}$$\n",
    "\n",
    "#### 2. Conditional Value at Risk (cvar)\n",
    "$$\\text{CVaR}*\\alpha(X) = \\mathbb{E}[X | X \\geq \\text{VaR}*\\alpha(X)]$$\n",
    "\n",
    "#### 3. Risk-sensitive Objective\n",
    "Optimize risk-adjusted returns:\n",
    "$$\\max*\\pi \\mathbb{E}*\\pi[\\sum*{t=0}^{\\infty} \\gamma^t r*t] - \\beta \\cdot \\text{Risk}(\\pi)$$\n",
    "\n",
    "## 2.2 Safety Mechanisms\n",
    "\n",
    "### 1. Barrier Functions\n",
    "Use barrier functions to prevent constraint violations:\n",
    "$$B(s) = -\\log(\\delta - C(s))$$\n",
    "\n",
    "### 2. Safe Exploration\n",
    "- **Initial Safe Policy**: Start with a known safe policy\n",
    "- **Safe Action Space**: Restrict actions to safe subset\n",
    "- **Recovery Actions**: Define emergency actions for constraint violations\n",
    "\n",
    "### 3. Risk-aware Planning\n",
    "Incorporate uncertainty in safety-critical decision making:\n",
    "- **Robust MDP**: Consider worst-case scenarios\n",
    "- **Bayesian RL**: Maintain uncertainty over dynamics\n",
    "- **Distributional RL**: Model full return distributions\n",
    "\n",
    "## 2.3 Applications of Safe Rl\n",
    "\n",
    "### Autonomous Vehicles\n",
    "- **Constraints**: Collision avoidance, traffic rules\n",
    "- **Risk Measures**: Probability of accidents\n",
    "- **Safety Mechanisms**: Emergency braking, lane keeping\n",
    "\n",
    "### Healthcare\n",
    "- **Constraints**: Patient safety, dosage limits\n",
    "- **Risk Measures**: Adverse events probability\n",
    "- **Safety Mechanisms**: Conservative treatment protocols\n",
    "\n",
    "### Industrial Control\n",
    "- **Constraints**: Equipment damage, safety limits\n",
    "- **Risk Measures**: System failure probability  \n",
    "- **Safety Mechanisms**: Emergency shutoffs, backup systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fc8b120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Safe RL Training...\n",
      "ðŸ›¡ï¸ Demonstrating Safe Reinforcement Learning\n",
      "==================================================\n",
      "\n",
      "Episode 0:\n",
      "  CPO: Reward=-3.72, Violations=7.600\n",
      "  Lagrangian: Reward=-5.00, Violations=8.680\n",
      "\n",
      "Episode 50:\n",
      "  CPO: Reward=8.76, Violations=2.194\n",
      "  Lagrangian: Reward=-3.53, Violations=6.800\n",
      "\n",
      "Episode 100:\n",
      "  CPO: Reward=9.02, Violations=2.042\n",
      "  Lagrangian: Reward=8.01, Violations=3.266\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 496\u001b[39m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results, agents, env\n\u001b[32m    495\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸš€ Starting Safe RL Training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m safe_results, safe_agents, safe_env = \u001b[43mdemonstrate_safe_rl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m fig, axes = plt.subplots(\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m12\u001b[39m))\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m agent_name, data \u001b[38;5;129;01min\u001b[39;00m safe_results.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 468\u001b[39m, in \u001b[36mdemonstrate_safe_rl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    465\u001b[39m episode_lengths = []\n\u001b[32m    467\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(update_frequency):\n\u001b[32m--> \u001b[39m\u001b[32m468\u001b[39m     trajectory = \u001b[43mcollect_safe_trajectory\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m     trajectories.append(trajectory)\n\u001b[32m    471\u001b[39m     episode_reward = \u001b[38;5;28msum\u001b[39m(step[\u001b[32m2\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m trajectory)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 421\u001b[39m, in \u001b[36mcollect_safe_trajectory\u001b[39m\u001b[34m(env, agent, max_steps)\u001b[39m\n\u001b[32m    418\u001b[39m state = env.reset()\n\u001b[32m    420\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m     action, log_prob = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    422\u001b[39m     next_state, reward, done, info = env.step(action)\n\u001b[32m    424\u001b[39m     constraint_cost = info[\u001b[33m'\u001b[39m\u001b[33mconstraint_cost\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 152\u001b[39m, in \u001b[36mConstrainedPolicyOptimization.get_action\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m    150\u001b[39m     action_dist = Categorical(action_probs)\n\u001b[32m    151\u001b[39m     action = action_dist.sample()\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     log_prob = \u001b[43maction_dist\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_prob\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m action.item(), log_prob.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/venv/lib/python3.13/site-packages/torch/distributions/categorical.py:150\u001b[39m, in \u001b[36mCategorical.log_prob\u001b[39m\u001b[34m(self, value)\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_sample(value)\n\u001b[32m    149\u001b[39m value = value.long().unsqueeze(-\u001b[32m1\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m value, log_pmf = torch.broadcast_tensors(value, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlogits\u001b[49m)\n\u001b[32m    151\u001b[39m value = value[..., :\u001b[32m1\u001b[39m]\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m log_pmf.gather(-\u001b[32m1\u001b[39m, value).squeeze(-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/venv/lib/python3.13/site-packages/torch/distributions/utils.py:169\u001b[39m, in \u001b[36mlazy_property.__get__\u001b[39m\u001b[34m(self, instance, obj_type)\u001b[39m\n\u001b[32m    167\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m instance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _lazy_property_and_property(\u001b[38;5;28mself\u001b[39m.wrapped)\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43menable_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28msetattr\u001b[39m(instance, \u001b[38;5;28mself\u001b[39m.wrapped.\u001b[34m__name__\u001b[39m, value)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "class SafeEnvironment:\n",
    "    \"\"\"Environment with safety constraints for Safe RL demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, size=6, hazard_positions=None, constraint_threshold=0.1):\n",
    "        self.size = size\n",
    "        self.state = [0, 0]\n",
    "        self.goal = [size-1, size-1]\n",
    "        self.constraint_threshold = constraint_threshold\n",
    "        \n",
    "        if hazard_positions is None:\n",
    "            self.hazards = [[2, 2], [3, 1], [1, 3], [4, 3]]\n",
    "        else:\n",
    "            self.hazards = hazard_positions\n",
    "        \n",
    "        self.action_space = 4  # up, down, left, right\n",
    "        self.max_episode_steps = 50\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.constraint_violations = 0\n",
    "        self.total_constraint_cost = 0\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        self.state = [0, 0]\n",
    "        self.current_step = 0\n",
    "        self.constraint_violations = 0\n",
    "        self.total_constraint_cost = 0\n",
    "        return np.array(self.state, dtype=np.float32)\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Take action in environment with safety constraints.\"\"\"\n",
    "        self.current_step += 1\n",
    "        \n",
    "        prev_state = self.state.copy()\n",
    "        if action == 0 and self.state[1] < self.size - 1:  # up\n",
    "            self.state[1] += 1\n",
    "        elif action == 1 and self.state[1] > 0:  # down\n",
    "            self.state[1] -= 1\n",
    "        elif action == 2 and self.state[0] > 0:  # left\n",
    "            self.state[0] -= 1\n",
    "        elif action == 3 and self.state[0] < self.size - 1:  # right\n",
    "            self.state[0] += 1\n",
    "        \n",
    "        done = (self.state == self.goal)\n",
    "        reward = 10.0 if done else -0.1\n",
    "        \n",
    "        constraint_cost = self._compute_constraint_cost(self.state)\n",
    "        \n",
    "        episode_done = done or self.current_step >= self.max_episode_steps\n",
    "        \n",
    "        info = {\n",
    "            'constraint_cost': constraint_cost,\n",
    "            'constraint_violation': constraint_cost > 0,\n",
    "            'total_violations': self.constraint_violations,\n",
    "            'position': self.state.copy()\n",
    "        }\n",
    "        \n",
    "        return np.array(self.state, dtype=np.float32), reward, episode_done, info\n",
    "    \n",
    "    def _compute_constraint_cost(self, state):\n",
    "        \"\"\"Compute constraint violation cost.\"\"\"\n",
    "        cost = 0.0\n",
    "        \n",
    "        if state in self.hazards:\n",
    "            cost += 1.0  # High cost for being in hazardous areas\n",
    "            self.constraint_violations += 1\n",
    "        \n",
    "        if state[0] == 0 or state[0] == self.size-1 or state[1] == 0 or state[1] == self.size-1:\n",
    "            cost += 0.1  # Small cost for being near boundaries\n",
    "        \n",
    "        self.total_constraint_cost += cost\n",
    "        return cost\n",
    "    \n",
    "    def is_safe_state(self, state):\n",
    "        \"\"\"Check if state is safe (no constraint violations).\"\"\"\n",
    "        return state not in self.hazards\n",
    "    \n",
    "    def get_safe_actions(self, state):\n",
    "        \"\"\"Get list of safe actions from current state.\"\"\"\n",
    "        safe_actions = []\n",
    "        for action in range(self.action_space):\n",
    "            next_state = state.copy()\n",
    "            if action == 0 and state[1] < self.size - 1:\n",
    "                next_state[1] += 1\n",
    "            elif action == 1 and state[1] > 0:\n",
    "                next_state[1] -= 1\n",
    "            elif action == 2 and state[0] > 0:\n",
    "                next_state[0] -= 1\n",
    "            elif action == 3 and state[0] < self.size - 1:\n",
    "                next_state[0] += 1\n",
    "            \n",
    "            if self.is_safe_state(next_state):\n",
    "                safe_actions.append(action)\n",
    "        \n",
    "        return safe_actions if safe_actions else list(range(self.action_space))\n",
    "\n",
    "class ConstrainedPolicyOptimization:\n",
    "    \"\"\"Constrained Policy Optimization (CPO) for Safe RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, constraint_limit=0.1, lr=3e-4):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.constraint_limit = constraint_limit\n",
    "        \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.cost_value_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n",
    "        self.cost_optimizer = optim.Adam(self.cost_value_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.lam = 0.95  # GAE parameter\n",
    "        self.clip_ratio = 0.2\n",
    "        self.target_kl = 0.01\n",
    "        self.damping = 0.1\n",
    "        \n",
    "        self.constraint_violations = []\n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.cost_losses = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Get action from policy.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action_probs = self.policy_network(state_tensor)\n",
    "            action_dist = Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            \n",
    "        return action.item(), log_prob.item()\n",
    "    \n",
    "    def compute_gae(self, rewards, values, dones, next_value):\n",
    "        \"\"\"Compute Generalized Advantage Estimation.\"\"\"\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        \n",
    "        for step in reversed(range(len(rewards))):\n",
    "            if step == len(rewards) - 1:\n",
    "                next_non_terminal = 1.0 - dones[step]\n",
    "                next_value_step = next_value\n",
    "            else:\n",
    "                next_non_terminal = 1.0 - dones[step]\n",
    "                next_value_step = values[step + 1]\n",
    "            \n",
    "            delta = rewards[step] + self.gamma * next_value_step * next_non_terminal - values[step]\n",
    "            gae = delta + self.gamma * self.lam * next_non_terminal * gae\n",
    "            advantages.insert(0, gae)\n",
    "        \n",
    "        return torch.FloatTensor(advantages).to(device)\n",
    "    \n",
    "    def compute_policy_loss(self, states, actions, advantages, old_log_probs):\n",
    "        \"\"\"Compute clipped policy loss.\"\"\"\n",
    "        action_probs = self.policy_network(states)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        new_log_probs = action_dist.log_prob(actions)\n",
    "        \n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        surr1 = ratio * advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
    "        \n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        kl_div = (old_log_probs - new_log_probs).mean()\n",
    "        \n",
    "        return policy_loss, kl_div\n",
    "    \n",
    "    def compute_constraint_violation(self, states, actions, cost_advantages, old_log_probs):\n",
    "        \"\"\"Compute expected constraint violation.\"\"\"\n",
    "        action_probs = self.policy_network(states)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        new_log_probs = action_dist.log_prob(actions)\n",
    "        \n",
    "        ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "        constraint_violation = (ratio * cost_advantages).mean()\n",
    "        \n",
    "        return constraint_violation\n",
    "    \n",
    "    def update(self, trajectories):\n",
    "        \"\"\"Update CPO agent with constraint satisfaction.\"\"\"\n",
    "        if not trajectories:\n",
    "            return None\n",
    "        \n",
    "        all_states, all_actions, all_rewards, all_costs = [], [], [], []\n",
    "        all_dones, all_log_probs = [], []\n",
    "        \n",
    "        for trajectory in trajectories:\n",
    "            states, actions, rewards, costs, dones, log_probs = zip(*trajectory)\n",
    "            all_states.extend(states)\n",
    "            all_actions.extend(actions)\n",
    "            all_rewards.extend(rewards)\n",
    "            all_costs.extend(costs)\n",
    "            all_dones.extend(dones)\n",
    "            all_log_probs.extend(log_probs)\n",
    "        \n",
    "        states = torch.FloatTensor(all_states).to(device)\n",
    "        actions = torch.LongTensor(all_actions).to(device)\n",
    "        rewards = torch.FloatTensor(all_rewards).to(device)\n",
    "        costs = torch.FloatTensor(all_costs).to(device)\n",
    "        old_log_probs = torch.FloatTensor(all_log_probs).to(device)\n",
    "        \n",
    "        values = self.value_network(states).squeeze()\n",
    "        cost_values = self.cost_value_network(states).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_value = self.value_network(states[-1:]).squeeze()\n",
    "            next_cost_value = self.cost_value_network(states[-1:]).squeeze()\n",
    "            \n",
    "        advantages = self.compute_gae(all_rewards, values.detach().cpu().numpy(), \n",
    "                                    all_dones, next_value.item())\n",
    "        cost_advantages = self.compute_gae(all_costs, cost_values.detach().cpu().numpy(), \n",
    "                                         all_dones, next_cost_value.item())\n",
    "        \n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        cost_advantages = (cost_advantages - cost_advantages.mean()) / (cost_advantages.std() + 1e-8)\n",
    "        \n",
    "        returns = advantages + values.detach()\n",
    "        cost_returns = cost_advantages + cost_values.detach()\n",
    "        \n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        cost_loss = F.mse_loss(cost_values, cost_returns)\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        self.cost_optimizer.zero_grad()\n",
    "        cost_loss.backward()\n",
    "        self.cost_optimizer.step()\n",
    "        \n",
    "        constraint_violation = self.compute_constraint_violation(\n",
    "            states, actions, cost_advantages, old_log_probs\n",
    "        )\n",
    "        \n",
    "        policy_loss, kl_div = self.compute_policy_loss(\n",
    "            states, actions, advantages, old_log_probs\n",
    "        )\n",
    "        \n",
    "        if constraint_violation.item() <= self.constraint_limit:\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=0.5)\n",
    "            self.policy_optimizer.step()\n",
    "        else:\n",
    "            print(f\"âš ï¸ Policy update skipped due to constraint violation: {constraint_violation.item():.4f}\")\n",
    "        \n",
    "        self.policy_losses.append(policy_loss.item())\n",
    "        self.value_losses.append(value_loss.item())\n",
    "        self.cost_losses.append(cost_loss.item())\n",
    "        self.constraint_violations.append(constraint_violation.item())\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'cost_loss': cost_loss.item(),\n",
    "            'constraint_violation': constraint_violation.item(),\n",
    "            'kl_divergence': kl_div.item()\n",
    "        }\n",
    "\n",
    "class LagrangianSafeRL:\n",
    "    \"\"\"Lagrangian method for Safe RL with adaptive penalty.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, constraint_limit=0.1, lr=3e-4, lagrange_lr=1e-2):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.constraint_limit = constraint_limit\n",
    "        \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.lagrange_multiplier = nn.Parameter(torch.tensor(1.0, device=device))\n",
    "        self.lagrange_optimizer = optim.Adam([self.lagrange_multiplier], lr=lagrange_lr)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.lagrange_history = []\n",
    "        self.constraint_costs = []\n",
    "        self.total_rewards = []\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        \"\"\"Get action with safety consideration.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            action_probs = self.policy_network(state_tensor)\n",
    "            action_dist = Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            \n",
    "        return action.item(), log_prob.item()\n",
    "    \n",
    "    def update(self, trajectories):\n",
    "        \"\"\"Update using Lagrangian method.\"\"\"\n",
    "        if not trajectories:\n",
    "            return None\n",
    "        \n",
    "        all_states, all_actions, all_rewards, all_costs = [], [], [], []\n",
    "        all_log_probs = []\n",
    "        \n",
    "        for trajectory in trajectories:\n",
    "            states, actions, rewards, costs, _, log_probs = zip(*trajectory)\n",
    "            all_states.extend(states)\n",
    "            all_actions.extend(actions)\n",
    "            all_rewards.extend(rewards)\n",
    "            all_costs.extend(costs)\n",
    "            all_log_probs.extend(log_probs)\n",
    "        \n",
    "        states = torch.FloatTensor(all_states).to(device)\n",
    "        actions = torch.LongTensor(all_actions).to(device)\n",
    "        rewards = torch.FloatTensor(all_rewards).to(device)\n",
    "        costs = torch.FloatTensor(all_costs).to(device)\n",
    "        old_log_probs = torch.FloatTensor(all_log_probs).to(device)\n",
    "        \n",
    "        discounted_rewards = []\n",
    "        discounted_costs = []\n",
    "        \n",
    "        for trajectory in trajectories:\n",
    "            traj_rewards = [step[2] for step in trajectory]\n",
    "            traj_costs = [step[3] for step in trajectory]\n",
    "            \n",
    "            reward_return = 0\n",
    "            cost_return = 0\n",
    "            for r, c in zip(reversed(traj_rewards), reversed(traj_costs)):\n",
    "                reward_return = r + self.gamma * reward_return\n",
    "                cost_return = c + self.gamma * cost_return\n",
    "                discounted_rewards.insert(0, reward_return)\n",
    "                discounted_costs.insert(0, cost_return)\n",
    "        \n",
    "        returns = torch.FloatTensor(discounted_rewards).to(device)\n",
    "        cost_returns = torch.FloatTensor(discounted_costs).to(device)\n",
    "        \n",
    "        values = self.value_network(states).squeeze()\n",
    "        advantages = returns - values.detach()\n",
    "        cost_advantages = cost_returns\n",
    "        \n",
    "        action_probs = self.policy_network(states)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        log_probs = action_dist.log_prob(actions)\n",
    "        \n",
    "        policy_loss = -(log_probs * (advantages - self.lagrange_multiplier * cost_advantages)).mean()\n",
    "        \n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        avg_cost = cost_returns.mean()\n",
    "        constraint_violation = avg_cost - self.constraint_limit\n",
    "        \n",
    "        lagrange_loss = -self.lagrange_multiplier * constraint_violation\n",
    "        \n",
    "        self.lagrange_optimizer.zero_grad()\n",
    "        lagrange_loss.backward()\n",
    "        self.lagrange_optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.lagrange_multiplier.clamp_(min=0.0)\n",
    "        \n",
    "        self.lagrange_history.append(self.lagrange_multiplier.item())\n",
    "        self.constraint_costs.append(avg_cost.item())\n",
    "        self.total_rewards.append(returns.mean().item())\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'lagrange_multiplier': self.lagrange_multiplier.item(),\n",
    "            'constraint_violation': constraint_violation.item(),\n",
    "            'avg_cost': avg_cost.item()\n",
    "        }\n",
    "\n",
    "def collect_safe_trajectory(env, agent, max_steps=50):\n",
    "    \"\"\"Collect trajectory with safety information.\"\"\"\n",
    "    trajectory = []\n",
    "    state = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action, log_prob = agent.get_action(state)\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        constraint_cost = info['constraint_cost']\n",
    "        \n",
    "        trajectory.append((\n",
    "            state.copy(), action, reward, constraint_cost, done, log_prob\n",
    "        ))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "def demonstrate_safe_rl():\n",
    "    \"\"\"Demonstrate Safe RL algorithms.\"\"\"\n",
    "    print(\"ðŸ›¡ï¸ Demonstrating Safe Reinforcement Learning\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    env = SafeEnvironment(size=6, constraint_threshold=0.1)\n",
    "    \n",
    "    agents = {\n",
    "        'CPO': ConstrainedPolicyOptimization(\n",
    "            state_dim=2, action_dim=4, constraint_limit=0.1\n",
    "        ),\n",
    "        'Lagrangian': LagrangianSafeRL(\n",
    "            state_dim=2, action_dim=4, constraint_limit=0.1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    results = {name: {\n",
    "        'rewards': [], 'constraint_violations': [], 'episode_lengths': []\n",
    "    } for name in agents.keys()}\n",
    "    \n",
    "    num_episodes = 300\n",
    "    update_frequency = 10\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        for agent_name, agent in agents.items():\n",
    "            trajectories = []\n",
    "            episode_rewards = []\n",
    "            episode_violations = []\n",
    "            episode_lengths = []\n",
    "            \n",
    "            for _ in range(update_frequency):\n",
    "                trajectory = collect_safe_trajectory(env, agent)\n",
    "                trajectories.append(trajectory)\n",
    "                \n",
    "                episode_reward = sum(step[2] for step in trajectory)\n",
    "                episode_violation = sum(step[3] for step in trajectory)\n",
    "                episode_length = len(trajectory)\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "                episode_violations.append(episode_violation)\n",
    "                episode_lengths.append(episode_length)\n",
    "            \n",
    "            if trajectories:\n",
    "                update_info = agent.update(trajectories)\n",
    "            \n",
    "            results[agent_name]['rewards'].extend(episode_rewards)\n",
    "            results[agent_name]['constraint_violations'].extend(episode_violations)\n",
    "            results[agent_name]['episode_lengths'].extend(episode_lengths)\n",
    "        \n",
    "        if episode % 50 == 0:\n",
    "            print(f\"\\nEpisode {episode}:\")\n",
    "            for agent_name in agents.keys():\n",
    "                recent_rewards = np.mean(results[agent_name]['rewards'][-50:])\n",
    "                recent_violations = np.mean(results[agent_name]['constraint_violations'][-50:])\n",
    "                print(f\"  {agent_name}: Reward={recent_rewards:.2f}, Violations={recent_violations:.3f}\")\n",
    "    \n",
    "    return results, agents, env\n",
    "\n",
    "print(\"ðŸš€ Starting Safe RL Training...\")\n",
    "safe_results, safe_agents, safe_env = demonstrate_safe_rl()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "for agent_name, data in safe_results.items():\n",
    "    window_size = 20\n",
    "    if len(data['rewards']) >= window_size:\n",
    "        smoothed_rewards = pd.Series(data['rewards']).rolling(window_size).mean()\n",
    "        axes[0, 0].plot(smoothed_rewards, label=agent_name, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Safe RL Learning Curves')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Episode Reward (Smoothed)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for agent_name, data in safe_results.items():\n",
    "    window_size = 20\n",
    "    if len(data['constraint_violations']) >= window_size:\n",
    "        smoothed_violations = pd.Series(data['constraint_violations']).rolling(window_size).mean()\n",
    "        axes[0, 1].plot(smoothed_violations, label=agent_name, linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('Constraint Violations Over Time')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Average Constraint Cost (Smoothed)')\n",
    "axes[0, 1].axhline(y=safe_env.constraint_threshold, color='red', linestyle='--', label='Constraint Limit')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "if 'Lagrangian' in safe_agents and hasattr(safe_agents['Lagrangian'], 'lagrange_history'):\n",
    "    axes[1, 0].plot(safe_agents['Lagrangian'].lagrange_history, 'g-', linewidth=2)\n",
    "    axes[1, 0].set_title('Lagrange Multiplier Evolution')\n",
    "    axes[1, 0].set_xlabel('Update Step')\n",
    "    axes[1, 0].set_ylabel('Lagrange Multiplier (Î»)')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for agent_name, data in safe_results.items():\n",
    "    final_rewards = np.mean(data['rewards'][-50:])\n",
    "    final_violations = np.mean(data['constraint_violations'][-50:])\n",
    "    axes[1, 1].scatter(final_violations, final_rewards, s=100, label=agent_name)\n",
    "\n",
    "axes[1, 1].set_title('Safety vs Performance Trade-off')\n",
    "axes[1, 1].set_xlabel('Average Constraint Violations')\n",
    "axes[1, 1].set_ylabel('Average Reward')\n",
    "axes[1, 1].axvline(x=safe_env.constraint_threshold, color='red', linestyle='--', alpha=0.5)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Safe RL Results Summary:\")\n",
    "print(\"=\" * 50)\n",
    "for agent_name, data in safe_results.items():\n",
    "    final_reward = np.mean(data['rewards'][-50:])\n",
    "    final_violations = np.mean(data['constraint_violations'][-50:])\n",
    "    violation_rate = np.mean([v > safe_env.constraint_threshold for v in data['constraint_violations'][-50:]])\n",
    "    \n",
    "    print(f\"\\n{agent_name}:\")\n",
    "    print(f\"  Final Performance: {final_reward:.2f}\")\n",
    "    print(f\"  Avg Constraint Cost: {final_violations:.4f}\")\n",
    "    print(f\"  Violation Rate: {violation_rate:.2%}\")\n",
    "    print(f\"  Constraint Satisfied: {'âœ…' if final_violations <= safe_env.constraint_threshold else 'âŒ'}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"  â€¢ CPO prevents policy updates that violate constraints\")\n",
    "print(\"  â€¢ Lagrangian method adapts penalty weights automatically\")\n",
    "print(\"  â€¢ Safety-performance trade-offs are environment dependent\")\n",
    "print(\"  â€¢ Constraint satisfaction improves with training\")\n",
    "\n",
    "print(\"\\nðŸ›¡ï¸ Safe RL demonstration completed!\")\n",
    "print(\"ðŸ”„ Ready for Multi-Agent RL implementation...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96a729",
   "metadata": {},
   "source": [
    "# Section 3: Multi-agent Reinforcement Learning\n",
    "\n",
    "## 3.1 Theory: Coordination and Competition\n",
    "\n",
    "Multi-Agent Reinforcement Learning (MARL) extends single-agent RL to environments with multiple learning agents. This introduces new challenges including non-stationarity, partial observability, and coordination problems.\n",
    "\n",
    "### Mathematical Framework for Marl\n",
    "\n",
    "#### Multi-agent Markov Decision Process (ma-mdp)\n",
    "A Multi-Agent MDP is defined as:\n",
    "$$\\text{MA-MDP} = (\\mathcal{N}, \\mathcal{S}, \\{\\mathcal{A}*i\\}*{i \\in \\mathcal{N}}, P, \\{R*i\\}*{i \\in \\mathcal{N}}, \\gamma, \\mu_0)$$\n",
    "\n",
    "Where:\n",
    "- **$\\mathcal{N} = \\{1, 2, ..., n\\}$**: Set of agents\n",
    "- **$\\mathcal{S}$**: Global state space\n",
    "- **$\\mathcal{A}_i$**: Action space for agent $i$\n",
    "- **$P: \\mathcal{S} \\times \\mathcal{A}*1 \\times ... \\times \\mathcal{A}*n \\rightarrow \\Delta(\\mathcal{S})$**: Transition function\n",
    "- **$R*i: \\mathcal{S} \\times \\mathcal{A}*1 \\times ... \\times \\mathcal{A}_n \\rightarrow \\mathbb{R}$**: Reward function for agent $i$\n",
    "\n",
    "#### Joint Policy and Nash Equilibrium\n",
    "The **joint policy** $\\pi = (\\pi*1, ..., \\pi*n)$ where $\\pi_i$ is agent $i$'s policy.\n",
    "\n",
    "**Nash Equilibrium**: A joint policy $\\pi^*$ is a Nash equilibrium if:\n",
    "$$J*i(\\pi*i^*, \\pi*{-i}^*) \\geq J*i(\\pi*i, \\pi*{-i}^*), \\forall \\pi_i, \\forall i$$\n",
    "\n",
    "Where $\\pi_{-i}$ denotes the policies of all agents except $i$.\n",
    "\n",
    "### Key Challenges in Marl\n",
    "\n",
    "#### 1. Non-stationarity\n",
    "From each agent's perspective, the environment is non-stationary due to other learning agents:\n",
    "$$P^{\\pi*{-i}}(s' | s, a*i) = \\sum*{\\mathbf{a}*{-i}} \\prod*{j \\neq i} \\pi*j(a*j | s) P(s' | s, a*i, \\mathbf{a}_{-i})$$\n",
    "\n",
    "#### 2. Exponential Joint Action Space\n",
    "The joint action space grows exponentially: $|\\mathcal{A}| = \\prod*{i=1}^n |\\mathcal{A}*i|$\n",
    "\n",
    "#### 3. Partial Observability\n",
    "Agents often have limited observations: $o*i = O*i(s, i)$\n",
    "\n",
    "#### 4. Credit Assignment\n",
    "Determining individual agent contributions to team success.\n",
    "\n",
    "### Marl Paradigms\n",
    "\n",
    "#### 1. Cooperative Marl\n",
    "- **Objective**: Maximize team reward $R*{team} = \\sum*{i=1}^n R_i$\n",
    "- **Examples**: Multi-robot coordination, team games\n",
    "- **Algorithms**: MADDPG, QMIX, VDN\n",
    "\n",
    "#### 2. Competitive Marl\n",
    "- **Objective**: Each agent maximizes individual reward\n",
    "- **Examples**: Game playing, resource allocation\n",
    "- **Algorithms**: Self-play, Population-based training\n",
    "\n",
    "#### 3. Mixed-motive Marl\n",
    "- **Objective**: Combination of individual and team objectives\n",
    "- **Examples**: Social dilemmas, economic systems\n",
    "- **Algorithms**: Multi-objective optimization\n",
    "\n",
    "## 3.2 Advanced Marl Algorithms\n",
    "\n",
    "### 1. Multi-agent Deep Deterministic Policy Gradient (maddpg)\n",
    "**Key Idea**: Centralized training with decentralized execution\n",
    "\n",
    "**Critic Update**:\n",
    "$$Q*i^{\\mu}(s, a*1, ..., a*n) = \\mathbb{E}[r*i + \\gamma Q*i^{\\mu'}(s', \\mu*1'(o*1'), ..., \\mu*n'(o_n'))]$$\n",
    "\n",
    "**Actor Update**:\n",
    "$$\\nabla*{\\theta*i} J*i = \\mathbb{E}[\\nabla*{a*i} Q*i^{\\mu}(s, a*1, ..., a*n)|*{a*i=\\mu*i(o*i)} \\nabla*{\\theta*i} \\mu*i(o*i)]$$\n",
    "\n",
    "### 2. Qmix (monotonic Value Function Factorization)\n",
    "**Key Idea**: Factor team Q-value while maintaining monotonicity\n",
    "\n",
    "**Mixing Network**:\n",
    "$$Q*{tot}(s, \\mathbf{a}) = f*{mix}(Q*1(o*1, a*1), ..., Q*n(o*n, a*n), s)$$\n",
    "\n",
    "**Monotonicity Constraint**:\n",
    "$$\\frac{\\partial Q*{tot}}{\\partial Q*i} \\geq 0, \\forall i$$\n",
    "\n",
    "### 3. Multi-agent Actor-critic (maac)\n",
    "**Centralized Critic**: Uses global information during training\n",
    "$$Q^{\\pi}(s, a*1, ..., a*n) = \\mathbb{E}*{\\pi}[\\sum*{t=0}^{\\infty} \\gamma^t r*t | s*0=s, a*{0,i}=a*i, \\forall i]$$\n",
    "\n",
    "**Decentralized Actor**: Each agent has its own policy\n",
    "$$\\pi*i(a*i | o*i) = \\text{softmax}(f*i(o_i))$$\n",
    "\n",
    "## 3.3 Communication in Marl\n",
    "\n",
    "### 1. Explicit Communication\n",
    "Agents exchange messages to coordinate:\n",
    "$$m*i^t = f*{comm}(o*i^t, h*i^{t-1})$$\n",
    "$$h*i^t = f*{update}(o*i^t, m*{-i}^t, h_i^{t-1})$$\n",
    "\n",
    "### 2. Implicit Communication\n",
    "Coordination through shared representations or attention mechanisms.\n",
    "\n",
    "### 3. Emergent Communication\n",
    "Communication protocols emerge through learning:\n",
    "$$\\mathcal{L}*{comm} = \\mathcal{L}*{task} + \\lambda \\mathcal{L}_{communication}$$\n",
    "\n",
    "## 3.4 Applications of Marl\n",
    "\n",
    "### Autonomous Vehicle Coordination\n",
    "- **Agents**: Individual vehicles\n",
    "- **Objective**: Safe and efficient traffic flow\n",
    "- **Challenges**: Real-time coordination, safety constraints\n",
    "\n",
    "### Multi-robot Systems\n",
    "- **Agents**: Individual robots\n",
    "- **Objective**: Collaborative task completion\n",
    "- **Challenges**: Partial observability, communication constraints\n",
    "\n",
    "### Financial Trading\n",
    "- **Agents**: Individual traders/algorithms\n",
    "- **Objective**: Profit maximization\n",
    "- **Challenges**: Market manipulation, information asymmetry\n",
    "\n",
    "### Game Playing\n",
    "- **Agents**: Individual players\n",
    "- **Objective**: Win/score maximization\n",
    "- **Challenges**: Opponent modeling, strategy adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a340760a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Starting Multi-Agent RL Training...\n",
      "ðŸ¤ Demonstrating Multi-Agent Reinforcement Learning\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 656\u001b[39m\n\u001b[32m    654\u001b[39m \u001b[38;5;66;03m# Run Multi-Agent RL demonstration\u001b[39;00m\n\u001b[32m    655\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸš€ Starting Multi-Agent RL Training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m marl_results, maddpg_agents, qmix_agent, marl_env = \u001b[43mdemonstrate_multi_agent_rl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[32m    659\u001b[39m fig, axes = plt.subplots(\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m, figsize=(\u001b[32m15\u001b[39m, \u001b[32m12\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 543\u001b[39m, in \u001b[36mdemonstrate_multi_agent_rl\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    540\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m    542\u001b[39m \u001b[38;5;66;03m# Create multi-agent environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m543\u001b[39m env = \u001b[43mMultiAgentEnvironment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_targets\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    545\u001b[39m \u001b[38;5;66;03m# Initialize algorithms\u001b[39;00m\n\u001b[32m    546\u001b[39m obs_dim = env.observation_space\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mMultiAgentEnvironment.__init__\u001b[39m\u001b[34m(self, grid_size, num_agents, num_targets)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.max_episode_steps = \u001b[32m100\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Initialize agent and target positions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Action space: 0=stay, 1=up, 2=down, 3=left, 4=right\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.action_space = \u001b[32m5\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mMultiAgentEnvironment.reset\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_agents):\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         pos = [\u001b[43mnp\u001b[49m.random.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.grid_size), np.random.randint(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.grid_size)]\n\u001b[32m     28\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agent_positions:\n\u001b[32m     29\u001b[39m             \u001b[38;5;28mself\u001b[39m.agent_positions.append(pos)\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiAgentEnvironment:\n",
    "    \"\"\"Multi-agent environment for MARL demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, grid_size=8, num_agents=4, num_targets=3):\n",
    "        self.grid_size = grid_size\n",
    "        self.num_agents = num_agents\n",
    "        self.num_targets = num_targets\n",
    "        self.max_episode_steps = 100\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        self.action_space = 5\n",
    "        self.observation_space = 2 + 2 * num_agents + 2 * num_targets  # pos + other_agents + targets\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment to initial state.\"\"\"\n",
    "        self.current_step = 0\n",
    "        \n",
    "        self.agent_positions = []\n",
    "        for _ in range(self.num_agents):\n",
    "            while True:\n",
    "                pos = [np.random.randint(0, self.grid_size), np.random.randint(0, self.grid_size)]\n",
    "                if pos not in self.agent_positions:\n",
    "                    self.agent_positions.append(pos)\n",
    "                    break\n",
    "        \n",
    "        self.target_positions = []\n",
    "        for _ in range(self.num_targets):\n",
    "            while True:\n",
    "                pos = [np.random.randint(0, self.grid_size), np.random.randint(0, self.grid_size)]\n",
    "                if pos not in self.agent_positions and pos not in self.target_positions:\n",
    "                    self.target_positions.append(pos)\n",
    "                    break\n",
    "        \n",
    "        self.targets_collected = [False] * self.num_targets\n",
    "        return self.get_observations()\n",
    "    \n",
    "    def get_observations(self):\n",
    "        \"\"\"Get observations for all agents.\"\"\"\n",
    "        observations = []\n",
    "        \n",
    "        for i in range(self.num_agents):\n",
    "            obs = []\n",
    "            \n",
    "            obs.extend([self.agent_positions[i][0] / self.grid_size, \n",
    "                       self.agent_positions[i][1] / self.grid_size])\n",
    "            \n",
    "            for j in range(self.num_agents):\n",
    "                if i != j:\n",
    "                    rel_pos = [(self.agent_positions[j][0] - self.agent_positions[i][0]) / self.grid_size,\n",
    "                              (self.agent_positions[j][1] - self.agent_positions[i][1]) / self.grid_size]\n",
    "                    obs.extend(rel_pos)\n",
    "            \n",
    "            for k, target_pos in enumerate(self.target_positions):\n",
    "                if not self.targets_collected[k]:\n",
    "                    rel_pos = [(target_pos[0] - self.agent_positions[i][0]) / self.grid_size,\n",
    "                              (target_pos[1] - self.agent_positions[i][1]) / self.grid_size]\n",
    "                    obs.extend(rel_pos)\n",
    "                else:\n",
    "                    obs.extend([0.0, 0.0])  # Target collected\n",
    "            \n",
    "            observations.append(np.array(obs, dtype=np.float32))\n",
    "        \n",
    "        return observations\n",
    "    \n",
    "    def step(self, actions):\n",
    "        \"\"\"Execute joint action and return results.\"\"\"\n",
    "        self.current_step += 1\n",
    "        rewards = [0.0] * self.num_agents\n",
    "        \n",
    "        new_positions = []\n",
    "        for i, action in enumerate(actions):\n",
    "            pos = self.agent_positions[i].copy()\n",
    "            \n",
    "            if action == 1 and pos[1] < self.grid_size - 1:  # up\n",
    "                pos[1] += 1\n",
    "            elif action == 2 and pos[1] > 0:  # down\n",
    "                pos[1] -= 1\n",
    "            elif action == 3 and pos[0] > 0:  # left\n",
    "                pos[0] -= 1\n",
    "            elif action == 4 and pos[0] < self.grid_size - 1:  # right\n",
    "                pos[0] += 1\n",
    "            \n",
    "            new_positions.append(pos)\n",
    "        \n",
    "        collision_agents = set()\n",
    "        for i in range(self.num_agents):\n",
    "            for j in range(i + 1, self.num_agents):\n",
    "                if new_positions[i] == new_positions[j]:\n",
    "                    collision_agents.add(i)\n",
    "                    collision_agents.add(j)\n",
    "        \n",
    "        for i in range(self.num_agents):\n",
    "            if i not in collision_agents:\n",
    "                self.agent_positions[i] = new_positions[i]\n",
    "            else:\n",
    "                rewards[i] -= 0.5  # Collision penalty\n",
    "        \n",
    "        targets_collected_this_step = []\n",
    "        for i in range(self.num_agents):\n",
    "            for j, target_pos in enumerate(self.target_positions):\n",
    "                if (not self.targets_collected[j] and \n",
    "                    self.agent_positions[i] == target_pos):\n",
    "                    self.targets_collected[j] = True\n",
    "                    rewards[i] += 10.0  # Target collection reward\n",
    "                    targets_collected_this_step.append(j)\n",
    "        \n",
    "        if targets_collected_this_step:\n",
    "            team_bonus = 2.0 * len(targets_collected_this_step)\n",
    "            for i in range(self.num_agents):\n",
    "                rewards[i] += team_bonus / self.num_agents\n",
    "        \n",
    "        for i in range(self.num_agents):\n",
    "            rewards[i] -= 0.1\n",
    "        \n",
    "        done = (all(self.targets_collected) or \n",
    "                self.current_step >= self.max_episode_steps)\n",
    "        \n",
    "        observations = self.get_observations()\n",
    "        info = {\n",
    "            'targets_collected': sum(self.targets_collected),\n",
    "            'total_targets': self.num_targets,\n",
    "            'collisions': len(collision_agents) // 2\n",
    "        }\n",
    "        \n",
    "        return observations, rewards, done, info\n",
    "\n",
    "class MADDPGAgent:\n",
    "    \"\"\"Multi-Agent Deep Deterministic Policy Gradient agent.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, num_agents, agent_id, lr=1e-3):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_agents = num_agents\n",
    "        self.agent_id = agent_id\n",
    "        \n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).to(device)\n",
    "        \n",
    "        global_obs_dim = obs_dim * num_agents\n",
    "        global_action_dim = action_dim * num_agents\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(global_obs_dim + global_action_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.actor_target = copy.deepcopy(self.actor)\n",
    "        self.critic_target = copy.deepcopy(self.critic)\n",
    "        \n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=lr)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        self.tau = 0.01  # Soft update rate\n",
    "        \n",
    "        self.actor_losses = []\n",
    "        self.critic_losses = []\n",
    "    \n",
    "    def get_action(self, observation, exploration_noise=0.1):\n",
    "        \"\"\"Get action with optional exploration noise.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(device)\n",
    "            action_probs = self.actor(obs_tensor)\n",
    "            \n",
    "            if exploration_noise > 0:\n",
    "                noise = torch.randn_like(action_probs) * exploration_noise\n",
    "                action_probs = torch.softmax(action_probs + noise, dim=-1)\n",
    "            \n",
    "            action_dist = Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            \n",
    "        return action.item()\n",
    "    \n",
    "    def update(self, batch, other_agents):\n",
    "        \"\"\"Update MADDPG agent using centralized training.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        states = torch.FloatTensor(states).to(device)\n",
    "        actions = torch.LongTensor(actions).to(device)\n",
    "        rewards = torch.FloatTensor(rewards[:, self.agent_id]).to(device)\n",
    "        next_states = torch.FloatTensor(next_states).to(device)\n",
    "        dones = torch.BoolTensor(dones).to(device)\n",
    "        \n",
    "        batch_size = states.shape[0]\n",
    "        \n",
    "        states_flat = states.view(batch_size, -1)\n",
    "        next_states_flat = next_states.view(batch_size, -1)\n",
    "        \n",
    "        actions_onehot = F.one_hot(actions, num_classes=self.action_dim).float()\n",
    "        actions_flat = actions_onehot.view(batch_size, -1)\n",
    "        \n",
    "        next_actions = []\n",
    "        with torch.no_grad():\n",
    "            for i in range(self.num_agents):\n",
    "                if i == self.agent_id:\n",
    "                    next_action_probs = self.actor_target(next_states[:, i])                \n",
    "                else:                    \n",
    "                    next_action_probs = other_agents[i].actor_target(next_states[:, i])\n",
    "                next_actions.append(next_action_probs)\n",
    "        \n",
    "        next_actions_concat = torch.cat(next_actions, dim=-1)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            critic_input = torch.cat([next_states_flat, next_actions_concat], dim=-1)\n",
    "            target_q_values = self.critic_target(critic_input).squeeze()\n",
    "            target_q_values = rewards + self.gamma * target_q_values * (~dones)\n",
    "        \n",
    "        current_q_input = torch.cat([states_flat, actions_flat], dim=-1)\n",
    "        current_q_values = self.critic(current_q_input).squeeze()\n",
    "        \n",
    "        critic_loss = F.mse_loss(current_q_values, target_q_values)\n",
    "        \n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0)\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        current_actions = []\n",
    "        for i in range(self.num_agents):\n",
    "            if i == self.agent_id:\n",
    "                current_actions.append(self.actor(states[:, i]))\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    current_actions.append(other_agents[i].actor(states[:, i]))\n",
    "        \n",
    "        current_actions_concat = torch.cat(current_actions, dim=-1)\n",
    "        actor_critic_input = torch.cat([states_flat, current_actions_concat], dim=-1)\n",
    "        actor_loss = -self.critic(actor_critic_input).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        self.soft_update()\n",
    "        \n",
    "        self.actor_losses.append(actor_loss.item())\n",
    "        self.critic_losses.append(critic_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'actor_loss': actor_loss.item(),\n",
    "            'critic_loss': critic_loss.item()\n",
    "        }\n",
    "    \n",
    "    def soft_update(self):\n",
    "        \"\"\"Soft update of target networks.\"\"\"\n",
    "        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "class QMIXAgent:\n",
    "    \"\"\"QMIX agent with value function factorization.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, num_agents, state_dim, lr=1e-3):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_agents = num_agents\n",
    "        self.state_dim = state_dim\n",
    "        \n",
    "        self.q_networks = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(obs_dim, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, action_dim)\n",
    "            ).to(device) for _ in range(num_agents)\n",
    "        ])\n",
    "        \n",
    "        self.mixing_network = nn.Sequential(\n",
    "            nn.Linear(state_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, num_agents * 32),  # Weights for mixing\n",
    "            nn.ReLU()\n",
    "        ).to(device)\n",
    "        \n",
    "        self.final_layer = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.target_q_networks = copy.deepcopy(self.q_networks)\n",
    "        self.target_mixing_network = copy.deepcopy(self.mixing_network)\n",
    "        self.target_final_layer = copy.deepcopy(self.final_layer)\n",
    "        \n",
    "        all_params = (list(self.q_networks.parameters()) + \n",
    "                     list(self.mixing_network.parameters()) + \n",
    "                     list(self.final_layer.parameters()))\n",
    "        self.optimizer = optim.Adam(all_params, lr=lr)\n",
    "        \n",
    "        self.gamma = 0.95\n",
    "        self.tau = 0.01\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.1\n",
    "        \n",
    "        self.losses = []\n",
    "        self.team_rewards = []\n",
    "    \n",
    "    def get_actions(self, observations):\n",
    "        \"\"\"Get actions for all agents.\"\"\"\n",
    "        actions = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i, obs in enumerate(observations):\n",
    "                obs_tensor = torch.FloatTensor(obs).unsqueeze(0).to(device)\n",
    "                q_values = self.q_networks[i](obs_tensor)\n",
    "                \n",
    "                if np.random.random() < self.epsilon:\n",
    "                    action = np.random.randint(self.action_dim)\n",
    "                else:\n",
    "                    action = q_values.argmax().item()\n",
    "                \n",
    "                actions.append(action)\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def mixing_forward(self, individual_q_values, state):\n",
    "        \"\"\"Forward pass through mixing network.\"\"\"\n",
    "        mixing_weights = self.mixing_network(state)\n",
    "        mixing_weights = mixing_weights.view(-1, self.num_agents, 32)\n",
    "        \n",
    "        mixing_weights = torch.abs(mixing_weights)\n",
    "        \n",
    "        individual_q_values = individual_q_values.unsqueeze(-1)  # [batch, agents, 1]\n",
    "        mixed_values = torch.bmm(mixing_weights.transpose(1, 2), individual_q_values)  # [batch, 32, 1]\n",
    "        mixed_values = mixed_values.squeeze(-1)  # [batch, 32]\n",
    "        \n",
    "        team_q_value = self.final_layer(mixed_values)\n",
    "        \n",
    "        return team_q_value\n",
    "    \n",
    "    def update(self, batch):\n",
    "        \"\"\"Update QMIX agent.\"\"\"\n",
    "        states, actions, rewards, next_states, dones = batch\n",
    "        \n",
    "        batch_size = len(states)\n",
    "        \n",
    "        states_tensor = torch.FloatTensor(states).to(device)\n",
    "        actions_tensor = torch.LongTensor(actions).to(device)\n",
    "        team_rewards = torch.FloatTensor([sum(r) for r in rewards]).to(device)\n",
    "        next_states_tensor = torch.FloatTensor(next_states).to(device)\n",
    "        dones_tensor = torch.BoolTensor(dones).to(device)\n",
    "        \n",
    "        states_flat = states_tensor.view(batch_size, -1)\n",
    "        next_states_flat = next_states_tensor.view(batch_size, -1)\n",
    "        \n",
    "        individual_q_values = []\n",
    "        for i in range(self.num_agents):\n",
    "            q_vals = self.q_networks[i](states_tensor[:, i])\n",
    "            chosen_q_vals = q_vals.gather(1, actions_tensor[:, i].unsqueeze(1)).squeeze()\n",
    "            individual_q_values.append(chosen_q_vals)\n",
    "        \n",
    "        individual_q_values = torch.stack(individual_q_values, dim=1)  # [batch, agents]\n",
    "        \n",
    "        team_q_values = self.mixing_forward(individual_q_values, states_flat).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_individual_q_values = []\n",
    "            for i in range(self.num_agents):\n",
    "                next_q_vals = self.target_q_networks[i](next_states_tensor[:, i])\n",
    "                max_next_q_vals = next_q_vals.max(1)[0]\n",
    "                next_individual_q_values.append(max_next_q_vals)\n",
    "            \n",
    "            next_individual_q_values = torch.stack(next_individual_q_values, dim=1)\n",
    "            \n",
    "            target_mixing_weights = self.target_mixing_network(next_states_flat)\n",
    "            target_mixing_weights = target_mixing_weights.view(-1, self.num_agents, 32)\n",
    "            target_mixing_weights = torch.abs(target_mixing_weights)\n",
    "            \n",
    "            next_individual_q_values_expanded = next_individual_q_values.unsqueeze(-1)\n",
    "            target_mixed_values = torch.bmm(\n",
    "                target_mixing_weights.transpose(1, 2), \n",
    "                next_individual_q_values_expanded\n",
    "            ).squeeze(-1)\n",
    "            \n",
    "            target_team_q_values = self.target_final_layer(target_mixed_values).squeeze()\n",
    "            target_team_q_values = team_rewards + self.gamma * target_team_q_values * (~dones_tensor)\n",
    "        \n",
    "        loss = F.mse_loss(team_q_values, target_team_q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            list(self.q_networks.parameters()) + \n",
    "            list(self.mixing_network.parameters()) + \n",
    "            list(self.final_layer.parameters()), \n",
    "            max_norm=1.0\n",
    "        )\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        if len(self.losses) % 100 == 0:\n",
    "            self.soft_update_targets()\n",
    "        \n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        \n",
    "        self.losses.append(loss.item())\n",
    "        self.team_rewards.append(team_rewards.mean().item())\n",
    "        \n",
    "        return {\n",
    "            'loss': loss.item(),\n",
    "            'team_reward': team_rewards.mean().item(),\n",
    "            'epsilon': self.epsilon\n",
    "        }\n",
    "    \n",
    "    def soft_update_targets(self):\n",
    "        \"\"\"Soft update of target networks.\"\"\"\n",
    "        for target, source in zip(self.target_q_networks, self.q_networks):\n",
    "            for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_mixing_network.parameters(), \n",
    "                                      self.mixing_network.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "        \n",
    "        for target_param, param in zip(self.target_final_layer.parameters(), \n",
    "                                      self.final_layer.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "\n",
    "class MultiAgentReplayBuffer:\n",
    "    \"\"\"Replay buffer for multi-agent learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, capacity, num_agents, obs_dim):\n",
    "        self.capacity = capacity\n",
    "        self.num_agents = num_agents\n",
    "        self.obs_dim = obs_dim\n",
    "        \n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.next_states = []\n",
    "        self.dones = []\n",
    "        self.position = 0\n",
    "        self.size = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in buffer.\"\"\"\n",
    "        if len(self.states) < self.capacity:\n",
    "            self.states.append(None)\n",
    "            self.actions.append(None)\n",
    "            self.rewards.append(None)\n",
    "            self.next_states.append(None)\n",
    "            self.dones.append(None)\n",
    "        \n",
    "        self.states[self.position] = state\n",
    "        self.actions[self.position] = action\n",
    "        self.rewards[self.position] = reward\n",
    "        self.next_states[self.position] = next_state\n",
    "        self.dones[self.position] = done\n",
    "        \n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample batch from buffer.\"\"\"\n",
    "        if self.size < batch_size:\n",
    "            return None\n",
    "        \n",
    "        indices = np.random.choice(self.size, batch_size, replace=False)\n",
    "        \n",
    "        batch_states = [self.states[i] for i in indices]\n",
    "        batch_actions = [self.actions[i] for i in indices]\n",
    "        batch_rewards = [self.rewards[i] for i in indices]\n",
    "        batch_next_states = [self.next_states[i] for i in indices]\n",
    "        batch_dones = [self.dones[i] for i in indices]\n",
    "        \n",
    "        return (batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones)\n",
    "\n",
    "def demonstrate_multi_agent_rl():\n",
    "    \"\"\"Demonstrate Multi-Agent RL algorithms.\"\"\"\n",
    "    print(\"ðŸ¤ Demonstrating Multi-Agent Reinforcement Learning\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    env = MultiAgentEnvironment(grid_size=8, num_agents=4, num_targets=3)\n",
    "    \n",
    "    obs_dim = env.observation_space\n",
    "    action_dim = env.action_space\n",
    "    num_agents = env.num_agents\n",
    "    \n",
    "    maddpg_agents = [\n",
    "        MADDPGAgent(obs_dim, action_dim, num_agents, i) \n",
    "        for i in range(num_agents)\n",
    "    ]\n",
    "    \n",
    "    state_dim = obs_dim * num_agents  # Global state dimension\n",
    "    qmix_agent = QMIXAgent(obs_dim, action_dim, num_agents, state_dim)\n",
    "    \n",
    "    maddpg_buffer = MultiAgentReplayBuffer(capacity=50000, num_agents=num_agents, obs_dim=obs_dim)\n",
    "    qmix_buffer = MultiAgentReplayBuffer(capacity=50000, num_agents=num_agents, obs_dim=obs_dim)\n",
    "    \n",
    "    results = {\n",
    "        'MADDPG': {'rewards': [], 'targets_collected': [], 'cooperation_rate': []},\n",
    "        'QMIX': {'rewards': [], 'targets_collected': [], 'cooperation_rate': []}\n",
    "    }\n",
    "    \n",
    "    num_episodes = 500\n",
    "    batch_size = 32\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        observations = env.reset()\n",
    "        episode_reward = 0\n",
    "        targets_collected = 0\n",
    "        cooperation_events = 0\n",
    "        \n",
    "        for step in range(100):\n",
    "            actions = []\n",
    "            for i, agent in enumerate(maddpg_agents):\n",
    "                action = agent.get_action(observations[i], exploration_noise=0.1)\n",
    "                actions.append(action)\n",
    "            \n",
    "            next_observations, rewards, done, info = env.step(actions)\n",
    "            \n",
    "            maddpg_buffer.push(observations, actions, rewards, next_observations, done)\n",
    "            \n",
    "            episode_reward += sum(rewards)\n",
    "            targets_collected = info['targets_collected']\n",
    "            if info['targets_collected'] > 0:\n",
    "                cooperation_events += 1\n",
    "            \n",
    "            observations = next_observations\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if maddpg_buffer.size > batch_size:\n",
    "            batch = maddpg_buffer.sample(batch_size)\n",
    "            for agent in maddpg_agents:\n",
    "                agent.update(batch, maddpg_agents)\n",
    "        \n",
    "        results['MADDPG']['rewards'].append(episode_reward)\n",
    "        results['MADDPG']['targets_collected'].append(targets_collected)\n",
    "        results['MADDPG']['cooperation_rate'].append(cooperation_events / max(1, step + 1))\n",
    "        \n",
    "        observations = env.reset()\n",
    "        episode_reward = 0\n",
    "        targets_collected = 0\n",
    "        cooperation_events = 0\n",
    "        \n",
    "        for step in range(100):\n",
    "            actions = qmix_agent.get_actions(observations)\n",
    "            next_observations, rewards, done, info = env.step(actions)\n",
    "            \n",
    "            qmix_buffer.push(observations, actions, rewards, next_observations, done)\n",
    "            \n",
    "            episode_reward += sum(rewards)\n",
    "            targets_collected = info['targets_collected']\n",
    "            if info['targets_collected'] > 0:\n",
    "                cooperation_events += 1\n",
    "            \n",
    "            observations = next_observations\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if qmix_buffer.size > batch_size:\n",
    "            batch = qmix_buffer.sample(batch_size)\n",
    "            qmix_agent.update(batch)\n",
    "        \n",
    "        results['QMIX']['rewards'].append(episode_reward)\n",
    "        results['QMIX']['targets_collected'].append(targets_collected)\n",
    "        results['QMIX']['cooperation_rate'].append(cooperation_events / max(1, step + 1))\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"\\nEpisode {episode}:\")\n",
    "            for algo_name in ['MADDPG', 'QMIX']:\n",
    "                recent_rewards = np.mean(results[algo_name]['rewards'][-50:])\n",
    "                recent_targets = np.mean(results[algo_name]['targets_collected'][-50:])\n",
    "                recent_coop = np.mean(results[algo_name]['cooperation_rate'][-50:])\n",
    "                print(f\"  {algo_name}: Reward={recent_rewards:.2f}, Targets={recent_targets:.1f}, Cooperation={recent_coop:.3f}\")\n",
    "    \n",
    "    return results, maddpg_agents, qmix_agent, env\n",
    "\n",
    "print(\"ðŸš€ Starting Multi-Agent RL Training...\")\n",
    "marl_results, maddpg_agents, qmix_agent, marl_env = demonstrate_multi_agent_rl()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "for algo_name, data in marl_results.items():\n",
    "    window_size = 20\n",
    "    if len(data['rewards']) >= window_size:\n",
    "        smoothed_rewards = pd.Series(data['rewards']).rolling(window_size).mean()\n",
    "        axes[0, 0].plot(smoothed_rewards, label=algo_name, linewidth=2)\n",
    "\n",
    "axes[0, 0].set_title('Multi-Agent RL Learning Curves')\n",
    "axes[0, 0].set_xlabel('Episode')\n",
    "axes[0, 0].set_ylabel('Total Team Reward (Smoothed)')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for algo_name, data in marl_results.items():\n",
    "    window_size = 20\n",
    "    if len(data['targets_collected']) >= window_size:\n",
    "        smoothed_targets = pd.Series(data['targets_collected']).rolling(window_size).mean()\n",
    "        axes[0, 1].plot(smoothed_targets, label=algo_name, linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('Target Collection Efficiency')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Targets Collected per Episode (Smoothed)')\n",
    "axes[0, 1].axhline(y=marl_env.num_targets, color='red', linestyle='--', label='Max Targets')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "for algo_name, data in marl_results.items():\n",
    "    window_size = 20\n",
    "    if len(data['cooperation_rate']) >= window_size:\n",
    "        smoothed_coop = pd.Series(data['cooperation_rate']).rolling(window_size).mean()\n",
    "        axes[1, 0].plot(smoothed_coop, label=algo_name, linewidth=2)\n",
    "\n",
    "axes[1, 0].set_title('Cooperation Rate Over Time')\n",
    "axes[1, 0].set_xlabel('Episode')\n",
    "axes[1, 0].set_ylabel('Cooperation Events per Step (Smoothed)')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "final_performance = {}\n",
    "for algo_name, data in marl_results.items():\n",
    "    final_reward = np.mean(data['rewards'][-50:])\n",
    "    final_targets = np.mean(data['targets_collected'][-50:])\n",
    "    final_coop = np.mean(data['cooperation_rate'][-50:])\n",
    "    \n",
    "    axes[1, 1].bar(algo_name, final_reward, alpha=0.7)\n",
    "    final_performance[algo_name] = {\n",
    "        'reward': final_reward,\n",
    "        'targets': final_targets,\n",
    "        'cooperation': final_coop\n",
    "    }\n",
    "\n",
    "axes[1, 1].set_title('Final Performance Comparison')\n",
    "axes[1, 1].set_ylabel('Average Team Reward (Last 50 Episodes)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Multi-Agent RL Results Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for algo_name, performance in final_performance.items():\n",
    "    print(f\"\\n{algo_name}:\")\n",
    "    print(f\"  Final Team Reward: {performance['reward']:.2f}\")\n",
    "    print(f\"  Target Collection Rate: {performance['targets']:.2f}/{marl_env.num_targets}\")\n",
    "    print(f\"  Cooperation Rate: {performance['cooperation']:.3f}\")\n",
    "    print(f\"  Success Rate: {performance['targets']/marl_env.num_targets:.1%}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"  â€¢ MADDPG uses centralized training with decentralized execution\")\n",
    "print(\"  â€¢ QMIX factorizes team value function while maintaining monotonicity\")\n",
    "print(\"  â€¢ Cooperation emerges through reward structure and learning\")\n",
    "print(\"  â€¢ Multi-agent coordination improves with experience\")\n",
    "\n",
    "print(\"\\nðŸ¤ Multi-Agent RL demonstration completed!\")\n",
    "print(\"ðŸ”„ Ready for Robust RL implementation...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fd8428",
   "metadata": {},
   "source": [
    "# Section 4: Robust Reinforcement Learning\n",
    "\n",
    "## 4.1 Theory: Handling Uncertainty and Adversarial Conditions\n",
    "\n",
    "Robust Reinforcement Learning addresses the challenge of learning policies that perform well under uncertainty, distributional shifts, and adversarial conditions. This is crucial for deploying RL agents in real-world environments where training and testing conditions may differ significantly.\n",
    "\n",
    "### Sources of Uncertainty in Rl\n",
    "\n",
    "#### 1. Model Uncertainty\n",
    "- **Transition Dynamics**: $P(s'|s,a)$ may be unknown or changing\n",
    "- **Reward Function**: $R(s,a)$ may be noisy or non-stationary\n",
    "- **Initial State Distribution**: $\\mu_0(s)$ may vary between episodes\n",
    "\n",
    "#### 2. Environmental Uncertainty\n",
    "- **Observation Noise**: $o*t = s*t + \\epsilon*t$ where $\\epsilon*t \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "- **Partial Observability**: Agent only observes $o*t$ instead of full state $s*t$\n",
    "- **Dynamic Environments**: Environment parameters change over time\n",
    "\n",
    "#### 3. Distributional Shift\n",
    "- **Covariate Shift**: $P*{train}(s) \\neq P*{test}(s)$\n",
    "- **Concept Drift**: $P*{train}(s'|s,a) \\neq P*{test}(s'|s,a)$\n",
    "- **Domain Gap**: Training and deployment environments differ\n",
    "\n",
    "### Mathematical Framework for Robust Rl\n",
    "\n",
    "#### Robust Markov Decision Process (rmdp)\n",
    "An RMDP extends the standard MDP to handle uncertainty:\n",
    "$$\\text{RMDP} = (\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma, \\mu_0)$$\n",
    "\n",
    "Where:\n",
    "- **$\\mathcal{P}$**: Uncertainty set of transition kernels\n",
    "- **$\\mathcal{R}$**: Uncertainty set of reward functions\n",
    "\n",
    "#### Robust Value Function\n",
    "The robust value function considers worst-case scenarios:\n",
    "$$V^{\\pi}*{robust}(s) = \\min*{P \\in \\mathcal{P}, R \\in \\mathcal{R}} V^{\\pi}_{P,R}(s)$$\n",
    "\n",
    "#### Distributionally Robust Optimization (dro)\n",
    "Optimize performance over a set of probability distributions:\n",
    "$$\\max*\\pi \\min*{P \\in \\mathcal{P}} \\mathbb{E}*{P}[\\sum*{t=0}^{\\infty} \\gamma^t R(s*t, a*t)]$$\n",
    "\n",
    "### Approaches to Robust Rl\n",
    "\n",
    "#### 1. Domain Randomization\n",
    "**Idea**: Train on diverse environments to improve generalization\n",
    "\n",
    "**Implementation**:\n",
    "- Randomize environment parameters during training\n",
    "- Sample from distribution: $\\theta \\sim p(\\theta)$\n",
    "- Train policy to work across parameter space\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "$$J*{DR}(\\pi) = \\mathbb{E}*{\\theta \\sim p(\\theta)}[J_\\theta(\\pi)]$$\n",
    "\n",
    "#### 2. Adversarial Training\n",
    "**Idea**: Train against adversarial perturbations\n",
    "\n",
    "**Min-Max Objective**:\n",
    "$$\\max*\\pi \\min*{\\delta} \\mathbb{E}[R(s + \\delta, \\pi(s + \\delta))]$$\n",
    "\n",
    "Subject to: $||\\delta|| \\leq \\epsilon$\n",
    "\n",
    "#### 3. Distributional Rl for Robustness\n",
    "**Idea**: Model full return distribution instead of expected value\n",
    "\n",
    "**Quantile Regression**:\n",
    "$$\\mathcal{L}(\\tau, \\hat{Z}) = \\mathbb{E}[(\\tau - \\mathbb{1}_{z < \\hat{Z}(\\tau)})(z - \\hat{Z}(\\tau))]$$\n",
    "\n",
    "#### 4. Bayesian Rl\n",
    "**Idea**: Maintain uncertainty over model parameters\n",
    "\n",
    "**Posterior Update**:\n",
    "$$P(\\theta|D) \\propto P(D|\\theta)P(\\theta)$$\n",
    "\n",
    "**Thompson Sampling**:\n",
    "$$\\pi*t = \\arg\\max*\\pi \\mathbb{E}*{\\theta \\sim P(\\theta|D*t)}[V^\\pi_\\theta]$$\n",
    "\n",
    "## 4.2 Risk Measures in Robust Rl\n",
    "\n",
    "### 1. Conditional Value at Risk (cvar)\n",
    "Optimize worst-case expected returns:\n",
    "$$\\text{CVaR}*\\alpha(Z) = \\mathbb{E}[Z | Z \\leq \\text{VaR}*\\alpha(Z)]$$\n",
    "\n",
    "### 2. Coherent Risk Measures\n",
    "Risk measure $\\rho$ is coherent if it satisfies:\n",
    "- **Monotonicity**: $X \\geq Y \\Rightarrow \\rho(X) \\leq \\rho(Y)$\n",
    "- **Translation Invariance**: $\\rho(X + c) = \\rho(X) - c$\n",
    "- **Positive Homogeneity**: $\\rho(cX) = c\\rho(X)$ for $c \\geq 0$\n",
    "- **Subadditivity**: $\\rho(X + Y) \\leq \\rho(X) + \\rho(Y)$\n",
    "\n",
    "### 3. Entropic Risk Measure\n",
    "$$\\rho_\\beta(Z) = \\frac{1}{\\beta} \\log \\mathbb{E}[e^{-\\beta Z}]$$\n",
    "\n",
    "## 4.3 Uncertainty Quantification\n",
    "\n",
    "### 1. Epistemic Vs Aleatoric Uncertainty\n",
    "- **Epistemic**: Model uncertainty (reducible with more data)\n",
    "- **Aleatoric**: Data uncertainty (irreducible noise)\n",
    "\n",
    "### 2. Ensemble Methods\n",
    "Maintain multiple models and aggregate predictions:\n",
    "$$\\mu(x) = \\frac{1}{M} \\sum*{i=1}^M f*i(x)$$\n",
    "$$\\sigma^2(x) = \\frac{1}{M} \\sum*{i=1}^M (f*i(x) - \\mu(x))^2$$\n",
    "\n",
    "### 3. Dropout-based Uncertainty\n",
    "Use Monte Carlo dropout for uncertainty estimation:\n",
    "$$\\mu(x) = \\frac{1}{T} \\sum*{t=1}^T f(x, \\epsilon*t)$$\n",
    "\n",
    "## 4.4 Applications of Robust Rl\n",
    "\n",
    "### Autonomous Driving\n",
    "- **Uncertainties**: Weather conditions, other drivers' behavior\n",
    "- **Robustness**: Safe driving across diverse conditions\n",
    "- **Methods**: Domain randomization, distributional RL\n",
    "\n",
    "### Financial Trading\n",
    "- **Uncertainties**: Market volatility, regime changes\n",
    "- **Robustness**: Consistent performance across market conditions\n",
    "- **Methods**: Risk-sensitive RL, robust optimization\n",
    "\n",
    "### Healthcare\n",
    "- **Uncertainties**: Patient variability, measurement noise\n",
    "- **Robustness**: Safe treatment across patient populations\n",
    "- **Methods**: Bayesian RL, conservative policy optimization\n",
    "\n",
    "### Robotics\n",
    "- **Uncertainties**: Sensor noise, actuator failures, environmental changes\n",
    "- **Robustness**: Reliable operation in unstructured environments\n",
    "- **Methods**: Adaptive control, robust MPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b194dc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RobustEnvironment:\n",
    "    \"\"\"Environment with configurable uncertainty for robust RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_size=6, uncertainty_level=0.1, dynamic_obstacles=True):\n",
    "        self.base_size = base_size\n",
    "        self.uncertainty_level = uncertainty_level\n",
    "        self.dynamic_obstacles = dynamic_obstacles\n",
    "        \n",
    "        self.current_size = base_size\n",
    "        self.noise_std = 0.0\n",
    "        self.action_failure_prob = 0.0\n",
    "        self.reward_noise_std = 0.0\n",
    "        \n",
    "        self.reset()\n",
    "        \n",
    "        self.action_space = 4  # up, down, left, right\n",
    "        self.max_episode_steps = 100\n",
    "    \n",
    "    def randomize_parameters(self):\n",
    "        \"\"\"Apply domain randomization to environment parameters.\"\"\"\n",
    "        size_variation = max(1, int(self.base_size * self.uncertainty_level))\n",
    "        self.current_size = np.random.randint(\n",
    "            max(3, self.base_size - size_variation),\n",
    "            self.base_size + size_variation + 1\n",
    "        )\n",
    "        \n",
    "        self.noise_std = np.random.uniform(0, self.uncertainty_level)\n",
    "        self.action_failure_prob = np.random.uniform(0, self.uncertainty_level)\n",
    "        self.reward_noise_std = np.random.uniform(0, self.uncertainty_level * 5)\n",
    "        \n",
    "        if self.dynamic_obstacles:\n",
    "            num_obstacles = np.random.randint(0, max(1, self.current_size // 2))\n",
    "            self.obstacles = []\n",
    "            for _ in range(num_obstacles):\n",
    "                obs_pos = [np.random.randint(1, self.current_size-1), \n",
    "                          np.random.randint(1, self.current_size-1)]\n",
    "                if obs_pos not in self.obstacles:\n",
    "                    self.obstacles.append(obs_pos)\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset environment with potential randomization.\"\"\"\n",
    "        self.randomize_parameters()\n",
    "        \n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [self.current_size-1, self.current_size-1]\n",
    "        self.current_step = 0\n",
    "        \n",
    "        if not hasattr(self, 'obstacles'):\n",
    "            self.obstacles = []\n",
    "        \n",
    "        return self.get_observation()\n",
    "    \n",
    "    def get_observation(self):\n",
    "        \"\"\"Get observation with potential noise.\"\"\"\n",
    "        obs = np.array([\n",
    "            self.agent_pos[0] / self.current_size,\n",
    "            self.agent_pos[1] / self.current_size,\n",
    "            (self.goal_pos[0] - self.agent_pos[0]) / self.current_size,\n",
    "            (self.goal_pos[1] - self.agent_pos[1]) / self.current_size,\n",
    "            self.current_size / 10.0,  # Environment size as feature\n",
    "            len(self.obstacles) / 10.0  # Number of obstacles\n",
    "        ], dtype=np.float32)\n",
    "        \n",
    "        if self.noise_std > 0:\n",
    "            noise = np.random.normal(0, self.noise_std, obs.shape)\n",
    "            obs += noise\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute action with potential failures and noise.\"\"\"\n",
    "        self.current_step += 1\n",
    "        \n",
    "        if np.random.random() < self.action_failure_prob:\n",
    "            action = 4  # Stay in place\n",
    "        \n",
    "        prev_pos = self.agent_pos.copy()\n",
    "        \n",
    "        if action == 0 and self.agent_pos[1] < self.current_size - 1:  # up\n",
    "            self.agent_pos[1] += 1\n",
    "        elif action == 1 and self.agent_pos[1] > 0:  # down\n",
    "            self.agent_pos[1] -= 1\n",
    "        elif action == 2 and self.agent_pos[0] > 0:  # left\n",
    "            self.agent_pos[0] -= 1\n",
    "        elif action == 3 and self.agent_pos[0] < self.current_size - 1:  # right\n",
    "            self.agent_pos[0] += 1\n",
    "        \n",
    "        if self.agent_pos in self.obstacles:\n",
    "            self.agent_pos = prev_pos  # Revert move\n",
    "            reward = -5.0  # Collision penalty\n",
    "        else:\n",
    "            done = (self.agent_pos == self.goal_pos)\n",
    "            if done:\n",
    "                reward = 10.0\n",
    "            else:\n",
    "                dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
    "                reward = -0.1 - 0.01 * dist\n",
    "        \n",
    "        if self.reward_noise_std > 0:\n",
    "            reward += np.random.normal(0, self.reward_noise_std)\n",
    "        \n",
    "        done = (self.agent_pos == self.goal_pos) or (self.current_step >= self.max_episode_steps)\n",
    "        \n",
    "        info = {\n",
    "            'environment_size': self.current_size,\n",
    "            'noise_level': self.noise_std,\n",
    "            'action_failure_prob': self.action_failure_prob,\n",
    "            'obstacles': len(self.obstacles)\n",
    "        }\n",
    "        \n",
    "        return self.get_observation(), reward, done, info\n",
    "\n",
    "class DomainRandomizationAgent:\n",
    "    \"\"\"RL agent trained with domain randomization for robustness.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, lr=3e-4):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        self.clip_ratio = 0.2\n",
    "        \n",
    "        self.policy_losses = []\n",
    "        self.value_losses = []\n",
    "        self.environment_diversity = []\n",
    "    \n",
    "    def get_action(self, observation):\n",
    "        \"\"\"Get action from policy with exploration.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(device)\n",
    "            action_probs = self.policy_network(obs_tensor)\n",
    "            action_dist = Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            value = self.value_network(obs_tensor)\n",
    "            \n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "    \n",
    "    def update(self, trajectories):\n",
    "        \"\"\"Update agent using PPO with domain randomization data.\"\"\"\n",
    "        if not trajectories:\n",
    "            return None\n",
    "        \n",
    "        all_obs, all_actions, all_rewards, all_log_probs, all_values = [], [], [], [], []\n",
    "        environment_params = []\n",
    "        \n",
    "        for trajectory in trajectories:\n",
    "            obs, actions, rewards, log_probs, values, env_params = zip(*trajectory)\n",
    "            all_obs.extend(obs)\n",
    "            all_actions.extend(actions)\n",
    "            all_rewards.extend(rewards)\n",
    "            all_log_probs.extend(log_probs)\n",
    "            all_values.extend(values)\n",
    "            environment_params.extend(env_params)\n",
    "        \n",
    "        observations = torch.FloatTensor(all_obs).to(device)\n",
    "        actions = torch.LongTensor(all_actions).to(device)\n",
    "        old_log_probs = torch.FloatTensor(all_log_probs).to(device)\n",
    "        values = torch.FloatTensor(all_values).to(device)\n",
    "        \n",
    "        returns = self.compute_returns(trajectories)\n",
    "        advantages = returns - values\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        for _ in range(4):  # Multiple epochs\n",
    "            action_probs = self.policy_network(observations)\n",
    "            action_dist = Categorical(action_probs)\\n            new_log_probs = action_dist.log_prob(actions)\n",
    "            \n",
    "            ratio = torch.exp(new_log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantages\n",
    "            surr2 = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio) * advantages\n",
    "            \n",
    "            policy_loss = -torch.min(surr1, surr2).mean()\n",
    "            \n",
    "            self.policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=0.5)\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            new_values = self.value_network(observations).squeeze()\n",
    "            value_loss = F.mse_loss(new_values, returns)\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.value_network.parameters(), max_norm=0.5)\n",
    "            self.value_optimizer.step()\n",
    "        \n",
    "        self.policy_losses.append(policy_loss.item())\n",
    "        self.value_losses.append(value_loss.item())\n",
    "        \n",
    "        unique_sizes = len(set([params['environment_size'] for params in environment_params]))\n",
    "        avg_noise = np.mean([params['noise_level'] for params in environment_params])\n",
    "        self.environment_diversity.append({'unique_sizes': unique_sizes, 'avg_noise': avg_noise})\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'environment_diversity': unique_sizes\n",
    "        }\n",
    "    \n",
    "    def compute_returns(self, trajectories):\n",
    "        \"\"\"Compute returns for all trajectories.\"\"\"\n",
    "        all_returns = []\n",
    "        \n",
    "        for trajectory in trajectories:\n",
    "            rewards = [step[2] for step in trajectory]\n",
    "            returns = []\n",
    "            G = 0\n",
    "            \n",
    "            for reward in reversed(rewards):\n",
    "                G = reward + self.gamma * G\n",
    "                returns.insert(0, G)\n",
    "            \n",
    "            all_returns.extend(returns)\n",
    "        \n",
    "        return torch.FloatTensor(all_returns).to(device)\n",
    "\n",
    "class AdversarialRobustAgent:\n",
    "    \"\"\"RL agent trained with adversarial perturbations.\"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim, action_dim, lr=3e-4, adversarial_strength=0.1):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.adversarial_strength = adversarial_strength\n",
    "        \n",
    "        self.policy_network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.value_network = nn.Sequential(\n",
    "            nn.Linear(obs_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.policy_optimizer = optim.Adam(self.policy_network.parameters(), lr=lr)\n",
    "        self.value_optimizer = optim.Adam(self.value_network.parameters(), lr=lr)\n",
    "        \n",
    "        self.gamma = 0.99\n",
    "        \n",
    "        self.robust_losses = []\n",
    "        self.adversarial_losses = []\n",
    "        self.perturbation_norms = []\n",
    "    \n",
    "    def generate_adversarial_observation(self, observation):\n",
    "        \"\"\"Generate adversarial perturbation using FGSM.\"\"\"\n",
    "        obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(device)\n",
    "        obs_tensor.requires_grad_(True)\n",
    "        \n",
    "        action_probs = self.policy_network(obs_tensor)\n",
    "        \n",
    "        entropy_loss = -(action_probs * torch.log(action_probs + 1e-8)).sum()\n",
    "        \n",
    "        entropy_loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            gradient = obs_tensor.grad.data\n",
    "            perturbation = self.adversarial_strength * torch.sign(gradient)\n",
    "            adversarial_obs = obs_tensor + perturbation\n",
    "            \n",
    "            adversarial_obs = torch.clamp(adversarial_obs, -2.0, 2.0)\n",
    "            \n",
    "            self.perturbation_norms.append(torch.norm(perturbation).item())\n",
    "        \n",
    "        return adversarial_obs.squeeze().cpu().numpy()\n",
    "    \n",
    "    def get_action(self, observation, use_adversarial=True):\n",
    "        \"\"\"Get action with optional adversarial robustness.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            obs_tensor = torch.FloatTensor(observation).unsqueeze(0).to(device)\n",
    "            action_probs = self.policy_network(obs_tensor)\n",
    "            action_dist = Categorical(action_probs)\n",
    "            action = action_dist.sample()\n",
    "            log_prob = action_dist.log_prob(action)\n",
    "            value = self.value_network(obs_tensor)\n",
    "        \n",
    "        original_action = action.item()\n",
    "        \n",
    "        if use_adversarial:\n",
    "            adversarial_obs = self.generate_adversarial_observation(observation)\n",
    "            with torch.no_grad():\n",
    "                adv_obs_tensor = torch.FloatTensor(adversarial_obs).unsqueeze(0).to(device)\n",
    "                adv_action_probs = self.policy_network(adv_obs_tensor)\n",
    "                adv_action_dist = Categorical(adv_action_probs)\n",
    "                adv_action = adv_action_dist.sample()\n",
    "            \n",
    "            return original_action, log_prob.item(), value.item()\n",
    "        \n",
    "        return original_action, log_prob.item(), value.item()\n",
    "    \n",
    "    def update(self, trajectories):\n",
    "        \"\"\"Update with adversarial training.\"\"\"\n",
    "        if not trajectories:\n",
    "            return None\n",
    "        \n",
    "        all_obs, all_actions, all_rewards, all_log_probs, all_values = [], [], [], [], []\n",
    "        \n",
    "        for trajectory in trajectories:\n",
    "            obs, actions, rewards, log_probs, values, _ = zip(*trajectory)\n",
    "            all_obs.extend(obs)\n",
    "            all_actions.extend(actions)\n",
    "            all_rewards.extend(rewards)\n",
    "            all_log_probs.extend(log_probs)\n",
    "            all_values.extend(values)\n",
    "        \n",
    "        observations = torch.FloatTensor(all_obs).to(device)\n",
    "        actions = torch.LongTensor(all_actions).to(device)\n",
    "        \n",
    "        all_returns = []\n",
    "        for trajectory in trajectories:\n",
    "            rewards = [step[2] for step in trajectory]\n",
    "            returns = []\n",
    "            G = 0\n",
    "            for reward in reversed(rewards):\n",
    "                G = reward + self.gamma * G\n",
    "                returns.insert(0, G)\n",
    "            all_returns.extend(returns)\n",
    "        \n",
    "        returns = torch.FloatTensor(all_returns).to(device)\n",
    "        values = torch.FloatTensor(all_values).to(device)\n",
    "        advantages = returns - values\n",
    "        \n",
    "        action_probs = self.policy_network(observations)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        log_probs = action_dist.log_prob(actions)\n",
    "        \n",
    "        policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        \n",
    "        adversarial_loss = 0\n",
    "        for i in range(min(32, len(observations))):  # Subsample for efficiency\n",
    "            obs = observations[i]\n",
    "            \n",
    "            obs_adv = obs.clone().detach()\n",
    "            obs_adv.requires_grad_(True)\n",
    "            \n",
    "            action_probs_adv = self.policy_network(obs_adv.unsqueeze(0))\n",
    "            entropy = -(action_probs_adv * torch.log(action_probs_adv + 1e-8)).sum()\n",
    "            \n",
    "            grad = torch.autograd.grad(entropy, obs_adv, create_graph=True)[0]\n",
    "            perturbation = self.adversarial_strength * torch.sign(grad)\n",
    "            obs_adversarial = obs + perturbation\n",
    "            \n",
    "            action_probs_original = self.policy_network(obs.unsqueeze(0))\n",
    "            action_probs_adversarial = self.policy_network(obs_adversarial.unsqueeze(0))\n",
    "            \n",
    "            kl_loss = F.kl_div(\n",
    "                torch.log(action_probs_adversarial + 1e-8),\n",
    "                action_probs_original,\n",
    "                reduction='batchmean'\n",
    "            )\n",
    "            adversarial_loss += kl_loss\n",
    "        \n",
    "        adversarial_loss /= min(32, len(observations))\n",
    "        \n",
    "        total_policy_loss = policy_loss + 0.1 * adversarial_loss\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        total_policy_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_network.parameters(), max_norm=0.5)\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.value_network.parameters(), max_norm=0.5)\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        self.robust_losses.append(total_policy_loss.item())\n",
    "        self.adversarial_losses.append(adversarial_loss.item())\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'adversarial_loss': adversarial_loss.item(),\n",
    "            'total_loss': total_policy_loss.item()\n",
    "        }\n",
    "\n",
    "def collect_robust_trajectory(env, agent, max_steps=100):\n",
    "    \"\"\"Collect trajectory with environment parameter tracking.\"\"\"\n",
    "    trajectory = []\n",
    "    observation = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action, log_prob, value = agent.get_action(observation)\n",
    "        next_observation, reward, done, info = env.step(action)\n",
    "        \n",
    "        trajectory.append((\n",
    "            observation.copy(), action, reward, log_prob, value, info.copy()\n",
    "        ))\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        observation = next_observation\n",
    "    \n",
    "    return trajectory\n",
    "\n",
    "def demonstrate_robust_rl():\n",
    "    \"\"\"Demonstrate Robust RL techniques.\"\"\"\n",
    "    print(\"ðŸ›¡ï¸ Demonstrating Robust Reinforcement Learning\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    environments = {\n",
    "        'low_uncertainty': RobustEnvironment(base_size=6, uncertainty_level=0.1),\n",
    "        'medium_uncertainty': RobustEnvironment(base_size=6, uncertainty_level=0.3),\n",
    "        'high_uncertainty': RobustEnvironment(base_size=6, uncertainty_level=0.5)\n",
    "    }\n",
    "    \n",
    "    obs_dim = 6\n",
    "    action_dim = 4\n",
    "    \n",
    "    agents = {\n",
    "        'Domain_Randomization': DomainRandomizationAgent(obs_dim, action_dim),\n",
    "        'Adversarial_Training': AdversarialRobustAgent(obs_dim, action_dim, adversarial_strength=0.1)\n",
    "    }\n",
    "    \n",
    "    results = {name: {\n",
    "        'rewards': {env_name: [] for env_name in environments.keys()},\n",
    "        'robustness_score': [],\n",
    "        'adaptation_rate': []\n",
    "    } for name in agents.keys()}\n",
    "    \n",
    "    num_episodes = 400\n",
    "    trajectories_per_update = 5\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        for agent_name, agent in agents.items():\n",
    "            all_trajectories = []\n",
    "            episode_rewards = {env_name: [] for env_name in environments.keys()}\n",
    "            \n",
    "            for env_name, env in environments.items():\n",
    "                env_trajectories = []\n",
    "                \n",
    "                for _ in range(trajectories_per_update):\n",
    "                    trajectory = collect_robust_trajectory(env, agent)\n",
    "                    env_trajectories.append(trajectory)\n",
    "                    \n",
    "                    episode_reward = sum(step[2] for step in trajectory)\n",
    "                    episode_rewards[env_name].append(episode_reward)\n",
    "                \n",
    "                all_trajectories.extend(env_trajectories)\n",
    "            \n",
    "            if all_trajectories:\n",
    "                update_info = agent.update(all_trajectories)\n",
    "            \n",
    "            for env_name in environments.keys():\n",
    "                results[agent_name]['rewards'][env_name].extend(episode_rewards[env_name])\n",
    "            \n",
    "            if episode_rewards['low_uncertainty'] and episode_rewards['high_uncertainty']:\n",
    "                low_perf = np.mean(episode_rewards['low_uncertainty'])\n",
    "                high_perf = np.mean(episode_rewards['high_uncertainty'])\n",
    "                robustness_score = high_perf / (low_perf + 1e-8)  # Performance retention\n",
    "                results[agent_name]['robustness_score'].append(robustness_score)\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            print(f\"\\nEpisode {episode}:\")\n",
    "            for agent_name in agents.keys():\n",
    "                if results[agent_name]['robustness_score']:\n",
    "                    recent_robustness = np.mean(results[agent_name]['robustness_score'][-10:])\n",
    "                    low_perf = np.mean(results[agent_name]['rewards']['low_uncertainty'][-20:])\n",
    "                    high_perf = np.mean(results[agent_name]['rewards']['high_uncertainty'][-20:])\n",
    "                    print(f\"  {agent_name}:\")\n",
    "                    print(f\"    Low Uncertainty: {low_perf:.2f}\")\n",
    "                    print(f\"    High Uncertainty: {high_perf:.2f}\")\n",
    "                    print(f\"    Robustness Score: {recent_robustness:.3f}\")\n",
    "    \n",
    "    return results, agents, environments\n",
    "\n",
    "print(\"ðŸš€ Starting Robust RL Training...\")\n",
    "robust_results, robust_agents, robust_environments = demonstrate_robust_rl()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "uncertainty_levels = ['low_uncertainty', 'medium_uncertainty', 'high_uncertainty']\n",
    "colors = ['green', 'orange', 'red']\n",
    "\n",
    "for i, agent_name in enumerate(robust_agents.keys()):\n",
    "    for j, (env_name, color) in enumerate(zip(uncertainty_levels, colors)):\n",
    "        window_size = 20\n",
    "        rewards = robust_results[agent_name]['rewards'][env_name]\n",
    "        if len(rewards) >= window_size:\n",
    "            smoothed_rewards = pd.Series(rewards).rolling(window_size).mean()\n",
    "            axes[i, 0].plot(smoothed_rewards, label=f'{env_name.replace(\"_\", \" \").title()}', \n",
    "                          color=color, linewidth=2)\n",
    "    \n",
    "    axes[i, 0].set_title(f'{agent_name} - Performance vs Uncertainty')\n",
    "    axes[i, 0].set_xlabel('Episode')\n",
    "    axes[i, 0].set_ylabel('Episode Reward (Smoothed)')\n",
    "    axes[i, 0].legend()\n",
    "    axes[i, 0].grid(True, alpha=0.3)\n",
    "\n",
    "for agent_name in robust_agents.keys():\n",
    "    window_size = 10\n",
    "    robustness_scores = robust_results[agent_name]['robustness_score']\n",
    "    if len(robustness_scores) >= window_size:\n",
    "        smoothed_robustness = pd.Series(robustness_scores).rolling(window_size).mean()\n",
    "        axes[0, 1].plot(smoothed_robustness, label=agent_name, linewidth=2)\n",
    "\n",
    "axes[0, 1].set_title('Robustness Score Over Time')\n",
    "axes[0, 1].set_xlabel('Episode')\n",
    "axes[0, 1].set_ylabel('Robustness Score (High/Low Performance)')\n",
    "axes[0, 1].axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Perfect Robustness')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "final_performance = {}\n",
    "for agent_name in robust_agents.keys():\n",
    "    final_low = np.mean(robust_results[agent_name]['rewards']['low_uncertainty'][-50:])\n",
    "    final_high = np.mean(robust_results[agent_name]['rewards']['high_uncertainty'][-50:])\n",
    "    final_performance[agent_name] = {'low': final_low, 'high': final_high}\n",
    "\n",
    "agents = list(final_performance.keys())\n",
    "x = np.arange(len(agents))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 1].bar(x - width/2, [final_performance[agent]['low'] for agent in agents], \n",
    "                       width, label='Low Uncertainty', alpha=0.8, color='green')\n",
    "bars2 = axes[1, 1].bar(x + width/2, [final_performance[agent]['high'] for agent in agents], \n",
    "                       width, label='High Uncertainty', alpha=0.8, color='red')\n",
    "\n",
    "axes[1, 1].set_title('Final Performance Comparison')\n",
    "axes[1, 1].set_ylabel('Average Reward (Last 50 Episodes)')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels([name.replace('_', '\\n') for name in agents])\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Robust RL Results Summary:\")\n",
    "print(\"=\" * 60)\n",
    "for agent_name in robust_agents.keys():\n",
    "    print(f\"\\n{agent_name}:\")\n",
    "    \n",
    "    low_perf = final_performance[agent_name]['low']\n",
    "    high_perf = final_performance[agent_name]['high']\n",
    "    performance_drop = (low_perf - high_perf) / low_perf * 100\n",
    "    \n",
    "    print(f\"  Low Uncertainty Performance: {low_perf:.2f}\")\n",
    "    print(f\"  High Uncertainty Performance: {high_perf:.2f}\")\n",
    "    print(f\"  Performance Drop: {performance_drop:.1f}%\")\n",
    "    print(f\"  Robustness Score: {high_perf/low_perf:.3f}\")\n",
    "\n",
    "if 'Adversarial_Training' in robust_agents:\n",
    "    adv_agent = robust_agents['Adversarial_Training']\n",
    "    if hasattr(adv_agent, 'perturbation_norms') and adv_agent.perturbation_norms:\n",
    "        avg_perturbation = np.mean(adv_agent.perturbation_norms[-100:])\n",
    "        print(f\"\\nAdversarial Training Statistics:\")\n",
    "        print(f\"  Average Perturbation Norm: {avg_perturbation:.4f}\")\n",
    "        print(f\"  Adversarial Strength: {adv_agent.adversarial_strength:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"  â€¢ Domain randomization improves generalization across environments\")\n",
    "print(\"  â€¢ Adversarial training enhances robustness to input perturbations\")\n",
    "print(\"  â€¢ Robustness often comes at the cost of peak performance\")\n",
    "print(\"  â€¢ Uncertainty quantification helps assess model confidence\")\n",
    "print(\"  â€¢ Real-world deployment requires robust policies\")\n",
    "\n",
    "print(\"\\nðŸ›¡ï¸ Robust RL demonstration completed!\")\n",
    "print(\"ðŸŽ¯ Advanced Deep RL CA14 implementation finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95638aa8",
   "metadata": {},
   "source": [
    "# Section 5: Comprehensive Evaluation and Real-world Applications\n",
    "\n",
    "This final section provides a comprehensive evaluation framework for comparing all advanced Deep RL methods and discusses real-world deployment considerations.\n",
    "\n",
    "## 5.1 Comprehensive Evaluation Framework\n",
    "\n",
    "Advanced Deep RL methods must be evaluated across multiple dimensions to understand their practical utility:\n",
    "\n",
    "### Performance Metrics\n",
    "- **Sample Efficiency**: How quickly algorithms learn from data\n",
    "- **Asymptotic Performance**: Final performance after convergence\n",
    "- **Robustness**: Performance under distribution shift and uncertainty\n",
    "- **Safety**: Constraint satisfaction and risk mitigation\n",
    "- **Scalability**: Performance in large-scale multi-agent settings\n",
    "\n",
    "### Evaluation Dimensions\n",
    "1. **Data Efficiency**: Offline vs. online learning requirements\n",
    "2. **Safety Constraints**: Hard vs. soft constraint satisfaction\n",
    "3. **Multi-Agent Coordination**: Centralized vs. decentralized approaches\n",
    "4. **Robustness**: Uncertainty handling and domain transfer\n",
    "5. **Computational Requirements**: Training and inference costs\n",
    "\n",
    "## 5.2 Real-world Deployment Considerations\n",
    "\n",
    "### Critical Factors for Practical Applications\n",
    "1. **Safety First**: Hard safety constraints in critical systems\n",
    "2. **Data Availability**: Leveraging existing datasets vs. online exploration\n",
    "3. **Coordination Requirements**: Multi-agent collaboration and competition\n",
    "4. **Environment Uncertainty**: Handling model mismatch and distribution shift\n",
    "5. **Regulatory Compliance**: Meeting industry standards and regulations\n",
    "\n",
    "### Application Domains\n",
    "- **Autonomous Vehicles**: Safe navigation with offline learning from driving data\n",
    "- **Financial Trading**: Multi-agent market interactions with risk constraints\n",
    "- **Healthcare**: Safe treatment optimization with limited data\n",
    "- **Robotics**: Robust manipulation under environmental uncertainty\n",
    "- **Energy Management**: Multi-agent coordination in smart grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8bfc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ComprehensiveEvaluator:\n",
    "    \"\"\"Framework for evaluating advanced Deep RL methods across multiple dimensions.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.evaluation_metrics = {\n",
    "            'sample_efficiency': [],\n",
    "            'asymptotic_performance': [],\n",
    "            'robustness_score': [],\n",
    "            'safety_violations': [],\n",
    "            'coordination_effectiveness': [],\n",
    "            'computational_cost': []\n",
    "        }\n",
    "        \n",
    "        self.method_results = {}\n",
    "    \n",
    "    def evaluate_sample_efficiency(self, training_curves, convergence_threshold=0.8):\n",
    "        \"\"\"Evaluate how quickly methods reach target performance.\"\"\"\n",
    "        efficiency_scores = {}\n",
    "        \n",
    "        for method_name, rewards in training_curves.items():\n",
    "            if not rewards:\n",
    "                efficiency_scores[method_name] = float('inf')\n",
    "                continue\n",
    "            \n",
    "            max_reward = max(rewards)\n",
    "            target_reward = convergence_threshold * max_reward\n",
    "            \n",
    "            convergence_episode = len(rewards)  # Default to end\n",
    "            for i, reward in enumerate(rewards):\n",
    "                if reward >= target_reward:\n",
    "                    convergence_episode = i\n",
    "                    break\n",
    "            \n",
    "            efficiency_scores[method_name] = convergence_episode\n",
    "        \n",
    "        return efficiency_scores\n",
    "    \n",
    "    def evaluate_asymptotic_performance(self, training_curves, final_episodes=50):\n",
    "        \"\"\"Evaluate final performance after convergence.\"\"\"\n",
    "        asymptotic_scores = {}\n",
    "        \n",
    "        for method_name, rewards in training_curves.items():\n",
    "            if len(rewards) >= final_episodes:\n",
    "                asymptotic_scores[method_name] = np.mean(rewards[-final_episodes:])\n",
    "            else:\n",
    "                asymptotic_scores[method_name] = np.mean(rewards) if rewards else 0.0\n",
    "        \n",
    "        return asymptotic_scores\n",
    "    \n",
    "    def evaluate_robustness(self, agents, test_environments, num_episodes=50):\n",
    "        \"\"\"Evaluate robustness across different environments.\"\"\"\n",
    "        robustness_scores = {}\n",
    "        \n",
    "        for agent_name, agent in agents.items():\n",
    "            environment_performances = []\n",
    "            \n",
    "            for env_name, env in test_environments.items():\n",
    "                episode_rewards = []\n",
    "                \n",
    "                for episode in range(num_episodes):\n",
    "                    obs = env.reset()\n",
    "                    total_reward = 0\n",
    "                    done = False\n",
    "                    \n",
    "                    while not done:\n",
    "                        if hasattr(agent, 'get_action'):\n",
    "                            if len(inspect.signature(agent.get_action).parameters) > 1:\n",
    "                                action, _, _ = agent.get_action(obs)\n",
    "                            else:\n",
    "                                action = agent.get_action(obs)\n",
    "                        else:\n",
    "                            action = np.random.randint(env.action_space)\n",
    "                        \n",
    "                        obs, reward, done, _ = env.step(action)\n",
    "                        total_reward += reward\n",
    "                    \n",
    "                    episode_rewards.append(total_reward)\n",
    "                \n",
    "                environment_performances.append(np.mean(episode_rewards))\n",
    "            \n",
    "            if environment_performances:\n",
    "                min_perf = min(environment_performances)\n",
    "                max_perf = max(environment_performances)\n",
    "                robustness_scores[agent_name] = min_perf / max_perf if max_perf > 0 else 0.0\n",
    "            else:\n",
    "                robustness_scores[agent_name] = 0.0\n",
    "        \n",
    "        return robustness_scores\n",
    "    \n",
    "    def evaluate_safety(self, agents, safe_environment, num_episodes=100):\n",
    "        \"\"\"Evaluate safety constraint satisfaction.\"\"\"\n",
    "        safety_scores = {}\n",
    "        \n",
    "        for agent_name, agent in agents.items():\n",
    "            violations = 0\n",
    "            total_steps = 0\n",
    "            \n",
    "            for episode in range(num_episodes):\n",
    "                obs = safe_environment.reset()\n",
    "                done = False\n",
    "                \n",
    "                while not done:\n",
    "                    if hasattr(agent, 'get_action'):\n",
    "                        if len(inspect.signature(agent.get_action).parameters) > 1:\n",
    "                            action, _, _ = agent.get_action(obs)\n",
    "                        else:\n",
    "                            action = agent.get_action(obs)\n",
    "                    else:\n",
    "                        action = np.random.randint(safe_environment.action_space)\n",
    "                    \n",
    "                    obs, reward, done, info = safe_environment.step(action)\n",
    "                    total_steps += 1\n",
    "                    \n",
    "                    if hasattr(safe_environment, 'constraint_violation'):\n",
    "                        if safe_environment.constraint_violation:\n",
    "                            violations += 1\n",
    "                    elif 'constraint_violation' in info:\n",
    "                        if info['constraint_violation']:\n",
    "                            violations += 1\n",
    "                    elif reward < -1.0:  # Assume large negative reward indicates violation\n",
    "                        violations += 1\n",
    "            \n",
    "            safety_scores[agent_name] = violations / total_steps if total_steps > 0 else 1.0\n",
    "        \n",
    "        return safety_scores\n",
    "    \n",
    "    def evaluate_coordination(self, multi_agent_results):\n",
    "        \"\"\"Evaluate multi-agent coordination effectiveness.\"\"\"\n",
    "        coordination_scores = {}\n",
    "        \n",
    "        for method_name, results in multi_agent_results.items():\n",
    "            if 'coordination_rewards' in results:\n",
    "                individual_perf = results.get('individual_performance', 0)\n",
    "                coordinated_perf = np.mean(results['coordination_rewards'][-50:])\n",
    "                \n",
    "                coordination_scores[method_name] = coordinated_perf - individual_perf\n",
    "            else:\n",
    "                coordination_scores[method_name] = 0.0\n",
    "        \n",
    "        return coordination_scores\n",
    "    \n",
    "    def compute_comprehensive_score(self, method_results):\n",
    "        \"\"\"Compute overall score combining all metrics.\"\"\"\n",
    "        comprehensive_scores = {}\n",
    "        \n",
    "        metrics = ['sample_efficiency', 'asymptotic_performance', 'robustness_score', \n",
    "                  'safety_score', 'coordination_effectiveness']\n",
    "        \n",
    "        normalized_scores = {}\n",
    "        for metric in metrics:\n",
    "            if metric in method_results:\n",
    "                values = list(method_results[metric].values())\n",
    "                if values:\n",
    "                    if metric == 'sample_efficiency':  # Lower is better\n",
    "                        min_val, max_val = min(values), max(values)\n",
    "                        normalized_scores[metric] = {\n",
    "                            method: 1 - (score - min_val) / (max_val - min_val) if max_val > min_val else 1.0\n",
    "                            for method, score in method_results[metric].items()\n",
    "                        }\n",
    "                    elif metric == 'safety_score':  # Lower is better (fewer violations)\n",
    "                        min_val, max_val = min(values), max(values)\n",
    "                        normalized_scores[metric] = {\n",
    "                            method: 1 - (score - min_val) / (max_val - min_val) if max_val > min_val else 1.0\n",
    "                            for method, score in method_results[metric].items()\n",
    "                        }\n",
    "                    else:  # Higher is better\n",
    "                        min_val, max_val = min(values), max(values)\n",
    "                        normalized_scores[metric] = {\n",
    "                            method: (score - min_val) / (max_val - min_val) if max_val > min_val else 1.0\n",
    "                            for method, score in method_results[metric].items()\n",
    "                        }\n",
    "        \n",
    "        weights = {\n",
    "            'sample_efficiency': 0.2,\n",
    "            'asymptotic_performance': 0.25,\n",
    "            'robustness_score': 0.25,\n",
    "            'safety_score': 0.2,\n",
    "            'coordination_effectiveness': 0.1\n",
    "        }\n",
    "        \n",
    "        methods = set()\n",
    "        for metric_scores in normalized_scores.values():\n",
    "            methods.update(metric_scores.keys())\n",
    "        \n",
    "        for method in methods:\n",
    "            score = 0\n",
    "            weight_sum = 0\n",
    "            \n",
    "            for metric, weight in weights.items():\n",
    "                if metric in normalized_scores and method in normalized_scores[metric]:\n",
    "                    score += weight * normalized_scores[metric][method]\n",
    "                    weight_sum += weight\n",
    "            \n",
    "            comprehensive_scores[method] = score / weight_sum if weight_sum > 0 else 0.0\n",
    "        \n",
    "        return comprehensive_scores, normalized_scores\n",
    "\n",
    "def create_evaluation_environments():\n",
    "    \"\"\"Create diverse test environments for robustness evaluation.\"\"\"\n",
    "    environments = {\n",
    "        'standard': RobustEnvironment(base_size=6, uncertainty_level=0.0),\n",
    "        'noisy': RobustEnvironment(base_size=6, uncertainty_level=0.2),\n",
    "        'large': RobustEnvironment(base_size=8, uncertainty_level=0.1),\n",
    "        'obstacles': RobustEnvironment(base_size=6, uncertainty_level=0.1, dynamic_obstacles=True)\n",
    "    }\n",
    "    return environments\n",
    "\n",
    "def run_comprehensive_evaluation():\n",
    "    \"\"\"Run comprehensive evaluation of all advanced RL methods.\"\"\"\n",
    "    print(\"ðŸ” Starting Comprehensive Evaluation of Advanced Deep RL Methods\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    evaluator = ComprehensiveEvaluator()\n",
    "    \n",
    "    test_environments = create_evaluation_environments()\n",
    "    \n",
    "    all_agents = {}\n",
    "    \n",
    "    if 'offline_agents' in globals():\n",
    "        for name, agent in offline_agents.items():\n",
    "            all_agents[f'Offline_{name}'] = agent\n",
    "    \n",
    "    if 'safe_agents' in globals():\n",
    "        for name, agent in safe_agents.items():\n",
    "            all_agents[f'Safe_{name}'] = agent\n",
    "    \n",
    "    if 'ma_agents' in globals():\n",
    "        for name, agent_list in ma_agents.items():\n",
    "            if isinstance(agent_list, list) and len(agent_list) > 0:\n",
    "                all_agents[f'MultiAgent_{name}'] = agent_list[0]\n",
    "            else:\n",
    "                all_agents[f'MultiAgent_{name}'] = agent_list\n",
    "    \n",
    "    if 'robust_agents' in globals():\n",
    "        for name, agent in robust_agents.items():\n",
    "            all_agents[f'Robust_{name}'] = agent\n",
    "    \n",
    "    training_curves = {}\n",
    "    \n",
    "    if 'offline_results' in globals():\n",
    "        for name in offline_results.keys():\n",
    "            if 'episode_rewards' in offline_results[name]:\n",
    "                training_curves[f'Offline_{name}'] = offline_results[name]['episode_rewards']\n",
    "    \n",
    "    if 'safe_results' in globals():\n",
    "        for name in safe_results.keys():\n",
    "            if 'rewards' in safe_results[name]:\n",
    "                training_curves[f'Safe_{name}'] = safe_results[name]['rewards']\n",
    "    \n",
    "    if 'ma_results' in globals():\n",
    "        for name in ma_results.keys():\n",
    "            if 'episode_rewards' in ma_results[name]:\n",
    "                training_curves[f'MultiAgent_{name}'] = ma_results[name]['episode_rewards']\n",
    "    \n",
    "    if 'robust_results' in globals():\n",
    "        for name in robust_results.keys():\n",
    "            if 'rewards' in robust_results[name] and 'low_uncertainty' in robust_results[name]['rewards']:\n",
    "                training_curves[f'Robust_{name}'] = robust_results[name]['rewards']['low_uncertainty']\n",
    "    \n",
    "    print(f\"ðŸ“Š Evaluating {len(all_agents)} methods across {len(test_environments)} environments\")\n",
    "    \n",
    "    evaluation_results = {}\n",
    "    \n",
    "    print(\"âš¡ Evaluating sample efficiency...\")\n",
    "    efficiency_scores = evaluator.evaluate_sample_efficiency(training_curves)\n",
    "    evaluation_results['sample_efficiency'] = efficiency_scores\n",
    "    \n",
    "    print(\"ðŸŽ¯ Evaluating asymptotic performance...\")\n",
    "    asymptotic_scores = evaluator.evaluate_asymptotic_performance(training_curves)\n",
    "    evaluation_results['asymptotic_performance'] = asymptotic_scores\n",
    "    \n",
    "    print(\"ðŸ›¡ï¸ Evaluating robustness...\")\n",
    "    robustness_scores = evaluator.evaluate_robustness(all_agents, test_environments)\n",
    "    evaluation_results['robustness_score'] = robustness_scores\n",
    "    \n",
    "    print(\"ðŸš¨ Evaluating safety...\")\n",
    "    if 'safe_envs' in globals():\n",
    "        safe_env = list(safe_envs.values())[0] if safe_envs else test_environments['standard']\n",
    "    else:\n",
    "        safe_env = test_environments['standard']\n",
    "    safety_scores = evaluator.evaluate_safety(all_agents, safe_env)\n",
    "    evaluation_results['safety_score'] = safety_scores\n",
    "    \n",
    "    print(\"ðŸ¤ Evaluating coordination...\")\n",
    "    coordination_scores = {}\n",
    "    if 'ma_results' in globals():\n",
    "        coordination_scores = evaluator.evaluate_coordination(ma_results)\n",
    "    evaluation_results['coordination_effectiveness'] = coordination_scores\n",
    "    \n",
    "    print(\"ðŸ“ˆ Computing comprehensive scores...\")\n",
    "    comprehensive_scores, normalized_scores = evaluator.compute_comprehensive_score(evaluation_results)\n",
    "    \n",
    "    return evaluation_results, comprehensive_scores, normalized_scores\n",
    "\n",
    "print(\"ðŸš€ Starting Comprehensive Evaluation...\")\n",
    "eval_results, comprehensive_scores, normalized_scores = run_comprehensive_evaluation()\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "if 'sample_efficiency' in eval_results and eval_results['sample_efficiency']:\n",
    "    methods = list(eval_results['sample_efficiency'].keys())\n",
    "    scores = list(eval_results['sample_efficiency'].values())\n",
    "    bars = axes[0, 0].bar(range(len(methods)), scores, alpha=0.8, color='skyblue')\n",
    "    axes[0, 0].set_title('Sample Efficiency\\n(Episodes to Convergence)')\n",
    "    axes[0, 0].set_xlabel('Methods')\n",
    "    axes[0, 0].set_ylabel('Episodes')\n",
    "    axes[0, 0].set_xticks(range(len(methods)))\n",
    "    axes[0, 0].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "if 'asymptotic_performance' in eval_results and eval_results['asymptotic_performance']:\n",
    "    methods = list(eval_results['asymptotic_performance'].keys())\n",
    "    scores = list(eval_results['asymptotic_performance'].values())\n",
    "    bars = axes[0, 1].bar(range(len(methods)), scores, alpha=0.8, color='lightgreen')\n",
    "    axes[0, 1].set_title('Asymptotic Performance\\n(Final Reward)')\n",
    "    axes[0, 1].set_xlabel('Methods')\n",
    "    axes[0, 1].set_ylabel('Average Reward')\n",
    "    axes[0, 1].set_xticks(range(len(methods)))\n",
    "    axes[0, 1].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "if 'robustness_score' in eval_results and eval_results['robustness_score']:\n",
    "    methods = list(eval_results['robustness_score'].keys())\n",
    "    scores = list(eval_results['robustness_score'].values())\n",
    "    bars = axes[0, 2].bar(range(len(methods)), scores, alpha=0.8, color='orange')\n",
    "    axes[0, 2].set_title('Robustness Score\\n(Min/Max Performance)')\n",
    "    axes[0, 2].set_xlabel('Methods')\n",
    "    axes[0, 2].set_ylabel('Robustness Score')\n",
    "    axes[0, 2].set_xticks(range(len(methods)))\n",
    "    axes[0, 2].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    axes[0, 2].axhline(y=1.0, color='red', linestyle='--', alpha=0.5, label='Perfect Robustness')\n",
    "    axes[0, 2].legend()\n",
    "    axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "if 'safety_score' in eval_results and eval_results['safety_score']:\n",
    "    methods = list(eval_results['safety_score'].keys())\n",
    "    scores = [1 - score for score in eval_results['safety_score'].values()]  \n",
    "    bars = axes[1, 0].bar(range(len(methods)), scores, alpha=0.8, color='lightcoral')\n",
    "    axes[1, 0].set_title('Safety Score\\n(1 - Violation Rate)')\n",
    "    axes[1, 0].set_xlabel('Methods')\n",
    "    axes[1, 0].set_ylabel('Safety Score')\n",
    "    axes[1, 0].set_xticks(range(len(methods)))\n",
    "    axes[1, 0].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "if comprehensive_scores:\n",
    "    methods = list(comprehensive_scores.keys())\n",
    "    scores = list(comprehensive_scores.values())\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(methods)))\n",
    "    bars = axes[1, 1].bar(range(len(methods)), scores, alpha=0.8, color=colors)\n",
    "    axes[1, 1].set_title('Comprehensive Score\\n(Weighted Average)')\n",
    "    axes[1, 1].set_xlabel('Methods')\n",
    "    axes[1, 1].set_ylabel('Comprehensive Score')\n",
    "    axes[1, 1].set_xticks(range(len(methods)))\n",
    "    axes[1, 1].set_xticklabels([m.replace('_', '\\n') for m in methods], rotation=45, ha='right')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "if comprehensive_scores and len(comprehensive_scores) >= 3:\n",
    "    top_methods = sorted(comprehensive_scores.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "    \n",
    "    categories = ['Sample\\nEfficiency', 'Asymptotic\\nPerformance', 'Robustness', \n",
    "                 'Safety', 'Coordination']\n",
    "    N = len(categories)\n",
    "    \n",
    "    angles = [n / float(N) * 2 * np.pi for n in range(N)]\n",
    "    angles += angles[:1]  # Complete the circle\n",
    "    \n",
    "    ax_radar = plt.subplot(2, 3, 6, projection='polar')\n",
    "    colors = ['red', 'green', 'blue']\n",
    "    \n",
    "    for i, (method, _) in enumerate(top_methods):\n",
    "        values = []\n",
    "        for metric in ['sample_efficiency', 'asymptotic_performance', 'robustness_score', \n",
    "                      'safety_score', 'coordination_effectiveness']:\n",
    "            if metric in normalized_scores and method in normalized_scores[metric]:\n",
    "                values.append(normalized_scores[metric][method])\n",
    "            else:\n",
    "                values.append(0.0)\n",
    "        \n",
    "        values += values[:1]  # Complete the circle\n",
    "        \n",
    "        ax_radar.plot(angles, values, 'o-', linewidth=2, label=method.replace('_', ' '), color=colors[i])\n",
    "        ax_radar.fill(angles, values, alpha=0.25, color=colors[i])\n",
    "    \n",
    "    ax_radar.set_xticks(angles[:-1])\n",
    "    ax_radar.set_xticklabels(categories)\n",
    "    ax_radar.set_ylim(0, 1)\n",
    "    ax_radar.set_title('Top 3 Methods Comparison', y=1.08)\n",
    "    ax_radar.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))\n",
    "    ax_radar.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“Š Comprehensive Evaluation Results:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nðŸ† Top 3 Overall Methods:\")\n",
    "if comprehensive_scores:\n",
    "    sorted_methods = sorted(comprehensive_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    for i, (method, score) in enumerate(sorted_methods[:3]):\n",
    "        print(f\"  {i+1}. {method}: {score:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Detailed Metrics:\")\n",
    "for metric_name, metric_results in eval_results.items():\n",
    "    if metric_results:\n",
    "        print(f\"\\n{metric_name.replace('_', ' ').title()}:\")\n",
    "        sorted_results = sorted(metric_results.items(), key=lambda x: x[1], \n",
    "                              reverse=(metric_name not in ['sample_efficiency', 'safety_score']))\n",
    "        for method, score in sorted_results:\n",
    "            if metric_name == 'sample_efficiency':\n",
    "                print(f\"  {method}: {score:.0f} episodes\")\n",
    "            elif metric_name == 'safety_score':\n",
    "                print(f\"  {method}: {(1-score)*100:.1f}% safety rate\")\n",
    "            else:\n",
    "                print(f\"  {method}: {score:.3f}\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Key Insights:\")\n",
    "print(\"  â€¢ Offline RL excels in data efficiency but may lack adaptability\")\n",
    "print(\"  â€¢ Safe RL provides constraint satisfaction at performance cost\")\n",
    "print(\"  â€¢ Multi-agent RL enables coordination but increases complexity\")\n",
    "print(\"  â€¢ Robust RL handles uncertainty but requires more computation\")\n",
    "print(\"  â€¢ Real-world applications require careful method selection\")\n",
    "print(\"  â€¢ Hybrid approaches often provide best overall performance\")\n",
    "\n",
    "print(\"\\nðŸŒŸ Recommendations for Deployment:\")\n",
    "print(\"  â€¢ Safety-critical: Prioritize Safe RL methods\")\n",
    "print(\"  â€¢ Limited data: Use Offline RL with safety constraints\")\n",
    "print(\"  â€¢ Multi-agent settings: MADDPG for continuous, QMIX for discrete\")\n",
    "print(\"  â€¢ Uncertain environments: Domain randomization + adversarial training\")\n",
    "print(\"  â€¢ Production systems: Comprehensive evaluation before deployment\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Comprehensive evaluation completed!\")\n",
    "print(\"ðŸ“š Advanced Deep RL CA14 notebook fully implemented!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753945c",
   "metadata": {},
   "source": [
    "# Summary and Conclusions\n",
    "\n",
    "## Key Takeaways from Advanced Deep Rl\n",
    "\n",
    "This comprehensive exploration of advanced Deep Reinforcement Learning has covered the cutting-edge methods essential for real-world deployment:\n",
    "\n",
    "### ðŸŽ¯ Core Advanced Rl Paradigms\n",
    "\n",
    "1. **Offline Reinforcement Learning**\n",
    "- **Conservative Q-Learning (CQL)**: Addresses overestimation bias in offline settings\n",
    "- **Implicit Q-Learning (IQL)**: Avoids distributional shift through expectile regression\n",
    "- **Key Insight**: Essential for domains with existing data but limited online interaction\n",
    "\n",
    "2. **Safe Reinforcement Learning**\n",
    "- **Constrained Policy Optimization (CPO)**: Hard constraint satisfaction\n",
    "- **Lagrangian Methods**: Adaptive penalty balancing performance and safety\n",
    "- **Key Insight**: Critical for safety-critical applications where violations are unacceptable\n",
    "\n",
    "3. **Multi-Agent Reinforcement Learning**\n",
    "- **MADDPG**: Centralized training, decentralized execution for continuous control\n",
    "- **QMIX**: Value function factorization for discrete action coordination\n",
    "- **Key Insight**: Enables coordination in complex multi-agent environments\n",
    "\n",
    "4. **Robust Reinforcement Learning**\n",
    "- **Domain Randomization**: Training across diverse environment configurations\n",
    "- **Adversarial Training**: Robustness to input perturbations and model uncertainty\n",
    "- **Key Insight**: Essential for deployment in uncertain, dynamic real-world environments\n",
    "\n",
    "### ðŸŒŸ Practical Implementation Insights\n",
    "\n",
    "- **Hyperparameter Sensitivity**: Advanced methods often require careful tuning\n",
    "- **Computational Requirements**: Robust methods need more resources but provide better generalization\n",
    "- **Data Requirements**: Offline methods leverage existing data, online methods need exploration\n",
    "- **Safety Trade-offs**: Safe methods may sacrifice peak performance for constraint satisfaction\n",
    "- **Scalability Considerations**: Multi-agent methods face coordination complexity\n",
    "\n",
    "### ðŸš€ Real-world Applications\n",
    "\n",
    "Advanced Deep RL methods are revolutionizing multiple domains:\n",
    "\n",
    "- **Autonomous Systems**: Safe navigation with offline learning from human demonstrations\n",
    "- **Financial Markets**: Multi-agent trading with risk constraints and robustness\n",
    "- **Healthcare**: Safe treatment optimization with limited data and safety constraints\n",
    "- **Robotics**: Robust manipulation under environmental uncertainty\n",
    "- **Resource Management**: Multi-agent coordination in smart grids and logistics\n",
    "\n",
    "### ðŸ”¬ Future Directions\n",
    "\n",
    "The field continues evolving toward:\n",
    "\n",
    "1. **Hybrid Approaches**: Combining offline, safe, multi-agent, and robust techniques\n",
    "2. **Foundation Models**: Pre-trained RL models for downstream adaptation\n",
    "3. **Neurosymbolic RL**: Incorporating symbolic reasoning for interpretability\n",
    "4. **Continual Learning**: Adaptation without catastrophic forgetting\n",
    "5. **Human-AI Collaboration**: Interactive learning from human feedback\n",
    "\n",
    "### ðŸ“š Educational Impact\n",
    "\n",
    "This CA14 demonstrates that mastering advanced Deep RL requires:\n",
    "\n",
    "- **Theoretical Understanding**: Mathematical foundations of each paradigm\n",
    "- **Practical Implementation**: Hands-on experience with state-of-the-art algorithms\n",
    "- **Critical Evaluation**: Comprehensive assessment across multiple metrics\n",
    "- **Real-world Perspective**: Understanding deployment challenges and trade-offs\n",
    "\n",
    "### ðŸŽ–ï¸ Final Reflection\n",
    "\n",
    "Advanced Deep Reinforcement Learning represents the frontier of artificial intelligence, enabling autonomous agents to learn, adapt, and operate safely in complex, uncertain, and multi-agent environments. The methods explored in this comprehensive study provide the tools necessary for the next generation of intelligent systems that will transform industries and improve human lives.\n",
    "\n",
    "The journey from basic RL to these advanced paradigms illustrates the rapid evolution of the field and highlights the importance of continuous learning and adaptation in both our algorithms and our understanding of intelligence itself.\n",
    "\n",
    "---\n",
    "\n",
    "**ðŸŽ¯ Congratulations on completing this comprehensive exploration of Advanced Deep Reinforcement Learning!**\n",
    "\n",
    "*This marks the culmination of your journey through cutting-edge RL methods. The knowledge and skills gained here will serve as a foundation for tackling the most challenging problems in artificial intelligence and machine learning.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2680d0f7",
   "metadata": {},
   "source": [
    "# Code Review and Improvements\n",
    "\n",
    "## Advanced RL Implementation Analysis\n",
    "\n",
    "### Strengths of Current Implementation\n",
    "\n",
    "1. **Comprehensive Coverage**: Implementation of all major advanced RL paradigms (offline, safe, multi-agent, robust)\n",
    "2. **Modular Architecture**: Clean separation between different algorithm families with shared utilities\n",
    "3. **Educational Value**: Extensive documentation, theoretical explanations, and practical examples\n",
    "4. **Scalability**: Framework designed to handle complex, real-world RL challenges\n",
    "5. **Evaluation Rigor**: Comprehensive evaluation metrics and comparison frameworks\n",
    "\n",
    "### Areas for Improvement\n",
    "\n",
    "#### 1. Offline RL Enhancements\n",
    "- **Current Limitation**: Basic dataset generation and limited offline evaluation scenarios\n",
    "- **Improvement**: Implement advanced offline techniques:\n",
    "  - **Batch-Constrained Q-Learning (BCQ)**: Explicit action space constraints\n",
    "  - **Advantage-Weighted Regression (AWR)**: Better policy extraction from offline data\n",
    "  - **Dataset distillation**: Compress large datasets for efficient training\n",
    "  - **Offline-to-online transfer**: Seamless transition from offline to online learning\n",
    "\n",
    "#### 2. Safe RL Extensions\n",
    "- **Current Limitation**: Binary constraint satisfaction without risk measures\n",
    "- **Improvement**: Add sophisticated safety mechanisms:\n",
    "  - **Risk-sensitive MDPs**: CVaR and other risk measures beyond expectations\n",
    "  - **Barrier functions**: Mathematical guarantees of safety\n",
    "  - **Recovery policies**: Automatic constraint violation recovery\n",
    "  - **Multi-objective optimization**: Explicit safety-performance trade-offs\n",
    "\n",
    "#### 3. Multi-Agent RL Advancements\n",
    "- **Current Limitation**: Limited to cooperative scenarios with basic communication\n",
    "- **Improvement**: Extend to complex multi-agent settings:\n",
    "  - **Competitive MARL**: Zero-sum games and adversarial multi-agent scenarios\n",
    "  - **Mixed-motive games**: Cooperation with conflicting objectives\n",
    "  - **Communication protocols**: Learned communication and language emergence\n",
    "  - **Hierarchical MARL**: Multi-level coordination and task decomposition\n",
    "\n",
    "#### 4. Robust RL Sophistication\n",
    "- **Current Limitation**: Basic uncertainty estimation and adversarial training\n",
    "- **Improvement**: Implement advanced robustness techniques:\n",
    "  - **Meta-learning for robustness**: Learn-to-learn across diverse environments\n",
    "  - **Distributional shift detection**: Automatic detection of environment changes\n",
    "  - **Adaptive robustness**: Dynamic adjustment of robustness mechanisms\n",
    "  - **Certified robustness**: Formal guarantees against adversarial perturbations\n",
    "\n",
    "### Performance Optimization Suggestions\n",
    "\n",
    "#### Computational Efficiency\n",
    "```python\n",
    "# Mixed precision training for faster convergence\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "@torch.cuda.amp.autocast()\n",
    "def train_step(batch):\n",
    "    # Training logic here\n",
    "    return loss\n",
    "\n",
    "scaler.scale(loss).backward()\n",
    "scaler.step(optimizer)\n",
    "scaler.update()\n",
    "```\n",
    "\n",
    "#### Memory Optimization\n",
    "```python\n",
    "# Experience replay with prioritized sampling\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "        self.priorities = np.zeros(capacity, dtype=np.float32)\n",
    "        self.position = 0\n",
    "        \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        priorities = self.priorities[:self.size] ** self.alpha\n",
    "        probs = priorities / priorities.sum()\n",
    "        \n",
    "        indices = np.random.choice(self.size, batch_size, p=probs)\n",
    "        weights = (self.size * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        \n",
    "        return self.buffer[indices], indices, weights\n",
    "```\n",
    "\n",
    "#### Distributed Training\n",
    "```python\n",
    "# Multi-GPU training for large-scale experiments\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def setup_distributed(rank, world_size):\n",
    "    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "    torch.cuda.set_device(rank)\n",
    "\n",
    "def train_distributed(rank, world_size, model):\n",
    "    setup_distributed(rank, world_size)\n",
    "    model = nn.parallel.DistributedDataParallel(model)\n",
    "    # Training loop...\n",
    "```\n",
    "\n",
    "### Advanced Techniques to Explore\n",
    "\n",
    "#### 1. Meta-Reinforcement Learning\n",
    "```python\n",
    "class MetaRLAgent:\n",
    "    def __init__(self, inner_lr=0.1, meta_lr=0.001):\n",
    "        self.inner_lr = inner_lr\n",
    "        self.meta_lr = meta_lr\n",
    "        self.policy = PolicyNetwork()\n",
    "        self.meta_optimizer = optim.Adam(self.policy.parameters(), lr=meta_lr)\n",
    "    \n",
    "    def adapt_to_task(self, task_data, adaptation_steps=5):\n",
    "        \"\"\"Fast adaptation to new tasks\"\"\"\n",
    "        adapted_params = []\n",
    "        for step in range(adaptation_steps):\n",
    "            loss = self.compute_task_loss(task_data)\n",
    "            grads = torch.autograd.grad(loss, self.policy.parameters())\n",
    "            adapted_params = [p - self.inner_lr * g for p, g in zip(self.policy.parameters(), grads)]\n",
    "        return adapted_params\n",
    "```\n",
    "\n",
    "#### 2. Hierarchical Reinforcement Learning\n",
    "```python\n",
    "class HierarchicalAgent:\n",
    "    def __init__(self):\n",
    "        self.meta_controller = MetaController()  # High-level goals\n",
    "        self.controller = Controller()  # Low-level actions\n",
    "        self.intrinsic_motivation = IntrinsicMotivation()\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if self.should_replan():\n",
    "            goal = self.meta_controller.select_goal(state)\n",
    "            self.current_goal = goal\n",
    "        \n",
    "        action = self.controller.select_action(state, self.current_goal)\n",
    "        return action\n",
    "    \n",
    "    def should_replan(self):\n",
    "        # Check if current goal is still relevant\n",
    "        return np.random.random() < 0.1  # Stochastic replanning\n",
    "```\n",
    "\n",
    "#### 3. Multi-Task and Transfer Learning\n",
    "```python\n",
    "class MultiTaskAgent:\n",
    "    def __init__(self, num_tasks):\n",
    "        self.shared_encoder = SharedEncoder()\n",
    "        self.task_specific_heads = nn.ModuleList([\n",
    "            TaskSpecificHead() for _ in range(num_tasks)\n",
    "        ])\n",
    "        self.task_selector = TaskSelector()\n",
    "    \n",
    "    def forward(self, state, task_id):\n",
    "        shared_features = self.shared_encoder(state)\n",
    "        task_output = self.task_specific_heads[task_id](shared_features)\n",
    "        return task_output\n",
    "    \n",
    "    def transfer_knowledge(self, source_task, target_task):\n",
    "        # Transfer learned representations\n",
    "        source_params = list(self.task_specific_heads[source_task].parameters())\n",
    "        target_params = list(self.task_specific_heads[target_task].parameters())\n",
    "        \n",
    "        for source_p, target_p in zip(source_params, target_params):\n",
    "            target_p.data.copy_(source_p.data * 0.5)  # Partial transfer\n",
    "```\n",
    "\n",
    "### Best Practices for Production Deployment\n",
    "\n",
    "#### Monitoring and Logging\n",
    "```python\n",
    "import wandb\n",
    "import logging\n",
    "\n",
    "def setup_monitoring():\n",
    "    # Weights & Biases for experiment tracking\n",
    "    wandb.init(project=\"advanced-rl-deployment\")\n",
    "    \n",
    "    # Structured logging\n",
    "    logging.basicConfig(\n",
    "        filename='rl_agent.log',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s'\n",
    "    )\n",
    "\n",
    "def log_training_metrics(agent, episode_reward, episode_length, safety_violations):\n",
    "    wandb.log({\n",
    "        'episode_reward': episode_reward,\n",
    "        'episode_length': episode_length,\n",
    "        'safety_violations': safety_violations,\n",
    "        'policy_loss': agent.policy_loss,\n",
    "        'value_loss': agent.value_loss,\n",
    "        'gradient_norm': agent.grad_norm\n",
    "    })\n",
    "    \n",
    "    logging.info(f\"Episode completed: reward={episode_reward:.2f}, \"\n",
    "                f\"length={episode_length}, violations={safety_violations}\")\n",
    "```\n",
    "\n",
    "#### Model Checkpointing and Versioning\n",
    "```python\n",
    "def save_checkpoint(agent, optimizer, episode, best_reward, version=\"v1.0\"):\n",
    "    checkpoint = {\n",
    "        'episode': episode,\n",
    "        'model_state_dict': agent.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'best_reward': best_reward,\n",
    "        'config': agent.config,\n",
    "        'version': version,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    filename = f'checkpoint_{version}_ep{episode}_{best_reward:.2f}.pth'\n",
    "    torch.save(checkpoint, filename)\n",
    "    \n",
    "    # Upload to model registry\n",
    "    # model_registry.upload(filename, metadata=checkpoint)\n",
    "\n",
    "def load_checkpoint(agent, optimizer, checkpoint_path):\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    agent.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    return (checkpoint['episode'], checkpoint['best_reward'], \n",
    "            checkpoint['config'], checkpoint['version'])\n",
    "```\n",
    "\n",
    "#### Automated Testing and Validation\n",
    "```python\n",
    "class RLValidator:\n",
    "    def __init__(self, agent, test_envs):\n",
    "        self.agent = agent\n",
    "        self.test_envs = test_envs  # List of test environments\n",
    "        \n",
    "    def validate_performance(self, num_episodes=100):\n",
    "        \"\"\"Comprehensive performance validation\"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        for env_name, env in self.test_envs.items():\n",
    "            rewards = []\n",
    "            violations = []\n",
    "            \n",
    "            for _ in range(num_episodes):\n",
    "                episode_reward, episode_violations = self.run_test_episode(env)\n",
    "                rewards.append(episode_reward)\n",
    "                violations.append(episode_violations)\n",
    "            \n",
    "            results[env_name] = {\n",
    "                'mean_reward': np.mean(rewards),\n",
    "                'std_reward': np.std(rewards),\n",
    "                'mean_violations': np.mean(violations),\n",
    "                'success_rate': np.mean([r > env.success_threshold for r in rewards])\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def run_test_episode(self, env):\n",
    "        \"\"\"Run single test episode\"\"\"\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        violations = 0\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            action = self.agent.get_action(state, training=False)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            total_reward += reward\n",
    "            if 'constraint_violation' in info and info['constraint_violation']:\n",
    "                violations += 1\n",
    "                \n",
    "            state = next_state\n",
    "        \n",
    "        return total_reward, violations\n",
    "```\n",
    "\n",
    "### Future Research Directions\n",
    "\n",
    "1. **Foundation Models for RL**: Large-scale pre-training of universal RL representations\n",
    "2. **Causal RL**: Learning causal relationships for better generalization and robustness\n",
    "3. **Neuro-Symbolic RL**: Combining neural networks with symbolic reasoning for interpretability\n",
    "4. **Energy-Based Models**: Using energy functions for more stable and sample-efficient learning\n",
    "5. **Self-Supervised RL**: Learning from task-agnostic objectives for better representation learning\n",
    "6. **Human-AI Alignment**: RL methods that align with human values and preferences\n",
    "7. **Quantum RL**: Leveraging quantum computing for exponential speedup in RL algorithms\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This assignment provides a comprehensive foundation in advanced deep reinforcement learning techniques essential for real-world deployment. The implemented methods demonstrate the current state-of-the-art while highlighting areas for future improvement and research.\n",
    "\n",
    "**Key Takeaway**: Advanced RL success requires understanding the fundamental trade-offs between performance, safety, robustness, and computational efficiency. Real-world deployment demands careful consideration of all these factors, often requiring hybrid approaches that combine multiple paradigms.\n",
    "\n",
    "The field of advanced RL is rapidly evolving, and the techniques explored here represent just the beginning of what's possible with intelligent autonomous systems.\n",
    "\n",
    "---\n",
    "\n",
    "*This concludes Computer Assignment 14: Advanced Deep Reinforcement Learning. The comprehensive exploration of offline, safe, multi-agent, and robust RL methods provides the foundation for tackling the most challenging real-world reinforcement learning problems.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5d68a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c97007a5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
