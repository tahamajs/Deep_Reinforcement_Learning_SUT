#!/bin/bash

# CA4: Advanced Policy Gradient Methods - Complete Run Script
# Ø§ÛŒÙ† Ø§Ø³Ú©Ø±ÛŒÙ¾Øª ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø±Ø§ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± visualizations Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯

echo "=========================================="
echo "ğŸš€ CA4: Advanced Policy Gradient Methods"
echo "=========================================="

# ØªÙ†Ø¸ÛŒÙ… Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ
export PYTHONPATH="${PYTHONPATH}:$(pwd)"
export MPLBACKEND="Agg"  # Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ø¨Ø¯ÙˆÙ† Ù†Ù…Ø§ÛŒØ´

# Ø§ÛŒØ¬Ø§Ø¯ ÙÙˆÙ„Ø¯Ø±Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
echo "ğŸ“ Ø§ÛŒØ¬Ø§Ø¯ ÙÙˆÙ„Ø¯Ø±Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²..."
mkdir -p visualizations
mkdir -p evaluation/results
mkdir -p models/saved_models
mkdir -p logs
mkdir -p experiments/advanced
mkdir -p benchmarks/results
mkdir -p analysis/reports

# Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§
echo "ğŸ“¦ Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§..."
if [ ! -d "venv" ]; then
    echo "Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­ÛŒØ· Ù…Ø¬Ø§Ø²ÛŒ..."
    python3 -m venv venv
fi

source venv/bin/activate
pip install -r requirements.txt

# Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
echo "ğŸ“¦ Ù†ØµØ¨ ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡..."
pip install plotly networkx scikit-learn opencv-python

# Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
echo "ğŸ” Ø¨Ø±Ø±Ø³ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡..."
if [ ! -f "CA4.ipynb" ]; then
    echo "âŒ ÙØ§ÛŒÙ„ CA4.ipynb ÛŒØ§ÙØª Ù†Ø´Ø¯!"
    exit 1
fi

if [ ! -f "training_examples.py" ]; then
    echo "âŒ ÙØ§ÛŒÙ„ training_examples.py ÛŒØ§ÙØª Ù†Ø´Ø¯!"
    exit 1
fi

# Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒØ¹
echo "ğŸ§ª Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒØ¹..."
source venv/bin/activate && python3 -c "
import sys
sys.path.append('.')
from experiments.experiments import run_quick_test
print('ØªØ³Øª REINFORCE...')
run_quick_test('CartPole-v1', 'reinforce', 50)
print('ØªØ³Øª Actor-Critic...')
run_quick_test('CartPole-v1', 'actor_critic', 50)
print('âœ… ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒØ¹ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù†Ø¯')
"

# Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
echo "ğŸ”¬ Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡..."
source venv/bin/activate && python3 -c "
import sys
sys.path.append('.')
from agents.advanced_algorithms import TRPOAgent, SACAgent, DDPGAgent
from environments.advanced_environments import CustomMountainCarEnv, CustomPendulumEnv
import numpy as np

print('ğŸ§ª ØªØ³Øª TRPO...')
env = CustomMountainCarEnv()
agent = TRPOAgent(env.observation_space.shape[0], env.action_space.n)
results = agent.train(env, num_episodes=100, print_every=50)
print(f'TRPO - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ: {np.mean(results[\"scores\"][-10:]):.2f}')

print('ğŸ§ª ØªØ³Øª SAC...')
env = CustomPendulumEnv()
agent = SACAgent(env.observation_space.shape[0], env.action_space.shape[0])
results = agent.train(env, num_episodes=100, print_every=50)
print(f'SAC - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ: {np.mean(results[\"scores\"][-10:]):.2f}')

print('ğŸ§ª ØªØ³Øª DDPG...')
agent = DDPGAgent(env.observation_space.shape[0], env.action_space.shape[0])
results = agent.train(env, num_episodes=100, print_every=50)
print(f'DDPG - Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø§Ù…ØªÛŒØ§Ø² Ù†Ù‡Ø§ÛŒÛŒ: {np.mean(results[\"scores\"][-10:]):.2f}')

print('âœ… ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù†Ø¯')
"

# Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
echo "ğŸ§  Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡..."
source venv/bin/activate && python3 -c "
import sys
sys.path.append('.')
from agents.advanced_networks import (
    CNNPolicyNetwork, LSTMPolicyNetwork, TransformerPolicyNetwork,
    DeepResidualPolicyNetwork, AttentionPolicyNetwork, EnsemblePolicyNetwork
)
import torch
import numpy as np

print('ğŸ§  ØªØ³Øª CNN Policy Network...')
cnn_net = CNNPolicyNetwork(input_channels=3, action_size=4)
test_input = torch.randn(1, 3, 84, 84)
output = cnn_net(test_input)
print(f'CNN Output shape: {output.shape}')

print('ğŸ§  ØªØ³Øª LSTM Policy Network...')
lstm_net = LSTMPolicyNetwork(state_size=4, action_size=2)
test_input = torch.randn(1, 4)
output = lstm_net(test_input)
print(f'LSTM Output shape: {output.shape}')

print('ğŸ§  ØªØ³Øª Transformer Policy Network...')
transformer_net = TransformerPolicyNetwork(state_size=4, action_size=2)
test_input = torch.randn(1, 4)
output = transformer_net(test_input)
print(f'Transformer Output shape: {output.shape}')

print('ğŸ§  ØªØ³Øª Deep Residual Policy Network...')
residual_net = DeepResidualPolicyNetwork(state_size=4, action_size=2)
test_input = torch.randn(1, 4)
output = residual_net(test_input)
print(f'Residual Output shape: {output.shape}')

print('ğŸ§  ØªØ³Øª Attention Policy Network...')
attention_net = AttentionPolicyNetwork(state_size=4, action_size=2)
test_input = torch.randn(1, 4)
output = attention_net(test_input)
print(f'Attention Output shape: {output.shape}')

print('ğŸ§  ØªØ³Øª Ensemble Policy Network...')
ensemble_net = EnsemblePolicyNetwork(state_size=4, action_size=2)
test_input = torch.randn(1, 4)
output = ensemble_net(test_input)
print(f'Ensemble Output shape: {output.shape}')

print('âœ… ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø´Ø¨Ú©Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹ØµØ¨ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù†Ø¯')
"

# Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Multi-Agent
echo "ğŸ‘¥ Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Multi-Agent..."
source venv/bin/activate && python3 -c "
import sys
sys.path.append('.')
from agents.multi_agent_systems import MultiAgentPolicyGradient, MetaLearningAgent
from environments.advanced_environments import MultiAgentWrapper
import gymnasium as gym

print('ğŸ‘¥ ØªØ³Øª Multi-Agent System...')
env = gym.make('CartPole-v1')
env = MultiAgentWrapper(env, num_agents=2)
multi_agent = MultiAgentPolicyGradient(num_agents=2, state_size=4, action_size=2)
multi_agent.train(env, num_episodes=50, print_every=25)

print('ğŸ§  ØªØ³Øª Meta-Learning Agent...')
meta_agent = MetaLearningAgent(state_size=4, action_size=2)
# Simulate task adaptation
task_experiences = [{'state': [0.1, 0.2, 0.3, 0.4], 'action': 0, 'reward': 1.0, 'next_state': [0.2, 0.3, 0.4, 0.5], 'done': False}]
adapted_network = meta_agent.adapt_to_task('task_1', task_experiences)
print('Meta-learning adaptation completed')

print('âœ… ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ø³ÛŒØ³ØªÙ…â€ŒÙ‡Ø§ÛŒ Multi-Agent ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù†Ø¯')
"

# Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ÛŒØ·â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
echo "ğŸŒ Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ÛŒØ·â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡..."
source venv/bin/activate && python3 -c "
import sys
sys.path.append('.')
from environments.advanced_environments import (
    AtariWrapper, CurriculumWrapper, NoisyObservationWrapper, RewardShapingWrapper
)
import gymnasium as gym

print('ğŸŒ ØªØ³Øª Atari Wrapper...')
env = gym.make('Breakout-v4')
env = AtariWrapper(env)
obs, info = env.reset()
print(f'Atari observation shape: {obs.shape}')

print('ğŸŒ ØªØ³Øª Curriculum Wrapper...')
env = gym.make('CartPole-v1')
env = CurriculumWrapper(env)
obs, info = env.reset()
print(f'Curriculum level: {info[\"level_name\"]}')

print('ğŸŒ ØªØ³Øª Noisy Observation Wrapper...')
env = gym.make('CartPole-v1')
env = NoisyObservationWrapper(env, noise_std=0.1)
obs, info = env.reset()
print(f'Noisy observation shape: {obs.shape}')

print('ğŸŒ ØªØ³Øª Reward Shaping Wrapper...')
def shaping_func(prev_state, action, next_state, reward):
    return 0.1 * (next_state[0] - prev_state[0])  # Position-based shaping
env = gym.make('CartPole-v1')
env = RewardShapingWrapper(env, shaping_function=shaping_func)
obs, info = env.reset()
print('Reward shaping wrapper initialized')

print('âœ… ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø­ÛŒØ·â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù†Ø¯')
"

# Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ ØªØ¬Ø³Ù… Ù¾ÛŒØ´Ø±ÙØªÙ‡
echo "ğŸ“Š Ø§Ø¬Ø±Ø§ÛŒ ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ ØªØ¬Ø³Ù… Ù¾ÛŒØ´Ø±ÙØªÙ‡..."
source venv/bin/activate && python3 -c "
import sys
sys.path.append('.')
from utils.advanced_visualization import AdvancedPolicyVisualizer, AdvancedTrainingVisualizer
import matplotlib.pyplot as plt
import numpy as np

print('ğŸ“Š ØªØ³Øª Advanced Policy Visualizer...')
viz = AdvancedPolicyVisualizer()

# Create dummy policy network for testing
import torch.nn as nn
policy_net = nn.Sequential(
    nn.Linear(2, 64),
    nn.ReLU(),
    nn.Linear(64, 2)
)

state_space = np.array([[-2, 2], [-2, 2]])
action_space = np.array([0, 1])

print('Creating policy landscape visualization...')
viz.plot_policy_landscape(policy_net, state_space, action_space)

print('ğŸ“Š ØªØ³Øª Advanced Training Visualizer...')
train_viz = AdvancedTrainingVisualizer()

# Create dummy training metrics
metrics = {
    'scores': np.random.randn(1000).cumsum(),
    'policy_losses': np.random.randn(1000),
    'value_losses': np.random.randn(1000),
    'entropy_losses': np.random.randn(1000)
}

print('Creating training metrics visualization...')
train_viz.plot_training_metrics(metrics)

print('âœ… ØªØ³Øªâ€ŒÙ‡Ø§ÛŒ ØªØ¬Ø³Ù… Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù†Ø¯')
"

# Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ
echo "ğŸ“š Ø§Ø¬Ø±Ø§ÛŒ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ..."
source venv/bin/activate && python3 -c "
import sys
sys.path.append('.')
from training_examples import (
    hyperparameter_sensitivity_study,
    curriculum_learning_example,
    performance_comparison_study,
    train_with_monitoring
)

print('ğŸ”¬ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ø­Ø³Ø§Ø³ÛŒØª Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§...')
hp_results = hyperparameter_sensitivity_study()
print(f'Ù†ØªØ§ÛŒØ¬ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ù‡Ø§ÛŒÙ¾Ø±Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§: {len(hp_results)} Ø±Ú©ÙˆØ±Ø¯')

print('ğŸ“ˆ Ù…Ø«Ø§Ù„ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªØ¯Ø±ÛŒØ¬ÛŒ...')
curriculum_results = curriculum_learning_example()
print(f'Ù†ØªØ§ÛŒØ¬ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ ØªØ¯Ø±ÛŒØ¬ÛŒ: {curriculum_results[\"final_performance\"]:.2f}')

print('âš–ï¸ Ù…Ø·Ø§Ù„Ø¹Ù‡ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¹Ù…Ù„Ú©Ø±Ø¯...')
comparison_results = performance_comparison_study()
print(f'Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…: {comparison_results[\"best_algorithm\"]}')

print('ğŸ“Š Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø§ Ù…Ø§Ù†ÛŒØªÙˆØ±ÛŒÙ†Ú¯...')
monitoring_results = train_with_monitoring('CartPole-v1', 200)
print(f'Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ù†Ù‡Ø§ÛŒÛŒ: {sum(monitoring_results[\"scores\"][-10:])/10:.2f}')
"

# Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡
echo "ğŸ”¬ Ø§Ø¬Ø±Ø§ÛŒ Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡..."
source venv/bin/activate && python3 -c "
import sys
sys.path.append('.')
from experiments.experiments import PolicyGradientExperiment, BenchmarkSuite
from agents.advanced_algorithms import TRPOAgent, SACAgent, DDPGAgent
from agents.advanced_networks import create_advanced_policy_network
from environments.advanced_environments import create_advanced_environment
from utils.advanced_visualization import AdvancedTrainingVisualizer, AdvancedAnalysisTools
import matplotlib.pyplot as plt
import numpy as np
import pickle

print('ğŸ”„ Ø¢Ø²Ù…Ø§ÛŒØ´ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡...')

# Create advanced environments
envs = {
    'CartPole-v1': create_advanced_environment('CartPole-v1'),
    'CustomMountainCar': create_advanced_environment('CustomMountainCar'),
    'CustomPendulum': create_advanced_environment('CustomPendulum')
}

# Test different algorithms
algorithms = {
    'REINFORCE': 'reinforce',
    'Actor-Critic': 'actor_critic',
    'TRPO': 'trpo',
    'SAC': 'sac',
    'DDPG': 'ddpg'
}

results = {}

for env_name, env in envs.items():
    print(f'Testing on {env_name}...')
    env_results = {}
    
    for alg_name, alg_type in algorithms.items():
        print(f'  Testing {alg_name}...')
        
        if alg_type == 'trpo':
            agent = TRPOAgent(env.observation_space.shape[0], env.action_space.n)
        elif alg_type == 'sac':
            agent = SACAgent(env.observation_space.shape[0], env.action_space.shape[0])
        elif alg_type == 'ddpg':
            agent = DDPGAgent(env.observation_space.shape[0], env.action_space.shape[0])
        else:
            # Use basic algorithms for comparison
            experiment = PolicyGradientExperiment(env_name)
            env_results[alg_name] = experiment.run_single_algorithm(alg_type, num_episodes=200)
            continue
        
        # Train advanced algorithm
        train_results = agent.train(env, num_episodes=200, print_every=50)
        env_results[alg_name] = train_results
    
    results[env_name] = env_results

# Save comprehensive results
with open('evaluation/results/advanced_comparison_results.pkl', 'wb') as f:
    pickle.dump(results, f)

print('ğŸ“Š Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡...')
viz = AdvancedTrainingVisualizer()

# Create comprehensive comparison plots
for env_name, env_results in results.items():
    if len(env_results) > 1:
        viz.plot_learning_curves_comparison(env_results, window_size=50)
        plt.savefig(f'visualizations/{env_name}_advanced_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        viz.plot_performance_distribution(env_results)
        plt.savefig(f'visualizations/{env_name}_performance_distribution.png', dpi=300, bbox_inches='tight')
        plt.close()

print('âœ… Ø¢Ø²Ù…Ø§ÛŒØ´â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù†Ø¯')
"

# Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡
echo "ğŸ“ˆ Ø§Ø¬Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡..."
source venv/bin/activate && python3 -c "
import sys
sys.path.append('.')
from utils.advanced_visualization import AdvancedAnalysisTools
import pickle
import numpy as np

print('ğŸ“ˆ ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡...')
analysis_tools = AdvancedAnalysisTools()

# Load results
with open('evaluation/results/advanced_comparison_results.pkl', 'rb') as f:
    results = pickle.load(f)

# Generate comprehensive analysis report
report = 'analysis/reports/comprehensive_analysis_report.txt'
with open(report, 'w') as f:
    f.write('=' * 80 + '\n')
    f.write('COMPREHENSIVE POLICY GRADIENT ANALYSIS REPORT\n')
    f.write('=' * 80 + '\n\n')
    
    for env_name, env_results in results.items():
        f.write(f'ENVIRONMENT: {env_name}\n')
        f.write('-' * 40 + '\n')
        
        for alg_name, alg_results in env_results.items():
            f.write(f'\\nAlgorithm: {alg_name}\n')
            
            if 'scores' in alg_results:
                scores = alg_results['scores']
                conv_analysis = analysis_tools.analyze_policy_convergence(scores)
                
                f.write(f'  Final Performance: {np.mean(scores[-100:]):.2f} Â± {np.std(scores[-100:]):.2f}\n')
                f.write(f'  Best Performance: {np.max(scores):.2f}\n')
                f.write(f'  Convergence: {conv_analysis[\"converged\"]}\n')
                f.write(f'  Stability: {conv_analysis[\"stability\"]:.4f}\n')
                
                if conv_analysis['convergence_episode']:
                    f.write(f'  Convergence Episode: {conv_analysis[\"convergence_episode\"]}\n')
        
        f.write('\\n' + '=' * 40 + '\\n')

print(f'ğŸ“Š Ú¯Ø²Ø§Ø±Ø´ ØªØ­Ù„ÛŒÙ„ Ø¯Ø± {report} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯')
print('âœ… ØªØ­Ù„ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯Ù†Ø¯')
"

# Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ
echo "ğŸ“‹ Ø§ÛŒØ¬Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´ Ù†Ù‡Ø§ÛŒÛŒ..."
source venv/bin/activate && python3 -c "
import os
import glob
import json
from datetime import datetime

# Collect all results
results_summary = {
    'timestamp': datetime.now().isoformat(),
    'visualizations': glob.glob('visualizations/*.png'),
    'results': glob.glob('evaluation/results/*.pkl'),
    'reports': glob.glob('analysis/reports/*.txt'),
    'logs': glob.glob('logs/*.log')
}

# Save summary
with open('evaluation/results/execution_summary.json', 'w') as f:
    json.dump(results_summary, f, indent=2)

print('ğŸ“‹ Ø®Ù„Ø§ØµÙ‡ Ø§Ø¬Ø±Ø§:')
print(f'  - ØªØ¹Ø¯Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§: {len(results_summary[\"visualizations\"])}')
print(f'  - ØªØ¹Ø¯Ø§Ø¯ Ù†ØªØ§ÛŒØ¬: {len(results_summary[\"results\"])}')
print(f'  - ØªØ¹Ø¯Ø§Ø¯ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§: {len(results_summary[\"reports\"])}')
print(f'  - Ø®Ù„Ø§ØµÙ‡ Ø¯Ø±: evaluation/results/execution_summary.json')
"

echo ""
echo "ğŸ‰ ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø§Ø¬Ø±Ø§ Ø´Ø¯Ù†Ø¯!"
echo "ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± visualizations Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯"
echo "ğŸ“ˆ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± analysis/reports Ù…ÙˆØ¬ÙˆØ¯Ù†Ø¯"
echo "ğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ú©Ø§Ù…Ù„ Ø¯Ø± ÙÙˆÙ„Ø¯Ø± evaluation/results Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯"
echo ""
echo "ğŸš€ Ù¾Ø±ÙˆÚ˜Ù‡ Policy Gradient Methods Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ØªÚ©Ù…ÛŒÙ„ Ø´Ø¯!"
echo "=========================================="