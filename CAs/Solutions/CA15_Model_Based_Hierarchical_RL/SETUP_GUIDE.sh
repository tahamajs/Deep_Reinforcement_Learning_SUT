#!/bin/bash

# CA15: Advanced Deep Reinforcement Learning - Complete Setup and Execution Guide
# This script provides comprehensive instructions for running all CA15 experiments

echo "üöÄ CA15: Advanced Deep Reinforcement Learning - Complete Setup Guide"
echo "=================================================================="

# Function to print section headers
print_section() {
    echo ""
    echo "üìã $1"
    echo "$(printf '=%.0s' {1..60})"
}

# Function to print step
print_step() {
    echo "üî∏ $1"
}

# Function to print success
print_success() {
    echo "‚úÖ $1"
}

# Function to print warning
print_warning() {
    echo "‚ö†Ô∏è  $1"
}

# Function to print info
print_info() {
    echo "‚ÑπÔ∏è  $1"
}

print_section "Project Overview"
echo "CA15 implements advanced deep reinforcement learning algorithms including:"
echo "‚Ä¢ Model-Based RL: Dynamics models, MPC, Dyna-Q"
echo "‚Ä¢ Hierarchical RL: Options, HAC, Goal-conditioned learning, Feudal networks"
echo "‚Ä¢ Planning: MCTS, Model-based value expansion, World models"
echo "‚Ä¢ Complete experiment framework with visualization and analysis"

print_section "Quick Start (Recommended)"
print_step "1. Install Dependencies"
echo "   pip install -r requirements.txt"
echo ""
print_step "2. Run All Experiments"
echo "   ./run.sh"
echo ""
print_step "3. View Results"
echo "   ‚Ä¢ Check visualizations/ folder for plots"
echo "   ‚Ä¢ Read results/ folder for detailed reports"
echo "   ‚Ä¢ Review logs/ for training details"

print_section "Alternative Execution Methods"
print_step "Method 1: Bash Script (Recommended)"
echo "   ./run.sh                    # Run all experiments"
echo "   ./run.sh model-based        # Run only model-based experiments"
echo "   ./run.sh hierarchical       # Run only hierarchical experiments"
echo "   ./run.sh planning           # Run only planning experiments"
echo "   ./run.sh notebook           # Run Jupyter notebook only"
echo ""

print_step "Method 2: Python Script"
echo "   python3 run_all_experiments.py --all           # Run all experiments"
echo "   python3 run_all_experiments.py --model-based   # Model-based only"
echo "   python3 run_all_experiments.py --hierarchical  # Hierarchical only"
echo "   python3 run_all_experiments.py --planning      # Planning only"
echo ""

print_step "Method 3: Jupyter Notebook"
echo "   jupyter notebook CA15.ipynb"
echo ""

print_step "Method 4: Individual Modules"
echo "   python3 training_examples.py                   # Training examples"
echo "   python3 experiments/runner.py                 # Experiment runner"
echo "   python3 experiments/hierarchical.py           # Hierarchical experiments"
echo "   python3 experiments/planning.py               # Planning experiments"

print_section "Project Structure"
echo "CA15_Model_Based_Hierarchical_RL/"
echo "‚îú‚îÄ‚îÄ run.sh                           # Main execution script"
echo "‚îú‚îÄ‚îÄ run_all_experiments.py           # Complete Python runner"
echo "‚îú‚îÄ‚îÄ CA15.ipynb                       # Main Jupyter notebook"
echo "‚îú‚îÄ‚îÄ training_examples.py             # Training examples"
echo "‚îú‚îÄ‚îÄ requirements.txt                 # Dependencies"
echo "‚îú‚îÄ‚îÄ README.md                        # Documentation"
echo "‚îú‚îÄ‚îÄ test_structure.py                # Structure test script"
echo "‚îÇ"
echo "‚îú‚îÄ‚îÄ model_based_rl/                  # Model-Based RL algorithms"
echo "‚îÇ   ‚îú‚îÄ‚îÄ algorithms.py               # DynamicsModel, ModelEnsemble, MPC, DynaQ"
echo "‚îÇ   ‚îî‚îÄ‚îÄ __init__.py"
echo "‚îÇ"
echo "‚îú‚îÄ‚îÄ hierarchical_rl/                 # Hierarchical RL algorithms"
echo "‚îÇ   ‚îú‚îÄ‚îÄ algorithms.py               # Options, HAC, Goal-Conditioned, Feudal"
echo "‚îÇ   ‚îú‚îÄ‚îÄ environments.py             # Hierarchical environment wrappers"
echo "‚îÇ   ‚îî‚îÄ‚îÄ __init__.py"
echo "‚îÇ"
echo "‚îú‚îÄ‚îÄ planning/                        # Planning algorithms"
echo "‚îÇ   ‚îú‚îÄ‚îÄ algorithms.py               # MCTS, MVE, LatentSpacePlanner, WorldModel"
echo "‚îÇ   ‚îî‚îÄ‚îÄ __init__.py"
echo "‚îÇ"
echo "‚îú‚îÄ‚îÄ experiments/                     # Experiment runners"
echo "‚îÇ   ‚îú‚îÄ‚îÄ runner.py                   # Unified experiment runner"
echo "‚îÇ   ‚îú‚îÄ‚îÄ hierarchical.py             # Hierarchical RL experiments"
echo "‚îÇ   ‚îú‚îÄ‚îÄ planning.py                 # Planning algorithm experiments"
echo "‚îÇ   ‚îî‚îÄ‚îÄ __init__.py"
echo "‚îÇ"
echo "‚îú‚îÄ‚îÄ environments/                    # Custom environments"
echo "‚îÇ   ‚îú‚îÄ‚îÄ grid_world.py               # Simple grid world"
echo "‚îÇ   ‚îî‚îÄ‚îÄ __init__.py"
echo "‚îÇ"
echo "‚îú‚îÄ‚îÄ utils/                           # Utility functions"
echo "‚îÇ   ‚îî‚îÄ‚îÄ __init__.py                  # ReplayBuffer, Logger, VisualizationUtils"
echo "‚îÇ"
echo "‚îú‚îÄ‚îÄ visualizations/                  # Generated plots (created after running)"
echo "‚îú‚îÄ‚îÄ results/                         # Experiment results (created after running)"
echo "‚îú‚îÄ‚îÄ logs/                            # Training logs (created after running)"
echo "‚îî‚îÄ‚îÄ data/                            # Collected data (created after running)"

print_section "Available Algorithms"

print_step "Model-Based RL"
echo "‚Ä¢ DynamicsModel: Neural network for environment dynamics learning"
echo "‚Ä¢ ModelEnsemble: Ensemble methods for uncertainty quantification"
echo "‚Ä¢ ModelPredictiveController: MPC using learned dynamics"
echo "‚Ä¢ DynaQAgent: Combining model-free and model-based learning"

print_step "Hierarchical RL"
echo "‚Ä¢ Option: Options framework implementation"
echo "‚Ä¢ HierarchicalActorCritic: Multi-level policies with different time scales"
echo "‚Ä¢ GoalConditionedAgent: Goal-conditioned RL with Hindsight Experience Replay"
echo "‚Ä¢ FeudalNetwork: Manager-worker architecture for goal-directed behavior"

print_step "Planning Algorithms"
echo "‚Ä¢ MonteCarloTreeSearch: MCTS with neural network guidance"
echo "‚Ä¢ ModelBasedValueExpansion: Recursive value expansion using learned models"
echo "‚Ä¢ LatentSpacePlanner: Planning in learned compact representations"
echo "‚Ä¢ WorldModel: End-to-end models for environment simulation and control"

print_section "Expected Results"
print_step "Generated Files"
echo "‚Ä¢ visualizations/ca15_complete_analysis_*.png: Comprehensive analysis plots"
echo "‚Ä¢ results/ca15_experiment_report_*.md: Detailed experiment reports"
echo "‚Ä¢ logs/: Training logs and metrics"
echo "‚Ä¢ data/: Collected training data"

print_step "Key Metrics"
echo "‚Ä¢ Sample Efficiency: Episodes needed to reach performance threshold"
echo "‚Ä¢ Final Performance: Average reward in final episodes"
echo "‚Ä¢ Computational Overhead: Planning time per episode"
echo "‚Ä¢ Success Rate: Goal achievement rate for hierarchical methods"

print_section "Troubleshooting"

print_step "Common Issues"
echo "1. CUDA Out of Memory:"
echo "   export CUDA_VISIBLE_DEVICES=\"\"  # Use CPU only"
echo ""
echo "2. Import Errors:"
echo "   export PYTHONPATH=\"\${PYTHONPATH}:\$(pwd)\""
echo ""
echo "3. Permission Denied:"
echo "   chmod +x run.sh run_all_experiments.py"
echo ""
echo "4. Missing Dependencies:"
echo "   pip install -r requirements.txt"

print_step "Performance Issues"
echo "‚Ä¢ Slow Training: Reduce number of episodes or use smaller networks"
echo "‚Ä¢ High Memory Usage: Reduce batch sizes or use gradient accumulation"
echo "‚Ä¢ Convergence Issues: Adjust learning rates or add learning rate scheduling"

print_section "Customization"

print_step "Adding New Algorithms"
echo "1. Create your algorithm class in the appropriate module"
echo "2. Add it to the __init__.py file"
echo "3. Include it in the experiment runner"

print_step "Custom Environments"
echo "1. Create your environment class in environments/"
echo "2. Implement the standard RL interface (reset(), step())"
echo "3. Update the experiment runners to use your environment"

print_step "Hyperparameter Tuning"
echo "Modify hyperparameters in run_all_experiments.py:"
echo "‚Ä¢ Learning rates, discount factors, exploration rates"
echo "‚Ä¢ Network architectures and training parameters"
echo "‚Ä¢ Environment-specific settings"

print_section "Advanced Usage"

print_step "Integration with Other Projects"
echo "from model_based_rl.algorithms import DynaQAgent"
echo "from hierarchical_rl.algorithms import GoalConditionedAgent"
echo "from planning.algorithms import MonteCarloTreeSearch"
echo ""
echo "# Use in your own experiments"
echo "agent = DynaQAgent(state_dim=4, action_dim=2)"
echo "# ... your training loop"

print_step "Research Extensions"
echo "‚Ä¢ Meta-Learning: Learning to learn world models across tasks"
echo "‚Ä¢ Multi-Agent Hierarchical RL: Coordinating multiple hierarchical agents"
echo "‚Ä¢ Safe Model-Based RL: Incorporating safety constraints in planning"
echo "‚Ä¢ Continual Learning: Updating world models with new experiences"

print_section "Performance Expectations"

print_step "Sample Efficiency"
echo "‚Ä¢ Model-based methods: 5-10x better than model-free"
echo "‚Ä¢ Hierarchical RL: Enables complex task decomposition"
echo "‚Ä¢ Planning algorithms: Better asymptotic performance"

print_step "Computational Trade-offs"
echo "‚Ä¢ MCTS: Highest computational cost, best performance"
echo "‚Ä¢ MPC: Moderate cost, good performance"
echo "‚Ä¢ Dyna-Q: Low cost, good sample efficiency"
echo "‚Ä¢ Goal-Conditioned: Moderate cost, excellent for multi-goal tasks"

print_section "Final Notes"
print_success "CA15 is a complete implementation of advanced deep RL algorithms"
print_success "All algorithms are fully implemented and ready for execution"
print_success "Comprehensive experiment framework with automatic visualization"
print_success "Modular design allows easy extension and customization"

print_info "For questions or issues, check the troubleshooting section"
print_info "All experiments use simple test environments for demonstration"
print_info "Results may vary significantly on more complex environments"

echo ""
echo "üéâ Ready to explore advanced deep reinforcement learning!"
echo "üöÄ Start with: ./run.sh"
echo ""
echo "Happy Learning! üéì"
