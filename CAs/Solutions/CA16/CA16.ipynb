{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b85a1e82",
   "metadata": {},
   "source": [
    "# CA16: Cutting-Edge Deep Reinforcement Learning - Foundation Models, Neurosymbolic RL, and Future Paradigms\n",
    "\n",
    "## Deep Reinforcement Learning - Advanced Topics and Emerging Paradigms\n",
    "\n",
    "This comprehensive notebook explores the latest frontiers in Deep Reinforcement Learning, covering foundation models, neurosymbolic approaches, continual learning, human-AI collaboration, and emerging paradigms that will shape the future of intelligent agents.\n",
    "\n",
    "## Topics Covered:\n",
    "\n",
    "### üß† **Foundation Models in RL**\n",
    "- Large-scale pre-trained RL models\n",
    "- Decision Transformer and Trajectory Transformers\n",
    "- Multi-task and multi-modal RL agents\n",
    "- In-context learning for RL\n",
    "\n",
    "### üî¨ **Neurosymbolic Reinforcement Learning**\n",
    "- Symbolic reasoning integration\n",
    "- Logic-guided policy learning\n",
    "- Interpretable and explainable RL\n",
    "- Causal reasoning in RL\n",
    "\n",
    "### üîÑ **Continual and Lifelong Learning**\n",
    "- Catastrophic forgetting in RL\n",
    "- Meta-learning and adaptation\n",
    "- Progressive neural networks\n",
    "- Memory systems for continual RL\n",
    "\n",
    "### ü§ù **Human-AI Collaborative RL**\n",
    "- Learning from human feedback (RLHF)\n",
    "- Interactive learning and teaching\n",
    "- Preference learning and reward modeling\n",
    "- Constitutional AI and value alignment\n",
    "\n",
    "### ‚ö° **Advanced Computational Methods**\n",
    "- Quantum-inspired RL algorithms\n",
    "- Neuromorphic computing for RL\n",
    "- Distributed and federated RL\n",
    "- Energy-efficient RL architectures\n",
    "\n",
    "### üåç **Real-World Deployment and Ethics**\n",
    "- Production RL systems\n",
    "- Ethical considerations and fairness\n",
    "- Robustness and reliability\n",
    "- Regulatory compliance and safety\n",
    "\n",
    "## Learning Objectives:\n",
    "1. Master foundation model architectures for reinforcement learning\n",
    "2. Implement neurosymbolic RL algorithms with interpretability\n",
    "3. Design continual learning systems that avoid catastrophic forgetting\n",
    "4. Build human-AI collaborative learning frameworks\n",
    "5. Explore quantum and neuromorphic computing paradigms\n",
    "6. Apply advanced RL to real-world production systems\n",
    "7. Address ethical considerations and societal impact\n",
    "8. Analyze emerging paradigms and future research directions\n",
    "\n",
    "### Session Structure:\n",
    "- **Section 1**: Foundation Models and Large-Scale RL\n",
    "- **Section 2**: Neurosymbolic RL and Interpretability\n",
    "- **Section 3**: Continual Learning and Meta-Learning\n",
    "- **Section 4**: Human-AI Collaborative Learning\n",
    "- **Section 5**: Advanced Computational Paradigms\n",
    "- **Section 6**: Real-World Deployment and Ethics\n",
    "- **Section 7**: Future Directions and Research Frontiers\n",
    "\n",
    "---\n",
    "**Assignment Date**: Cutting-Edge Deep RL - Lesson 16  \n",
    "**Estimated Time**: 4-5 hours  \n",
    "**Difficulty**: Research-Level Advanced  \n",
    "**Prerequisites**: CA1-CA15 completed\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cc93e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using device: cpu\n",
      "\n",
      "üß† Cutting-Edge Deep RL Environment Initialized!\n",
      "üî¨ Advanced Topics: Foundation Models, Neurosymbolic RL, Continual Learning\n",
      "ü§ù Human-AI Collaboration, Quantum RL, Ethics & Future Paradigms\n",
      "‚ö° Ready for next-generation reinforcement learning research!\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical, MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple, OrderedDict\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import gym\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any, Callable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"üöÄ Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üí´ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üî¢ CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "FOUNDATION_MODEL_CONFIG = {\n",
    "    'model_dim': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'context_length': 1024,\n",
    "    'dropout': 0.1,\n",
    "    'layer_norm_eps': 1e-5,\n",
    "    'max_position_embeddings': 2048\n",
    "}\n",
    "\n",
    "NEUROSYMBOLIC_CONFIG = {\n",
    "    'logic_embedding_dim': 128,\n",
    "    'symbolic_vocab_size': 1000,\n",
    "    'reasoning_steps': 5,\n",
    "    'symbolic_weight': 0.3,\n",
    "    'neural_weight': 0.7,\n",
    "    'interpretability_threshold': 0.8\n",
    "}\n",
    "\n",
    "CONTINUAL_LEARNING_CONFIG = {\n",
    "    'ewc_lambda': 1000,\n",
    "    'memory_size': 10000,\n",
    "    'num_tasks': 10,\n",
    "    'adaptation_lr': 1e-4,\n",
    "    'meta_lr': 1e-3,\n",
    "    'forgetting_threshold': 0.1\n",
    "}\n",
    "\n",
    "HUMAN_AI_CONFIG = {\n",
    "    'preference_model_dim': 256,\n",
    "    'reward_model_lr': 3e-4,\n",
    "    'human_feedback_ratio': 0.1,\n",
    "    'preference_batch_size': 64,\n",
    "    'kl_penalty': 0.1,\n",
    "    'value_alignment_weight': 1.0\n",
    "}\n",
    "\n",
    "QUANTUM_RL_CONFIG = {\n",
    "    'num_qubits': 8,\n",
    "    'circuit_depth': 10,\n",
    "    'quantum_lr': 0.01,\n",
    "    'entanglement_layers': 3,\n",
    "    'measurement_shots': 1024,\n",
    "    'quantum_advantage_threshold': 1.5\n",
    "}\n",
    "\n",
    "print(\"\\nüß† Cutting-Edge Deep RL Environment Initialized!\")\n",
    "print(\"üî¨ Advanced Topics: Foundation Models, Neurosymbolic RL, Continual Learning\")\n",
    "print(\"ü§ù Human-AI Collaboration, Quantum RL, Ethics & Future Paradigms\")\n",
    "print(\"‚ö° Ready for next-generation reinforcement learning research!\")\n",
    "print(\"\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57300682",
   "metadata": {},
   "source": [
    "# Section 1: Foundation Models in Reinforcement Learning\n",
    "\n",
    "Foundation models represent a paradigm shift in AI, where large-scale pre-trained models can be adapted to various downstream tasks. In RL, this concept translates to training massive models on diverse experiences that can then be fine-tuned for specific tasks.\n",
    "\n",
    "## 1.1 Theoretical Foundations\n",
    "\n",
    "### Decision Transformers\n",
    "The Decision Transformer reframes RL as a sequence modeling problem, where the goal is to generate actions conditioned on desired returns.\n",
    "\n",
    "**Key Insight**: Instead of learning value functions or policy gradients, we model:\n",
    "$$P(a_t | s_{1:t}, a_{1:t-1}, R_{t:T})$$\n",
    "\n",
    "Where $R_{t:T}$ represents the desired return-to-go from time $t$ to episode end $T$.\n",
    "\n",
    "### Trajectory Transformers\n",
    "Extend transformers to model entire trajectories:\n",
    "$$P(\\tau | g) = \\prod_{t=0}^{T} P(s_{t+1}, r_t, a_t | s_{1:t}, a_{1:t-1}, g)$$\n",
    "\n",
    "Where $g$ represents the goal or task specification.\n",
    "\n",
    "### Multi-Task Pre-training\n",
    "Foundation models in RL are trained on massive datasets containing:\n",
    "- Multiple environments and tasks\n",
    "- Diverse behavioral policies\n",
    "- Various skill demonstrations\n",
    "- Cross-modal experiences (vision, language, control)\n",
    "\n",
    "**Training Objective**:\n",
    "$$\\mathcal{L} = \\sum_{\\mathcal{D}_i} \\mathbb{E}_{\\tau \\sim \\mathcal{D}_i} [-\\log P(\\tau | \\text{context}_i)]$$\n",
    "\n",
    "### In-Context Learning for RL\n",
    "Similar to language models, RL foundation models can adapt to new tasks through in-context learning:\n",
    "- Provide few-shot demonstrations\n",
    "- Model infers task structure and optimal behavior\n",
    "- No gradient updates required\n",
    "\n",
    "## 1.2 Advantages and Challenges\n",
    "\n",
    "### Advantages:\n",
    "1. **Sample Efficiency**: Leverage pre-training for rapid adaptation\n",
    "2. **Generalization**: Transfer knowledge across diverse tasks\n",
    "3. **Few-Shot Learning**: Adapt to new tasks with minimal data\n",
    "4. **Unified Architecture**: Single model for multiple domains\n",
    "\n",
    "### Challenges:\n",
    "1. **Computational Requirements**: Massive models need significant resources\n",
    "2. **Data Requirements**: Need diverse, high-quality training data\n",
    "3. **Task Distribution**: Performance depends on training task diversity\n",
    "4. **Fine-tuning Complexity**: Avoiding catastrophic forgetting during adaptation\n",
    "\n",
    "### Scaling Laws in RL\n",
    "Similar to language models, RL foundation models exhibit scaling laws:\n",
    "- **Model Size**: Larger models achieve better performance\n",
    "- **Data Scale**: More diverse training data improves generalization\n",
    "- **Compute**: Increased training compute enables larger models\n",
    "\n",
    "**Empirical Scaling Relationship**:\n",
    "$$\\text{Performance} \\propto \\alpha N^{\\beta} D^{\\gamma} C^{\\delta}$$\n",
    "\n",
    "Where $N$ = model parameters, $D$ = dataset size, $C$ = compute budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33514387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Foundation Models Implementation Complete!\n",
      "üìä Key Components:\n",
      "  ‚Ä¢ DecisionTransformer: Sequence-based RL with transformers\n",
      "  ‚Ä¢ MultiTaskRLFoundationModel: Multi-task pre-training framework\n",
      "  ‚Ä¢ InContextLearningRL: Few-shot adaptation without gradient updates\n",
      "  ‚Ä¢ FoundationModelTrainer: Scalable training infrastructure\n",
      "\n",
      "‚ú® Ready for large-scale RL foundation model training!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer-based RL models.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        \n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class DecisionTransformer(nn.Module):\n",
    "    \"\"\"Decision Transformer for sequence-based RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, model_dim=512, num_heads=8, num_layers=6, \n",
    "                 max_length=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.model_dim = model_dim\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        self.state_embedding = nn.Linear(state_dim, model_dim)\n",
    "        self.action_embedding = nn.Linear(action_dim, model_dim)\n",
    "        self.return_embedding = nn.Linear(1, model_dim)\n",
    "        self.timestep_embedding = nn.Embedding(max_length, model_dim)\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(model_dim, max_length * 3)  # 3x for s,a,r tokens\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=model_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=4 * model_dim,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(model_dim)\n",
    "        \n",
    "        self.action_head = nn.Linear(model_dim, action_dim)\n",
    "        self.value_head = nn.Linear(model_dim, 1)\n",
    "        self.return_head = nn.Linear(model_dim, 1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize transformer weights.\"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "    \n",
    "    def forward(self, states, actions, returns_to_go, timesteps, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through Decision Transformer.\n",
    "        \n",
    "        Args:\n",
    "            states: (batch_size, seq_len, state_dim)\n",
    "            actions: (batch_size, seq_len, action_dim)\n",
    "            returns_to_go: (batch_size, seq_len, 1)\n",
    "            timesteps: (batch_size, seq_len)\n",
    "            attention_mask: (batch_size, seq_len * 3)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = states.shape[0], states.shape[1]\n",
    "        \n",
    "        state_embeddings = self.state_embedding(states)\n",
    "        action_embeddings = self.action_embedding(actions)\n",
    "        return_embeddings = self.return_embedding(returns_to_go)\n",
    "        time_embeddings = self.timestep_embedding(timesteps)\n",
    "        \n",
    "        state_embeddings += time_embeddings\n",
    "        action_embeddings += time_embeddings\n",
    "        return_embeddings += time_embeddings\n",
    "        \n",
    "        stacked_inputs = torch.stack([\n",
    "            return_embeddings, state_embeddings, action_embeddings\n",
    "        ], dim=2).reshape(batch_size, 3 * seq_len, self.model_dim)\n",
    "        \n",
    "        stacked_inputs = self.pos_encoding(stacked_inputs.transpose(0, 1)).transpose(0, 1)\n",
    "        stacked_inputs = self.layer_norm(stacked_inputs)\n",
    "        stacked_inputs = self.dropout(stacked_inputs)\n",
    "        \n",
    "        transformer_output = self.transformer(stacked_inputs, src_key_padding_mask=attention_mask)\n",
    "        \n",
    "        transformer_output = transformer_output.reshape(batch_size, seq_len, 3, self.model_dim)\n",
    "        \n",
    "        return_preds = self.return_head(transformer_output[:, :, 0])  # Return tokens\n",
    "        state_preds = transformer_output[:, :, 1]  # State tokens (for representation)\n",
    "        action_preds = self.action_head(transformer_output[:, :, 2])  # Action tokens\n",
    "        value_preds = self.value_head(transformer_output[:, :, 1])  # Value from state tokens\n",
    "        \n",
    "        return {\n",
    "            'action_preds': action_preds,\n",
    "            'value_preds': value_preds,\n",
    "            'return_preds': return_preds,\n",
    "            'state_representations': state_preds\n",
    "        }\n",
    "    \n",
    "    def get_action(self, states, actions, returns_to_go, timesteps, temperature=1.0):\n",
    "        \"\"\"Get action for inference.\"\"\"\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.forward(states, actions, returns_to_go, timesteps)\n",
    "            action_logits = outputs['action_preds'][:, -1] / temperature\n",
    "            \n",
    "            if self.action_dim > 1:\n",
    "                action_probs = F.softmax(action_logits, dim=-1)\n",
    "                action = torch.multinomial(action_probs, 1)\n",
    "            else:\n",
    "                action = torch.tanh(action_logits)  # For continuous actions\n",
    "            \n",
    "            return action\n",
    "\n",
    "class MultiTaskRLFoundationModel(nn.Module):\n",
    "    \"\"\"Multi-task foundation model for RL.\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim, action_dim, task_dim, model_dim=512, num_heads=8, num_layers=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.task_dim = task_dim\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        self.task_embedding = nn.Embedding(task_dim, model_dim)\n",
    "        \n",
    "        self.decision_transformer = DecisionTransformer(\n",
    "            state_dim, action_dim, model_dim, num_heads, num_layers\n",
    "        )\n",
    "        \n",
    "        self.task_heads = nn.ModuleDict({\n",
    "            f'task_{i}': nn.Linear(model_dim, action_dim)\n",
    "            for i in range(task_dim)\n",
    "        })\n",
    "        \n",
    "        self.context_encoder = nn.LSTM(model_dim, model_dim, batch_first=True)\n",
    "        self.adaptation_network = nn.Sequential(\n",
    "            nn.Linear(model_dim, model_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(model_dim, model_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, states, actions, returns_to_go, timesteps, task_ids, context_length=10):\n",
    "        \"\"\"Forward pass with task conditioning.\"\"\"\n",
    "        batch_size = states.shape[0]\n",
    "        \n",
    "        task_embeds = self.task_embedding(task_ids)  # (batch_size, model_dim)\n",
    "        task_embeds = task_embeds.unsqueeze(1).expand(-1, states.shape[1], -1)\n",
    "        \n",
    "        conditioned_states = states + task_embeds[:, :, :self.state_dim]\n",
    "        \n",
    "        outputs = self.decision_transformer(conditioned_states, actions, returns_to_go, timesteps)\n",
    "        \n",
    "        state_representations = outputs['state_representations']\n",
    "        task_specific_actions = []\n",
    "        \n",
    "        for i, task_id in enumerate(task_ids):\n",
    "            task_head = self.task_heads[f'task_{task_id.item()}']\n",
    "            task_action = task_head(state_representations[i])\n",
    "            task_specific_actions.append(task_action)\n",
    "        \n",
    "        outputs['task_specific_actions'] = torch.stack(task_specific_actions)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def adapt_to_new_task(self, context_trajectories, num_adaptation_steps=5):\n",
    "        \"\"\"Few-shot adaptation to new task using in-context learning.\"\"\"\n",
    "        context_features = []\n",
    "        \n",
    "        for trajectory in context_trajectories:\n",
    "            states, actions, returns = trajectory['states'], trajectory['actions'], trajectory['returns']\n",
    "            timesteps = torch.arange(len(states))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.decision_transformer(states, actions, returns, timesteps)\n",
    "                context_features.append(outputs['state_representations'].mean(dim=1))\n",
    "        \n",
    "        context_features = torch.stack(context_features)\n",
    "        context_encoding, _ = self.context_encoder(context_features.unsqueeze(0))\n",
    "        \n",
    "        adaptation_params = self.adaptation_network(context_encoding.squeeze(0).mean(dim=0))\n",
    "        \n",
    "        return adaptation_params\n",
    "\n",
    "class InContextLearningRL:\n",
    "    \"\"\"In-context learning for RL foundation models.\"\"\"\n",
    "    \n",
    "    def __init__(self, foundation_model, context_length=50):\n",
    "        self.foundation_model = foundation_model\n",
    "        self.context_length = context_length\n",
    "        self.context_buffer = deque(maxlen=context_length)\n",
    "    \n",
    "    def add_context(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add experience to context buffer.\"\"\"\n",
    "        self.context_buffer.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'next_state': next_state,\n",
    "            'done': done\n",
    "        })\n",
    "    \n",
    "    def get_action(self, current_state, desired_return, temperature=1.0):\n",
    "        \"\"\"Get action using in-context learning.\"\"\"\n",
    "        if len(self.context_buffer) == 0:\n",
    "            return np.random.randint(self.foundation_model.action_dim)\n",
    "        \n",
    "        context_states = []\n",
    "        context_actions = []\n",
    "        context_returns = []\n",
    "        context_timesteps = []\n",
    "        \n",
    "        cumulative_return = 0\n",
    "        for i, exp in enumerate(reversed(list(self.context_buffer))):\n",
    "            context_states.append(exp['state'])\n",
    "            context_actions.append(exp['action'])\n",
    "            cumulative_return += exp['reward']\n",
    "            context_returns.append([cumulative_return])\n",
    "            context_timesteps.append(len(self.context_buffer) - i - 1)\n",
    "        \n",
    "        context_states.reverse()\n",
    "        context_actions.reverse()\n",
    "        context_returns.reverse()\n",
    "        context_timesteps.reverse()\n",
    "        \n",
    "        context_states.append(current_state)\n",
    "        context_actions.append(np.zeros(self.foundation_model.action_dim))  # Placeholder\n",
    "        context_returns.append([desired_return])\n",
    "        context_timesteps.append(len(self.context_buffer))\n",
    "        \n",
    "        states = torch.FloatTensor(context_states).unsqueeze(0).to(device)\n",
    "        actions = torch.FloatTensor(context_actions).unsqueeze(0).to(device)\n",
    "        returns_to_go = torch.FloatTensor(context_returns).unsqueeze(0).to(device)\n",
    "        timesteps = torch.LongTensor(context_timesteps).unsqueeze(0).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action = self.foundation_model.get_action(states, actions, returns_to_go, timesteps, temperature)\n",
    "        \n",
    "        return action.cpu().numpy().flatten()\n",
    "\n",
    "class FoundationModelTrainer:\n",
    "    \"\"\"Training framework for RL foundation models.\"\"\"\n",
    "    \n",
    "    def __init__(self, model, learning_rate=1e-4, weight_decay=1e-2):\n",
    "        self.model = model\n",
    "        self.optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=1000)\n",
    "        \n",
    "        self.training_stats = {\n",
    "            'losses': [],\n",
    "            'action_losses': [],\n",
    "            'value_losses': [],\n",
    "            'return_losses': []\n",
    "        }\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Single training step.\"\"\"\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        states = batch['states'].to(device)\n",
    "        actions = batch['actions'].to(device)\n",
    "        returns_to_go = batch['returns_to_go'].to(device)\n",
    "        timesteps = batch['timesteps'].to(device)\n",
    "        target_actions = batch['target_actions'].to(device)\n",
    "        target_returns = batch['target_returns'].to(device)\n",
    "        \n",
    "        outputs = self.model(states, actions, returns_to_go, timesteps)\n",
    "        \n",
    "        action_loss = F.mse_loss(outputs['action_preds'], target_actions)\n",
    "        value_loss = F.mse_loss(outputs['value_preds'], target_returns)\n",
    "        return_loss = F.mse_loss(outputs['return_preds'], target_returns)\n",
    "        \n",
    "        total_loss = action_loss + 0.5 * value_loss + 0.1 * return_loss\n",
    "        \n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        self.scheduler.step()\n",
    "        \n",
    "        self.training_stats['losses'].append(total_loss.item())\n",
    "        self.training_stats['action_losses'].append(action_loss.item())\n",
    "        self.training_stats['value_losses'].append(value_loss.item())\n",
    "        self.training_stats['return_losses'].append(return_loss.item())\n",
    "        \n",
    "        return total_loss.item()\n",
    "\n",
    "print(\"üß† Foundation Models Implementation Complete!\")\n",
    "print(\"üìä Key Components:\")\n",
    "print(\"  ‚Ä¢ DecisionTransformer: Sequence-based RL with transformers\")\n",
    "print(\"  ‚Ä¢ MultiTaskRLFoundationModel: Multi-task pre-training framework\")\n",
    "print(\"  ‚Ä¢ InContextLearningRL: Few-shot adaptation without gradient updates\")\n",
    "print(\"  ‚Ä¢ FoundationModelTrainer: Scalable training infrastructure\")\n",
    "print(\"\\n‚ú® Ready for large-scale RL foundation model training!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383bdef",
   "metadata": {},
   "source": [
    "# Section 2: Neurosymbolic Reinforcement Learning\n",
    "\n",
    "Neurosymbolic RL combines the learning capabilities of neural networks with the reasoning power of symbolic systems, creating interpretable and more robust intelligent agents.\n",
    "\n",
    "## 2.1 Theoretical Foundations\n",
    "\n",
    "### The Neurosymbolic Paradigm\n",
    "Traditional RL systems struggle with:\n",
    "- **Interpretability**: Understanding why decisions were made\n",
    "- **Compositional Reasoning**: Combining learned concepts systematically\n",
    "- **Sample Efficiency**: Learning abstract rules from limited data\n",
    "- **Transfer**: Applying learned knowledge to new domains\n",
    "\n",
    "**Neurosymbolic RL** addresses these challenges by integrating:\n",
    "- **Neural Components**: Learning from raw sensory data\n",
    "- **Symbolic Components**: Logical reasoning and rule-based inference\n",
    "- **Hybrid Architectures**: Seamless integration of both paradigms\n",
    "\n",
    "### Core Components\n",
    "\n",
    "#### 1. Symbolic Knowledge Representation\n",
    "Represent environment knowledge using formal logic:\n",
    "- **Predicate Logic**: $\\text{at}(\\text{agent}, x, y) \\land \\text{obstacle}(x+1, y) \\rightarrow \\neg \\text{move\\_right}$\n",
    "- **Temporal Logic**: $\\square (\\text{goal\\_reached} \\rightarrow \\Diamond \\text{reward})$\n",
    "- **Probabilistic Logic**: $P(\\text{success} | \\text{action}, \\text{state}) = 0.8$\n",
    "\n",
    "#### 2. Neural-Symbolic Integration Patterns\n",
    "\n",
    "**Pattern 1: Neural Perception + Symbolic Reasoning**\n",
    "$$\\pi(a|s) = \\text{SymbolicPlanner}(\\text{NeuralPerception}(s))$$\n",
    "\n",
    "**Pattern 2: Symbolic-Guided Neural Learning**\n",
    "$$\\mathcal{L} = \\mathcal{L}_{\\text{RL}} + \\lambda \\mathcal{L}_{\\text{logic}}$$\n",
    "\n",
    "**Pattern 3: Hybrid Representations**\n",
    "$$h = \\text{Combine}(h_{\\text{neural}}, h_{\\text{symbolic}})$$\n",
    "\n",
    "### Logical Policy Learning\n",
    "Learn policies that satisfy logical constraints:\n",
    "\n",
    "**Constraint Satisfaction**:\n",
    "$$\\pi^* = \\arg\\max_\\pi \\mathbb{E}_\\pi[R] \\text{ subject to } \\phi \\models \\psi$$\n",
    "\n",
    "Where $\\phi$ represents the policy behavior and $\\psi$ represents logical constraints.\n",
    "\n",
    "**Logic-Regularized RL**:\n",
    "$$\\mathcal{L} = -\\mathbb{E}_\\pi[R] + \\alpha \\cdot \\text{LogicViolation}(\\pi, \\psi)$$\n",
    "\n",
    "### Compositional Learning\n",
    "Enable agents to compose learned primitives:\n",
    "\n",
    "**Hierarchical Composition**:\n",
    "- **Skills**: $\\pi_1, \\pi_2, \\ldots, \\pi_k$\n",
    "- **Meta-Policy**: $\\pi_{\\text{meta}}(k|s)$\n",
    "- **Composition Rule**: $\\pi(a|s) = \\sum_k \\pi_{\\text{meta}}(k|s) \\pi_k(a|s)$\n",
    "\n",
    "**Logical Composition**:\n",
    "- **Primitive Predicates**: $p_1, p_2, \\ldots, p_n$\n",
    "- **Logical Operators**: $\\land, \\lor, \\neg, \\rightarrow$\n",
    "- **Complex Behaviors**: $\\psi = p_1 \\land (p_2 \\lor \\neg p_3) \\rightarrow p_4$\n",
    "\n",
    "## 2.2 Interpretability and Explainability\n",
    "\n",
    "### Attention-Based Explanations\n",
    "Use attention mechanisms to highlight decision factors:\n",
    "$$\\alpha_i = \\frac{\\exp(e_i)}{\\sum_j \\exp(e_j)}, \\quad e_i = f_{\\text{att}}(h_i)$$\n",
    "\n",
    "### Counterfactual Reasoning\n",
    "Generate explanations through counterfactuals:\n",
    "- **Question**: \"What if state $s$ were different?\"\n",
    "- **Counterfactual State**: $s' = s + \\delta$\n",
    "- **Action Change**: $\\Delta a = \\pi(s') - \\pi(s)$\n",
    "- **Explanation**: \"If $x$ were true, agent would do $y$ instead\"\n",
    "\n",
    "### Causal Discovery in RL\n",
    "Learn causal relationships between variables:\n",
    "$$X \\rightarrow Y \\text{ if } I(Y; \\text{do}(X)) > 0$$\n",
    "\n",
    "Where $I$ is mutual information and $\\text{do}(X)$ represents intervention.\n",
    "\n",
    "### Logical Rule Extraction\n",
    "Extract interpretable rules from trained policies:\n",
    "1. **State Abstraction**: Group similar states\n",
    "2. **Action Patterns**: Identify consistent action choices\n",
    "3. **Rule Formation**: Convert patterns to logical rules\n",
    "4. **Rule Validation**: Test rules on new data\n",
    "\n",
    "## 2.3 Advanced Neurosymbolic Architectures\n",
    "\n",
    "### Differentiable Neural Module Networks (dNMNs)\n",
    "Compose neural modules based on language instructions:\n",
    "- **Modules**: $\\{m_1, m_2, \\ldots, m_k\\}$\n",
    "- **Composition**: Dynamic module assembly\n",
    "- **Training**: End-to-end differentiable\n",
    "\n",
    "### Graph Neural Networks for Symbolic Reasoning\n",
    "Represent knowledge as graphs and use GNNs:\n",
    "- **Nodes**: Entities, concepts, states\n",
    "- **Edges**: Relations, transitions, dependencies\n",
    "- **Message Passing**: Propagate information through graph\n",
    "- **Reasoning**: Multi-hop inference over graph structure\n",
    "\n",
    "### Memory-Augmented Networks\n",
    "External memory for symbolic knowledge storage:\n",
    "- **Memory Matrix**: $M \\in \\mathbb{R}^{N \\times D}$\n",
    "- **Attention**: $w = \\text{softmax}(q^T M)$\n",
    "- **Read**: $r = w^T M$\n",
    "- **Write**: $M \\leftarrow M + w \\odot \\text{update}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2474f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Neurosymbolic RL classes implemented successfully!\n",
      "Components: LogicalPredicate, LogicalRule, SymbolicKnowledgeBase\n",
      "Neural modules: NeuralPerceptionModule, SymbolicReasoningModule\n",
      "Policy: NeurosymbolicPolicy with interpretable reasoning\n",
      "Agent: NeurosymbolicAgent with training capabilities\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Any\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "class LogicalOperator(Enum):\n",
    "    AND = \"and\"\n",
    "    OR = \"or\"\n",
    "    NOT = \"not\"\n",
    "    IMPLIES = \"implies\"\n",
    "\n",
    "@dataclass\n",
    "class LogicalPredicate:\n",
    "    name: str\n",
    "    args: List[str]\n",
    "    truth_value: float = 0.0\n",
    "    \n",
    "    def __str__(self):\n",
    "        if self.args:\n",
    "            return f\"{self.name}({', '.join(self.args)})\"\n",
    "        return self.name\n",
    "\n",
    "@dataclass\n",
    "class LogicalRule:\n",
    "    premises: List[LogicalPredicate]\n",
    "    conclusion: LogicalPredicate\n",
    "    operator: LogicalOperator\n",
    "    confidence: float = 1.0\n",
    "    \n",
    "    def evaluate(self, facts: Dict[str, float]) -> float:\n",
    "        \"\"\"Evaluate rule given current facts (fuzzy logic)\"\"\"\n",
    "        premise_values = []\n",
    "        for premise in self.premises:\n",
    "            key = str(premise)\n",
    "            premise_values.append(facts.get(key, 0.0))\n",
    "        \n",
    "        if self.operator == LogicalOperator.AND:\n",
    "            premise_truth = min(premise_values) if premise_values else 0.0\n",
    "        elif self.operator == LogicalOperator.OR:\n",
    "            premise_truth = max(premise_values) if premise_values else 0.0\n",
    "        elif self.operator == LogicalOperator.NOT:\n",
    "            premise_truth = 1.0 - max(premise_values) if premise_values else 1.0\n",
    "        elif self.operator == LogicalOperator.IMPLIES:\n",
    "            premise_truth = min(premise_values) if premise_values else 0.0\n",
    "        \n",
    "        conclusion_key = str(self.conclusion)\n",
    "        current_conclusion = facts.get(conclusion_key, 0.0)\n",
    "        \n",
    "        if self.operator == LogicalOperator.IMPLIES:\n",
    "            return min(1.0, 1.0 - premise_truth + current_conclusion) * self.confidence\n",
    "        \n",
    "        return premise_truth * self.confidence\n",
    "\n",
    "class SymbolicKnowledgeBase:\n",
    "    \"\"\"Knowledge base for storing and reasoning with logical rules\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.rules: List[LogicalRule] = []\n",
    "        self.facts: Dict[str, float] = {}\n",
    "        self.predicates: Dict[str, LogicalPredicate] = {}\n",
    "    \n",
    "    def add_rule(self, rule: LogicalRule):\n",
    "        \"\"\"Add a logical rule to the knowledge base\"\"\"\n",
    "        self.rules.append(rule)\n",
    "    \n",
    "    def add_fact(self, predicate: LogicalPredicate, truth_value: float):\n",
    "        \"\"\"Add a fact to the knowledge base\"\"\"\n",
    "        key = str(predicate)\n",
    "        self.facts[key] = truth_value\n",
    "        self.predicates[key] = predicate\n",
    "    \n",
    "    def forward_chain(self, max_iterations: int = 10) -> Dict[str, float]:\n",
    "        \"\"\"Forward chaining inference with fuzzy logic\"\"\"\n",
    "        for iteration in range(max_iterations):\n",
    "            changed = False\n",
    "            for rule in self.rules:\n",
    "                rule_activation = rule.evaluate(self.facts)\n",
    "                conclusion_key = str(rule.conclusion)\n",
    "                \n",
    "                old_value = self.facts.get(conclusion_key, 0.0)\n",
    "                new_value = max(old_value, rule_activation)\n",
    "                \n",
    "                if new_value != old_value:\n",
    "                    self.facts[conclusion_key] = new_value\n",
    "                    changed = True\n",
    "            \n",
    "            if not changed:\n",
    "                break\n",
    "        \n",
    "        return self.facts\n",
    "    \n",
    "    def explain_decision(self, query: str) -> List[str]:\n",
    "        \"\"\"Generate explanation for why a fact is true\"\"\"\n",
    "        explanations = []\n",
    "        for rule in self.rules:\n",
    "            if str(rule.conclusion) == query:\n",
    "                activation = rule.evaluate(self.facts)\n",
    "                if activation > 0.1:  # Threshold for meaningful activation\n",
    "                    premise_str = f\" {rule.operator.value} \".join([str(p) for p in rule.premises])\n",
    "                    explanations.append(f\"{query} because {premise_str} (confidence: {activation:.2f})\")\n",
    "        return explanations\n",
    "\n",
    "class NeuralPerceptionModule(nn.Module):\n",
    "    \"\"\"Neural module for perceiving raw state and extracting symbolic predicates\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, predicate_dim: int, hidden_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.predicate_dim = predicate_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, predicate_dim),\n",
    "            nn.Sigmoid()  # Output probabilities for predicates\n",
    "        )\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(hidden_dim, num_heads=4)\n",
    "        \n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: Raw state representation [batch, state_dim]\n",
    "        Returns:\n",
    "            predicates: Predicate truth values [batch, predicate_dim]\n",
    "            attention_weights: Attention weights for interpretability\n",
    "        \"\"\"\n",
    "        features = self.encoder[:-1](state)  # All layers except final sigmoid\n",
    "        \n",
    "        features_expanded = features.unsqueeze(1)  # [batch, 1, hidden_dim]\n",
    "        attended_features, attention_weights = self.attention(\n",
    "            features_expanded, features_expanded, features_expanded\n",
    "        )\n",
    "        attended_features = attended_features.squeeze(1)\n",
    "        \n",
    "        predicates = torch.sigmoid(self.encoder[-1](attended_features))\n",
    "        \n",
    "        return predicates, attention_weights\n",
    "\n",
    "class SymbolicReasoningModule:\n",
    "    \"\"\"Module for symbolic reasoning using knowledge base\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_base: SymbolicKnowledgeBase):\n",
    "        self.kb = knowledge_base\n",
    "        self.predicate_names = [\n",
    "            \"near_goal\", \"obstacle_ahead\", \"low_energy\", \"high_reward_area\",\n",
    "            \"safe_position\", \"explored_area\", \"time_pressure\", \"resource_available\"\n",
    "        ]\n",
    "    \n",
    "    def reason(self, neural_predicates: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Perform symbolic reasoning given neural predicate activations\n",
    "        \n",
    "        Args:\n",
    "            neural_predicates: Tensor of predicate activations [predicate_dim]\n",
    "        Returns:\n",
    "            Inferred facts and their truth values\n",
    "        \"\"\"\n",
    "        self.kb.facts.clear()\n",
    "        for i, pred_name in enumerate(self.predicate_names):\n",
    "            if i < len(neural_predicates):\n",
    "                pred = LogicalPredicate(pred_name, [])\n",
    "                self.kb.add_fact(pred, float(neural_predicates[i]))\n",
    "        \n",
    "        inferred_facts = self.kb.forward_chain()\n",
    "        \n",
    "        return inferred_facts\n",
    "\n",
    "class NeurosymbolicPolicy(nn.Module):\n",
    "    \"\"\"Policy that combines neural perception with symbolic reasoning\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, predicate_dim: int = 8):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.predicate_dim = predicate_dim\n",
    "        \n",
    "        self.perception = NeuralPerceptionModule(state_dim, predicate_dim)\n",
    "        \n",
    "        self.kb = SymbolicKnowledgeBase()\n",
    "        self._initialize_domain_knowledge()\n",
    "        \n",
    "        self.reasoning = SymbolicReasoningModule(self.kb)\n",
    "        \n",
    "        self.action_net = nn.Sequential(\n",
    "            nn.Linear(predicate_dim * 2, 64),  # *2 for neural + symbolic features\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(predicate_dim * 2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def _initialize_domain_knowledge(self):\n",
    "        \"\"\"Initialize knowledge base with domain-specific rules\"\"\"\n",
    "        \n",
    "        obstacle_pred = LogicalPredicate(\"obstacle_ahead\", [])\n",
    "        safe_pred = LogicalPredicate(\"safe_position\", [])\n",
    "        avoid_pred = LogicalPredicate(\"avoid_forward\", [])\n",
    "        \n",
    "        rule1 = LogicalRule(\n",
    "            premises=[obstacle_pred, LogicalPredicate(\"safe_position\", [])],\n",
    "            conclusion=avoid_pred,\n",
    "            operator=LogicalOperator.AND,\n",
    "            confidence=0.9\n",
    "        )\n",
    "        self.kb.add_rule(rule1)\n",
    "        \n",
    "        near_goal_pred = LogicalPredicate(\"near_goal\", [])\n",
    "        high_reward_pred = LogicalPredicate(\"high_reward_area\", [])\n",
    "        approach_pred = LogicalPredicate(\"approach_goal\", [])\n",
    "        \n",
    "        rule2 = LogicalRule(\n",
    "            premises=[near_goal_pred, high_reward_pred],\n",
    "            conclusion=approach_pred,\n",
    "            operator=LogicalOperator.AND,\n",
    "            confidence=0.95\n",
    "        )\n",
    "        self.kb.add_rule(rule2)\n",
    "        \n",
    "        low_energy_pred = LogicalPredicate(\"low_energy\", [])\n",
    "        resource_pred = LogicalPredicate(\"resource_available\", [])\n",
    "        collect_pred = LogicalPredicate(\"collect_resource\", [])\n",
    "        \n",
    "        rule3 = LogicalRule(\n",
    "            premises=[low_energy_pred, resource_pred],\n",
    "            conclusion=collect_pred,\n",
    "            operator=LogicalOperator.AND,\n",
    "            confidence=0.85\n",
    "        )\n",
    "        self.kb.add_rule(rule3)\n",
    "        \n",
    "        time_pred = LogicalPredicate(\"time_pressure\", [])\n",
    "        explored_pred = LogicalPredicate(\"explored_area\", [])\n",
    "        explore_pred = LogicalPredicate(\"explore_quickly\", [])\n",
    "        \n",
    "        rule4 = LogicalRule(\n",
    "            premises=[time_pred, explored_pred],\n",
    "            conclusion=explore_pred,\n",
    "            operator=LogicalOperator.AND,\n",
    "            confidence=0.8\n",
    "        )\n",
    "        self.kb.add_rule(rule4)\n",
    "    \n",
    "    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict]:\n",
    "        \"\"\"\n",
    "        Forward pass through neurosymbolic policy\n",
    "        \n",
    "        Returns:\n",
    "            action_logits: Action probability logits\n",
    "            value: State value estimate\n",
    "            explanations: Interpretability information\n",
    "        \"\"\"\n",
    "        batch_size = state.shape[0]\n",
    "        \n",
    "        neural_predicates, attention_weights = self.perception(state)\n",
    "        \n",
    "        symbolic_features_list = []\n",
    "        explanations = {\"neural_predicates\": [], \"symbolic_inferences\": [], \"explanations\": []}\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            symbolic_facts = self.reasoning.reason(neural_predicates[i])\n",
    "            \n",
    "            symbolic_features = torch.zeros(self.predicate_dim)\n",
    "            for j, pred_name in enumerate(self.reasoning.predicate_names):\n",
    "                if j < self.predicate_dim:\n",
    "                    symbolic_features[j] = symbolic_facts.get(pred_name, 0.0)\n",
    "            \n",
    "            symbolic_features_list.append(symbolic_features)\n",
    "            \n",
    "            explanations[\"neural_predicates\"].append(neural_predicates[i].detach())\n",
    "            explanations[\"symbolic_inferences\"].append(symbolic_features)\n",
    "            \n",
    "            sample_explanations = []\n",
    "            for fact_name, truth_value in symbolic_facts.items():\n",
    "                if truth_value > 0.5:  # High activation threshold\n",
    "                    fact_explanations = self.kb.explain_decision(fact_name)\n",
    "                    sample_explanations.extend(fact_explanations)\n",
    "            explanations[\"explanations\"].append(sample_explanations)\n",
    "        \n",
    "        symbolic_features = torch.stack(symbolic_features_list).to(state.device)\n",
    "        \n",
    "        combined_features = torch.cat([neural_predicates, symbolic_features], dim=1)\n",
    "        \n",
    "        action_logits = self.action_net(combined_features)\n",
    "        values = self.value_net(combined_features)\n",
    "        \n",
    "        explanations[\"attention_weights\"] = attention_weights\n",
    "        \n",
    "        return action_logits, values, explanations\n",
    "    \n",
    "    def get_action(self, state: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, Dict]:\n",
    "        \"\"\"Get action from policy with explanations\"\"\"\n",
    "        action_logits, values, explanations = self.forward(state)\n",
    "        \n",
    "        if deterministic:\n",
    "            actions = torch.argmax(action_logits, dim=-1)\n",
    "        else:\n",
    "            action_dist = torch.distributions.Categorical(logits=action_logits)\n",
    "            actions = action_dist.sample()\n",
    "        \n",
    "        return actions, explanations\n",
    "\n",
    "class NeurosymbolicAgent:\n",
    "    \"\"\"Complete neurosymbolic RL agent with training capabilities\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4):\n",
    "        self.policy = NeurosymbolicPolicy(state_dim, action_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        \n",
    "        self.training_history = {\n",
    "            'rewards': [],\n",
    "            'losses': [],\n",
    "            'explanations': []\n",
    "        }\n",
    "    \n",
    "    def train_step(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                   rewards: torch.Tensor, next_states: torch.Tensor, \n",
    "                   dones: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"Training step with advantage actor-critic\"\"\"\n",
    "        \n",
    "        action_logits, values, explanations = self.policy(states)\n",
    "        next_action_logits, next_values, _ = self.policy(next_states)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            targets = rewards + 0.99 * next_values.squeeze() * (1 - dones.float())\n",
    "            advantages = targets - values.squeeze()\n",
    "        \n",
    "        action_dist = torch.distributions.Categorical(logits=action_logits)\n",
    "        log_probs = action_dist.log_prob(actions)\n",
    "        policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "        \n",
    "        value_loss = F.mse_loss(values.squeeze(), targets.detach())\n",
    "        \n",
    "        entropy = action_dist.entropy().mean()\n",
    "        entropy_bonus = 0.01 * entropy\n",
    "        \n",
    "        total_loss = policy_loss + 0.5 * value_loss - entropy_bonus\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        train_info = {\n",
    "            'total_loss': total_loss.item(),\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': entropy.item(),\n",
    "            'avg_value': values.mean().item(),\n",
    "            'explanations': explanations\n",
    "        }\n",
    "        \n",
    "        self.training_history['losses'].append(total_loss.item())\n",
    "        \n",
    "        return train_info\n",
    "\n",
    "print(\"‚úÖ Neurosymbolic RL classes implemented successfully!\")\n",
    "print(\"Components: LogicalPredicate, LogicalRule, SymbolicKnowledgeBase\")\n",
    "print(\"Neural modules: NeuralPerceptionModule, SymbolicReasoningModule\") \n",
    "print(\"Policy: NeurosymbolicPolicy with interpretable reasoning\")\n",
    "print(\"Agent: NeurosymbolicAgent with training capabilities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba2703f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected character after line continuation character (4116690020.py, line 297)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 297\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(f\\\"Episode {episode}: Reward = {episode_reward:.2f}\\\")\\\u001b[39m\n            ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unexpected character after line continuation character\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "import seaborn as sns\n",
    "\n",
    "class SymbolicGridWorld(gym.Env):\n",
    "    \"\"\"GridWorld environment with symbolic predicates for neurosymbolic RL\"\"\"\n",
    "    \n",
    "    def __init__(self, size=8):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [size-1, size-1]\n",
    "        \n",
    "        self.obstacles = set()\n",
    "        for _ in range(size // 2):\n",
    "            x, y = random.randint(1, size-2), random.randint(1, size-2)\n",
    "            if [x, y] != self.goal_pos:\n",
    "                self.obstacles.add((x, y))\n",
    "        \n",
    "        self.resources = set()\n",
    "        for _ in range(size // 3):\n",
    "            x, y = random.randint(0, size-1), random.randint(0, size-1)\n",
    "            if [x, y] != self.goal_pos and (x, y) not in self.obstacles:\n",
    "                self.resources.add((x, y))\n",
    "        \n",
    "        self.energy = 10\n",
    "        self.max_energy = 10\n",
    "        self.time_step = 0\n",
    "        self.max_time = size * size\n",
    "        self.collected_resources = set()\n",
    "        self.visited_positions = set()\n",
    "        \n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=1, shape=(12,), dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.actions = {\n",
    "            0: [-1, 0],  # Up\n",
    "            1: [1, 0],   # Down\n",
    "            2: [0, -1],  # Left\n",
    "            3: [0, 1]    # Right\n",
    "        }\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.energy = self.max_energy\n",
    "        self.time_step = 0\n",
    "        self.collected_resources = set()\n",
    "        self.visited_positions = {tuple(self.agent_pos)}\n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        old_pos = self.agent_pos.copy()\n",
    "        new_pos = [\n",
    "            self.agent_pos[0] + self.actions[action][0],\n",
    "            self.agent_pos[1] + self.actions[action][1]\n",
    "        ]\n",
    "        \n",
    "        if 0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size:\n",
    "            if tuple(new_pos) not in self.obstacles:\n",
    "                self.agent_pos = new_pos\n",
    "                self.energy -= 1  # Moving costs energy\n",
    "        \n",
    "        self.time_step += 1\n",
    "        self.visited_positions.add(tuple(self.agent_pos))\n",
    "        \n",
    "        if tuple(self.agent_pos) in self.resources and tuple(self.agent_pos) not in self.collected_resources:\n",
    "            self.collected_resources.add(tuple(self.agent_pos))\n",
    "            self.energy = min(self.max_energy, self.energy + 3)\n",
    "        \n",
    "        reward = self._calculate_reward()\n",
    "        \n",
    "        terminated = (self.agent_pos == self.goal_pos or \n",
    "                     self.energy <= 0 or \n",
    "                     self.time_step >= self.max_time)\n",
    "        \n",
    "        return self._get_observation(), reward, terminated, False, {}\n",
    "    \n",
    "    def _calculate_reward(self):\n",
    "        \"\"\"Calculate reward based on current state\"\"\"\n",
    "        reward = 0\n",
    "        \n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward += 100\n",
    "        \n",
    "        goal_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
    "        reward -= goal_dist * 0.1\n",
    "        \n",
    "        if self.energy <= 0:\n",
    "            reward -= 50\n",
    "        \n",
    "        if tuple(self.agent_pos) in self.resources and tuple(self.agent_pos) not in self.collected_resources:\n",
    "            reward += 10\n",
    "        \n",
    "        if tuple(self.agent_pos) not in self.visited_positions:\n",
    "            reward += 1\n",
    "        \n",
    "        reward -= 0.01\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get observation with symbolic predicates\"\"\"\n",
    "        obs = np.zeros(12, dtype=np.float32)\n",
    "        \n",
    "        obs[0] = self.agent_pos[0] / self.size\n",
    "        obs[1] = self.agent_pos[1] / self.size\n",
    "        \n",
    "        obs[2] = self._near_goal()          # near_goal\n",
    "        obs[3] = self._obstacle_ahead()     # obstacle_ahead  \n",
    "        obs[4] = self._low_energy()         # low_energy\n",
    "        obs[5] = self._high_reward_area()   # high_reward_area\n",
    "        obs[6] = self._safe_position()      # safe_position\n",
    "        obs[7] = self._explored_area()      # explored_area\n",
    "        obs[8] = self._time_pressure()      # time_pressure\n",
    "        obs[9] = self._resource_available() # resource_available\n",
    "        \n",
    "        obs[10] = self.energy / self.max_energy  # energy_level\n",
    "        obs[11] = self.time_step / self.max_time # time_progress\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def _near_goal(self) -> float:\n",
    "        \"\"\"Check if near goal\"\"\"\n",
    "        dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
    "        return max(0, 1.0 - dist / (2 * self.size))\n",
    "    \n",
    "    def _obstacle_ahead(self) -> float:\n",
    "        \"\"\"Check if obstacle is ahead in any direction\"\"\"\n",
    "        for action in range(4):\n",
    "            new_pos = [\n",
    "                self.agent_pos[0] + self.actions[action][0],\n",
    "                self.agent_pos[1] + self.actions[action][1]\n",
    "            ]\n",
    "            if (0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and \n",
    "                tuple(new_pos) in self.obstacles):\n",
    "                return 1.0\n",
    "        return 0.0\n",
    "    \n",
    "    def _low_energy(self) -> float:\n",
    "        \"\"\"Check if energy is low\"\"\"\n",
    "        return max(0, 1.0 - self.energy / (self.max_energy * 0.3))\n",
    "    \n",
    "    def _high_reward_area(self) -> float:\n",
    "        \"\"\"Check if in high reward area (near goal or resource)\"\"\"\n",
    "        goal_reward = self._near_goal()\n",
    "        \n",
    "        resource_reward = 0.0\n",
    "        for resource in self.resources:\n",
    "            dist = abs(self.agent_pos[0] - resource[0]) + abs(self.agent_pos[1] - resource[1])\n",
    "            resource_reward = max(resource_reward, max(0, 1.0 - dist / 3))\n",
    "        \n",
    "        return max(goal_reward, resource_reward)\n",
    "    \n",
    "    def _safe_position(self) -> float:\n",
    "        \"\"\"Check if in safe position (not near obstacles)\"\"\"\n",
    "        min_dist = float('inf')\n",
    "        for obstacle in self.obstacles:\n",
    "            dist = abs(self.agent_pos[0] - obstacle[0]) + abs(self.agent_pos[1] - obstacle[1])\n",
    "            min_dist = min(min_dist, dist)\n",
    "        \n",
    "        if min_dist == float('inf'):\n",
    "            return 1.0\n",
    "        return min(1.0, min_dist / 3)\n",
    "    \n",
    "    def _explored_area(self) -> float:\n",
    "        \"\"\"Check if current area has been explored\"\"\"\n",
    "        return 1.0 if tuple(self.agent_pos) in self.visited_positions else 0.0\n",
    "    \n",
    "    def _time_pressure(self) -> float:\n",
    "        \"\"\"Check if under time pressure\"\"\"\n",
    "        return max(0, (self.time_step - self.max_time * 0.7) / (self.max_time * 0.3))\n",
    "    \n",
    "    def _resource_available(self) -> float:\n",
    "        \"\"\"Check if resource is available at current position\"\"\"\n",
    "        return 1.0 if (tuple(self.agent_pos) in self.resources and \n",
    "                      tuple(self.agent_pos) not in self.collected_resources) else 0.0\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Render the environment\"\"\"\n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        \n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0], obs[1]] = -1\n",
    "        \n",
    "        for res in self.resources:\n",
    "            if res not in self.collected_resources:\n",
    "                grid[res[0], res[1]] = 0.5\n",
    "        \n",
    "        for res in self.collected_resources:\n",
    "            grid[res[0], res[1]] = 0.3\n",
    "        \n",
    "        for pos in self.visited_positions:\n",
    "            if grid[pos[0], pos[1]] == 0:\n",
    "                grid[pos[0], pos[1]] = 0.1\n",
    "        \n",
    "        grid[self.goal_pos[0], self.goal_pos[1]] = 2\n",
    "        \n",
    "        grid[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "        \n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(grid, cmap='RdYlBu', vmin=-1, vmax=2)\n",
    "        plt.colorbar(label='Cell Type')\n",
    "        plt.title(f'Neurosymbolic GridWorld (Step: {self.time_step}, Energy: {self.energy})')\n",
    "        \n",
    "        legend_elements = [\n",
    "            plt.Rectangle((0,0),1,1, facecolor='darkred', label='Obstacle'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='orange', label='Resource'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='yellow', label='Collected Resource'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='lightblue', label='Visited'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='blue', label='Goal'),\n",
    "            plt.Rectangle((0,0),1,1, facecolor='red', label='Agent')\n",
    "        ]\n",
    "        plt.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "def train_neurosymbolic_agent(env, agent, episodes=1000, render_every=200):\n",
    "    \"\"\"Train neurosymbolic agent and collect interpretability data\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_explanations = []\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_explanation_log = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action, explanations = agent.policy.get_action(state_tensor, deterministic=False)\n",
    "            action = action.item()\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            \n",
    "            if explanations['explanations'][0]:  # If there are explanations\n",
    "                episode_explanation_log.append({\n",
    "                    'step': env.time_step,\n",
    "                    'state': state.copy(),\n",
    "                    'action': action,\n",
    "                    'reward': reward,\n",
    "                    'explanations': explanations['explanations'][0].copy(),\n",
    "                    'neural_predicates': explanations['neural_predicates'][0].numpy().copy(),\n",
    "                    'symbolic_inferences': explanations['symbolic_inferences'][0].numpy().copy()\n",
    "                })\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_explanations.append(episode_explanation_log)\n",
    "        \n",
    "        if episode % render_every == 0:\n",
    "            print(f\\\"Episode {episode}: Reward = {episode_reward:.2f}\\\")\\\n",
    "            if episode_explanation_log:\n",
    "                print(\\\"Sample explanations:\\\")\n",
    "                for exp in episode_explanation_log[:3]:  # Show first 3 explanations\n",
    "                    if exp['explanations']:\n",
    "                        print(f\\\"  Step {exp['step']}: {exp['explanations'][0]}\\\"\")\n",
    "            print()\n",
    "        \n",
    "        if episode > 10 and episode % 10 == 0:\n",
    "            train_states, train_actions, train_rewards, train_next_states, train_dones = [], [], [], [], []\n",
    "            \n",
    "            for _ in range(32):  # Small batch\n",
    "                state, _ = env.reset()\n",
    "                for _ in range(10):  # Short episodes for training\n",
    "                    state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                    action, _ = agent.policy.get_action(state_tensor)\n",
    "                    action = action.item()\n",
    "                    \n",
    "                    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                    done = terminated or truncated\n",
    "                    \n",
    "                    train_states.append(state)\n",
    "                    train_actions.append(action)\n",
    "                    train_rewards.append(reward)\n",
    "                    train_next_states.append(next_state)\n",
    "                    train_dones.append(done)\n",
    "                    \n",
    "                    if done:\n",
    "                        break\n",
    "                    state = next_state\n",
    "            \n",
    "            train_states = torch.FloatTensor(np.array(train_states))\n",
    "            train_actions = torch.LongTensor(train_actions)\n",
    "            train_rewards = torch.FloatTensor(train_rewards)\n",
    "            train_next_states = torch.FloatTensor(np.array(train_next_states))\n",
    "            train_dones = torch.BoolTensor(train_dones)\n",
    "            \n",
    "            train_info = agent.train_step(train_states, train_actions, train_rewards, train_next_states, train_dones)\n",
    "            agent.training_history['rewards'].append(np.mean(episode_rewards[-10:]))\n",
    "    \n",
    "    return episode_rewards, episode_explanations\n",
    "\n",
    "print(\"Creating Symbolic GridWorld Environment...\")\n",
    "env = SymbolicGridWorld(size=6)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "print(f\"Environment created with state_dim={state_dim}, action_dim={action_dim}\")\n",
    "\n",
    "print(\"Creating Neurosymbolic Agent...\")\n",
    "agent = NeurosymbolicAgent(state_dim, action_dim, lr=1e-3)\n",
    "\n",
    "print(\"‚úÖ Environment and Agent ready!\")\n",
    "print(\"Next: Run training to see neurosymbolic reasoning in action\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c7bdf",
   "metadata": {},
   "source": [
    "# Section 3: Human-AI Collaborative Learning\n",
    "\n",
    "Human-AI collaborative learning represents a paradigm where AI agents learn not just from environment interaction, but also from human guidance, feedback, and collaboration to achieve superhuman performance.\n",
    "\n",
    "## 3.1 Theoretical Foundations\n",
    "\n",
    "### The Human-AI Collaboration Paradigm\n",
    "\n",
    "Traditional RL assumes agents learn independently from environment feedback. **Human-AI Collaborative Learning** extends this by incorporating human intelligence:\n",
    "\n",
    "- **Human Expertise Integration**: Leverage human domain knowledge and intuition\n",
    "- **Interactive Learning**: Real-time human feedback during agent training\n",
    "- **Shared Control**: Dynamic handoff between human and AI decision-making\n",
    "- **Explanatory AI**: AI explains decisions to humans for better collaboration\n",
    "\n",
    "### Learning from Human Feedback (RLHF)\n",
    "\n",
    "**Preference-Based Learning**:\n",
    "Instead of engineering reward functions, learn from human preferences:\n",
    "\n",
    "$$r_{\\theta}(s, a) = \\text{RewardModel}_{\\theta}(s, a)$$\n",
    "\n",
    "Where the reward model is trained on human preference data:\n",
    "$$\\mathcal{D} = \\{(s_i, a_i^1, a_i^2, y_i)\\}$$\n",
    "\n",
    "Where $y_i \\in \\{0, 1\\}$ indicates whether human prefers action $a_i^1$ over $a_i^2$ in state $s_i$.\n",
    "\n",
    "**Bradley-Terry Model** for preferences:\n",
    "$$P(a^1 \\succ a^2 | s) = \\frac{\\exp(r_{\\theta}(s, a^1))}{\\exp(r_{\\theta}(s, a^1)) + \\exp(r_{\\theta}(s, a^2))}$$\n",
    "\n",
    "**Training Objective**:\n",
    "$$\\mathcal{L}(\\theta) = -\\mathbb{E}_{(s,a^1,a^2,y) \\sim \\mathcal{D}}[y \\log P(a^1 \\succ a^2 | s) + (1-y) \\log P(a^2 \\succ a^1 | s)]$$\n",
    "\n",
    "### Interactive Imitation Learning\n",
    "\n",
    "**DAgger (Dataset Aggregation)**:\n",
    "Iteratively collect expert demonstrations on learned policy trajectories:\n",
    "\n",
    "1. Train policy $\\pi_i$ on current dataset $\\mathcal{D}_i$\n",
    "2. Execute $\\pi_i$ to collect states $\\{s_t\\}$\n",
    "3. Query expert for optimal actions $\\{a_t^*\\}$ on $\\{s_t\\}$\n",
    "4. Aggregate: $\\mathcal{D}_{i+1} = \\mathcal{D}_i \\cup \\{(s_t, a_t^*)\\}$\n",
    "\n",
    "**SMILe (Safe Multi-agent Imitation Learning)**:\n",
    "Learn from multiple human experts with safety constraints:\n",
    "$$\\pi^* = \\arg\\min_\\pi \\sum_i w_i \\mathcal{L}_{\\text{imitation}}(\\pi, \\pi_i^{\\text{expert}}) + \\lambda \\mathcal{L}_{\\text{safety}}(\\pi)$$\n",
    "\n",
    "### Shared Autonomy and Control\n",
    "\n",
    "**Arbitration Between Human and AI**:\n",
    "Dynamic switching between human and AI control:\n",
    "\n",
    "$$a_t = \\begin{cases}\n",
    "a_t^{\\text{human}} & \\text{if } \\alpha_t > \\tau \\\\\n",
    "a_t^{\\text{AI}} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $\\alpha_t$ represents human authority level at time $t$.\n",
    "\n",
    "**Confidence-Based Handoff**:\n",
    "$$\\alpha_t = f(\\text{confidence}_{\\text{AI}}(s_t), \\text{urgency}(s_t), \\text{human\\_availability}(t))$$\n",
    "\n",
    "**Blended Control**:\n",
    "Combine human and AI actions based on context:\n",
    "$$a_t = w_t \\cdot a_t^{\\text{human}} + (1 - w_t) \\cdot a_t^{\\text{AI}}$$\n",
    "\n",
    "### Trust and Calibration\n",
    "\n",
    "**Trust Modeling**:\n",
    "Model human trust in AI decisions:\n",
    "$$T_{t+1} = T_t + \\alpha \\cdot (\\text{outcome}_t - T_t) \\cdot \\text{surprise}_t$$\n",
    "\n",
    "Where:\n",
    "- $T_t$: Trust level at time $t$\n",
    "- $\\text{outcome}_t$: Actual performance outcome\n",
    "- $\\text{surprise}_t$: Difference between expected and actual outcome\n",
    "\n",
    "**Calibrated Confidence**:\n",
    "Ensure AI confidence matches actual performance:\n",
    "$$\\text{Calibration Error} = \\mathbb{E}[|\\text{Confidence} - \\text{Accuracy}|]$$\n",
    "\n",
    "**Trust-Aware Policy**:\n",
    "Modify policy to maintain appropriate human trust:\n",
    "$$\\pi_{\\text{trust}}(a|s) = \\pi(a|s) \\cdot f_{\\text{trust}}(a, s, T_t)$$\n",
    "\n",
    "## 3.2 Human Feedback Integration Methods\n",
    "\n",
    "### Critiquing and Advice\n",
    "Allow humans to provide structured feedback:\n",
    "\n",
    "**Action Critiquing**:\n",
    "- Human observes AI action and provides feedback\n",
    "- Types: \"Good action\", \"Bad action\", \"Better action would be...\"\n",
    "- Update policy based on critique\n",
    "\n",
    "**State-Action Advice**:\n",
    "$$\\mathcal{L}_{\\text{advice}} = -\\log \\pi(a_{\\text{advised}} | s) \\cdot w_{\\text{confidence}}$$\n",
    "\n",
    "### Demonstration and Intervention\n",
    "\n",
    "**Human Demonstrations**:\n",
    "- Collect expert trajectories: $\\tau_{\\text{expert}} = \\{(s_0, a_0), (s_1, a_1), \\ldots\\}$\n",
    "- Learn via behavioral cloning or inverse RL\n",
    "- Active learning: query human on uncertain states\n",
    "\n",
    "**Intervention Learning**:\n",
    "- Human takes control when AI makes mistakes\n",
    "- Learn from intervention patterns\n",
    "- Identify failure modes and correction strategies\n",
    "\n",
    "### Preference Learning and Ranking\n",
    "\n",
    "**Pairwise Preferences**:\n",
    "Show human two action sequences and ask for preference\n",
    "$$\\mathcal{P} = \\{(\\tau_1, \\tau_2, \\text{preference})\\}$$\n",
    "\n",
    "**Trajectory Ranking**:\n",
    "Rank multiple trajectories by performance\n",
    "$$\\tau_1 \\succ \\tau_2 \\succ \\ldots \\succ \\tau_k$$\n",
    "\n",
    "**Active Preference Learning**:\n",
    "Intelligently select which comparisons to show human:\n",
    "$$\\text{query}^* = \\arg\\max_{\\text{query}} \\text{InformationGain}(\\text{query})$$\n",
    "\n",
    "## 3.3 Collaborative Decision Making\n",
    "\n",
    "### Shared Mental Models\n",
    "Align human and AI understanding of the task:\n",
    "\n",
    "**Common Ground**:\n",
    "- Shared representation of environment\n",
    "- Agreed-upon goal decomposition  \n",
    "- Common terminology and concepts\n",
    "\n",
    "**Theory of Mind**:\n",
    "AI models human beliefs, intentions, and capabilities:\n",
    "$$\\text{AI\\_Model}(\\text{human\\_belief}(s_t), \\text{human\\_goal}, \\text{human\\_capability})$$\n",
    "\n",
    "### Communication Protocols\n",
    "\n",
    "**Natural Language Interface**:\n",
    "- AI explains decisions in natural language\n",
    "- Human provides feedback via natural language\n",
    "- Bidirectional communication for coordination\n",
    "\n",
    "**Multimodal Communication**:\n",
    "- Visual indicators (attention, confidence)\n",
    "- Gestural input from humans\n",
    "- Audio feedback and alerts\n",
    "\n",
    "### Coordination Strategies\n",
    "\n",
    "**Task Allocation**:\n",
    "Divide tasks based on comparative advantage:\n",
    "$$\\text{Assign}(T_i) = \\begin{cases}\n",
    "\\text{Human} & \\text{if } \\text{Advantage}_{\\text{human}}(T_i) > \\text{Advantage}_{\\text{AI}}(T_i) \\\\\n",
    "\\text{AI} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Dynamic Role Assignment**:\n",
    "Roles change based on context, performance, and availability:\n",
    "- **Leader-Follower**: One party leads, other assists\n",
    "- **Peer Collaboration**: Equal partnership with negotiation\n",
    "- **Hierarchical**: Clear command structure with delegation\n",
    "\n",
    "## 3.4 Advanced Collaborative Learning Paradigms\n",
    "\n",
    "### Constitutional AI\n",
    "Train AI systems to follow high-level principles:\n",
    "\n",
    "1. **Constitutional Training**: Define principles in natural language\n",
    "2. **Self-Critiquing**: AI evaluates its own responses against principles\n",
    "3. **Iterative Refinement**: Improve responses based on principle violations\n",
    "\n",
    "**Constitutional Loss**:\n",
    "$$\\mathcal{L}_{\\text{constitutional}} = \\mathcal{L}_{\\text{task}} + \\lambda \\sum_i \\text{Violation}(\\text{principle}_i)$$\n",
    "\n",
    "### Cooperative Inverse Reinforcement Learning (Co-IRL)\n",
    "Learn shared reward functions through interaction:\n",
    "\n",
    "$$R^* = \\arg\\max_R \\log P(\\tau_{\\text{human}} | R) + \\log P(\\tau_{\\text{AI}} | R) + \\text{Cooperation}(R)$$\n",
    "\n",
    "### Multi-Agent Human-AI Teams\n",
    "Extend collaboration to multi-agent settings:\n",
    "\n",
    "**Team Formation**:\n",
    "- Optimal team composition (humans + AI agents)\n",
    "- Role specialization and capability matching\n",
    "- Communication network topology\n",
    "\n",
    "**Collective Intelligence**:\n",
    "$$\\text{Team\\_Performance} > \\max(\\text{Individual\\_Performance})$$\n",
    "\n",
    "### Continual Human-AI Co-Evolution\n",
    "Humans and AI systems improve together over time:\n",
    "\n",
    "**Co-Adaptation**:\n",
    "- AI adapts to human preferences and style\n",
    "- Humans develop better collaboration skills with AI\n",
    "- Mutual model updates and learning\n",
    "\n",
    "**Lifelong Collaboration**:\n",
    "- Maintain collaboration quality over extended periods\n",
    "- Handle changes in human capabilities and preferences\n",
    "- Evolve communication and coordination protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "161eb87b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Human-AI Collaborative Learning implementation complete!\n",
      "Components implemented:\n",
      "- PreferenceRewardModel: Learn from human preferences\n",
      "- HumanFeedbackCollector: Simulate human feedback\n",
      "- CollaborativeAgent: Agent that learns from human feedback\n",
      "- Trust modeling and intervention mechanisms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "@dataclass\n",
    "class HumanPreference:\n",
    "    \"\"\"Represents a human preference between two trajectories or actions\"\"\"\n",
    "    state: np.ndarray\n",
    "    action1: int\n",
    "    action2: int \n",
    "    preference: int  # 0 if action1 preferred, 1 if action2 preferred\n",
    "    confidence: float = 1.0\n",
    "    timestamp: float = 0.0\n",
    "\n",
    "@dataclass \n",
    "class HumanFeedback:\n",
    "    \"\"\"Different types of human feedback\"\"\"\n",
    "    feedback_type: str  # 'preference', 'critique', 'demonstration', 'intervention'\n",
    "    content: any\n",
    "    confidence: float = 1.0\n",
    "    context: Dict = None\n",
    "\n",
    "class PreferenceRewardModel(nn.Module):\n",
    "    \"\"\"Neural network that learns to predict human preferences\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(state_dim + 1, hidden_dim),  # +1 for action (discrete)\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1)  # Output scalar reward\n",
    "        )\n",
    "        \n",
    "        self.confidence_net = nn.Sequential(\n",
    "            nn.Linear(state_dim + 1, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, 1),\n",
    "            nn.Sigmoid()  # Confidence between 0 and 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, states: torch.Tensor, actions: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass through reward model\n",
    "        \n",
    "        Args:\n",
    "            states: State tensors [batch, state_dim]\n",
    "            actions: Action tensors [batch] (discrete)\n",
    "        \n",
    "        Returns:\n",
    "            rewards: Predicted rewards [batch]\n",
    "            confidences: Prediction confidences [batch]\n",
    "        \"\"\"\n",
    "        actions_normalized = actions.float().unsqueeze(1) / self.action_dim\n",
    "        \n",
    "        state_action = torch.cat([states, actions_normalized], dim=1)\n",
    "        \n",
    "        rewards = self.encoder(state_action).squeeze(-1)\n",
    "        confidences = self.confidence_net(state_action).squeeze(-1)\n",
    "        \n",
    "        return rewards, confidences\n",
    "    \n",
    "    def preference_probability(self, state: torch.Tensor, action1: torch.Tensor, action2: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Calculate probability of preferring action1 over action2 using Bradley-Terry model\"\"\"\n",
    "        reward1, conf1 = self.forward(state, action1)\n",
    "        reward2, conf2 = self.forward(state, action2)\n",
    "        \n",
    "        prob = torch.sigmoid(reward1 - reward2)\n",
    "        return prob, (conf1 + conf2) / 2  # Average confidence\n",
    "\n",
    "class HumanFeedbackCollector:\n",
    "    \"\"\"Simulates and manages human feedback collection\"\"\"\n",
    "    \n",
    "    def __init__(self, true_reward_fn: Optional[Callable] = None):\n",
    "        self.preferences: List[HumanPreference] = []\n",
    "        self.feedback_history: List[HumanFeedback] = []\n",
    "        self.true_reward_fn = true_reward_fn\n",
    "        self.noise_level = 0.1  # Human feedback noise\n",
    "        \n",
    "    def collect_preference(self, state: np.ndarray, action1: int, action2: int, \n",
    "                          use_true_reward: bool = True) -> HumanPreference:\n",
    "        \"\"\"Simulate human preference collection\"\"\"\n",
    "        \n",
    "        if use_true_reward and self.true_reward_fn is not None:\n",
    "            reward1 = self.true_reward_fn(state, action1)\n",
    "            reward2 = self.true_reward_fn(state, action2)\n",
    "            \n",
    "            reward1 += np.random.normal(0, self.noise_level)\n",
    "            reward2 += np.random.normal(0, self.noise_level)\n",
    "            \n",
    "            preference = 0 if reward1 > reward2 else 1\n",
    "            confidence = min(1.0, max(0.1, abs(reward1 - reward2)))\n",
    "        else:\n",
    "            preference = random.choice([0, 1])\n",
    "            confidence = random.uniform(0.5, 1.0)\n",
    "        \n",
    "        pref = HumanPreference(\n",
    "            state=state,\n",
    "            action1=action1,\n",
    "            action2=action2,\n",
    "            preference=preference,\n",
    "            confidence=confidence\n",
    "        )\n",
    "        \n",
    "        self.preferences.append(pref)\n",
    "        return pref\n",
    "    \n",
    "    def collect_critique(self, state: np.ndarray, action: int, ai_reward: float) -> HumanFeedback:\n",
    "        \"\"\"Simulate human critique of AI action\"\"\"\n",
    "        \n",
    "        if self.true_reward_fn is not None:\n",
    "            true_reward = self.true_reward_fn(state, action)\n",
    "            \n",
    "            reward_diff = true_reward - ai_reward\n",
    "            \n",
    "            if reward_diff > 0.5:\n",
    "                critique = \"good_action\"\n",
    "                confidence = min(1.0, reward_diff)\n",
    "            elif reward_diff < -0.5:\n",
    "                critique = \"bad_action\"\n",
    "                confidence = min(1.0, abs(reward_diff))\n",
    "            else:\n",
    "                critique = \"neutral\"\n",
    "                confidence = 0.5\n",
    "        else:\n",
    "            critique = random.choice([\"good_action\", \"bad_action\", \"neutral\"])\n",
    "            confidence = random.uniform(0.3, 1.0)\n",
    "        \n",
    "        feedback = HumanFeedback(\n",
    "            feedback_type=\"critique\",\n",
    "            content=critique,\n",
    "            confidence=confidence,\n",
    "            context={\"state\": state, \"action\": action, \"ai_reward\": ai_reward}\n",
    "        )\n",
    "        \n",
    "        self.feedback_history.append(feedback)\n",
    "        return feedback\n",
    "    \n",
    "    def get_preference_dataset(self) -> List[HumanPreference]:\n",
    "        \"\"\"Get all collected preferences\"\"\"\n",
    "        return self.preferences\n",
    "    \n",
    "    def clear_history(self):\n",
    "        \"\"\"Clear feedback history\"\"\"\n",
    "        self.preferences.clear()\n",
    "        self.feedback_history.clear()\n",
    "\n",
    "class CollaborativeAgent:\n",
    "    \"\"\"RL Agent that learns from human feedback\"\"\"\n",
    "    \n",
    "    def __init__(self, state_dim: int, action_dim: int, lr: float = 3e-4):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.policy = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "        self.reward_model = PreferenceRewardModel(state_dim, action_dim)\n",
    "        \n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.value_optimizer = torch.optim.Adam(self.value_net.parameters(), lr=lr)\n",
    "        self.reward_optimizer = torch.optim.Adam(self.reward_model.parameters(), lr=lr)\n",
    "        \n",
    "        self.human_trust = 0.8  # Initial trust level\n",
    "        self.ai_confidence_history = deque(maxlen=100)\n",
    "        self.collaboration_history = []\n",
    "        \n",
    "    def get_action(self, state: torch.Tensor, use_learned_reward: bool = True) -> Tuple[int, Dict]:\n",
    "        \"\"\"Get action with collaboration information\"\"\"\n",
    "        with torch.no_grad():\n",
    "            logits = self.policy(state)\n",
    "            action_probs = F.softmax(logits, dim=-1)\n",
    "            \n",
    "            action_dist = torch.distributions.Categorical(action_probs)\n",
    "            action = action_dist.sample().item()\n",
    "            \n",
    "            if use_learned_reward:\n",
    "                reward, confidence = self.reward_model(state.unsqueeze(0), torch.tensor([action]))\n",
    "                ai_confidence = confidence.item()\n",
    "                predicted_reward = reward.item()\n",
    "            else:\n",
    "                ai_confidence = action_probs.max().item()\n",
    "                predicted_reward = None\n",
    "            \n",
    "            self.ai_confidence_history.append(ai_confidence)\n",
    "            \n",
    "            intervention_threshold = self._compute_intervention_threshold()\n",
    "            should_request_human = ai_confidence < intervention_threshold\n",
    "            \n",
    "            collab_info = {\n",
    "                'action': action,\n",
    "                'ai_confidence': ai_confidence,\n",
    "                'predicted_reward': predicted_reward,\n",
    "                'action_probs': action_probs.numpy(),\n",
    "                'should_request_human': should_request_human,\n",
    "                'human_trust': self.human_trust,\n",
    "                'intervention_threshold': intervention_threshold\n",
    "            }\n",
    "            \n",
    "            return action, collab_info\n",
    "    \n",
    "    def _compute_intervention_threshold(self) -> float:\n",
    "        \"\"\"Compute when to request human intervention based on trust and performance\"\"\"\n",
    "        base_threshold = 0.5\n",
    "        trust_adjustment = (1.0 - self.human_trust) * 0.3  # Lower trust = lower threshold\n",
    "        \n",
    "        if len(self.ai_confidence_history) > 10:\n",
    "            recent_avg_confidence = np.mean(list(self.ai_confidence_history)[-10:])\n",
    "            performance_adjustment = (0.7 - recent_avg_confidence) * 0.2\n",
    "        else:\n",
    "            performance_adjustment = 0\n",
    "        \n",
    "        threshold = base_threshold + trust_adjustment + performance_adjustment\n",
    "        return np.clip(threshold, 0.2, 0.8)\n",
    "    \n",
    "    def train_reward_model(self, preferences: List[HumanPreference], epochs: int = 10):\n",
    "        \"\"\"Train reward model from human preferences\"\"\"\n",
    "        if len(preferences) < 2:\n",
    "            return\n",
    "        \n",
    "        total_loss = 0\n",
    "        for epoch in range(epochs):\n",
    "            epoch_loss = 0\n",
    "            random.shuffle(preferences)\n",
    "            \n",
    "            for pref in preferences:\n",
    "                state = torch.FloatTensor(pref.state).unsqueeze(0)\n",
    "                action1 = torch.tensor([pref.action1])\n",
    "                action2 = torch.tensor([pref.action2])\n",
    "                \n",
    "                prob, avg_conf = self.reward_model.preference_probability(state, action1, action2)\n",
    "                \n",
    "                if pref.preference == 0:  # action1 preferred\n",
    "                    loss = -torch.log(prob + 1e-8)\n",
    "                else:  # action2 preferred\n",
    "                    loss = -torch.log(1 - prob + 1e-8)\n",
    "                \n",
    "                loss = loss * pref.confidence\n",
    "                \n",
    "                self.reward_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.reward_optimizer.step()\n",
    "                \n",
    "                epoch_loss += loss.item()\n",
    "            \n",
    "            total_loss += epoch_loss\n",
    "        \n",
    "        return total_loss / (len(preferences) * epochs)\n",
    "    \n",
    "    def update_trust(self, predicted_outcome: float, actual_outcome: float, surprise_factor: float = 1.0):\n",
    "        \"\"\"Update human trust based on AI performance\"\"\"\n",
    "        learning_rate = 0.1\n",
    "        prediction_error = actual_outcome - predicted_outcome\n",
    "        \n",
    "        normalized_error = np.tanh(prediction_error / 2.0)  # Bound between -1 and 1\n",
    "        \n",
    "        trust_update = learning_rate * normalized_error * surprise_factor\n",
    "        self.human_trust = np.clip(self.human_trust + trust_update, 0.0, 1.0)\n",
    "        \n",
    "        self.collaboration_history.append({\n",
    "            'predicted_outcome': predicted_outcome,\n",
    "            'actual_outcome': actual_outcome,\n",
    "            'prediction_error': prediction_error,\n",
    "            'trust_update': trust_update,\n",
    "            'new_trust': self.human_trust\n",
    "        })\n",
    "    \n",
    "    def train_policy_from_rewards(self, states: torch.Tensor, actions: torch.Tensor, \n",
    "                                 rewards: torch.Tensor, next_states: torch.Tensor, \n",
    "                                 dones: torch.Tensor) -> Dict[str, float]:\n",
    "        \"\"\"Train policy using learned rewards (Reinforcement Learning from Human Feedback)\"\"\"\n",
    "        \n",
    "        action_logits = self.policy(states)\n",
    "        action_dist = torch.distributions.Categorical(logits=action_logits)\n",
    "        log_probs = action_dist.log_prob(actions)\n",
    "        \n",
    "        values = self.value_net(states).squeeze()\n",
    "        next_values = self.value_net(next_states).squeeze()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            targets = rewards + 0.99 * next_values * (1 - dones.float())\n",
    "            advantages = targets - values\n",
    "        \n",
    "        policy_loss = -(log_probs * advantages.detach()).mean()\n",
    "        \n",
    "        value_loss = F.mse_loss(values, targets.detach())\n",
    "        \n",
    "        entropy = action_dist.entropy().mean()\n",
    "        entropy_bonus = 0.01 * entropy\n",
    "        \n",
    "        total_loss = policy_loss + 0.5 * value_loss - entropy_bonus\n",
    "        \n",
    "        self.policy_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        value_loss_separate = F.mse_loss(self.value_net(states).squeeze(), targets.detach())\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss_separate.backward()\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        return {\n",
    "            'policy_loss': policy_loss.item(),\n",
    "            'value_loss': value_loss.item(),\n",
    "            'entropy': entropy.item(),\n",
    "            'total_loss': total_loss.item()\n",
    "        }\n",
    "    \n",
    "    def get_collaboration_stats(self) -> Dict:\n",
    "        \"\"\"Get collaboration statistics\"\"\"\n",
    "        if not self.collaboration_history:\n",
    "            return {}\n",
    "        \n",
    "        recent_history = self.collaboration_history[-50:]  # Last 50 interactions\n",
    "        \n",
    "        return {\n",
    "            'current_trust': self.human_trust,\n",
    "            'avg_prediction_error': np.mean([h['prediction_error'] for h in recent_history]),\n",
    "            'trust_volatility': np.std([h['new_trust'] for h in recent_history]),\n",
    "            'collaboration_count': len(self.collaboration_history),\n",
    "            'recent_performance': np.mean([1 - abs(h['prediction_error']) for h in recent_history])\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Human-AI Collaborative Learning implementation complete!\")\n",
    "print(\"Components implemented:\")\n",
    "print(\"- PreferenceRewardModel: Learn from human preferences\") \n",
    "print(\"- HumanFeedbackCollector: Simulate human feedback\")\n",
    "print(\"- CollaborativeAgent: Agent that learns from human feedback\")\n",
    "print(\"- Trust modeling and intervention mechanisms\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6eae2719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Collaborative Learning Experiment...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'spaces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 365\u001b[39m\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m env, agent, feedback_collector\n\u001b[32m    364\u001b[39m \u001b[38;5;66;03m# Initialize experiment components\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m365\u001b[39m env, agent, feedback_collector = \u001b[43msetup_collaborative_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;66;03m# Test single episode with collaboration info\u001b[39;00m\n\u001b[32m    368\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mü§ñ Testing single episode with collaboration...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 346\u001b[39m, in \u001b[36msetup_collaborative_experiment\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSetting up Collaborative Learning Experiment...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# Create environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m env = \u001b[43mCollaborativeGridWorld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m state_dim = env.observation_space.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    348\u001b[39m action_dim = env.action_space.n\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mCollaborativeGridWorld.__init__\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mself\u001b[39m.reset()\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Action space: 4 directions\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mself\u001b[39m.action_space = \u001b[43mspaces\u001b[49m.Discrete(\u001b[32m4\u001b[39m)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Observation space: position + contextual features\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.observation_space = spaces.Box(low=\u001b[32m0\u001b[39m, high=\u001b[32m1\u001b[39m, shape=(\u001b[32m8\u001b[39m,), dtype=np.float32)\n",
      "\u001b[31mNameError\u001b[39m: name 'spaces' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "class CollaborativeGridWorld(gym.Env):\n",
    "    \"\"\"GridWorld specifically designed for human-AI collaboration testing\"\"\"\n",
    "    \n",
    "    def __init__(self, size=6):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.reset()\n",
    "        \n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(8,), dtype=np.float32)\n",
    "        \n",
    "        self.actions = {0: [-1, 0], 1: [1, 0], 2: [0, -1], 3: [0, 1]}  # Up, Down, Left, Right\n",
    "        \n",
    "        self.true_reward_weights = {\n",
    "            'goal_distance': -0.1,\n",
    "            'obstacle_penalty': -5.0,\n",
    "            'goal_reward': 10.0,\n",
    "            'efficiency_bonus': 0.5,\n",
    "            'exploration_bonus': 0.1\n",
    "        }\n",
    "    \n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [self.size-1, self.size-1]\n",
    "        \n",
    "        self.obstacles = set()\n",
    "        for i in range(2, 5):\n",
    "            self.obstacles.add((i, 2))\n",
    "        for j in range(1, 4):\n",
    "            self.obstacles.add((2, j))\n",
    "        \n",
    "        self.obstacles.discard(tuple(self.goal_pos))\n",
    "        \n",
    "        self.visited_positions = {tuple(self.agent_pos)}\n",
    "        self.step_count = 0\n",
    "        self.max_steps = self.size * self.size * 2\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        old_pos = self.agent_pos.copy()\n",
    "        new_pos = [\n",
    "            self.agent_pos[0] + self.actions[action][0],\n",
    "            self.agent_pos[1] + self.actions[action][1]\n",
    "        ]\n",
    "        \n",
    "        reward = 0\n",
    "        \n",
    "        if (0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and \n",
    "            tuple(new_pos) not in self.obstacles):\n",
    "            self.agent_pos = new_pos\n",
    "            \n",
    "            old_dist = abs(old_pos[0] - self.goal_pos[0]) + abs(old_pos[1] - self.goal_pos[1])\n",
    "            new_dist = abs(new_pos[0] - self.goal_pos[0]) + abs(new_pos[1] - self.goal_pos[1])\n",
    "            reward += self.true_reward_weights['goal_distance'] * (new_dist - old_dist)\n",
    "            \n",
    "            if tuple(new_pos) not in self.visited_positions:\n",
    "                reward += self.true_reward_weights['exploration_bonus']\n",
    "                self.visited_positions.add(tuple(new_pos))\n",
    "        else:\n",
    "            reward += self.true_reward_weights['obstacle_penalty']\n",
    "        \n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward += self.true_reward_weights['goal_reward']\n",
    "            efficiency = max(0, 1 - self.step_count / (self.size * 2))\n",
    "            reward += self.true_reward_weights['efficiency_bonus'] * efficiency\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        terminated = (self.agent_pos == self.goal_pos or self.step_count >= self.max_steps)\n",
    "        \n",
    "        return self._get_observation(), reward, terminated, False, {}\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get observation with useful features for learning\"\"\"\n",
    "        obs = np.zeros(8, dtype=np.float32)\n",
    "        \n",
    "        obs[0] = self.agent_pos[0] / self.size\n",
    "        obs[1] = self.agent_pos[1] / self.size\n",
    "        \n",
    "        goal_dx = (self.goal_pos[0] - self.agent_pos[0]) / self.size\n",
    "        goal_dy = (self.goal_pos[1] - self.agent_pos[1]) / self.size\n",
    "        obs[2] = goal_dx\n",
    "        obs[3] = goal_dy\n",
    "        \n",
    "        goal_dist = abs(self.agent_pos[0] - self.goal_pos[0]) + abs(self.agent_pos[1] - self.goal_pos[1])\n",
    "        obs[4] = goal_dist / (2 * self.size)\n",
    "        \n",
    "        for i, (dx, dy) in enumerate([[0, 1], [0, -1], [1, 0], [-1, 0]]):\n",
    "            next_pos = [self.agent_pos[0] + dx, self.agent_pos[1] + dy]\n",
    "            if (next_pos[0] < 0 or next_pos[0] >= self.size or \n",
    "                next_pos[1] < 0 or next_pos[1] >= self.size or\n",
    "                tuple(next_pos) in self.obstacles):\n",
    "                obs[5] = 1.0  # Obstacle detected\n",
    "                break\n",
    "        \n",
    "        obs[6] = self.step_count / self.max_steps\n",
    "        \n",
    "        obs[7] = len(self.visited_positions) / (self.size * self.size)\n",
    "        \n",
    "        return obs\n",
    "    \n",
    "    def true_reward_function(self, state, action):\n",
    "        \"\"\"True reward function for simulating human feedback\"\"\"\n",
    "        old_pos = self.agent_pos.copy()\n",
    "        old_step = self.step_count\n",
    "        old_visited = self.visited_positions.copy()\n",
    "        \n",
    "        obs, reward, done, truncated, _ = self.step(action)\n",
    "        \n",
    "        self.agent_pos = old_pos\n",
    "        self.step_count = old_step\n",
    "        self.visited_positions = old_visited\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def render_collaboration(self, collab_info=None):\n",
    "        \"\"\"Render environment with collaboration information\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        grid = np.zeros((self.size, self.size))\n",
    "        for obs in self.obstacles:\n",
    "            grid[obs[0], obs[1]] = -1\n",
    "        for pos in self.visited_positions:\n",
    "            if grid[pos[0], pos[1]] == 0:\n",
    "                grid[pos[0], pos[1]] = 0.3\n",
    "        grid[self.goal_pos[0], self.goal_pos[1]] = 2\n",
    "        grid[self.agent_pos[0], self.agent_pos[1]] = 1\n",
    "        \n",
    "        im = ax1.imshow(grid, cmap='RdYlBu', vmin=-1, vmax=2)\n",
    "        ax1.set_title(f'Collaborative GridWorld (Step: {self.step_count})')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        legend_elements = [\n",
    "            Rectangle((0,0),1,1, facecolor='darkred', label='Obstacle'),\n",
    "            Rectangle((0,0),1,1, facecolor='lightblue', label='Visited'),\n",
    "            Rectangle((0,0),1,1, facecolor='blue', label='Goal'),\n",
    "            Rectangle((0,0),1,1, facecolor='red', label='Agent')\n",
    "        ]\n",
    "        ax1.legend(handles=legend_elements, loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        \n",
    "        if collab_info:\n",
    "            ax2.axis('off')\n",
    "            info_text = f\"\"\"Collaboration Status:\n",
    "            \n",
    "AI Confidence: {collab_info['ai_confidence']:.3f}\n",
    "Human Trust: {collab_info['human_trust']:.3f}\n",
    "Intervention Threshold: {collab_info['intervention_threshold']:.3f}\n",
    "Request Human Help: {collab_info['should_request_human']}\n",
    "\n",
    "Action Probabilities:\n",
    "\"\"\"\n",
    "            for i, prob in enumerate(collab_info['action_probs']):\n",
    "                action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "                info_text += f\"  {action_names[i]}: {prob:.3f}\\n\"\n",
    "            \n",
    "            if collab_info['predicted_reward'] is not None:\n",
    "                info_text += f\"\\nPredicted Reward: {collab_info['predicted_reward']:.3f}\"\n",
    "                \n",
    "            ax2.text(0.1, 0.9, info_text, transform=ax2.transAxes, fontsize=10,\n",
    "                    verticalalignment='top', fontfamily='monospace')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def train_collaborative_agent(env, agent, feedback_collector, episodes=500, \n",
    "                             feedback_frequency=10, render_frequency=100):\n",
    "    \"\"\"Train agent with human feedback integration\"\"\"\n",
    "    \n",
    "    episode_rewards = []\n",
    "    trust_history = []\n",
    "    collaboration_events = []\n",
    "    \n",
    "    print(\"Starting Collaborative Training...\")\n",
    "    print(f\"Episodes: {episodes}, Feedback every: {feedback_frequency} episodes\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        episode_reward = 0\n",
    "        episode_steps = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            action, collab_info = agent.get_action(state_tensor, use_learned_reward=True)\n",
    "            \n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            episode_reward += reward\n",
    "            \n",
    "            episode_steps.append({\n",
    "                'state': state.copy(),\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'next_state': next_state.copy(),\n",
    "                'done': done,\n",
    "                'collab_info': collab_info\n",
    "            })\n",
    "            \n",
    "            if collab_info['predicted_reward'] is not None:\n",
    "                agent.update_trust(\n",
    "                    predicted_outcome=collab_info['predicted_reward'],\n",
    "                    actual_outcome=reward,\n",
    "                    surprise_factor=1.0 - collab_info['ai_confidence']\n",
    "                )\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        trust_history.append(agent.human_trust)\n",
    "        \n",
    "        if episode % feedback_frequency == 0 and episode > 0:\n",
    "            print(f\"\\n--- Episode {episode}: Collecting Human Feedback ---\")\n",
    "            \n",
    "            feedback_count = 0\n",
    "            for step_data in episode_steps[-10:]:  # Last 10 steps of episode\n",
    "                if random.random() < 0.3:  # 30% chance of feedback per step\n",
    "                    available_actions = list(range(env.action_space.n))\n",
    "                    if step_data['action'] in available_actions:\n",
    "                        available_actions.remove(step_data['action'])\n",
    "                    \n",
    "                    if available_actions:\n",
    "                        alt_action = random.choice(available_actions)\n",
    "                        \n",
    "                        pref = feedback_collector.collect_preference(\n",
    "                            state=step_data['state'],\n",
    "                            action1=step_data['action'],\n",
    "                            action2=alt_action,\n",
    "                            use_true_reward=True\n",
    "                        )\n",
    "                        feedback_count += 1\n",
    "                        \n",
    "                        critique = feedback_collector.collect_critique(\n",
    "                            state=step_data['state'],\n",
    "                            action=step_data['action'],\n",
    "                            ai_reward=step_data['collab_info'].get('predicted_reward', 0)\n",
    "                        )\n",
    "            \n",
    "            print(f\"Collected {feedback_count} preference comparisons\")\n",
    "            \n",
    "            if len(feedback_collector.preferences) > 5:\n",
    "                reward_loss = agent.train_reward_model(\n",
    "                    feedback_collector.preferences,\n",
    "                    epochs=5\n",
    "                )\n",
    "                print(f\"Reward model loss: {reward_loss:.4f}\")\n",
    "            \n",
    "            if len(episode_steps) > 10:\n",
    "                states = torch.FloatTensor([step['state'] for step in episode_steps])\n",
    "                actions = torch.LongTensor([step['action'] for step in episode_steps])\n",
    "                next_states = torch.FloatTensor([step['next_state'] for step in episode_steps])\n",
    "                dones = torch.BoolTensor([step['done'] for step in episode_steps])\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    learned_rewards, _ = agent.reward_model(states, actions)\n",
    "                \n",
    "                policy_stats = agent.train_policy_from_rewards(\n",
    "                    states, actions, learned_rewards, next_states, dones\n",
    "                )\n",
    "                print(f\"Policy loss: {policy_stats['policy_loss']:.4f}\")\n",
    "                \n",
    "            collaboration_events.append({\n",
    "                'episode': episode,\n",
    "                'reward': episode_reward,\n",
    "                'trust': agent.human_trust,\n",
    "                'feedback_count': feedback_count,\n",
    "                'total_preferences': len(feedback_collector.preferences)\n",
    "            })\n",
    "        \n",
    "        if episode % render_frequency == 0 and episode > 0:\n",
    "            print(f\"\\nEpisode {episode}:\")\n",
    "            print(f\"  Reward: {episode_reward:.2f}\")\n",
    "            print(f\"  Human Trust: {agent.human_trust:.3f}\")\n",
    "            print(f\"  Total Preferences Collected: {len(feedback_collector.preferences)}\")\n",
    "            \n",
    "            collab_stats = agent.get_collaboration_stats()\n",
    "            if collab_stats:\n",
    "                print(f\"  Avg Prediction Error: {collab_stats['avg_prediction_error']:.3f}\")\n",
    "                print(f\"  Recent Performance: {collab_stats['recent_performance']:.3f}\")\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'trust_history': trust_history,\n",
    "        'collaboration_events': collaboration_events,\n",
    "        'final_preferences': feedback_collector.preferences\n",
    "    }\n",
    "\n",
    "def setup_collaborative_experiment():\n",
    "    \"\"\"Setup and run collaborative learning experiment\"\"\"\n",
    "    \n",
    "    print(\"Setting up Collaborative Learning Experiment...\")\n",
    "    \n",
    "    env = CollaborativeGridWorld(size=6)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    action_dim = env.action_space.n\n",
    "    \n",
    "    agent = CollaborativeAgent(state_dim, action_dim, lr=1e-3)\n",
    "    \n",
    "    feedback_collector = HumanFeedbackCollector(\n",
    "        true_reward_fn=lambda state, action: env.true_reward_function(state, action)\n",
    "    )\n",
    "    \n",
    "    print(f\"Environment: {state_dim}D state, {action_dim} actions\")\n",
    "    print(f\"Agent: CollaborativeAgent with preference learning\")\n",
    "    print(f\"Feedback: Simulated human with {feedback_collector.noise_level} noise level\")\n",
    "    \n",
    "    return env, agent, feedback_collector\n",
    "\n",
    "env, agent, feedback_collector = setup_collaborative_experiment()\n",
    "\n",
    "print(\"\\nü§ñ Testing single episode with collaboration...\")\n",
    "state, _ = env.reset()\n",
    "action, collab_info = agent.get_action(torch.FloatTensor(state).unsqueeze(0))\n",
    "print(f\"AI Action: {action}, Confidence: {collab_info['ai_confidence']:.3f}\")\n",
    "print(f\"Should request human help: {collab_info['should_request_human']}\")\n",
    "\n",
    "print(\"\\n‚úÖ Collaborative Learning setup complete!\")\n",
    "print(\"Ready to run: train_collaborative_agent(env, agent, feedback_collector)\")\n",
    "print(\"This will train the agent with simulated human feedback\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493b884",
   "metadata": {},
   "source": [
    "# Section 4: Foundation Models in Reinforcement Learning\n",
    "\n",
    "Foundation models represent a paradigm shift in RL, leveraging pre-trained large models to achieve sample-efficient learning and strong generalization across diverse tasks and domains.\n",
    "\n",
    "## 4.1 Theoretical Foundations\n",
    "\n",
    "### The Foundation Model Paradigm in RL\n",
    "\n",
    "**Traditional RL Limitations**:\n",
    "- **Sample Inefficiency**: Learning from scratch on each task\n",
    "- **Poor Generalization**: Overfitting to specific environments\n",
    "- **Limited Transfer**: Difficulty sharing knowledge across domains\n",
    "- **Representation Learning**: Learning both policy and representations simultaneously\n",
    "\n",
    "**Foundation Model Advantages**:\n",
    "- **Pre-trained Representations**: Rich features learned from large datasets\n",
    "- **Few-Shot Learning**: Rapid adaptation to new tasks with minimal data\n",
    "- **Cross-Domain Transfer**: Knowledge sharing across different environments\n",
    "- **Compositional Reasoning**: Understanding of complex task structures\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "**Foundation Model as Universal Approximator**:\n",
    "$$f_{\\theta}: \\mathcal{X} \\rightarrow \\mathcal{Z}$$\n",
    "\n",
    "Where $\\mathcal{X}$ is input space (observations, language, etc.) and $\\mathcal{Z}$ is latent representation space.\n",
    "\n",
    "**Task-Specific Adaptation**:\n",
    "$$\\pi_{\\phi}^{(i)}(a|s) = g_{\\phi}(f_{\\theta}(s), \\text{context}_i)$$\n",
    "\n",
    "Where $g_{\\phi}$ is a task-specific head and $\\text{context}_i$ provides task information.\n",
    "\n",
    "**Multi-Task Objective**:\n",
    "$$\\mathcal{L} = \\sum_{i=1}^{T} w_i \\mathcal{L}_i(\\pi_{\\phi}^{(i)}) + \\lambda \\mathcal{L}_{\\text{reg}}(\\theta, \\phi)$$\n",
    "\n",
    "Where $T$ is number of tasks, $w_i$ are task weights, and $\\mathcal{L}_{\\text{reg}}$ is regularization.\n",
    "\n",
    "### Transfer Learning in RL\n",
    "\n",
    "**Three Paradigms**:\n",
    "\n",
    "1. **Feature Transfer**: Use pre-trained features\n",
    "   $$\\pi(a|s) = \\text{Head}(\\text{FrozenFoundationModel}(s))$$\n",
    "\n",
    "2. **Fine-Tuning**: Adapt entire model\n",
    "   $$\\theta^{*} = \\arg\\min_{\\theta} \\mathcal{L}_{\\text{task}}(\\theta) + \\lambda ||\\theta - \\theta_0||^2$$\n",
    "\n",
    "3. **Prompt-Based Learning**: Task specification through prompts\n",
    "   $$\\pi(a|s, p) = \\text{FoundationModel}(s, p)$$\n",
    "   \n",
    "   Where $p$ is a task-specific prompt.\n",
    "\n",
    "### Cross-Modal Learning\n",
    "\n",
    "**Vision-Language-Action Models**:\n",
    "$$\\pi(a|v, l) = f(v, l) \\text{ where } v \\in \\mathcal{V}, l \\in \\mathcal{L}, a \\in \\mathcal{A}$$\n",
    "\n",
    "**Unified Representations**:\n",
    "- Visual observations $\\rightarrow$ Vision transformer features\n",
    "- Language instructions $\\rightarrow$ Language model embeddings  \n",
    "- Actions $\\rightarrow$ Shared action space representations\n",
    "\n",
    "**Cross-Modal Alignment**:\n",
    "$$\\mathcal{L}_{\\text{align}} = ||\\text{Embed}_V(v) - \\text{Embed}_L(\\text{describe}(v))||^2$$\n",
    "\n",
    "## 4.2 Large Language Models for RL\n",
    "\n",
    "### LLMs as World Models\n",
    "\n",
    "**Chain-of-Thought Reasoning**:\n",
    "```\n",
    "Thought: I need to navigate to the goal while avoiding obstacles.\n",
    "Action: Move right to avoid the wall on the left.\n",
    "Observation: I see a clear path ahead.\n",
    "Thought: The goal is north of my position.\n",
    "Action: Move up toward the goal.\n",
    "```\n",
    "\n",
    "**Structured Reasoning**:\n",
    "$$\\text{Action} = \\text{LLM}(\\text{State}, \\text{Goal}, \\text{History}, \\text{Reasoning Template})$$\n",
    "\n",
    "### Prompt Engineering for RL\n",
    "\n",
    "**Task Specification Prompts**:\n",
    "```\n",
    "Task: Navigate a robot to collect all gems in a maze.\n",
    "Rules: \n",
    "- Avoid obstacles (marked as #)\n",
    "- Collect gems (marked as *)  \n",
    "- Reach exit (marked as E)\n",
    "Current state: [ASCII representation]\n",
    "Choose action: [up, down, left, right]\n",
    "```\n",
    "\n",
    "**Few-Shot Learning Prompts**:\n",
    "```\n",
    "Example 1:\n",
    "State: Agent at (0,0), Goal at (1,1), No obstacles\n",
    "Action: right (move toward goal)\n",
    "Result: Reached (1,0)\n",
    "\n",
    "Example 2: \n",
    "State: Agent at (1,0), Goal at (1,1)\n",
    "Action: up (move toward goal)\n",
    "Result: Reached goal, +10 reward\n",
    "\n",
    "Current situation:\n",
    "State: [current state]\n",
    "Action: [your choice]\n",
    "```\n",
    "\n",
    "### LLM-Based Hierarchical Planning\n",
    "\n",
    "**High-Level Planning**:\n",
    "$$\\text{Subgoals} = \\text{LLM}_{\\text{planner}}(\\text{Task}, \\text{Environment})$$\n",
    "\n",
    "**Low-Level Execution**:\n",
    "$$a_t = \\pi_{\\text{low}}(s_t, \\text{current\\_subgoal})$$\n",
    "\n",
    "**Plan Refinement**:\n",
    "$$\\text{Updated\\_Plan} = \\text{LLM}_{\\text{planner}}(\\text{Original\\_Plan}, \\text{Execution\\_Feedback})$$\n",
    "\n",
    "## 4.3 Vision Transformers in RL\n",
    "\n",
    "### ViT for State Representation\n",
    "\n",
    "**Patch Embedding**:\n",
    "$$\\text{Patches} = \\text{Reshape}(\\text{Image}_{H \\times W \\times C}) \\rightarrow \\mathbb{R}^{N \\times P^2 \\cdot C}$$\n",
    "\n",
    "Where $N = HW/P^2$ is number of patches and $P$ is patch size.\n",
    "\n",
    "**Spatial-Temporal Attention**:\n",
    "- **Spatial**: Attend to important regions in current frame\n",
    "- **Temporal**: Attend to relevant frames in history\n",
    "- **Action**: Attend to action-relevant features\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Action Prediction Head**:\n",
    "$$\\pi(a|s) = \\text{MLP}(\\text{ViT}(s)[\\text{CLS}])$$\n",
    "\n",
    "Where $[\\text{CLS}]$ is the classification token embedding.\n",
    "\n",
    "### Multi-Modal Fusion\n",
    "\n",
    "**Visual-Language Fusion**:\n",
    "$$h_{\\text{fused}} = \\text{Attention}(h_{\\text{vision}}, h_{\\text{language}}, h_{\\text{language}})$$\n",
    "\n",
    "**Hierarchical Feature Integration**:\n",
    "- **Low-level**: Pixel features, edge detection\n",
    "- **Mid-level**: Objects, spatial relationships  \n",
    "- **High-level**: Scene understanding, semantic concepts\n",
    "\n",
    "### Attention-Based Policy Networks\n",
    "\n",
    "**Self-Attention for State Processing**:\n",
    "$$A_{\\text{state}} = \\text{SelfAttention}(\\text{StateFeatures})$$\n",
    "\n",
    "**Cross-Attention for Action Selection**:\n",
    "$$A_{\\text{action}} = \\text{CrossAttention}(\\text{ActionQueries}, \\text{StateFeatures})$$\n",
    "\n",
    "**Multi-Head Architecture**:\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h)W^O$$\n",
    "\n",
    "## 4.4 Foundation Model Training Strategies\n",
    "\n",
    "### Pre-Training Objectives\n",
    "\n",
    "**Masked Language Modeling (MLM)**:\n",
    "$$\\mathcal{L}_{\\text{MLM}} = -\\sum_{i \\in \\text{masked}} \\log p(x_i | x_{\\setminus i})$$\n",
    "\n",
    "**Masked Image Modeling (MIM)**:  \n",
    "$$\\mathcal{L}_{\\text{MIM}} = ||\\text{Reconstruct}(\\text{Mask}(\\text{Image})) - \\text{Image}||^2$$\n",
    "\n",
    "**Contrastive Learning**:\n",
    "$$\\mathcal{L}_{\\text{contrastive}} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k} \\exp(\\text{sim}(z_i, z_k)/\\tau)}$$\n",
    "\n",
    "### Multi-Task Pre-Training\n",
    "\n",
    "**Joint Training Objective**:\n",
    "$$\\mathcal{L}_{\\text{joint}} = \\sum_{t=1}^{T} \\lambda_t \\mathcal{L}_t + \\mathcal{L}_{\\text{reg}}$$\n",
    "\n",
    "**Task Sampling Strategies**:\n",
    "- **Uniform Sampling**: Equal probability for all tasks\n",
    "- **Importance Sampling**: Weight by task difficulty/importance\n",
    "- **Curriculum Learning**: Gradually increase task complexity\n",
    "\n",
    "**Parameter Sharing Strategies**:\n",
    "- **Shared Encoder**: Common feature extraction\n",
    "- **Task-Specific Heads**: Specialized output layers\n",
    "- **Adapter Layers**: Small task-specific modifications\n",
    "\n",
    "### Fine-Tuning Approaches\n",
    "\n",
    "**Full Fine-Tuning**:\n",
    "- Update all parameters for target task\n",
    "- Risk of catastrophic forgetting\n",
    "- Requires substantial computational resources\n",
    "\n",
    "**Parameter-Efficient Fine-Tuning**:\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)**:\n",
    "$$W' = W + AB$$\n",
    "where $A \\in \\mathbb{R}^{d \\times r}$, $B \\in \\mathbb{R}^{r \\times d}$ with $r << d$.\n",
    "\n",
    "**Adapter Layers**:\n",
    "$$h' = h + \\text{Adapter}(h) = h + W_2 \\sigma(W_1 h + b_1) + b_2$$\n",
    "\n",
    "**Prefix Tuning**:\n",
    "Add learnable prefix vectors to transformer inputs.\n",
    "\n",
    "### Continual Learning for Foundation Models\n",
    "\n",
    "**Elastic Weight Consolidation (EWC)**:\n",
    "$$\\mathcal{L}_{\\text{EWC}} = \\mathcal{L}_{\\text{task}} + \\lambda \\sum_i F_i (\\theta_i - \\theta_i^*)^2$$\n",
    "\n",
    "Where $F_i$ is Fisher information matrix diagonal.\n",
    "\n",
    "**Progressive Networks**:\n",
    "- Freeze previous task parameters\n",
    "- Add new columns for new tasks\n",
    "- Lateral connections for knowledge transfer\n",
    "\n",
    "**Meta-Learning for Rapid Adaptation**:\n",
    "$$\\theta' = \\theta - \\alpha \\nabla_{\\theta} \\mathcal{L}_{\\text{support}}(\\theta)$$\n",
    "$$\\mathcal{L}_{\\text{meta}} = \\mathbb{E}_{\\text{tasks}} [\\mathcal{L}_{\\text{query}}(\\theta')]$$\n",
    "\n",
    "## 4.5 Emergent Capabilities\n",
    "\n",
    "### Few-Shot Task Learning\n",
    "Foundation models demonstrate remarkable ability to adapt to new tasks with minimal examples:\n",
    "\n",
    "**In-Context Learning**:\n",
    "- Provide examples in input prompt\n",
    "- Model adapts without parameter updates\n",
    "- Emergent capability from scale and diversity\n",
    "\n",
    "**Meta-Learning Through Pre-Training**:\n",
    "- Learn to learn from pre-training data distribution\n",
    "- Transfer learning strategies emerge naturally\n",
    "- Rapid adaptation to distribution shifts\n",
    "\n",
    "### Compositional Reasoning\n",
    "Combine primitive skills to solve complex tasks:\n",
    "\n",
    "**Skill Composition**:\n",
    "$$\\text{ComplexTask} = \\text{Compose}(\\text{Skill}_1, \\text{Skill}_2, \\ldots, \\text{Skill}_k)$$\n",
    "\n",
    "**Hierarchical Planning**:\n",
    "- Decompose complex goals into subgoals\n",
    "- Learn primitive skills for subgoal achievement\n",
    "- Compose skills dynamically based on context\n",
    "\n",
    "### Cross-Domain Transfer\n",
    "Knowledge learned in one domain transfers to related domains:\n",
    "\n",
    "**Domain Adaptation**:\n",
    "$$\\mathcal{L}_{\\text{adapt}} = \\mathcal{L}_{\\text{target}} + \\lambda \\mathcal{L}_{\\text{domain}}$$\n",
    "\n",
    "**Universal Policies**:\n",
    "Single policy that works across multiple environments with different dynamics, observation spaces, and action spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c888848e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Foundation Models in RL implementation complete!\n",
      "Components implemented:\n",
      "- VisionTransformer: Process visual observations with attention\n",
      "- LanguageEncoder: Process text instructions\n",
      "- CrossModalFusion: Fuse vision and language representations\n",
      "- FoundationPolicy: Multi-modal policy with interpretable attention\n",
      "- FewShotLearner: Rapid task adaptation\n",
      "- PromptTemplate: Task specification through natural language\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import MultiheadAttention, LayerNorm\n",
    "import math\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Positional encoding for transformer models\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
    "                           (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with multi-head attention and feedforward layers\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        \n",
    "        self.feedforward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        attn_out, attn_weights = self.attention(x, x, x, attn_mask=mask)\n",
    "        x = self.norm1(x + attn_out)\n",
    "        \n",
    "        ff_out = self.feedforward(x)\n",
    "        x = self.norm2(x + ff_out)\n",
    "        \n",
    "        return x, attn_weights\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\"Vision Transformer for processing visual observations\"\"\"\n",
    "    \n",
    "    def __init__(self, img_size: int = 84, patch_size: int = 16, in_channels: int = 3,\n",
    "                 d_model: int = 256, n_heads: int = 8, n_layers: int = 6, \n",
    "                 d_ff: int = 1024, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.patch_embed = nn.Conv2d(in_channels, d_model, \n",
    "                                   kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        \n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, self.n_patches + 1, d_model))\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_ff, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Input images [batch, channels, height, width]\n",
    "        Returns:\n",
    "            features: Encoded features [batch, n_patches + 1, d_model]\n",
    "            attentions: List of attention weights for visualization\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        x = self.patch_embed(x)\n",
    "        \n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        \n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        attentions = []\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(x)\n",
    "            attentions.append(attn)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x, attentions\n",
    "\n",
    "class LanguageEncoder(nn.Module):\n",
    "    \"\"\"Transformer-based language encoder for processing instructions\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size: int, d_model: int = 256, n_heads: int = 8,\n",
    "                 n_layers: int = 4, max_seq_len: int = 128, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_seq_len)\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(d_model, n_heads, d_model * 4, dropout)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self.norm = LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, tokens, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokens: Input token indices [batch, seq_len]\n",
    "            attention_mask: Attention mask [batch, seq_len]\n",
    "        Returns:\n",
    "            encoded: Encoded language features [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        x = self.embedding(tokens) * math.sqrt(self.d_model)\n",
    "        x = self.pos_encoding(x.transpose(0, 1)).transpose(0, 1)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            mask = attention_mask.unsqueeze(1).repeat(1, attention_mask.size(1), 1)\n",
    "            mask = mask.masked_fill(mask == 0, float('-inf'))\n",
    "        else:\n",
    "            mask = None\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x, _ = layer(x, mask)\n",
    "        \n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class CrossModalFusion(nn.Module):\n",
    "    \"\"\"Fuse visual and language representations using cross-attention\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vision_to_lang = MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        self.lang_to_vision = MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.norm1 = LayerNorm(d_model)\n",
    "        self.norm2 = LayerNorm(d_model)\n",
    "        \n",
    "        self.fusion_net = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "        \n",
    "    def forward(self, vision_features, lang_features, lang_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vision_features: Visual features [batch, n_patches, d_model]\n",
    "            lang_features: Language features [batch, seq_len, d_model]\n",
    "            lang_mask: Language attention mask\n",
    "        Returns:\n",
    "            fused_features: Fused multi-modal features [batch, d_model]\n",
    "        \"\"\"\n",
    "        vision_attended, _ = self.vision_to_lang(\n",
    "            vision_features, lang_features, lang_features, key_padding_mask=lang_mask\n",
    "        )\n",
    "        vision_features = self.norm1(vision_features + vision_attended)\n",
    "        \n",
    "        lang_attended, _ = self.lang_to_vision(\n",
    "            lang_features, vision_features, vision_features\n",
    "        )\n",
    "        lang_features = self.norm2(lang_features + lang_attended)\n",
    "        \n",
    "        vision_pooled = vision_features[:, 0]  # Use CLS token\n",
    "        lang_pooled = lang_features.mean(dim=1)  # Average pooling\n",
    "        \n",
    "        combined = torch.cat([vision_pooled, lang_pooled], dim=1)\n",
    "        fused = self.fusion_net(combined)\n",
    "        \n",
    "        return fused\n",
    "\n",
    "class FoundationPolicy(nn.Module):\n",
    "    \"\"\"Foundation model-based policy with vision and language understanding\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 img_size: int = 84,\n",
    "                 patch_size: int = 16, \n",
    "                 in_channels: int = 3,\n",
    "                 vocab_size: int = 1000,\n",
    "                 action_dim: int = 4,\n",
    "                 d_model: int = 256,\n",
    "                 n_heads: int = 8,\n",
    "                 n_layers: int = 6,\n",
    "                 max_seq_len: int = 64,\n",
    "                 dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "        self.vision_encoder = VisionTransformer(\n",
    "            img_size, patch_size, in_channels, d_model, n_heads, n_layers, \n",
    "            d_model * 4, dropout\n",
    "        )\n",
    "        \n",
    "        self.language_encoder = LanguageEncoder(\n",
    "            vocab_size, d_model, n_heads, n_layers // 2, max_seq_len, dropout\n",
    "        )\n",
    "        \n",
    "        self.fusion = CrossModalFusion(d_model, n_heads, dropout)\n",
    "        \n",
    "        self.policy_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, action_dim)\n",
    "        )\n",
    "        \n",
    "        self.value_head = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, 1)\n",
    "        )\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "    \n",
    "    def forward(self, images, instructions=None, instruction_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Visual observations [batch, channels, height, width]\n",
    "            instructions: Language instructions [batch, seq_len] (optional)\n",
    "            instruction_mask: Instruction attention mask [batch, seq_len]\n",
    "        Returns:\n",
    "            action_logits: Action probabilities [batch, action_dim]\n",
    "            values: State values [batch]\n",
    "            attention_info: Dictionary with attention weights for visualization\n",
    "        \"\"\"\n",
    "        vision_features, vision_attentions = self.vision_encoder(images)\n",
    "        \n",
    "        if instructions is not None:\n",
    "            lang_features = self.language_encoder(instructions, instruction_mask)\n",
    "            \n",
    "            fused_features = self.fusion(vision_features, lang_features, instruction_mask)\n",
    "        else:\n",
    "            fused_features = vision_features[:, 0]\n",
    "        \n",
    "        action_logits = self.policy_head(fused_features)\n",
    "        values = self.value_head(fused_features).squeeze(-1)\n",
    "        \n",
    "        attention_info = {\n",
    "            'vision_attentions': vision_attentions,\n",
    "            'fused_features': fused_features\n",
    "        }\n",
    "        \n",
    "        return action_logits, values, attention_info\n",
    "    \n",
    "    def get_action(self, images, instructions=None, instruction_mask=None, \n",
    "                   deterministic=False):\n",
    "        \"\"\"Get action from policy\"\"\"\n",
    "        with torch.no_grad():\n",
    "            action_logits, values, attention_info = self.forward(\n",
    "                images, instructions, instruction_mask\n",
    "            )\n",
    "            \n",
    "            if deterministic:\n",
    "                actions = torch.argmax(action_logits, dim=-1)\n",
    "            else:\n",
    "                action_dist = torch.distributions.Categorical(logits=action_logits)\n",
    "                actions = action_dist.sample()\n",
    "        \n",
    "        return actions, values, attention_info\n",
    "\n",
    "class FewShotLearner:\n",
    "    \"\"\"Few-shot learning using foundation models\"\"\"\n",
    "    \n",
    "    def __init__(self, foundation_model: FoundationPolicy):\n",
    "        self.foundation_model = foundation_model\n",
    "        self.task_examples = []\n",
    "    \n",
    "    def add_example(self, image, instruction, action, reward):\n",
    "        \"\"\"Add a few-shot example\"\"\"\n",
    "        self.task_examples.append({\n",
    "            'image': image,\n",
    "            'instruction': instruction,\n",
    "            'action': action,\n",
    "            'reward': reward\n",
    "        })\n",
    "    \n",
    "    def adapt_to_task(self, support_data, lr=1e-4, steps=10):\n",
    "        \"\"\"Adapt foundation model to new task using few examples\"\"\"\n",
    "        optimizer = torch.optim.Adam(self.foundation_model.parameters(), lr=lr)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            total_loss = 0\n",
    "            \n",
    "            for example in support_data:\n",
    "                images = example['images'].unsqueeze(0)\n",
    "                instructions = example['instructions'].unsqueeze(0)\n",
    "                actions = example['actions'].unsqueeze(0)\n",
    "                rewards = example['rewards'].unsqueeze(0)\n",
    "                \n",
    "                action_logits, values, _ = self.foundation_model(images, instructions)\n",
    "                \n",
    "                action_loss = F.cross_entropy(action_logits, actions)\n",
    "                value_loss = F.mse_loss(values, rewards)\n",
    "                \n",
    "                loss = action_loss + 0.5 * value_loss\n",
    "                total_loss += loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        return total_loss.item()\n",
    "\n",
    "class PromptTemplate:\n",
    "    \"\"\"Template for generating task prompts\"\"\"\n",
    "    \n",
    "    def __init__(self, task_type: str):\n",
    "        self.task_type = task_type\n",
    "        self.templates = {\n",
    "            'navigation': \"Navigate to {goal} while avoiding {obstacles}. Current position: {position}.\",\n",
    "            'collection': \"Collect all {objects} in the environment. Collected: {collected}/{total}.\",\n",
    "            'interaction': \"Interact with {target} to {action}. Available actions: {actions}.\",\n",
    "            'puzzle': \"Solve the puzzle by {instruction}. Current state: {state}.\"\n",
    "        }\n",
    "    \n",
    "    def generate_prompt(self, **kwargs) -> str:\n",
    "        \"\"\"Generate prompt from template\"\"\"\n",
    "        if self.task_type in self.templates:\n",
    "            return self.templates[self.task_type].format(**kwargs)\n",
    "        else:\n",
    "            return f\"Complete the task: {kwargs.get('instruction', 'Unknown task')}\"\n",
    "    \n",
    "    def tokenize_prompt(self, prompt: str, tokenizer, max_length: int = 64) -> Dict:\n",
    "        \"\"\"Tokenize prompt for model input\"\"\"\n",
    "        words = prompt.lower().split()\n",
    "        \n",
    "        vocab = {'[PAD]': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3}\n",
    "        for word in words:\n",
    "            if word not in vocab:\n",
    "                vocab[word] = len(vocab)\n",
    "        \n",
    "        tokens = [vocab.get(word, vocab['[UNK]']) for word in words]\n",
    "        \n",
    "        if len(tokens) < max_length:\n",
    "            tokens = tokens + [vocab['[PAD]']] * (max_length - len(tokens))\n",
    "            mask = [1] * len(words) + [0] * (max_length - len(words))\n",
    "        else:\n",
    "            tokens = tokens[:max_length]\n",
    "            mask = [1] * max_length\n",
    "        \n",
    "        return {\n",
    "            'tokens': torch.tensor(tokens),\n",
    "            'mask': torch.tensor(mask, dtype=torch.bool),\n",
    "            'vocab': vocab\n",
    "        }\n",
    "\n",
    "print(\"‚úÖ Foundation Models in RL implementation complete!\")\n",
    "print(\"Components implemented:\")\n",
    "print(\"- VisionTransformer: Process visual observations with attention\")\n",
    "print(\"- LanguageEncoder: Process text instructions\") \n",
    "print(\"- CrossModalFusion: Fuse vision and language representations\")\n",
    "print(\"- FoundationPolicy: Multi-modal policy with interpretable attention\")\n",
    "print(\"- FewShotLearner: Rapid task adaptation\")\n",
    "print(\"- PromptTemplate: Task specification through natural language\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9df536c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up comprehensive experiments...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiModalGridWorld' object has no attribute 'prompt_template'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 382\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSetting up comprehensive experiments...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    381\u001b[39m \u001b[38;5;66;03m# Create multi-modal environment\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m mm_env = \u001b[43mMultiModalGridWorld\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m84\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[38;5;66;03m# Test environment\u001b[39;00m\n\u001b[32m    385\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTesting multi-modal environment...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mMultiModalGridWorld.__init__\u001b[39m\u001b[34m(self, size, render_size)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mself\u001b[39m.size = size\n\u001b[32m     10\u001b[39m \u001b[38;5;28mself\u001b[39m.render_size = render_size\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Action space\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mself\u001b[39m.action_space = spaces.Discrete(\u001b[32m4\u001b[39m)  \u001b[38;5;66;03m# Up, Down, Left, Right\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mMultiModalGridWorld.reset\u001b[39m\u001b[34m(self, task_type, seed)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mself\u001b[39m.max_steps = \u001b[38;5;28mself\u001b[39m.size * \u001b[38;5;28mself\u001b[39m.size\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.task_type = task_type\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_observation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 135\u001b[39m, in \u001b[36mMultiModalGridWorld._get_observation\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;66;03m# Generate instruction\u001b[39;00m\n\u001b[32m    134\u001b[39m instruction_text = \u001b[38;5;28mself\u001b[39m._generate_instruction()\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m instruction_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprompt_template\u001b[49m.tokenize_prompt(\n\u001b[32m    136\u001b[39m     instruction_text, \u001b[38;5;28;01mNone\u001b[39;00m, max_length=\u001b[32m64\u001b[39m\n\u001b[32m    137\u001b[39m )\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    140\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m'\u001b[39m: image,\n\u001b[32m    141\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minstruction\u001b[39m\u001b[33m'\u001b[39m: instruction_data[\u001b[33m'\u001b[39m\u001b[33mtokens\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m    142\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minstruction_mask\u001b[39m\u001b[33m'\u001b[39m: instruction_data[\u001b[33m'\u001b[39m\u001b[33mmask\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    143\u001b[39m }\n",
      "\u001b[31mAttributeError\u001b[39m: 'MultiModalGridWorld' object has no attribute 'prompt_template'"
     ]
    }
   ],
   "source": [
    "\n",
    "class MultiModalGridWorld(gym.Env):\n",
    "    \"\"\"Enhanced GridWorld with visual rendering and language instructions\"\"\"\n",
    "    \n",
    "    def __init__(self, size=8, render_size=84):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.render_size = render_size\n",
    "        self.reset()\n",
    "        \n",
    "        self.action_space = spaces.Discrete(4)  # Up, Down, Left, Right\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            'image': spaces.Box(low=0, high=255, shape=(3, render_size, render_size), dtype=np.uint8),\n",
    "            'instruction': spaces.Box(low=0, high=1000, shape=(64,), dtype=np.int32),\n",
    "            'instruction_mask': spaces.Box(low=0, high=1, shape=(64,), dtype=np.bool_)\n",
    "        })\n",
    "        \n",
    "        self.actions = {0: [-1, 0], 1: [1, 0], 2: [0, -1], 3: [0, 1]}\n",
    "        self.action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "        \n",
    "        self.tasks = [\n",
    "            'navigation', 'collection', 'avoidance', 'exploration'\n",
    "        ]\n",
    "        \n",
    "        self.prompt_template = PromptTemplate('navigation')\n",
    "    \n",
    "    def reset(self, task_type='navigation', seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        \n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [self.size-1, self.size-1]\n",
    "        \n",
    "        self.obstacles = set()\n",
    "        self.treasures = set()\n",
    "        self.visited = {tuple(self.agent_pos)}\n",
    "        \n",
    "        for _ in range(self.size // 2):\n",
    "            x, y = np.random.randint(1, self.size-1), np.random.randint(1, self.size-1)\n",
    "            if [x, y] not in [self.agent_pos, self.goal_pos]:\n",
    "                self.obstacles.add((x, y))\n",
    "        \n",
    "        if task_type == 'collection':\n",
    "            for _ in range(3):\n",
    "                x, y = np.random.randint(0, self.size), np.random.randint(0, self.size)\n",
    "                if ([x, y] not in [self.agent_pos, self.goal_pos] and \n",
    "                    (x, y) not in self.obstacles):\n",
    "                    self.treasures.add((x, y))\n",
    "        \n",
    "        self.collected_treasures = set()\n",
    "        self.step_count = 0\n",
    "        self.max_steps = self.size * self.size\n",
    "        self.task_type = task_type\n",
    "        \n",
    "        return self._get_observation(), {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        old_pos = self.agent_pos.copy()\n",
    "        new_pos = [\n",
    "            self.agent_pos[0] + self.actions[action][0],\n",
    "            self.agent_pos[1] + self.actions[action][1]\n",
    "        ]\n",
    "        \n",
    "        reward = -0.1  # Step penalty\n",
    "        \n",
    "        if (0 <= new_pos[0] < self.size and 0 <= new_pos[1] < self.size and \n",
    "            tuple(new_pos) not in self.obstacles):\n",
    "            self.agent_pos = new_pos\n",
    "            self.visited.add(tuple(new_pos))\n",
    "            \n",
    "            if self.task_type == 'navigation':\n",
    "                old_dist = abs(old_pos[0] - self.goal_pos[0]) + abs(old_pos[1] - self.goal_pos[1])\n",
    "                new_dist = abs(new_pos[0] - self.goal_pos[0]) + abs(new_pos[1] - self.goal_pos[1])\n",
    "                reward += (old_dist - new_dist) * 0.1\n",
    "                \n",
    "                if new_pos == self.goal_pos:\n",
    "                    reward += 10\n",
    "                    \n",
    "            elif self.task_type == 'collection':\n",
    "                if tuple(new_pos) in self.treasures and tuple(new_pos) not in self.collected_treasures:\n",
    "                    self.collected_treasures.add(tuple(new_pos))\n",
    "                    reward += 5\n",
    "                    \n",
    "                if len(self.collected_treasures) == len(self.treasures):\n",
    "                    reward += 20\n",
    "                    \n",
    "            elif self.task_type == 'exploration':\n",
    "                if tuple(new_pos) not in self.visited:\n",
    "                    reward += 1\n",
    "        else:\n",
    "            reward -= 1  # Collision penalty\n",
    "        \n",
    "        self.step_count += 1\n",
    "        \n",
    "        terminated = self._check_termination()\n",
    "        \n",
    "        return self._get_observation(), reward, terminated, False, {}\n",
    "    \n",
    "    def _check_termination(self):\n",
    "        if self.task_type == 'navigation':\n",
    "            return self.agent_pos == self.goal_pos or self.step_count >= self.max_steps\n",
    "        elif self.task_type == 'collection':\n",
    "            return (len(self.collected_treasures) == len(self.treasures) or \n",
    "                   self.step_count >= self.max_steps)\n",
    "        elif self.task_type == 'exploration':\n",
    "            return (len(self.visited) >= self.size * self.size * 0.8 or \n",
    "                   self.step_count >= self.max_steps)\n",
    "        return self.step_count >= self.max_steps\n",
    "    \n",
    "    def _get_observation(self):\n",
    "        \"\"\"Get multi-modal observation\"\"\"\n",
    "        image = self._render_image()\n",
    "        \n",
    "        instruction_text = self._generate_instruction()\n",
    "        instruction_data = self.prompt_template.tokenize_prompt(\n",
    "            instruction_text, None, max_length=64\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'instruction': instruction_data['tokens'],\n",
    "            'instruction_mask': instruction_data['mask']\n",
    "        }\n",
    "    \n",
    "    def _render_image(self):\n",
    "        \"\"\"Render environment as RGB image\"\"\"\n",
    "        image = np.ones((3, self.render_size, self.render_size), dtype=np.uint8) * 255\n",
    "        \n",
    "        cell_size = self.render_size // self.size\n",
    "        \n",
    "        for i in range(self.size + 1):\n",
    "            x = i * cell_size\n",
    "            image[:, x:x+1, :] = 200\n",
    "            y = i * cell_size\n",
    "            image[:, :, y:y+1] = 200\n",
    "        \n",
    "        for obs_x, obs_y in self.obstacles:\n",
    "            x1, x2 = obs_x * cell_size, (obs_x + 1) * cell_size\n",
    "            y1, y2 = obs_y * cell_size, (obs_y + 1) * cell_size\n",
    "            image[:, x1:x2, y1:y2] = 0\n",
    "        \n",
    "        for treasure_x, treasure_y in self.treasures:\n",
    "            if (treasure_x, treasure_y) not in self.collected_treasures:\n",
    "                x1, x2 = treasure_x * cell_size, (treasure_x + 1) * cell_size\n",
    "                y1, y2 = treasure_y * cell_size, (treasure_y + 1) * cell_size\n",
    "                image[0, x1:x2, y1:y2] = 255  # Red\n",
    "                image[1, x1:x2, y1:y2] = 255  # Green  \n",
    "                image[2, x1:x2, y1:y2] = 0    # Blue (Yellow = Red + Green)\n",
    "        \n",
    "        goal_x, goal_y = self.goal_pos\n",
    "        x1, x2 = goal_x * cell_size, (goal_x + 1) * cell_size\n",
    "        y1, y2 = goal_y * cell_size, (goal_y + 1) * cell_size\n",
    "        image[0, x1:x2, y1:y2] = 0    # Red\n",
    "        image[1, x1:x2, y1:y2] = 255  # Green\n",
    "        image[2, x1:x2, y1:y2] = 0    # Blue\n",
    "        \n",
    "        agent_x, agent_y = self.agent_pos\n",
    "        x1, x2 = agent_x * cell_size, (agent_x + 1) * cell_size  \n",
    "        y1, y2 = agent_y * cell_size, (agent_y + 1) * cell_size\n",
    "        image[0, x1:x2, y1:y2] = 255  # Red\n",
    "        image[1, x1:x2, y1:y2] = 0    # Green\n",
    "        image[2, x1:x2, y1:y2] = 0    # Blue\n",
    "        \n",
    "        return image\n",
    "    \n",
    "    def _generate_instruction(self):\n",
    "        \"\"\"Generate natural language instruction\"\"\"\n",
    "        if self.task_type == 'navigation':\n",
    "            return f\"Navigate to the goal at position ({self.goal_pos[0]}, {self.goal_pos[1]}). \" \\\n",
    "                   f\"Current position: ({self.agent_pos[0]}, {self.agent_pos[1]}). \" \\\n",
    "                   f\"Avoid obstacles and find the shortest path.\"\n",
    "        elif self.task_type == 'collection':\n",
    "            total = len(self.treasures)\n",
    "            collected = len(self.collected_treasures)\n",
    "            return f\"Collect all {total} treasures. Progress: {collected}/{total} collected. \" \\\n",
    "                   f\"Yellow squares are treasures. Current position: ({self.agent_pos[0]}, {self.agent_pos[1]}).\"\n",
    "        elif self.task_type == 'exploration':\n",
    "            explored = len(self.visited)\n",
    "            total_cells = self.size * self.size\n",
    "            return f\"Explore the environment. Visit at least 80% of cells. \" \\\n",
    "                   f\"Progress: {explored}/{total_cells} cells visited.\"\n",
    "        else:\n",
    "            return f\"Complete the task. Current position: ({self.agent_pos[0]}, {self.agent_pos[1]}).\"\n",
    "\n",
    "def visualize_attention_maps(model, observation, save_path=None):\n",
    "    \"\"\"Visualize attention maps from foundation model\"\"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        images = observation['image'].unsqueeze(0).float() / 255.0\n",
    "        instructions = observation['instruction'].unsqueeze(0)\n",
    "        instruction_mask = observation['instruction_mask'].unsqueeze(0)\n",
    "        \n",
    "        action_logits, values, attention_info = model(images, instructions, instruction_mask)\n",
    "        \n",
    "        vision_attention = attention_info['vision_attentions'][-1][0]  # [n_heads, n_patches+1, n_patches+1]\n",
    "        \n",
    "        cls_attention = vision_attention.mean(0)[0, 1:]  # [n_patches]\n",
    "        \n",
    "        n_patches_per_dim = int(np.sqrt(len(cls_attention)))\n",
    "        attention_map = cls_attention.reshape(n_patches_per_dim, n_patches_per_dim)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        \n",
    "        original_image = images[0].permute(1, 2, 0).numpy()\n",
    "        axes[0].imshow(original_image)\n",
    "        axes[0].set_title('Original Image')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        im = axes[1].imshow(attention_map.numpy(), cmap='hot', interpolation='bilinear')\n",
    "        axes[1].set_title('Vision Attention Map')\n",
    "        axes[1].axis('off')\n",
    "        plt.colorbar(im, ax=axes[1])\n",
    "        \n",
    "        from scipy.ndimage import zoom\n",
    "        attention_resized = zoom(attention_map.numpy(), \n",
    "                               (original_image.shape[0] / attention_map.shape[0],\n",
    "                                original_image.shape[1] / attention_map.shape[1]))\n",
    "        \n",
    "        axes[2].imshow(original_image)\n",
    "        axes[2].imshow(attention_resized, alpha=0.6, cmap='hot')\n",
    "        axes[2].set_title('Attention Overlay')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "        plt.show()\n",
    "        \n",
    "        action_probs = F.softmax(action_logits[0], dim=0)\n",
    "        action_names = ['Up', 'Down', 'Left', 'Right']\n",
    "        \n",
    "        print(\"Action Predictions:\")\n",
    "        for i, (action, prob) in enumerate(zip(action_names, action_probs)):\n",
    "            print(f\"  {action}: {prob:.3f}\")\n",
    "        print(f\"Predicted Value: {values[0]:.3f}\")\n",
    "\n",
    "def compare_models_performance(environments, models, episodes_per_env=100):\n",
    "    \"\"\"Compare performance of different models across environments\"\"\"\n",
    "    \n",
    "    results = {model_name: {env_name: [] for env_name in environments.keys()} \n",
    "              for model_name in models.keys()}\n",
    "    \n",
    "    for env_name, env in environments.items():\n",
    "        print(f\"\\nTesting environment: {env_name}\")\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"  Testing model: {model_name}\")\n",
    "            episode_rewards = []\n",
    "            \n",
    "            for episode in range(episodes_per_env):\n",
    "                obs, _ = env.reset()\n",
    "                episode_reward = 0\n",
    "                done = False\n",
    "                \n",
    "                while not done:\n",
    "                    if hasattr(model, 'get_action'):\n",
    "                        images = torch.FloatTensor(obs['image']).unsqueeze(0) / 255.0\n",
    "                        instructions = obs['instruction'].unsqueeze(0)\n",
    "                        instruction_mask = obs['instruction_mask'].unsqueeze(0)\n",
    "                        \n",
    "                        action, _, _ = model.get_action(images, instructions, instruction_mask)\n",
    "                        action = action.item()\n",
    "                    else:\n",
    "                        action = env.action_space.sample()\n",
    "                    \n",
    "                    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "                    done = terminated or truncated\n",
    "                    episode_reward += reward\n",
    "                \n",
    "                episode_rewards.append(episode_reward)\n",
    "            \n",
    "            results[model_name][env_name] = episode_rewards\n",
    "            avg_reward = np.mean(episode_rewards)\n",
    "            std_reward = np.std(episode_rewards)\n",
    "            print(f\"    Average reward: {avg_reward:.2f} ¬± {std_reward:.2f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_learning_curves(training_histories, save_path=None):\n",
    "    \"\"\"Plot learning curves for different approaches\"\"\"\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    for name, history in training_histories.items():\n",
    "        episodes = range(len(history['rewards']))\n",
    "        plt.plot(episodes, history['rewards'], label=name, alpha=0.7)\n",
    "        \n",
    "        if len(history['rewards']) > 10:\n",
    "            window = min(50, len(history['rewards']) // 10)\n",
    "            moving_avg = np.convolve(history['rewards'], np.ones(window)/window, mode='valid')\n",
    "            plt.plot(range(window-1, len(history['rewards'])), moving_avg, \n",
    "                    label=f'{name} (MA)', linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.title('Learning Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    for name, history in training_histories.items():\n",
    "        if 'losses' in history and history['losses']:\n",
    "            episodes = range(len(history['losses']))\n",
    "            plt.plot(episodes, history['losses'], label=name, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    for name, history in training_histories.items():\n",
    "        if 'success_rate' in history and history['success_rate']:\n",
    "            episodes = range(len(history['success_rate']))\n",
    "            plt.plot(episodes, history['success_rate'], label=name, alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.title('Success Rate Over Time')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    plt.show()\n",
    "\n",
    "print(\"Setting up comprehensive experiments...\")\n",
    "\n",
    "mm_env = MultiModalGridWorld(size=6, render_size=84)\n",
    "\n",
    "print(\"Testing multi-modal environment...\")\n",
    "obs, _ = mm_env.reset(task_type='navigation')\n",
    "print(f\"Image shape: {obs['image'].shape}\")\n",
    "print(f\"Instruction tokens: {obs['instruction'].shape}\")\n",
    "print(f\"Instruction mask: {obs['instruction_mask'].shape}\")\n",
    "\n",
    "print(\"Creating foundation model...\")\n",
    "foundation_model = FoundationPolicy(\n",
    "    img_size=84,\n",
    "    patch_size=16,\n",
    "    in_channels=3,\n",
    "    vocab_size=1000,\n",
    "    action_dim=4,\n",
    "    d_model=128,  # Smaller for demo\n",
    "    n_heads=4,\n",
    "    n_layers=3,\n",
    "    max_seq_len=64,\n",
    "    dropout=0.1\n",
    ")\n",
    "\n",
    "print(\"Testing foundation model...\")\n",
    "images = torch.FloatTensor(obs['image']).unsqueeze(0) / 255.0\n",
    "instructions = obs['instruction'].unsqueeze(0)\n",
    "instruction_mask = obs['instruction_mask'].unsqueeze(0)\n",
    "\n",
    "action_logits, values, attention_info = foundation_model(images, instructions, instruction_mask)\n",
    "print(f\"Action logits shape: {action_logits.shape}\")\n",
    "print(f\"Values shape: {values.shape}\")\n",
    "print(f\"Number of attention layers: {len(attention_info['vision_attentions'])}\")\n",
    "\n",
    "print(\"\\n‚úÖ Comprehensive experimental setup complete!\")\n",
    "print(\"Available functions:\")\n",
    "print(\"- visualize_attention_maps(): Visualize model attention\")  \n",
    "print(\"- compare_models_performance(): Compare different approaches\")\n",
    "print(\"- plot_learning_curves(): Plot training progress\")\n",
    "print(\"\\nEnvironment supports multiple task types: 'navigation', 'collection', 'exploration'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dba8e3",
   "metadata": {},
   "source": [
    "# Conclusion and Future Directions\n",
    "\n",
    "## Summary of Advanced Deep RL Concepts\n",
    "\n",
    "This notebook has explored cutting-edge topics in Deep Reinforcement Learning that represent the current frontier of research and applications. We covered four major paradigms:\n",
    "\n",
    "### 1. Continual Learning in RL\n",
    "- **Key Insight**: Agents must learn new tasks while retaining knowledge from previous experiences\n",
    "- **Main Challenges**: Catastrophic forgetting, interference between tasks, scalability\n",
    "- **Solutions**: Elastic Weight Consolidation, Progressive Networks, Meta-learning approaches\n",
    "- **Applications**: Robotics, adaptive systems, lifelong learning agents\n",
    "\n",
    "### 2. Neurosymbolic Reinforcement Learning  \n",
    "- **Key Insight**: Combining neural learning with symbolic reasoning for interpretable and robust agents\n",
    "- **Main Challenges**: Integration of continuous and discrete representations, knowledge representation\n",
    "- **Solutions**: Differentiable programming, logic-based constraints, hybrid architectures\n",
    "- **Applications**: Autonomous systems, healthcare, safety-critical domains\n",
    "\n",
    "### 3. Human-AI Collaborative Learning\n",
    "- **Key Insight**: Leverage human expertise and feedback to improve agent learning and performance\n",
    "- **Main Challenges**: Trust modeling, preference learning, real-time collaboration\n",
    "- **Solutions**: RLHF, preference-based rewards, shared autonomy frameworks\n",
    "- **Applications**: Human-robot interaction, personalized AI, assisted decision-making\n",
    "\n",
    "### 4. Foundation Models in RL\n",
    "- **Key Insight**: Pre-trained large models enable sample-efficient learning and strong generalization\n",
    "- **Main Challenges**: Transfer learning, multi-modal integration, computational efficiency\n",
    "- **Solutions**: Vision transformers, cross-modal attention, prompt engineering\n",
    "- **Applications**: General-purpose AI agents, few-shot learning, multi-task systems\n",
    "\n",
    "## Interconnections Between Paradigms\n",
    "\n",
    "These four approaches are not isolated but can be combined synergistically:\n",
    "\n",
    "**Continual + Neurosymbolic**: Symbolic knowledge provides structure for continual learning, preventing catastrophic forgetting through logical constraints.\n",
    "\n",
    "**Human-AI + Foundation Models**: Foundation models provide better initialization for human-AI collaboration, while human feedback can guide foundation model fine-tuning.\n",
    "\n",
    "**Neurosymbolic + Foundation Models**: Foundation models can learn to perform symbolic reasoning, while symbolic structures can guide foundation model architectures.\n",
    "\n",
    "**All Four Combined**: A truly advanced RL system might use foundation models as initialization, incorporate human feedback for alignment, use symbolic reasoning for interpretability, and support continual learning for adaptation.\n",
    "\n",
    "## Current Research Frontiers\n",
    "\n",
    "### Emerging Challenges\n",
    "1. **Scalability**: How do these methods scale to real-world complexity?\n",
    "2. **Sample Efficiency**: Can we achieve superhuman performance with minimal data?\n",
    "3. **Robustness**: How do agents handle distribution shifts and adversarial conditions?\n",
    "4. **Alignment**: How do we ensure AI systems pursue intended objectives?\n",
    "5. **Interpretability**: Can we understand and verify agent decision-making?\n",
    "\n",
    "### Promising Directions\n",
    "1. **Unified Architectures**: Single models that combine multiple paradigms\n",
    "2. **Meta-Learning**: Learning to learn across paradigms and domains\n",
    "3. **Causal Reasoning**: Understanding cause-and-effect relationships\n",
    "4. **Compositional Learning**: Building complex behaviors from simple primitives\n",
    "5. **Multi-Agent Collaboration**: Scaling human-AI collaboration to teams\n",
    "\n",
    "## Practical Implementation Insights\n",
    "\n",
    "### Key Lessons Learned\n",
    "1. **Start Simple**: Begin with simplified versions before adding complexity\n",
    "2. **Modular Design**: Build components that can be combined and reused\n",
    "3. **Interpretability First**: Design for explainability from the beginning\n",
    "4. **Human-Centered**: Consider human factors in system design\n",
    "5. **Robust Evaluation**: Test across diverse scenarios and failure modes\n",
    "\n",
    "### Implementation Best Practices\n",
    "1. **Gradual Integration**: Introduce new paradigms incrementally\n",
    "2. **Ablation Studies**: Understand the contribution of each component\n",
    "3. **Multi-Metric Evaluation**: Use diverse evaluation criteria beyond reward\n",
    "4. **Failure Analysis**: Learn from failures and edge cases\n",
    "5. **Ethical Considerations**: Address bias, fairness, and safety concerns\n",
    "\n",
    "## Future Applications\n",
    "\n",
    "### Near-Term (1-3 years)\n",
    "- **Personalized AI Assistants**: Agents that adapt to individual preferences and learn continuously\n",
    "- **Robotic Process Automation**: Intelligent automation that can handle exceptions and learn from feedback\n",
    "- **Educational AI**: Tutoring systems that adapt teaching strategies based on student progress\n",
    "- **Healthcare Support**: AI systems that assist medical professionals with decision-making\n",
    "\n",
    "### Medium-Term (3-7 years)\n",
    "- **Autonomous Vehicles**: Self-driving cars that learn from human drivers and adapt to new environments\n",
    "- **Smart Cities**: Urban systems that optimize resource allocation through continuous learning\n",
    "- **Scientific Discovery**: AI agents that collaborate with researchers to generate and test hypotheses\n",
    "- **Creative AI**: Systems that collaborate with humans in creative endeavors\n",
    "\n",
    "### Long-Term (7+ years)\n",
    "- **General Intelligence**: AI systems that can perform any cognitive task that humans can do\n",
    "- **Scientific AI**: Autonomous systems capable of conducting independent scientific research\n",
    "- **Collaborative Societies**: Seamless integration of human and AI capabilities in all aspects of society\n",
    "- **Space Exploration**: AI systems capable of autonomous operation in extreme and unknown environments\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The field of Deep Reinforcement Learning continues to evolve rapidly, with these advanced paradigms representing the current cutting edge. Each approach addresses fundamental limitations of traditional RL and opens new possibilities for creating more capable, reliable, and aligned AI systems.\n",
    "\n",
    "The key to success in this field is not just understanding individual techniques, but recognizing how they can be combined to create systems that are greater than the sum of their parts. As we move forward, the most impactful advances will likely come from principled integration of these paradigms with careful attention to real-world constraints and human values.\n",
    "\n",
    "### Final Recommendations for Further Learning\n",
    "\n",
    "1. **Hands-On Implementation**: Build and experiment with these systems yourself\n",
    "2. **Stay Current**: Follow recent papers and conferences (NeurIPS, ICML, ICLR, AAAI)\n",
    "3. **Interdisciplinary Learning**: Study cognitive science, philosophy, and domain-specific knowledge\n",
    "4. **Community Engagement**: Participate in research communities and open-source projects\n",
    "5. **Ethical Reflection**: Consider the societal implications of your work\n",
    "\n",
    "The future of AI lies not just in more powerful algorithms, but in systems that can learn, reason, collaborate, and adapt in ways that align with human values and capabilities. These advanced RL paradigms provide the building blocks for that future.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You have completed CA16 - Advanced Topics in Deep Reinforcement Learning**\n",
    "\n",
    "This comprehensive exploration has covered the most cutting-edge approaches in modern RL research. You now have the theoretical foundations and practical implementation skills to contribute to the next generation of intelligent systems.\n",
    "\n",
    "*\"The best way to predict the future is to invent it.\"* - Alan Kay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c484b4d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b723a23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb4c3e26",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}