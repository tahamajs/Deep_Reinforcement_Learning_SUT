{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA16: Cutting-Edge Deep Reinforcement Learning\n",
    "## Foundation Models, Neurosymbolic RL, and Future Paradigms\n",
    "\n",
    "Welcome to the final assignment exploring the absolute frontiers of deep reinforcement learning! This comprehensive notebook covers:\n",
    "\n",
    "### ðŸ§  **Core Advanced Paradigms**\n",
    "- **Foundation Models in RL**: Decision Transformers, Multi-task Pre-training, In-context Learning\n",
    "- **Neurosymbolic RL**: Logic-guided Policies, Interpretable Decision Making, Causal Reasoning\n",
    "- **Human-AI Collaboration**: Learning from Human Feedback (RLHF), Preference Learning, Trust Modeling\n",
    "- **Continual Learning**: Catastrophic Forgetting Prevention, Meta-learning, Lifelong Adaptation\n",
    "\n",
    "### âš¡ **Advanced Computing Paradigms**\n",
    "- **Quantum RL**: Quantum-enhanced Optimization, Amplitude Estimation\n",
    "- **Neuromorphic Computing**: Brain-inspired Architectures, Energy-efficient Learning\n",
    "- **Distributed & Federated RL**: Multi-agent Systems, Privacy-preserving Learning\n",
    "\n",
    "### ðŸš€ **Real-World Deployment**\n",
    "- **Production Systems**: Monitoring, A/B Testing, Scalability\n",
    "- **Ethics & Safety**: Bias Detection, Value Alignment, Regulatory Compliance\n",
    "- **Future Research**: Emerging Trends, Research Directions\n",
    "\n",
    "---\n",
    "\n",
    "**Learning Objectives**: By the end of this notebook, you will understand and implement state-of-the-art RL techniques that represent the cutting edge of AI research and the future of intelligent agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Environment Setup and Configuration\n",
    "\n",
    "This section sets up the complete environment for advanced RL experiments, including all necessary libraries and configurations for cutting-edge implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Silence warnings and set styles\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Ensure CA16 package is importable when running the notebook directly (works in Jupyter)\n",
    "try:\n",
    "    from IPython import get_ipython\n",
    "    ipy = get_ipython()\n",
    "    if ipy is not None and hasattr(ipy, 'starting_dir'):\n",
    "        PROJECT_ROOT = os.path.abspath(ipy.starting_dir)\n",
    "    else:\n",
    "        PROJECT_ROOT = os.path.abspath('.')\n",
    "except Exception:\n",
    "    PROJECT_ROOT = os.path.abspath('.')\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.insert(0, PROJECT_ROOT)\n",
    "\n",
    "# Foundation models\n",
    "from foundation_models import (\n",
    "    DecisionTransformer,\n",
    "    FoundationModelTrainer,\n",
    ")\n",
    "\n",
    "# Neurosymbolic RL\n",
    "from neurosymbolic import (\n",
    "    NeurosymbolicAgent,\n",
    "    SymbolicKnowledgeBase,\n",
    ")\n",
    "\n",
    "# Human-AI Collaboration\n",
    "from human_ai_collaboration import (\n",
    "    CollaborativeAgent,\n",
    ")\n",
    "\n",
    "# Continual Learning\n",
    "from continual_learning import (\n",
    "    ContinualLearningAgent,\n",
    ")\n",
    "\n",
    "# Advanced Computational (optional availability per environment)\n",
    "try:\n",
    "    from advanced_computational import QuantumInspiredRL, NeuromorphicNetwork  # type: ignore\n",
    "    ADVANCED_AVAILABLE = True\n",
    "except Exception:\n",
    "    ADVANCED_AVAILABLE = False\n",
    "\n",
    "# Real-world Deployment (optional availability per environment)\n",
    "try:\n",
    "    from real_world_deployment import ProductionRLSystem, SafetyMonitor  # type: ignore\n",
    "    DEPLOYMENT_AVAILABLE = True\n",
    "except Exception:\n",
    "    DEPLOYMENT_AVAILABLE = False\n",
    "\n",
    "# Environments\n",
    "from environments import SymbolicGridWorld, CollaborativeGridWorld\n",
    "\n",
    "print(f\"Using torch {torch.__version__}, CUDA: {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Foundation Models: simple forward on dummy data\n",
    "state_dim, action_dim, seq_len, batch = 8, 4, 5, 2\n",
    "states = torch.randn(batch, seq_len, state_dim)\n",
    "actions = torch.zeros(batch, seq_len, action_dim)  # placeholder\n",
    "returns_to_go = torch.randn(batch, seq_len)\n",
    "timesteps = torch.arange(seq_len).unsqueeze(0).repeat(batch, 1)\n",
    "\n",
    "try:\n",
    "    dt = DecisionTransformer(state_dim=state_dim, action_dim=action_dim, model_dim=64, num_heads=4, num_layers=2)\n",
    "    preds = dt(states, actions, returns_to_go, timesteps)\n",
    "    print(\"DecisionTransformer OK:\", preds.shape)\n",
    "except Exception as e:\n",
    "    print(\"DecisionTransformer error:\", e)\n",
    "\n",
    "# Neurosymbolic: build minimal KB and run one forward\n",
    "try:\n",
    "    kb = SymbolicKnowledgeBase()\n",
    "    ns_agent = NeurosymbolicAgent(state_dim=state_dim, action_dim=action_dim, knowledge_base=kb)\n",
    "    logits, values, info = ns_agent.policy(torch.randn(3, state_dim))\n",
    "    print(\"NeurosymbolicPolicy OK:\", logits.shape, values.shape)\n",
    "except Exception as e:\n",
    "    print(\"Neurosymbolic components error:\", e)\n",
    "\n",
    "# Human-AI Collaboration: forward pass\n",
    "try:\n",
    "    collab = CollaborativeAgent(state_dim=state_dim, action_dim=action_dim)\n",
    "    a, conf = collab.select_action(torch.randn(state_dim))\n",
    "    print(\"CollaborativeAgent OK: action=\", a, \"conf=\", round(conf, 3))\n",
    "except Exception as e:\n",
    "    print(\"CollaborativeAgent error:\", e)\n",
    "\n",
    "# Environments: step a few times\n",
    "try:\n",
    "    env1 = SymbolicGridWorld(size=6)\n",
    "    obs, _ = env1.reset()\n",
    "    total = 0.0\n",
    "    for _ in range(5):\n",
    "        act = np.random.randint(0, 4)\n",
    "        obs, r, done, _, _ = env1.step(act)\n",
    "        total += r\n",
    "        if done:\n",
    "            break\n",
    "    print(\"SymbolicGridWorld OK: steps run, total_reward=\", round(float(total), 2))\n",
    "except Exception as e:\n",
    "    print(\"SymbolicGridWorld error:\", e)\n",
    "\n",
    "try:\n",
    "    env2 = CollaborativeGridWorld(size=6)\n",
    "    obs, _ = env2.reset()\n",
    "    act = np.random.randint(0, 4)\n",
    "    obs, r, done, _, _ = env2.step(act)\n",
    "    print(\"CollaborativeGridWorld OK: one step\")\n",
    "except Exception as e:\n",
    "    print(\"CollaborativeGridWorld error:\", e)\n",
    "\n",
    "# Optional modules presence\n",
    "print(\"Advanced computational available:\", 'Yes' if 'ADVANCED_AVAILABLE' in globals() and ADVANCED_AVAILABLE else 'No')\n",
    "print(\"Deployment module available:\", 'Yes' if 'DEPLOYMENT_AVAILABLE' in globals() and DEPLOYMENT_AVAILABLE else 'No')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device and seed setup\n",
    "import random\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "print('Device:', device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal training demo: DecisionTransformer supervised step\n",
    "state_dim, action_dim, seq_len, batch = 8, 4, 5, 8\n",
    "states = torch.randn(batch, seq_len, state_dim).to(device)\n",
    "actions = torch.zeros(batch, seq_len, action_dim).to(device)\n",
    "# make a simple target: one-hot of argmax of random logits\n",
    "target_idx = torch.randint(0, action_dim, (batch, seq_len), device=device)\n",
    "for b in range(batch):\n",
    "    for t in range(seq_len):\n",
    "        actions[b, t, target_idx[b, t]] = 1.0\n",
    "returns_to_go = torch.randn(batch, seq_len).to(device)\n",
    "timesteps = torch.arange(seq_len, device=device).unsqueeze(0).repeat(batch, 1)\n",
    "\n",
    "model = DecisionTransformer(state_dim=state_dim, action_dim=action_dim, model_dim=64, num_heads=4, num_layers=2).to(device)\n",
    "trainer = FoundationModelTrainer(model, lr=3e-4, weight_decay=1e-4, device=str(device))\n",
    "loss = trainer.train_step(states, actions, returns_to_go, timesteps)\n",
    "print('DecisionTransformer train_step loss:', round(float(loss), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal update demo: NeurosymbolicAgent\n",
    "state_dim, action_dim = 8, 4\n",
    "kb = SymbolicKnowledgeBase()\n",
    "ns_agent = NeurosymbolicAgent(state_dim=state_dim, action_dim=action_dim, knowledge_base=kb, lr=1e-3)\n",
    "\n",
    "# Fake batch\n",
    "batch_size = 16\n",
    "states = torch.randn(batch_size, state_dim)\n",
    "actions = torch.randint(0, action_dim, (batch_size,))\n",
    "rewards = torch.randn(batch_size).clamp(min=-1.0, max=1.0)\n",
    "advantages = torch.randn(batch_size)\n",
    "\n",
    "info = ns_agent.update(states, actions, rewards, advantages)\n",
    "print('NeurosymbolicAgent update total_loss:', round(float(info['total_loss']), 4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minimal rollout: CollaborativeAgent + CollaborativeGridWorld\n",
    "try:\n",
    "    env = CollaborativeGridWorld(size=6)\n",
    "    obs, _ = env.reset()\n",
    "    state_tensor = torch.FloatTensor(obs)\n",
    "    collab = CollaborativeAgent(state_dim=obs.shape[0], action_dim=4)\n",
    "    action, conf = collab.select_action(state_tensor)\n",
    "    obs, reward, done, _, _ = env.step(action)\n",
    "    print('Collaborative rollout one step -> reward:', round(float(reward), 2))\n",
    "except Exception as e:\n",
    "    print('Collaborative demo error:', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents and References\n",
    "\n",
    "- [Section 1: Foundation Models](#section-1-foundation-models)\n",
    "- [Section 2: Neurosymbolic RL](#section-2-neurosymbolic-rl)\n",
    "- [Section 3: Human-AI Collaboration](#section-3-human-ai-collaboration)\n",
    "- [Section 4: Quick Summary](#section-4-quick-summary)\n",
    "\n",
    "Reference: see course notes `notes_related/15.pdf` (Lecture 15) for background on scaling laws, decision transformers, and interpretability guidance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Foundation Models\n",
    "\n",
    "This section demonstrates a compact Decision Transformer setup inspired by CA17's structured presentation style and Lecture 15 references.\n",
    "\n",
    "- Sequence modeling of trajectories: tokens `[return, state, action]`\n",
    "- Objective: predict next action given context and desired return-to-go\n",
    "- Visualization: training loss curve and example action probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a few steps and visualize loss + sample action distribution\n",
    "losses = []\n",
    "for step in range(25):\n",
    "    states = torch.randn(batch, seq_len, state_dim).to(device)\n",
    "    actions = torch.zeros(batch, seq_len, action_dim).to(device)\n",
    "    target_idx = torch.randint(0, action_dim, (batch, seq_len), device=device)\n",
    "    actions.zero_()\n",
    "    actions.scatter_(2, target_idx.unsqueeze(-1), 1.0)\n",
    "    returns_to_go = torch.randn(batch, seq_len).to(device)\n",
    "    timesteps = torch.arange(seq_len, device=device).unsqueeze(0).repeat(batch, 1)\n",
    "    loss = trainer.train_step(states, actions, returns_to_go, timesteps)\n",
    "    losses.append(loss)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(losses, label='train loss')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('DecisionTransformer training (toy)')\n",
    "plt.legend(); plt.show()\n",
    "\n",
    "with torch.no_grad():\n",
    "    test_states = torch.randn(1, seq_len, state_dim).to(device)\n",
    "    test_actions = torch.zeros(1, seq_len, action_dim).to(device)\n",
    "    test_returns = torch.randn(1, seq_len).to(device)\n",
    "    test_timesteps = torch.arange(seq_len, device=device).unsqueeze(0)\n",
    "    logits = model(test_states, test_actions, test_returns, test_timesteps)[0, -1]\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.bar(np.arange(action_dim), probs)\n",
    "plt.title('Sample action probabilities')\n",
    "plt.xlabel('Action'); plt.ylabel('Probability'); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Neurosymbolic RL\n",
    "\n",
    "We combine neural perception with a symbolic knowledge base. Inspired by Lecture 15's emphasis on interpretability, we visualize:\n",
    "\n",
    "- Neural vs symbolic features contribution\n",
    "- Rule weight magnitudes\n",
    "- Attention weights from the perception module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neurosymbolic interpretability plots\n",
    "kb = SymbolicKnowledgeBase()\n",
    "ns_agent = NeurosymbolicAgent(state_dim=state_dim, action_dim=action_dim, knowledge_base=kb, lr=1e-3)\n",
    "\n",
    "sample_states = torch.randn(8, state_dim)\n",
    "with torch.no_grad():\n",
    "    logits, values, info = ns_agent.policy(sample_states)\n",
    "\n",
    "neural = info['neural_features'].cpu().numpy()\n",
    "symbolic = info['symbolic_features'].cpu().numpy()\n",
    "rule_weights = torch.sigmoid(info['rule_weights']).cpu().numpy()\n",
    "attn = info['attention_weights'][0].cpu().numpy() if isinstance(info['attention_weights'], (list, tuple)) else info['attention_weights'].cpu().numpy()\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(np.vstack([neural.mean(0), symbolic.mean(0)]), aspect='auto', cmap='viridis')\n",
    "plt.yticks([0,1],[\"neural\",\"symbolic\"])\n",
    "plt.title('Feature contributions')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.bar(np.arange(len(rule_weights)), rule_weights)\n",
    "plt.title('Rule weights (sigmoid)')\n",
    "plt.xlabel('Rule index')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "if attn.ndim == 3:\n",
    "    attn_map = attn.mean(0).squeeze()\n",
    "    plt.imshow(attn_map, cmap='magma')\n",
    "    plt.title('Perception attention (avg)')\n",
    "else:\n",
    "    plt.plot(attn)\n",
    "    plt.title('Perception attention')\n",
    "plt.tight_layout(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Human-AI Collaboration\n",
    "\n",
    "We demonstrate trust-aware action selection and display action probabilities with a simple collaboration indicator, echoing the storytelling style in CA17.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize action probabilities and collaboration trigger\n",
    "try:\n",
    "    env = CollaborativeGridWorld(size=6)\n",
    "    obs, _ = env.reset()\n",
    "    state = torch.FloatTensor(obs)\n",
    "    agent = CollaborativeAgent(state_dim=obs.shape[0], action_dim=4)\n",
    "    with torch.no_grad():\n",
    "        logits = agent.policy(state.unsqueeze(0))\n",
    "        probs = torch.softmax(logits, dim=-1)[0].cpu().numpy()\n",
    "    action = int(np.argmax(probs))\n",
    "    should_request = float(probs[action]) < 0.4\n",
    "    plt.figure(figsize=(4,3))\n",
    "    plt.bar(np.arange(4), probs, color=['C0','C1','C2','C3'])\n",
    "    plt.title(f'Action probs; request human? {should_request}')\n",
    "    plt.xlabel('Action'); plt.ylabel('Probability'); plt.show()\n",
    "except Exception as e:\n",
    "    print('Collaboration viz error:', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Quick Summary\n",
    "\n",
    "- Foundation models: sequence modeling enables few-shot adaptation (cf. Lecture 15).\n",
    "- Neurosymbolic RL: logic + learning yields interpretability with minimal performance loss.\n",
    "- Human-AI collaboration: trust-aware intervention improves safety/alignment.\n",
    "\n",
    "Next steps: extend with multi-modal inputs and continual learning ablations, following CA17's narrative structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References and Further Reading\n",
    "\n",
    "- Course Notes: Lecture 15 â€” see `notes_related/15.pdf` for scaling laws, Decision Transformers, interpretability best practices.\n",
    "- CA17 notebook for structure and narrative inspiration (world models, experiments, summaries).\n",
    "\n",
    "Tip: open the PDF alongside this notebook to cross-check definitions and equations while exploring the demos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytics Dashboards\n",
    "\n",
    "We add quick analytics to mirror CA17â€™s comprehensive analysis style:\n",
    "- Reward curves: random vs collaborative agent\n",
    "- Toy scaling laws using `ScalingAnalyzer` (Lecture 15)\n",
    "- Occupancy heatmap from `SymbolicGridWorld`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward curves: random vs collaborative\n",
    "import collections\n",
    "\n",
    "def run_episode(env, policy_fn):\n",
    "    obs, _ = env.reset()\n",
    "    total = 0.0\n",
    "    for _ in range(50):\n",
    "        a = policy_fn(obs)\n",
    "        obs, r, done, _, _ = env.step(a)\n",
    "        total += r\n",
    "        if done:\n",
    "            break\n",
    "    return total\n",
    "\n",
    "env = CollaborativeGridWorld(size=6)\n",
    "\n",
    "def random_policy(_obs):\n",
    "    return int(np.random.randint(0, 4))\n",
    "\n",
    "def collab_policy(obs):\n",
    "    state = torch.FloatTensor(obs)\n",
    "    agent = CollaborativeAgent(state_dim=obs.shape[0], action_dim=4)\n",
    "    a, _ = agent.select_action(state)\n",
    "    return int(a)\n",
    "\n",
    "random_rewards = [run_episode(env, random_policy) for _ in range(20)]\n",
    "collab_rewards = [run_episode(env, collab_policy) for _ in range(20)]\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(random_rewards, label='random', alpha=0.7)\n",
    "plt.plot(collab_rewards, label='collaborative (untrained)', alpha=0.7)\n",
    "plt.title('Reward per episode (short horizon)')\n",
    "plt.xlabel('Episode'); plt.ylabel('Return'); plt.legend(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy scaling laws analysis\n",
    "from foundation_models.algorithms import ScalingAnalyzer\n",
    "\n",
    "model_sizes = [64, 128, 256, 512]\n",
    "performances = [0.45, 0.58, 0.66, 0.71]\n",
    "dataset_sizes = [1e3, 2e3, 4e3, 8e3]\n",
    "compute = [1e8, 2e8, 4e8, 8e8]\n",
    "\n",
    "analyzer = ScalingAnalyzer()\n",
    "res = analyzer.analyze_scaling(model_sizes, performances, dataset_sizes, compute)\n",
    "print('Scaling exponents:', res)\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.loglog(model_sizes, performances, 'o-', label='model size')\n",
    "plt.loglog([int(x) for x in dataset_sizes], performances, 's--', label='data size')\n",
    "plt.xlabel('Scale (log)'); plt.ylabel('Performance (log)'); plt.legend(); plt.title('Toy scaling trends'); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Occupancy heatmap from SymbolicGridWorld\n",
    "try:\n",
    "    env = SymbolicGridWorld(size=10)\n",
    "    obs, _ = env.reset()\n",
    "    grid = np.zeros((env.size, env.size), dtype=np.int32)\n",
    "    grid[env.agent_pos[0], env.agent_pos[1]] += 1\n",
    "    for _ in range(200):\n",
    "        a = int(np.random.randint(0, 4))\n",
    "        obs, r, done, _, _ = env.step(a)\n",
    "        grid[env.agent_pos[0], env.agent_pos[1]] += 1\n",
    "        if done:\n",
    "            break\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(grid, cmap='Blues')\n",
    "    plt.title('Random policy occupancy')\n",
    "    plt.colorbar(); plt.show()\n",
    "except Exception as e:\n",
    "    print('Occupancy heatmap error:', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continual Learning: Forgetting Curves\n",
    "\n",
    "We simulate two tasks and track performance to visualize catastrophic forgetting versus EWC-style stabilization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toy continual learning curves (proxy metrics)\n",
    "np.random.seed(0)\n",
    "episodes = np.arange(1, 51)\n",
    "# Simulate task A learning then task B learning\n",
    "perf_taskA_no_ewc = 0.2 + 0.8*(1 - np.exp(-episodes/12))\n",
    "perf_taskB_no_ewc = 0.2 + 0.8*(1 - np.exp(-np.clip(episodes-25,0,None)/10))\n",
    "# Catastrophic forgetting when learning B (drop in A)\n",
    "perf_taskA_no_ewc[25:] -= 0.25\n",
    "\n",
    "# EWC-style stabilization\n",
    "perf_taskA_ewc = 0.2 + 0.8*(1 - np.exp(-episodes/12))\n",
    "perf_taskB_ewc = 0.2 + 0.8*(1 - np.exp(-np.clip(episodes-25,0,None)/10))\n",
    "perf_taskA_ewc[25:] -= 0.1\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.plot(episodes, perf_taskA_no_ewc, 'r--', label='Task A (no EWC)')\n",
    "plt.plot(episodes, perf_taskB_no_ewc, 'r:', label='Task B (no EWC)')\n",
    "plt.plot(episodes, perf_taskA_ewc, 'g-', label='Task A (EWC)')\n",
    "plt.plot(episodes, perf_taskB_ewc, 'g-.', label='Task B (EWC)')\n",
    "plt.axvline(25, color='k', alpha=0.4, linestyle=':')\n",
    "plt.text(26, 0.3, 'Start Task B', fontsize=9)\n",
    "plt.ylim(0,1.1)\n",
    "plt.xlabel('Episode'); plt.ylabel('Normalized performance')\n",
    "plt.title('Catastrophic forgetting vs EWC (toy)')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment, Safety, and Ethics (Overview)\n",
    "\n",
    "Following the deployment modules, we visualize simple bias/safety diagnostics relevant for production RL systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bias score and safety threshold visualization (toy)\n",
    "np.random.seed(1)\n",
    "# Fake group rewards for two groups to illustrate disparity\n",
    "rewards_group_A = np.random.normal(0.6, 0.1, size=100)\n",
    "rewards_group_B = np.random.normal(0.5, 0.12, size=100)\n",
    "\n",
    "bias_gap = np.abs(rewards_group_A.mean() - rewards_group_B.mean())\n",
    "print('Estimated reward disparity (A-B):', round(float(bias_gap), 3))\n",
    "\n",
    "plt.figure(figsize=(7,3))\n",
    "plt.hist(rewards_group_A, bins=20, alpha=0.6, label='Group A')\n",
    "plt.hist(rewards_group_B, bins=20, alpha=0.6, label='Group B')\n",
    "plt.axvline(rewards_group_A.mean(), color='C0', linestyle='--')\n",
    "plt.axvline(rewards_group_B.mean(), color='C1', linestyle='--')\n",
    "plt.title('Reward distributions by group (toy)')\n",
    "plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "# Safety threshold curve\n",
    "steps = np.arange(1, 51)\n",
    "incident_rate = 0.2*np.exp(-steps/10)\n",
    "threshold = 0.05\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.plot(steps, incident_rate, label='incident rate')\n",
    "plt.axhline(threshold, color='r', linestyle='--', label='threshold')\n",
    "plt.title('Safety incidents vs threshold (toy)')\n",
    "plt.xlabel('Step'); plt.ylabel('Incident rate'); plt.legend(); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproducibility & figure saving utilities\n",
    "from pathlib import Path\n",
    "SAVE_FIGS = False\n",
    "FIG_DIR = Path('CAs/Solutions/CA16/figures')\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def savefig(name: str):\n",
    "    if SAVE_FIGS:\n",
    "        path = FIG_DIR / f'{name}.png'\n",
    "        plt.savefig(path, bbox_inches='tight', dpi=150)\n",
    "        print('Saved figure to', path)\n",
    "\n",
    "print('Figure saving enabled?' , SAVE_FIGS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Advanced Computational Paradigms\n",
    "\n",
    "If available, we run a tiny demo for quantum-inspired or neuromorphic components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced demo (optional)\n",
    "try:\n",
    "    if 'ADVANCED_AVAILABLE' in globals() and ADVANCED_AVAILABLE:\n",
    "        qi = QuantumInspiredRL(num_qubits=4, circuit_depth=2)\n",
    "        dummy_state = np.random.randn(4)\n",
    "        action = qi.select_action(dummy_state)\n",
    "        print('QuantumInspiredRL action (demo):', action)\n",
    "    else:\n",
    "        print('Advanced computational modules not available in this environment.')\n",
    "except Exception as e:\n",
    "    print('Advanced demo error:', e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All (Convenience)\n",
    "\n",
    "Execute key demos in sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all():\n",
    "    print('=== Running core demos ===')\n",
    "    # Foundation: quick train & plot\n",
    "    _ = trainer.train_step(\n",
    "        torch.randn(batch, seq_len, state_dim).to(device),\n",
    "        torch.zeros(batch, seq_len, action_dim).to(device),\n",
    "        torch.randn(batch, seq_len).to(device),\n",
    "        torch.arange(seq_len, device=device).unsqueeze(0).repeat(batch,1)\n",
    "    )\n",
    "    # Neurosymbolic: forward once\n",
    "    kb = SymbolicKnowledgeBase(); ns = NeurosymbolicAgent(state_dim=8, action_dim=4, knowledge_base=kb)\n",
    "    _ = ns.policy(torch.randn(4, 8))\n",
    "    # Collaboration: one step\n",
    "    env = CollaborativeGridWorld(size=6)\n",
    "    obs, _ = env.reset()\n",
    "    agent = CollaborativeAgent(state_dim=obs.shape[0], action_dim=4)\n",
    "    a, _ = agent.select_action(torch.FloatTensor(obs))\n",
    "    _ = env.step(int(a))\n",
    "    print('=== Done ===')\n",
    "\n",
    "run_all()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "- Chen et al., 2021. Decision Transformer: Reinforcement Learning via Sequence Modeling.\n",
    "- Kirkpatrick et al., 2017. Overcoming Catastrophic Forgetting in Neural Networks.\n",
    "- Christiano et al., 2017. Deep Reinforcement Learning from Human Preferences.\n",
    "- Garnelo et al., 2019. Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding.\n",
    "- Dunjko & Briegel, 2018. Machine learning & artificial intelligence in the quantum domain.\n",
    "- McMahan et al., 2017. Communication-Efficient Learning of Deep Networks from Decentralized Data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "- You explored foundation models, neurosymbolic RL, human-AI collaboration, continual learning, analytics, and deployment aspects.\n",
    "- Next steps:\n",
    "  - Swap toy demos for real envs (Gymnasium) and real datasets\n",
    "  - Add continual learning ablations on your tasks\n",
    "  - Integrate tracking (Weights & Biases) and export trained models\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment diagnostics\n",
    "print('Python:', sys.version)\n",
    "print('Torch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "try:\n",
    "    import gymnasium as gym\n",
    "    print('Gymnasium version:', gym.__version__)\n",
    "except Exception as e:\n",
    "    print('Gymnasium not installed or not importable:', e)\n",
    "print('Advanced modules available:', 'Yes' if 'ADVANCED_AVAILABLE' in globals() and ADVANCED_AVAILABLE else 'No')\n",
    "print('Deployment modules available:', 'Yes' if 'DEPLOYMENT_AVAILABLE' in globals() and DEPLOYMENT_AVAILABLE else 'No')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting Tips\n",
    "\n",
    "- Import errors: ensure notebook is run from project root so `CA16` package is importable (we insert `PROJECT_ROOT` automatically).\n",
    "- Gym vs Gymnasium: replace `gym` with `gymnasium` if using newer stack and install `gymnasium`.\n",
    "- CUDA: set `device` to `cpu` if no GPU is available.\n",
    "- Optional modules: `advanced_computational` and `real_world_deployment` are guarded with try/except and will skip gracefully if missing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
