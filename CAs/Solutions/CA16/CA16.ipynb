{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59e03e80",
   "metadata": {},
   "source": [
    "# Computer Assignment 16: Cutting-edge Deep Reinforcement Learning - Foundation Models, Neurosymbolic Rl, and Future Paradigms\n",
    "\n",
    "## Course Information\n",
    "- **Course**: Deep Reinforcement Learning (DRL)\n",
    "- **Instructor**: Dr.Rohban\n",
    "- **Institution**: Sharif University of Technology\n",
    "- **Semester**: Fall 2024\n",
    "- **Assignment Number**: CA16\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By completing this assignment, students will be able to:\n",
    "\n",
    "1. **Master Foundation Models in RL**: Understand and implement large-scale pre-trained RL models including Decision Transformers, Trajectory Transformers, and multi-task/multi-modal architectures that leverage in-context learning for efficient adaptation.\n",
    "\n",
    "2. **Develop Neurosymbolic Reinforcement Learning Systems**: Integrate symbolic reasoning with neural networks through logic-guided policy learning, interpretable RL architectures, and causal reasoning frameworks for enhanced explainability and robustness.\n",
    "\n",
    "3. **Design Continual and Lifelong Learning Agents**: Build meta-learning systems that avoid catastrophic forgetting using progressive neural networks, elastic weight consolidation, and memory-based continual learning approaches.\n",
    "\n",
    "4. **Implement Human-AI Collaborative Learning**: Create RLHF (Reinforcement Learning from Human Feedback) systems, interactive learning frameworks, and preference-based reward modeling for value-aligned AI development.\n",
    "\n",
    "5. **Explore Advanced Computational Paradigms**: Investigate quantum-inspired RL algorithms, neuromorphic computing architectures, distributed/federated RL systems, and energy-efficient learning approaches.\n",
    "\n",
    "6. **Address Real-World Deployment Challenges**: Design production-ready RL systems with robustness, fairness, ethical considerations, regulatory compliance, and safety guarantees for real-world applications.\n",
    "\n",
    "7. **Analyze Future Research Directions**: Evaluate emerging paradigms in RL including constitutional AI, multi-modal learning, federated learning, and interdisciplinary approaches that will shape the future of intelligent agents.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before starting this assignment, ensure you have:\n",
    "\n",
    "- **Mathematical Background**:\n",
    "- Advanced probability, information theory, and Bayesian methods\n",
    "- Causal inference and symbolic logic\n",
    "- Meta-learning theory and continual learning mathematics\n",
    "- Quantum computing fundamentals (optional)\n",
    "\n",
    "- **Technical Skills**:\n",
    "- Expert PyTorch proficiency (transformer architectures, meta-learning)\n",
    "- Experience with large-scale model training and deployment\n",
    "- Understanding of distributed systems and federated learning\n",
    "- Knowledge of ethical AI and responsible ML practices\n",
    "\n",
    "- **Prior Knowledge**:\n",
    "- Completion of CA1-CA15 assignments\n",
    "- Strong foundation in deep learning architectures (transformers, attention)\n",
    "- Understanding of advanced RL algorithms and multi-agent systems\n",
    "- Experience with real-world RL deployment challenges\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "This assignment is structured as follows:\n",
    "\n",
    "### Section 1: Foundation Models and Large-scale Rl\n",
    "- Decision Transformers: Sequence modeling for offline RL\n",
    "- Trajectory Transformers and in-context learning\n",
    "- Multi-task and multi-modal foundation models\n",
    "- Scaling laws and emergent capabilities in RL\n",
    "\n",
    "### Section 2: Neurosymbolic Rl and Interpretability\n",
    "- Integration of symbolic reasoning with neural networks\n",
    "- Logic-guided policy learning and constraint satisfaction\n",
    "- Interpretable RL through attention mechanisms and rule extraction\n",
    "- Causal discovery and reasoning in reinforcement learning\n",
    "\n",
    "### Section 3: Continual Learning and Meta-learning\n",
    "- Catastrophic forgetting: Causes and mitigation strategies\n",
    "- Progressive neural networks and elastic weight consolidation\n",
    "- Meta-learning for fast adaptation and few-shot RL\n",
    "- Memory systems and rehearsal-based continual learning\n",
    "\n",
    "### Section 4: Human-ai Collaborative Learning\n",
    "- Reinforcement Learning from Human Feedback (RLHF)\n",
    "- Interactive learning and preference elicitation\n",
    "- Constitutional AI and value alignment techniques\n",
    "- Human-in-the-loop reinforcement learning\n",
    "\n",
    "### Section 5: Advanced Computational Paradigms\n",
    "- Quantum-inspired optimization and amplitude estimation\n",
    "- Neuromorphic computing for energy-efficient RL\n",
    "- Distributed and federated reinforcement learning\n",
    "- Edge computing and resource-constrained RL\n",
    "\n",
    "### Section 6: Real-world Deployment and Ethics\n",
    "- Production RL systems: Monitoring, A/B testing, and deployment\n",
    "- Ethical considerations: Fairness, bias, and societal impact\n",
    "- Robustness, safety, and regulatory compliance\n",
    "- Responsible AI development and governance\n",
    "\n",
    "## Project Structure\n",
    "\n",
    "```\n",
    "CA16/\n",
    "├── CA16.ipynb                      # Main assignment notebook\n",
    "├── agents/                         # Cutting-edge RL agent implementations\n",
    "│   ├── foundation_agents.py        # Decision Transformers, Trajectory models\n",
    "│   ├── neurosymbolic_agents.py     # Logic-guided and interpretable RL\n",
    "│   ├── continual_agents.py         # Meta-learning and continual learning agents\n",
    "│   ├── collaborative_agents.py     # RLHF and human-AI collaborative systems\n",
    "│   └── advanced_agents.py          # Quantum, neuromorphic, and distributed RL\n",
    "├── environments/                   # Advanced environment implementations\n",
    "│   ├── foundation_env.py           # Multi-task and multi-modal environments\n",
    "│   ├── neurosymbolic_env.py        # Environments requiring symbolic reasoning\n",
    "│   ├── continual_env.py            # Continual learning and domain shift scenarios\n",
    "│   ├── collaborative_env.py        # Human-AI interaction environments\n",
    "│   └── deployment_env.py           # Real-world deployment simulation environments\n",
    "├── models/                         # Advanced neural architectures\n",
    "│   ├── foundation_models.py        # Transformer-based RL models\n",
    "│   ├── neurosymbolic_models.py     # Neural-symbolic hybrid architectures\n",
    "│   ├── continual_models.py         # Progressive and elastic networks\n",
    "│   ├── collaborative_models.py     # Preference and reward modeling networks\n",
    "│   └── advanced_models.py          # Quantum, neuromorphic, and distributed models\n",
    "├── experiments/                    # Research-level experiments\n",
    "│   ├── foundation_experiments.py   # Large-scale RL and in-context learning\n",
    "│   ├── neurosymbolic_experiments.py# Interpretability and causal reasoning\n",
    "│   ├── continual_experiments.py    # Meta-learning and catastrophic forgetting\n",
    "│   ├── collaborative_experiments.py# RLHF and human feedback learning\n",
    "│   └── deployment_experiments.py   # Production systems and ethical analysis\n",
    "└── utils/                          # Advanced utility functions\n",
    "    ├── foundation_utils.py         # Transformer utilities and scaling tools\n",
    "    ├── neurosymbolic_utils.py      # Symbolic reasoning and logic utilities\n",
    "    ├── continual_utils.py          # Meta-learning and memory system utilities\n",
    "    ├── collaborative_utils.py      # Human feedback and preference utilities\n",
    "    └── deployment_utils.py         # Production monitoring and ethical analysis tools\n",
    "```\n",
    "\n",
    "## Contents Overview\n",
    "\n",
    "### Theoretical Foundations\n",
    "- **Foundation Models**: Scaling laws, emergent capabilities, in-context learning\n",
    "- **Neurosymbolic RL**: Symbolic reasoning, logic integration, interpretability\n",
    "- **Continual Learning**: Plasticity-stability trade-offs, meta-learning theory\n",
    "- **Human-AI Collaboration**: Preference learning, value alignment, interactive learning\n",
    "- **Advanced Computing**: Quantum algorithms, neuromorphic principles, distributed systems\n",
    "- **Ethics & Deployment**: Responsible AI, fairness, robustness, regulatory frameworks\n",
    "\n",
    "### Implementation Components\n",
    "- **Foundation Systems**: Large-scale transformers, multi-modal architectures, trajectory modeling\n",
    "- **Neurosymbolic Systems**: Logic integration, rule extraction, causal mechanisms\n",
    "- **Continual Systems**: Progressive networks, elastic consolidation, memory replay\n",
    "- **Collaborative Systems**: RLHF pipelines, preference modeling, interactive learning\n",
    "- **Advanced Systems**: Quantum circuits, neuromorphic networks, federated learning\n",
    "\n",
    "### Research Topics\n",
    "- **Emerging Paradigms**: Constitutional AI, multi-modal learning, interdisciplinary approaches\n",
    "- **Scalability Challenges**: Training large models, distributed optimization, energy efficiency\n",
    "- **Interpretability**: Explainable decisions, causal understanding, trustworthy AI\n",
    "- **Real-World Impact**: Deployment challenges, ethical considerations, societal implications\n",
    "\n",
    "## Evaluation Criteria\n",
    "\n",
    "Your implementation will be evaluated based on:\n",
    "\n",
    "1. **Research Depth (25%)**: Understanding and implementation of cutting-edge concepts\n",
    "2. **Technical Innovation (30%)**: Creative solutions and novel implementations\n",
    "3. **Ethical Analysis (20%)**: Consideration of societal impact and responsible AI practices\n",
    "4. **Experimental Rigor (15%)**: Thorough evaluation and comparative analysis\n",
    "5. **Future Vision (10%)**: Insightful analysis of emerging trends and research directions\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "1. **Research Review**: Study recent papers on foundation models, neurosymbolic RL, and emerging paradigms\n",
    "2. **Infrastructure Setup**: Configure advanced computing resources for large-scale experiments\n",
    "3. **Ethical Framework**: Establish guidelines for responsible AI development and evaluation\n",
    "4. **Incremental Exploration**: Start with foundation models, then explore neurosymbolic and continual learning\n",
    "5. **Interdisciplinary Integration**: Connect RL concepts with broader AI and computing paradigms\n",
    "\n",
    "## Expected Outcomes\n",
    "\n",
    "By the end of this assignment, you will have:\n",
    "\n",
    "- **Research Expertise**: Ability to understand and implement state-of-the-art RL research\n",
    "- **Interdisciplinary Knowledge**: Understanding of connections between RL and other AI fields\n",
    "- **Ethical Awareness**: Skills in responsible AI development and deployment\n",
    "- **Future-Ready Skills**: Knowledge of emerging paradigms and research directions\n",
    "- **Innovation Capabilities**: Ability to develop novel RL approaches and applications\n",
    "\n",
    "---\n",
    "\n",
    "**Note**: This assignment represents the cutting edge of deep reinforcement learning research, exploring how RL intersects with foundation models, symbolic reasoning, continual learning, and advanced computing paradigms. The focus is on understanding current limitations and envisioning future possibilities while maintaining ethical responsibility.\n",
    "\n",
    "Let's explore the frontiers of intelligent agents! 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cc93e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal, Categorical, MultivariateNormal\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple, OrderedDict\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import time\n",
    "import gym\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any, Callable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from abc import ABC, abstractmethod\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "print(f\"🚀 Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"💫 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"🔢 CUDA Version: {torch.version.cuda}\")\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "FOUNDATION_MODEL_CONFIG = {\n",
    "    'model_dim': 512,\n",
    "    'num_heads': 8,\n",
    "    'num_layers': 6,\n",
    "    'context_length': 1024,\n",
    "    'dropout': 0.1,\n",
    "    'layer_norm_eps': 1e-5,\n",
    "    'max_position_embeddings': 2048\n",
    "}\n",
    "\n",
    "NEUROSYMBOLIC_CONFIG = {\n",
    "    'logic_embedding_dim': 128,\n",
    "    'symbolic_vocab_size': 1000,\n",
    "    'reasoning_steps': 5,\n",
    "    'symbolic_weight': 0.3,\n",
    "    'neural_weight': 0.7,\n",
    "    'interpretability_threshold': 0.8\n",
    "}\n",
    "\n",
    "CONTINUAL_LEARNING_CONFIG = {\n",
    "    'ewc_lambda': 1000,\n",
    "    'memory_size': 10000,\n",
    "    'num_tasks': 10,\n",
    "    'adaptation_lr': 1e-4,\n",
    "    'meta_lr': 1e-3,\n",
    "    'forgetting_threshold': 0.1\n",
    "}\n",
    "\n",
    "HUMAN_AI_CONFIG = {\n",
    "    'preference_model_dim': 256,\n",
    "    'reward_model_lr': 3e-4,\n",
    "    'human_feedback_ratio': 0.1,\n",
    "    'preference_batch_size': 64,\n",
    "    'kl_penalty': 0.1,\n",
    "    'value_alignment_weight': 1.0\n",
    "}\n",
    "\n",
    "QUANTUM_RL_CONFIG = {\n",
    "    'num_qubits': 8,\n",
    "    'circuit_depth': 10,\n",
    "    'quantum_lr': 0.01,\n",
    "    'entanglement_layers': 3,\n",
    "    'measurement_shots': 1024,\n",
    "    'quantum_advantage_threshold': 1.5\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57300682",
   "metadata": {},
   "source": [
    "# Section 1: Foundation Models in Reinforcement Learning\n",
    "\n",
    "Foundation models represent a paradigm shift in AI, where large-scale pre-trained models can be adapted to various downstream tasks. In RL, this concept translates to training massive models on diverse experiences that can then be fine-tuned for specific tasks.\n",
    "\n",
    "## 1.1 Theoretical Foundations\n",
    "\n",
    "### Decision Transformers\n",
    "The Decision Transformer reframes RL as a sequence modeling problem, where the goal is to generate actions conditioned on desired returns.\n",
    "\n",
    "**Key Insight**: Instead of learning value functions or policy gradients, we model:\n",
    "$$P(a*t | s*{1:t}, a*{1:t-1}, R*{t:T})$$\n",
    "\n",
    "Where $R_{t:T}$ represents the desired return-to-go from time $t$ to episode end $T$.\n",
    "\n",
    "### Trajectory Transformers\n",
    "Extend transformers to model entire trajectories:\n",
    "$$P(\\tau | g) = \\prod*{t=0}^{T} P(s*{t+1}, r*t, a*t | s*{1:t}, a*{1:t-1}, g)$$\n",
    "\n",
    "Where $g$ represents the goal or task specification.\n",
    "\n",
    "### Multi-task Pre-training\n",
    "Foundation models in RL are trained on massive datasets containing:\n",
    "- Multiple environments and tasks\n",
    "- Diverse behavioral policies\n",
    "- Various skill demonstrations\n",
    "- Cross-modal experiences (vision, language, control)\n",
    "\n",
    "**Training Objective**:\n",
    "$$\\mathcal{L} = \\sum*{\\mathcal{D}*i} \\mathbb{E}*{\\tau \\sim \\mathcal{D}*i} [-\\log P(\\tau | \\text{context}_i)]$$\n",
    "\n",
    "### In-context Learning for Rl\n",
    "Similar to language models, RL foundation models can adapt to new tasks through in-context learning:\n",
    "- Provide few-shot demonstrations\n",
    "- Model infers task structure and optimal behavior\n",
    "- No gradient updates required\n",
    "\n",
    "## 1.2 Advantages and Challenges\n",
    "\n",
    "### Advantages:\n",
    "1. **Sample Efficiency**: Leverage pre-training for rapid adaptation\n",
    "2. **Generalization**: Transfer knowledge across diverse tasks\n",
    "3. **Few-Shot Learning**: Adapt to new tasks with minimal data\n",
    "4. **Unified Architecture**: Single model for multiple domains\n",
    "\n",
    "### Challenges:\n",
    "1. **Computational Requirements**: Massive models need significant resources\n",
    "2. **Data Requirements**: Need diverse, high-quality training data\n",
    "3. **Task Distribution**: Performance depends on training task diversity\n",
    "4. **Fine-tuning Complexity**: Avoiding catastrophic forgetting during adaptation\n",
    "\n",
    "### Scaling Laws in Rl\n",
    "Similar to language models, RL foundation models exhibit scaling laws:\n",
    "- **Model Size**: Larger models achieve better performance\n",
    "- **Data Scale**: More diverse training data improves generalization\n",
    "- **Compute**: Increased training compute enables larger models\n",
    "\n",
    "**Empirical Scaling Relationship**:\n",
    "$$\\text{Performance} \\propto \\alpha N^{\\beta} D^{\\gamma} C^{\\delta}$$\n",
    "\n",
    "Where $N$ = model parameters, $D$ = dataset size, $C$ = compute budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33514387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported foundation model classes from foundation_models.algorithms\n"
     ]
    }
   ],
   "source": [
    "# Import DecisionTransformer and foundation model utilities from package files\n",
    "from foundation_models.algorithms import (\n",
    "    DecisionTransformer,\n",
    "    MultiTaskRLFoundationModel,\n",
    "    InContextLearningRL,\n",
    "    FoundationModelTrainer,\n",
    "    PositionalEncoding,\n",
    ")\n",
    "\n",
    "print(\"Imported foundation model classes from foundation_models.algorithms\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5383bdef",
   "metadata": {},
   "source": [
    "# Section 2: Neurosymbolic Reinforcement Learning\n",
    "\n",
    "Neurosymbolic RL combines the learning capabilities of neural networks with the reasoning power of symbolic systems, creating interpretable and more robust intelligent agents.\n",
    "\n",
    "## 2.1 Theoretical Foundations\n",
    "\n",
    "### The Neurosymbolic Paradigm\n",
    "Traditional RL systems struggle with:\n",
    "- **Interpretability**: Understanding why decisions were made\n",
    "- **Compositional Reasoning**: Combining learned concepts systematically\n",
    "- **Sample Efficiency**: Learning abstract rules from limited data\n",
    "- **Transfer**: Applying learned knowledge to new domains\n",
    "\n",
    "**Neurosymbolic RL** addresses these challenges by integrating:\n",
    "- **Neural Components**: Learning from raw sensory data\n",
    "- **Symbolic Components**: Logical reasoning and rule-based inference\n",
    "- **Hybrid Architectures**: Seamless integration of both paradigms\n",
    "\n",
    "### Core Components\n",
    "\n",
    "#### 1. Symbolic Knowledge Representation\n",
    "Represent environment knowledge using formal logic:\n",
    "- **Predicate Logic**: $\\text{at}(\\text{agent}, x, y) \\land \\text{obstacle}(x+1, y) \\rightarrow \\neg \\text{move\\_right}$\n",
    "- **Temporal Logic**: $\\square (\\text{goal\\_reached} \\rightarrow \\Diamond \\text{reward})$\n",
    "- **Probabilistic Logic**: $P(\\text{success} | \\text{action}, \\text{state}) = 0.8$\n",
    "\n",
    "#### 2. Neural-symbolic Integration Patterns\n",
    "\n",
    "**Pattern 1: Neural Perception + Symbolic Reasoning**\n",
    "$$\\pi(a|s) = \\text{SymbolicPlanner}(\\text{NeuralPerception}(s))$$\n",
    "\n",
    "**Pattern 2: Symbolic-Guided Neural Learning**\n",
    "$$\\mathcal{L} = \\mathcal{L}*{\\text{RL}} + \\lambda \\mathcal{L}*{\\text{logic}}$$\n",
    "\n",
    "**Pattern 3: Hybrid Representations**\n",
    "$$h = \\text{Combine}(h*{\\text{neural}}, h*{\\text{symbolic}})$$\n",
    "\n",
    "### Logical Policy Learning\n",
    "Learn policies that satisfy logical constraints:\n",
    "\n",
    "**Constraint Satisfaction**:\n",
    "$$\\pi^* = \\arg\\max*\\pi \\mathbb{E}*\\pi[R] \\text{ subject to } \\phi \\models \\psi$$\n",
    "\n",
    "Where $\\phi$ represents the policy behavior and $\\psi$ represents logical constraints.\n",
    "\n",
    "**Logic-Regularized RL**:\n",
    "$$\\mathcal{L} = -\\mathbb{E}_\\pi[R] + \\alpha \\cdot \\text{LogicViolation}(\\pi, \\psi)$$\n",
    "\n",
    "### Compositional Learning\n",
    "Enable agents to compose learned primitives:\n",
    "\n",
    "**Hierarchical Composition**:\n",
    "- **Skills**: $\\pi*1, \\pi*2, \\ldots, \\pi_k$\n",
    "- **Meta-Policy**: $\\pi_{\\text{meta}}(k|s)$\n",
    "- **Composition Rule**: $\\pi(a|s) = \\sum*k \\pi*{\\text{meta}}(k|s) \\pi_k(a|s)$\n",
    "\n",
    "**Logical Composition**:\n",
    "- **Primitive Predicates**: $p*1, p*2, \\ldots, p_n$\n",
    "- **Logical Operators**: $\\land, \\lor, \\neg, \\rightarrow$\n",
    "- **Complex Behaviors**: $\\psi = p*1 \\land (p*2 \\lor \\neg p*3) \\rightarrow p*4$\n",
    "\n",
    "## 2.2 Interpretability and Explainability\n",
    "\n",
    "### Attention-based Explanations\n",
    "Use attention mechanisms to highlight decision factors:\n",
    "$$\\alpha*i = \\frac{\\exp(e*i)}{\\sum*j \\exp(e*j)}, \\quad e*i = f*{\\text{att}}(h_i)$$\n",
    "\n",
    "### Counterfactual Reasoning\n",
    "Generate explanations through counterfactuals:\n",
    "- **Question**: \"What if state $s$ were different?\"\n",
    "- **Counterfactual State**: $s' = s + \\delta$\n",
    "- **Action Change**: $\\Delta a = \\pi(s') - \\pi(s)$\n",
    "- **Explanation**: \"If $x$ were true, agent would do $y$ instead\"\n",
    "\n",
    "### Causal Discovery in Rl\n",
    "Learn causal relationships between variables:\n",
    "$$X \\rightarrow Y \\text{ if } I(Y; \\text{do}(X)) > 0$$\n",
    "\n",
    "Where $I$ is mutual information and $\\text{do}(X)$ represents intervention.\n",
    "\n",
    "### Logical Rule Extraction\n",
    "Extract interpretable rules from trained policies:\n",
    "1. **State Abstraction**: Group similar states\n",
    "2. **Action Patterns**: Identify consistent action choices\n",
    "3. **Rule Formation**: Convert patterns to logical rules\n",
    "4. **Rule Validation**: Test rules on new data\n",
    "\n",
    "## 2.3 Advanced Neurosymbolic Architectures\n",
    "\n",
    "### Differentiable Neural Module Networks (dnmns)\n",
    "Compose neural modules based on language instructions:\n",
    "- **Modules**: $\\{m*1, m*2, \\ldots, m_k\\}$\n",
    "- **Composition**: Dynamic module assembly\n",
    "- **Training**: End-to-end differentiable\n",
    "\n",
    "### Graph Neural Networks for Symbolic Reasoning\n",
    "Represent knowledge as graphs and use GNNs:\n",
    "- **Nodes**: Entities, concepts, states\n",
    "- **Edges**: Relations, transitions, dependencies\n",
    "- **Message Passing**: Propagate information through graph\n",
    "- **Reasoning**: Multi-hop inference over graph structure\n",
    "\n",
    "### Memory-augmented Networks\n",
    "External memory for symbolic knowledge storage:\n",
    "- **Memory Matrix**: $M \\in \\mathbb{R}^{N \\times D}$\n",
    "- **Attention**: $w = \\text{softmax}(q^T M)$\n",
    "- **Read**: $r = w^T M$\n",
    "- **Write**: $M \\leftarrow M + w \\odot \\text{update}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2474f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported neurosymbolic components from neurosymbolic package\n"
     ]
    }
   ],
   "source": [
    "# Import neurosymbolic components from module files\n",
    "from neurosymbolic.policies import (\n",
    "    NeurosymbolicPolicy,\n",
    "    NeurosymbolicAgent,\n",
    "    NeuralPerceptionModule,\n",
    "    SymbolicReasoningModule,\n",
    ")\n",
    "from neurosymbolic.knowledge_base import SymbolicKnowledgeBase, LogicalPredicate, LogicalRule\n",
    "\n",
    "print(\"Imported neurosymbolic components from neurosymbolic package\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2703f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported SymbolicGridWorld from environments.symbolic_env\n"
     ]
    }
   ],
   "source": [
    "# Import the SymbolicGridWorld environment from the environments package\n",
    "from environments.symbolic_env import SymbolicGridWorld\n",
    "\n",
    "print(\"Imported SymbolicGridWorld from environments.symbolic_env\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c7bdf",
   "metadata": {},
   "source": [
    "# Section 3: Human-ai Collaborative Learning\n",
    "\n",
    "Human-AI collaborative learning represents a paradigm where AI agents learn not just from environment interaction, but also from human guidance, feedback, and collaboration to achieve superhuman performance.\n",
    "\n",
    "## 3.1 Theoretical Foundations\n",
    "\n",
    "### The Human-ai Collaboration Paradigm\n",
    "\n",
    "Traditional RL assumes agents learn independently from environment feedback. **Human-AI Collaborative Learning** extends this by incorporating human intelligence:\n",
    "\n",
    "- **Human Expertise Integration**: Leverage human domain knowledge and intuition\n",
    "- **Interactive Learning**: Real-time human feedback during agent training\n",
    "- **Shared Control**: Dynamic handoff between human and AI decision-making\n",
    "- **Explanatory AI**: AI explains decisions to humans for better collaboration\n",
    "\n",
    "### Learning from Human Feedback (rlhf)\n",
    "\n",
    "**Preference-Based Learning**:\n",
    "Instead of engineering reward functions, learn from human preferences:\n",
    "\n",
    "$$r*{\\theta}(s, a) = \\text{RewardModel}*{\\theta}(s, a)$$\n",
    "\n",
    "Where the reward model is trained on human preference data:\n",
    "$$\\mathcal{D} = \\{(s*i, a*i^1, a*i^2, y*i)\\}$$\n",
    "\n",
    "Where $y*i \\in \\{0, 1\\}$ indicates whether human prefers action $a*i^1$ over $a*i^2$ in state $s*i$.\n",
    "\n",
    "**Bradley-Terry Model** for preferences:\n",
    "$$P(a^1 \\succ a^2 | s) = \\frac{\\exp(r*{\\theta}(s, a^1))}{\\exp(r*{\\theta}(s, a^1)) + \\exp(r_{\\theta}(s, a^2))}$$\n",
    "\n",
    "**Training Objective**:\n",
    "$$\\mathcal{L}(\\theta) = -\\mathbb{E}_{(s,a^1,a^2,y) \\sim \\mathcal{D}}[y \\log P(a^1 \\succ a^2 | s) + (1-y) \\log P(a^2 \\succ a^1 | s)]$$\n",
    "\n",
    "### Interactive Imitation Learning\n",
    "\n",
    "**DAgger (Dataset Aggregation)**:\n",
    "Iteratively collect expert demonstrations on learned policy trajectories:\n",
    "\n",
    "1. Train policy $\\pi*i$ on current dataset $\\mathcal{D}*i$\n",
    "2. Execute $\\pi*i$ to collect states $\\{s*t\\}$\n",
    "3. Query expert for optimal actions $\\{a*t^*\\}$ on $\\{s*t\\}$\n",
    "4. Aggregate: $\\mathcal{D}*{i+1} = \\mathcal{D}*i \\cup \\{(s*t, a*t^*)\\}$\n",
    "\n",
    "**SMILe (Safe Multi-agent Imitation Learning)**:\n",
    "Learn from multiple human experts with safety constraints:\n",
    "$$\\pi^* = \\arg\\min*\\pi \\sum*i w*i \\mathcal{L}*{\\text{imitation}}(\\pi, \\pi*i^{\\text{expert}}) + \\lambda \\mathcal{L}*{\\text{safety}}(\\pi)$$\n",
    "\n",
    "### Shared Autonomy and Control\n",
    "\n",
    "**Arbitration Between Human and AI**:\n",
    "Dynamic switching between human and AI control:\n",
    "\n",
    "$$a_t = \\begin{cases}\n",
    "a*t^{\\text{human}} & \\text{if } \\alpha*t > \\tau \\\\\n",
    "a_t^{\\text{AI}} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "Where $\\alpha_t$ represents human authority level at time $t$.\n",
    "\n",
    "**Confidence-Based Handoff**:\n",
    "$$\\alpha*t = f(\\text{confidence}*{\\text{AI}}(s*t), \\text{urgency}(s*t), \\text{human\\_availability}(t))$$\n",
    "\n",
    "**Blended Control**:\n",
    "Combine human and AI actions based on context:\n",
    "$$a*t = w*t \\cdot a*t^{\\text{human}} + (1 - w*t) \\cdot a_t^{\\text{AI}}$$\n",
    "\n",
    "### Trust and Calibration\n",
    "\n",
    "**Trust Modeling**:\n",
    "Model human trust in AI decisions:\n",
    "$$T*{t+1} = T*t + \\alpha \\cdot (\\text{outcome}*t - T*t) \\cdot \\text{surprise}_t$$\n",
    "\n",
    "Where:\n",
    "- $T_t$: Trust level at time $t$\n",
    "- $\\text{outcome}_t$: Actual performance outcome\n",
    "- $\\text{surprise}_t$: Difference between expected and actual outcome\n",
    "\n",
    "**Calibrated Confidence**:\n",
    "Ensure AI confidence matches actual performance:\n",
    "$$\\text{Calibration Error} = \\mathbb{E}[|\\text{Confidence} - \\text{Accuracy}|]$$\n",
    "\n",
    "**Trust-Aware Policy**:\n",
    "Modify policy to maintain appropriate human trust:\n",
    "$$\\pi*{\\text{trust}}(a|s) = \\pi(a|s) \\cdot f*{\\text{trust}}(a, s, T_t)$$\n",
    "\n",
    "## 3.2 Human Feedback Integration Methods\n",
    "\n",
    "### Critiquing and Advice\n",
    "Allow humans to provide structured feedback:\n",
    "\n",
    "**Action Critiquing**:\n",
    "- Human observes AI action and provides feedback\n",
    "- Types: \"Good action\", \"Bad action\", \"Better action would be...\"\n",
    "- Update policy based on critique\n",
    "\n",
    "**State-Action Advice**:\n",
    "$$\\mathcal{L}*{\\text{advice}} = -\\log \\pi(a*{\\text{advised}} | s) \\cdot w_{\\text{confidence}}$$\n",
    "\n",
    "### Demonstration and Intervention\n",
    "\n",
    "**Human Demonstrations**:\n",
    "- Collect expert trajectories: $\\tau*{\\text{expert}} = \\{(s*0, a*0), (s*1, a_1), \\ldots\\}$\n",
    "- Learn via behavioral cloning or inverse RL\n",
    "- Active learning: query human on uncertain states\n",
    "\n",
    "**Intervention Learning**:\n",
    "- Human takes control when AI makes mistakes\n",
    "- Learn from intervention patterns\n",
    "- Identify failure modes and correction strategies\n",
    "\n",
    "### Preference Learning and Ranking\n",
    "\n",
    "**Pairwise Preferences**:\n",
    "Show human two action sequences and ask for preference\n",
    "$$\\mathcal{P} = \\{(\\tau*1, \\tau*2, \\text{preference})\\}$$\n",
    "\n",
    "**Trajectory Ranking**:\n",
    "Rank multiple trajectories by performance\n",
    "$$\\tau*1 \\succ \\tau*2 \\succ \\ldots \\succ \\tau_k$$\n",
    "\n",
    "**Active Preference Learning**:\n",
    "Intelligently select which comparisons to show human:\n",
    "$$\\text{query}^* = \\arg\\max_{\\text{query}} \\text{InformationGain}(\\text{query})$$\n",
    "\n",
    "## 3.3 Collaborative Decision Making\n",
    "\n",
    "### Shared Mental Models\n",
    "Align human and AI understanding of the task:\n",
    "\n",
    "**Common Ground**:\n",
    "- Shared representation of environment\n",
    "- Agreed-upon goal decomposition  \n",
    "- Common terminology and concepts\n",
    "\n",
    "**Theory of Mind**:\n",
    "AI models human beliefs, intentions, and capabilities:\n",
    "$$\\text{AI\\*Model}(\\text{human\\*belief}(s*t), \\text{human\\*goal}, \\text{human\\_capability})$$\n",
    "\n",
    "### Communication Protocols\n",
    "\n",
    "**Natural Language Interface**:\n",
    "- AI explains decisions in natural language\n",
    "- Human provides feedback via natural language\n",
    "- Bidirectional communication for coordination\n",
    "\n",
    "**Multimodal Communication**:\n",
    "- Visual indicators (attention, confidence)\n",
    "- Gestural input from humans\n",
    "- Audio feedback and alerts\n",
    "\n",
    "### Coordination Strategies\n",
    "\n",
    "**Task Allocation**:\n",
    "Divide tasks based on comparative advantage:\n",
    "$$\\text{Assign}(T_i) = \\begin{cases}\n",
    "\\text{Human} & \\text{if } \\text{Advantage}*{\\text{human}}(T*i) > \\text{Advantage}*{\\text{AI}}(T*i) \\\\\n",
    "\\text{AI} & \\text{otherwise}\n",
    "\\end{cases}$$\n",
    "\n",
    "**Dynamic Role Assignment**:\n",
    "Roles change based on context, performance, and availability:\n",
    "- **Leader-Follower**: One party leads, other assists\n",
    "- **Peer Collaboration**: Equal partnership with negotiation\n",
    "- **Hierarchical**: Clear command structure with delegation\n",
    "\n",
    "## 3.4 Advanced Collaborative Learning Paradigms\n",
    "\n",
    "### Constitutional Ai\n",
    "Train AI systems to follow high-level principles:\n",
    "\n",
    "1. **Constitutional Training**: Define principles in natural language\n",
    "2. **Self-Critiquing**: AI evaluates its own responses against principles\n",
    "3. **Iterative Refinement**: Improve responses based on principle violations\n",
    "\n",
    "**Constitutional Loss**:\n",
    "$$\\mathcal{L}*{\\text{constitutional}} = \\mathcal{L}*{\\text{task}} + \\lambda \\sum*i \\text{Violation}(\\text{principle}*i)$$\n",
    "\n",
    "### Cooperative Inverse Reinforcement Learning (co-irl)\n",
    "Learn shared reward functions through interaction:\n",
    "\n",
    "$$R^* = \\arg\\max*R \\log P(\\tau*{\\text{human}} | R) + \\log P(\\tau_{\\text{AI}} | R) + \\text{Cooperation}(R)$$\n",
    "\n",
    "### Multi-agent Human-ai Teams\n",
    "Extend collaboration to multi-agent settings:\n",
    "\n",
    "**Team Formation**:\n",
    "- Optimal team composition (humans + AI agents)\n",
    "- Role specialization and capability matching\n",
    "- Communication network topology\n",
    "\n",
    "**Collective Intelligence**:\n",
    "$$\\text{Team\\*Performance} > \\max(\\text{Individual\\*Performance})$$\n",
    "\n",
    "### Continual Human-ai Co-evolution\n",
    "Humans and AI systems improve together over time:\n",
    "\n",
    "**Co-Adaptation**:\n",
    "- AI adapts to human preferences and style\n",
    "- Humans develop better collaboration skills with AI\n",
    "- Mutual model updates and learning\n",
    "\n",
    "**Lifelong Collaboration**:\n",
    "- Maintain collaboration quality over extended periods\n",
    "- Handle changes in human capabilities and preferences\n",
    "- Evolve communication and coordination protocols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "161eb87b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HumanFeedback' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import human-AI collaboration modules from package files\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuman_ai_collaboration\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreference_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      3\u001b[39m     PreferenceRewardModel,\n\u001b[32m      4\u001b[39m     HumanFeedbackCollector,\n\u001b[32m      5\u001b[39m     HumanPreference,\n\u001b[32m      6\u001b[39m     HumanFeedback,\n\u001b[32m      7\u001b[39m     CollaborativeAgent,\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mImported PreferenceRewardModel and human feedback utilities\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA16/human_ai_collaboration/__init__.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreference_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreferenceRewardModel, HumanPreference, HumanFeedback\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeedback_collector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanFeedbackCollector, InteractiveFeedbackCollector\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollaborative_agent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CollaborativeAgent, HumanAIPartnership\n\u001b[32m     15\u001b[39m __all__ = [\n\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mPreferenceRewardModel\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHumanPreference\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mHumanAIPartnership\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     23\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA16/human_ai_collaboration/collaborative_agent.py:25\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeedback_collector\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanFeedbackCollector\n\u001b[32m     22\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43;01mCollaborativeAgent\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43;03m\"\"\"Agent that collaborates with humans for decision making.\"\"\"\u001b[39;49;00m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[38;5;250;43m \u001b[39;49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcollaboration_threshold\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA16/human_ai_collaboration/collaborative_agent.py:181\u001b[39m, in \u001b[36mCollaborativeAgent\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    178\u001b[39m     successful = \u001b[38;5;28msum\u001b[39m(\u001b[32m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m recent \u001b[38;5;28;01mif\u001b[39;00m c.get(\u001b[33m\"\u001b[39m\u001b[33mhuman_override\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m    179\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m successful / \u001b[38;5;28mlen\u001b[39m(recent) \u001b[38;5;28;01mif\u001b[39;00m recent \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.5\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn_from_feedback\u001b[39m(\u001b[38;5;28mself\u001b[39m, feedback_batch: List[\u001b[43mHumanFeedback\u001b[49m]):\n\u001b[32m    182\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Learn from human feedback.\"\"\"\u001b[39;00m\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m feedback \u001b[38;5;129;01min\u001b[39;00m feedback_batch:\n",
      "\u001b[31mNameError\u001b[39m: name 'HumanFeedback' is not defined"
     ]
    }
   ],
   "source": [
    "# Import human-AI collaboration modules from package files\n",
    "from human_ai_collaboration.preference_model import (\n",
    "    PreferenceRewardModel,\n",
    "    HumanFeedbackCollector,\n",
    "    HumanPreference,\n",
    "    HumanFeedback,\n",
    "    CollaborativeAgent,\n",
    ")\n",
    "\n",
    "print(\"Imported PreferenceRewardModel and human feedback utilities\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eae2719",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HumanFeedbackCollector' from 'human_ai_collaboration.preference_model' (/Users/tahamajs/Documents/uni/DRL/CAs/Solutions/CA16/human_ai_collaboration/preference_model.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Import CollaborativeGridWorld and collaborative tooling\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01menvironments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollaborative_env\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CollaborativeGridWorld\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuman_ai_collaboration\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreference_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HumanFeedbackCollector, CollaborativeAgent\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mImported CollaborativeGridWorld and collaboration modules\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'HumanFeedbackCollector' from 'human_ai_collaboration.preference_model' (/Users/tahamajs/Documents/uni/DRL/CAs/Solutions/CA16/human_ai_collaboration/preference_model.py)"
     ]
    }
   ],
   "source": [
    "# Import CollaborativeGridWorld and collaborative tooling\n",
    "from environments.collaborative_env import CollaborativeGridWorld\n",
    "from human_ai_collaboration.preference_model import HumanFeedbackCollector, CollaborativeAgent\n",
    "\n",
    "print(\"Imported CollaborativeGridWorld and collaboration modules\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493b884",
   "metadata": {},
   "source": [
    "# Section 4: Foundation Models in Reinforcement Learning\n",
    "\n",
    "Foundation models represent a paradigm shift in RL, leveraging pre-trained large models to achieve sample-efficient learning and strong generalization across diverse tasks and domains.\n",
    "\n",
    "## 4.1 Theoretical Foundations\n",
    "\n",
    "### The Foundation Model Paradigm in Rl\n",
    "\n",
    "**Traditional RL Limitations**:\n",
    "- **Sample Inefficiency**: Learning from scratch on each task\n",
    "- **Poor Generalization**: Overfitting to specific environments\n",
    "- **Limited Transfer**: Difficulty sharing knowledge across domains\n",
    "- **Representation Learning**: Learning both policy and representations simultaneously\n",
    "\n",
    "**Foundation Model Advantages**:\n",
    "- **Pre-trained Representations**: Rich features learned from large datasets\n",
    "- **Few-Shot Learning**: Rapid adaptation to new tasks with minimal data\n",
    "- **Cross-Domain Transfer**: Knowledge sharing across different environments\n",
    "- **Compositional Reasoning**: Understanding of complex task structures\n",
    "\n",
    "### Mathematical Framework\n",
    "\n",
    "**Foundation Model as Universal Approximator**:\n",
    "$$f_{\\theta}: \\mathcal{X} \\rightarrow \\mathcal{Z}$$\n",
    "\n",
    "Where $\\mathcal{X}$ is input space (observations, language, etc.) and $\\mathcal{Z}$ is latent representation space.\n",
    "\n",
    "**Task-Specific Adaptation**:\n",
    "$$\\pi*{\\phi}^{(i)}(a|s) = g*{\\phi}(f*{\\theta}(s), \\text{context}*i)$$\n",
    "\n",
    "Where $g*{\\phi}$ is a task-specific head and $\\text{context}*i$ provides task information.\n",
    "\n",
    "**Multi-Task Objective**:\n",
    "$$\\mathcal{L} = \\sum*{i=1}^{T} w*i \\mathcal{L}*i(\\pi*{\\phi}^{(i)}) + \\lambda \\mathcal{L}_{\\text{reg}}(\\theta, \\phi)$$\n",
    "\n",
    "Where $T$ is number of tasks, $w*i$ are task weights, and $\\mathcal{L}*{\\text{reg}}$ is regularization.\n",
    "\n",
    "### Transfer Learning in Rl\n",
    "\n",
    "**Three Paradigms**:\n",
    "\n",
    "1. **Feature Transfer**: Use pre-trained features\n",
    "   $$\\pi(a|s) = \\text{Head}(\\text{FrozenFoundationModel}(s))$$\n",
    "\n",
    "2. **Fine-Tuning**: Adapt entire model\n",
    "   $$\\theta^{*} = \\arg\\min*{\\theta} \\mathcal{L}*{\\text{task}}(\\theta) + \\lambda ||\\theta - \\theta_0||^2$$\n",
    "\n",
    "3. **Prompt-Based Learning**: Task specification through prompts\n",
    "   $$\\pi(a|s, p) = \\text{FoundationModel}(s, p)$$\n",
    "   \n",
    "   Where $p$ is a task-specific prompt.\n",
    "\n",
    "### Cross-modal Learning\n",
    "\n",
    "**Vision-Language-Action Models**:\n",
    "$$\\pi(a|v, l) = f(v, l) \\text{ where } v \\in \\mathcal{V}, l \\in \\mathcal{L}, a \\in \\mathcal{A}$$\n",
    "\n",
    "**Unified Representations**:\n",
    "- Visual observations $\\rightarrow$ Vision transformer features\n",
    "- Language instructions $\\rightarrow$ Language model embeddings  \n",
    "- Actions $\\rightarrow$ Shared action space representations\n",
    "\n",
    "**Cross-Modal Alignment**:\n",
    "$$\\mathcal{L}*{\\text{align}} = ||\\text{Embed}*V(v) - \\text{Embed}_L(\\text{describe}(v))||^2$$\n",
    "\n",
    "## 4.2 Large Language Models for Rl\n",
    "\n",
    "### Llms as World Models\n",
    "\n",
    "**Chain-of-Thought Reasoning**:\n",
    "```\n",
    "Thought: I need to navigate to the goal while avoiding obstacles.\n",
    "Action: Move right to avoid the wall on the left.\n",
    "Observation: I see a clear path ahead.\n",
    "Thought: The goal is north of my position.\n",
    "Action: Move up toward the goal.\n",
    "```\n",
    "\n",
    "**Structured Reasoning**:\n",
    "$$\\text{Action} = \\text{LLM}(\\text{State}, \\text{Goal}, \\text{History}, \\text{Reasoning Template})$$\n",
    "\n",
    "### Prompt Engineering for Rl\n",
    "\n",
    "**Task Specification Prompts**:\n",
    "```\n",
    "Task: Navigate a robot to collect all gems in a maze.\n",
    "Rules: \n",
    "- Avoid obstacles (marked as #)\n",
    "- Collect gems (marked as *)  \n",
    "- Reach exit (marked as E)\n",
    "Current state: [ASCII representation]\n",
    "Choose action: [up, down, left, right]\n",
    "```\n",
    "\n",
    "**Few-Shot Learning Prompts**:\n",
    "```\n",
    "Example 1:\n",
    "State: Agent at (0,0), Goal at (1,1), No obstacles\n",
    "Action: right (move toward goal)\n",
    "Result: Reached (1,0)\n",
    "\n",
    "Example 2: \n",
    "State: Agent at (1,0), Goal at (1,1)\n",
    "Action: up (move toward goal)\n",
    "Result: Reached goal, +10 reward\n",
    "\n",
    "Current situation:\n",
    "State: [current state]\n",
    "Action: [your choice]\n",
    "```\n",
    "\n",
    "### Llm-based Hierarchical Planning\n",
    "\n",
    "**High-Level Planning**:\n",
    "$$\\text{Subgoals} = \\text{LLM}_{\\text{planner}}(\\text{Task}, \\text{Environment})$$\n",
    "\n",
    "**Low-Level Execution**:\n",
    "$$a*t = \\pi*{\\text{low}}(s*t, \\text{current\\*subgoal})$$\n",
    "\n",
    "**Plan Refinement**:\n",
    "$$\\text{Updated\\*Plan} = \\text{LLM}*{\\text{planner}}(\\text{Original\\*Plan}, \\text{Execution\\*Feedback})$$\n",
    "\n",
    "## 4.3 Vision Transformers in Rl\n",
    "\n",
    "### Vit for State Representation\n",
    "\n",
    "**Patch Embedding**:\n",
    "$$\\text{Patches} = \\text{Reshape}(\\text{Image}_{H \\times W \\times C}) \\rightarrow \\mathbb{R}^{N \\times P^2 \\cdot C}$$\n",
    "\n",
    "Where $N = HW/P^2$ is number of patches and $P$ is patch size.\n",
    "\n",
    "**Spatial-Temporal Attention**:\n",
    "- **Spatial**: Attend to important regions in current frame\n",
    "- **Temporal**: Attend to relevant frames in history\n",
    "- **Action**: Attend to action-relevant features\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "**Action Prediction Head**:\n",
    "$$\\pi(a|s) = \\text{MLP}(\\text{ViT}(s)[\\text{CLS}])$$\n",
    "\n",
    "Where $[\\text{CLS}]$ is the classification token embedding.\n",
    "\n",
    "### Multi-modal Fusion\n",
    "\n",
    "**Visual-Language Fusion**:\n",
    "$$h*{\\text{fused}} = \\text{Attention}(h*{\\text{vision}}, h*{\\text{language}}, h*{\\text{language}})$$\n",
    "\n",
    "**Hierarchical Feature Integration**:\n",
    "- **Low-level**: Pixel features, edge detection\n",
    "- **Mid-level**: Objects, spatial relationships  \n",
    "- **High-level**: Scene understanding, semantic concepts\n",
    "\n",
    "### Attention-based Policy Networks\n",
    "\n",
    "**Self-Attention for State Processing**:\n",
    "$$A_{\\text{state}} = \\text{SelfAttention}(\\text{StateFeatures})$$\n",
    "\n",
    "**Cross-Attention for Action Selection**:\n",
    "$$A_{\\text{action}} = \\text{CrossAttention}(\\text{ActionQueries}, \\text{StateFeatures})$$\n",
    "\n",
    "**Multi-Head Architecture**:\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}*1, \\ldots, \\text{head}*h)W^O$$\n",
    "\n",
    "## 4.4 Foundation Model Training Strategies\n",
    "\n",
    "### Pre-training Objectives\n",
    "\n",
    "**Masked Language Modeling (MLM)**:\n",
    "$$\\mathcal{L}*{\\text{MLM}} = -\\sum*{i \\in \\text{masked}} \\log p(x*i | x*{\\setminus i})$$\n",
    "\n",
    "**Masked Image Modeling (MIM)**:  \n",
    "$$\\mathcal{L}_{\\text{MIM}} = ||\\text{Reconstruct}(\\text{Mask}(\\text{Image})) - \\text{Image}||^2$$\n",
    "\n",
    "**Contrastive Learning**:\n",
    "$$\\mathcal{L}*{\\text{contrastive}} = -\\log \\frac{\\exp(\\text{sim}(z*i, z*j)/\\tau)}{\\sum*{k} \\exp(\\text{sim}(z*i, z*k)/\\tau)}$$\n",
    "\n",
    "### Multi-task Pre-training\n",
    "\n",
    "**Joint Training Objective**:\n",
    "$$\\mathcal{L}*{\\text{joint}} = \\sum*{t=1}^{T} \\lambda*t \\mathcal{L}*t + \\mathcal{L}_{\\text{reg}}$$\n",
    "\n",
    "**Task Sampling Strategies**:\n",
    "- **Uniform Sampling**: Equal probability for all tasks\n",
    "- **Importance Sampling**: Weight by task difficulty/importance\n",
    "- **Curriculum Learning**: Gradually increase task complexity\n",
    "\n",
    "**Parameter Sharing Strategies**:\n",
    "- **Shared Encoder**: Common feature extraction\n",
    "- **Task-Specific Heads**: Specialized output layers\n",
    "- **Adapter Layers**: Small task-specific modifications\n",
    "\n",
    "### Fine-tuning Approaches\n",
    "\n",
    "**Full Fine-Tuning**:\n",
    "- Update all parameters for target task\n",
    "- Risk of catastrophic forgetting\n",
    "- Requires substantial computational resources\n",
    "\n",
    "**Parameter-Efficient Fine-Tuning**:\n",
    "\n",
    "**LoRA (Low-Rank Adaptation)**:\n",
    "$$W' = W + AB$$\n",
    "where $A \\in \\mathbb{R}^{d \\times r}$, $B \\in \\mathbb{R}^{r \\times d}$ with $r << d$.\n",
    "\n",
    "**Adapter Layers**:\n",
    "$$h' = h + \\text{Adapter}(h) = h + W*2 \\sigma(W*1 h + b*1) + b*2$$\n",
    "\n",
    "**Prefix Tuning**:\n",
    "Add learnable prefix vectors to transformer inputs.\n",
    "\n",
    "### Continual Learning for Foundation Models\n",
    "\n",
    "**Elastic Weight Consolidation (EWC)**:\n",
    "$$\\mathcal{L}*{\\text{EWC}} = \\mathcal{L}*{\\text{task}} + \\lambda \\sum*i F*i (\\theta*i - \\theta*i^*)^2$$\n",
    "\n",
    "Where $F_i$ is Fisher information matrix diagonal.\n",
    "\n",
    "**Progressive Networks**:\n",
    "- Freeze previous task parameters\n",
    "- Add new columns for new tasks\n",
    "- Lateral connections for knowledge transfer\n",
    "\n",
    "**Meta-Learning for Rapid Adaptation**:\n",
    "$$\\theta' = \\theta - \\alpha \\nabla*{\\theta} \\mathcal{L}*{\\text{support}}(\\theta)$$\n",
    "$$\\mathcal{L}*{\\text{meta}} = \\mathbb{E}*{\\text{tasks}} [\\mathcal{L}_{\\text{query}}(\\theta')]$$\n",
    "\n",
    "## 4.5 Emergent Capabilities\n",
    "\n",
    "### Few-shot Task Learning\n",
    "Foundation models demonstrate remarkable ability to adapt to new tasks with minimal examples:\n",
    "\n",
    "**In-Context Learning**:\n",
    "- Provide examples in input prompt\n",
    "- Model adapts without parameter updates\n",
    "- Emergent capability from scale and diversity\n",
    "\n",
    "**Meta-Learning Through Pre-Training**:\n",
    "- Learn to learn from pre-training data distribution\n",
    "- Transfer learning strategies emerge naturally\n",
    "- Rapid adaptation to distribution shifts\n",
    "\n",
    "### Compositional Reasoning\n",
    "Combine primitive skills to solve complex tasks:\n",
    "\n",
    "**Skill Composition**:\n",
    "$$\\text{ComplexTask} = \\text{Compose}(\\text{Skill}*1, \\text{Skill}*2, \\ldots, \\text{Skill}_k)$$\n",
    "\n",
    "**Hierarchical Planning**:\n",
    "- Decompose complex goals into subgoals\n",
    "- Learn primitive skills for subgoal achievement\n",
    "- Compose skills dynamically based on context\n",
    "\n",
    "### Cross-domain Transfer\n",
    "Knowledge learned in one domain transfers to related domains:\n",
    "\n",
    "**Domain Adaptation**:\n",
    "$$\\mathcal{L}*{\\text{adapt}} = \\mathcal{L}*{\\text{target}} + \\lambda \\mathcal{L}_{\\text{domain}}$$\n",
    "\n",
    "**Universal Policies**:\n",
    "Single policy that works across multiple environments with different dynamics, observation spaces, and action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dba8e3",
   "metadata": {},
   "source": [
    "# Conclusion and Future Directions\n",
    "\n",
    "## Summary of Advanced Deep Rl Concepts\n",
    "\n",
    "This notebook has explored cutting-edge topics in Deep Reinforcement Learning that represent the current frontier of research and applications. We covered four major paradigms:\n",
    "\n",
    "### 1. Continual Learning in Rl\n",
    "- **Key Insight**: Agents must learn new tasks while retaining knowledge from previous experiences\n",
    "- **Main Challenges**: Catastrophic forgetting, interference between tasks, scalability\n",
    "- **Solutions**: Elastic Weight Consolidation, Progressive Networks, Meta-learning approaches\n",
    "- **Applications**: Robotics, adaptive systems, lifelong learning agents\n",
    "\n",
    "### 2. Neurosymbolic Reinforcement Learning\n",
    "- **Key Insight**: Combining neural learning with symbolic reasoning for interpretable and robust agents\n",
    "- **Main Challenges**: Integration of continuous and discrete representations, knowledge representation\n",
    "- **Solutions**: Differentiable programming, logic-based constraints, hybrid architectures\n",
    "- **Applications**: Autonomous systems, healthcare, safety-critical domains\n",
    "\n",
    "### 3. Human-ai Collaborative Learning\n",
    "- **Key Insight**: Leverage human expertise and feedback to improve agent learning and performance\n",
    "- **Main Challenges**: Trust modeling, preference learning, real-time collaboration\n",
    "- **Solutions**: RLHF, preference-based rewards, shared autonomy frameworks\n",
    "- **Applications**: Human-robot interaction, personalized AI, assisted decision-making\n",
    "\n",
    "### 4. Foundation Models in Rl\n",
    "- **Key Insight**: Pre-trained large models enable sample-efficient learning and strong generalization\n",
    "- **Main Challenges**: Transfer learning, multi-modal integration, computational efficiency\n",
    "- **Solutions**: Vision transformers, cross-modal attention, prompt engineering\n",
    "- **Applications**: General-purpose AI agents, few-shot learning, multi-task systems\n",
    "\n",
    "## Interconnections between Paradigms\n",
    "\n",
    "These four approaches are not isolated but can be combined synergistically:\n",
    "\n",
    "**Continual + Neurosymbolic**: Symbolic knowledge provides structure for continual learning, preventing catastrophic forgetting through logical constraints.\n",
    "\n",
    "**Human-AI + Foundation Models**: Foundation models provide better initialization for human-AI collaboration, while human feedback can guide foundation model fine-tuning.\n",
    "\n",
    "**Neurosymbolic + Foundation Models**: Foundation models can learn to perform symbolic reasoning, while symbolic structures can guide foundation model architectures.\n",
    "\n",
    "**All Four Combined**: A truly advanced RL system might use foundation models as initialization, incorporate human feedback for alignment, use symbolic reasoning for interpretability, and support continual learning for adaptation.\n",
    "\n",
    "## Current Research Frontiers\n",
    "\n",
    "### Emerging Challenges\n",
    "1. **Scalability**: How do these methods scale to real-world complexity?\n",
    "2. **Sample Efficiency**: Can we achieve superhuman performance with minimal data?\n",
    "3. **Robustness**: How do agents handle distribution shifts and adversarial conditions?\n",
    "4. **Alignment**: How do we ensure AI systems pursue intended objectives?\n",
    "5. **Interpretability**: Can we understand and verify agent decision-making?\n",
    "\n",
    "### Promising Directions\n",
    "1. **Unified Architectures**: Single models that combine multiple paradigms\n",
    "2. **Meta-Learning**: Learning to learn across paradigms and domains\n",
    "3. **Causal Reasoning**: Understanding cause-and-effect relationships\n",
    "4. **Compositional Learning**: Building complex behaviors from simple primitives\n",
    "5. **Multi-Agent Collaboration**: Scaling human-AI collaboration to teams\n",
    "\n",
    "## Practical Implementation Insights\n",
    "\n",
    "### Key Lessons Learned\n",
    "1. **Start Simple**: Begin with simplified versions before adding complexity\n",
    "2. **Modular Design**: Build components that can be combined and reused\n",
    "3. **Interpretability First**: Design for explainability from the beginning\n",
    "4. **Human-Centered**: Consider human factors in system design\n",
    "5. **Robust Evaluation**: Test across diverse scenarios and failure modes\n",
    "\n",
    "### Implementation Best Practices\n",
    "1. **Gradual Integration**: Introduce new paradigms incrementally\n",
    "2. **Ablation Studies**: Understand the contribution of each component\n",
    "3. **Multi-Metric Evaluation**: Use diverse evaluation criteria beyond reward\n",
    "4. **Failure Analysis**: Learn from failures and edge cases\n",
    "5. **Ethical Considerations**: Address bias, fairness, and safety concerns\n",
    "\n",
    "## Future Applications\n",
    "\n",
    "### Near-term (1-3 Years)\n",
    "- **Personalized AI Assistants**: Agents that adapt to individual preferences and learn continuously\n",
    "- **Robotic Process Automation**: Intelligent automation that can handle exceptions and learn from feedback\n",
    "- **Educational AI**: Tutoring systems that adapt teaching strategies based on student progress\n",
    "- **Healthcare Support**: AI systems that assist medical professionals with decision-making\n",
    "\n",
    "### Medium-term (3-7 Years)\n",
    "- **Autonomous Vehicles**: Self-driving cars that learn from human drivers and adapt to new environments\n",
    "- **Smart Cities**: Urban systems that optimize resource allocation through continuous learning\n",
    "- **Scientific Discovery**: AI agents that collaborate with researchers to generate and test hypotheses\n",
    "- **Creative AI**: Systems that collaborate with humans in creative endeavors\n",
    "\n",
    "### Long-term (7+ Years)\n",
    "- **General Intelligence**: AI systems that can perform any cognitive task that humans can do\n",
    "- **Scientific AI**: Autonomous systems capable of conducting independent scientific research\n",
    "- **Collaborative Societies**: Seamless integration of human and AI capabilities in all aspects of society\n",
    "- **Space Exploration**: AI systems capable of autonomous operation in extreme and unknown environments\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "The field of Deep Reinforcement Learning continues to evolve rapidly, with these advanced paradigms representing the current cutting edge. Each approach addresses fundamental limitations of traditional RL and opens new possibilities for creating more capable, reliable, and aligned AI systems.\n",
    "\n",
    "The key to success in this field is not just understanding individual techniques, but recognizing how they can be combined to create systems that are greater than the sum of their parts. As we move forward, the most impactful advances will likely come from principled integration of these paradigms with careful attention to real-world constraints and human values.\n",
    "\n",
    "### Final Recommendations for Further Learning\n",
    "\n",
    "1. **Hands-On Implementation**: Build and experiment with these systems yourself\n",
    "2. **Stay Current**: Follow recent papers and conferences (NeurIPS, ICML, ICLR, AAAI)\n",
    "3. **Interdisciplinary Learning**: Study cognitive science, philosophy, and domain-specific knowledge\n",
    "4. **Community Engagement**: Participate in research communities and open-source projects\n",
    "5. **Ethical Reflection**: Consider the societal implications of your work\n",
    "\n",
    "The future of AI lies not just in more powerful algorithms, but in systems that can learn, reason, collaborate, and adapt in ways that align with human values and capabilities. These advanced RL paradigms provide the building blocks for that future.\n",
    "\n",
    "---\n",
    "\n",
    "**Congratulations! You have completed CA16 - Advanced Topics in Deep Reinforcement Learning**\n",
    "\n",
    "This comprehensive exploration has covered the most cutting-edge approaches in modern RL research. You now have the theoretical foundations and practical implementation skills to contribute to the next generation of intelligent systems.\n",
    "\n",
    "*\"The best way to predict the future is to invent it.\"* - Alan Kay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c484b4d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0b723a23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb4c3e26",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa853d16",
   "metadata": {},
   "source": [
    "# Code Review and Improvements\n",
    "\n",
    "## Implementation Analysis\n",
    "\n",
    "### Strengths\n",
    "- Clear modular separation between agents, environments, models, and utilities enabling reusability and testing.\n",
    "- Uses PyTorch and Gymnasium consistently; training loops include evaluation checkpoints and visualization tools.\n",
    "- Includes modern RL techniques (e.g., actor-critic, policy regularization, or task-specific adaptations) tailored to the assignment.\n",
    "\n",
    "### Weaknesses / Risks\n",
    "- Training loops may not use mixed precision, gradient accumulation, or automated checkpoint pruning, which can limit scale and throughput.\n",
    "- Data collection and buffer management can grow unbounded in long-running experiments.\n",
    "- Limited explicit uncertainty modeling (if task requires stochastic predictions or safe exploration).\n",
    "\n",
    "## Suggested Improvements\n",
    "\n",
    "1. Computational Efficiency\n",
    "- Add mixed precision with torch.cuda.amp and a GradScaler.\n",
    "- Use batched, parallel environment collectors (e.g., vectorized envs or multiprocessing) to increase sample throughput.\n",
    "\n",
    "2. Memory & Replay\n",
    "- Replace naive lists of trajectories with a circular PrioritizedReplayBuffer to bound memory and focus learning on useful transitions.\n",
    "\n",
    "3. Model & Architecture\n",
    "- Consider deeper residual blocks, or transformer-style modules for sequence/dynamics modeling if sequences are long.\n",
    "- Add uncertainty heads (mean + variance) to dynamics or reward predictors for robust planning and safe exploration.\n",
    "\n",
    "## Advanced Techniques to Try\n",
    "- Meta-learning (MAML/RL^2) for rapid adaptation across tasks.\n",
    "- Contrastive representation learning to improve latent structure and sample efficiency.\n",
    "- Hierarchical RL for long-horizon tasks: temporal abstraction and options.\n",
    "\n",
    "## Performance and Scaling\n",
    "- Add gradient accumulation to emulate large batch training without extra memory.\n",
    "- Use model parallelism or pipeline parallelism for very large networks.\n",
    "- Implement early stopping based on validation metrics and retain best checkpoints via a ModelVersionManager.\n",
    "\n",
    "## Monitoring, Validation, and Reproducibility\n",
    "- Integrate experiment tracking (Weights & Biases, TensorBoard) to record hyperparameters, metrics, and artifacts.\n",
    "- Create a small test suite validating core API contracts for agents (select_action, update, save/load) and a world-model validation suite (prediction errors, physics checks).\n",
    "- Add explicit random seed setting in `utils.set_seed(seed)` and log seeds with experiment metadata.\n",
    "\n",
    "## Deployment Considerations\n",
    "- Save models with metadata, version hashes, and performance metrics. Use torch.jit or ONNX for inference performance if needed.\n",
    "- Provide a small FastAPI wrapper for inference and an example `serve_world_model.py` for production predictions.\n",
    "\n",
    "## Future Research Directions\n",
    "- Explore continual learning to adapt without catastrophic forgetting.\n",
    "- Investigate causal representations to improve generalization under distribution shift.\n",
    "- If relevant, consider neuromorphic-friendly architectures or quantum approaches for experimental research avenues.\n",
    "\n",
    "## Best Practices Summary\n",
    "- Start with a small, well-tested baseline; progressively add complexity and validate via ablation.\n",
    "- Monitor computational costs and maintain reproducibility through versioned artifacts and fixed seeds.\n",
    "- Keep notebooks focused on pedagogy: show minimal runnable examples and point to `experiments/` scripts for larger-scale runs.\n",
    "\n",
    "This section provides actionable items to enhance performance, robustness, and reproducibility for the CA16 codebase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
