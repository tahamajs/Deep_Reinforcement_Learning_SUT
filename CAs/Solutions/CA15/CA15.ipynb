{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "306b4334",
   "metadata": {},
   "source": [
    "# Computer Assignment 15: Advanced Deep Reinforcement Learning\n",
    "## Model-Based RL and Hierarchical RL\n",
    "\n",
    "**Course:** Deep Reinforcement Learning  \n",
    "**Institution:** Sharif University of Technology  \n",
    "**Semester:** Fall 2024\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Environment Setup](#setup)\n",
    "2. [Model-Based RL](#model-based)\n",
    "3. [Hierarchical RL](#hierarchical)\n",
    "4. [Planning Algorithms](#planning)\n",
    "5. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b57583",
   "metadata": {},
   "source": [
    "## 1. Environment Setup <a name='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bebb9da5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:16.713422Z",
     "iopub.status.busy": "2025-10-03T00:17:16.712974Z",
     "iopub.status.idle": "2025-10-03T00:17:20.967229Z",
     "shell.execute_reply": "2025-10-03T00:17:20.965974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment setup complete!\n",
      "PyTorch version: 2.8.0\n",
      "Device: CPU\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add current directory to path\n",
    "sys.path.insert(0, os.path.abspath(\".\"))\n",
    "\n",
    "# Standard libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import deque\n",
    "import random\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"✅ Environment setup complete!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c4a6b4",
   "metadata": {},
   "source": [
    "### Import CA15 Components from Local Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f028f2b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:20.975358Z",
     "iopub.status.busy": "2025-10-03T00:17:20.974649Z",
     "iopub.status.idle": "2025-10-03T00:17:21.224466Z",
     "shell.execute_reply": "2025-10-03T00:17:21.224079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported all CA15 components\n"
     ]
    }
   ],
   "source": [
    "# Import Model-Based RL components\n",
    "from model_based_rl.algorithms import (\n",
    "    DynamicsModel,\n",
    "    ModelEnsemble,\n",
    "    ModelPredictiveController,\n",
    "    DynaQAgent\n",
    ")\n",
    "\n",
    "# Import Hierarchical RL components\n",
    "from hierarchical_rl.algorithms import (\n",
    "    Option,\n",
    "    HierarchicalActorCritic,\n",
    "    GoalConditionedAgent,\n",
    "    FeudalNetwork\n",
    ")\n",
    "\n",
    "from hierarchical_rl.environments import HierarchicalRLEnvironment\n",
    "\n",
    "# Import Planning algorithms\n",
    "from planning.algorithms import (\n",
    "    MCTSNode,\n",
    "    MonteCarloTreeSearch,\n",
    "    ModelBasedValueExpansion,\n",
    "    LatentSpacePlanner,\n",
    "    WorldModel\n",
    ")\n",
    "\n",
    "# Import environments\n",
    "from environments.grid_world import SimpleGridWorld\n",
    "\n",
    "# Import utilities\n",
    "from utils import (\n",
    "    ReplayBuffer,\n",
    "    PrioritizedReplayBuffer,\n",
    "    RunningStats,\n",
    "    Logger,\n",
    "    VisualizationUtils,\n",
    "    EnvironmentUtils\n",
    ")\n",
    "\n",
    "print(\"✅ Successfully imported all CA15 components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f64a9",
   "metadata": {},
   "source": [
    "## 2. Model-Based Reinforcement Learning <a name='model-based'></a>\n",
    "\n",
    "Model-based RL learns an explicit model of the environment's dynamics.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Dynamics Model**: $\\hat{s}_{t+1} = f_\\theta(s_t, a_t)$\n",
    "- **Model Ensemble**: Multiple models for uncertainty\n",
    "- **MPC**: Model Predictive Control\n",
    "- **Dyna-Q**: Combines model-free and model-based learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb2ca7",
   "metadata": {},
   "source": [
    "### 2.1 Dynamics Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "699df922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:21.227738Z",
     "iopub.status.busy": "2025-10-03T00:17:21.227489Z",
     "iopub.status.idle": "2025-10-03T00:17:21.241485Z",
     "shell.execute_reply": "2025-10-03T00:17:21.239989Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamics Model Architecture:\n",
      "DynamicsModel(\n",
      "  (transition_net): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=128, out_features=5, bias=True)\n",
      "  )\n",
      "  (uncertainty_net): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=5, bias=True)\n",
      "    (3): Softplus(beta=1.0, threshold=20.0)\n",
      "  )\n",
      ")\n",
      "\n",
      "✅ Dynamics model test:\n",
      "  Input state shape: torch.Size([1, 4])\n",
      "  Predicted next state mean shape: torch.Size([1, 4])\n",
      "  Predicted reward mean shape: torch.Size([1, 1])\n",
      "  Prediction includes uncertainty (std): ['next_state_mean', 'reward_mean', 'next_state_std', 'reward_std']\n",
      "\n",
      "  Sampled next state shape: torch.Size([4])\n",
      "  Sampled reward shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# Create a simple dynamics model\n",
    "state_dim = 4\n",
    "action_dim = 2\n",
    "hidden_dim = 128\n",
    "\n",
    "dynamics_model = DynamicsModel(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=hidden_dim\n",
    ")\n",
    "\n",
    "print(\"Dynamics Model Architecture:\")\n",
    "print(dynamics_model)\n",
    "\n",
    "# Test forward pass - note that forward returns a dictionary\n",
    "test_state = torch.randn(1, state_dim)\n",
    "test_action = torch.randn(1, action_dim)\n",
    "prediction = dynamics_model(test_state, test_action)\n",
    "\n",
    "print(f\"\\n✅ Dynamics model test:\")\n",
    "print(f\"  Input state shape: {test_state.shape}\")\n",
    "print(f\"  Predicted next state mean shape: {prediction['next_state_mean'].shape}\")\n",
    "print(f\"  Predicted reward mean shape: {prediction['reward_mean'].shape}\")\n",
    "print(f\"  Prediction includes uncertainty (std): {list(prediction.keys())}\")\n",
    "\n",
    "# To get a sample prediction\n",
    "next_state_sample, reward_sample = dynamics_model.sample_prediction(test_state, test_action)\n",
    "print(f\"\\n  Sampled next state shape: {next_state_sample.shape}\")\n",
    "print(f\"  Sampled reward shape: {reward_sample.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c15c4d6",
   "metadata": {},
   "source": [
    "### 2.2 Model Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6b38763",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:21.245723Z",
     "iopub.status.busy": "2025-10-03T00:17:21.245208Z",
     "iopub.status.idle": "2025-10-03T00:17:23.306236Z",
     "shell.execute_reply": "2025-10-03T00:17:23.301467Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Ensemble with 5 models created\n",
      "\n",
      "Training ensemble for 10 steps...\n",
      "  Step 0: Loss = 15.3016\n",
      "  Step 3: Loss = 10.7525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 6: Loss = 8.1383\n",
      "  Step 9: Loss = 6.1376\n",
      "\n",
      "✅ Ensemble training complete\n"
     ]
    }
   ],
   "source": [
    "# Create model ensemble\n",
    "ensemble_size = 5\n",
    "model_ensemble = ModelEnsemble(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    ensemble_size=ensemble_size\n",
    ")\n",
    "\n",
    "print(f\"Model Ensemble with {ensemble_size} models created\")\n",
    "\n",
    "# Generate dummy training data\n",
    "batch_size = 32\n",
    "states = torch.randn(batch_size, state_dim)\n",
    "actions = torch.randn(batch_size, action_dim)\n",
    "next_states = torch.randn(batch_size, state_dim)\n",
    "rewards = torch.randn(batch_size)\n",
    "\n",
    "# Train ensemble\n",
    "print(\"\\nTraining ensemble for 10 steps...\")\n",
    "for i in range(10):\n",
    "    loss = model_ensemble.train_step(states, actions, next_states, rewards)\n",
    "    if i % 3 == 0:\n",
    "        print(f\"  Step {i}: Loss = {loss:.4f}\")\n",
    "\n",
    "print(\"\\n✅ Ensemble training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f20d12",
   "metadata": {},
   "source": [
    "### 2.3 Model Predictive Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e03cdc21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:23.314577Z",
     "iopub.status.busy": "2025-10-03T00:17:23.313252Z",
     "iopub.status.idle": "2025-10-03T00:17:23.644214Z",
     "shell.execute_reply": "2025-10-03T00:17:23.643509Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPC Controller created\n",
      "  Planning horizon: 10\n",
      "  Action samples: 100\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension specified as 0 but tensor has no dimensions",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Plan an action\u001b[39;00m\n\u001b[32m     14\u001b[39m current_state = torch.randn(state_dim)\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m planned_action = \u001b[43mmpc_controller\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplan_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ Planned action shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplanned_action.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Planned action values: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mplanned_action\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA15/model_based_rl/algorithms.py:184\u001b[39m, in \u001b[36mModelPredictiveController.plan_action\u001b[39m\u001b[34m(self, state, goal_state)\u001b[39m\n\u001b[32m    181\u001b[39m current_state = state\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.horizon):\n\u001b[32m--> \u001b[39m\u001b[32m184\u001b[39m     next_state, reward = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_ensemble\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_mean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcurrent_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m goal_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    189\u001b[39m         goal_state_tensor = torch.FloatTensor(goal_state).to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA15/model_based_rl/algorithms.py:151\u001b[39m, in \u001b[36mModelEnsemble.predict_mean\u001b[39m\u001b[34m(self, state, action)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict_mean\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[32m    150\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get ensemble mean prediction.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpredict_ensemble\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     next_states = torch.stack([pred[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions])\n\u001b[32m    154\u001b[39m     rewards = torch.stack([pred[\u001b[32m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m pred \u001b[38;5;129;01min\u001b[39;00m predictions])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA15/model_based_rl/algorithms.py:144\u001b[39m, in \u001b[36mModelEnsemble.predict_ensemble\u001b[39m\u001b[34m(self, state, action)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.models:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         pred = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m         predictions.append(pred)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m predictions\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA15/model_based_rl/algorithms.py:83\u001b[39m, in \u001b[36mDynamicsModel.sample_prediction\u001b[39m\u001b[34m(self, state, action)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_prediction\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, action):\n\u001b[32m     82\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sample from the predictive distribution.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m     next_state = torch.normal(output[\u001b[33m\"\u001b[39m\u001b[33mnext_state_mean\u001b[39m\u001b[33m\"\u001b[39m], output[\u001b[33m\"\u001b[39m\u001b[33mnext_state_std\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     86\u001b[39m     reward = torch.normal(output[\u001b[33m\"\u001b[39m\u001b[33mreward_mean\u001b[39m\u001b[33m\"\u001b[39m], output[\u001b[33m\"\u001b[39m\u001b[33mreward_std\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/uni/DRL/CAs/Solutions/CA15/model_based_rl/algorithms.py:57\u001b[39m, in \u001b[36mDynamicsModel.forward\u001b[39m\u001b[34m(self, state, action)\u001b[39m\n\u001b[32m     54\u001b[39m     action = action.unsqueeze(\u001b[32m0\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m action.dtype == torch.long:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     action_one_hot = torch.zeros(\u001b[43maction\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m.action_dim).to(\n\u001b[32m     58\u001b[39m         action.device\n\u001b[32m     59\u001b[39m     )\n\u001b[32m     60\u001b[39m     action_one_hot.scatter_(\u001b[32m1\u001b[39m, action.unsqueeze(\u001b[32m1\u001b[39m), \u001b[32m1\u001b[39m)\n\u001b[32m     61\u001b[39m     action = action_one_hot\n",
      "\u001b[31mIndexError\u001b[39m: Dimension specified as 0 but tensor has no dimensions"
     ]
    }
   ],
   "source": [
    "# Create MPC controller\n",
    "mpc_controller = ModelPredictiveController(\n",
    "    model_ensemble=model_ensemble,\n",
    "    action_dim=action_dim,\n",
    "    horizon=10,\n",
    "    num_samples=100\n",
    ")\n",
    "\n",
    "print(\"MPC Controller created\")\n",
    "print(\"  Planning horizon: 10\")\n",
    "print(\"  Action samples: 100\")\n",
    "\n",
    "# Plan an action\n",
    "current_state = torch.randn(state_dim)\n",
    "planned_action = mpc_controller.plan_action(current_state)\n",
    "\n",
    "print(f\"\\n✅ Planned action shape: {planned_action.shape}\")\n",
    "print(f\"  Planned action values: {planned_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb702c65",
   "metadata": {},
   "source": [
    "### 2.4 Dyna-Q Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a18be6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:23.648765Z",
     "iopub.status.busy": "2025-10-03T00:17:23.648211Z",
     "iopub.status.idle": "2025-10-03T00:17:23.705661Z",
     "shell.execute_reply": "2025-10-03T00:17:23.704642Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dyna-Q Agent created\n",
      "  Q-Network: Discrete action space with 2 actions\n",
      "  Integrated dynamics model for planning\n",
      "\n",
      "✅ Selected action: 0\n",
      "  Buffer size: 1\n"
     ]
    }
   ],
   "source": [
    "# Create Dyna-Q agent\n",
    "dyna_agent = DynaQAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "print(\"Dyna-Q Agent created\")\n",
    "print(f\"  Q-Network: Discrete action space with {action_dim} actions\")\n",
    "print(f\"  Integrated dynamics model for planning\")\n",
    "\n",
    "# Test action selection\n",
    "test_state = np.random.randn(state_dim)\n",
    "action = dyna_agent.get_action(test_state, epsilon=0.1)\n",
    "\n",
    "print(f\"\\n✅ Selected action: {action}\")\n",
    "\n",
    "# Store experience\n",
    "next_state = np.random.randn(state_dim)\n",
    "dyna_agent.store_experience(test_state, action, 1.0, next_state, False)\n",
    "print(f\"  Buffer size: {len(dyna_agent.buffer)}\")\n",
    "\n",
    "# Perform some planning steps\n",
    "if len(dyna_agent.buffer) >= 5:\n",
    "    dyna_agent.planning_step(num_planning_steps=5)\n",
    "    print(f\"  Performed 5 planning steps using the learned model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8c3eb6",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Reinforcement Learning <a name='hierarchical'></a>\n",
    "\n",
    "Hierarchical RL uses temporal abstraction for long-horizon tasks.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Options Framework**: Temporally extended actions\n",
    "- **Goal-Conditioned RL**: Policies conditioned on goals\n",
    "- **HAC**: Hierarchical Actor-Critic\n",
    "- **Feudal Networks**: Manager-worker hierarchy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dc335c",
   "metadata": {},
   "source": [
    "### 3.1 Options Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcb2fb68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:23.709504Z",
     "iopub.status.busy": "2025-10-03T00:17:23.709040Z",
     "iopub.status.idle": "2025-10-03T00:17:23.758896Z",
     "shell.execute_reply": "2025-10-03T00:17:23.757808Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Option.__init__() got an unexpected keyword argument 'option_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.random.random() < \u001b[32m0.1\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Create option\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m option = \u001b[43mOption\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitiation_set\u001b[49m\u001b[43m=\u001b[49m\u001b[43msimple_initiation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[43msimple_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtermination_condition\u001b[49m\u001b[43m=\u001b[49m\u001b[43msimple_termination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43moption_id\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOption created (ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moption.option_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Test option\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: Option.__init__() got an unexpected keyword argument 'option_id'"
     ]
    }
   ],
   "source": [
    "# Define option components\n",
    "def simple_initiation(state):\n",
    "    \"\"\"Option can be initiated anywhere\"\"\"\n",
    "    return True\n",
    "\n",
    "def simple_policy(state):\n",
    "    \"\"\"Simple random policy for the option\"\"\"\n",
    "    return np.random.randint(0, action_dim)\n",
    "\n",
    "def simple_termination(state):\n",
    "    \"\"\"Terminate with low probability\"\"\"\n",
    "    return np.random.random() < 0.1\n",
    "\n",
    "# Create option\n",
    "option = Option(\n",
    "    initiation_set=simple_initiation,\n",
    "    policy=simple_policy,\n",
    "    termination_condition=simple_termination,\n",
    "    option_id=0\n",
    ")\n",
    "\n",
    "print(f\"Option created (ID: {option.option_id})\")\n",
    "\n",
    "# Test option\n",
    "test_state = np.random.randn(state_dim)\n",
    "print(f\"\\n✅ Option test:\")\n",
    "print(f\"  Can initiate: {option.can_initiate(test_state)}\")\n",
    "print(f\"  Action: {option.get_action(test_state)}\")\n",
    "print(f\"  Should terminate: {option.should_terminate(test_state)}\")\n",
    "\n",
    "# Simulate option execution\n",
    "print(f\"\\n  Simulating option execution:\")\n",
    "steps = 0\n",
    "while not option.should_terminate(test_state) and steps < 10:\n",
    "    action = option.get_action(test_state)\n",
    "    test_state = np.random.randn(state_dim)  # Simulate state transition\n",
    "    steps += 1\n",
    "print(f\"  Option executed for {steps} steps before terminating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8304bfa4",
   "metadata": {},
   "source": [
    "### 3.2 Hierarchical Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccc916c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:23.762426Z",
     "iopub.status.busy": "2025-10-03T00:17:23.762167Z",
     "iopub.status.idle": "2025-10-03T00:17:23.812651Z",
     "shell.execute_reply": "2025-10-03T00:17:23.811762Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HierarchicalActorCritic.__init__() got an unexpected keyword argument 'subgoal_dims'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create HAC agent\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m hac_agent = \u001b[43mHierarchicalActorCritic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_levels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubgoal_dims\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mHierarchical Actor-Critic created\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Number of levels: 3\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: HierarchicalActorCritic.__init__() got an unexpected keyword argument 'subgoal_dims'"
     ]
    }
   ],
   "source": [
    "# Create HAC agent\n",
    "hac_agent = HierarchicalActorCritic(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    num_levels=3,\n",
    "    subgoal_dims=[8, 6, 4],\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "print(\"Hierarchical Actor-Critic created\")\n",
    "print(f\"  Number of levels: 3\")\n",
    "print(f\"  Subgoal dimensions: [8, 6, 4]\")\n",
    "\n",
    "# Test action selection at different levels\n",
    "test_state = np.random.randn(state_dim)\n",
    "print(f\"\\n✅ Hierarchical action selection:\")\n",
    "action = hac_agent.select_action(test_state, level=0)\n",
    "print(f\"  Level 0 (highest) output: {action.shape if hasattr(action, 'shape') else action}\")\n",
    "\n",
    "# The lowest level produces primitive actions\n",
    "if hac_agent.num_levels > 1:\n",
    "    primitive_action = hac_agent.select_action(test_state, level=hac_agent.num_levels-1)\n",
    "    print(f\"  Level {hac_agent.num_levels-1} (lowest) action: {primitive_action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858e5867",
   "metadata": {},
   "source": [
    "### 3.3 Goal-Conditioned Agent with HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "231eb9df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:23.815721Z",
     "iopub.status.busy": "2025-10-03T00:17:23.815419Z",
     "iopub.status.idle": "2025-10-03T00:17:23.878940Z",
     "shell.execute_reply": "2025-10-03T00:17:23.877679Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GoalConditionedAgent.__init__() got an unexpected keyword argument 'lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create goal-conditioned agent\u001b[39;00m\n\u001b[32m      2\u001b[39m goal_dim = \u001b[32m4\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m gc_agent = \u001b[43mGoalConditionedAgent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgoal_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgoal_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mher_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGoal-Conditioned Agent with Hindsight Experience Replay\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  State dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: GoalConditionedAgent.__init__() got an unexpected keyword argument 'lr'"
     ]
    }
   ],
   "source": [
    "# Create goal-conditioned agent\n",
    "goal_dim = 4\n",
    "gc_agent = GoalConditionedAgent(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    goal_dim=goal_dim,\n",
    "    lr=1e-3,\n",
    "    her_k=4\n",
    ")\n",
    "\n",
    "print(\"Goal-Conditioned Agent with Hindsight Experience Replay\")\n",
    "print(f\"  State dimension: {state_dim}\")\n",
    "print(f\"  Goal dimension: {goal_dim}\")\n",
    "print(f\"  HER replay ratio k: 4 (generates 4 additional goals per transition)\")\n",
    "\n",
    "# Test goal-conditioned action\n",
    "test_state = np.random.randn(state_dim)\n",
    "test_goal = np.random.randn(goal_dim)\n",
    "action = gc_agent.get_action(test_state, test_goal, noise_scale=0.1)\n",
    "\n",
    "print(f\"\\n✅ Goal-conditioned action: {action}\")\n",
    "print(f\"  Action shape: {action.shape}\")\n",
    "\n",
    "# Store experience with HER\n",
    "next_state = np.random.randn(state_dim)\n",
    "reward = -np.linalg.norm(next_state - test_goal)\n",
    "gc_agent.store_experience(test_state, action, reward, next_state, False, test_goal)\n",
    "\n",
    "print(f\"\\n  Experience stored with HER augmentation\")\n",
    "print(f\"  Buffer size: {len(gc_agent.buffer)}\")\n",
    "print(f\"  (HER creates {gc_agent.her_k} additional synthetic transitions per real transition)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc51885",
   "metadata": {},
   "source": [
    "### 3.4 Feudal Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1066bd7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:23.883900Z",
     "iopub.status.busy": "2025-10-03T00:17:23.883165Z",
     "iopub.status.idle": "2025-10-03T00:17:23.919791Z",
     "shell.execute_reply": "2025-10-03T00:17:23.918945Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FeudalNetwork.__init__() got an unexpected keyword argument 'manager_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create feudal network\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m feudal_agent = \u001b[43mFeudalNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmanager_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mworker_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFeudal Network (Manager-Worker Hierarchy)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Manager goal dimension: 16\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: FeudalNetwork.__init__() got an unexpected keyword argument 'manager_dim'"
     ]
    }
   ],
   "source": [
    "# Create feudal network\n",
    "feudal_agent = FeudalNetwork(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    manager_dim=16,\n",
    "    worker_dim=32,\n",
    "    lr=1e-3\n",
    ")\n",
    "\n",
    "print(\"Feudal Network (Manager-Worker Hierarchy)\")\n",
    "print(f\"  Manager goal dimension: 16\")\n",
    "print(f\"  Worker hidden dimension: 32\")\n",
    "\n",
    "# Test action selection\n",
    "test_state = np.random.randn(state_dim)\n",
    "action, goal = feudal_agent.select_action(test_state)\n",
    "\n",
    "print(f\"\\n✅ Feudal network output:\")\n",
    "print(f\"  Worker action: {action}\")\n",
    "print(f\"  Manager goal embedding shape: {goal.shape if hasattr(goal, 'shape') else 'scalar'}\")\n",
    "print(f\"\\n  The manager produces goals for the worker to achieve\")\n",
    "print(f\"  The worker selects primitive actions to reach the manager's goal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b868c0",
   "metadata": {},
   "source": [
    "## 4. Planning Algorithms <a name='planning'></a>\n",
    "\n",
    "Advanced planning methods for decision-making.\n",
    "\n",
    "### Key Concepts:\n",
    "- **MCTS**: Monte Carlo Tree Search for discrete action spaces\n",
    "- **World Models**: Complete environment simulators with VAE\n",
    "- **Latent Space Planning**: Planning in learned low-dimensional representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5aa2a6",
   "metadata": {},
   "source": [
    "### 4.1 Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cad6f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:23.923704Z",
     "iopub.status.busy": "2025-10-03T00:17:23.923009Z",
     "iopub.status.idle": "2025-10-03T00:17:23.971179Z",
     "shell.execute_reply": "2025-10-03T00:17:23.970106Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MonteCarloTreeSearch.__init__() got an unexpected keyword argument 'env'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Create MCTS\u001b[39;00m\n\u001b[32m     19\u001b[39m mcts_env = SimpleMCTSEnv()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m mcts = \u001b[43mMonteCarloTreeSearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmcts_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexploration_constant\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\n\u001b[32m     25\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mMonte Carlo Tree Search\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Simulations per search: 50\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: MonteCarloTreeSearch.__init__() got an unexpected keyword argument 'env'"
     ]
    }
   ],
   "source": [
    "# Create simple environment for MCTS\n",
    "class SimpleMCTSEnv:\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.goal = 10\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = 0\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Actions: -1, 0, +1\n",
    "        self.state += (action - 1)\n",
    "        reward = -abs(self.state - self.goal)\n",
    "        done = abs(self.state - self.goal) < 0.5\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "# Create MCTS\n",
    "mcts_env = SimpleMCTSEnv()\n",
    "mcts = MonteCarloTreeSearch(\n",
    "    env=mcts_env,\n",
    "    num_simulations=50,\n",
    "    exploration_constant=1.4,\n",
    "    max_depth=20\n",
    ")\n",
    "\n",
    "print(\"Monte Carlo Tree Search\")\n",
    "print(f\"  Simulations per search: 50\")\n",
    "print(f\"  Exploration constant (UCB): 1.4\")\n",
    "print(f\"  Max depth: 20\")\n",
    "\n",
    "# Run MCTS\n",
    "root_state = mcts_env.reset()\n",
    "print(f\"\\n  Starting state: {root_state}, Goal: {mcts_env.goal}\")\n",
    "best_action = mcts.get_best_action(root_state)\n",
    "\n",
    "print(f\"\\n✅ MCTS best action: {best_action}\")\n",
    "print(f\"  (MCTS explored the tree and selected the most promising action)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae63f7",
   "metadata": {},
   "source": [
    "### 4.2 World Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6075d436",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:23.975384Z",
     "iopub.status.busy": "2025-10-03T00:17:23.975133Z",
     "iopub.status.idle": "2025-10-03T00:17:24.043750Z",
     "shell.execute_reply": "2025-10-03T00:17:24.039520Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "WorldModel.__init__() got an unexpected keyword argument 'state_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Create world model with VAE-style architecture\u001b[39;00m\n\u001b[32m      2\u001b[39m latent_dim = \u001b[32m16\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m world_model = \u001b[43mWorldModel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstate_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWorld Model (VAE-based)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  State dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: WorldModel.__init__() got an unexpected keyword argument 'state_dim'"
     ]
    }
   ],
   "source": [
    "# Create world model with VAE-style architecture\n",
    "latent_dim = 16\n",
    "world_model = WorldModel(\n",
    "    state_dim=state_dim,\n",
    "    action_dim=action_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    hidden_dim=128\n",
    ")\n",
    "\n",
    "print(\"World Model (VAE-based)\")\n",
    "print(f\"  State dimension: {state_dim}\")\n",
    "print(f\"  Latent dimension: {latent_dim}\")\n",
    "print(f\"  Hidden dimension: 128\")\n",
    "\n",
    "# Test encoding\n",
    "test_state = torch.randn(1, state_dim)\n",
    "latent_mean, latent_log_var = world_model.encode(test_state)\n",
    "latent = world_model.reparameterize(latent_mean, latent_log_var)\n",
    "\n",
    "print(f\"\\n✅ World model encoding:\")\n",
    "print(f\"  Input state shape: {test_state.shape}\")\n",
    "print(f\"  Latent mean shape: {latent_mean.shape}\")\n",
    "print(f\"  Latent log_var shape: {latent_log_var.shape}\")\n",
    "print(f\"  Sampled latent shape: {latent.shape}\")\n",
    "\n",
    "# Test decoding\n",
    "reconstructed = world_model.decode(latent)\n",
    "print(f\"\\n  Reconstruction:\")\n",
    "print(f\"  Reconstructed state shape: {reconstructed.shape}\")\n",
    "reconstruction_error = F.mse_loss(reconstructed, test_state)\n",
    "print(f\"  Reconstruction MSE: {reconstruction_error.item():.4f}\")\n",
    "\n",
    "# Test dynamics prediction in latent space\n",
    "test_action = torch.randn(1, action_dim)\n",
    "next_latent = world_model.predict_next_latent(latent, test_action)\n",
    "predicted_reward = world_model.predict_reward(latent, test_action)\n",
    "\n",
    "print(f\"\\n✅ Latent dynamics prediction:\")\n",
    "print(f\"  Current latent shape: {latent.shape}\")\n",
    "print(f\"  Action shape: {test_action.shape}\")\n",
    "print(f\"  Next latent shape: {next_latent.shape}\")\n",
    "print(f\"  Predicted reward shape: {predicted_reward.shape}\")\n",
    "\n",
    "# Decode next latent to see predicted next state\n",
    "predicted_next_state = world_model.decode(next_latent)\n",
    "print(f\"\\n  Predicted next state shape: {predicted_next_state.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92929d6",
   "metadata": {},
   "source": [
    "### 4.3 Latent Space Planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b502141",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-03T00:17:24.049043Z",
     "iopub.status.busy": "2025-10-03T00:17:24.048619Z",
     "iopub.status.idle": "2025-10-03T00:17:24.111035Z",
     "shell.execute_reply": "2025-10-03T00:17:24.109904Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LatentSpacePlanner.__init__() got an unexpected keyword argument 'dynamics_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     20\u001b[39m reward_model = nn.Sequential(\n\u001b[32m     21\u001b[39m     nn.Linear(latent_dim + action_dim, \u001b[32m64\u001b[39m),\n\u001b[32m     22\u001b[39m     nn.ReLU(),\n\u001b[32m     23\u001b[39m     nn.Linear(\u001b[32m64\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m     24\u001b[39m )\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Create latent planner\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m latent_planner = \u001b[43mLatentSpacePlanner\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdynamics_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatent_dynamics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreward_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlatent_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mplanning_horizon\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\n\u001b[32m     34\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLatent Space Planner\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Latent dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatent_dim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: LatentSpacePlanner.__init__() got an unexpected keyword argument 'dynamics_model'"
     ]
    }
   ],
   "source": [
    "# Create components for latent planner\n",
    "encoder = nn.Sequential(\n",
    "    nn.Linear(state_dim, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, latent_dim)\n",
    ")\n",
    "\n",
    "decoder = nn.Sequential(\n",
    "    nn.Linear(latent_dim, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, state_dim)\n",
    ")\n",
    "\n",
    "latent_dynamics = DynamicsModel(\n",
    "    state_dim=latent_dim,\n",
    "    action_dim=action_dim,\n",
    "    hidden_dim=64\n",
    ")\n",
    "\n",
    "reward_model = nn.Sequential(\n",
    "    nn.Linear(latent_dim + action_dim, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "\n",
    "# Create latent planner\n",
    "latent_planner = LatentSpacePlanner(\n",
    "    encoder=encoder,\n",
    "    decoder=decoder,\n",
    "    dynamics_model=latent_dynamics,\n",
    "    reward_model=reward_model,\n",
    "    latent_dim=latent_dim,\n",
    "    planning_horizon=10\n",
    ")\n",
    "\n",
    "print(\"Latent Space Planner\")\n",
    "print(f\"  Latent dimension: {latent_dim}\")\n",
    "print(f\"  Planning horizon: 10 steps\")\n",
    "print(f\"  Plans in learned latent space (more efficient)\")\n",
    "\n",
    "# Test planning in latent space\n",
    "test_state = torch.randn(state_dim)\n",
    "planned_actions = latent_planner.plan(test_state, num_candidates=50)\n",
    "\n",
    "print(f\"\\n✅ Latent space planning:\")\n",
    "print(f\"  Input state shape: {test_state.shape}\")\n",
    "print(f\"  Number of candidate action sequences: 50\")\n",
    "print(f\"  Planned actions shape: {planned_actions.shape}\")\n",
    "print(f\"  (Actions for next step from the best sequence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff8ede3",
   "metadata": {},
   "source": [
    "## 5. Conclusion <a name='conclusion'></a>\n",
    "\n",
    "This notebook demonstrated the implementation and usage of advanced deep RL algorithms:\n",
    "\n",
    "### ✅ Model-Based RL:\n",
    "- **Dynamics Models**: Neural networks that learn environment transitions\n",
    "- **Model Ensembles**: Multiple models for uncertainty quantification\n",
    "- **MPC**: Model Predictive Control for planning with learned models\n",
    "- **Dyna-Q**: Integrates model-free Q-learning with model-based planning\n",
    "\n",
    "### ✅ Hierarchical RL:\n",
    "- **Options Framework**: Temporal abstraction with initiation, policy, and termination\n",
    "- **Hierarchical Actor-Critic**: Multi-level policies with subgoal generation\n",
    "- **Goal-Conditioned RL**: Policies conditioned on desired goals with HER\n",
    "- **Feudal Networks**: Manager-worker hierarchy with goal embeddings\n",
    "\n",
    "### ✅ Planning Algorithms:\n",
    "- **MCTS**: Tree-based search for discrete action spaces\n",
    "- **World Models**: VAE-based environment simulators with latent dynamics\n",
    "- **Latent Space Planning**: Efficient planning in learned representations\n",
    "\n",
    "### Key Advantages:\n",
    "\n",
    "1. **Sample Efficiency**: Model-based methods significantly reduce required environment interactions\n",
    "2. **Long-Horizon Tasks**: Hierarchical methods enable solving tasks with temporal dependencies\n",
    "3. **Generalization**: Goal-conditioned agents transfer knowledge across different goals\n",
    "4. **Planning**: Learned models enable sophisticated look-ahead and strategic decision-making\n",
    "5. **Uncertainty Awareness**: Ensemble methods and probabilistic models quantify prediction uncertainty\n",
    "\n",
    "### When to Use Each Method:\n",
    "\n",
    "- **Model-Based RL**: When environment interactions are expensive (robotics, real-world applications)\n",
    "- **Hierarchical RL**: For tasks requiring long sequences of actions or natural skill decomposition\n",
    "- **Planning**: When you need explicit reasoning about future outcomes or exploration\n",
    "- **Goal-Conditioned**: For multi-task learning or when goals change frequently\n",
    "\n",
    "### Future Directions:\n",
    "\n",
    "- Combining model-based and hierarchical approaches for maximum efficiency\n",
    "- Meta-learning for rapid adaptation to new environments\n",
    "- World models for imagination and offline learning\n",
    "- Hierarchical planning in learned latent spaces\n",
    "\n",
    "---\n",
    "\n",
    "**All implementations are modular and importable from the corresponding Python modules.**\n",
    "\n",
    "**Sharif University of Technology - Deep Reinforcement Learning - Fall 2024**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
