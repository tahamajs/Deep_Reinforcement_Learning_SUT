{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed58e69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make CA15 package importable when running this notebook\n",
    "import sys\n",
    "import os\n",
    "# Add current directory and parent dir to sys.path (helps when launching notebook from different working directories)\n",
    "sys.path.insert(0, os.path.abspath(\".\"))\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "print(\"Configured sys.path for CA15 imports:\", sys.path[0:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf055d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick smoke test for imports — run this cell to check compatibility\n",
    "try:\n",
    "    import CA15\n",
    "    print(\"CA15 package version:\", CA15.get_version())\n",
    "    from CA15.environments.grid_world import SimpleGridWorld\n",
    "    from CA15.model_based_rl.algorithms import DynamicsModel\n",
    "    print(\"Imported symbols:\", SimpleGridWorld.__name__, DynamicsModel.__name__)\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    print(\"Import test failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f836f3b",
   "metadata": {},
   "source": [
    "# Use canonical implementations from the CA15 package\n",
    "from CA15.model_based_rl.algorithms import (\n",
    "    DynamicsModel, ModelEnsemble, ModelPredictiveController, DynaQAgent,\n",
    ")\n",
    "from CA15.hierarchical_rl.algorithms import (\n",
    "    Option, HierarchicalActorCritic, GoalConditionedAgent, FeudalNetwork,\n",
    ")\n",
    "from CA15.planning.algorithms import (\n",
    "    MCTSNode, MonteCarloTreeSearch, ModelBasedValueExpansion, LatentSpacePlanner, WorldModel,\n",
    ")\n",
    "from CA15.environments.grid_world import SimpleGridWorld\n",
    "from CA15.training_examples import ReplayBuffer, PrioritizedReplayBuffer, RunningStats\n",
    "\n",
    "print(\"Imported core CA15 algorithm and environment classes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6747ed",
   "metadata": {},
   "source": [
    "## Import Required Libraries\n",
    "\n",
    "We'll import essential libraries for implementing model-based and hierarchical RL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d455e03f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m     18\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     21\u001b[39m     DynamicsModel,\n\u001b[32m     22\u001b[39m     ModelEnsemble,\n\u001b[32m     23\u001b[39m     ModelPredictiveController,\n\u001b[32m     24\u001b[39m     DynaQAgent,\n\u001b[32m     25\u001b[39m \n\u001b[32m     26\u001b[39m     Option,\n\u001b[32m     27\u001b[39m     HierarchicalActorCritic,\n\u001b[32m     28\u001b[39m     GoalConditionedAgent,\n\u001b[32m     29\u001b[39m     FeudalNetwork,\n\u001b[32m     30\u001b[39m     HierarchicalRLEnvironment,\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m     MCTSNode,\n\u001b[32m     33\u001b[39m     MonteCarloTreeSearch,\n\u001b[32m     34\u001b[39m     ModelBasedValueExpansion,\n\u001b[32m     35\u001b[39m     LatentSpacePlanner,\n\u001b[32m     36\u001b[39m     WorldModel,\n\u001b[32m     37\u001b[39m \n\u001b[32m     38\u001b[39m     SimpleGridWorld,\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m     ExperimentRunner,\n\u001b[32m     41\u001b[39m     HierarchicalRLExperiment,\n\u001b[32m     42\u001b[39m     PlanningAlgorithmsExperiment,\n\u001b[32m     43\u001b[39m \n\u001b[32m     44\u001b[39m     ReplayBuffer,\n\u001b[32m     45\u001b[39m     PrioritizedReplayBuffer,\n\u001b[32m     46\u001b[39m     RunningStats,\n\u001b[32m     47\u001b[39m     Logger,\n\u001b[32m     48\u001b[39m     NeuralNetworkUtils,\n\u001b[32m     49\u001b[39m     VisualizationUtils,\n\u001b[32m     50\u001b[39m     EnvironmentUtils,\n\u001b[32m     51\u001b[39m     ExperimentUtils,\n\u001b[32m     52\u001b[39m     set_device,\n\u001b[32m     53\u001b[39m     get_device,\n\u001b[32m     54\u001b[39m     to_tensor\n\u001b[32m     55\u001b[39m )\n\u001b[32m     57\u001b[39m np.random.seed(\u001b[32m42\u001b[39m)\n\u001b[32m     58\u001b[39m torch.manual_seed(\u001b[32m42\u001b[39m)\n",
      "\u001b[31mImportError\u001b[39m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import copy\n",
    "import math\n",
    "import gym\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from . import (\n",
    "    DynamicsModel,\n",
    "    ModelEnsemble,\n",
    "    ModelPredictiveController,\n",
    "    DynaQAgent,\n",
    "    \n",
    "    Option,\n",
    "    HierarchicalActorCritic,\n",
    "    GoalConditionedAgent,\n",
    "    FeudalNetwork,\n",
    "    HierarchicalRLEnvironment,\n",
    "    \n",
    "    MCTSNode,\n",
    "    MonteCarloTreeSearch,\n",
    "    ModelBasedValueExpansion,\n",
    "    LatentSpacePlanner,\n",
    "    WorldModel,\n",
    "    \n",
    "    SimpleGridWorld,\n",
    "    \n",
    "    ExperimentRunner,\n",
    "    HierarchicalRLExperiment,\n",
    "    PlanningAlgorithmsExperiment,\n",
    "    \n",
    "    ReplayBuffer,\n",
    "    PrioritizedReplayBuffer,\n",
    "    RunningStats,\n",
    "    Logger,\n",
    "    NeuralNetworkUtils,\n",
    "    VisualizationUtils,\n",
    "    EnvironmentUtils,\n",
    "    ExperimentUtils,\n",
    "    set_device,\n",
    "    get_device,\n",
    "    to_tensor\n",
    ")\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "MODEL_BASED_CONFIG = {\n",
    "    'model_lr': 1e-3,\n",
    "    'planning_horizon': 10,\n",
    "    'model_ensemble_size': 5,\n",
    "    'imagination_rollouts': 100,\n",
    "    'model_training_freq': 10\n",
    "}\n",
    "\n",
    "HIERARCHICAL_CONFIG = {\n",
    "    'num_levels': 3,\n",
    "    'option_timeout': 20,\n",
    "    'subgoal_threshold': 0.1,\n",
    "    'meta_controller_lr': 3e-4,\n",
    "    'controller_lr': 1e-3\n",
    "}\n",
    "\n",
    "PLANNING_CONFIG = {\n",
    "    'mcts_simulations': 100,\n",
    "    'exploration_constant': 1.4,\n",
    "    'planning_depth': 5,\n",
    "    'beam_width': 10\n",
    "}\n",
    "\n",
    "print(\"🚀 Libraries imported successfully!\")\n",
    "print(\"📦 CA15 modular package loaded!\")\n",
    "print(\"📊 Configurations loaded for Model-Based and Hierarchical RL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de69bf0",
   "metadata": {},
   "source": [
    "# Section 1: Model-based Reinforcement Learning\n",
    "\n",
    "Model-Based RL learns an explicit model of the environment dynamics and uses it for planning and control.\n",
    "\n",
    "## 1.1 Theoretical Foundation\n",
    "\n",
    "### Environment Dynamics Model\n",
    "The goal is to learn a transition model $p(s*{t+1}, r*t | s*t, a*t)$ that predicts next states and rewards.\n",
    "\n",
    "**Key Components:**\n",
    "- **Deterministic Model**: $s*{t+1} = f(s*t, a_t) + \\epsilon$\n",
    "- **Stochastic Model**: $s*{t+1} \\sim p(\\cdot | s*t, a_t)$\n",
    "- **Ensemble Methods**: Multiple models to capture uncertainty\n",
    "\n",
    "### Model-predictive Control (mpc)\n",
    "Uses the learned model to plan actions by optimizing over a finite horizon:\n",
    "\n",
    "$$a^**t = \\arg\\max*{a*t, \\ldots, a*{t+H-1}} \\sum*{k=0}^{H-1} \\gamma^k r*{t+k}$$\n",
    "\n",
    "where states are predicted using the learned model.\n",
    "\n",
    "### Dyna-q Algorithm\n",
    "Combines model-free and model-based learning:\n",
    "1. **Direct RL**: Update Q-function from real experience\n",
    "2. **Planning**: Use model to generate simulated experience\n",
    "3. **Model Learning**: Update dynamics model from real data\n",
    "\n",
    "### Advantages and Challenges\n",
    "**Advantages:**\n",
    "- Sample efficiency through planning\n",
    "- Can handle sparse rewards\n",
    "- Enables what-if analysis\n",
    "\n",
    "**Challenges:**\n",
    "- Model bias and compounding errors\n",
    "- Computational complexity\n",
    "- Partial observability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001221c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Model-Based RL components implemented successfully!\n",
      "📝 Key components:\n",
      "  • DynamicsModel: Neural network for environment dynamics\n",
      "  • ModelEnsemble: Multiple models for uncertainty quantification\n",
      "  • ModelPredictiveController: MPC for action planning\n",
      "  • DynaQAgent: Dyna-Q algorithm combining model-free and model-based learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"🧠 Model-Based RL components loaded from CA15 package!\")\n",
    "print(\"📝 Key components:\")\n",
    "print(\"  • DynamicsModel: Neural network for environment dynamics\")\n",
    "print(\"  • ModelEnsemble: Multiple models for uncertainty quantification\")\n",
    "print(\"  • ModelPredictiveController: MPC for action planning\")\n",
    "print(\"  • DynaQAgent: Dyna-Q algorithm combining model-free and model-based learning\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e610b2",
   "metadata": {},
   "source": [
    "# Section 2: Hierarchical Reinforcement Learning\n",
    "\n",
    "Hierarchical RL decomposes complex tasks into simpler subtasks through temporal and spatial abstraction.\n",
    "\n",
    "## 2.1 Theoretical Foundation\n",
    "\n",
    "### Options Framework\n",
    "An **option** is a closed-loop policy for taking actions over a period of time. Formally, an option consists of:\n",
    "- **Initiation set** $I$: States where the option can be initiated\n",
    "- **Policy** $\\pi$: Action selection within the option\n",
    "- **Termination condition** $\\beta$: Probability of terminating the option\n",
    "\n",
    "### Semi-markov Decision Process (smdp)\n",
    "Options extend MDPs to SMDPs where:\n",
    "- Actions can take variable amounts of time\n",
    "- Temporal abstraction enables hierarchical planning\n",
    "- Q-learning over options: $Q(s,o) = r + \\gamma^k Q(s', o')$\n",
    "\n",
    "### Goal-conditioned Rl\n",
    "Learn policies conditioned on goals: $\\pi(a|s,g)$\n",
    "- **Hindsight Experience Replay (HER)**: Learn from failed attempts\n",
    "- **Universal Value Function**: $V(s,g)$ for any goal $g$\n",
    "- **Intrinsic Motivation**: Generate own goals for exploration\n",
    "\n",
    "### Hierarchical Actor-critic (hac)\n",
    "Multi-level hierarchy where:\n",
    "- **High-level policy**: Selects subgoals\n",
    "- **Low-level policy**: Executes actions to reach subgoals\n",
    "- **Temporal abstraction**: Different time scales at each level\n",
    "\n",
    "### Feudal Networks\n",
    "Hierarchical architecture with:\n",
    "- **Manager**: Sets goals for workers\n",
    "- **Worker**: Executes actions to achieve goals\n",
    "- **Feudal objective**: Manager maximizes reward, Worker maximizes goal achievement\n",
    "\n",
    "## 2.2 Key Advantages\n",
    "\n",
    "**Sample Efficiency:**\n",
    "- Reuse learned skills across tasks\n",
    "- Faster learning through temporal abstraction\n",
    "\n",
    "**Interpretability:**\n",
    "- Hierarchical structure mirrors human thinking\n",
    "- Decomposable and explainable decisions\n",
    "\n",
    "**Transfer Learning:**\n",
    "- Skills transfer across related environments\n",
    "- Compositional generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fb4f2f",
   "metadata": {},
   "source": [
    "# Section 3: Advanced Planning and Control\n",
    "\n",
    "Advanced planning algorithms combine learned models with sophisticated search techniques.\n",
    "\n",
    "## 3.1 Monte Carlo Tree Search (mcts)\n",
    "\n",
    "MCTS is a best-first search algorithm that uses Monte Carlo simulations for decision making.\n",
    "\n",
    "### Mcts Algorithm Steps:\n",
    "1. **Selection**: Navigate down the tree using UCB1 formula\n",
    "2. **Expansion**: Add new child nodes to the tree\n",
    "3. **Simulation**: Run random rollouts from leaf nodes\n",
    "4. **Backpropagation**: Update node values with simulation results\n",
    "\n",
    "### Ucb1 Selection Formula:\n",
    "$$UCB1(s,a) = Q(s,a) + c \\sqrt{\\frac{\\ln N(s)}{N(s,a)}}$$\n",
    "\n",
    "Where:\n",
    "- $Q(s,a)$: Average reward for action $a$ in state $s$\n",
    "- $N(s)$: Visit count for state $s$\n",
    "- $N(s,a)$: Visit count for action $a$ in state $s$\n",
    "- $c$: Exploration constant\n",
    "\n",
    "### Alphazero Integration\n",
    "Combines MCTS with neural networks:\n",
    "- **Policy Network**: $p(a|s)$ guides selection\n",
    "- **Value Network**: $v(s)$ estimates leaf values\n",
    "- **Self-Play**: Generates training data through MCTS games\n",
    "\n",
    "## 3.2 Model-based Value Expansion (mve)\n",
    "\n",
    "Uses learned models to expand value function estimates:\n",
    "\n",
    "$$V*{MVE}(s) = \\max*a \\left[ r(s,a) + \\gamma \\sum_{s'} p(s'|s,a) V(s') \\right]$$\n",
    "\n",
    "### Trajectory Optimization\n",
    "- **Cross-Entropy Method (CEM)**: Iterative sampling and fitting\n",
    "- **Random Shooting**: Sample multiple action sequences\n",
    "- **Model Predictive Path Integral (MPPI)**: Information-theoretic approach\n",
    "\n",
    "## 3.3 Latent Space Planning\n",
    "\n",
    "Planning in learned latent representations:\n",
    "\n",
    "### World Models Architecture:\n",
    "1. **Vision Model (V)**: Encodes observations to latent states\n",
    "2. **Memory Model (M)**: Predicts next latent states  \n",
    "3. **Controller Model (C)**: Maps latent states to actions\n",
    "\n",
    "### Planet Algorithm:\n",
    "- **Recurrent State Space Model (RSSM)**:\n",
    "- Deterministic path: $h*t = f(h*{t-1}, a_{t-1})$\n",
    "- Stochastic path: $s*t \\sim p(s*t | h_t)$\n",
    "- **Planning**: Cross-entropy method in latent space\n",
    "- **Learning**: Variational inference for world model\n",
    "\n",
    "## 3.4 Challenges and Solutions\n",
    "\n",
    "### Model Bias\n",
    "- **Problem**: Learned models have prediction errors\n",
    "- **Solutions**: \n",
    "- Model ensembles for uncertainty quantification\n",
    "- Conservative planning with uncertainty penalties\n",
    "- Robust optimization techniques\n",
    "\n",
    "### Computational Complexity\n",
    "- **Problem**: Planning is computationally expensive\n",
    "- **Solutions**:\n",
    "- Hierarchical planning with multiple time scales\n",
    "- Approximate planning with limited horizons\n",
    "- Parallel Monte Carlo simulations\n",
    "\n",
    "### Exploration Vs Exploitation\n",
    "- **Problem**: Balancing exploration and exploitation in planning\n",
    "- **Solutions**:\n",
    "- UCB-based selection in MCTS\n",
    "- Optimistic initialization\n",
    "- Information-gain based rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e4c08",
   "metadata": {},
   "source": [
    "# Section 4: Practical Demonstrations and Experiments\n",
    "\n",
    "This section provides hands-on experiments to demonstrate the concepts and implementations.\n",
    "\n",
    "## 4.1 Experiment Setup\n",
    "\n",
    "We'll create practical experiments to showcase:\n",
    "\n",
    "1. **Model-Based vs Model-Free Comparison**\n",
    "- Sample efficiency analysis\n",
    "- Performance on different environments\n",
    "- Computational overhead comparison\n",
    "\n",
    "2. **Hierarchical RL Benefits**\n",
    "- Multi-goal navigation tasks\n",
    "- Skill reuse and transfer\n",
    "- Temporal abstraction advantages\n",
    "\n",
    "3. **Planning Algorithm Comparison**\n",
    "- MCTS vs random rollouts\n",
    "- Value expansion effectiveness\n",
    "- Latent space planning benefits\n",
    "\n",
    "4. **Integration Study**\n",
    "- Combining all methods\n",
    "- Real-world application scenarios\n",
    "- Performance analysis and trade-offs\n",
    "\n",
    "## 4.2 Metrics and Evaluation\n",
    "\n",
    "### Performance Metrics:\n",
    "- **Sample Efficiency**: Steps to reach performance threshold\n",
    "- **Asymptotic Performance**: Final average reward\n",
    "- **Computation Time**: Planning and learning overhead\n",
    "- **Memory Usage**: Model storage requirements\n",
    "- **Transfer Performance**: Success on related tasks\n",
    "\n",
    "### Statistical Analysis:\n",
    "- Multiple random seeds for reliability\n",
    "- Confidence intervals and significance tests\n",
    "- Learning curve analysis\n",
    "- Ablation studies for each component\n",
    "\n",
    "## 4.3 Environments for Testing\n",
    "\n",
    "### Simple Grid World:\n",
    "- **Purpose**: Basic concept demonstration\n",
    "- **Features**: Discrete states, clear visualization\n",
    "- **Challenges**: Navigation, goal reaching\n",
    "\n",
    "### Continuous Control:\n",
    "- **Purpose**: Real-world applicability\n",
    "- **Features**: Continuous state-action spaces\n",
    "- **Challenges**: Precise control, dynamic systems\n",
    "\n",
    "### Hierarchical Tasks:\n",
    "- **Purpose**: Multi-level decision making\n",
    "- **Features**: Natural task decomposition\n",
    "- **Challenges**: Long-horizon planning, skill coordination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34dcf5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CA15'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCA15\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrunner\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExperimentRunner\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCA15\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhierarchical\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HierarchicalRLExperiment\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mCA15\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperiments\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplanning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PlanningAlgorithmsExperiment\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'CA15'"
     ]
    }
   ],
   "source": [
    "# Import training and evaluation utilities from the CA15 package\n",
    "from CA15.training_examples import (\n",
    "    train_model_based_rl_agent,\n",
    "    train_hierarchical_rl_agent,\n",
    "    train_goal_conditioned_agent,\n",
    "    train_feudal_network_agent,\n",
    "    train_mcts_agent,\n",
    "    train_latent_space_planner,\n",
    ")\n",
    "\n",
    "print(\"Imported CA15 training functions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8cc3d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'HierarchicalRLExperiment' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m hierarchical_exp = \u001b[43mHierarchicalRLExperiment\u001b[49m()\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🎯 Hierarchical RL Experiment Setup Complete!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m🏃‍♂️ Key features being tested:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'HierarchicalRLExperiment' is not defined"
     ]
    }
   ],
   "source": [
    "# Import utilities exposed by the CA15 package\n",
    "from CA15 import (\n",
    "    VisualizationUtils,\n",
    "    Logger,\n",
    "    to_tensor,\n",
    "    set_device,\n",
    "    get_device,\n",
    ")\n",
    "\n",
    "print(\"Imported CA15 utilities and helpers\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cfb988",
   "metadata": {},
   "source": [
    "# Code Review and Improvements\n",
    "\n",
    "## Advanced Model-Based and Hierarchical RL Implementation Analysis\n",
    "\n",
    "### Strengths of Current Implementation\n",
    "\n",
    "1. **Comprehensive Algorithm Coverage**: Implementation of all major advanced RL paradigms including model-based learning, hierarchical decomposition, and sophisticated planning algorithms\n",
    "2. **Modular and Scalable Architecture**: Clean separation between different algorithm families with reusable components and extensible design patterns\n",
    "3. **Advanced Neural Architectures**: State-of-the-art implementations including world models, feudal networks, and latent space planning systems\n",
    "4. **Theoretical Rigor**: Strong foundation in both model-based theory (dynamics learning, uncertainty quantification) and hierarchical theory (temporal abstraction, multi-timescale learning)\n",
    "5. **Practical Evaluation Frameworks**: Comprehensive experimental setups with proper statistical analysis, visualization, and comparative studies\n",
    "\n",
    "### Areas for Improvement\n",
    "\n",
    "#### 1. Model-Based RL Enhancements\n",
    "- **Current Limitation**: Basic dynamics model learning with limited uncertainty handling\n",
    "- **Improvement**: Advanced model-based techniques:\n",
    "  - **Probabilistic Models**: Implement Bayesian neural networks for better uncertainty quantification\n",
    "  - **Model-Based Meta-Learning**: Learn-to-learn dynamics models across tasks\n",
    "  - **Causal Discovery**: Learn causal relationships in environment dynamics\n",
    "  - **Multi-Step Prediction**: Long-horizon prediction with temporal hierarchies\n",
    "  - **Model Regularization**: Advanced regularization techniques for better generalization\n",
    "\n",
    "#### 2. Hierarchical RL Extensions\n",
    "- **Current Limitation**: Fixed hierarchy with limited skill discovery\n",
    "- **Improvement**: More sophisticated hierarchical systems:\n",
    "  - **Automatic Skill Discovery**: Unsupervised learning of reusable skills\n",
    "  - **Dynamic Hierarchies**: Adaptive hierarchy depth based on task complexity\n",
    "  - **Cross-Level Communication**: Better information flow between hierarchy levels\n",
    "  - **Meta-Hierarchical Learning**: Learning to construct hierarchies\n",
    "  - **Compositional Skills**: Combining primitive skills into complex behaviors\n",
    "\n",
    "#### 3. Planning Algorithm Advancements\n",
    "- **Current Limitation**: Basic MCTS and model-based value expansion\n",
    "- **Improvement**: Cutting-edge planning techniques:\n",
    "  - **AlphaZero Integration**: Neural network guided MCTS with self-play\n",
    "  - **MuZero Architecture**: Unified model-based planning framework\n",
    "  - **Efficient Planning**: Approximate planning methods for real-time control\n",
    "  - **Hierarchical Planning**: Multi-level planning with temporal abstraction\n",
    "  - **Risk-Aware Planning**: Planning under uncertainty with risk measures\n",
    "\n",
    "### Future Research Directions\n",
    "\n",
    "1. **Foundation Models for RL**: Large-scale pre-training of universal world models and hierarchical policies\n",
    "2. **Causal Hierarchical RL**: Learning causal hierarchies for better generalization and interpretability\n",
    "3. **Neuro-Symbolic Hierarchical Systems**: Combining neural networks with symbolic planning\n",
    "4. **Multi-Agent Hierarchical RL**: Hierarchical coordination in multi-agent systems\n",
    "5. **Quantum-Enhanced Planning**: Leveraging quantum computing for exponential planning speedup\n",
    "6. **Human-AI Hierarchical Collaboration**: Hierarchical systems that collaborate with humans\n",
    "7. **Energy-Efficient Hierarchical RL**: Optimizing for computational and energy constraints\n",
    "8. **Robust Hierarchical Systems**: Hierarchies that maintain performance under distribution shifts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78af5bd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
