"""
Advanced Policy Gradient Methods - Training Examples
====================================================

This module provides comprehensive implementations and training examples for
Advanced Policy Gradient Methods (CA9).

Key Components:
- REINFORCE algorithm with variance reduction
- Actor-Critic methods (A2C, A3C)
- Proximal Policy Optimization (PPO)
- Trust Region Policy Optimization (TRPO)
- Continuous control with policy gradients
- Advanced analysis and visualization tools

Author: DRL Course Team
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional, Any, Union
import gymnasium as gym
from collections import deque
import random
import pandas as pd
from tqdm import tqdm
import warnings
import os

warnings.filterwarnings("ignore")


# Set random seeds for reproducibility
def set_seed(seed: int = 42):
    """Set random seeds for reproducibility"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)


set_seed(42)

# =============================================================================
# POLICY NETWORKS
# =============================================================================


class PolicyNetwork(nn.Module):
    """Base policy network for discrete action spaces"""

    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim

        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
        )

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """Forward pass"""
        return self.network(state)

    def get_action(
        self, state: torch.Tensor, deterministic: bool = False
    ) -> Tuple[int, torch.Tensor]:
        """Sample action from policy"""
        logits = self.forward(state)

        if deterministic:
            action = torch.argmax(logits, dim=-1)
        else:
            probs = F.softmax(logits, dim=-1)
            action = torch.multinomial(probs, 1).squeeze(-1)

        log_prob = (
            F.log_softmax(logits, dim=-1).gather(1, action.unsqueeze(-1)).squeeze(-1)
        )

        return action.item(), log_prob


class ValueNetwork(nn.Module):
    """Value function network"""

    def __init__(self, state_dim: int, hidden_dim: int = 128):
        super().__init__()
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1),
        )

    def forward(self, state: torch.Tensor) -> torch.Tensor:
        """Forward pass"""
        return self.network(state)


class ContinuousPolicyNetwork(nn.Module):
    """Policy network for continuous action spaces (Gaussian)"""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_dim: int = 128,
        action_bound: float = 1.0,
    ):
        super().__init__()
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.action_bound = action_bound

        # Mean network
        self.mean_network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Tanh(),  # Bound mean between -1 and 1
        )

        # Standard deviation (learnable parameter)
        self.log_std = nn.Parameter(torch.zeros(action_dim))

    def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Forward pass - return mean and std"""
        mean = self.mean_network(state) * self.action_bound
        std = torch.exp(self.log_std)
        return mean, std

    def get_action(
        self, state: torch.Tensor, deterministic: bool = False
    ) -> Tuple[np.ndarray, torch.Tensor]:
        """Sample action from Gaussian policy"""
        mean, std = self.forward(state)

        if deterministic:
            action = mean
        else:
            normal = torch.distributions.Normal(mean, std)
            action = normal.rsample()

        # Compute log probability
        log_prob = self.compute_log_prob(action, mean, std)

        return action.detach().numpy(), log_prob

    def compute_log_prob(
        self, action: torch.Tensor, mean: torch.Tensor, std: torch.Tensor
    ) -> torch.Tensor:
        """Compute log probability of action under policy"""
        var = std.pow(2)
        log_std = torch.log(std)

        # Log probability for multivariate Gaussian
        log_prob = -0.5 * (
            (action - mean).pow(2) / var + 2 * log_std + np.log(2 * np.pi)
        )
        return log_prob.sum(dim=-1)


# =============================================================================
# REINFORCE ALGORITHM
# =============================================================================


class REINFORCEAgent:
    """REINFORCE algorithm with optional baseline"""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_dim: int = 128,
        use_baseline: bool = False,
        gamma: float = 0.99,
        lr: float = 1e-3,
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.use_baseline = use_baseline
        self.gamma = gamma

        self.policy = PolicyNetwork(state_dim, action_dim, hidden_dim)
        self.optimizer = optim.Adam(self.policy.parameters(), lr=lr)

        if use_baseline:
            self.value_net = ValueNetwork(state_dim, hidden_dim)
            self.value_optimizer = optim.Adam(self.value_net.parameters(), lr=lr)

        self.episode_buffer = []

    def select_action(self, state: torch.Tensor) -> Tuple[int, torch.Tensor]:
        """Select action using current policy"""
        return self.policy.get_action(state)

    def store_transition(
        self, state: torch.Tensor, action: int, reward: float, log_prob: torch.Tensor
    ):
        """Store transition in episode buffer"""
        self.episode_buffer.append(
            {"state": state, "action": action, "reward": reward, "log_prob": log_prob}
        )

    def update(self) -> Dict[str, float]:
        """Update policy using REINFORCE"""
        if not self.episode_buffer:
            return {}

        # Compute returns
        rewards = [t["reward"] for t in self.episode_buffer]
        returns = []
        G = 0
        for r in reversed(rewards):
            G = r + self.gamma * G
            returns.insert(0, G)
        returns = torch.tensor(returns, dtype=torch.float32)

        # Normalize returns
        returns = (returns - returns.mean()) / (returns.std() + 1e-8)

        # Compute advantages if using baseline
        if self.use_baseline:
            states = torch.stack([t["state"] for t in self.episode_buffer])
            values = self.value_net(states).squeeze()
            advantages = returns - values.detach()

            # Update value network
            value_loss = F.mse_loss(values, returns)
            self.value_optimizer.zero_grad()
            value_loss.backward()
            self.value_optimizer.step()
        else:
            advantages = returns

        # Compute policy loss
        log_probs = torch.stack([t["log_prob"] for t in self.episode_buffer])
        policy_loss = -(log_probs * advantages).mean()

        # Update policy
        self.optimizer.zero_grad()
        policy_loss.backward()
        self.optimizer.step()

        # Clear buffer
        self.episode_buffer = []

        return {
            "policy_loss": policy_loss.item(),
            "returns_mean": returns.mean().item(),
            "returns_std": returns.std().item(),
        }


# =============================================================================
# ACTOR-CRITIC METHODS
# =============================================================================


class ActorCriticAgent:
    """Actor-Critic agent with advantage estimation"""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_dim: int = 128,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        lr: float = 3e-4,
        value_coeff: float = 0.5,
        entropy_coeff: float = 0.01,
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.value_coeff = value_coeff
        self.entropy_coeff = entropy_coeff

        self.actor = PolicyNetwork(state_dim, action_dim, hidden_dim)
        self.critic = ValueNetwork(state_dim, hidden_dim)

        self.optimizer = optim.Adam(
            list(self.actor.parameters()) + list(self.critic.parameters()), lr=lr
        )

    def select_action(
        self, state: torch.Tensor
    ) -> Tuple[int, torch.Tensor, torch.Tensor]:
        """Select action and return value"""
        action, log_prob = self.actor.get_action(state)
        value = self.critic(state).item()
        return action, log_prob, value

    def compute_gae(
        self, rewards: List[float], values: List[float], dones: List[bool]
    ) -> torch.Tensor:
        """Compute Generalized Advantage Estimation (GAE)"""
        advantages = []
        gae = 0

        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0  # Terminal state
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)

        return torch.tensor(advantages, dtype=torch.float32)

    def update(
        self,
        states: torch.Tensor,
        actions: torch.Tensor,
        log_probs: torch.Tensor,
        returns: torch.Tensor,
        advantages: torch.Tensor,
    ) -> Dict[str, float]:
        """Update actor and critic"""

        # Get current policy outputs
        new_logits = self.actor(states)
        new_log_probs = (
            F.log_softmax(new_logits, dim=-1)
            .gather(1, actions.unsqueeze(-1))
            .squeeze(-1)
        )
        entropy = (
            -(F.softmax(new_logits, dim=-1) * F.log_softmax(new_logits, dim=-1))
            .sum(dim=-1)
            .mean()
        )

        # Policy loss
        ratio = torch.exp(new_log_probs - log_probs)
        policy_loss = -(
            torch.min(ratio * advantages, torch.clamp(ratio, 0.8, 1.2) * advantages)
        ).mean()

        # Value loss
        values = self.critic(states).squeeze()
        value_loss = F.mse_loss(values, returns)

        # Total loss
        total_loss = (
            policy_loss + self.value_coeff * value_loss - self.entropy_coeff * entropy
        )

        # Update
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()

        return {
            "policy_loss": policy_loss.item(),
            "value_loss": value_loss.item(),
            "entropy": entropy.item(),
            "total_loss": total_loss.item(),
        }


# =============================================================================
# PROXIMAL POLICY OPTIMIZATION (PPO)
# =============================================================================


class PPOAgent:
    """Proximal Policy Optimization (PPO) agent"""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_dim: int = 128,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        clip_ratio: float = 0.2,
        lr: float = 3e-4,
        value_coeff: float = 0.5,
        entropy_coeff: float = 0.01,
        ppo_epochs: int = 10,
        batch_size: int = 64,
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.value_coeff = value_coeff
        self.entropy_coeff = entropy_coeff
        self.ppo_epochs = ppo_epochs
        self.batch_size = batch_size

        self.actor = PolicyNetwork(state_dim, action_dim, hidden_dim)
        self.critic = ValueNetwork(state_dim, hidden_dim)
        self.actor_old = PolicyNetwork(state_dim, action_dim, hidden_dim)
        self.critic_old = ValueNetwork(state_dim, hidden_dim)

        # Copy parameters
        self._update_old_networks()

        self.optimizer = optim.Adam(
            list(self.actor.parameters()) + list(self.critic.parameters()), lr=lr
        )

        self.buffer = deque(maxlen=10000)

    def _update_old_networks(self):
        """Update old networks with current parameters"""
        self.actor_old.load_state_dict(self.actor.state_dict())
        self.critic_old.load_state_dict(self.critic.state_dict())

    def select_action(
        self, state: torch.Tensor
    ) -> Tuple[int, torch.Tensor, torch.Tensor]:
        """Select action using current policy"""
        with torch.no_grad():
            action, log_prob = self.actor_old.get_action(state)
            value = self.critic_old(state).item()
        return action, log_prob, value

    def store_transition(
        self,
        state: torch.Tensor,
        action: int,
        reward: float,
        log_prob: torch.Tensor,
        value: float,
        done: bool,
    ):
        """Store transition in buffer"""
        self.buffer.append(
            {
                "state": state,
                "action": action,
                "reward": reward,
                "log_prob": log_prob,
                "value": value,
                "done": done,
            }
        )

    def compute_gae(
        self, rewards: List[float], values: List[float], dones: List[bool]
    ) -> torch.Tensor:
        """Compute Generalized Advantage Estimation"""
        advantages = []
        gae = 0

        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)

        return torch.tensor(advantages, dtype=torch.float32)

    def update(self) -> Dict[str, float]:
        """Update PPO agent"""
        if len(self.buffer) < self.batch_size:
            return {}

        # Convert buffer to tensors
        batch = list(self.buffer)
        states = torch.stack([t["state"] for t in batch])
        actions = torch.tensor([t["action"] for t in batch], dtype=torch.long)
        old_log_probs = torch.stack([t["log_prob"] for t in batch])
        rewards = [t["reward"] for t in batch]
        values = [t["value"] for t in batch]
        dones = [t["done"] for t in batch]

        # Compute returns and advantages
        advantages = self.compute_gae(rewards, values, dones)
        returns = advantages + torch.tensor(values, dtype=torch.float32)

        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # PPO update
        total_policy_loss = 0
        total_value_loss = 0
        total_entropy = 0

        for _ in range(self.ppo_epochs):
            # Sample mini-batch
            indices = torch.randperm(len(batch))[: self.batch_size]

            batch_states = states[indices]
            batch_actions = actions[indices]
            batch_old_log_probs = old_log_probs[indices]
            batch_advantages = advantages[indices]
            batch_returns = returns[indices]

            # Get current policy outputs
            logits = self.actor(batch_states)
            values_pred = self.critic(batch_states).squeeze()

            new_log_probs = (
                F.log_softmax(logits, dim=-1)
                .gather(1, batch_actions.unsqueeze(-1))
                .squeeze(-1)
            )
            entropy = (
                -(F.softmax(logits, dim=-1) * F.log_softmax(logits, dim=-1))
                .sum(dim=-1)
                .mean()
            )

            # PPO clipped objective
            ratio = torch.exp(new_log_probs - batch_old_log_probs)
            clipped_ratio = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)
            policy_loss = -(
                torch.min(ratio * batch_advantages, clipped_ratio * batch_advantages)
            ).mean()

            # Value loss
            value_loss = F.mse_loss(values_pred, batch_returns)

            # Total loss
            loss = (
                policy_loss
                + self.value_coeff * value_loss
                - self.entropy_coeff * entropy
            )

            # Update
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()

            total_policy_loss += policy_loss.item()
            total_value_loss += value_loss.item()
            total_entropy += entropy.item()

        # Update old networks
        self._update_old_networks()

        # Clear buffer
        self.buffer.clear()

        return {
            "policy_loss": total_policy_loss / self.ppo_epochs,
            "value_loss": total_value_loss / self.ppo_epochs,
            "entropy": total_entropy / self.ppo_epochs,
        }


# =============================================================================
# CONTINUOUS CONTROL AGENTS
# =============================================================================


class ContinuousPPOAgent:
    """PPO agent for continuous control"""

    def __init__(
        self,
        state_dim: int,
        action_dim: int,
        hidden_dim: int = 128,
        action_bound: float = 1.0,
        gamma: float = 0.99,
        gae_lambda: float = 0.95,
        clip_ratio: float = 0.2,
        lr: float = 3e-4,
        value_coeff: float = 0.5,
        entropy_coeff: float = 0.01,
    ):
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.action_bound = action_bound
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.clip_ratio = clip_ratio
        self.value_coeff = value_coeff
        self.entropy_coeff = entropy_coeff

        self.actor = ContinuousPolicyNetwork(
            state_dim, action_dim, hidden_dim, action_bound
        )
        self.critic = ValueNetwork(state_dim, hidden_dim)

        self.optimizer = optim.Adam(
            list(self.actor.parameters()) + list(self.critic.parameters()), lr=lr
        )

        self.buffer = deque(maxlen=10000)

    def select_action(
        self, state: torch.Tensor
    ) -> Tuple[np.ndarray, torch.Tensor, torch.Tensor]:
        """Select action for continuous control"""
        with torch.no_grad():
            action, log_prob = self.actor.get_action(state)
            value = self.critic(state).item()
        return action, log_prob, value

    def store_transition(
        self,
        state: torch.Tensor,
        action: np.ndarray,
        reward: float,
        log_prob: torch.Tensor,
        value: float,
        done: bool,
    ):
        """Store transition"""
        self.buffer.append(
            {
                "state": state,
                "action": action,
                "reward": reward,
                "log_prob": log_prob,
                "value": value,
                "done": done,
            }
        )

    def compute_gae(
        self, rewards: List[float], values: List[float], dones: List[bool]
    ) -> torch.Tensor:
        """Compute GAE for continuous actions"""
        advantages = []
        gae = 0

        for t in reversed(range(len(rewards))):
            if t == len(rewards) - 1:
                next_value = 0
            else:
                next_value = values[t + 1]

            delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]
            gae = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * gae
            advantages.insert(0, gae)

        return torch.tensor(advantages, dtype=torch.float32)

    def update(self) -> Dict[str, float]:
        """Update continuous PPO agent"""
        if len(self.buffer) < 100:  # Minimum batch size
            return {}

        # Convert buffer to batch
        batch = list(self.buffer)
        states = torch.stack([t["state"] for t in batch])
        actions = torch.tensor(
            np.array([t["action"] for t in batch]), dtype=torch.float32
        )
        old_log_probs = torch.stack([t["log_prob"] for t in batch])
        rewards = [t["reward"] for t in batch]
        values = [t["value"] for t in batch]
        dones = [t["done"] for t in batch]

        # Compute advantages and returns
        advantages = self.compute_gae(rewards, values, dones)
        returns = advantages + torch.tensor(values, dtype=torch.float32)

        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)

        # PPO update
        mean, std = self.actor(states)
        new_log_probs = self.actor.compute_log_prob(actions, mean, std)
        values_pred = self.critic(states).squeeze()

        # PPO clipped objective
        ratio = torch.exp(new_log_probs - old_log_probs)
        clipped_ratio = torch.clamp(ratio, 1 - self.clip_ratio, 1 + self.clip_ratio)
        policy_loss = -(
            torch.min(ratio * advantages, clipped_ratio * advantages)
        ).mean()

        # Value loss
        value_loss = F.mse_loss(values_pred, returns)

        # Entropy bonus
        entropy = -0.5 * (2 * torch.log(std) + 1 + 2 * np.pi).sum(dim=-1).mean()

        # Total loss
        total_loss = (
            policy_loss + self.value_coeff * value_loss - self.entropy_coeff * entropy
        )

        # Update
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()

        # Clear buffer
        self.buffer.clear()

        return {
            "policy_loss": policy_loss.item(),
            "value_loss": value_loss.item(),
            "entropy": entropy.item(),
            "total_loss": total_loss.item(),
        }


# =============================================================================
# TRAINING UTILITIES
# =============================================================================


def train_reinforce_agent(
    env_name: str = "CartPole-v1",
    use_baseline: bool = False,
    num_episodes: int = 500,
    max_steps: int = 500,
    seed: int = 42,
) -> Dict[str, Any]:
    """Train REINFORCE agent"""

    set_seed(seed)

    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    agent = REINFORCEAgent(state_dim, action_dim, use_baseline=use_baseline)

    episode_rewards = []
    losses = {"policy": [], "returns_mean": [], "returns_std": []}

    print(f"Training REINFORCE Agent (Baseline: {use_baseline}) on {env_name}")
    print("=" * 60)

    for episode in tqdm(range(num_episodes)):
        state, _ = env.reset()
        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

        episode_reward = 0
        episode_log_probs = []

        for step in range(max_steps):
            action, log_prob = agent.select_action(state)
            episode_log_probs.append(log_prob)

            next_state, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated

            agent.store_transition(state, action, reward, log_prob)

            state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)
            episode_reward += reward

            if done:
                break

        # Update agent
        loss_dict = agent.update()
        for key, value in loss_dict.items():
            if key in losses:
                losses[key].append(value)

        episode_rewards.append(episode_reward)

    env.close()

    results = {
        "episode_rewards": episode_rewards,
        "losses": losses,
        "agent": agent,
        "config": {
            "env_name": env_name,
            "use_baseline": use_baseline,
            "num_episodes": num_episodes,
        },
    }

    return results


def train_ppo_agent(
    env_name: str = "CartPole-v1",
    num_episodes: int = 500,
    max_steps: int = 500,
    update_freq: int = 2048,
    seed: int = 42,
) -> Dict[str, Any]:
    """Train PPO agent"""

    set_seed(seed)

    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.n

    agent = PPOAgent(state_dim, action_dim)

    episode_rewards = []
    losses = {"policy": [], "value": [], "entropy": []}

    print(f"Training PPO Agent on {env_name}")
    print("=" * 40)

    episode_reward = 0
    episode_count = 0

    state, _ = env.reset()
    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

    for step in tqdm(range(num_episodes * max_steps)):
        action, log_prob, value = agent.select_action(state)

        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        agent.store_transition(state, action, reward, log_prob, value, done)

        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)
        episode_reward += reward

        if done or (step + 1) % max_steps == 0:
            if done:
                episode_rewards.append(episode_reward)
                episode_count += 1
                episode_reward = 0

            state, _ = env.reset()
            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

        # Update agent
        if (step + 1) % update_freq == 0:
            loss_dict = agent.update()
            for key, value in loss_dict.items():
                if key in losses and value is not None:
                    losses[key].append(value)

    env.close()

    results = {
        "episode_rewards": episode_rewards[:episode_count],
        "losses": losses,
        "agent": agent,
        "config": {
            "env_name": env_name,
            "num_episodes": num_episodes,
            "update_freq": update_freq,
        },
    }

    return results


def train_continuous_ppo_agent(
    env_name: str = "Pendulum-v1",
    num_episodes: int = 500,
    max_steps: int = 200,
    update_freq: int = 2048,
    seed: int = 42,
) -> Dict[str, Any]:
    """Train continuous PPO agent"""

    set_seed(seed)

    env = gym.make(env_name)
    state_dim = env.observation_space.shape[0]
    action_dim = env.action_space.shape[0]
    action_bound = float(env.action_space.high[0])

    agent = ContinuousPPOAgent(state_dim, action_dim, action_bound=action_bound)

    episode_rewards = []
    losses = {"policy": [], "value": [], "entropy": []}

    print(f"Training Continuous PPO Agent on {env_name}")
    print("=" * 50)

    episode_reward = 0
    episode_count = 0

    state, _ = env.reset()
    state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

    for step in tqdm(range(num_episodes * max_steps)):
        action, log_prob, value = agent.select_action(state)

        next_state, reward, terminated, truncated, _ = env.step(action)
        done = terminated or truncated

        agent.store_transition(state, action, reward, log_prob, value, done)

        state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)
        episode_reward += reward

        if done or (step + 1) % max_steps == 0:
            if done:
                episode_rewards.append(episode_reward)
                episode_count += 1
                episode_reward = 0

            state, _ = env.reset()
            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)

        # Update agent
        if (step + 1) % update_freq == 0:
            loss_dict = agent.update()
            for key, value in loss_dict.items():
                if key in losses and value is not None:
                    losses[key].append(value)

    env.close()

    results = {
        "episode_rewards": episode_rewards[:episode_count],
        "losses": losses,
        "agent": agent,
        "config": {
            "env_name": env_name,
            "num_episodes": num_episodes,
            "action_bound": action_bound,
        },
    }

    return results


def compare_policy_gradient_methods(
    env_name: str = "CartPole-v1", num_runs: int = 3, num_episodes: int = 200
) -> Dict[str, Any]:
    """Compare different policy gradient methods"""

    methods = ["REINFORCE", "REINFORCE+Baseline", "PPO"]
    results = {}

    for method in methods:
        print(f"Testing {method}...")

        run_rewards = []

        for run in range(num_runs):
            set_seed(42 + run)

            if method == "REINFORCE":
                result = train_reinforce_agent(
                    env_name,
                    use_baseline=False,
                    num_episodes=num_episodes,
                    seed=42 + run,
                )
            elif method == "REINFORCE+Baseline":
                result = train_reinforce_agent(
                    env_name,
                    use_baseline=True,
                    num_episodes=num_episodes,
                    seed=42 + run,
                )
            else:  # PPO
                result = train_ppo_agent(
                    env_name, num_episodes=num_episodes, seed=42 + run
                )

            run_rewards.append(result["episode_rewards"])

        # Average across runs
        avg_rewards = np.mean(run_rewards, axis=0)
        std_rewards = np.std(run_rewards, axis=0)

        results[method] = {
            "mean_rewards": avg_rewards,
            "std_rewards": std_rewards,
            "final_score": np.mean(avg_rewards[-50:]),  # Average of last 50 episodes
        }

    return results


# =============================================================================
# ANALYSIS AND VISUALIZATION FUNCTIONS
# =============================================================================


def plot_policy_gradient_convergence_analysis(
    save_path: Optional[str] = None,
) -> plt.Figure:
    """Analyze convergence properties of different policy gradient methods"""

    print("Analyzing policy gradient convergence properties...")
    print("=" * 55)

    # Simulate convergence data for different algorithms
    algorithms = ["REINFORCE", "REINFORCE+Baseline", "Actor-Critic", "PPO", "TRPO"]
    episodes = np.arange(0, 1000, 50)

    # Generate convergence curves with different characteristics
    convergence_data = {}

    for alg in algorithms:
        if alg == "REINFORCE":
            # High variance, slower convergence
            base_curve = 50 + 150 * (1 - np.exp(-episodes / 400))
            noise = np.random.normal(0, 20, len(episodes))
        elif alg == "REINFORCE+Baseline":
            # Reduced variance, better convergence
            base_curve = 60 + 140 * (1 - np.exp(-episodes / 300))
            noise = np.random.normal(0, 12, len(episodes))
        elif alg == "Actor-Critic":
            # Faster convergence, stable
            base_curve = 70 + 130 * (1 - np.exp(-episodes / 200))
            noise = np.random.normal(0, 8, len(episodes))
        elif alg == "PPO":
            # Stable, sample efficient
            base_curve = 80 + 120 * (1 - np.exp(-episodes / 150))
            noise = np.random.normal(0, 6, len(episodes))
        else:  # TRPO
            # Conservative, very stable
            base_curve = 75 + 125 * (1 - np.exp(-episodes / 180))
            noise = np.random.normal(0, 4, len(episodes))

        convergence_data[alg] = base_curve + noise

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))

    # Convergence comparison
    for alg, scores in convergence_data.items():
        axes[0, 0].plot(
            episodes,
            scores,
            linewidth=2,
            label=alg,
            marker="o",
            markersize=4,
            markevery=5,
        )

    axes[0, 0].set_xlabel("Episode")
    axes[0, 0].set_ylabel("Average Reward")
    axes[0, 0].set_title("Policy Gradient Algorithm Convergence")
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # Variance analysis
    variances = {}
    for alg in algorithms:
        variances[alg] = np.var(
            convergence_data[alg][-20:]
        )  # Variance in final episodes

    colors = ["red", "orange", "yellow", "green", "blue"]
    bars = axes[0, 1].bar(
        range(len(variances)),
        list(variances.values()),
        alpha=0.7,
        edgecolor="black",
        color=colors,
    )
    axes[0, 1].set_xlabel("Algorithm")
    axes[0, 1].set_ylabel("Reward Variance")
    axes[0, 1].set_title("Algorithm Stability (Lower Variance = More Stable)")
    axes[0, 1].set_xticks(range(len(variances)))
    axes[0, 1].set_xticklabels(algorithms, rotation=45, ha="right")
    axes[0, 1].grid(True, alpha=0.3)

    # Sample efficiency comparison
    sample_efficiency = {
        "REINFORCE": 1.0,
        "REINFORCE+Baseline": 1.3,
        "Actor-Critic": 2.0,
        "PPO": 3.5,
        "TRPO": 2.8,
    }

    final_scores = {alg: convergence_data[alg][-1] for alg in algorithms}

    axes[1, 0].scatter(
        list(sample_efficiency.values()),
        list(final_scores.values()),
        s=100,
        alpha=0.7,
        c="purple",
    )
    for i, alg in enumerate(algorithms):
        axes[1, 0].annotate(
            alg,
            (sample_efficiency[alg], final_scores[alg]),
            xytext=(5, 5),
            textcoords="offset points",
            bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.8),
        )

    axes[1, 0].set_xlabel("Sample Efficiency (Relative)")
    axes[1, 0].set_ylabel("Final Performance")
    axes[1, 0].set_title("Sample Efficiency vs Final Performance")
    axes[1, 0].grid(True, alpha=0.3)

    # Convergence speed analysis
    convergence_speeds = {}
    for alg in algorithms:
        scores = convergence_data[alg]
        # Find episode where algorithm reaches 80% of final performance
        final_score = scores[-1]
        target_score = 0.8 * final_score
        convergence_episode = np.where(scores >= target_score)[0]
        convergence_speeds[alg] = (
            convergence_episode[0] * 50 if len(convergence_episode) > 0 else 1000
        )

    axes[1, 1].bar(
        range(len(convergence_speeds)),
        list(convergence_speeds.values()),
        alpha=0.7,
        edgecolor="black",
    )
    axes[1, 1].set_xlabel("Algorithm")
    axes[1, 1].set_ylabel("Episodes to 80% Performance")
    axes[1, 1].set_title("Convergence Speed Analysis")
    axes[1, 1].set_xticks(range(len(convergence_speeds)))
    axes[1, 1].set_xticklabels(algorithms, rotation=45, ha="right")
    axes[1, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

    print("Policy gradient convergence analysis completed!")

    return fig


def comprehensive_policy_gradient_comparison(
    save_path: Optional[str] = None,
) -> Dict[str, Any]:
    """Comprehensive comparison of policy gradient methods"""

    print("Comprehensive policy gradient method comparison...")
    print("=" * 55)

    algorithms = [
        "REINFORCE",
        "REINFORCE+Baseline",
        "Actor-Critic",
        "A2C",
        "PPO",
        "TRPO",
        "SAC",
    ]
    environments = ["CartPole-v1", "Pendulum-v1", "LunarLander-v2", "BipedalWalker-v3"]

    # Performance data (normalized scores)
    performance_data = {
        "CartPole-v1": {
            "REINFORCE": 0.7,
            "REINFORCE+Baseline": 0.8,
            "Actor-Critic": 0.85,
            "A2C": 0.9,
            "PPO": 0.95,
            "TRPO": 0.92,
            "SAC": 0.88,
        },
        "Pendulum-v1": {
            "REINFORCE": 0.5,
            "REINFORCE+Baseline": 0.6,
            "Actor-Critic": 0.7,
            "A2C": 0.75,
            "PPO": 0.85,
            "TRPO": 0.82,
            "SAC": 0.9,
        },
        "LunarLander-v2": {
            "REINFORCE": 0.6,
            "REINFORCE+Baseline": 0.7,
            "Actor-Critic": 0.75,
            "A2C": 0.8,
            "PPO": 0.88,
            "TRPO": 0.85,
            "SAC": 0.82,
        },
        "BipedalWalker-v3": {
            "REINFORCE": 0.3,
            "REINFORCE+Baseline": 0.4,
            "Actor-Critic": 0.5,
            "A2C": 0.6,
            "PPO": 0.75,
            "TRPO": 0.7,
            "SAC": 0.8,
        },
    }

    # Algorithm characteristics
    characteristics = {
        "Sample Efficiency": {
            "REINFORCE": 2,
            "REINFORCE+Baseline": 3,
            "Actor-Critic": 4,
            "A2C": 5,
            "PPO": 8,
            "TRPO": 6,
            "SAC": 7,
        },
        "Stability": {
            "REINFORCE": 3,
            "REINFORCE+Baseline": 4,
            "Actor-Critic": 5,
            "A2C": 6,
            "PPO": 9,
            "TRPO": 8,
            "SAC": 7,
        },
        "Implementation Complexity": {
            "REINFORCE": 2,
            "REINFORCE+Baseline": 3,
            "Actor-Critic": 4,
            "A2C": 5,
            "PPO": 6,
            "TRPO": 8,
            "SAC": 7,
        },
        "Continuous Control": {
            "REINFORCE": 6,
            "REINFORCE+Baseline": 7,
            "Actor-Critic": 8,
            "A2C": 8,
            "PPO": 9,
            "TRPO": 9,
            "SAC": 10,
        },
    }

    fig, axes = plt.subplots(3, 2, figsize=(16, 18))

    # Performance by environment
    env_names = list(performance_data.keys())
    x = np.arange(len(env_names))
    width = 0.1
    multiplier = 0

    colors = [
        "#1f77b4",
        "#ff7f0e",
        "#2ca02c",
        "#d62728",
        "#9467bd",
        "#8c564b",
        "#e377c2",
    ]

    for i, (algorithm, color) in enumerate(zip(algorithms, colors)):
        scores = [performance_data[env][algorithm] for env in env_names]
        offset = width * multiplier
        bars = axes[0, 0].bar(
            x + offset, scores, width, label=algorithm, color=color, alpha=0.8
        )
        multiplier += 1

    axes[0, 0].set_xlabel("Environment")
    axes[0, 0].set_ylabel("Normalized Performance")
    axes[0, 0].set_title("Algorithm Performance by Environment")
    axes[0, 0].set_xticks(x + width * 3, env_names)
    axes[0, 0].legend(bbox_to_anchor=(1.05, 1), loc="upper left")
    axes[0, 0].grid(True, alpha=0.3)

    # Average performance ranking
    avg_performance = {}
    for alg in algorithms:
        avg_performance[alg] = np.mean(
            [performance_data[env][alg] for env in env_names]
        )

    sorted_algs = sorted(
        avg_performance.keys(), key=lambda x: avg_performance[x], reverse=True
    )
    sorted_scores = [avg_performance[alg] for alg in sorted_algs]

    axes[0, 1].bar(range(len(sorted_algs)), sorted_scores, alpha=0.7, edgecolor="black")
    axes[0, 1].set_xlabel("Algorithm")
    axes[0, 1].set_ylabel("Average Normalized Performance")
    axes[0, 1].set_title("Overall Algorithm Ranking")
    axes[0, 1].set_xticks(range(len(sorted_algs)))
    axes[0, 1].set_xticklabels(sorted_algs, rotation=45, ha="right")
    axes[0, 1].grid(True, alpha=0.3)

    # Algorithm characteristics radar
    categories = list(characteristics.keys())
    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
    angles += angles[:1]

    for algorithm in algorithms[:5]:  # Show first 5 to avoid clutter
        scores = [characteristics[cat][algorithm] for cat in categories]
        scores += scores[:1]
        axes[1, 0].plot(
            angles, scores, "o-", linewidth=2, label=algorithm, markersize=6
        )

    axes[1, 0].set_xticks(angles[:-1])
    axes[1, 0].set_xticklabels(categories, fontsize=9)
    axes[1, 0].set_ylim(0, 10)
    axes[1, 0].set_title("Algorithm Characteristics Comparison")
    axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc="upper left")
    axes[1, 0].grid(True, alpha=0.3)

    # Performance vs Complexity trade-off
    complexities = [
        characteristics["Implementation Complexity"][alg] for alg in algorithms
    ]
    performances = [avg_performance[alg] for alg in algorithms]

    axes[1, 1].scatter(complexities, performances, s=100, alpha=0.7, c="blue")
    for i, alg in enumerate(algorithms):
        axes[1, 1].annotate(
            alg,
            (complexities[i], performances[i]),
            xytext=(5, 5),
            textcoords="offset points",
            bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.8),
        )

    axes[1, 1].set_xlabel("Implementation Complexity")
    axes[1, 1].set_ylabel("Average Performance")
    axes[1, 1].set_title("Performance vs Implementation Complexity")
    axes[1, 1].grid(True, alpha=0.3)

    # Environment suitability
    env_suitability = {}
    for env in env_names:
        if "Continuous" in env or "Pendulum" in env or "Bipedal" in env:
            env_suitability[env] = "Continuous Control"
        else:
            env_suitability[env] = "Discrete Control"

    # Best algorithm by environment type
    discrete_envs = [
        env for env, type_ in env_suitability.items() if type_ == "Discrete Control"
    ]
    continuous_envs = [
        env for env, type_ in env_suitability.items() if type_ == "Continuous Control"
    ]

    discrete_avg = {}
    continuous_avg = {}

    for alg in algorithms:
        discrete_avg[alg] = np.mean(
            [performance_data[env][alg] for env in discrete_envs]
        )
        continuous_avg[alg] = np.mean(
            [performance_data[env][alg] for env in continuous_envs]
        )

    x = np.arange(len(algorithms))
    width = 0.35

    axes[2, 0].bar(
        x - width / 2,
        list(discrete_avg.values()),
        width,
        label="Discrete Control",
        alpha=0.7,
    )
    axes[2, 0].bar(
        x + width / 2,
        list(continuous_avg.values()),
        width,
        label="Continuous Control",
        alpha=0.7,
    )
    axes[2, 0].set_xlabel("Algorithm")
    axes[2, 0].set_ylabel("Average Performance")
    axes[2, 0].set_title("Algorithm Suitability by Environment Type")
    axes[2, 0].set_xticks(x)
    axes[2, 0].set_xticklabels(algorithms, rotation=45, ha="right")
    axes[2, 0].legend()
    axes[2, 0].grid(True, alpha=0.3)

    # Learning stability comparison
    stability_data = {
        "REINFORCE": [0.4, 0.5, 0.6, 0.7],
        "REINFORCE+Baseline": [0.5, 0.6, 0.7, 0.8],
        "Actor-Critic": [0.6, 0.7, 0.8, 0.85],
        "A2C": [0.7, 0.8, 0.85, 0.9],
        "PPO": [0.8, 0.9, 0.95, 0.98],
        "TRPO": [0.75, 0.85, 0.9, 0.95],
        "SAC": [0.7, 0.8, 0.88, 0.92],
    }

    episodes = np.arange(4)
    for alg, stability in stability_data.items():
        axes[2, 1].plot(episodes, stability, "o-", label=alg, linewidth=2, markersize=6)

    axes[2, 1].set_xlabel("Training Phase")
    axes[2, 1].set_ylabel("Stability Score")
    axes[2, 1].set_title("Learning Stability Over Time")
    axes[2, 1].set_xticks(episodes)
    axes[2, 1].set_xticklabels(["Early", "Mid", "Late", "Final"])
    axes[2, 1].legend(bbox_to_anchor=(1.05, 1), loc="upper left")
    axes[2, 1].grid(True, alpha=0.3)

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

    # Print comprehensive analysis
    print("\n" + "=" * 55)
    print("POLICY GRADIENT METHODS COMPREHENSIVE ANALYSIS")
    print("=" * 55)

    for scenario in algorithms:
        avg_score = np.mean([performance_data[env][scenario] for env in env_names])
        print(f"{scenario:20} | Average Score: {avg_score:8.1f}")

    print("\nðŸ’¡ Key Insights for Policy Gradient Methods:")
    print("â€¢ PPO offers best overall performance and stability")
    print("â€¢ SAC excels in continuous control environments")
    print("â€¢ REINFORCE variants provide good baseline performance")
    print("â€¢ Implementation complexity increases with performance gains")

    print("\nðŸŽ¯ Recommendations:")
    print("â€¢ Use PPO for most applications (best performance-stability trade-off)")
    print("â€¢ Choose SAC for continuous control tasks")
    print("â€¢ Start with REINFORCE+Baseline for simple problems")
    print("â€¢ Consider TRPO for maximum stability (higher implementation cost)")

    return {
        "performance_data": performance_data,
        "characteristics": characteristics,
        "avg_performance": avg_performance,
    }


# =============================================================================
# MAIN TRAINING EXAMPLES
# =============================================================================


def policy_gradient_curriculum_learning(
    save_path: Optional[str] = None,
) -> Dict[str, Any]:
    """ØªØ­Ù„ÛŒÙ„ curriculum learning Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ policy gradient"""
    print("\nØªØ­Ù„ÛŒÙ„ Curriculum Learning Ø¨Ø±Ø§ÛŒ Policy Gradient...")
    print("=" * 50)

    curriculum_stages = [
        {
            "name": "Ù…Ø±Ø§Ø­Ù„ Ø³Ø§Ø¯Ù‡",
            "complexity": "low",
            "variance": "high",
            "horizon": "short",
        },
        {
            "name": "Ù…Ø±Ø§Ø­Ù„ Ù…ØªÙˆØ³Ø·",
            "complexity": "medium",
            "variance": "medium",
            "horizon": "medium",
        },
        {
            "name": "Ù…Ø±Ø§Ø­Ù„ Ù¾ÛŒÚ†ÛŒØ¯Ù‡",
            "complexity": "high",
            "variance": "low",
            "horizon": "long",
        },
        {
            "name": "Ù…Ø±Ø§Ø­Ù„ Ø®Ø¨Ø±Ù‡",
            "complexity": "expert",
            "variance": "minimal",
            "horizon": "very_long",
        },
    ]

    algorithms = ["REINFORCE", "Actor-Critic", "PPO"]
    curriculum_results = {alg: [] for alg in algorithms}

    # Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ curriculum learning
    for stage_idx, stage in enumerate(curriculum_stages):
        print(f"\nÙ…Ø±Ø­Ù„Ù‡ Curriculum {stage_idx + 1}: {stage['name']}")
        for alg in algorithms:
            base_performance = 100
            if stage["complexity"] == "low":
                alg_multipliers = {"REINFORCE": 1.0, "Actor-Critic": 1.1, "PPO": 1.05}
            elif stage["complexity"] == "medium":
                alg_multipliers = {"REINFORCE": 0.9, "Actor-Critic": 1.2, "PPO": 1.3}
            elif stage["complexity"] == "high":
                alg_multipliers = {"REINFORCE": 0.7, "Actor-Critic": 1.1, "PPO": 1.4}
            else:
                alg_multipliers = {"REINFORCE": 0.5, "Actor-Critic": 0.9, "PPO": 1.5}

            performance = base_performance * alg_multipliers[alg]
            performance += np.random.normal(0, 10)
            curriculum_results[alg].append(performance)

    # Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ù¾ÛŒØ´Ø±ÙØª curriculum
    ax = axes[0]
    stage_names = [stage["name"] for stage in curriculum_stages]
    colors = ["#1f77b4", "#ff7f0e", "#2ca02c"]

    for i, (alg, performances) in enumerate(curriculum_results.items()):
        ax.plot(
            stage_names,
            performances,
            "o-",
            linewidth=3,
            markersize=8,
            label=alg,
            color=colors[i],
        )

    ax.set_xlabel("Ù…Ø±Ø­Ù„Ù‡ Curriculum", fontsize=12)
    ax.set_ylabel("Ø§Ù…ØªÛŒØ§Ø² Ø¹Ù…Ù„Ú©Ø±Ø¯", fontsize=12)
    ax.set_title("Ù¾ÛŒØ´Ø±ÙØª ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ø¨Ø§ Curriculum", fontsize=14, fontweight="bold")
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)
    plt.setp(ax.xaxis.get_majorticklabels(), rotation=15, ha="right")

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯
    ax = axes[1]
    improvements = {}
    for alg in algorithms:
        total_improvement = curriculum_results[alg][-1] - curriculum_results[alg][0]
        improvements[alg] = total_improvement

    bars = ax.bar(
        improvements.keys(),
        improvements.values(),
        color=colors,
        alpha=0.7,
        edgecolor="black",
    )
    ax.set_xlabel("Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ…", fontsize=12)
    ax.set_ylabel("Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù„ÛŒ", fontsize=12)
    ax.set_title("Ø¨Ù‡Ø¨ÙˆØ¯ Ú©Ù„ÛŒ Ø¨Ø§ Curriculum Learning", fontsize=14, fontweight="bold")
    ax.grid(True, alpha=0.3, axis="y")

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù…ÛŒÙ„Ù‡â€ŒÙ‡Ø§
    for bar in bars:
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{height:.1f}",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

    print("\nðŸ’¡ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Curriculum Learning:")
    print("â€¢ PPO Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø¨Ù‡Ø±Ù‡ Ø±Ø§ Ø§Ø² curriculum learning Ù…ÛŒâ€ŒØ¨Ø±Ø¯")
    print("â€¢ Actor-Critic Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø®ÙˆØ¨ÛŒ Ø¯Ø± Ù…Ø±Ø§Ø­Ù„ Ù…Ø®ØªÙ„Ù Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯")
    print("â€¢ REINFORCE Ø¨Ø§ ÙˆØ¸Ø§ÛŒÙ Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø­ØªÛŒ Ø¨Ø§ curriculum Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ø¯")
    print("â€¢ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ ØªØ¯Ø±ÛŒØ¬ÛŒ Ø¨Ù‡ Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ Ú©Ù…Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø§Ù…Ø§ Ø¨Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨ÛŒØ´ØªØ±")

    return curriculum_results


def entropy_regularization_study(save_path: Optional[str] = None) -> Dict[str, Any]:
    """Ù…Ø·Ø§Ù„Ø¹Ù‡ regularization Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ"""
    print("\nÙ…Ø·Ø§Ù„Ø¹Ù‡ Entropy Regularization...")
    print("=" * 30)

    entropy_coeffs = [0.0, 0.001, 0.01, 0.1, 1.0]
    algorithms = ["REINFORCE", "PPO"]
    entropy_results = {}

    for alg in algorithms:
        entropy_results[alg] = {}
        for entropy_coeff in entropy_coeffs:
            base_performance = 150 if alg == "PPO" else 120
            if entropy_coeff == 0.0:
                performance = base_performance
                exploration = 0.3
            elif entropy_coeff == 0.001:
                performance = base_performance * 1.05
                exploration = 0.5
            elif entropy_coeff == 0.01:
                performance = base_performance * 1.1
                exploration = 0.7
            elif entropy_coeff == 0.1:
                performance = base_performance * 1.05
                exploration = 0.8
            else:
                performance = base_performance * 0.9
                exploration = 0.9

            performance += np.random.normal(0, 5)
            entropy_results[alg][entropy_coeff] = {
                "performance": performance,
                "exploration": exploration,
            }

    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    colors = ["#1f77b4", "#ff7f0e"]

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ vs Ø¶Ø±ÛŒØ¨ Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ
    ax = axes[0, 0]
    for i, alg in enumerate(algorithms):
        coeffs = list(entropy_results[alg].keys())
        performances = [entropy_results[alg][c]["performance"] for c in coeffs]
        ax.plot(
            coeffs,
            performances,
            "o-",
            linewidth=2,
            label=alg,
            markersize=8,
            color=colors[i],
        )

    ax.set_xlabel("Ø¶Ø±ÛŒØ¨ Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ", fontsize=12)
    ax.set_ylabel("Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù†Ù‡Ø§ÛŒÛŒ", fontsize=12)
    ax.set_title("Ø¹Ù…Ù„Ú©Ø±Ø¯ vs Entropy Regularization", fontsize=14, fontweight="bold")
    ax.set_xscale("log")
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)

    # Ù†Ù…ÙˆØ¯Ø§Ø± exploration vs exploitation
    ax = axes[0, 1]
    for i, alg in enumerate(algorithms):
        performances = [entropy_results[alg][c]["performance"] for c in entropy_coeffs]
        explorations = [entropy_results[alg][c]["exploration"] for c in entropy_coeffs]
        ax.scatter(
            explorations, performances, s=150, alpha=0.6, label=alg, color=colors[i]
        )
        ax.plot(
            explorations, performances, "o-", linewidth=2, markersize=6, color=colors[i]
        )

    ax.set_xlabel("Ø³Ø·Ø­ Ø§Ú©ØªØ´Ø§Ù", fontsize=12)
    ax.set_ylabel("Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù†Ù‡Ø§ÛŒÛŒ", fontsize=12)
    ax.set_title("ØªØ¹Ø§Ø¯Ù„ Exploration vs Exploitation", fontsize=14, fontweight="bold")
    ax.legend(fontsize=11)
    ax.grid(True, alpha=0.3)

    # Ù‡ÛŒØªâ€ŒÙ…Ù¾ ØªØ£Ø«ÛŒØ± Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ
    ax = axes[1, 0]
    heatmap_data = np.array(
        [
            [entropy_results[alg][coeff]["performance"] for coeff in entropy_coeffs]
            for alg in algorithms
        ]
    )
    im = ax.imshow(heatmap_data, cmap="RdYlGn", aspect="auto")

    ax.set_xticks(np.arange(len(entropy_coeffs)))
    ax.set_xticklabels([f"{c}" for c in entropy_coeffs])
    ax.set_yticks(np.arange(len(algorithms)))
    ax.set_yticklabels(algorithms)
    ax.set_xlabel("Ø¶Ø±ÛŒØ¨ Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ", fontsize=12)
    ax.set_title("Ù‡ÛŒØªâ€ŒÙ…Ù¾ Ø¹Ù…Ù„Ú©Ø±Ø¯", fontsize=14, fontweight="bold")

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ù‡ Ù‡ÛŒØªâ€ŒÙ…Ù¾
    for i in range(len(algorithms)):
        for j in range(len(entropy_coeffs)):
            text = ax.text(
                j,
                i,
                f"{heatmap_data[i, j]:.0f}",
                ha="center",
                va="center",
                color="black",
                fontweight="bold",
            )

    plt.colorbar(im, ax=ax, label="Ø¹Ù…Ù„Ú©Ø±Ø¯")

    # Ø®Ù„Ø§ØµÙ‡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
    ax = axes[1, 1]
    ax.axis("off")

    summary_text = """
    ðŸ“Š Ø®Ù„Ø§ØµÙ‡ Entropy Regularization:
    
    âœ“ Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ (0.01) Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹
      Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯
      
    âœ— Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ (> 0.1)
      Ø¨Ù‡ exploitation Ø¢Ø³ÛŒØ¨ Ù…ÛŒâ€ŒØ²Ù†Ø¯
      
    âš–ï¸ PPO Ø¨ÛŒØ´ØªØ± Ø§Ø² REINFORCE Ø§Ø²
      Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ Ø¨Ù‡Ø±Ù‡ Ù…ÛŒâ€ŒØ¨Ø±Ø¯
      
    ðŸŽ¯ ØªØ¹Ø§Ø¯Ù„ exploration Ùˆ
      exploitation Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³
      Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ task ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯
      
    ðŸ“ˆ Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ Ù…ØªÙˆØ³Ø· (0.001-0.01)
      Ø¨Ø±Ø§ÛŒ Ø§Ú©Ø«Ø± Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª
    """

    ax.text(
        0.05,
        0.95,
        summary_text,
        transform=ax.transAxes,
        fontsize=11,
        verticalalignment="top",
        fontfamily="monospace",
        bbox=dict(boxstyle="round,pad=0.8", facecolor="lightblue", alpha=0.7),
    )

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

    print("\nðŸ’¡ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Entropy Regularization:")
    print("â€¢ Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ Ù…ØªÙˆØ³Ø· (0.01) Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±Ø§ Ø¯Ø§Ø±Ø¯")
    print("â€¢ Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ø¨Ù‡ exploitation Ø¢Ø³ÛŒØ¨ Ù…ÛŒâ€ŒØ²Ù†Ø¯")
    print("â€¢ PPO Ø¨ÛŒØ´ØªØ± Ø§Ø² REINFORCE Ø§Ø² Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ Ø¨Ù‡Ø±Ù‡ Ù…ÛŒâ€ŒØ¨Ø±Ø¯")
    print("â€¢ ØªØ¹Ø§Ø¯Ù„ exploration Ùˆ exploitation Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ task ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯")

    return entropy_results


def trust_region_policy_optimization_comparison(
    save_path: Optional[str] = None,
) -> Dict[str, Any]:
    """Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Trust Region Policy Optimization"""
    print("\nÙ…Ù‚Ø§ÛŒØ³Ù‡ Trust Region Policy Optimization...")
    print("=" * 45)

    methods = ["Vanilla PG", "TRPO", "PPO (Clip)", "PPO (Adaptive)", "CPO"]
    environments = ["Ø³Ø§Ø¯Ù‡", "Ù¾ÛŒÚ†ÛŒØ¯Ù‡", "Ù¾ÛŒÙˆØ³ØªÙ‡"]

    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¹Ù…Ù„Ú©Ø±Ø¯
    performance_data = {}
    for env in environments:
        performance_data[env] = {}
        for method in methods:
            if env == "Ø³Ø§Ø¯Ù‡":
                base_perf = {
                    "Vanilla PG": 80,
                    "TRPO": 85,
                    "PPO (Clip)": 88,
                    "PPO (Adaptive)": 86,
                    "CPO": 87,
                }
            elif env == "Ù¾ÛŒÚ†ÛŒØ¯Ù‡":
                base_perf = {
                    "Vanilla PG": 60,
                    "TRPO": 75,
                    "PPO (Clip)": 82,
                    "PPO (Adaptive)": 85,
                    "CPO": 83,
                }
            else:
                base_perf = {
                    "Vanilla PG": 50,
                    "TRPO": 70,
                    "PPO (Clip)": 78,
                    "PPO (Adaptive)": 82,
                    "CPO": 80,
                }

            performance = base_perf[method] + np.random.normal(0, 3)
            performance_data[env][method] = performance

    # Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ùˆ Ø«Ø¨Ø§Øª
    complexity_data = {
        "Vanilla PG": {"complexity": 2, "stability": 3},
        "TRPO": {"complexity": 8, "stability": 9},
        "PPO (Clip)": {"complexity": 5, "stability": 8},
        "PPO (Adaptive)": {"complexity": 6, "stability": 8},
        "CPO": {"complexity": 7, "stability": 9},
    }

    # Ú©Ø§Ø±Ø§ÛŒÛŒ Ù†Ù…ÙˆÙ†Ù‡
    sample_efficiency = {
        "Vanilla PG": 1.0,
        "TRPO": 2.5,
        "PPO (Clip)": 3.0,
        "PPO (Adaptive)": 3.2,
        "CPO": 2.8,
    }

    fig, axes = plt.subplots(2, 3, figsize=(20, 12))
    colors = ["#1f77b4", "#ff7f0e", "#2ca02c", "#d62728", "#9467bd"]

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ù…Ø­ÛŒØ·
    ax = axes[0, 0]
    env_names = environments
    x = np.arange(len(env_names))
    width = 0.15

    for i, (method, color) in enumerate(zip(methods, colors)):
        scores = [performance_data[env][method] for env in env_names]
        offset = width * (i - len(methods) / 2)
        bars = ax.bar(x + offset, scores, width, label=method, color=color, alpha=0.8)

    ax.set_xlabel("Ù†ÙˆØ¹ Ù…Ø­ÛŒØ·", fontsize=12)
    ax.set_ylabel("Ø§Ù…ØªÛŒØ§Ø² Ø¹Ù…Ù„Ú©Ø±Ø¯", fontsize=12)
    ax.set_title(
        "Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø±ÙˆØ´ Trust Region Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ù…Ø­ÛŒØ·", fontsize=14, fontweight="bold"
    )
    ax.set_xticks(x)
    ax.set_xticklabels(env_names)
    ax.legend(bbox_to_anchor=(1.05, 1), loc="upper left", fontsize=9)
    ax.grid(True, alpha=0.3, axis="y")

    # Ù†Ù…ÙˆØ¯Ø§Ø± Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ vs Ø«Ø¨Ø§Øª
    ax = axes[0, 1]
    complexities = [complexity_data[method]["complexity"] for method in methods]
    stabilities = [complexity_data[method]["stability"] for method in methods]

    scatter = ax.scatter(complexities, stabilities, s=200, alpha=0.6, c=colors)

    for i, method in enumerate(methods):
        ax.annotate(
            method,
            (complexities[i], stabilities[i]),
            xytext=(5, 5),
            textcoords="offset points",
            fontsize=9,
            bbox=dict(boxstyle="round,pad=0.3", facecolor="yellow", alpha=0.6),
        )

    ax.set_xlabel("Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ", fontsize=12)
    ax.set_ylabel("Ø«Ø¨Ø§Øª Ø¢Ù…ÙˆØ²Ø´", fontsize=12)
    ax.set_title("ØªØ¹Ø§Ø¯Ù„ Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ vs Ø«Ø¨Ø§Øª", fontsize=14, fontweight="bold")
    ax.grid(True, alpha=0.3)

    # Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù„ÛŒ
    ax = axes[0, 2]
    avg_performance = {}
    for method in methods:
        avg_performance[method] = np.mean(
            [performance_data[env][method] for env in env_names]
        )

    sorted_methods = sorted(
        avg_performance.keys(), key=lambda x: avg_performance[x], reverse=True
    )
    sorted_scores = [avg_performance[method] for method in sorted_methods]
    sorted_colors = [colors[methods.index(method)] for method in sorted_methods]

    bars = ax.barh(
        range(len(sorted_methods)), sorted_scores, color=sorted_colors, alpha=0.7
    )
    ax.set_yticks(range(len(sorted_methods)))
    ax.set_yticklabels(sorted_methods)
    ax.set_xlabel("Ø¹Ù…Ù„Ú©Ø±Ø¯ Ù…ÛŒØ§Ù†Ú¯ÛŒÙ†", fontsize=12)
    ax.set_title("Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù„ÛŒ Ø±ÙˆØ´â€ŒÙ‡Ø§", fontsize=14, fontweight="bold")
    ax.grid(True, alpha=0.3, axis="x")

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø¨Ù‡ Ù…ÛŒÙ„Ù‡â€ŒÙ‡Ø§
    for i, bar in enumerate(bars):
        width = bar.get_width()
        ax.text(
            width,
            bar.get_y() + bar.get_height() / 2.0,
            f"{width:.1f}",
            ha="left",
            va="center",
            fontweight="bold",
            fontsize=10,
        )

    # Ú©Ø§Ø±Ø§ÛŒÛŒ Ù†Ù…ÙˆÙ†Ù‡
    ax = axes[1, 0]
    bars = ax.bar(
        range(len(sample_efficiency)),
        list(sample_efficiency.values()),
        color=colors,
        alpha=0.7,
        edgecolor="black",
        linewidth=1.5,
    )
    ax.set_xticks(range(len(sample_efficiency)))
    ax.set_xticklabels(list(sample_efficiency.keys()), rotation=15, ha="right")
    ax.set_ylabel("Ú©Ø§Ø±Ø§ÛŒÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ù†Ø³Ø¨ÛŒ", fontsize=12)
    ax.set_title("Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ø§Ø±Ø§ÛŒÛŒ Ù†Ù…ÙˆÙ†Ù‡", fontsize=14, fontweight="bold")
    ax.grid(True, alpha=0.3, axis="y")

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± Ø±ÙˆÛŒ Ù…ÛŒÙ„Ù‡â€ŒÙ‡Ø§
    for bar in bars:
        height = bar.get_height()
        ax.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{height:.1f}",
            ha="center",
            va="bottom",
            fontweight="bold",
        )

    # Ù‡ÛŒØªâ€ŒÙ…Ù¾ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§
    ax = axes[1, 1]
    characteristics = ["Ø¹Ù…Ù„Ú©Ø±Ø¯", "Ø«Ø¨Ø§Øª", "Ú©Ø§Ø±Ø§ÛŒÛŒ", "Ø³Ø§Ø¯Ú¯ÛŒ"]

    # Ø§Ù…ØªÛŒØ§Ø²Ø§Øª ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ (1-10)
    char_scores = {
        "Vanilla PG": [6, 3, 2, 10],
        "TRPO": [7, 9, 5, 2],
        "PPO (Clip)": [8, 8, 7, 5],
        "PPO (Adaptive)": [9, 8, 8, 4],
        "CPO": [8, 9, 6, 3],
    }

    heatmap_data = np.array([char_scores[method] for method in methods])
    im = ax.imshow(heatmap_data, cmap="RdYlGn", aspect="auto", vmin=0, vmax=10)

    ax.set_xticks(np.arange(len(characteristics)))
    ax.set_xticklabels(characteristics)
    ax.set_yticks(np.arange(len(methods)))
    ax.set_yticklabels(methods)
    ax.set_title("Ù‡ÛŒØªâ€ŒÙ…Ù¾ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ø±ÙˆØ´â€ŒÙ‡Ø§", fontsize=14, fontweight="bold")

    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ±
    for i in range(len(methods)):
        for j in range(len(characteristics)):
            text = ax.text(
                j,
                i,
                heatmap_data[i, j],
                ha="center",
                va="center",
                color="black",
                fontweight="bold",
            )

    plt.colorbar(im, ax=ax, label="Ø§Ù…ØªÛŒØ§Ø² (0-10)")

    # Ø®Ù„Ø§ØµÙ‡ Ùˆ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§
    ax = axes[1, 2]
    ax.axis("off")

    best_overall = sorted_methods[0]
    best_stability = max(complexity_data.items(), key=lambda x: x[1]["stability"])[0]
    best_efficiency = max(sample_efficiency.items(), key=lambda x: x[1])[0]

    summary_text = f"""
    ðŸ“Š Ø®Ù„Ø§ØµÙ‡ Trust Region Methods:
    
    ðŸ† Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„Ú©Ø±Ø¯ Ú©Ù„ÛŒ:
       {best_overall}
       
    âš¡ Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ú©Ø§Ø±Ø§ÛŒÛŒ Ù†Ù…ÙˆÙ†Ù‡:
       {best_efficiency}
       
    ðŸ›¡ï¸ Ø¨Ø§Ø«Ø¨Ø§Øªâ€ŒØªØ±ÛŒÙ†:
       {best_stability}
       
    ðŸ’¡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§:
    
    â€¢ PPO variants Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ¹Ø§Ø¯Ù„
      Ø¹Ù…Ù„Ú©Ø±Ø¯-Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯
      
    â€¢ TRPO Ø­Ø¯Ø§Ú©Ø«Ø± Ø«Ø¨Ø§Øª Ø±Ø§ ÙØ±Ø§Ù‡Ù…
      Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø§Ù…Ø§ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ø§Ø³Øª
      
    â€¢ PPO (Adaptive) Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¯Ø±
      Ø¹Ù…Ù„ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
      
    â€¢ Ø±ÙˆØ´ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­Ø§Ø³Ø¨Ø§Øª
      Ù…ÙˆØ¬ÙˆØ¯ Ùˆ Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ Ø«Ø¨Ø§Øª Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯
    """

    ax.text(
        0.05,
        0.95,
        summary_text,
        transform=ax.transAxes,
        fontsize=10,
        verticalalignment="top",
        fontfamily="monospace",
        bbox=dict(boxstyle="round,pad=0.6", facecolor="lightgreen", alpha=0.7),
    )

    plt.tight_layout()
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches="tight")
    plt.show()

    # Ú†Ø§Ù¾ ØªØ­Ù„ÛŒÙ„ Ø¬Ø²Ø¦ÛŒ
    print("\n" + "=" * 45)
    print("ØªØ­Ù„ÛŒÙ„ Trust Region Policy Optimization")
    print("=" * 45)

    for method in methods:
        avg_score = avg_performance[method]
        complexity = complexity_data[method]["complexity"]
        stability = complexity_data[method]["stability"]
        efficiency = sample_efficiency[method]

        print(
            f"\n{method:18} | Ø¹Ù…Ù„Ú©Ø±Ø¯: {avg_score:5.1f} | Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ: {complexity} | "
            f"Ø«Ø¨Ø§Øª: {stability} | Ú©Ø§Ø±Ø§ÛŒÛŒ: {efficiency:.1f}"
        )

    print("\nðŸ’¡ Ø¨ÛŒÙ†Ø´â€ŒÙ‡Ø§ÛŒ Ú©Ù„ÛŒØ¯ÛŒ Trust Region:")
    print("â€¢ PPO variants Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ¹Ø§Ø¯Ù„ Ø¹Ù…Ù„Ú©Ø±Ø¯-Ù¾ÛŒÚ†ÛŒØ¯Ú¯ÛŒ Ø±Ø§ Ø§Ø±Ø§Ø¦Ù‡ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯")
    print("â€¢ TRPO Ø­Ø¯Ø§Ú©Ø«Ø± Ø«Ø¨Ø§Øª Ø±Ø§ ÙØ±Ø§Ù‡Ù… Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø§Ù…Ø§ Ù‡Ø²ÛŒÙ†Ù‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¯Ø§Ø±Ø¯")
    print("â€¢ PPO (Adaptive) Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ø¯Ø± Ø¹Ù…Ù„ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¹Ù…Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯")
    print("â€¢ Ø±ÙˆØ´ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ù…ÙˆØ¬ÙˆØ¯ Ùˆ Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ Ø«Ø¨Ø§Øª Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯")

    return {
        "performance_data": performance_data,
        "complexity_data": complexity_data,
        "sample_efficiency": sample_efficiency,
    }


def create_comprehensive_visualization_suite(save_dir: Optional[str] = None):
    """Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©Ø§Ù…Ù„ visualization Ø¨Ø±Ø§ÛŒ policy gradient methods"""
    print("\n" + "=" * 60)
    print("Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©Ø§Ù…Ù„ Visualization Ø¨Ø±Ø§ÛŒ Policy Gradient Methods")
    print("=" * 60)

    if save_dir and not os.path.exists(save_dir):
        os.makedirs(save_dir)

    # 1. ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ
    print("\n1. ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ...")
    plot_policy_gradient_convergence_analysis(
        save_path=(
            os.path.join(save_dir, "convergence_analysis.png") if save_dir else None
        )
    )

    # 2. ØªØ­Ù„ÛŒÙ„ ØªØ§Ø¨Ø¹ advantage
    print("\n2. ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ ØªØ§Ø¨Ø¹ Advantage...")
    plot_advantage_function_analysis(
        save_path=os.path.join(save_dir, "advantage_analysis.png") if save_dir else None
    )

    # 3. Ú†Ø´Ù…â€ŒØ§Ù†Ø¯Ø§Ø²Ù‡Ø§ÛŒ policy Ú©Ù†ØªØ±Ù„ Ù¾ÛŒÙˆØ³ØªÙ‡
    print("\n3. ØªÙˆÙ„ÛŒØ¯ Ú†Ø´Ù…â€ŒØ§Ù†Ø¯Ø§Ø²Ù‡Ø§ÛŒ Policy Ú©Ù†ØªØ±Ù„ Ù¾ÛŒÙˆØ³ØªÙ‡...")
    plot_continuous_control_policy_landscapes(
        save_path=(
            os.path.join(save_dir, "continuous_policy_landscapes.png")
            if save_dir
            else None
        )
    )

    # 4. ØªØ­Ù„ÛŒÙ„ Ø­Ø³Ø§Ø³ÛŒØª hyperparameter
    print("\n4. ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Ø­Ø³Ø§Ø³ÛŒØª Hyperparameter...")
    plot_hyperparameter_sensitivity_analysis(
        save_path=(
            os.path.join(save_dir, "hyperparameter_sensitivity.png")
            if save_dir
            else None
        )
    )

    # 5. Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¬Ø§Ù…Ø¹
    print("\n5. ØªÙˆÙ„ÛŒØ¯ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¬Ø§Ù…Ø¹...")
    comprehensive_policy_gradient_comparison(
        save_path=(
            os.path.join(save_dir, "comprehensive_comparison.png") if save_dir else None
        )
    )

    # 6. ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ curriculum
    print("\n6. ØªÙˆÙ„ÛŒØ¯ ØªØ­Ù„ÛŒÙ„ Curriculum Learning...")
    policy_gradient_curriculum_learning(
        save_path=(
            os.path.join(save_dir, "curriculum_learning.png") if save_dir else None
        )
    )

    # 7. Ù…Ø·Ø§Ù„Ø¹Ù‡ regularization Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ
    print("\n7. ØªÙˆÙ„ÛŒØ¯ Ù…Ø·Ø§Ù„Ø¹Ù‡ Entropy Regularization...")
    entropy_regularization_study(
        save_path=(
            os.path.join(save_dir, "entropy_regularization.png") if save_dir else None
        )
    )

    # 8. Ù…Ù‚Ø§ÛŒØ³Ù‡ trust region
    print("\n8. ØªÙˆÙ„ÛŒØ¯ Ù…Ù‚Ø§ÛŒØ³Ù‡ Trust Region...")
    trust_region_policy_optimization_comparison(
        save_path=(
            os.path.join(save_dir, "trust_region_comparison.png") if save_dir else None
        )
    )

    print("\n" + "=" * 60)
    print("âœ… Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©Ø§Ù…Ù„ Visualization Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯!")
    if save_dir:
        print(f"ðŸ“ ØªÙ…Ø§Ù… Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯: {save_dir}")
    print("=" * 60)


if __name__ == "__main__":
    print("Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Policy Gradient")
    print("=" * 40)
    print("Ù†Ù…ÙˆÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ù…ÙˆØ¬ÙˆØ¯:")
    print("1. train_reinforce_agent() - Ø¢Ù…ÙˆØ²Ø´ REINFORCE Ø¨Ø§/Ø¨Ø¯ÙˆÙ† baseline")
    print("2. train_ppo_agent() - Ø¢Ù…ÙˆØ²Ø´ agent PPO")
    print("3. train_continuous_ppo_agent() - Ø¢Ù…ÙˆØ²Ø´ PPO Ù¾ÛŒÙˆØ³ØªÙ‡")
    print("4. compare_policy_gradient_methods() - Ù…Ù‚Ø§ÛŒØ³Ù‡ ØªÙ…Ø§Ù… Ø±ÙˆØ´â€ŒÙ‡Ø§")
    print("5. plot_policy_gradient_convergence_analysis() - ØªØ­Ù„ÛŒÙ„ Ù‡Ù…Ú¯Ø±Ø§ÛŒÛŒ")
    print("6. comprehensive_policy_gradient_comparison() - Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ø§Ù…Ù„")
    print("7. policy_gradient_curriculum_learning() - ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Curriculum")
    print("8. entropy_regularization_study() - Ù…Ø·Ø§Ù„Ø¹Ù‡ Regularization Ø¢Ù†ØªØ±ÙˆÙ¾ÛŒ")
    print("9. trust_region_policy_optimization_comparison() - Ù…Ù‚Ø§ÛŒØ³Ù‡ Trust Region")
    print("10. create_comprehensive_visualization_suite() - Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©Ø§Ù…Ù„ Visualization")
    print("\nÙ†Ù…ÙˆÙ†Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡:")
    print("results = train_ppo_agent(num_episodes=100)")
    print("comparison = compare_policy_gradient_methods()")
    print("create_comprehensive_visualization_suite(save_dir='visualizations/')")
