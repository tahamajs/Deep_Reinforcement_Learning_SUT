{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CA9: Advanced Policy Gradient Methods\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This assignment presents a comprehensive study of advanced policy gradient methods in deep reinforcement learning, covering the progression from basic policy gradients to state-of-the-art algorithms like Proximal Policy Optimization (PPO) and continuous control applications. We implement and analyze the theoretical foundations, practical implementations, and performance characteristics of various policy-based approaches, including REINFORCE, actor-critic methods, PPO, and trust region methods.\n",
    "\n",
    "**Keywords:** Policy gradient methods, REINFORCE, actor-critic, PPO, continuous control, variance reduction, trust region methods, generalized advantage estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Introduction and Learning Objectives](#introduction-and-learning-objectives)\n",
    "2. [Environment Setup](#environment-setup)\n",
    "3. [Section 1: Policy Gradient Foundations](#section-1-policy-gradient-foundations)\n",
    "4. [Section 2: REINFORCE Algorithm](#section-2-reinforce-algorithm)  \n",
    "5. [Section 3: Variance Reduction Techniques](#section-3-variance-reduction-techniques)\n",
    "6. [Section 4: Actor-Critic Methods](#section-4-actor-critic-methods)\n",
    "7. [Section 5: Proximal Policy Optimization (PPO)](#section-5-proximal-policy-optimization-ppo)\n",
    "8. [Section 6: Continuous Control](#section-6-continuous-control)\n",
    "9. [Section 7: Advanced Topics](#section-7-advanced-topics)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction and Learning Objectives\n",
    "\n",
    "Policy gradient methods represent a fundamental paradigm in reinforcement learning, directly optimizing the policy function to maximize expected returns. Unlike value-based methods that learn value functions and derive policies, policy gradient methods parameterize the policy directly and optimize it using gradient ascent.\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will understand:\n",
    "\n",
    "1. **Policy Gradient Foundations**:\n",
    "   - Policy gradient theorem and mathematical derivation\n",
    "   - REINFORCE algorithm and its limitations\n",
    "   - Variance reduction techniques (baselines, advantage functions)\n",
    "   \n",
    "2. **Advanced Policy Optimization**:\n",
    "   - Proximal Policy Optimization (PPO) algorithm\n",
    "   - Trust region methods and constrained optimization\n",
    "   - Generalized Advantage Estimation (GAE)\n",
    "   \n",
    "3. **Continuous Control**:\n",
    "   - Gaussian policies for continuous action spaces\n",
    "   - Action bound handling and numerical stability\n",
    "   - Policy gradient adaptations for continuous domains\n",
    "\n",
    "4. **Implementation Skills**:\n",
    "   - Complete policy gradient implementations from scratch\n",
    "   - Hyperparameter tuning and performance optimization\n",
    "   - Comparative analysis of different algorithms\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- **Mathematical Background**: Calculus, probability theory, linear algebra, statistics\n",
    "- **Programming Skills**: Advanced Python, PyTorch, NumPy, Matplotlib\n",
    "- **RL Knowledge**: MDPs, value functions, basic policy gradients, experience replay\n",
    "- **Previous Course Knowledge**: CA1-CA8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First, we set up the environment and import all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add project path\n",
    "sys.path.append(os.path.dirname(os.path.abspath('__file__')))\n",
    "\n",
    "# Core numerical and ML libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gymnasium as gym\n",
    "\n",
    "# Visualization and analysis\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set reproducible seeds\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seed for reproducible results\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ Environment setup completed!\")\n",
    "print(f\"üîß Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(f\"üêç PyTorch: {torch.__version__}\")\n",
    "print(f\"üéÆ Gymnasium: {gym.__version__}\")\n",
    "print(f\"üî¢ NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import project modules\n",
    "try:\n",
    "    from utils.utils import device\n",
    "    from utils.policy_gradient_visualizer import PolicyGradientVisualizer\n",
    "    from agents.reinforce import REINFORCEAgent, REINFORCEAnalyzer\n",
    "    from agents.baseline_reinforce import VarianceAnalyzer\n",
    "    from agents.actor_critic import ActorCriticAnalyzer\n",
    "    from agents.ppo import AdvancedPolicyGradientAnalyzer\n",
    "    from agents.continuous_control import ContinuousControlAnalyzer, ContinuousActorNetwork\n",
    "    from utils.hyperparameter_tuning import HyperparameterTuner, PolicyGradientBenchmark\n",
    "    \n",
    "    print(\"‚úÖ All project modules imported successfully!\")\n",
    "    print(f\"üñ•Ô∏è Computing device: {device}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Error importing modules: {e}\")\n",
    "    print(\"üîß Some demonstrations may not be available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Policy Gradient Foundations\n",
    "\n",
    "Before diving into specific algorithms, let's understand the theoretical foundations of policy gradient methods.\n",
    "\n",
    "### Policy Gradient Theorem\n",
    "\n",
    "The policy gradient theorem tells us how to compute the gradient of the expected return with respect to policy parameters:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi_\\theta}(s,a)]$$\n",
    "\n",
    "**Key Insight**: We can estimate gradients using samples from the current policy!\n",
    "\n",
    "### Mathematical Derivation\n",
    "\n",
    "Starting from the objective (expected return):\n",
    "\n",
    "$$J(\\theta) = \\mathbb{E}[R_\\tau] = \\sum_\\tau P(\\tau|\\theta) R(\\tau)$$\n",
    "\n",
    "Taking the gradient:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\sum_\\tau \\nabla_\\theta P(\\tau|\\theta) R(\\tau)$$\n",
    "\n",
    "Using the identity $\\nabla_\\theta \\log f(\\theta) = \\frac{\\nabla_\\theta f(\\theta)}{f(\\theta)}$:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\sum_\\tau P(\\tau|\\theta) \\nabla_\\theta \\log P(\\tau|\\theta) R(\\tau)$$\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[\\nabla_\\theta \\log P(\\tau|\\theta) R(\\tau)]$$\n",
    "\n",
    "Since $\\log P(\\tau|\\theta) = \\sum_{t=0}^T \\log \\pi_\\theta(a_t|s_t) + \\log T(s_{t+1}|s_t,a_t)$:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) R(\\tau)\\right]$$\n",
    "\n",
    "This becomes the **REINFORCE** algorithm.\n",
    "\n",
    "### Advantages of Policy Gradients\n",
    "\n",
    "1. **Continuous Action Spaces**: Natural handling of continuous actions\n",
    "2. **Stochastic Policies**: Learn mixed strategies\n",
    "3. **Stable Updates**: Smoother convergence than value-based methods\n",
    "4. **Direct Optimization**: Optimize exactly what we care about\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "1. **High Variance**: Gradient estimates can be noisy\n",
    "2. **Sample Inefficiency**: Often requires more samples than value methods\n",
    "3. **Slow Convergence**: Can converge slowly due to variance\n",
    "4. **Local Minima**: May get stuck in poor local optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate Policy Gradient Intuition\n",
    "from utils.policy_gradient_visualizer import PolicyGradientVisualizer\n",
    "\n",
    "pg_visualizer = PolicyGradientVisualizer()\n",
    "print(\"üéØ Demonstrating Policy Gradient Intuition...\")\n",
    "\n",
    "intuition_results = pg_visualizer.demonstrate_policy_gradient_intuition()\n",
    "\n",
    "print(\"\\nüÜö Value-based vs Policy-based Comparison...\")\n",
    "pg_visualizer.compare_value_vs_policy_methods()\n",
    "\n",
    "print(\"\\nüìà Advanced Visualizations...\")\n",
    "pg_visualizer.create_advanced_visualizations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: REINFORCE Algorithm\n",
    "\n",
    "REINFORCE (REward Increment = Nonnegative Factor √ó Offset Reinforcement √ó Characteristic Eligibility) is the simplest policy gradient algorithm, implementing the policy gradient theorem directly.\n",
    "\n",
    "### Algorithm Overview\n",
    "\n",
    "**Key Idea**: Use complete episode returns to estimate the policy gradient.\n",
    "\n",
    "**Algorithm Steps**:\n",
    "1. Initialize policy parameters Œ∏\n",
    "2. For each episode:\n",
    "   - Generate trajectory œÑ = {s‚ÇÄ, a‚ÇÄ, r‚ÇÄ, s‚ÇÅ, a‚ÇÅ, r‚ÇÅ, ...} following œÄ_Œ∏\n",
    "   - For each time step t:\n",
    "      - Compute return G_t = Œ£(k=t to T) Œ≥^(k-t) * r_k\n",
    "      - Update: Œ∏ ‚Üê Œ∏ + Œ± * ‚àá_Œ∏ log œÄ_Œ∏(a_t|s_t) * G_t\n",
    "\n",
    "### Mathematical Foundation\n",
    "\n",
    "The REINFORCE update rule directly implements the policy gradient theorem:\n",
    "\n",
    "$$\\theta_{t+1} = \\theta_t + \\alpha \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) G_t$$\n",
    "\n",
    "where G_t is the return (cumulative discounted reward) from time step t.\n",
    "\n",
    "### Key Properties\n",
    "\n",
    "- **Unbiased**: The gradient estimate is unbiased\n",
    "- **High Variance**: Uses full episode returns, leading to high variance\n",
    "- **Episode-based**: Requires complete episodes for updates\n",
    "- **On-policy**: Updates using trajectories from current policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train REINFORCE Agent\n",
    "from agents.reinforce import REINFORCEAnalyzer\n",
    "\n",
    "print(\"üöÄ Training REINFORCE Agent on CartPole-v1...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "reinforce_analyzer = REINFORCEAnalyzer()\n",
    "reinforce_agent = reinforce_analyzer.train_and_analyze(\n",
    "    env_name='CartPole-v1', \n",
    "    num_episodes=300\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Training completed! Check the plots above for learning curves.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REINFORCE Implementation Details\n",
    "\n",
    "```python\n",
    "class REINFORCEAgent:\n",
    "    def update(self):\n",
    "        \"\"\"Update policy using REINFORCE algorithm\"\"\"\n",
    "        episode_returns = []\n",
    "        episode_log_probs = []\n",
    "        \n",
    "        # Compute returns for each time step\n",
    "        for t in range(len(self.states)):\n",
    "            G_t = sum(self.gamma**k * self.rewards[t+k] \n",
    "                     for k in range(len(self.rewards[t:])))\n",
    "            episode_returns.append(G_t)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        returns = torch.tensor(episode_returns, dtype=torch.float32)\n",
    "        log_probs = torch.stack(self.log_probs)\n",
    "        \n",
    "        # Compute policy loss\n",
    "        policy_loss = -(log_probs * returns).mean()\n",
    "        \n",
    "        # Update policy\n",
    "        self.optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return policy_loss.item()\n",
    "```\n",
    "\n",
    "### Observations\n",
    "\n",
    "- REINFORCE achieved reasonable performance on CartPole\n",
    "- High variance in training rewards\n",
    "- Slow convergence compared to more advanced methods\n",
    "- Sensitive to hyperparameters (learning rate, discount factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Variance Reduction Techniques\n",
    "\n",
    "REINFORCE suffers from high variance in gradient estimates because it uses full episode returns. This leads to:\n",
    "- Slow convergence\n",
    "- Unstable training\n",
    "- Need for many episodes to get reliable gradient estimates\n",
    "\n",
    "### The Variance Problem\n",
    "\n",
    "The variance comes from several sources:\n",
    "1. **Environment randomness**: Stochastic dynamics and rewards\n",
    "2. **Policy randomness**: Stochastic action selection\n",
    "3. **Episode length variability**: Different episode lengths\n",
    "4. **Return variance**: Cumulative reward variance over time\n",
    "\n",
    "### Baseline Subtraction\n",
    "\n",
    "**Key Idea**: Subtract a baseline b(s) from returns without introducing bias.\n",
    "\n",
    "#### Mathematical Foundation\n",
    "\n",
    "The policy gradient with baseline:\n",
    "\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot (G_t - b(s_t)) \\right]$$\n",
    "\n",
    "#### Proof of Unbiasedness\n",
    "\n",
    "$$\\mathbb{E}[\\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot b(s_t)] = b(s_t) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s_t)$$\n",
    "\n",
    "$$= b(s_t) \\nabla_\\theta \\sum_a \\pi_\\theta(a|s_t) = b(s_t) \\nabla_\\theta 1 = 0$$\n",
    "\n",
    "#### Common Baseline Choices\n",
    "\n",
    "1. **Constant Baseline**: b = average return over recent episodes\n",
    "2. **State-Value Baseline**: b(s) = V(s) - learned value function\n",
    "3. **Moving Average**: b = exponentially decaying average of returns\n",
    "\n",
    "### Advantage Function\n",
    "\n",
    "The advantage function combines the benefits of baseline subtraction:\n",
    "\n",
    "$$A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s)$$\n",
    "\n",
    "This measures how much better action a is compared to the average action in state s.\n",
    "\n",
    "**Key Insight**: The advantage function has lower variance than raw returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Variance Reduction Techniques\n",
    "from agents.baseline_reinforce import VarianceAnalyzer\n",
    "\n",
    "print(\"üìâ Comparing Variance Reduction Techniques...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "variance_analyzer = VarianceAnalyzer()\n",
    "baseline_results = variance_analyzer.compare_baseline_methods(\n",
    "    env_name='CartPole-v1',\n",
    "    num_episodes=200\n",
    ")\n",
    "\n",
    "print(\"\\nüéØ Variance reduction analysis completed!\")\n",
    "print(\"Check plots above to see how different baselines affect variance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance Reduction Results Analysis\n",
    "\n",
    "From our experiments:\n",
    "\n",
    "- **No Baseline**: High variance, slow convergence, but straightforward\n",
    "- **Moving Average Baseline**: Simple but limited effectiveness\n",
    "- **Value Function Baseline**: Most effective variance reduction, best performance\n",
    "\n",
    "#### Why Value Function Baselines Work Best\n",
    "\n",
    "1. **State-specific**: Adapts to different states' expected returns\n",
    "2. **Learned optimization**: Continuously improved through training\n",
    "3. "Optimal baseline\": Minimizes variance theoretically\n",
    "\n",
    "#### Practical Implementation Tips\n",
    "\n",
    "```python\n",
    "# Value function baseline\n",
    "def compute_advantage(rewards, values, gamma):\n",
    "    \"\"\"Compute advantages using learned value function\"\"\"\n",
    "    advantages = []\n",
    "    for t in range(len(rewards)):\n",
    "        # Compute return from time t\n",
    "        G_t = sum(gamma**k * rewards[t+k] \n",
    "                 for k in range(len(rewards[t:])))\n",
    "        \n",
    "        # Compute advantage: return - value\n",
    "        A_t = G_t - values[t]\n",
    "        advantages.append(A_t)\n",
    "    \n",
    "    return advantages\n",
    "```"
   ]
  }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
